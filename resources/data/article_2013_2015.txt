Introduction
Un système de recherche d'information (SRI) est un module logiciel qui sélectionne, à partir d'une collection de documents, une liste de documents potentiellement pertinents en réponse à une requête utilisateur. Le processus suivi par un SRI est composé de 3 étapes.
Indexation. Cette étape permet de passer d'un document textuel à un document qui peut être utilisé dans la RI. Elle se base sur l'extraction des mots les plus importants des textes. Lors de cette étape, les mots vides tels que le, la, les sont généralement éliminés ; les termes sont ensuite racinisés, c'est-à-dire que des règles de transformation sur les termes sont appliquées afin d'obtenir un radical, limitant les variantes des termes à une forme unique ; enfin une pondération reflète l'importance des différents radicaux obtenus. Dans un cadre non dynamique, l'indexation est réalisée sur l'ensemble des documents, avant toute recherche.
Calcul des scores de pertinence des documents. Lorsqu'une requête est soumise au système, des scores de pertinence sont attribués aux termes qui la composent, en tenant compte de leur présence dans les documents. Ces scores sont ensuite combinés pour calculer le score global de chacun des documents de la collection. Il existe de nombreux modèles de pondéra-tion. La plupart sont basés sur les facteurs T F et IDF . L'expression T F (Term Frequency) correspond à la fréquence du terme dans le document, tandis que l'IDF (Inverse Document Frequency) désigne la fréquence inverse du terme dans le document, inversement proportionnel au nombre de documents qui contiennent le terme.
Reformulation de la requête. Cette étape permet de créer une requête plus adéquate à la RI que celle initialement formulée par l'utilisateur. Le principe de la reformulation automatique est de modifier la requête de l'utilisateur en ajoutant des termes significatifs ou en ré-estimant leurs poids. Dans sa version automatique, il s'agit de considérer les premiers documents restitués comme pertinents et d'ajouter des termes issus de ces documents ; de nouveaux poids sont également estimés et les scores des documents recalculés pour fournir la réponse finale du système. Ce paramètre n'est pas étudié dans le travail présenté dans cet article.
Chacune de ces étapes fait intervenir différents paramètres : par exemple lors de l'indexation, il est possible de choisir entre différents outils de racinisation, lors du calcul des scores de pertinence des documents, différents modèles de pondération peuvent être choisis. Les différents paramètres étudiés dans cet article sont présentés en figure 1.
L'efficacité d'un SRI est évaluée en calculant des mesures de performance comme le rappel, la précision et d'autres mesures associées. Depuis ses débuts, le domaine de la RI est très actif pour fournir de nouvelles propositions correspondant à une évolution de ces trois étapes. Lorsqu'un nouveau modèle de RI est proposé, ses paramètres sont étudiés, mais sans considé-rer les effets croisés. Par exemple dans Ponte et Croft (1998), ce sont les paramètres du modèle lui-même qui sont étudiés sans regarder l'influence du choix de l'algorithme de racinisation. Les modèles d'apprentissage d'ordonnancement (Learning to rank en anglais) considèrent de nombreux paramètres tels que les fréquences TF et IDF, la taille des documents et des caractéristiques comme les scores BM25, LMIR, PageRank des documents (Qin et al., 2010). Ces approches ont pour objectif d'optimiser l'ordonnancement des documents mais ne cherchent pas à connaître l'impact des paramètres. Quelques travaux visent à sélectionner les variables importantes, donc à étudier leur influence (Laporte et al., 2014;Naini et Altingovde, 2014).
Ainsi, généralement, les paramètres sont étudiés de façon indépendante, sans considérer les effets croisés des paramètres. Compte tenu du nombre de paramètres, l'étude des effets croisés est difficile et implique au préalable de collecter des données suffisantes pour le faire. Cet article s'attaque à ce problème. Ainsi, dans cette étude, nous nous appuyons sur un ensemble de données massif (2 millions de configurations) dans lequel les différents paramètres varient.
La littérature du domaine ne s'est que peu intéressée à une analyse de cette nature. Presque tous les articles et les thèses du domaine de la RI rapportent des études montrant la variation de mesures de performance en fonction d'un ou plusieurs paramètres, mais il ne s'agit pas d'une analyse en parallèle de paramètres variés. Quelques travaux se sont cependant intéressés à utiliser les méthodes d'analyse pour étudier les résultats de moteurs de recherche sur un ensemble de requêtes. Banks et al. (1999)   Compaoré et al. (2011) présentent une étude qui a les mêmes objectifs que ceux de ce papier. Cependant, le nombre d'éléments analysés et donc les combinaisons de paramètres est bien moindre. Dans leurs études, les auteurs montrent que les paramètres qui ont le plus d'influence sont différents en fonction que l'on considère les besoins d'information faciles ou difficiles. Bigot et al. (2014) utilisent les résultats d'une analyse pour sélectionner la configuration de système la plus adaptée en fonction de la difficulté du besoin d'information. L'objectif de l'analyse que nous présentons dans le présent papier est d'étudier à grande échelle les caractéristiques des SRI dans le but de déterminer les meilleures combinaisons de paramètres selon certaines mesures de performance. Ce travail a été mené dans le cadre du projet ANR CAAS (Contextual Analysis and Adaptive Search) ANR-10-CORD-001-01.
La suite de cet article est structurée comme suit. La section 2 présente la méthode utilisée pour obtenir les données ainsi que les données elles-mêmes. La section 3 présente l'analyse de la dépendance entre les paramètres et leur influence mutuelle. La section 4 s'attache à déterminer quelles sont les valeurs de paramètres les plus susceptibles de conduire à de bonnes performances du moteur de recherche correspondant. La section 5 conclut cet article.
Variantes de moteurs de recherche et paramétrage
Les données ont été générées via l'interface RunGeneration présentée dans Louédec et Mothe (2013) et qui est une sur-couche à la plateforme Terrier.
Terrier et RunGeneration
La plateforme de RI TERRIER (Ounis et al., 2006) possède de nombreuses possibilités de paramétrage, tant au niveau de l'indexation (indexation par blocs de différentes tailles, choix de la racinisation, etc.) que de la recherche (différents modèles de pondération pour la mise en correspondance entre la requête et les documents, différentes normalisations des poids) et de la reformulation de requêtes. Une fois paramétré, Terrier permet, pour un besoin d'information ou un ensemble de besoins, de retrouver les documents susceptibles de répondre à ce besoin.
L'interface RunGeneration a comme objectif de faciliter le paramétrage d'une chaine de traitement sous Terrier. Une fois les paramètres sélectionnés au travers de l'interface, celle-ci crée le fichier "terrier.properties" indispensable à Terrier et contenant l'ensemble des paramètres. Le second objectif de l'interface RunGeneration est de permettre de lancer plusieurs combinaisons de paramètres simultanément, ce qu'il n'est pas possible de faire en utilisant la plateforme Terrier. Ainsi plusieurs indexations et recherches sont effectuées sur les mêmes données via une seule intervention de l'utilisateur. Celui-ci peut par exemple demander en une
FIG. 1 -Paramètres de la génération des données.
action plusieurs indexations de documents avec des paramètres différents. L'interface a été développée pour fonctionner sur les versions 3.0 et 3.5 de Terrier 1 (Louédec et Mothe, 2013). Ainsi, lors de la génération des données utiles à notre analyse, le principe est le suivant : pour chaque combinaison de paramètres, une liste de documents retrouvés en réponse au besoin d'information est constituée. Cette réponse du système est alors évaluée sur la base de mesures de RI. Ainsi, pour une combinaison de paramètres, nous connaissons la valeur de chacune des caractéristiques correspondant aux paramètres du moteur et aux mesures de performance.
Paramètres utilisés lors de la génération de données
La figure 1 indique les variables utilisées lors de la génération des données ainsi que les modalités de ces variables paramètres. Le nombre de combinaisons obtenues est de 2 263 800. Les variables correspondant à des paramètres du système sont qualitatives.
Collection d'évaluation utilisée
Compte tenu du nombre de combinaisons et des temps nécessaires pour générer les données, dans cette étude, nous n'avons utilisé qu'une seule collection de documents : la collection TREC-8 de la tâche adhoc de TREC 2 . Elle comprend environ 530 000 documents soit 2 Go ; chaque document est composé en moyenne de 532 mots. La collection comprend également 50 besoins d'information et les jugements de pertinence des documents associés à ces besoins. Sur ce jeu de données, nous n'avons pas pris en compte les paramètres de reformulation de requêtes afin de ne pas rendre le nombre de combinaisons possibles trop grand pour être généré. Plutôt nous avons fait l'hypothèse qu'une première analyse permettrait de faire ressortir les paramètres principaux qui eux pourront être combinés avec les paramètres de reformulation. En effet, les principes de reformulation (implantés dans Terrier) s'appuient tous sur l'utilisation des premiers documents retrouvés suite à une première recherche. Aussi, optimiser la précision dans ces premiers documents, optimise à priori la reformulation de requêtes.
Caractéristiques d'évaluation associées
Afin d'évaluer les résultats obtenus nous avons utilisé trec_eval 3 qui calcule plus de 100 mesures de performance telles que bpref , AP et P @5. Cependant, nous avons restreint les variables utilisées à celles qui sont les moins corrélées. Ainsi, nous avons conservé les 6 mesures de performance préconisées dans Baccini et al. (2012). Elles sont résumées dans la table 1. Elles sont toutes quantitatives à valeur continue et leurs valeurs sont comprises entre 0 et 1. 
Distribution des valeurs des variables de mesure de performance
La figure 2 montre la distribution des valeurs des variables correspondant aux mesures d'évaluation de la performance. On note que le minimum 0 est atteint pour chacune des variables ; en revanche le maximum 1 n'est atteint que pour la P @30 et la iprec@recall0. L'ensemble de ces figures montre que les valeurs sont faibles ; la valeur 0 est la plus fréquente, montrant ainsi que beaucoup de configurations échouent dans la RI. Par ailleurs, comme les données ne suivent pas une loi normale, il faudra faire attention aux analyses réa-lisées par la suite pour ne choisir que celles qui s'appliquent à des variables qui ne suivent pas une loi normale. Cependant, les tests et méthodes que nous utilisons dans la suite restent valident car nous travaillons sur un grand jeu de données.
Corrélations entre variables de même type
Nous avons étudié la corrélation d'une part entre les variables correspondant aux paramètres du SRI et d'autre part entre variables d'évaluation des moteurs.
Nous avons analysé le lien entre les paramètres du moteur de RI, pris deux à deux afin de savoir si certaines de ces variables paramètres ont des rôles similaires ou sont liées entre elles. Pour cela, nous avons réalisé le test du chi 2 . Soit l'hypothèse H0 «les deux variables sont indépendantes» contre H1 «les deux variables ont un lien». Nous rejetons l'hypothèse H0 si la p-value est inférieure à 0, 05. Après avoir réalisé le test pour chacune des variables, nous concluons que toutes les variables qualitatives sont indépendantes deux à deux. Elles ont donc chacune leur rôle spécifique.
En revanche, en ce qui concerne les variables quantitatives correspondant aux mesures de performance, nous avons constaté qu'il existe une corrélation. Cette corrélation est plus ou moins importante en fonction des mesures que l'on compare. La figure 5  Ainsi les mesures de performance, déjà réduites à 6 pour plus de 100 au départ sont assez redondantes dans leur capacité à mesurer les performances des systèmes puisque corrélées, même si elles ne mesurent pas le même phénomène. Les paramètres du système en revanche n'étant pas corrélés, cela a un sens de chercher à optimiser chacun de ces paramètres.
Corrélations entre variables paramètres et variables d'évaluation
Afin d'étudier l'effet des paramètres sur les mesures de performance, nous avons effectué une analyse de la variance (ANOVA).
Soit l'hypothèse H0 «Le paramètre n'a pas d'effet sur la mesure de performance» contre H1 «le paramètre a un effet sur la mesure de performance». Nous rejetons H0 si la p-value est < 0, 05.
Nous avons étudié cette corrélation sur chacune des mesures de performance. La table 2  paramètres ont un effet significatif. Ces trois paramètres sans effet pourront donc être fixés dans la génération éventuelle d'autres données. Pour être réellement exhaustif, il faudrait vérifier que ces paramètres n'ont pas d'influence lorsque l'on change de collection, mais compte tenu de leur nature, la probabilité que cela soit le cas est forte.
Effet significatif
Effet non significatif TrecQueryTagsProcess, Topic BlocSize, IgnoreEmptyDocuments RetrievingModel, Stemmer IgnoreLowIdfTerms TAB. 2 -Effets significatifs et non significatifs pour l'ensemble des mesures de performance.
Variables ayant le plus d'influence
Nous avons utilisé la méthode Stepwise qui est une régression linéaire multiple Bendel et Afifi (1977) pour étudier l'influence relative des différentes variables paramètres. Cette mé-thode ajoute les variables les plus significatives du modèle et retire les moins significatives pas à pas. Dans le cadre de notre étude, elle a pour objectif de sélectionner les paramètres qui ont le plus d'influence sur les mesures de performance. C'est une combinaison de la mé-thode Forward et de la méthode Backward. La première méthode part du modèle vide et ajoute les variables les plus significatives du modèle progressivement, tandis que la seconde part du modèle complet et élimine progressivement les variables les moins significatives du modèle.
Après avoir réalisé cette analyse sur les différentes mesures de performance, nous observons que les trois variables supprimées sont BlocsSize, IgnoreEmptyDocuments et IgnoreLowIdf T erms, comme dans le cas de l'étude des corrélations précédente.
Au final cette méthode sélectionne donc les paramètres T opic, T recQueryT agsP rocess, RetrievingM odel et Stemmer. La variable T opic est la plus significative du modèle, suivi de T recQueryT agsP rocess, puis de RetrievingM odel et de Stemmer. Le besoin d'information considéré est le paramètre dont dépend le plus les résultats. Cela est un résultat important concernant la variabilité des résultats. On aurait pu penser que le modèle de recherche utilisé pouvait être le plus important des paramètres. La formulation du besoin d'information est éga-lement importante puisque le paramètre T recQueryT agsP rocess correspond aux parties du besoin d'information pris en compte lors du traitement. Lorsque seul le titre est considéré, il correspond à quelques mots, taille typique des requêtes sur le web. Lorsque les autres champs sont également considérés, il peut s'agir de requêtes plus longues, donnant un contexte précis du besoin d'information. Cette première analyse avait pour objet de déterminer les paramètres du moteur les plus importants ou qui influencent le plus la performance d'une recherche. Dans la section suivante, nous déterminons quelles sont les valeurs de paramètres les plus susceptibles de conduire à de bons résultats.
4 Paramètres des SRI pour des classes de précision L'objectif de cette analyse est d'étudier les valeurs des paramètres du moteur de recherche qui peuvent être associées à des valeurs de précision. Nous nous sommes appuyés dans cette étude sur la AP qui est la mesure consensuelle lorsqu'il s'agit de comparer globalement plusieurs systèmes et qui est utilisée en particulier dans la campagne d'évaluation TREC (trec.nist.gov) (Voorhees, 2007).
Classification mixte
La classification mixte est une méthode de classification qui a pour objectif d'obtenir, à partir des facteurs issus d'une analyse des correspondances multiples (ACM), des classes d'individus les plus cohérentes possibles en constituant les groupes les plus homogènes.
Dans notre cas d'étude, nous utilisons cette méthode afin d'associer à une classe de valeur de AP les paramètres de moteurs. L'idée sous-jacente est de favoriser les combinaisons de paramètres qui sont plutôt associées à des valeurs fortes de AP et d'éviter les combinaisons de paramètres plutôt associées à des valeurs faibles de AP .
Cette étude nécessite d'appliquer une ACM, méthode qui s'applique sur des variables qualitatives. Afin de transformer le paramètre d'évaluation qualitatif considéré (l'AP ) en valeurs qualitatives, nous avons créé des classes de valeurs de AP . Les classes ont été définies de sorte d'avoir des effectifs comparables. Nous noterons dans la suite la classe map1 la classe ayant FIG. 6 -Valeur des paramètres pour les classes de AP.
les valeurs de AP les plus faibles jusqu'à map4 la classe ayant les valeurs de AP les plus fortes. La figure 6 présente les résultats du croisement entre les classes de AP et les variables paramètres selon la méthode de classification mixte.
Nous pouvons observer dans la figure 6 les modalités présentes dans chacune de ces classes. La classe 1 contient des mesures de performance à valeurs faibles, la classe 2 des mesures de performance à valeurs moyennes, la classe 3 des mesures de performance à valeurs très faibles et la classe 4 des mesures de performance à valeurs élevées.
La combinaison de paramètres la plus représentative pour la classe 4, c'est-à-dire pour la classe ayant des mesures de performance à valeurs élevées est :
-IgnoreEmptyDocuments = TRUE -Stemmer = PS -RetrievingModel = LemurTFIDF -TrecQueryTagsProcess = TITLE -IgnoreLowIdfTerms = TRUE -IgnoreEmptyDocuments = TRUE -Stemmer = PS -RetrievingModel = LemurTFIDF -TrecQueryTagsProcess = TITLE -IgnoreLowIdfTerms = TRUE La combinaison de paramètres la plus représentative pour la classe 3, c'est-à-dire pour la classe ayant des mesures de performance à valeurs très faibles (map1) est :
-IgnoreEmptyDocuments = FALSE -Stemmer = Crop -RetrievingModel = DFI0 -TrecQueryTagsProcess = NARR -IgnoreLowIdfTerms = FALSE Cette combinaison de paramètres est donc à éviter, de même, que la combinaison de paramètres la plus représentative pour la classe 1, c'est-à-dire pour la classe ayant des mesures de performance à valeurs faibles (map2).
Conclusions et perspectives
Dans cet article, nous nous sommes intéressés à une analyse massive de résultats de recherche d'information obtenus par un paramétrage du système. Ainsi, de nombreux paramètres ont été analysés, en étudiant les effets croisés de ceux-ci. Nous avons pu distinguer les requêtes en fonction de leur niveau de difficulté et définir les paramètres qui ont le plus d'influence en fonction de ces classes ainsi que leurs valeurs les plus adaptées. Un aspect qui reste à étudier est l'influence de la collection sur les résultats obtenus. En effet, nous nous sommes ici intéressés à une collection unique (TREC8).
Dans la suite de ces travaux, nous allons travailler sur des méthodes sélectives de recherche d'information, c'est à dire des méthodes qui adaptent le traitement en fonction des cas rencontrés. Ainsi, toutes les requêtes ne seront pas traitées de la même façon par le moteur, mais les paramètres du système seront au contraire différents en fonction du type de requêtes.
Le projet CAAS, financé par l'ANR dans le cadre de l'appel Contint 2010, a permis de développer le travail présenté ici. Nous remercions également Anthony Bigot et Sébastien Déjean pour leurs précieux conseils.

Introduction
L'analyse d'opinions est une tâche de fouille de textes qui consiste en l'identification et la classification des textes subjectifs en plusieurs catégories d'opinions (polarités). Dans la dernière décennie, beaucoup de travaux se sont penchés sur cette problématique, en prenant le problème sous différents angles (principalement statistique et/ou linguistique). Cependant, la question de visualisation n'a pas bénéficié de cet intérêt. La plupart des travaux proposent une visualisation basique (e.g., graphiques en secteurs), ce qui est clairement insuffisant dans un contexte de big data où l'utilisateur a d'autant plus besoin d'explorer les données dans l'ensemble, mais aussi dans le détail.
Dans ce travail, nous nous situons dans un contexte de veille sur le Web et nous nous intéressons au problème d'analyse d'opinions dans un contexte de veille. Ainsi, nous proposons une méthode de visualisation d'opinions basée sur l'utilisation de termes clés afin de restituer le maximum d'information à l'utilisateur. Notre méthode est implémentée au sein de la plateforme de veille AMIEI 1 . La section suivante présente la problématique de recherche que nous traitons. La section 3 présente le processus général de veille avec la plateforme AMIEI. La section 4 présente notre approche pour l'analyse d'opinions et la visualisation des résultats. Enfin, la section 5 présente un exemple d'application sur un corpus de tweets politiques.
Contexte et Problématique
L'analyse d'opinions est un domaine de recherche qui se concentre sur l'identification et la classification des opinions dans les données textuelles. Beaucoup de travaux se sont intéressés à l'une ou l'autre de ces problématiques mais la plupart se sont intéressés à la classification d'opinions, i.e., l'association d'un texte à une catégorie d'opinions (e.g., opinion positive vs. négative).
La problématique a été majoritairement approchée sous un angle statistique et/ou linguistique. D'un point de vue statistique, le texte est représenté sur l'espace de descripteurs (e.g., termes) afin qu'il puisse être traité par les outils d'apprentissage statistique, e.g., Pak et Paroubek (2010); Pang et al. (2002). Ces méthodes sont connues pour leur généricité (bon rappel). De l'autre part, les méthodes de linguistique, également appelées méthodes à base de règles, ont été largement déployées pour l'analyse d'opinions, e.g., Kennedy et Inkpen (2006); Wilson et al. (2005). Ces méthodes sont connues pour leur spécificité (bonne précision). Enfin, d'autres travaux ont tenté de mixer la généricité de la statistique et la spécificité de la linguistique afin de proposer des méthodes à la fois robustes et précises (méthodes hybrides), e.g., Dermouche et al. (2013); Kamps et al. (2004); Turney et Littman (2003).
Dans ce travail, nous nous intéressons au problème de visualisation de l'opinion. En effet, la problématique de visualisation n'a pas été suffisamment étudiée dans ce domaine en se contentant de visualiser les proportions de chaque polarité d'opinion sur un graphique. Cette méthode est clairement insuffisante dans le cas où l'on veut savoir davantage sur ses données. Par exemple, dans le domaine industriel, il serait intéressant d'identifier les idées redondantes et les concepts qui sont présents dans une catégorie d'opinion et pas dans une autre. Une telle visualisation a des applications directes dans plusieurs domaines, e.g., la veille stratégique et économique, la CRM, la e-réputation, etc.
Acquisition de l'information
Cette phase permet l'acquisition de l'information selon plusieurs modes : -Un moteur de recherche pour faire des recherches ponctuelles pouvant être capitalisées. -Un automate de collecte pour des opérations récurrentes à des fins de capitalisation. 
Capitalisation et traitement
Partage de l'information
Le partage et la diffusion des informations acquises et validées, ainsi que les résultats de l'analyse se font à travers un portail de consultation, permettant la recherche et le partage des informations organisées par thématique avec une gestion des droits d'accès à partir de profils prédéfinis. Le partage peut également se faire via "Mon espace" ; un module permettant de personnaliser, pour chaque utilisateur, son accès à la plateforme AMI EI.
Pour l'analyse d'opinions, la plateforme AMIEI offre la fonctionnalités suivantes : -Indicateurs classiques de l'opinion globale dans un corpus de documents (distribution des documents sur les classes de polarité). -Visualisation des termes clés pour chaque classe de polarités. Un terme clé doit être fréquent et discriminant vis-à-vis de la classe d'opinion qu'il caractérise. -Evolution des termes clés à travers le temps. -Soit c la classe d'opinion du document d (classe la plus probable). -Evaluer chaque terme w i du document d selon un critère de spécificité (pouvoir discriminatif du terme au regard de la classe d'opinions). Ici, nous choisissons comme critère le gain informationnel (IG). Ensuite, trier les termes w i du document selon ce critère : IG(w m |c) > IG(w n |c) > ... > IG(w p |c). -Les K premiers termes sont ceux qui "expliquent" le mieux cette classification. Nous précisons que les termes discriminants de deux classes différentes sont deux ensembles disjoints. En effet, un terme ne peut être responsable d'affecter un texte qu'à une seule classe.
Méthode et implémentation
Visualisation
La visualisation est une étape clé dans le processus d'analyse d'opinions, d'autant plus dans un contexte de big data. En effet, l'information utile est encore plus enfouie et difficile à retrouver, ce qui nécessite des techniques de visualisation efficaces et adaptées à ce contexte particulier. Dans AMIEI, nous proposons de visualiser l'opinion contenue dans un corpus de FIG. 2 -Visualisation en nuage de termes (extrait).
FIG. 3 -Visualisation en fisheye (extrait).
documents par un "nuage de termes" construit à partir de l'ensemble des termes discriminants responsables de la classification (cf. section 4.1.). Chaque terme discriminant est ainsi repré-senté par une taille proportionnelle à sa fréquence dans le corpus des textes. Nous proposons également une visualisation temporelle du nuage de terms en utilisant la technique de fisheye.
Etude de Cas
Nous réalisons une expérimentation sur un corpus composé de 50000 tweets issus d'une collecte massive réalisée par la plateforme de veille AMIEI dans la soirée du 02 Mai 2012 avec le tag "#ledebat" (400000 tweets collectés). Ces tweets sont relatif au débat télévisé du second tour de l'élection présidentielle française de 2012 ayant opposé F. Hollande et N. Sarkozy. Nous appliquons, comme prétraitement, la suppression de mots outils et de numériques.
Les Figures 2 et 3 représentent la visualisation du résultat d'analyse du corpus Politique. Pour une meilleure lisibilité, seulement une sélection de termes fréquents est représentée ici. La polarité des termes est représentés par une couleur (vert pour le positif et rouge pour le négatif). Ces résultats nous ont permis de cerner les termes et les concepts les plus importants dans chaque catégorie d'opinion. A partir de cette visualisation, nous pouvons tirer plusieurs enseignements dont voici quelques uns : -Le concept de "changement" dans toutes ses variantes (slogan phare de la campagne du candidat F. Hollande) est largement repris par les internautes, et ce de manière positive.

Introduction
Le projet ANR IMAGIWEB 1 consiste à analyser et à suivre l'évolution de l'image (au sens de l'opinion) sur la toile, d'une part des personnages politiques à travers le réseau social Twitter, et d'autre part de l'entreprise EDF vis-à-vis du nucléaire en utilisant des blogs comme données. Ce projet regroupe différents partenaires parmi lesquels un laboratoire de recherche en science politique, des entreprises et des laboratoires de recherche en fouille de données.
Dans un premier temps, les tweets et blogs récoltés sont annotés manuellement pour relater l'opinion qu'ils véhiculent. Par la suite, l'enjeu sera de détecter automatiquement les opinions grâce à des méthodes de fouille d'opinion. Au-delà de la détection des opinions, pour mieux comprendre et analyser le contenu des tweets et des blogs, l'enjeu est aussi de les visualiser et de les explorer. Ainsi, un autre objectif du projet consiste à fournir à l'utilisateur, qu'il soit politologue, sociologue, marketeur ou encore analyste, un outil pour explorer les données (issues de tweets ou de blogs) et pour analyser en ligne l'opinion selon différents points de vue (sujets, temps, ...). L'analyse OLAP (OnLine Analytical Processing) permet de répondre à cet objectif de navigation, d'analyse et de visualisation.
L'OLAP sur des données textuelles correspond à une thématique de recherche récente avec des enjeux scientifiques importants. En effet, si l'OLAP a su montrer tout son potentiel analytique sur des "données classiques", la prise en compte de données textuelles nécessite une adaptation ou une évolution de l'OLAP pour prendre en compte les spécificités de ces données (Ravat et al., 2007;Zhang et al., 2009). Quelques travaux de recherche encore plus récents portent sur l'analyse OLAP de tweets, un cas particulier de données textuelles (Ben Kraiem et al., 2014;Bringay et al., 2011). Dans ce contexte, l'objectif de ce papier est de (1) démontrer l'intérêt de l'analyse OLAP pour ce type de données en se basant sur des cas d'étude réels, (2) relater une implémentation concrète "classique" en utilisant des outils existants.
Pour ce faire, dans la section 2 nous commençons par présenter les deux cas d'étude. Dans la section 3, nous évoquons les aspects de modélisation multidimensionnelle et de navigation. Dans la section 4, nous exposons la mise en oeuvre, avant de conclure dans la section 5.
Deux cas d'étude
Dans le cadre du projet IMAGIWEB, deux cas d'étude sont traités : des tweets à caractère politique et des billets de blogs traitant de l'entreprise EDF et du nucléaire. Pour chacun des cas, un processus d'annotation manuelle concernant l'opinion véhiculée a été mis en place.
Données tweets et besoins d'analyse
Dans le cadre du projet IMAGIWEB, les tweets ont été recueillis grâce à l'API Streaming de Twitter. Ce sont des tweets en français, à caractère politique, portant sur Nicolas Sarkozy et François Hollande, avant et après les élections prési-dentielles de 2012. Les données extraites sont le contenu du tweet, le pseudonyme du twittos, la date du tweet, l'image (à savoir François Hollande ou Nicolas Sarkozy, c'est à dire l'entité sur laquelle porte le tweet), l'URL qui mène vers le tweet.
Une annotation est faite par un annotateur sur un extrait ou un passage d'un tweet. L'annotateur détermine l'opinion contenue dans le passage (avec une polarité allant de -2 pour une opinion très négative à +2 pour une opinion très positive en passant par le zéro si l'opinion est neutre ou par le NULL s'il n'y a pas d'opinion) ainsi que la cible (le sujet sur lequel porte le passage) et la sous-cible. Les cibles et sous-cibles ont été déterminées par les membres du projet. Citons comme exemple de cible "bilan", "compétences", "positionnement". Pour la cible "positionnement", les sous-cibles sont "alliance", "écologie", "économie" et "sociétal". Enfin l'annotateur donne un niveau de confiance dans son annotation. 4073 tweets ont été annotés manuellement, ce qui a donné lieu à 5674 annotations.
Les données tweets constituent le terrain d'analyse des chercheurs en science politique et en sociologie. Les politologues souhaitent pouvoir suivre l'évolution dans le temps des deux images que sont François Hollande et Nicolas Sarkozy à travers Twitter. L'analyse de ces données, à la fois des tweets eux-mêmes et de leurs annotations, constitue un premier enjeu du projet.
Données blogs et besoins d'analyse
Les blogs à analyser concernent tout ce qui touche à EDF et au nucléaire. À partir d'un ensemble de blogs, tous les articles, en français, avec au moins une occurrence du sigle EDF ou des mots "Electricité de France" et de "nucléaire" ont été collectés. Les données contiennent le titre de l'article, l'URL du site web dont provient l'article, la date, le contenu textuel et l'image (sécurité, emploi ou prix). Les données blogs contiennent également le passage annoté (à chaque article correspond un ou plusieurs passages), la cible ("politique", "tarifs" ou encore "risques"), la sous-cible (par exemple "démantèlement/durée de vie" ou "expertise/incident" pour la cible "risques"), la polarité et la confiance. 560 articles ont été annotés manuellement en 3420 annotations (6,1 annotations par article en moyenne).
Par rapport aux besoins, les marketeurs d'EDF souhaitent centrer leur analyse sur les notions de cibles, de polarité. Ils souhaitent également pouvoir naviguer dans les données selon le type de structure (organisme) dont est issu le blog. Cette information peut être portée par l'extension du site web. Par exemple, une organisation à but non lucratif aura généralement un site web avec l'extension ".org" alors qu'une société aura un site web avec une extension ".com". Dans le modèle associé aux tweets (cf. figure 1), deux faits sont observés : ANNOTATION et TWEET. À ces faits sont associées plusieurs dimensions, comme le temps, la cible ou encore l'annotateur. Dans la dimension temps, on retrouve plusieurs niveaux de granularité de l'information avec deux hiérarchies : {jour, semaine, année} et {jour, mois, trimestre, année}.
Le fait TWEET va permettre de compter le nombre de tweets et de RT Confiance donne un indice quant à la confiance accordée à la polarité par l'annotateur. Le modèle permet également de retrouver l'image, la cible, l'annotateur, et bien sûr, le temps. La dimension annotateur rendra possible la comparaison de l'annotation automatique à celle manuelle le moment venu. Les dimensions image, cible et temps sont cruciales pour l'analyse. Une des particularités de ce modèle est de retrouver la polarité et la confiance aussi bien en mesure qu'en dimension. Cela permet de visualiser les données selon différentes manières. La polarité en tant que dimension permet par exemple de visualiser le nombre de fois où la polarité +2 est affectée alors qu'en la plaçant en tant que mesure, elle peut être agrégée avec des fonctions comme la somme ou la moyenne. Le modèle pour les blogs est assez similaire à celui des tweets. On retrouve deux faits Article et Annotation. Les mesures et les fonctions d'agrégat associées sont identiques. On retrouve également plusieurs dimensions en commun, à savoir la polarité, la confiance, la cible et le temps. Toutes ces notions similaires sont en fait celles associées au besoin commun concernant l'analyse de l'opinion. En revanche, notons comme différence que les blogs disposent d'un titre grâce à la dimension Blog et qu'ils sont également porteurs d'informations sur la structure qui héberge l'article (grâce à l'extension du site web) via une dimension Structure.
À partir du modèle multidimensionnel, pour introduire la navigation, la notion de cube OLAP est utilisée. Ainsi, deux cubes ont été créés concernant les données issues de Twitter et il en est de même pour les blogs. La navigation se caractérise par l'application d'opérateurs tels que le Drill Down qui permet d'aller vers un niveau plus détaillé selon la hiérarchie de dimension définie préalablement dans le modèle, en appliquant une fonction d'agrégat sur la mesure qui est observée. Il s'agit par exemple de passer de l'observation de la polarité moyenne par trimestre à l'observation par mois selon la hiérarchie temporelle. L'opérateur inverse s'appelle le Roll Up. Notons également l'existence de l'opérateur Slice & Dice qui permet de sélectionner certaines valeurs pour certains axes d'analyse. Par exemple, dans un cube qui permet d'observer le nombre de tweets par mois et par cible, il serait possible de sélectionner quelques cibles sur lesquelles on souhaite se focaliser.
Mise en oeuvre
Dans le cadre du projet IMAGIWEB, nous avons retenu MySQL comme SGBD en raison de contraintes techniques du projet. Nous avons également choisi de développer notre propre ETL (Extract Transform Load, phase correspondant à l'alimentation des données) car nous souhaitions pouvoir apporter des transformations très particulières en lien avec le contenu textuel (relatives à la fouille de texte) pour la suite du projet. Enfin, nous avons préféré le serveur OLAP Pentaho Mondrian en lui greffant l'interface graphique Saiku pour l'étendue de sa communauté et la prise en main de son environnement. L'implémentation résultante permet de naviguer dans les données en construisant des tableaux de bord très facilement pour l'utilisateur comme nous l'illustrons par la suite sur les données Twitter. Notons qu'il y a un menu sur l'interface qui permet également de représenter les données issues de la navigation sous forme de différents types de graphiques qui sont générés très simplement par l'utilisateur.
Initialement, le politologue peut par exemple observer la polarité moyenne en fonction du temps en trimestre pour les entités Hollande et Sarkozy. Puis, pour observer de façon plus précise, il peut obtenir le détail par mois (ce qui correspond au niveau OLAP à une opération de Drill Down), en se focalisant simplement sur Hollande (réalisant ainsi une opération de Slice), obtenant ainsi les résultats figurant dans la figure 2.
Ainsi, le politologue peut constater une baisse importante de popularité entre le mois de Mai et le mois de Juin (polarité de -0.346 à -0.658). Il peut ensuite détailler les cibles sur lesquelles cette baisse est plus importante, en ajoutant la dimension Cible dans les résultats. Le tableau 1 qui en résulte permet d'observer que l'opinion des Twittos a particulièrement diminué sur ses performances (-0.294 à -1.000) mais aussi sur son positionnement et son projet.
FIG. 2 -Polarité moyenne et nombre de tweets en fonction du temps en mois pour Hollande
L'intérêt pour le chercheur en science politique est ici, sur la base de la navigation, de pouvoir établir des liens entre l'opinion exprimée sur le web et des évènements de la vie politique, d'observer également à quel point le Web est un miroir ou non de l'opinion publique au sens large (comparaison avec les sondages d'opinion classiques).
Conclusion
Dans le cadre du projet IMAGIWEB, l'analyse OLAP était une des pistes à explorer pour visualiser les données. La mise en oeuvre de l'architecture décisionnelle a répondu à de réels 

Introduction
De nos jours, le maintien opérationnel d'un Système d'Information est devenu un des critères essentiels pour toute entreprise, ou personne cherchant à délivrer un service, ou simplement souhaitant communiquer. Le côté déplaisant de l'interconnexion mondiale des Systèmes d'Information réside dans un phénomène appelé "Cybercriminalité". Des personnes, des groupes mal intentionnés ont pour objectif de nuire aux informations d'une entreprise, d'une personne voire d'un Etat. Conséquemment, la détection des intrusions doit permettre de protéger le Système d'Information. L'objectif de cet article est de présenter dans un premier temps l'état de l'art en matière de détection d'intrusions et dans un second temps d'aborder les travaux menés afin de faciliter la visualisation des flux. La première partie de cet article sera consacrée à l'étude de l'existant dans laquelle nous présenterons les différentes approches de détection d'intrusions et leurs limites. Ensuite, nous nous intéresserons à la motivation de nos travaux et nous proposerons une solution. Nous détaillerons par la suite, la première phase de nos travaux ainsi que les résultats et nous terminerons par une conclusion et les perspectives.
Étude de l'existant
Une multitude d'outils (Antivirus, IDS, IPS, HIDS, Firewall) permettent aujourd'hui de mettre en place une sécurité "relative" pour l'ensemble du Système d'Information. Les principaux risques résiduels sont l'absence de constat en temps réel sur le signalement des comportements anormaux et sur l'exploitation des vulnérabilités. Il convient donc de répondre en fournissant des contremesures dans des délais raisonnables.
Les différentes solutions de détection d'intrusions
Les systèmes de détection des intrusions sont divisés selon les 3 familles distinctes :
-NIDS (Network Intrusion Detection System) est une sonde chargée d'analyser l'activité réseau du segment où elle est placée et de signaler les transactions anormales (Bhruyan et al (2011)). -HIDS (Host-Based Intrusion Detection System) est basée sur l'analyse d'un hôte selon les produits utilisés, une HDIS surveille le trafic à destination de l'interface réseau, l'activité système et logiciel, les périphériques amovibles pouvant être connectés. -HYBRIDES, qui rassemble les informations des NIDS et HIDS et produit des alertes aussi bien sur des aspects réseau qu'applicatifs. Il existe aussi une variante appelée "IPS" (Intrusion Prevention System) étant capable d'appliquer une politique de sécurité lors d'une intrusion. Un autre concept nommé CIDN 1 décrit par Fung (2011) offre la possibilité de partager des informations au travers un espace communautaire sur Internet. Les différentes solutions s'appuient sur deux méthodes, la première est fondée sur une comparaison d'une tentative d'intrusion par rapport à une base de signatures. Ce type de système recherche dans les trames réseau un schéma qui correspond à une signature connue via de l'extraction de motifs. Il est possible d'ajouter de nouvelles signatures, c'est à dire de créer une expression régulière qui correspondra par son contenu à une activité malveillante ou abusive. La seconde méthode repose sur des modèles comportementaux appelés "profils". Ils sont utilisés pour détecter les comportements déviant des profils définis. Les anomalies peuvent "signaler" une intrusion ou un nouveau comportement. Dans le second cas, il convient d'ajouter ces nouveaux comportements afin de diminuer les " faux positifs". Le concept de détection des anomalies repose sur une analyse statistique et un apprentissage temporel des comportements. Plusieurs principes de mise en place sont disponibles comme "IDES" 2 (Lunt et al, 1992), ou "EMERALD" 3 (Porras et Neumann, 1997 
Limitation des solutions existantes
Les principales limites des outils présentés dans les chapitres précédents résident dans le fait que l'analyse des événements et journaux systèmes est souvent considérés comme fastidieuse. De plus, ils ne prennent pas encore en compte l'évolution quasi permanente d'un Système d'Information. Par exemple, la sécurité d'un entrepôt de données peut être mise en cause par la non réévaluation du ou des serveurs hébergeant ce dernier. La structuration organisationnelle et l'analyse de risques s'avèrent donc indispensables. 
Motivations et proposition
Réalisation de la première phase
Cette phase constitue un tout en soi dans la mesure où la visualisation des données pour les utilisateurs est un enjeu crucial en terme de prise de décisions sur les problématiques de sécurité. Il s'agit du préambule à la "fouille de données" qui sera effectuée dans les phases suivantes. Un des principaux équipements de sécurité est le "Pare-Feu" , les données brutes envoyées en temps réel par l'ensemble des équipements de filtrage sont traitées selon une extraction de motifs .
Description des données
Le réseau SP1 propose des services à destination de 14 millions de personnes. Les données peuvent être considérées comme sensibles et portent sur une quantité de 9.2 Teraoctets et plusieurs dizaines de millions d'euros par jours. Ces données sont hétérogènes et proviennent de plusieurs sources différentes. Le contenu des variables listées ci-dessous est exporté vers les conteneurs de données. La phase 1 se focalisera uniquement sur l'analyse et la représentation graphique de ces dernières.
-adresse ip source, adresse ip de destination, port de destination, protocole (udp et tcp) -date et heure de la connexion -numéro de la règle du pare feu appliquée, action appliquée par la politique de filtrage.
Le tableau 2 synthétise le volume en nombre de lignes traitées par les équipements de filtrage. 
Scénario de visualisation
La représentation graphique de l'ensemble des flux autorisés selon la période souhaitée relève du problème de vision de grands graphes (voir figure 2), mais il est possible d'extraire des "sous graphes" basés sur du "requêtage" qui visent à sélectionner les modalités de certaines variables (adresses source et de destination ainsi que les services et protocoles). En revanche, l'analyse d'un graphique fondé sur les flux rejetés (même agrégés) comme le montre la figure 3 s'avère simple mais aussi efficace. Une adresse IP tente de se connecter à plusieurs autres adresses sur le port "135". Une recherche de répertoires partagés peut être à l'origine de ce type de comportement. 
Résultat
A l'issue de la phase 1, Le traitement des informations recueillies sur les différents équipe-ments de filtrage permet de visualiser rapidement les tentatives de connexion depuis plusieurs sources vers plusieurs destinations. Ceci rend possible de soulever des interrogations sur cette transaction et de mettre en place une action de surveillance. D'autres options ont été créées afin d'offrir une visualisation des règles de filtrage les plus utilisées. En cas de doute sur une 8. https ://www.perl.org/ 9. Logiciel de visualisation graphique, http ://www.graphviz.org 10. AfterGlow, outil de génération graphique, http ://afterglow.sourceforge.net/ 11. Responsables de la sécurité du système d'information, ingénieurs sécurité, administrateurs réseau adresse Ip, il est possible de lister toutes les activités de cette dernière selon des critères de temps, de destination, de protocoles et de ports utilisés.

Introduction
Dans cet article, nous proposons une nouvelle méthode de classification non supervisée de documents multilingues de corpus comparable bruité afin d'améliorer l'extraction des lexiques de traduction. Nous nous basons sur l'approche de (Rouane et al., 2007) dans la réingénierie des modèles UML et (Mimouni et al., 2012) dans la RI qui ont profité d'un couplage entre l'aspect formel et le relationnel afin de prendre en compte des relations entre les objets d'un même contexte. Nous avons choisi d'effectuer un couplage entre l'Analyse Formelle de Concepts (AFC) et les modèles vectoriels. En effet, l'AFC, appliquée dans un contexte de fouille de textes, permet d'extraire des classes de documents sous formes de CFs. D'un autre côté, les modèles vectoriels basés sur les vecteurs des extensions des CFs extraits, permettent d'aligner les CFs des différentes langues en calculant le degré de similarité des Concepts Fermés monolingues extraits, dans l'objectif de générer des CFs multilingues.
Extraction de Concepts Fermés à partir de corpus comparables
En classification de documents, un Concept Fermé est le couple < T, D >, avec T l'ensemble des termes des documents qui appartiennent à tous les documents D, et D, l'ensemble des documents qui contiennent tous les termes de T . Dans notre contexte de recherche, un Concept Fermé représente une classe de documents regroupés selon un ensemble de termes représentatifs. L'extraction des Concepts Fermés, à partir d'un corpus comparable français-anglais, est précédée par une étape de pré-traitement linguistique du corpus comparable bilingue mais aussi une réorganisation du contenu des documents du corpus en question est né-cessaire. Les concepts en sortie sont de la forme : CF =< {t 1 , t 2 , . . . , t n }, {d 1 , d 2 , . . . , d m } > tel que {t 1 , t 2 , . . . , t n } (ou extension) est l'ensemble des termes qui composent un termset fermé et {d 1 , d 2 , . . . , d m } (ou intension) l'ensemble de documents dans lesquels {t 1 , t 2 , . . . , t n } sont apparus ensemble avec une fréquence supérieure ou égale à minsupp. La sortie est composée de l'ensemble des Concepts Fermés français CF f r et des Concepts Fermés anglais CF en séparément.

?
rabdesselam/fr/ Résumé. Les résultats de toute opération de classification ou de classement d'objets dépendent fortement de la mesure de proximité choisie. L'utilisateur est amené à choisir une mesure parmi les nombreuses mesures de proximité existantes. Or, selon la notion d'équivalence topologique choisie, certaines sont plus ou moins équivalentes. Dans cet article, nous proposons une nouvelle approche de comparaison et de classement de mesures de proximité, dans une structure topologique et dans un objectif de discrimination. Le concept d'équivalence topologique fait appel à la structure de voisinage local. Nous proposons alors de définir l'équivalence topologique entre deux mesures de proximité à travers la structure topologique induite par chaque mesure dans un contexte de discrimination. Nous proposons également un critère pour choisir la "meilleure" mesure adaptée aux données considérées, parmi quelques mesures de proximité les plus utilisées dans le cadre de données quantitatives. Le choix de la "meilleure" mesure de proximité discriminante peut être vérifié a posteriori par une méthode d'apprentissage supervisée de type SVM, analyse discriminante ou encore régression Logistique, appliquée dans un contexte topologique. Le principe de l'approche proposée est illustré à partir d'un exemple de données quantitatives réelles avec huit mesures de proximité classiques de la littérature. Des expérimentations ont permis d'évaluer la performance de cette approche topologique de discrimination en terme de taille et/ou de dimension des données considérées et de sélection de la "meilleur" mesure de proximité discriminante.
Introduction
La comparaison d'objets, de situations ou d'idées sont des tâches essentielles pour évaluer une situation, pour classer des préférences ou encore pour structurer un ensemble d'éléments matériels ou abstraits, etc. En un mot pour comprendre et agir, il faut savoir comparer. Ces comparaisons que le cerveau accomplit naturellement, doivent cependant être explicitées si l'on veut les faire accomplir à une machine. Pour cela, on fait appel aux mesures de proximité. Une mesure de proximité est une fonction qui mesure la ressemblance ou la dissemblance entre deux objets d'un ensemble. Ces mesures de proximité ont des propriétés mathématiques et des axiomes précis. Mais est-ce que ces mesures sont toutes équivalentes ? Peuvent-elles être utilisées dans la pratique de manière indifférenciée ? Produiront-elles les mêmes bases d'apprentissage qui serviront comme entrée pour l'estimation de la classe d'appartenance d'un nouvel objet. Si nous savons que la réponse est non, alors comment pouvoir décider laquelle utiliser ? Certes, le contexte de l'étude ainsi que le type de données considérées peuvent aider à sélectionner quelques mesures de proximités, mais laquelle choisir parmi cette sélection ?
On retrouve cette problèmatique dans le cadre d'une classification supervisée ou d'une discrimination. L'affectation ou le classement d'un objet anonyme à une classe dépend en partie de la base d'apprentissage utilisée. Selon la mesure de proximité choisie, cette base d'apprentissage change et par conséquent le résultat du classement aussi.
On s'intéresse ici au degré d'équivalence topologique de discrimination de ces mesures de proximité. Plusieurs études d'équivalence topologique de mesures de proximité ont été proposées Batagelj et Bren (1992, 1995; Rifqi et al. (2003); Lesot et al. (2009);Zighed et al. (2012) mais pas dans un objectif de discrimination. Cet article met donc l'accent sur la façon de construction la matrice d'adjacence induite par une mesure de proximité, tout en tenant compte des classes d'appartenance des objets, connues a priori, en juxtaposant des matrices d'adjacence intra-groupe et inter-groupes Abdesselam (2014). Un critère de sélection de la "meilleure" mesure est proposé. On vérifie en effet a posteriori qu'elle est bien une bonne mesure discriminante en utilisant la méthode des SVM multi-classes.
TAB. 1 -Quelques mesures de proximité.
Où, p désigne la dimension de l'espace, Cet article est organisé comme suit. Dans la section 2, après avoir rappelé les notions de structure, de graphe et d'équivalence topologique, nous présentons la façon dont a été construite la matrice d'adjacence dans un but de discrimination, le choix de la mesure du degré d'équivalence topologique entre deux mesures de proximité ainsi que le critère de sélection de la "meilleure" mesure discriminante. Un exemple illustratif est commenté en section 3. Une conclusion et quelques perspectives de cette approche sont données en section 4.
Le tableau 1 présente quelques mesures de proximité classiques utilisées pour des données continues, définies sur R p .
Equivalence topologique
L'équivalence topologique repose en fait sur la notion de graphe topologique que l'on dé-signe également sous le nom de graphe de voisinage. L'idée de base est en fait assez simple : deux mesures de proximité sont équivalentes si les graphes topologiques correspondants induits sur l'ensemble des objets restent identiques. Mesurer la ressemblance entre mesures de proximité revient à comparer les graphes de voisinage et à mesurer leur ressemblance. Nous allons tout d'abord définir de manière plus précise ce qu'est un graphe topologique et comment le construire. Nous proposons ensuite une mesure de proximité entre graphes topologiques qui servira par la suite à comparer les mesures de proximité.
Graphe topologique
Sur un ensemble de points x, y, z, . . . de R p , on peut, au moyen d'une mesure de proximité u définir une relation de voisinage V u qui sera une relation binaire sur E × E. Pour simplifier la compréhension mais sans nuire à la généralité du propos, considérons un ensemble d'objets E = {x, y, z, . . .} de n = |E| objets plongés dans R p . Ainsi, pour une mesure de proximité u donnée, nous pouvons construire un graphe de voisinage sur un ensemble d'individus dont les sommets sont les individus et les arêtes sont définis par une propriété de la relation de voisinage.
De nombreuses définitions sont possibles pour construire cette relation binaire de voisinage. On peut, par exemple, choisir l'Arbre de Longueur Minimale (ALM) Kim et Lee (2003), le Graphe de Gabriel (GG) Park et al. (2006), ou encore le Graphe des Voisins Relatifs (GVR) Toussaint (1980), dont tous les couples de points voisins vérifient la propriété suivante :
) V u (x, y) = 0 sinon c'est-à-dire, si les couples de points vérifient ou pas l'inégalité ultratriangulaire (1), condition ultramétrique.
La figure 1 montre, dans R
, un exemple de graphe topologique GVR parfaitement défini par la matrice d'adjacence V u associée, formée de 0 et de 1. Sur le plan géométrique, cela signifie que l'hyper-Lunule (intersection des deux hypersphères centrées sur les deux points) est vide.
Comparaison de mesures de proximité
On dispose de p variables quantitatives explicatives (prédicteurs) {x j ; j = 1, p} et d'une variable qualitative cible à expliquer y, partition de n = ? q k=1 n k individus-objets en q modalités-groupes {G k ; k = 1, q}.
Pour toute mesure de proximité u i donnée, on construit, selon la propriété (1), la matrice d'adjacence binaire globale V ui qui se présente comme une juxtaposition de q matrices d'adjacence symétriques Intra-groupe {V
A noter que la matrice d'adjacence partitionnée globale V u i ainsi construite, n'est pas symétrique. En effet, pour deux objets
• Le premier objectif est de regrouper les différentes mesures de proximité considérées, selon leur similitude topologique pour mieux visualiser leur ressemblance dans un contexte de discrimination.
Pour mesurer le degré d'équivalence topologique de discrimination entre deux mesures de proximité u i et u j , nous proposons de tester si les matrices d'adjacence associées V u i et V u j sont différentes ou pas. Le degré d'équivalence topologique entre deux mesures de proximité est mesuré par la quantité :
0 sinon.
• Le second objectif consiste à établir un critère d'aide à la sélection de la "meilleure" mesure de proximité ; celle qui parmi toutes les mesures considérées, discrimine au mieux les q groupes.
On note, V u * = diag (1 G 1 , . . . , 1 G k , . . . , 1 G q ) la matrice d'adjacence symétrique blocdiagonale de référence "discrimination parfaite" associée à la mesure de proximité inconnue, notée u * , où, 1 n k désigne le vecteur d'ordre n k dont toutes les composantes sont égales à 1 et 1 G k = 1 n k t 1 n k , la matrice carrée d'ordre n k dont tous les éléments sont égaux à 1.
On peut ainsi établir le degré d'équivalence topologique de discrimination S(V u i , V u * ) entre chaque mesure de proximité u i considérée et la mesure de référence u * . Enfin, afin d'évaluer autrement le choix de la "meilleure" mesure de proximité discriminante proposée par cette approche, nous avons appliqué a posteriori une technique de classement par SVM Multiclasses (MSVM) sur la matrice d'adjacence associée à chacune des mesures de proximité considérée y compris à la mesure de référence u * .
Exemple d'application
Pour illustrer notre approche, nous considérons ici un jeu de données bien connu et relativement simple, celui des Iris Fisher (1936); Anderson (1935). Ces données ont été proposées comme données de référence pour l'analyse discriminante et la classification par le statisticien Ronald Aylmer Fisher en 1933. Les données complètes se trouvent notamment dans UCI (2013). Quatre variables (longueur et largeur des sépales et pétales) ont été observées sur 50 fleurs de chacune des 3 espèces d'Iris (Iris Setosa, Iris Virginica, Iris Versicolor).
Comparaison et classement des mesures de proximité
Les principaux résultats de l'approche proposée sont présentés dans les tableaux et le graphique suivants. Ils permettent de visualiser les mesures qui sont proches les unes des autres selon l'objectif de discrimination.
Pour ce jeu de données, le tableau 2 récapitule les similarités entre les 8 mesures de proximité et montre que la mesure u T ch de Tchebychev est la plus proche de la mesure u * de référence.
Une Analyse en Composantes Principales (ACP) suivie d'une Classification Hiérarchique Ascendante (CHA) ont été effectuées à partir de la matrice de similarités entre les 8 mesures de proximité considérées, afin de les partitionner dans des groupes homogènes et de visualiser leurs ressemblances.
L'application d'un algorithme de construction d'une CHA selon le critère de Ward, Ward Jr (1963), permet d'obtenir le dendogramme de la figure 2. Le vecteur similarités S(V ui , V u * ) de 
TAB. 3 -Classement de la mesure de référence.
Au vu des résultats présentés dans le tableau 3 de la partition en 5 classes de mesures de proximité, la mesure de référence inconnue u * , projetée en élément supplémentaire, serait donc plus proche des mesures de la classe 3, c'est-à-dire, de la mesure de Tchebychev u T ch qui serait pour ces données, la "meilleure" mesure de proximité parmi les 8 mesures considé-rées. Ce résultat confirme celui constaté dans le tableau 2, à savoir, une plus grande similarité S(V u T ch , V u * ) = 68.10% de la mesure de Tchebychev avec celle de référence u * .
Les mesures discriminantes selon les MSVM
Cette partie consiste à valider les résultats du choix de la meilleure mesure au vu de la matrice de référence a posteriori en utilisant les MSVM. Nous utilisons ici le modèle M SV M LLW , Lee et al. (2004), considéré comme le plus fondé théoriquement du fait que sa solution donne un classifieur qui converge vers celui de Bayes.
Mesure
Erreur Pour ce faire, nous allons appliquer une des techniques de sélection du modèle consistant à tester plusieurs valeurs du paramètre et à choisir celle qui minimise l'erreur test calculée par validation croisée. Dans cet exemple, nous testons 10 valeurs du paramètres (entre 1 et 100) pour toutes les bases de données. Aprés simulations, la valeur choisie est C = 1.
Les principaux résultats du modèle M SV M LLW , appliqué sur chacune des matrices d'adjacence induites par les mesures de proximité considérées, sont présentés dans le tableau 4.
Le meilleur taux d'erreur est celui donné par les mesures de Tchebechev u T ch et Euclidienne u E , qui est aussi celui enregistré pour la matrice de référence.
L'application du modèle MSVM montre que les mesures de proximité de Tchebechev u T ch et Euclidienne u E sont les plus adaptées pour différencier et séparer au mieux les 3 espèces d'Iris. Ce résultat confirme celui obtenu précédemment, à savoir le choix de la mesure de Tchebychev u T ch comme la mesure plus proche, parmi les huit mesures considérées, de la mesure de référence et donc la plus discriminante.
Expérimentations
Nous avons procédé à des expérimentations sur d'autres jeux de données afin d'essayer d'évaluer l'effet des données, de leur taille et/ou de leur dimension sur les résultats de la classification des mesures de proximité toujours dans un but de discrimination. Est-ce que, par exemple, les mesures de proximité se regroupent différemment selon le jeu de données utilisé ? Selon la taille de l'échantillon et/ou le nombre de variables explicatives considérées dans un même ensemble de données ?
Pour répondre à ces questions, nous avons donc appliqué l'approche proposée sur différents jeux de données, présentés dans le tableau 5, qui proviennent tous du référentiel UCI (2013). L'objectif est de comparer les résultats des classifications des mesures de proximité de toutes ces expérimentations ainsi que la "meilleure" mesure discriminante proposée pour chacun de ces jeux de données.
Etant donné un ensemble de données explicatives X (n,p) à n objets et p variables, et une variable à expliquer ou à discriminer Y q à q modalités-classes.
Pour analyser l'effet du changement de dimension, nous avons considéré le jeu de données "Waveform Database Generator" pour générer 3 échantillons n?4 de taille n = 2000 objets et de dimension p égale respectivement à 40, 20 et à 10 variables.
De même, pour évaluer l'effet du changement de la taille de l'échantillon, nous avons éga-lement généré 3 autres échantillons n?5 de taille n égale respectivement à 3000, 1500 et à 500 objets et de même dimension p égale à 30 variables.
Les principaux résultats de ces expérimentations, à savoir les équivalences topologiques des mesures de proximité discriminantes et l'affectation de la mesure de référence u * dans la classe la plus proche, sont présentés dans le tableau 6.
Pour chacune de ces expérimentations, nous avons retenu une partition en cinq classes de mesures de proximité afin de les comparer et de bien distinguer les mesures de la classe d'appartenance de la mesure de référence, c'est-à-dire les mesures les plus discriminantes.
Les regroupements des mesures de proximité obtenus pour les trois jeux de données n?4 sont pratiquement identiques, il n'y a donc pas vraiment d'effet de la dimension. 
une nouvelle approche d'équivalence entre mesures de proximité dans un contexte de discrimination. Cette approche topologique est basée sur la notion de graphe de voisinage induit par la mesure de proximité. D'un point de vue pratique, dans ce papier, les mesures que nous avons comparées sont toutes construites sur des données quantitatives. Mais ce travail peut parfaitement s'étendre aux données qualitatives en choisissant la bonne structure topologique adaptée.
Nous envisageons d'étendre ce travail à d'autres structures topologiques et d'utiliser un critère de comparaison, autre que les techniques de classification, afin de valider le degré d'équivalence entre deux mesures de proximité. Par exemple, évaluer le degré d'équivalence topologique de discrimination entre deux mesures de proximité en appliquant le test non paramètrique du coefficient de concordance de Kappa, calculé à partir des matrices d'adjacence associées, Abdesselam et Zighed (2011). Cela va permettre de donner une signification statistique du degré de concordance entre les deux matrices de ressemblance et de valider ou pas l'équivalence topologique de discrimination, c'est-à-dire si vraiment elles induisent ou pas la même structure de voisinage sur les groupes d'objets à séparer.
Enfin, les expérimentations menées sur différents jeux de données ont montré qu'il n'y pas du tout d'effet de la dimension et pas vraiment d'effet de la taille de l'échantillon aussi bien sur les regroupement des mesures de proximité que sur le résultat du choix de la meilleure mesure discriminante.
proximity measures in a topological structure and a goal of discrimination. The concept of topological equivalence uses the structure of local neighborhood.
Then we propose to define the topological equivalence between two proximity measures, in the context of discrimination, through the topological structure induced by each measure. We also propose a criterion for choosing the "best" measure adapted to data considered among some of the most used proximity measures for quantitative data. The choice of the "best" discriminating proximity measure can be verified retrospectively by a supervised learning method type SVM, discriminant analysis or Logistic regression applied in a topological context.
The principle of the proposed approach is illustrated using a real quantitative data example with eight conventional proximity measures of literature. Experiments have evaluated the performance of this discriminant topological approach in terms of size and/or dimension of the relevant data and of selecting the "best" discriminant proximity measure.

Introduction
Ce papier propose un nouveau mécanisme d'optimisation pour l'algorithme de classification automatique évidentielle semi-supervisée SECM , qui est le premier à reposer sur des contraintes exprimées sous la forme de données étiquetées. Les algorithmes de classification évidentielle (Masson et Denoeux, 2008) reposent sur le cadre théorique des fonctions de croyance et permettent de représenter tous les types d'affectations partielles grâce au concept de partition crédale qui étend la notion de partition stricte, floue et possibiliste. Ces méthodes évidentielles ont été étendues dans le cadre semi-supervisé (Antoine et al., 2012 pour pouvoir tirer partie de contraintes de type Must-Link (ML) et Cannot-Link (CL) qui spécifient si deux données doivent ou non appartenir à la même classe. La transformation des informations disponibles a priori en ce type de contraintes peut néanmoins induire une perte de connaissance. L'algorithme SECM a été proposé récemment pour tirer partie de données partiellement étiquetées . Cependant, l'algorithme SECM initial repose sur une optimisation stricte qui respecte l'ensemble des contraintes et notamment la positivité des masses de croyances associées à l'affectation d'un point à une classe. Cette contrainte théorique entraîne la formation d'un problème complexe. Nous proposons donc de modifier le mécanisme d'optimisation en relâchant la contrainte de positivité, à l'instar de ce qui est fait dans (Bouchachia et Pedrycz, 2006), et en s'assurant a posteriori de l'optimisation que les masses de croyances sont positives. Nos résultats expérimentaux montrent que notre heuristique ne dégrade pas les performances de l'algorithme SECM et permet de gagner de manière significative en complexité sur nos jeux de tests.
Ce papier est organisé comme suit : la section 2 présente les concepts fondamentaux de la théorie des fonctions de croyance et les principales méthodes de classification automatique sous contraintes. L'algorithme semi-supervisé SECM  est ensuite décrit dans la section 3 et un nouveau schéma d'optimisation est proposé. Enfin, les résultats de l'algorithme sont présentés dans la section 4. Le papier conclut sur l'intérêt de la nouvelle méthode d'optimisation.
2 Travaux existants 2.1 Les fonctions de croyance L'intérêt principal d'un algorithme de classification évidentielle est de pouvoir représenter le doute concernant l'affectation d'un point à un cluster. Pour ce faire, ces méthodes reposent sur la théorie de l'évidence de Dempster-Shafer, également appelée théorie des fonctions de croyance (Shafer, 1976;Smets et Kennes, 1994). Soit ? une variable prenant ses valeurs dans un ensemble fini ? = {? 1 , . . . , ? c } appelé cadre de discernement. La connaissance partielle concernant la valeur de ? peut être représentée par une fonction de masses m, qui est une application de l'ensemble des parties de
Les sous-ensembles A ? ? tels que m(A) > 0 sont appelés les éléments focaux de m. La quantité m(A) s'interprète comme la quantité de croyance allouée à A et qui, faute d'information complémentaire, ne peut être allouée à aucun autre sous-ensemble de A. L'ignorance totale correspond à m(?) = 1 alors qu'une certitude totale se rapporte à l'allocation complète de la masse de croyance sur un unique singleton de ?. Si tous les ensembles focaux de m sont des singletons, alors la fonction de masses de croyances est équivalente à une distribution de probabilités. La quantité m(?) peut être interprétée comme la croyance que la valeur réelle de ? n'appartient pas à ?. Quand m(?) = 0, la fonction de croyance est dite normalisée. La connaissance exprimée par une fonction de croyance peut aussi être représentée par une fonction de plausibilité pl : 2 ? ? [0, 1] définie comme suit :
B?A? =? La quantité pl(A) est interprétée comme le degré maximal de croyance qui peut potentiellement être affecté à l'hypothèse selon laquelle la vraie valeur de ? appartient à A. Quand une décision doit être prise concernant la valeur de ?, il est intéressant de transformer une fonction de masses en probabilité pignistique (Smets et Kennes, 1994) :
??A où |A| dénote la cardinalité de A ? ?. Quand il existe m(?) ? = 0, une étape de normalisation doit précéder la transformation pignistique. La normalisation de Dempster, qui consiste à diviser toutes les masses par 1 ? m(?), est une méthode classique de normalisation.
Algorithme des c-moyennes évidentielles
La version évidentielle des k-moyennes, ECM, est un algorithme de classification automatique qui construit une partition crédale à partir des données. Dans ce formalisme, la connaissance partielle concernant l'appartenance d'un objet x i est représentée par une fonction de croyance m i sur l'ensemble ? des classes possibles. Ainsi, un degré de croyance peut être affecté aux singletons (comme dans les approches floues et possibilistes) mais également à tous les sous-ensembles de ?. Soit {x 1 , . . . x n } un ensemble d'individus dans R p à classer dans un ensemble ? = {? 1 , . . . ? c } de c classes. Pour chaque objet x i , la fonction de croyance m i est calculée en plaçant une grande (resp. petite) quantité de croyance sur le sous-ensemble proche (resp. éloigné) en terme de distance de x i . La distance d ij est une métrique définie entre un objet x i et une représentation dans R p d'un sous-ensemble A j ? ?. Similairement à l'algorithme des c-moyennes floues, chaque classe ? k est représentée par un prototype v k . Pour chaque sous-ensemble A j ? ?, A j ? = ?, un centre v j est calculé comme le barycentre des centres associés aux classes composant A j :
La distance d 2 ij peut être définie comme une distance euclidienne (Masson et Denoeux, 2008). Plus récemment, une variante a été proposée pour prendre en compte une distance de Mahalanobis (Antoine et al., 2012). Similairement aux travaux de (Gustafson et Kessel, 1979), cette distance permet de détecter des clusters ayant différentes formes géométriques, grâce à une matrice de covariance floue S k associée à chaque cluster ? k et qui doit être optimisée.
Ensuite, similairement à ce qui est fait pour les prototypes, pour chaque sous-ensemble de A j qui n'est pas un singleton, une matrice S j est calculée en moyennant les matrices incluses dans
L'algorithme ECM minimise la fonction objectif suivante en fonction des matrices M, V et S précédentes :
Comme m i? correspond à la croyance que x i est un point aberrant, son cas est traité séparément du reste des autres sous-ensembles. Le paramètre ? indique la distance de l'ensemble des objets à l'ensemble vide. Il est intéressant de remarquer à ce niveau qu'une pénalité des sous-ensembles A j ? ? avec une grande cardinalité a été introduite avec la pondération |A j | ? . L'exposant ? permet de contrôler le degré de cette pénalisation.
Tout comme pour les c-moyennes floues, la partition est construite selon un processus itératif qui optimise alternativement les matrices M, V et S. La complexité d'un algorithme évidentiel est linéaire avec le nombre de données mais exponentiel avec le nombre de classes. En conséquence, il est crucial pour ce type de méthodes de minimiser les calculs réalisés dans les phases d'optimisation comme cela est proposé dans cet article.
Algorithmes semi-supervisés
La plupart des méthodes de classification automatique ont été améliorées pour prendre en compte la connaissance experte sous la forme de contraintes soit entre paires de données de type Must-Link (ML) ou Cannot-Link (CL) qui indiquent si deux points doivent ou non appartenir au même cluster, soit sous la forme de données étiquetées (Wagstaff et al., 2001). Citons par exemple des algorithmes de type k-moyennes (Wagstaff et al., 2001;Basu et al., 2002), hiérarchiques (Davidson et Ravi, 2005), basés sur la densité (Ruiz et al., 2010;Lelis et Sander, 2009), des méthodes spectrales (Wang et Davidson, 2010) ainsi que des algorithmes dédiés aux flux de données (Ruiz et al., 2009). D'autres travaux se sont intéressés à l'intégration de contraintes dans l'algorithme des c-moyennes floues (Grira et al., 2006;Pedrycz, 1985;Bensaid et al., 1996;Pedrycz et Waletzky, 1997a). Pour palier les limitations des algorithmes flous en présence de bruit ou de points aberrants, des méthodes possibilistes (Krishnapuram et Keller, 1993;Sen et Davé, 1998) et plus récemment évidentielles ont été proposées (Masson et Denoeux, 2008. Ces dernières ont également été étendues au cas semi-supervisé pour bénéficier des avantages des modèles basés sur les fonctions de croyance dans la prise en compte de la connaissance experte. Les travaux proposés reposent soit sur des contraintes ML et CL (Antoine et al., 2012 soit, plus récemment, sur des données partiellement étiquetées avec l'algorithme SECM . D'un point de vue formel, deux approches ont été proposées dans la littérature pour prendre en compte les contraintes et les étiquettes pendant le processus de classification automatique. En premier lieu, il est possible de modifier le processus des algorithmes de classification, soit durant la phase d'initialisation (Basu et al., 2002), soit pendant la phase de convergence. Dans ce dernier cas, on peut soit imposer un respect strict des contraintes comme dans COP Kmeans (Wagstaff et al., 2001), soit modifier la fonction objectif pour pénaliser les solutions qui ne respectent pas complètement les contraintes (Pedrycz et Waletzky, 1997b). Par exemple, dans (Bouchachia et Pedrycz, 2003), les auteurs décrivent un FCM amélioré dont la fonction objectif introduit un terme de pénalité qui considère à la fois l'appartenance actuelle des points i aux classes k notée u ik , mais également l'appartenance telle qu'elle devrait être à partir des contraintes de l'expert notée˜unotée˜ notée˜u ik comme le montre l'équation (7).
TAB. 2 -Plausibilités calculées à partir de la partition crédale.
où U et V dénotent respectivement la matrice d'appartenance et les coordonnées des centres des clusters. ? est un paramètre de régulation qui permet d'équilibrer l'importance du respect des contraintes dans la fonction objectif.
En second lieu, d'autres méthodes proposent d'adapter la métrique en fonction des contraintes et étiquettes fournies par l'expert comme dans l'algorithme MPC k-means (Bilenko et al., 2004). Par exemple, dans (Bouchachia et Pedrycz, 2006), les auteurs proposent une mé-thode pour adapter une distance de Gustafson-Kessel (Gustafson et Kessel, 1979). Nous proposons dans ce papier de considérer un modèle de contraintes flexibles avec une modification de la fonction objectif qui pénalise les solutions ne respectant pas les données étiquetées.
3 Algorithme SECM 3.1 Formalisation du problème L'idée principale de l'algorithme proposé dans  est d'ajouter un terme de pénalité dans la fonction objectif de ECM afin de prendre en compte un ensemble d'objets étiquetés. La démarche suivie est la même que dans (Bouchachia et Pedrycz, 2003) mais rapportée aux algorithmes évidentiels. L'expression d'un objet étiqueté peut se traduire sous la forme d'une fonction quantifiant la croyance sur l'appartenance de l'objet à une classe. Considérons dans un premier temps une partition crédale connue et définie par le tableau 1. Elle représente la connaissance partielle de l'appartenance de quatre objets à deux classes. Il est alors possible de calculer la plausibilité de chaque objet x i pour chaque classe ? k , comme illustré par le tableau 2. On remarque alors qu'une plausibilité nulle permet de déduire avec certitude qu'un élément n'appartient pas à une classe. Ainsi, l'observation de pl i (? 1 ) = 0 permet de déduire que x 1 , un objet atypique, et x 4 , un objet affecté avec certitude dans la classe ? 2 , ne font pas partie de la classe ? 1 . En revanche, les objets dont la plausibilité pour une classe est élevée ont des chances d'appartenir à cette classe. Ainsi, pl i (? 1 ) = 1 apparaît pour l'objet x 2 , qui appartient à la classe ? 2 avec certitude, et pour l'objet x 3 , qui appartient soit à ? 1 , soit à ? 2 .
Supposons maintenant que l'on ne dispose pas de la partition crédale, mais qu'il existe des contraintes sous formes d'étiquettes. Par exemple, l'objet x i est inclus dans la classe ? k . Il est alors possible d'imposer la contrainte pl i (? k ) = 1. L'effet sera d'exiger :
-une croyance élevée pour les fonctions de masse ayant un sous-ensemble comprenant ? k , donc toutes les fonctions de masses qui ont un degré de croyance plus ou moins fort sur le fait que x i appartienne à ? k , -des valeurs faibles pour toutes les fonctions de masses ayant un sous-ensemble qui n'incluent pas ? k . La contrainte entre un objet x i et la classe ? k est donc respectée pour de nombreuses solutions allant de la certitude totale que x i appartienne à ? k jusqu'à l'incertitude complète de l'affectation de x i entre ? k ou plusieurs autres classes de ?. La contrainte est donc flexible car elle permet si nécessaire de garder un doute quant à l'affectation de l'objet à la classe. Par conséquent, cela limite l'influence négative d'une contrainte bruitée.
Lorsqu'un expert crée des contraintes d'étiquettes, il peut avoir un doute entre plusieurs classes pour un unique objet. Par exemple, l'objet x i appartient à une des classes du sousensemble A j ? ?. Cette information se modélise alors sous la forme d'une contrainte sur la plausibilité de A j : pl i (A j ) = 1. Cela revient à favoriser les fonctions de masses ayant au moins une classe dans A j . Cette contrainte, qui généralise la précédente, permet d'établir un terme de pénalité à ajouter à la fonction objectif de ECM :
La nouvelle fonction objectif est alors la suivante :
sous les contraintes (5)  
Optimisation
L'optimisation du nouveau critère consiste, de la même manière que pour l'algorithme ECM, à minimiser alternativement les matrices M, V et S. Le terme de pénalité J S ne dé-pendant ni de V, ni de S, leur mise à jour est similaire à ECM, et leur formule est présen-tée dans (Masson et Denoeux, 2008). La partition crédale M est au contraire présente dans J S . En fixant ? à 2 alors la minimisation de la fonction objectif par rapport à M devient un problème quadratique à contraintes linéaires. Ce problème peut être résolu par une méthode classique d'optimisation (Ye et Tse, 1989), néanmoins de nombreux auteurs se trouvant dans un contexte similaire proposent d'optimiser directement la fonction objectif sans prendre en compte les contraintes de positivité sur la partition (6), afin de réduire le temps de convergence de l'algorithme.
Afin de résoudre le problème de minimisation contraint par (5), des multiplicateurs de Lagrange ? 1 , . . . ? n sont introduits et le Lagrangien défini : 
A j ??,A j ? =? Annuler les dérivées partielles permet d'obtenir les équations suivantes :
Aj ??,Aj ? =? En utilisant (15) et (16) dans (17), il est possible d'écrire :
Cette équation peut finalement être utilisée dans (15) et (16) pour obtenir la mise à jour des fonctions de masse, ?i = 1, n et ?j/A j ? ?, A j ? = ? : 
A j ??,A j ? =? Comme le numérateur de la première partie de l'équation (19) permet l'obtention de valeurs négatives, la fonction de masse m ij peut être négative. Ces valeurs négatives sont accentuées par l'importance donnée aux contraintes. Si l'utilisateur reste dans un cas normal d'utilisation des contraintes, c'est-à-dire ? ? 0.8 (cf. partie 4.2), les valeurs de m ij < 0 seront proches de 0. Une fonction de réajustement est donc envisageable sans que l'optimisation soit dégradée :
avec 
Expérimentations
Les expériences menées sur plusieurs jeux de données consistent à comparer les résultats obtenus par SECM lorsque la mise à jour des fonctions de masse utilise l'optimisation de (Ye et Tse, 1989), noté SECM-classic, avec SECM et l'optimisation proposée, noté SECM-do.
Données et méthode d'évaluation
Jeux de données : Plusieurs jeux de données issue de l'UCI Machine Learning Repository ont été employés. Le tableau 3 indique leurs caractéristiques ainsi que la métrique utilisée pour les expériences. Il faut noter que LettersIJL correspond au jeu de données Letters modifié comme (Bilenko et al., 2004  Protocole expérimental : Une expérience consiste, pour un certain pourcentage de contraintes, à exécuter 25 fois l'algorithme SECM avec 25 jeux de contraintes différents. Afin d'éviter les optima locaux, chaque exécution teste cinq initialisations aléatoires des centres de gravité et récupère les résultats obtenus par l'initialisation ayant la fonction objectif minimale.
Résultats
Jeux de données réelles : La figure 1 montre l'évolution de l'indice de Rand moyen obtenu avec SECM-classic et SECM-do par rapport au pourcentage de contraintes pour Iris et Wine. Des résultats similaires ont été trouvés avec Ionosphere et LettersIJL. Le coefficient ? est fixé à 0.5. Il est ainsi possible de remarquer (1) que l'ajout progressif de contraintes améliore l'indice de Rand et (2) que l'algorithme SECM-do présente de meilleurs résultats que SECM-classic. Pour ces expériences, nous avons également constaté que les valeurs des fonctions objectif de SECM-do sont plus petites que celles de SECM-classic. Des résultats similaires ont été trouvés pour ? = 0.3 et ? = 0.8. La nouvelle optimisation permet donc d'obtenir un meilleur minimum grâce à sa relaxation des contraintes, ce qui implique de meilleurs résultats de classification. Il faut également noter que pour ? = 0.8, les résultats obtenus prouvent que la fonction de réajustement de SECM-do ne dégrade en rien les solutions.
Pour ces mêmes expériences, le temps CPU a été observé afin de comparer la vitesse d'exé-cution des deux algorithmes. Le tableau 4 présente les résultats obtenus. Il est ainsi aisé de voir que l'algorithme SECM-do est plus rapide que l'algorithme SECM-classic. à une forte valeur. De plus, nous avons choisi de réduire l'incertitude trouvée par la partition finale en fixant ? à une valeur élevée. Ainsi, ECM avec c = 2, ? = 3 et ? 2 = 1000 trouve la partition crédale dure représentée par la figure 2(b). Nous pouvons remarquer que ECM ne permet pas d'isoler correctement l'avion. Dans une seconde expérience, nous introduisons des contraintes sur la partition comme illustré Figure 2(c). Chaque pixel de la première (respectivement seconde) zone est affectée à ? 1 (respectivement ? 2 ). L'algorithme SECM est alors exécuté avec les mêmes paramètres que ECM. La partition crédale résultante est présentée Figure 2(d). Nous pouvons constater que les contraintes ont permis de lever l'indétermination de la plupart des pixels alloués à ?.
Conclusion
Nous avons présenté dans cet article une nouvelle méthode d'optimisation pour l'algorithme de classification automatique intitulé SECM. Ce dernier est une variante de l'algorithme évidentiel ECM prenant en compte des contraintes d'étiquettes. Il repose sur la minimisation d'une fonction objectif avec des contraintes linéaires et non linéaires, ce qui impose l'utilisation de méthodes d'optimisation avancées. De plus, l'utilisation des fonctions de masses liées aux méthodes évidentielles rend la complexité de l'algorithme linéaire par rapport au nombre d'objets et exponentielle par rapport aux nombre de classes. Nous proposons donc de relâ-cher les contraintes de positivité sur les fonctions de masses, c'est-à-dire sur les contraintes non linéaires, afin de réduire l'optimisation de la partition à la méthode des multiplicateurs de Lagrange. Le respect des contraintes de positivité est ensuite vérifié par une méthode de réajustement. Nous avons montré sur un ensemble de jeux de données que cette nouvelle technique permet non seulement d'augmenter la rapidité de SECM mais qu'elle permet également d'améliorer les performances en trouvant de meilleurs minima. Les travaux futurs porteront sur l'étude de nouveaux formalismes plus rapides permettant de conserver la majeure partie de l'expressivité des méthodes évidentielles avec une complexité largement réduite pour permettre de traiter des jeux de données avec un nombre de classes plus important.

Introduction
Les systèmes de recommandation basés sur le contenu suivent généralement un processus en deux étapes : (i) Création d'une représentation du besoin des utilisateurs ainsi que des informations à recommander. (ii) Comparaison des représentations afin d'évaluer la pertinence d'une information pour un utilisateur en fonction de son profil. Notre approche consiste à automatiser l'indexation à l'aide de processus d'inférence sur une ontologie d'indexation intégrant les vocabulaires contrôlés (e.g. thésaurus, nomenclatures, listes) définis par les documentalistes pour modéliser le domaine. Le respect de la vision métier sur le domaine permet une supervision simplifiée pour les documentalistes, garantissant la qualité de l'indexation.
Automatisation du processus d'indexation
La classification multi-label consiste à associer des étiquettes à des items (Tsoumakas et Katakis, 2007). Cet article propose une méthode pour enrichir sémantiquement une ontologie en adoptant des processus d'apprentissage automatique pour indexer et décrire l'indexation de façon à réduire l'écart entre le point de vue des experts et les règles d'indexation. L'approche proposée repose sur les quatre phases suivantes :
Phase 1 : utilisation du travail d'indexation déjà fait par les documentalistes et d'un processus d'analyse de texte pour extraire des mots-clés afin de générer une matrice qui présente la fréquence de chaque mot-clé en fonction de chaque étiquette.
Phase 2 : utilisation de la matrice afin de définir des règles capables de déterminer si un document doit être associé à une étiquette sur la base des mots-clés qu'il contient. Deux seuils de fréquence sont définis, ? et ?. Les mots-clés dont la fréquence est supérieure au seuil ? sont considérés comme des indices fiables. La présence d'un seul de ces mots est considérée comme suffisante pour que le document soit associé à l'étiquette. Le seuil de fréquence infé-rieur est ?. Dans ce cas, nous avons besoin d'une combinaison de ?-termes (dont la fréquence est supérieure à ?) pour prendre la décision d'associer un document avec l'étiquette. Plus d'informations sur les règles d'indexation peuvent être trouvées dans (Werner et al., 2014).
Phase 3 : la classification fournit deux types de résultats. Le premier est la découverte de la classe de subsomption la plus spécifique. Le second est la déduction des classes d'équiva-lence lorsque les contraintes logiques sont équivalentes. D'une part, cela signifie que lorsqu'un document est étiqueté (lors de la phase 4) par une classe qui possède des subsumants, ce document est également marqué par les classes subsumantes. D'autre part, lorsqu'un document est étiqueté avec une classe qui a des classes d'équivalence alors ce document est également étiqueté avec ces classes équivalentes. Ces deux éléments peuvent permettre la classification multi-label. De plus, sachant que les étiquettes peuvent être organisées de façon hiérarchique il peut s'agir d'un processus de classification hiérarchique multi-label (HMC).
Phase 4 : la phase de réalisation consiste à trouver toutes les classes les plus spécifiques des individus. Cette phase est mise en oeuvre par le moteur d'inférence. Les phases 3 et 4 utilisent des raisonneurs comme FaCT ++, HermiT ou Pellet.

Introduction
Un flux de données est une séquence, potentiellement infinie, non-stationnaire (la distribution de probabilité des données peut changer au fil du temps) de données arrivant en continu. Dans le cas d'un flux, l'accès aléatoire aux données n'est pas possible et le stockage de toutes les données arrivant est infaisable. Le clustering de flux de données nécessite un processus capable de partitionner des observations de façon continue avec des restrictions au niveau de la mémoire et du temps. Dans la littérature, de nombreux algorithmes de clustering de flux de données ont été adaptés à partir des algorithmes de clustering traditionnel, par exemple, la méthode DbScan (Cao et al. (2006); Isaksson et al. (2012)) basée sur la densité, la méthode de partitionnement k-means (Ackermann et al. (2012)), ou encore la méthode basée sur le passage de message AP (Affinity Propagation) (Zhang et al. (2008)). Dans cet article, nous proposons le modèle G-Stream, qui permet de découvrir des clusters de formes arbitraires dans un flux de données en constante évolution. Les caractéristiques et les principaux avantages de G-Stream sont décrits ci-dessous : (a) La structure topologique qui est représentée par un graphe dans lequel chaque noeud représente un cluster. Les noeuds (clusters) voisins sont reliés par des arêtes. La taille du graphe est évolutive. (b) L'utilisation d'une fonction d'oubli afin de réduire l'impact des anciennes données dont la pertinence diminue au fil du temps. Les liens entre les noeuds sont également pondérés. (c) Contrairement à de nombreux algorithmes qui utilisent un nombre important de données pour initialiser leur modèle, G-Stream utilise seulement deux noeuds au départ. (d) Toutes les fonctions de G-Stream sont effectuées en-ligne. (e) L'utilisation de la notion de réservoir pour maintenir, de façon temporaire, les données très éloignées des prototypes courants. L'article est organisé comme suit : d'abord, la section 2 décrit plusieurs travaux liés au problème de clustering de flux de données. Ensuite, la section 3 présente notre nouvelle approche de clustering de flux de données, appelée G-Stream. Puis, dans la section 4, nous rapportons une évaluation expérimentale. Enfin, la section 5 conclut cet article et présente nos futurs travaux de recherche.
Travaux similaires
Cette section présente un bref état de l'art qui concerne les problèmes de clustering de flux de données. Nous mettons ainsi en évidence les algorithmes les plus pertinents proposés dans la littérature pour faire face à ce problème. La plupart des algorithmes existants (par exemple, CluStream proposé par (Aggarwal et al. (2003)), DenStream de (Cao et al. (2006)), StreamKM++ de (Ackermann et al. (2012)) divisent le processus de clustering en deux phases : (a) En-ligne, dans cette phase, les données sont résumées, (b) Hors-ligne, dans cette phase, les clusters finaux sont calculés à partir de la quantification fournie par la phase en-ligne. Les deux algorithmes CluStream et DenStream utilisent une extension temporelle du Clustering Feature vector proposée par (Zhang et al. (1996)) (appelée micro-clusters) afin de maintenir des résumés statistiques sur les données ainsi que leur temps d'arrivée, ceci durant la phase en-ligne. En créant deux types de micro-clusters (potentiel et outlier micro-clusters), DenStream surmonte l'un des principaux inconvénients de CluStream, sa sensibilité au bruit. Dans la phase hors-ligne, les micro-clusters trouvés lors de la phase en-ligne sont considérés comme des pseudo-points et seront transmis à une variante de k-means dans l'algorithme CluStream (resp. une variante de DbScan dans l'algorithme DenStream), afin de déterminer les clusters finaux. StreamKM++ est une extension de l'algorithme k-means++ pour le flux de données. Les auteurs de (Isaksson et al. (2012)) ont proposé SOStream, qui est un algorithme de clustering de flux de données, basé sur la densité, inspiré à la fois du principe de l'algorithme DbScan et celui des cartes auto-organisatrices (SOM) de (Kohonen et al. (2001)). L'algorithme E-Stream, qui est proposé par (Udommanetanakit et al. (2007)), classe l'évolution des données en cinq catégories : apparition, disparition, auto-évolution, fusion et division. Il utilise une autre structure de données pour sauvegarder des statistiques sommaires, nommée ?-bin histogramme. (Zhang et al. (2008)) présentent une extension de l'algorithme Affinity Propagation pour le flux de données, appelé StrAP et qui utilise un réservoir pour maintenir d'éventuels outliers. Les auteurs de (Bouguelia et al. (2013)) ont proposé une version incrémentale de l'algorithme GNG de (Fritzke (1994)), appelée AING. où ? 1 > 0, qui est une constante définissant le taux de décroissance du poids au fil du temps. t désigne le temps courant et t 0 est le temps d'arrivée de la donnée. Le poids d'un noeud est calculé à partir des poids des données qui lui sont affectées :
où m est le nombre de données affectées au noeud c au temps courant t. Quand le poids d'un noeud est inférieur à une valeur donnée, alors ce noeud est considéré comme obsolète et sera supprimé (ainsi que ses liens). Gestion des arêtes : la procédure de gestion des arêtes effectue des opérations liées à la mise à jour des arêtes du graphe (les étapes 14-19 de l'algorithme 1). Lors de l'incrémentation de l'âge des arêtes, l'instant de création d'une arête est pris en compte. Contrairement à la fonction d'oubli, l'âge des liens sera renforcé par la fonction exponentielle f 2 (t) = 2 ?2(t?t0) où ? 2 > 0, définit le taux de croissance au temps courant t, t 0 est le temps de création de l'arête. L'étape suivante consiste à ajouter une nouvelle arête reliant les deux noeuds les plus proches. La dernière étape consiste à supprimer chaque lien dépassant un âge maximum.
Gestion du réservoir : l'objectif de l'utilisation d'un réservoir est de maintenir, temporairement, les données éloignées. Comme nous l'avons mentionné précédemment, chaque noeud a un seuil de distance. Les premières données du flux sont affectées aux noeuds les plus proches sans prendre en considération les seuils de distances. Le seuil de distance de chaque noeud est mis-à-jour en prenant la distance maximale du noeud au point le plus éloigné qui lui est affecté. Lorsque le réservoir est plein, ses données sont re-transmises à l'apprentissage. Elles sont placées au début du flux de données, DS, afin de les traiter en premier. Les seuils de distance des noeuds sont mis-à-jour en conséquence.
Évaluation expérimentale
Dans cette section, nous présentons une évaluation expérimentale de l'algorithme GStream. Nous avons comparé notre algorithme avec l'algorithme GNG ainsi qu'avec deux algorithmes pertinents de clustering de flux de données. Nos expériences ont été réali-sées sur la plateforme MATLAB en utilisant des données réelles et synthétiques. Les bases de données réelles, Shuttle (43500x9) et KddCup1 (49402x34), ont été prises à partir du répertoire UCI. Les bases DS1 (9153x2) et DS2 (5458x2) sont générées à l'aide du programme disponible sur http://impca.curtin.edu.au/local/software/ synthetic-data-sets.tar.bz2. Comme nous l'avons expliqué dans la section 3, les 
Conclusion
Dans ce papier, nous avons proposé, G-Stream, une méthode efficace pour le clustering topologique en-ligne de flux de données évolutives. Dans G-Stream, les noeuds ainsi que les arêtes composant la structure topologique sont pondérés. A partir de deux noeuds, G-Stream compare les données arrivant aux prototypes courants ; il sauvegarde celles très éloignées dans un réservoir ; il apprend les seuils de distance automatiquement ainsi que plusieurs noeuds sont créés à la fois. L'évaluation expérimentale sur des bases de données réelles et synthétiques a démontré l'efficacité de G-Stream à découvrir des clusters de formes arbitraires. Les résultats obtenus sont prometteurs. Nous envisageons à l'avenir d'appliquer le principe des fenêtres adaptatives, de rendre notre algorithme le plus autonome possible et de le développer en Spark.

Introduction
Le Web Sémantique a été lancé en 2001 par le W3C 1 pour promouvoir le partage et la créa-tion de données structurées sur le Web en proposant des recommandations pour la description de données (RDF), d'ontologies (RDFS, OWL), et des méthodes et outils associés (SPARQL, ...) pour gérer les connaissances. Actuellement le Web Sémantique correspond à des centaines de bases RDF communautaires (e.g. DBPedia Notre contribution est de fournir une méthode originale d'évaluation de mises à jour, inspirée du raisonnement par cas, utilisant exclusivement les données de la base RDF mise à jour (i.e. ne nécessitant pas l'utilisation d'une ontologie ou de méta-données). Par cette méthode, une mise à jour candidate est évaluée positivement si ses modifications dans la base RDF rendent -selon certains critères -la partie cible mise à jour dans la base plus structurellement similaire à d'autres parties de la base. Notre méthode d'évaluation de la cohérence peut être décomposée en 3 étapes : (i) extraction des contextes de la mise à jour depuis la base, (ii) ré-cupération des parties de la base similaires à la mise à jour et à ses contextes et (iii) évaluation par similarité de la cohérence des données de la mise à jour par rapport à la base.
En Section 2 nous définissons une mise à jour RDF et ses contextes (première étape de notre approche). En Section 3 nous détaillons notre méthode d'évaluation de mise à jour en définissant quelles sous-parties de la base sont prises en compte lors de l'évaluation d'une mise à jour (Section 3.1), et enfin comment évaluer la cohérence d'une mise à jour RDF par rapport à une base (Section 3.2).
Mise à jour RDF
Nous introduisons ici quelques définitions pour formaliser les notions de mises à jour RDF et de contextes associés dans une base RDF.
Nous rappelons quelques vocabulaires du Web Sémantique et introduisons quelques termes utilisés dans la suite de l'article. Ainsi, nous considérerons comme synonymes les termes document RDF, base RDF et ensemble de triplets RDF ; pour un document RDF D nous noterons R D l'ensemble des ressources -sujet, prédicat ou objet -des triplets de D ; nous appellerons une ressource noeud une ressource étant soit sujet, soit objet d'un triplet ; pour un document RDF D, nous noterons N D l'ensemble des ressources noeud de D ; nous appellerons document RDF connexe un document RDF dans lequel il y a un chemin connectant chaque ressource noeud du document à une autre, en d'autres termes si le graphe RDF représentant le document est un graphe connexe ; nous appellerons degré d'une ressource noeud le nombre de triplets la contenant dans une base RDF, en d'autres termes son degré dans le graphe RDF.
Notons aussi que implicitement nous désignons toujours les données d'une base RDF sans (avant) que les modifications d'une mise à jour ne lui soient appliquées. Enfin, toute mise à jour d'une base RDF peut être vue en tant que combinaison de deux sections : une section d'ajout qui contient ce que la mise à jour ajoute à la base et une section de suppression qui contient ce que la mise à jour supprime dans la base.
Définition 1 (Mise à jour RDF). Une mise à jour RDF u d'une base RDF B est un couple d'ensembles de triplets RDF (A, R) tels que :
Une mise à jour RDF qui ajoute des informations à une base doit apporter de nouveaux éléments liés à des données déjà existantes. Une mise à jour qui supprime des informations peut uniquement supprimer des données déjà existantes dans la base. Les sections d'ajout et de suppression ne contiennent pas de triplets en commun (l'ordre d'application de la suppression ou de l'ajout dans la base n'a pas d'importance). Une mise à jour contient nécessairement une section d'ajout et forme un document connexe (autrement il s'agit de 2 mises à jour distinctes).
De la Définition 1 nous pouvons classer les mises à jour RDF en deux catégories : les mises à jour d'ajout, définies par A = ? et R = ?, et les mises à jour de modification, définies par A = ? et R = ?. Le cas des suppressions « pure » est discuté en conclusion.
Pour comparer les données d'une mise à jour à une base RDF selon notre évaluation, nous utilisons le voisinage dans la base de toutes les ressources de la mise à jour. Ainsi les contextes d'une mise à jour sont obtenus grâce aux voisinages des sections d'ajout et de suppression.
Définition 2 (Contextes de mise à jour RDF). Soient un ensemble de triplets RDF B, une mise à jour RDF u = (A, R) candidate à B et n ? N un rang de voisinage. Soit la fonction voisinage n B (r) : N B ? B retournant tous les triplets de B connectés à r par un chemin de longueur égale ou inférieure à n, appelée fonction de voisinage de r.
Les contextes d'une mise à jour u candidate à B sont les deux ensembles de triplets RDF I u et F u définis par :
-I u appelé le contexte initial de u dans B tel que
Le contexte initial représente l'état initial de la partie de la base autour des ressources de la mise à jour candidate, le contexte final représente l'état théorique de la base si la mise à jour était appliquée. 3 Évaluer la cohérence par mesure de la similarité
Nous considérons qu'une mise à jour est cohérente avec une base si on peut trouver suffisamment de sous-parties de la base suffisamment similaires avec les contextes de la mise à jour. Nous procédons en 3 étapes : (i) recherche dans la base de sous-parties structurellement comparables aux contextes de la mise à jour, (ii) quantification de la similarité entre chaque sous-partie et les contextes de la base, (iii) conclusion sur la cohérence de la mise à jour.
Contexte initial I u1 .
Contexte final F u1 .
FIG. 2 -Contextes de u 1 (En pointillés : voisinage dans la base des ressources de u 1 )
Trouver des références dans la base
Deux ensembles de triplets RDF peuvent être structurellement comparés si leurs ressources noeuds sont liées de façon similaire, incluant (au moins) une ressource commune.
Définition 3 (Ensembles de triplets RDF comparables). Soit deux ensembles de triplets RDF connexes G et H.
G est comparable à H s'il existe une fonction de transformation f :
Deux ensembles comparables contiennent au moins une ressource commune. De plus, en théorie des graphes, on dira qu'un ensemble de triplets RDF est comparable à un autre si il est homomorphique à une partie d'un autre, sans considérer l'orientation des arcs. Comparer une mise à jour à une base entière signifie comparer structurellement les contextes initial et final de la mise à jour à chaque sous-partie de la base comparable à la mise à jour. Nous appelons références ces sous-parties de la base dépendantes de la mise à jour.
DBPedia, en tant que références pour u 1 en Fig. 1 (Élé-ments en commun avec u 1 en traits épais orange).
De la base DBPedia, deux références pour u 1 peuvent être extraites, notées D 1 et D 2 et représentées en Fig. 3. D 1 et D 2 contiennent plusieurs ressources en commun avec u 1 . D 1 suit le même modèle que u 1 avec une boisson liée à une ville liée à une personne, alors que D 2 concerne une boisson liée à une entreprise liée à une personne.
Évaluer la cohérence d'une mise à jour
Dans notre approche, si les modifications d'un mise à jour rendent la partie ciblée de la base plus similaire à d'autres parties (existantes) de la base alors nous évaluons positivement cette mise à jour.
Nous proposons d'évaluer la similarité de la mise à jour par rapport à chacune de ses réfé-rences à l'aide d'une mesure de la similarité structurelle entre deux graphes. Dans l'évaluation en Définition 5, nous supposons l'usage d'une mesure donnant un score de similarité dans R + tel que plus le score est élevé, plus la similarité est grande (un score de 0 signifie aucune similarité). Plusieurs mesures sont utilisables telles que la distance d'édition entre deux graphes, le coefficient de Jaccard, etc. . On note similarity la mesure de similarité entre deux graphes RDF avec similarity(u, La valeur de similarité seule n'importe pas dans notre évaluation, seul le signe de la diffé-rence entre l'état final et initial indique si la mise à jour apporte des informations similaires à ce qui est déjà connu.
Exemple 4. Dans cet exemple, nous choisissons d'utiliser une mesure de similarité en considérant dans chaque ensemble de triplets l'ensemble des ressources et l'ensemble des couples de ressources (sujet, relation) et (relation, objet) où le score est calculé simplement -pédago-giquement -avec similarity = +1 pour chaque ressource commune et +2 pour chaque couple de ressources communes. La différence de similarité entre la référence D 1 et les contextes de u 1 est positive (similarity(F u1 , D 1 ) ? similarity(I u1 , D 1 ) = 12 ? 11) et celle entre D 2 et les contextes de u 1 est nulle (similarity(F u1 , D 2 ) ? similarity(I u1 , D 2 ) = 9 ? 9), ainsi, avec un nombre minimum de références de 1, on a eval(u 1 , B, 2, 1) = true.
La mise à jour u 1 est donc cohérente avec la base DBPedia : les modifications de la mise à jour créent des données structurellement similaires à des parties de la base. Cette mise à jour peut être appliquée à la base.
Conclusion
Dans cet article nous proposons une approche d'intégration, ou de mise à jour, de données dans des bases RDF par une évaluation de la cohérence des mises à jour en fonction de leur

Introduction
Networks are studied in numerous contexts such as biology, sociology, online social networks, marketing, etc. Graphs are mathematical representations of networks, where the entities are called nodes and the connections are called edges. Very large graphs are difficult to analyse and it is often beneficial to divide them in smaller homogeneous components easier to handle. The process of decomposing a network has received different names : graph clustering (in data analysis), modularization, community structure identification. The clusters can be called communities or modules ; in this paper we use those words as synonyms.
Assessing the quality of a graph partition requires a modularization criterion. This function will be optimized to find the best partition. Various modularization criteria have been formulated in the past to address different practical applications. Those criteria differ in the definition given to the notion of community or cluster.
To understand the differences between the optimal partitions obtained by each criterion we show how to represent them using the same basic formalism. In this paper we use the Mathematical Relational Analysis (MRA) to express six linear modularization criteria. Linear criteria are easy to handle, for instance, the Louvain method can be adapted to linear quality functions (see Campigotto et al. (2014)). The six criteria studied are : the Newman-Girvan modularity, the Zahn-Condorcet criterion, the Owsi´nskiOwsi´nski-Zadrozny criterion, the Deviation to Uniformity, the Deviation to Indetermination index and the Balanced Modularity (details in section 3). The relational representation allows to understand the properties of those modularization criteria. It allows to easily identify the criteria suffering from a resolution limit, first discussed by Fortunato et Barthelemy (2006). We will complete this theoretical study by some experiments on real and synthetic networks, demonstrating the effectiveness of our classification. This paper is organized as follows : Section 2 presents the Mathematical Relational Analysis approach, we introduce the property of balance for linear criteria and its relation to the property of resolution limit. In Section 3, we present the six linear modularization criteria in the relational formalism. Next, Section 4 presents some experiments on real and artificial graphs to confirm the theoretical properties found previously.
Relational Analysis approach
There is a strong link between the Mathematical Relational Analysis 2 and graph theory : a graph is a mathematical structure that represents binary relations between objects belonging to the same set. Therefore, a non-oriented and non-weighted graph G = (V, E), with N = |V | nodes and M = |E| edges, is a binary symmetric relation on its set of nodes V represented by its adjacency matrix A as follows :
We denote the degree d i of node i the number of edges incident to i. It can be calculated by summing up the terms of the row (or column) i of the adjacency matrix :
N 2 the density of edges of the whole graph.
Partitioning a graph implies defining an equivalence relation on the set of nodes V , that means a symmetric, reflexive and transitive relation. Mathematically, an equivalence relation is represented by a square matrix X of order N = |V |, whose entries are defined as follows :
2. For more details about Relational Analysis theory see Marcotorchino et Michaud (1979) and Marcotorchino (1984).
Modularizing a graph implies to find X as close as possible to A. A modularization criterion F (X) is a function which measures either a similarity or a distance between A and X. Therefore, the problem of modularization can be written as a function to optimize F (X) where the unknown X is subject to the constraints of an equivalence relation 3 .
We define as well ¯ X and ¯ A as the inverse relation of X and A respectively. Their entries are defined as ¯ x ii = 1 ? x ii and ¯ a ii = 1 ? a ii respectively. In the following we denote ? the optimal number of clusters, that means the number of clusters of the partition X which maximizes the criterion F (X).
Linear balanced criteria
Every linear criterion is an affine function of X, therefore in relational notation it can be written as :
where the function ?(a ii ) depends only on the original data (for instance the adjacency matrix). In the following we will use K to denote any constant depending only on the original data.
Definition 1 (Property of linear balance) A linear criterion is balanced if it can be written in the following general form :
where ?(.) and ¯ ?(.) are non negative functions depending only on the original data and
3. In fact, the problem of modularization can be written in the general form :
subject to the constraints of an equivalence relation :
x ii ? {0, 1} Binary
The exact solving of this 0 ? 1 linear program due to the size of the constraints is impractical for big networks. So, heuristic approaches are the only reasonable way to proceed.
By replacing ¯
x by its definition 1 ? x ii , equation (4) can be rewritten as follows :
From this expression we can deduce the importance of the property of balance for linear criteria. If the criterion is a function to maximize, the presence and/or absence of the terms ? ii and ¯ ? ii has the following impact on the optimal solution :
the solution that maximizes F (X) is the partition where all nodes are clustered together in a single cluster, so ? = 1 and
) and
then the optimal solution that maximizes F (X) is the partition where all nodes are separated, so ? = N and
In other words, the optimization of a linear criterion who does not verify the property of balance will either cluster all the nodes in a single cluster or isolate each node in its own cluster, therefore forcing the user to fix the number of clusters in advance.
We can deduce from the previous paragraphs that the values taken by the functions ? and ¯ ? create a sort of balance between the fact of generating as many clusters as possible, ? = N , and the fact generating only one cluster, ? = 1.
In the following we will call the quantity N i=1 N i =1 ?(a ii )x ii the term of positive agreements and the quantity N
ii the term of negative agreements.
Different levels of balance
We define two levels of balance for all linear balanced criterion :
Definition 2 (Property of local balance) A balanced linear criterion whose functions ? ii and ¯ ? ii satisfy
where K L is a constant depending only upon the pair (i, i ) (therefore not depending on global properties of the graph) has the property of local balance.
Some remarks about definition 2 : -Since K L depends only on properties of the pair (i, i
) , that is local properties, we call this property local balance.
-When we talk about global properties we refer to the total number of nodes, the total number of edges or other properties describing the global structure of the graph. -In the particular case of local balance where
), that is ? ii and ¯ ? ii sum up to a constant, we have the following situation : whereas ? ii increases ¯ ? ii decreases and vice versa.
Let us consider the special case where ?(a ii ) = a ii , the general term of the adjacency matrix. A null model is a graph with the same total number of edges and nodes and where the edges are randomly distributed. Let us denote the general term of the adjacency matrix of this random graph ¯ ?(a ii ). A criterion based on a null model considers that a random graph does not have community structure. The goal of such a criterion is to maximize the deviation between the real graph, represented by ?(a ii ) and the null model version of this graph, represented by ¯ ?(a ii ) as shown in equation (5).
That implies
. This constraint implies that ¯ ? ii depends upon the total number of edges M . Consequently, the decision of clustering together two sub-graphs depends on a characteristic of the whole network and the criterion is not scale invariant because it depends on a global property of the graph.
The definition of null model for linear criteria can be generalized as follows :
Definition 3 (Criterion based on a null model) A balanced linear criterion whose functions ? ii and ¯ ? ii satisfy the following conditions :
where g(K G ) is a function depending on global properties of the graph K G is a criterion based on a null model. K G can be for example the total number of edges or nodes. We can deduce from definitions 2 and 3 that a linear criterion can not be local balanced and based on a null model at the same time.
In the particular case where ¯ ? decreases if the size of the network increases, it becomes negligible for large graphs. As explained previously, if this term tends to zero, the optimization of the criterion will tend to put together the nodes more easily. For instance, a single edge between two sub-graphs would be interpreted by the criterion as a sign of a strong correlation between the two clusters, and optimizing the criterion would lead to the merge of the two clusters. Such a criterion is said to have a resolution limit.
The resolution limit was introduced by Fortunato et Barthelemy (2006), where the authors studied the resolution limit of the modularity of Newman-Girvan. They demonstrated that modularity optimization may fail to identify modules smaller than a scale which depends on global characteristics of the graph even weakly interconnected complete graphs, which represent the best identifiable communities, would be merged by this kind of optimization criteria if the network is sufficiently large. According to Kumpula et al. (2007) the resolution limit is present in any modularization criterion based on global optimization of intra-cluster edges and extracommunity links and on a comparison to any null model.
In section 4 we will show how criteria having a resolution limit fail to identify certain groups of densely connected nodes.
Modularization criteria in relational notation
Graph clustering criteria depend strongly on the meaning given to the notion of community. In this section, we describe six linear modularization criteria and their relational coding in Table 1. We assume that the graphs we want to modularize are scale-free, that means that their degree distribution follows a power law.
1. The Zahn-Condorcet criterion (1785, 1964) : C.T. Zahn (see Zahn (1964)) was the first author who studied the problem of finding an equivalence relation X, which best approximates a given symmetric relation A in the sense of minimizing the distance of the symmetric difference. However the criterion defined by Zahn corresponds to the dual Condorcet's criterion (see Condorcet (1785)) introduced in Relational Consensus and whose relational coding is given in Marcotorchino et Michaud (1979). This criterion requires that every node in each cluster be connected to at least as half as the total nodes inside the cluster. Consequently, for each cluster the fraction of within cluster edges is at least 50% (see Conde-Céspedes (2013) for the demonstration).
2. The Owsí nski-Zadro? zny criterion (1986) (see Owsi´nskiOwsi´nski et Zadro? zny (1986)) it is a generalization of Condorcet's function. It has a parameter ?, which allows, according to the context, to define the minimal percentage of required within-cluster edges : ?. For ? = 0.5 this criterion is equivalent to Condorcet's criterion. The parameter ? defines the balance between the positive agreements term and the negative agreements term. For each cluster the density of edges is at least ?% (see Conde-Céspedes (2013)).
3. The Newman-Girvan criterion (2004) (see Newman et Girvan (2004)) : It is the best known modularization criterion, called sometimes simply modularity. It relies upon a null model. Its definition involves a comparison of the number of within-cluster edges in the real network and the expected number of such edges in a random graph where edges are distributed following the independence structure (a network without regard to community structure). In fact, the modularity measures the deviation to independence. As mention in the previous section, this criterion, based on a null model and it has a resolution limit (see Fortunato et Barthelemy (2006)). In fact, as the network becomes larger M ?? ?, the term ¯ ? ii = ai.a .i 2M tends to zero for since the degree distribution follows a power law.
4. The Deviation to Uniformity (2013) This criterion maximizes the deviation to the uniformity structure, it was proposed in Conde-Céspedes (2013). It compares the number of within-cluster edges in the real graph and the expected number of such edges in a random graph (the null model) where edges are uniformly distributed, thus all the nodes have the same degree equal to the average degree of the graph. This criterion is based on a null model and it has a resolution limit. indeed ? ?? 0 as N ?? ?.
The Deviation to Indetermination (2013)
Analogously to Newman-Girvan function, this criterion compares the number of within-cluster edges in the real network and the expected number of such edges in a random graph where edges are distributed following the indetermination structure 4 (a graph without regard to community structure), introduced in Marcotorchino (2013) and . The Deviation to Indetermination is based on a null model, therefore it has a resolution limit.
The Balanced modularity (2013) This criterion, introduced in Conde-Céspedes et
Marcotorchino (2013), was constructed by adding to the Newman-Girvan modularity a term taking into account the absence of edges ¯ A. Whereas Newman-Girvan modularity compares the actual value of a ii to its equivalent in the case of a random graph ai.a .i 2M , the new term compares the value of ¯ a ii to its version in case of a random graph
. It is based on a null model and it has a resolution limit.
where 4. There exists a duality between the independence structure and the indetermination structure (see Marcotorchino (1984), Marcotorchino (1985) and Ah-Pine et Marcotorchino (2007)).
The six linear criteria of Table 1 verify the property of balance, so it is not necessary to fix in advance the number of clusters, more specifically : From Tables 1 and 2 one can easily deduce that for the criteria having a resolution limit the quantity ¯ ? ii decreases when the size of the graph becomes larger.
Tests with real and artificial networks
We modularized six real networks of different sizes : Jazz (Gleiser et Danon (2003)), Internet (Hoerdt et Magoni (2003)), Web nd.edu (Albert et al. (1999)), Amazon (Yang et Leskovec (2012) 5 ) and Youtube (Mislove et al. (2007)). We ran a generic version of Louvain Algorithm (see Campigotto et al. (2014) and Blondel et al. (2008)) until achievement of a stable value of each criterion. The number of clusters obtained for each network is shown in Table 3. Table 3 shows that the Zahn-Condorcet and Owsi´nskiOwsi´nski-Zadro? zny criteria generate many more clusters than the other criteria having a resolution limit, for which the number of clusters is rather comparable. Moreover, this difference increases with the network size. Notice that the number of clusters for the Owsi´nskiOwsi´nski-Zadro? zny criterion decreases with ?, that is the minimal required fraction of within-cluster edges, so the criterion becomes more flexible.
Only ground-truth overlapping communities are defined on these previuos real networks. This fact makes difficult to judge the quality of the obtained partitions. That si why we generated five benchmark LFR graphs (see Lancichinetti et al. (2008)) of different sizes 1000, 5000, 10000, 100000 and 500000. The input parameters are the same as those considered in Lancichinetti et Fortunato (2009). The average degree is 20, the maximum degree 50, the exponent 5. the data was taken from http://snap.stanford.edu/data/com-Amazon.html. of the degree distribution is -2 and that of the community size distribution is -1. In order to test the existence of resolution limit we chose small communities sizes, ranging from 10 to 50 nodes, and a low mixing parameter, 0.10. So, the communities are clearly defined. Figure 1 shows the average number of clusters for 100 runs of the generic Louvain algorithm. Network size: N FIGURE 1 -Average number of cluster for artificial LFR graphs (logarithmic scale). Figure 1 shows clearly the difference between the behaviour of those criteria having a resolution limit (NG, DU, DI and BM) and the behaviour of criteria locally defined (ZC and OZ). As the size of the network increases the four criteria suffering from resolution-limit detect fewer clusters than those predefined. The number of clusters is rather comparable for these four functions, one reason can be the fact that the term of negative agreements tends to zero when the network gets bigger. Conversely, the criteria locally defined identified more clusters than those predefined, specially ZC. The criterion which best approaches the real number of clusters is OZ with ? = 0.2. Figure 2 shows the average Normalized Mutual Information for the partitions in Figure 1. Figure 2 shows that the average NMI decreases with the network size for criteria having a resolution limit. The criterion with the highest NMI is OZ with ? = 0.2 which guarantees an within-cluster density of 20%.
Number of clusters
Conclusions
We presented six linear modularization criteria in relational notation, Zahn-Condorcet, Owsi´nskiOwsi´nski-Zadro? zny, the Newman-Girvan modularity, the Deviation to Uniformity index, the Deviation to Indetermination index and the Balanced-Modularity. This notation allowed us to easily identify the criteria suffering from a resolution limit. We found that the first two criteria had a local definition whereas the others, based on a null model, had a resolution limit. These findings were confirmed by modularizing real and artificial graphs using a generic version of the Louvain algorithm. We compared the number of clusters found by the six criteria and the Normalized Mutual information for artificial graphs. The results showed that those criteria ba-

Introduction
En apprentissage automatique, la précision et le rappel sont des mesures classiques pour évaluer les résultats et la performance des algorithmes utilisés. Ces mesures sont essentiellement utilisées en apprentissage supervisé (Sokolova et al., 2006), en classification simple (Jain, 2010) et croisée Hanczar et Nadif (2013) et en recherche d'information (Manning et al., 2008). Dans ce dernier cas, la performance de l'algorithme employé est évaluée à partir de la similarité entre l'ensemble de documents retrouvés et l'ensemble des documents cibles. Cette similarité se base sur la précision et le rappel. De la même manière en classification simple (resp. croisée), les algorithmes identifient des groupes (resp. biclusters) d'éléments qui sont comparés à des groupes (resp. biclusters) de référence. En apprentissage supervisé, l'évaluation d'un classeur se fait en comparant les classes prédites avec les vraies classes sur un ensemble de test. On mesure la similarité entre les classes prédites et les vraies classes en calculant leur préci-sion et rappel. Cependant cette approche ne tient pas compte du taux de vrais négatifs. Pour ces raisons, on préfère dans certains cas utiliser le couple sensibilité-spécificité que le couple précision-rappel dans ce contexte. La précision et le rappel sont donc deux mesures très utilisées dans les procédures d'évaluations de nombreux domaines. Il est extrêmement fréquent de combiner ces deux valeurs afin de construire des indices de performance tel que la F-mesure ou l'indice de Jaccard (Albatineh et Niewiadomska-Bugaj, 2011).
Par défaut les indices de performance donnent la même importance à la précision et au rappel. Or dans de nombreux cas, on peut vouloir privilégier l'un par rapport à l'autre. Par exemple, en génomique des groupes de gènes ayant des profils d'expression similaires sont identifiés en utilisant des méthodes de classification. Ces groupes sont comparés à des classifications de gènes issues de bases de connaissance afin d'estimer leur pertinence biologique (Datta et Datta, 2006). L'objectif de ces analyses est de capturer le plus d'information biologique dans les groupes de gènes, on veut donc privilégier le rappel par rapport à la précision dans ce contexte. Certains indices de performance ont une variante introduisant un paramètre permettant de contrôler le compromis précision-rappel comme c'est le cas de la F-mesure qui est une généralisation de l'indice de Dice. Pour d'autre mesures, le contrôle du compromis précision-rappel est plus difficile, comme c'est le cas de l'indice de Jaccard. Dans cet article nous analysons les différents indices de performance en fonction du compromis précision-rappel. Nous proposons également un nouvel outil d'analyse qu'est l'espace de compromis qui présente de nombreux avantages par rapport à l'espace précision-rappel.
Dans la section 2, nous présentons les différents indices de performance étudiés ainsi que leurs variantes sensibles au compromis. Dans la section 3, nous rappelons les propriétés de l'espace précision-rappel. Nous analysons le comportement des différents indices dans cet espace. Dans la section 4, nous définissons l'espace de compromis et nous montrons comment représenter les performances par les courbes de compromis. Dans la section 5, nous montrons les avantages à travailler dans l'espace de compromis en particulier pour la sélection de modèles et la comparaison d'algorithmes. Nous illustrons ces propriétés avec un exemple dans le contexte du biclustering. Dans la section 6, nous exposons nos conclusions et perspectives.
2 Indices basés sur le couple précision et rappel
Définitions
Soit D un ensemble des données contenant N éléments. Nous appelons groupe cible le sous-ensemble T ? D que nous recherchons. Un algorithme dont l'objectif est de retrouver le groupe cible produit un groupe X. Pour mesurer la qualité de ce groupe X, un indice de performance est utilisé afin d'évaluer la similitude entre T et X. Ces indices de performances sont généralement basés sur deux valeurs : la précision et le rappel. La précision représente la proportion de X qui recouvre T quant au rappel il exprime la proportion de T retrouvé par X. Ces deux indices prennent les formes suivantes :
Les principaux indices de performances utilisés sont une combinaison de la précision et du rappel. Dans cet article nous étudierons les quatre plus populaires : l'indice de Kulczynski, Fmesure, Folke et Jaccard. Ces travaux pourront être facilement étendus à d'autres indices. Par défaut chacun de ces indices donne la même importance à la précision et au rappel. Cependant on peut construire des versions pondérées permettant de privilégier la précision par rapport au rappel ou inversement.
L'indice de Kulczynski
L'indice de Kulczynski est la moyenne arithmétique de la précision et du rappel.
Une version pondérée introduit le paramètre R ? [0, +?] qui permet de contrôler le compromis entre la précision et le rappel. Plus R est grand, plus le rappel est important, le point d'équilibre est atteint pour R = 1. Nous réécrivons cet indice en effectuant le changement de variable suivant : ? = R R+1 , ? ? [0, 1] contrôle désormais le compromis et le point d'équilibre est atteint pour ? = 0.5.
La F-mesure
La F-mesure, appelée aussi indice de Dice, est le rapport entre l'intersection et la somme des tailles du groupe X et du groupe cible T . C'est aussi la moyenne harmonique entre la précision et la rappel. 
L'indice de Folke
L'indice de Folke correspond à la moyenne géométrique de la précision et du rappel.
Il est possible de pondérer le moyenne géométrique en introduisant un paramètre ? ? [0, 1]. Plus ? est grand plus le rappel est important et le point d'équilibre est atteint pour ? = 0.5.
L'indice de Jaccard
L'indice de Jaccard est le rapport entre l'intersection et l'union du groupe X et le groupe cible T .
Il n'est pas facile de définir une version pondérée de l'indice de Jaccard à cause de la pré-sence du terme pre.rec au dénominateur. Nous voulons un indice pondéré ayant les proprié-tés suivantes : I Jac (T, X, ?) ? [0, 1] ; I Jac (T, T, ?) = 1 ; I Jac (T, X, 0.5) = I Jac (T, X) ; I Jac (T, X, 0) = pre ; I Jac (T, X, 1) = rec. Pour cela nous proposons l'indice suivant :
3 L'espace précision-rappel L'espace précision-rappel, illustré dans la figure 1, est un espace à deux dimensions dans lequel les abscisses et ordonnées représentent respectivement le rappel et la précision (Buckland et Gey, 1994). Une performance est représentée par un point dans cet espace (le point blanc par exemple). Le principe de l'espace précision-rappel est proche de celui de l'espace ROC qui représente le taux de vrais positifs en fonction du taux de faux positifs (Fawcett, 2006). Plusieurs relations ont d'ailleurs été identifiées entre ces deux espaces (Davis et Goadrich, 2006). Un point dans l'espace précision-rappel représente tous les groupes de taille |X| = |T | rec pre ayant une intersection avec le groupe cible de |T ? X| = |T |rec. Le point (1,1) (point noir), maximisant la précision et le rappel, représente le groupe idéal et dans ce cas il y a une parfaite correspondance avec le groupe cible (X = T ). Le point (1, |D| rec puisqu'on a |D| ? |X|. Tous les groupes dont la performance se situe sur la droite pre = |T | |D| rec sont ceux dont |T ? X| est minimale. Cette droite représente tous les groupes dont |T ? X| est nulle. La plupart des algorithmes a un paramètre permettant de contrôler la taille du résultat X. Pour chaque taille de X on obtient des valeurs de précision et rappel différentes. La performance d'un algorithme peut donc être représentée par un ensemble de points et approximée par une courbe dans l'espace précision-rappel. Dans la figure 1, on donne un exemple de courbe précision-rappel. On peut tirer plusieurs informations sur les performances de ces différents groupes même sans se référer à un indice en particulier. Si un point domine un autre, c-à-d si sa précision et son rappel sont supérieurs, alors on peut conclure qu'il aura une meilleure performance quelque soit l'indice utilisé. Les points noirs représentent les points dominants de la courbe, il ne sont dominés par aucun autre point et représentent les performances des meilleurs groupes. Il n'y a pas de rapport de domination entre ces types de points, il est nécessaire d'utiliser un indice pour les comparer.
Le comportement des différents indices de performances peut se visualiser en dessinant leur iso-ligne dans l'espace précision-rappel. Une iso-ligne est un ensemble de points dans l'espace précision-rappel ayant tous la même valeur d'indice (Flach, 2003;Hanczar et Nadif, 2013). La figure 2 montre les iso-lignes des indices de Kulczynski, F-mesure, Folke et Jaccard. Les lignes en gras représentent les iso-lignes lorsque ? = 0.5. Pour les quatre indices, nous observons que les iso-lignes ont une symétrie autour de l'axe pre = rec, ceci signifie que la précision et le rappel ont la même importance. Par contre les différents indices ne considèrent pas la différence entre précision et rappel de la même façon. Cette différence n'est pas prise en compte dans l'indice de Kulczynski, alors que les autres indices la pénalisent. L'indice de Folke pénalise moins que la F-mesure et l'indice de Jaccard. Ces deux derniers sont équivalents car ils sont compatibles,
Dans la figure 2, les lignes en pointillées représentent les iso-lignes pour ? = 0.2 et les lignes pleines ? = 0.8. La modification de la valeur de ? déforme les iso-lignes, ce qui permet de donner plus d'importance à la précision ou au rappel. A noter que pour pre = rec les indices de Kulczynski, F-mesure et Folke retournent la même valeur quelque soit ?. L'indice de Jaccard a un comportement différent, il pénalise le fait que ? s'approche de 0.5. Dans la L'espace de compromis, que nous proposons, offre un nouvel outil de visualisation des performances des résultats ou des algorithmes en fonction du compromis précision-rappel. Il y a certaines similitudes avec les "cost curves" utilisées en apprentissage supervisé (Drummond et Holte, 2006). L'espace de compromis représente en abscisse ? et en ordonnée l'indice de performance. La performance d'un groupe X est représentée dans cet espace par une courbe f (?). On a une correspondance entre les points de l'espace précision-rappel et les courbes Cette dernière courbe définit le domaine d'application des indices de performances pour un problème donné, illustré dans la figure 3 par les zones blanches. Un point situé dans l'une des zones grises, signifie que le groupe correspondant à de moins bonnes performances que le groupe maximal et peut donc être considéré comme non informatif. On constate que le domaine d'application de l'indice de Kulczynski est beaucoup plus petit que celui des autres indices. Cela est dû au fait que cet indice ne pénalise pas la différence entre précision et rappel. La ligne en pointillé représente le groupe contenant un unique élément appartenant au groupe cible. Le groupe parfait est représenté par la droite f (?) = 1. A l'inverse les groupes ayant une intersection nulle sont représentés par la droite f (?) = 0. Les groupes aléatoires sont représentés par les courbes partant du point (0, |T | |D| ).
Courbe optimale de compromis
Comme nous l'avons illustré dans la figure 1, la performance d'un algorithme peut être représentée par une courbe dans l'espace précision-rappel. A chaque point de cette courbe correspond une courbe dans l'espace de compromis. On peut représenter la courbe précision-rappel par un ensemble de courbes dans l'espace de compromis. La figure 4 donne la représentation de la courbe précision-rappel de la figure 1 dans l'espace de compromis pour les différents indices de performance. On s'intéressera particulièrement à l'enveloppe supérieure de cet ensemble de courbes, représentée en gras dans la figure 4 que nous appellerons courbe optimale de compromis. Cette dernière représente les meilleurs performances de l'algorithme pour tous les compromis. On s'aperçoit que les courbes formant l'enveloppe supérieure correspondent tous à des points dominants de la courbe précision-rappel. Les points dominés ont toujours leur courbe en dessous de la courbe optimale de compromis. Dans le cas de l'indice de Kulczynski, les courbes formant l'enveloppe supérieure correspondent aux points de l'enveloppe convexe de la courbe précision-rappel. Ces courbes de compromis permettent d'analyser les résultats bien plus facilement que les courbes précision-rappel. 5 Application des courbes de compromis 5.1 Sélection de modèles L'utilisation de l'espace de compromis permet d'identifier très facilement le résultat optimal pour un compromis donné. Ceci est illustré dans la figure 5 à travers un problème de classification croisée. Nous avons généré une matrice de données aléatoires dans laquelle un bicluster a été introduit, ce dernier suit un modèle additif selon la définition de Madeira et Oliveira (2004). Nous utilisons l'algorithme CC (Cheng & Church) pour retrouver ce bicluster (Cheng et Church, 2000). La similarité entre le bicluster retourné par l'algorithme et le bicluster recherché est alors calculée par les différents indices de performance. Cet algorithme dispose d'un paramètre permettant de contrôler la taille du bicluster retourné, nous pouvons donc représenter les performances de cet algorithme par une courbe précision-rappel (figure 5). A partir de cette courbe il n'est pas facile de déterminer le meilleur bicluster pour un compromis de précision-rappel donné. Même en ajoutant les iso-lignes au graphique, la comparaison des différents biclusters n'est pas intuitive. Dans la figure 5 est représentée la courbe optimale de compromis pour la F-mesure. A partir de cette courbe on peut instantanément identifier le meilleur bicluster pour un compromis donné. On a aussi une décomposition de la valeur de ? en une série d'intervalles qui sont délimités sur le graphique par les lignes verticales pointillées, pour lesquels le meilleur bicluster est donné. Sur notre exemple on constate qu'il y a sept intervalles, nous nous intéresserons donc qu'aux sept biclusters correspondants, identifiés sur la figure par leur taille. Pour le dernier intervalle (? > 0.74) le meilleur bicluster est la matrice entière, la courbe optimale de compromis est confondue avec la courbe du bicluster maximal. Notons qu'il n'est pas possible d'identifier visuellement ces biclusters dans l'espace précision-rappel car ils ne correspondent ni à l'ensemble des points dominants ni à l'enveloppe convexe de la courbe précision-rappel (sauf dans le cas de l'indice de Kulczynski). Il est également très facile de travailler avec des contraintes sur la précision ou le rappel dans l'espace de compromis. Nous rappelons que la précision et le rappel se lisent à l'extrémité de chaque courbe de compromis. Lorsqu'on demande une précision minimale pre min , il suffit de considérer unique les courbes de compromis qui partent au-dessus du seuil minimum c-à-d f (0) > pre min . De même avec un rappel minimum rec min , on ne conserve que les courbes qui arrivent au-dessus du seuil de rappel c-à-d f (1) > rec min .
Comparaison d'algorithmes
L'espace de compris simplifie également grandement la comparaison des algorithmes. Nous reprenons l'exemple de classification croisée précédent dans lequel un autre algorithme, ISA (Bergmann et al., 2003), est testé et comparé à CC. Les performances de ce nouvel algorithme sont représentées dans l'espace précision-rappel et l'espace de compromis par la courbe grise dans la figure 6. Dans l'espace précision-rappel les deux courbes se croisent plusieurs fois, aucun des deux algorithmes n'est donc absolument meilleur que l'autre. Il est difficile de voir dans quelles conditions CC est meilleur que ISA et inversement. Dans l'espace de compromis on visualise immédiatement quel est le meilleur algorithme pour chaque valeur de compromis. Pour ? < 0.28 CC est meilleur que ISA, pour 0.28 < ? < 0.83, ISA est meilleur, pour ? > 0.83 les deux algorithmes retournent un bicluster contenant toute la matrice de donnée et ont donc des performances identiques. La distance entre les deux courbes permet de FIG. 5 -Identification des meilleurs biclusters dans l'espace précision-rappel et l'espace de compromis. A gauche, la courbe précision-rappel. A droite, la courbe optimale de compromis.
visualiser la différence de qualité entre les deux algorithmes. Dans l'espace précision-rappel les courbes des deux algorithmes se croisent trois fois, laissant penser qu'il y a deux intervalles de ? pour lesquelles CC est meilleur (de même pour ISA). Les courbes optimales de compromis montrent que l'identité du meilleur algorithme ne change qu'une fois, en ? = 0.28. Dans l'espace précision-rappel, CC a une meilleurez précision que ISA ; 14 fois sur 20 ce qui laisse penser que CC est plus souvent meilleur que ISA. L'espace de compromis nous montre qu'au contraire l'intervalle [0, 0.28] pour lequel CC est meilleur est deux fois plus petit que celui de ISA [0.28,0.83]. Cet exemple illustre bien la facilité de la comparaison d'algorithmes dans l'espace de compromis.
FIG. 6 -Identification du meilleur algorithme. A gauche, les courbes précision-rappel. A droite, les courbes optimales de compromis.

Introduction
Les requêtes skyline sont importantes dans les applications qui nécessitent la localisation des réponses selon plusieurs critères. Ayant un ensemble de points dans un espace vectoriel de d dimensions, un algorithme traitant ce type de requêtes doit retourner l'ensemble des points de S dits non dominés. Il est meilleur pour ce type d'algorithmes de fonctionner progressivement Kossmann et al. (2002) car les utilisateurs sont souvent impatients de recevoir des réponses. La relation de dominance se définit comme suit Börzsonyi et al. (2001) : Soit S un ensemble de données de d dimensions (d critères) sur lequel va porter l'opérateur skyline. Soit D l'ensemble de toutes les dimensions D = {d 1 , , d d }. Soient p et q deux points de S. La relation de dominance (?) suivant D est pour 1 ? i, j ? d ,p domine q ?? {?d i ? D, p(i) ? q(i)} et {?d j ? D, p(j) < q(j)}. Lorsque aucun point ne domine l'autre on dit qu'ils sont non dominés ou concurrents. L'opérateur skyline renvoie l'ensemble des points concurrents, suivant toutes les dimensions D : SkyD(S) = {p ? S/ ? S : q ? p} Dans ce papier, nous présentons une solution analytique pour déduire l'ensemble de points candidats afin d'éviter de parcourir l'ensemble S entièrement. Nous donnons un nouveau théo-rème pour l'élimination des points non candidats. Notre méthode est basée sur le tri Tan et al. (2001) et à la différence avec ce travail, où les tests entre les points balayent tout l'ensemble S, nous donnons des théorèmes pour la déduction des points les plus évidents et qui constituent les premières solutions à présenter. Nous montrerons que la combinaison de DC Divide-andConquer avec notre méthode fournit des résultats meilleurs que lorsqu'il est appliqué tout seul. Le reste de ce papier se présente comme suit. La section 2 présente les travaux liés à cette problématique. La section 3 donne notre approche. La section 4 présente les résultats des expérimentations. La conclusion et les travaux futurs sont donnés dans la section 5. Börzsonyi et al. (2001) était le premier travail ayant adapté l'optimisation au sens de Pareto dans les bases de données. Intuitivement, le calcul du skyline consiste à comparer chaque point p avec tous les autres et si aucun point ne le domine alors p est un point skyline. L'algorithme BNL Börzsonyi et al. (2001) utilise cette technique directe. Il met en mémoire une liste candidate et teste à chaque fois si un nouveau point p domine un ou plusieurs points déjà insérés. Si c'est le cas, il est inséré et l'ensemble des points dominé est écarté sinon il passe au point suivant. Cette méthode peut être utilisée facilement et ne requiert pas de prétraitement, sauf qu'elle est gourmande en mémoire et en temps de calcul. DC Börzsonyi et al. (2001) divise l'entrée en plusieurs partitions et détermine le skyline de chaque partition. Par la suite, les skyline sont fusionnés et les points dominés sont écartés. Cette méthode est meilleure que BNL mais souffre des multiples duplications lors de la fusion. Notons que ni BNL ni DC ne fonctionnent en on-line. L'algorithme Bitmap Tan et al. (2001) consiste à encoder dans des vecteurs bitmap toutes les informations de chaque point selon le nombre de points distincts sur chaque axe. La comparaison des vecteurs bitmap se fait par la suite. Bitmap est progressive mais nécessite beaucoup d'opérations et de codage en commençant par la détermination des points distincts dans chaque axe car il y aura beaucoup de tests dupliqués. Index Tan et al. (2001) consiste à trier les données sur chacun des d axes dans un ordre croissant. Afin de déter-miner le skyline, les points sont testés de façon circulaire. Le problème est que la récupération des coordonnées des points peut prendre du temps. Cette méthode est bien adaptée pour les applications on-line, elle retourne aussitôt les premiers points, sauf que les auteurs ne déduise pas les skyline induit par le tri. NN Kossmann et al. (2002) est un algorithme qui utilise les RTrees pour indexer les données. Il partitionne l'espace selon chaque axe selon le point le proche voisin de l'origine. NN est progressif et est efficace dans un espace à deux dimensions, mais il souffre du problème de duplications des éliminations pour 3 dimensions et plus. A partir de 4 dimensions, il devient difficile de l'appliquer. Les auteurs proposent différentes techniques pour remédier à ces problèmes. Branch and Bound Skyline Papadias et al. (2003) exploite les R-Tree, la méthode de Branch and Bound et NN afin de calculer, en on-line, le skyline. Son plus grand problème est qu'il souffre de requêtes redondantes. Yuan et al. (2005) proposent Skycube. Il calculent les skyline fils de toutes les combinaisons possibles des points dans le treillis. Lorsqu'ils passent au niveau supérieur ou inférieur du treillis ils fusionnent ces fils.
Travaux liés
La méthode DCRD
Notre méthode DCRD pour Divide-and-Conquer for Reduced Data est une méthode analytique qui détermine l'espace candidat en se basant sur le tri. L'utilisation directe de l'espace Pareto est simple pour un espace à 2 dimensions, mais au-delà de 3 dimensions, il faut ajouter des méthodes efficaces pour calculer cet espace. Nous donnons un nouveau théorème qui permet de déduire cet espace. Dans Index, les tests de dominance se font entre tous les points sans l'exploitation de la concurrence induite par le tri. Par exemple, il est impossible qu'un point A, ayant la valeur minimale unique sur un axe X, soit dominé par un autre point. Ce qui nous mène à donner le théorème 1. Preuve. Suite à la discussion précédente, sur l'axe i, il est impossible qu'un autre point puisse dominer le point p, puisqu'aucune valeur sur cet axe ne sera inférieure à celle de p. Si pour tous les autres axes, le point q domine p, il sera impossible qu'il le domine sur i, d'où p est soit concurrent avec q soit le domine. Définition du conflit entre les points ayant des valeurs minimales sur le même axe. Il est fréquent que deux points ou plus aient la même valeur minimale sur un axe. Ainsi, il faut résoudre ce conflit de dominance avant de passer au calcul du skyline définitif. Ainsi, nous donnons le théorème 2 suivant : Théorème 2 : Existence de plusieurs points minimaux sur un axe i Soient p et q deux points de S tel qu'il existe un axe i avec p Preuve. Ceci revient à résoudre le conflit entre p et q dans les autres sous-espaces. Le test montrera soit la dominance soit la concurrence entre eux sur les autres axes.
Phase 1 : Tri des données et déduction des premiers points skyline
trier les valeurs de S par ordre croissant ; -Extraire l'ensemble Sky des premiers points skyline en utilisant les théorèmes 1 et 2 ;
Phase 2 : Réduction de l'espace de données
Cette phase consiste à limiter l'espace de données en filtrant l'ensemble de données selon deux points appelés M in sys et M ax sys , autrement dit, on détermine l'espace de dominance dans lequel se trouvent tous les points candidats. M in sys est le même que le point idéal de Pareto. Le point M ax sys est un point virtuel qui permet de délimiter cet espace. Il sert à éli-miner un espace important non utile. En deux dimensions, il se confond au point nadir mais, à plus de dimensions, ils sont différents. Le point nadir est un point pour lequel la fonction à optimiser est maximale ? et ce n'est pas notre cas car M ax sys est dominé. D'une manière analytique, nous déterminons les points M in sys et M ax sys à partir des coordonnées des points skyline déduits de la phase précédente : M in sys et M ax sys possède chacun d composantes. Chaque composante M in sys [i] (resp. M ax sys [i]) de M in sys (resp. M ax sys ) est la valeur minimale(resp. maximale) de toutes les composantes des points de Sky. Calcul de l'espace des candidats. Dans cette étape, l'ensemble des données candidats est ré-duit à un hypercube. Pour l'obtenir, nous énonçons et appliquons le théorème3 suivant :  
. p ne peut dominer q sur l'axe d. Donc, il y a au moins un point m appartenant au skyline actuel qui domine p.
Phase 3 : Calcul du skyline final
A l'issue de la phase 2, l'espace déduit contient les points candidats. Il ne reste que de les comparer entre eux et éliminer les points dominés, nous appliquons ainsi DC.
Expérimentations
Les expérimentations ont été réalisées dans une machine dotée d'Intel Core i5 2,50 GHz, et de 4 Go de RAM, sous Windows 7, 64 bits. Le programme est écrit sous Java. MySQL 5.1.41 est utilisé comme système gestion des bases de données. Nous avons utilisé les mêmes bases de données synthétiques de Börzsonyi et al. (2001)  Kossmann et al. (2002). Il s'agit de trois types de données : corrélées, anti-corrélées et indépen-dantes. Nous avons calculé le temps nécessaire en secondes pour retourner le skyline en tenant compte de la dimensionalité et la cardinalité.
Comparaison entre DC et DCRD dans le type anti-corrélé. En fixant la dimension à 5 et en variant la cardinalité de 10000 à 100000 points. La figure 1 montre que pour ce type de données, DCRD a rendu les résultats dans des temps meilleurs puisqu'il y a eu des éliminations de points inutiles. Pour 10000 points et en variant la dimension de 1 à 10, nous remarquons sur la figure 6 que DCRD a commencé à rendre les réponses rapidement à partir de d=6. On déduit ainsi que le Comparaison entre DC et DCRD dans le type corrélé Ce type de données est intéressant et facile à manier ; même un algorithme naïf pourrait présenter de bonnes performances. En comparant ces deux méthodes sur la cardinalité et en fixant le nombre d'attributs à 5, nous remarquons sur la figure 3 que DCRD a dépassé de loin DC. Ceci est dû au nombre important de points qui ont été éliminés. La faiblesse de DC est qu'il exécute des tests de dominance sur tout l'ensemble de données d'une façon aveugle. De même, en comparant DC et DCRD selon la dimension, nous avons fixé la cardinalité à 10000 et nous avons varié le nombre d'attributs. Puisque le nombre de points éliminé est important, la figure 4 montre que DCRD a été très rapide alors que DC a consommé plus de temps. Ceci est toujours le cas puisque DC ne fait aucun traitement préalable et exécute des partitions sur tout l'ensemble d'entrée. Ceci a un effet sur les temps de réponse.

Introduction
L'explosion d'internet, couplée à l'effet de la mondialisation, a pour résultat d'interconnecter les personnes, les entreprises, les états. Le côté déplaisant de cette interconnexion mondiale des Systèmes d'Information réside dans un phénomène appelé "Cybercriminalité". Des personnes, des groupes mal intentionnés ont pour objectif de nuire dans un but pécuniaire ou pour une "cause", aux informations d'une entreprise, d'une personne voire d'un Etat. Il n'est pas rare que des faits de "cyber-attaques" soient relatés dans les médias envers des grandes socié-tés comme "Google","Visa","Sony", "Apple". La sécurité d'un Système d'Information se doit d'être présente afin de garantir la confidentialité, l'intégrité, la disponibilité de l'information. De ce fait, il existe une multitude d'équipements de sécurité qui permettent de détecter les comportements anormaux. Un des principaux équipements de sécurité est le "Pare-Feu" 1 ou plus communément appelé "Firewall". Il a pour mission comme le décrit Al-Shaer et Hamed (2003) de filtrer , selon une politique fondée sur les flux autorisés à pénétrer dans un réseau selon leurs sources, leurs destinations et les services souhaités (navigation internet, transfert de fichiers, etc... ). Par son positionnement, il donne une visibilité totale de l'ensemble des flux. Cet équipement offre aussi la possibilité "d'historiser" vers des journaux les flux ayant été autorisés ou interdits. L'exploitation et l'analyse des journaux d'événements liés aux équi-pements de sécurité sont devenues primordiales pour la maîtrise des flux et la détection des intrusions ainsi que pour la vérification du bien fondé de la politique de filtrage mise en place (Golnabi et al, 2006). Dans ce contexte, les constructeurs d'équipements de filtrage mettent à disposition des logiciels permettant d'analyser les flux. Ces derniers nécessitent un accès et une connaissance dudit équipement. La détection des anomalies et des comportements anormaux est conséquemment réservée à ces seuls utilisateurs. La problématique de la représentation des événements de sécurité est tellement répandue que plusieurs outils ont même été regroupés au . Le principe est de modéliser un système de "monitoring" et de visualisation des données réseau en temps réel permettant de détecter rapidement les tentatives d'intrusions.
Composition du projet "D113"
Le projet "D113" est composé de quatre phases qui s'inscrivent dans le cadre d'un travail de thèse en sécurité s'appuyant sur des données issues des différents équipements et outils de sécurité. Les différentes phases du projet se déclinent selon la liste suivante.
- Notre démonstration de logiciel portera uniquement sur la phase 1 qui est le préambule à la "fouille de données" qui sera effectuée dans les phases suivantes. La première phase constitue un tout en soi dans la mesure où la visualisation des données pour les utilisateurs est un enjeu crucial en termes de prise de décisions sur les problématiques de sécurité. Ces trois sites sont opérationnels, c'est à dire que les données traitées et analysées dans les sections suivantes correspondent à des données de production. Pour des raisons de confidentialité les adresses IP ont été anonymisées. Le réseau SP1 est doté de son propre conteneur de données qui est alimenté par les événements envoyés en temps réel par le "Firewall". Les réseaux SAB1 et SQ1 mutualisent un même conteneur. Les données brutes envoyées par l'ensemble des équipements filtrants sont traitées selon une extraction de motifs.
Description des données
Le contenu des variables listées ci-dessous sont exportées vers des conteneurs de données. 
Conclusions et perspectives
A l'issue de la phase 1, l'ensemble des événements liés au filtrage est exporté en temps réel vers des conteneurs de données. Dans un souci de performance et compte-tenu de l'importance

Introduction
Les travaux de cette dernière décennie dans le domaine de la découverte de connaissances, comme ceux notamment de ) et de (Tiwari et al. (2010)), témoignent du vif intérêt pour le problème de l'extraction des ensembles d'items fré-quents, des motifs séquentiels, des motifs structurels (dans les données de type arbres, graphes ou treillis), et la recherche de méthodes efficaces pour extraire ces motifs. Dans cet article, nous nous intéressons à la notion de proportion analogique, essentiellement étudiée dans le domaine de l'intelligence artificielle, pour extraire de nouveaux types de motifs dans les bases de données. Les proportions analogiques relient quatre objets A, B, C, D du même type dans une assertion de la forme « A est à B ce que C est à D ». Ils permettent d'exprimer l'identité (ou la proximité) des rapports existant entre deux paires d'éléments. Des exemples typiques de cette notion en langage naturel sont : « le veau est à la vache ce que le poulain est à la jument », « l'aurochs est au boeuf ce que le mammouth est à l'éléphant ». Ces relations permettent d'exprimer que ce qui distingue A de B est comparable à ce qui distingue C de D. Les exemples ci-dessus montrent la diversité (et la potentielle complexité) des sémantiques possibles du connecteur « est à » intervenant dans une proportion analogique. Dans le premier exemple, ce connecteur représente une relation de filiation tandis que dans la seconde, il exprime une évolution possible. Le connecteur « ce que » de la proportion représente généralement l'identité ou la similarité. Quand les éléments A, B, C et D sont des valeurs numériques, la relation peut être définie en utilisant les proportions mathéma-tiques classiques, comme la proportion géométrique : A/B = C/D (e.g., 1/3 = 2/6) ou la proportion arithmétique : A ? B = C ? D (e.g., 5 ? 3 = 9 ? 7). Quand les objets A et B, resp. C et D, représentent les mêmes entités à différents moments ou états de leur vie (par exemple, A et B décrivent le même endroit à deux moments différents), la proportion analogique peut exprimer des évolutions similaires. De manière générale, les proportions analogiques permettent de trouver des parallèles entre quatre événements ou situations.
Nous cherchons à exploiter la notion de proportion analogique dans le contexte des bases de données relationnelles afin d'extraire des combinaisons de quatre n-uplets liés par une telle relation. Notre objectif est de découvrir des parallèles entre des paires de n-uplets, i.e., des paires d'éléments qui sont dans les mêmes rapports. Ces parallèles ne reflètent pas forcément une relation de proximité (A est aussi proche de B que C est proche de D), mais plutôt une transformation semblable (On passe de A à B comme on passe de C à D). Ces parallèles sont d'une importance majeure puisqu'il permettent de modéliser des règles d'évolution reproductible dans les systèmes écolo-giques (les états de deux littoraux qui évoluent dans les mêmes directions : apparition et disparition des mêmes espèces, évolution d'une pollution d'une région à une autre), des mouvements sociétaux (extension d'une crise géopolitique ou comparaison avec des successions d'événements passés), ou des déplacements parallèles d'objets.
Les contributions de cet article sont les suivantes. Nous proposons une méthode pour identifier les proportions analogiques dans les bases de données. À cette fin, nous suivons une approche vectorielle pour définir la notion de proportion analogique adaptée au modèle relationnel. Puis nous montrons qu'il est possible de ramener le problème d'énumération de toutes les combinaisons de quatre n-uplets liés par une relation d'analogie, à un problème de clustering moyennant un prétraitement et l'utilisation d'une métrique. Ceci permet de rassembler des paires d'éléments qui définissent des vecteurs égaux ou presque. Nous analysons ensuite les résultats de notre approche appliquée à un jeu de données réelles.
Notre article est organisé comme suit. En section 2, nous introduisons la notion de proportion analogique et nous proposons une définition graduelle de celle-ci adaptée au contexte des bases de données. La section 3 présente notre approche de découverte des proportions analogiques et l'algorithme qui en découle, tandis que la section 4 détaille les expérimentations effectuées. Enfin, nous présentons les travaux relatifs à notre proposition (Section 5) puis nous concluons (Section 6).
Proportions analogiques et modélisation
Les proportions analogiques
Cette section s'appuie sur les références (Miclet et Prade (2009)) et (Lepage (2012)). Une proportion analogique est une assertion de la forme « A est à B ce que C est à D», notée par la suite Un algorithme naïf pour énumérer les proportions analogiques issues d'un ensemble d'objets de cardinalité n, a une complexité temporelle en n 4 . En utilisant un point de vue vectoriel de la notion de proportion analogique, les objets A, B, C et D désignent des points d'un espace à n dimensions. S'ils forment une relation d'analogie alors ces points forment un parallèlogramme. Par exemple, la figure 1 montre la relation de proportion analogique existant entre les points A(1, 2), B(4, 4), C(3, 1), D(6, 3) représentés dans un repère orthonormé.
Ainsi quatre objets A, B, C et D sont en proportion analogique si et seulement si
La relation de proportion analogique liant A, B, C, et D peut être alors symbolisée par le vecteur ? ? ? AB ( ou ? ? ? CD). Dans ce cas particulier (la conformité est la relation d'identité), il est possible de définir un algorithme dont la complexité temporelle est en n 2 : celui-ci calcule tous les vecteurs existant entre les paires de n-uplets et rassemblent toutes les paires de nuplets définissant des vecteurs égaux en une classe d'équivalence (Lepage (2012)). Une classe d'équivalence, représentée par un vecteur, rassemble ainsi des paires de points qui, prises deux à deux, sont en proportion analogique « selon ce vecteur ». Il est alors aisé de générer l'ensemble de toutes les relations d'analogie à partir de chacune des classes d'équivalence.  
Supposons que
Modéliser les proportions analogiques selon une approche géométrique
La modélisation des proportions analogiques dans le contexte des bases de données relationnelles est influencée par les propriétés du modèle relationnel. Soit un ensemble d'attributs {A 1 , . . . , A m }, un schéma de relation est défini comme un sous-ensemble d'attributs S = {A i1 , . . . , A in }. Une relation définie en termes d'un schéma de relation S est un sous-ensemble fini du produit cartésien des domaines de chacun des attributs de S. Chaque élément d'une relation est appelé n-uplet qui peut être représenté par 1. La propriété de permutation des moyens permet d'éviter de calculer à la fois
un point décrit par n dimensions. Une base de données est un ensemble fini de relations. D'autres contraintes additionnelles comme les dépendences fonctionnelles et les dépendances d'inclusion permettent de restreindre le contenu des relations. Il serait intéressant d'en tenir compte dans la recherche des proportions analogiques mais nous nous limiterons ici au cas de la recherche de proportions existantes entre quatre points. Ainsi les n-uplets A, B, C, et D d'une relation sont considérés comme des points à n dimensions, et sont dénotés comme suit :
Comme dit précédemment, A, B, C, et D sont liés par une relation de proportion analogique si et seulement si
L'égalité est difficile à obtenir quand on considère des jeux de données réels. Il convient alors de rendre cette définition plus flexible, notamment en donnant une vision plus graduelle de la relation de proportion analogique. Deux vecteurs ne doivent plus être égaux mais presque égaux ce qui revient à mesurer dans quelle mesure || ? ? ? AB ? ? ? ? CD|| est proche de 0. On cherche alors à évaluer la « distorsion » entre les deux vecteurs. Pour permettre la commensurabilité des dimensions lorsque les attributs portent sur des domaines différents, les valeurs des vecteurs sont normalisées afin qu'elles appartiennent à l'intervalle [0, 1]. Pour cela, chaque valeur v du domaine actif d'un attribut peut être remplacée par la valeur suivante :
v ? min att max att ? min att où min att et max att désignent respectivement la valeur minimale et la valeur maximale du domaine actif de l'attribut.
Plusieurs stratégies peuvent être utilisées pour mesurer à quel point l'expression || ? ? ? AB ? ? ? ? CD|| est proche de || ? ? 0 ||. Différentes normes peuvent être utilisées comme la norme de Minkowsky (norme p), qui donnera la longueur du vecteur correctif permettant de passer de ? ? ? AB à ? ? ? CD, ou, la norme infinie qui donnera la coordonnée maximale de ce vecteur correctif.
Définition 1 : Distorsion analogique fondée sur une norme infinie Soit A, B, C, et D quatre n-uplets de n dimensions.
La norme infinie retourne la plus grande différence de dimension entre les deux vecteurs. Les deux vecteurs sont d'autant plus égaux que le changement maximal sur une dimension est proche de zéro.
Définition 2 : Distorsion analogique fondée sur la norme p Soit A, B, C, et D quatre n-uplets de n dimensions.
Dans ce cas, la définition met l'accent sur la longueur du vecteur correctif permettant de passer de
Dans tous les cas (Définition 1 ou Définition 2), la relation de proportion analogique est d'autant plus vraie que la distorsion est proche de 0.
Proposition : Les deux définitions de distorsion vérifient les propriétés fondamentales des proportions analogiques. En effet, les propriétés suivantes sont vérifiées :
puisque les deux définitions reposent sur une valeur absolue des différences entre chaque coordonnée.
-la permutation des moyens : la relation Dist(
Dans la suite, nous utiliserons la norme infinie qui est la plus drastique et possède un meilleur pouvoir de discrimination dans la mesure où elle évite tout effet de compromis entre les composantes des vecteurs.
Découvrir les proportions analogiques
Notre objectif est de découvrir toutes les proportions analogiques présentes dans un ensemble de données et si possible de dégager des tendances, i.e., les différents vecteurs représentatifs des proportions analogiques découvertes. Une approche naïve pourrait consister à énumérer tous les vecteurs et à calculer la distorsion entre chaque paire de vecteurs, puis à ne garder que les paires de vecteurs dont la valeur de distorsion ne dé-passe pas un certain seuil. Cependant, une telle approche poserait la question du choix du seuil, très dépendant des données. Il nous semble par ailleurs préférable d'utiliser une technique permettant de fournir une vue synthétique des motifs découverts. Une approche de type clustering semble tout à fait appropriée dans ce contexte. Un argument supplémentaire en faveur d'une telle approche est lié à l'objectif final que nous nous sommes fixé, à savoir l'extension des langages d'interrogation de bases de données avec des requêtes analogiques, i.e., des requêtes visant à découvrir des proportions analogiques existant dans un ensemble de données. En effet, l'identification de classes d'équivalence regroupant les paires de points représentant des vecteurs (presque) égaux permettrait la définition d'index, utiles pour optimiser l'évaluation de telles requêtes. Le problème ici étudié, qui consiste à regrouper des vecteurs à n dimensions égaux ou presque, se ramène à un problème de clustering classique dès lors que l'on dispose d'une métrique. Or les définitions des distorsions analogiques (Dist( ? ? u , ? ? v )) satisfont les conditions qui caractérisent les métriques, soit :
-l'identité des indiscernables : Dist( ? ? u , ? ? v ) = 0 ssi ? ? u = ? ? v , -la propriété de symétrie et -l'inégalité triangulaire. Différentes approches de clustering sont donc utilisables, comme les k-means et l'approche hiérarchique (Xu et al. (2005)). Notre objectif étant de tester l'approche et de la valider sur des jeux de données réels, puis d'identifier les relations découvertes, nous avons choisi de reprendre un algorithme de clustering hiérarchique (Xu et al. (2005)) dont les étapes sont énoncées dans l'Algorithme 1. Cet algorithme requiert une étape préalable de construction des vecteurs à partir des points de la relation. 
Data
Expérimentations
Dans cette section, nous illustrons notre approche avec des données électorales afin de découvrir des parallèles entre les résultats des votes de différentes régions, puis des évolutions des résultats de votes d'une année à l'autre. Pour cela, nous avons exploité les jeux de données ouverts décrivant les résultats des élections présidentielles en    Figure 2).
Les expérimentations visaient à montrer que le cadre proposé permet de mettre en évidence des parallèles existant dans les données, ce que montrent nos premiers résul-tats. Vu la nature de la relation de proportion analogique, l'approche peut évidemment s'appliquer à bien d'autres domaines, comme la recherche de trajectoires parallèles d'objets mobiles, moyennant une adaptation, ou dans les domaines environnemental et sociétal, pour découvrir des évolutions analogues.
5 État de l'art L'originalité de l'approche présentée ici tient à la nature même du type de régularité que l'on cherche à découvrir dans les données. La majeure partie des travaux en fouille de données visent à découvrir des caractéristiques fréquentes (vues comme des valeurs d'attribut ou des séquences de valeurs d'attribut lorsqu'un aspect temporel est pris en compte) dans un ensemble de données. Avec les proportions analogiques, qui sont des relations quaternaires, nous cherchons à vérifier l'existence de "parallèles" entre des couples d'objets d'une collection. Une proportion analogique, lorsqu'elle met en parallèle les situations de deux éléments à deux moments différents, peut être vue comme une sorte de règle d'évolution. Dans un tel contexte, la recherche de proportions analogiques peut constituer une alternative aux approches de la littérature visant à -extraire des règles d'évolution dans des graphes (Berlingerio et al. (2009)), ou à -découvrir des trajectoires parallèles d'objets en mouvement (Vlachos et al. (2002); Chen et al. (2005); Lee et al. (2007); Li et al. (2013)), ou encore à -classer des séquences d'événements (Studer et al. (2010); Guigourès et al. (2014); Lin et al. (2003); Malinowski et al. (2013); Zhou et al. (2013)).
Quoi qu'il en soit, l'approche proposée fournit un cadre plus général que celui dédié spécifiquement à l'extraction de règles d'évolution dans les données. En effet, la notion de proportion analogique n'implique pas l'existence d'une dimension temporelle et peut servir à décrire des parallèles de nature très variée entre deux paires d'objets.

Introduction
Actuellement, la recherche et la détection de similitudes s'effectuent en deux phases : une première phase de recherche de sources candidates, suivie d'une seconde de comparaison de ces sources possibles avec le document que l'on suspecte d'être un plagiat. La phase de collecte est de plus en plus optimale grâce à l'amélioration de l'efficacité des moteurs de recherche en local et sur le Web. C'est à la seconde phase que cet article s'intéresse. Une fois qu'une source candidate est trouvée, elle doit être comparée avec le document sur lequel pèse les soupçons. À l'heure actuelle, la plupart des logiciels anti-plagiat, une fois une liste de sources candidates constituée, se contentent de comparer mot à mot le document analysé avec chaque source possible. Cette technique permet seulement de détecter les similitudes de types « copier/coller ». Bien que cette approche ait prouvé son efficacité et suffise la plupart du temps, en France près d'un étudiant sur deux a déjà eu recours au « copier/coller » (Gibney, 2006), une énorme faille persiste. En effet, le fait de reformuler ou tout simplement de paraphraser un texte, en utilisant des synonymes par exemple, rend la plupart des techniques actuelles caduques. Certains articles (Callison-Burch et al., 2008;Bannard et Callison-Burch, 2005) se sont tout de même intéressés à la détection de reformulations paraphrastiques avec des approches d'alignement. Malgré le fait que ces approches soient plus robustes à l'ajout et à la suppression de mots ainsi qu'à l'utilisation de synonymes, elles restent toutefois inefficaces face aux reformulations non paraphrastiques comme le passage de la forme active à la forme passive. L'approche proposée consiste à comparer les deux textes, phrase par phrase, et non plus mot à mot et à rechercher si une phrase de l'un des textes comporte le même sens qu'une phrase dans l'autre texte. Ceci repose sur l'hypothèse que lorsqu'on paraphrase ou reformule un texte, on garde le sens de celui-ci et ainsi on garde les mots-clés principaux, porteurs du plus de sens de chaque phrase. Après avoir défini quelques notions et présenté l'état de l'art, nous décrirons d'abord comment segmenter le texte en unités de sens, pour ensuite procéder sur chacune de ces unités à l'extraction des mots porteurs de sens, afin de rechercher des concordances de mots de même concept dans un autre texte. Enfin, nous testerons trois algorithmes utilisant notre approche et nous déterminerons le meilleur seuil pour chacun d'entre eux. Le seuil est le nombre minimum de concepts identiques dans deux phrases permettant d'affirmer que l'une est la reformulation de l'autre. Pour finir, nous présenterons l'évaluation de notre approche en comparant la méthode retenue aux méthodes classiques de détection des paraphrases par alignement.
2 La comparaison au-delà du « copier/coller »
La notion de comparaison
La « comparaison de deux documents » est un terme assez vague. Pour comparer correctement deux documents, il faut repérer leurs points communs (leurs similitudes) et leurs diffé-rences. Les similitudes étant plus simples à détecter, il est de convention de chercher à repérer celles-ci en premier lieu et d'en déduire ensuite les différences, représentées alors par le reste du document. Cependant, la plupart des comparaisons textuelles se limitent au « copier/coller », or ce ne sont pas les seules similitudes pouvant être recensées dans un texte.
La notion de similitudes
Bien que l'on puisse avoir au sein d'un document des tableaux, images, graphiques ou tout autre type de données, cet article traite seulement des similitudes d'ordre textuel. On distingue plusieurs types de similitudes allant de la ressemblance jusqu'à l'identité même (SimacLejeune, 2013b). Les ressemblances sont les types de similitudes les plus difficiles à repérer et sont pour cause le point faible des logiciels anti-plagiat actuels. Dans notre cas, on distingue trois types majeurs de similitudes textuelles, de la plus simple à détecter à la plus complexe :
-la copie, qui consiste à copier mot à mot tout ou partie d'un texte dans un autre. Pour exemple, considérons la phrase suivante présente dans un texte : « En cinquante ans, grâce à des efforts considérables dans la recherche et l'élaboration de la fusion, la performance des plasmas a été multipliée par 10'000. » Elle sera recopiée à l'identique dans un autre texte ; -la paraphrase, aussi appelée reformulation paraphrastique, qui consiste à reprendre une phrase d'un texte pour la détailler ou l'expliciter. Elle conserve donc l'ordre des éléments évoqués, autorisant simplement le changement de vocabulaire, l'ajout, la suppression et la substitution de mots. Toujours en considérant la phrase de l'exemple précédent, une paraphrase possible serait : « En une cinquantaine d'années, grâce à un immense effort de recherche, la performance des plasmas produits par les machines de fusion a été multipliée par 10000. » On remarque la conservation des concepts, mais aussi la substitution ou la suppression de certains d'entre eux ; -la reformulation, qui autorise elle toutes modifications textuelles à condition que le sens de la phrase soit conservé. Cela donne souvent lieu à un changement d'ordre des concepts. La reformulation de la phrase exemple serait : « La performance des plasmas produits par les machines de fusion a été multipliée par 10,000 grâce à un immense effort de la recherche bien que cela ait pris une cinquantaine d'années. »
La notion de concept
Un concept est une idée, un sens représenté par un mot ou un groupe de mots. Les reformulations et paraphrases exploitent les propriétés paradigmatiques des mots (leur capacité à se substituer mutuellement) et entraînent ainsi des changements de vocabulaire mais elles conservent les concepts et les idées exprimées (Duclaye, 2003). Il est alors, dans le cadre de la détection de similitudes, plus judicieux de représenter un mot par un concept plutôt que par son identité ou sa définition. Par exemple, il est plus judicieux de représenter un mot par un tableau de tous les mots par lesquels il peut être substitué (un tableau de ses synonymes, lui compris) plutôt que seulement par lui-même.
État de l'art
Lorsque les processus anti-plagiat comparent deux documents, ils recherchent les éléments de l'un également présents dans l'autre. Ils tentent de détecter des similitudes, toutes informations communes laissant penser qu'un plagiat a pu avoir lieu. La comparaison mot à mot est certes efficace pour trouver les zones de « copier/coller » mais les plagiaires ne se contentent plus de copier des éléments depuis une source, ils essaient à présent de camoufler leurs emprunts d'idées derrière des modifications syntaxiques. Les recherches de Barron-Cedeño et al. (2013) se concentrant sur la détection de paraphrases appliquée dans le cadre de la détection du plagiat démontrent que le phénomène de paraphrasage nuit aux systèmes anti-plagiat et rend la détection de similitudes plus difficile. Il faut donc tenter de détecter les paraphrases et les reformulations par des moyens différents, car bien que souvent associés ces deux termes représentent des opérations textuelles bien distinctes. Toutefois, les travaux linguistiques ayant portés sur leur définition, s'accordent sur le fait que ce sont des opérations de modifications de texte, certes bien différentes, mais qui conservent toutes deux le sens (Harris, 1957;Martin, 1976;Duclaye, 2003).
Des recherches (Gülich et Kotschi, 1983;Eshkol-Taravella et Grabar, 2014) se sont attardées à chercher des marqueurs de reformulations afin de mieux les repérer par la suite et d'étudier leur fonctionnement et leur construction. D'autres recherches se sont cantonnées à étudier les limites de la détection des paraphrases (Vila et al., 2011) en estimant au contraire qu'il n'existait pas de caractérisation complète sur le plan linguistique et computationnelle de la paraphrase.
Face à ces difficultés, des chercheurs se sont concentrés sur des approches alternatives ne permettant pas de détecter concrètement des reformulations mais de tout de même déterminer qu'un texte en contient :
-les approches stylométriques (Iyer et Singh, 2005) qui suggèrent qu'en analysant des statistiques de fréquences de mots ou bien d'autres caractéristiques d'un texte on peut en reconnaître l'auteur, et ainsi, si un passage du document ne possède pas les mêmes caractéristiques que le reste du document, on peut en déduire que ce passage aura été emprunté à un autre auteur (Oberreuter et Velásquez, 2013;van Halteren, 2004;Jardino et al., 2007) ; -les approches de calcul de distances (Simac-Lejeune, 2013a) qui propose de calculer une distance « sémantique » entre deux textes après avoir extrait les mots clefs de chaque texte, exposant ainsi l'emprunt probable de l'un dans l'autre.
En dehors de ces approches, la majorité des travaux portent sur la détection des reformulations paraphrastiques, comme les recherches de Eshkol-Taravella et Grabar (2014) portant sur leur détection dans des corpus oraux. Les approches les plus répandues sont les méthodes par alignement (Callison-Burch et al., 2008;Bannard et Callison-Burch, 2005). Servant la plupart du temps dans un contexte bi-linguale (alignement d'un texte et de sa traduction), elles consistent à aligner deux textes par leurs mots ou groupes de mots en communs et ainsi de repérer les mots ou groupes de mots différents mais équivalents. Certaines recherches (Shen et al., 2006), visant à produire des paraphrases, se sont également avérées intéressantes. En effet, étudiant la possibilité de générer automatiquement des paraphrases, un processus d'assemblage puis de désassemblage s'est dégagé, remettant ainsi sur le devant de la scène les méthodes d'alignement. Proche de ces méthodes avec alignement, on peut citer le travail de Fenoglio et al. (2007) traitant de la comparaison de versions de documents textuels à la façon des serveurs de versions. Il met en lumière les transformations élémentaires (déplacements, insertions, suppressions et remplacements de blocs de caractères), identifiées depuis longtemps par les spécialistes de la génétique textuelle (de Biasi, 2000;Grésillon, 1994) comme éléments fondateurs d'une paraphrase.
Toutefois, le cadre théorique le plus souvent adopté est la théorie linguistique Sens-Texte (Kahane, 2003) élaborée dans les années 1960 par Mel'? cuk, notamment son système de paraphrasage (Žolkovskij et Mel'? cuk, 1967;Mel'? cuk, 1992;Mili´cevi´cMili´cevi´Mili´cevi´c, 2007) comme dans le travail de Mili´cevi´cMili´cevi´Mili´cevi´c (2010). Ce dernier met également en avant des approches sémantiques qui permettent de s'approcher d'une détection de reformulations. La plupart des règles sémantiques de paraphrasage trouvées jusqu'ici mettent en jeu un découpage du texte en proposition et des liens communicatifs et rhétoriques entre celles-ci (Danlos, 2006), coïncidant ainsi, dans la plupart des cas, à la définition d'une reformulation qui se contente d'être une paraphrase avec changement d'ordre des propositions.
La reformulation non paraphrastique étant bien plus complexe à détecter que sa voisine la paraphrase, les études se concentrant uniquement sur elle se font plus rares. Mais dès lors qu'on sait que la reformulation conserve également le sens du texte (Harris, 1957;Martin, 1976;Duclaye, 2003) et que le mécanisme de paraphrase le plus utilisé est le changement de lexique (Barron-Cedeño et al., 2013), on peut envisager d'appliquer plus ou moins les mêmes approches sémantiques que pour la paraphrase ou bien même, des approches plus naïves de recherche de correspondances de concepts.
3 Notre approche
Segmentation
Dans un premier temps, l'idée est de segmenter le document que l'on suspecte être un plagiat. Plusieurs algorithmes de segmentation ont été évalués :
-la segmentation par nombre de blocs : on découpe le document en un certain nombre de blocs de même taille (de même nombre de mots), peu importe la taille finale de chaque bloc ; -la segmentation par taille de blocs : on découpe le document par blocs d'une certaine taille (un certain nombre de mots), peu importe le nombre de blocs créés ; -la segmentation par pourcentage que représente un bloc sur l'ensemble du document (e.g. une segmentation comme celle-ci avec en paramètre un pourcentage de 1% pour un bloc reviendrait à une segmentation en 100 blocs, chaque bloc représentant 1%) ; -la segmentation par granularité (Simac-Lejeune, 2013b) : il s'agit d'une segmentation hiérarchique, on découpe le texte en nb blocs de même taille, puis on redécoupe chaque bloc ainsi obtenu en nb blocs de même taille, et ainsi de suite sur une profondeur limite définie. Ceci permettant d'affiner l'analyse niveau par niveau ; -la segmentation par paragraphe : chaque segment représentant un paragraphe ; -la segmentation par phrase : chaque segment représentant une phrase du document ; -la segmentation par proposition : chaque segment représentant une unité minimale de sens, les délimiteurs étant la ponctuation de fin de phrase mais aussi les virgules, les conjonctions de coordination et divers mots de liaison ou de causalité. Chaque algorithme a fait l'étude, via de nombreux tests et corrections, à l'optimisation de ses paramètres afin de mettre l'accent sur la rapidité du processus de découpage mais aussi sur la pertinence des métadonnées extraites dans chaque segment. Il est important que chaque segment conserve un sens afin d'être potentiellement sujet à une reformulation. Une segmentation en unité de sens a donc été choisie. Un découpage par phrase ou par proposition est à privilé-gier car une segmentation à faible granularité, comme celle par paragraphe, donne lieu à des segments trop volumineux pour l'étape d'extraction qui suivra. Au contraire, une segmentation à trop grand niveau de granularité pourrait, en plus d'entraîner un temps d'exécution plus important (plus de segments à traiter), occasionner une perte d'informations dans sa globalité (aucune liaison entre les concepts extraits). C'est pourquoi la segmentation qui a été retenue est une fusion du découpage par phrase et du découpage par taille de blocs : une segmentation par phrase mais d'une taille minimale (en mots). On conserve ainsi une unité de sens (une ou plusieurs phrases) tout en gardant une taille suffisamment importante pour pouvoir obtenir une pertinence raisonnable des métadonnées extraites mais suffisamment petite pour être considé-rée comme indépendante et donc éventuellement reformulée. Après divers tests, le seuil a été fixé à 15 mots, taille moyenne des phrases dans la langue française. Avec un seuil si faible, c'est l'une des méthodes de segmentation évaluée les plus chronophages mais pour notre étude elle garantit un rapport taille/pertinence optimal.
Extraction de mots clefs (mots porteurs de sens)
La seconde étape du processus est une étape d'extraction des mots porteurs de sens de chaque segment c'est-à-dire des mots représentant les concepts que le plagiaire a été obligé de réutiliser s'il voulait conserver le sens de la phrase, même s'il a pu les remplacer par des synonymes. Pour déterminer les mots porteurs de sens d'un texte, l'étiqueteur morphosyntaxique TreeTagger (Schmid, 1994) a été utilisé. Il détermine la classe lexicale, le "Part Of Speech" de chaque unité lexicale (token) du texte. De façon plus commune, on peut dire que pour chaque mot ou élément du texte, TreeTagger détermine s'il s'agit d'un nom, d'un verbe, d'un adjectif, d'une ponctuation, etc. L'étiquetage morphosyntaxique permet d'identifier les mots clefs d'un texte par leur classe lexicale. Plutôt que de discriminer les mots vides (stop words) par leur taille, ceci pouvant générer des erreurs (e.g. un mot de moins de trois lettres n'est pas pertinent, un contre exemple est le mot « as » qui peut être important, et le mot « mais » qui est simplement une conjonction), on les discriminera par leur "Part Of Speech".
Dans notre cas, les mots pertinents à conserver sont un peu plus riches sémantiquement que les mots clefs habituels. On ne conserve pas seulement les noms communs et propres, il est important de garder aussi les adjectifs, les verbes et également les adverbes, en réalité tout mot porteur de sens au sein d'une phrase. On néglige donc les méthodes les plus courantes pour extraire des mots clefs, les méthodes fréquentielles (Lee et Baik, 2004) qui consiste pour chaque mot du texte à calculer sa fréquence d'apparition dans le texte. C'est pour cela que le terme de mots clefs est ici un abus de langage et que nous allons préférer le terme de mots porteurs de sens d'une phrase. Tout mot porteur de sens d'une phrase doit être conservé, peu importe son nombre d'occurrences dans le texte.
Un filtre de mots vides a été ajouté à la sortie de TreeTagger afin d'être certain de la pertinence des mots porteurs de sens extraits. Ainsi en couplant les deux techniques, l'efficacité de l'étiquetage est passée d'environ 96% à quasiment 100%.
Considérons la phrase suivante : « Ce peu de masse disparue crée une grande quantité d'énergie comme le démontre la fameuse formule d'Einstein E=mc2. » Ses mots porteurs de sens extraits seraient « peu, masse, disparue, crée, grande, quantité, énergie, démontre, fameuse, formule, Einstein, E=mc2 ».
Thésaurus -chargement d'un dictionnaire de synonymes
Parallèlement à cela, un dictionnaire de synonymes est chargé. Pour chaque mot, on a donc accès à un tableau contenant tous les mots de la langue par lesquels il peut être substitué. L'efficacité de notre approche dépendant en grande partie du contenu de cette ressource, il est important de faire la différence entre des synonymes et des mots de substitution possibles. Par exemple, pour le mot « père », « papa » serait un synonyme alors que « parent » serait un mot de substitution envisageable. Autre exemple, le mot « île » a pour synonyme « îlot », « archipel » ou bien encore « atoll » mais aucunement les mots « tâche » ou « pâté » qui eux se trouvent pourtant dans notre tableau et peuvent servir de mot de substitution. En effet, on peut très bien imaginer dans un poème une phrase telle que « cette tâche au milieu de l'océan » faisant référence à un îlot. En règle générale, « îlot » et « tâche » ne sont pas synonymes mais ici, ils représentent le même concept.
Dès lors, un concept est un mot porteur de sens ainsi que tous ses mots de substitution possibles contenus dans son tableau.
Le tableau 1 représente une partie des mots de substitution correspondant aux mots porteurs de sens extraits sur la phrase exemple lors de l'étape précédente. On remarque, comme dans l'exemple de l'îlot cité précédemment, que le terme « énergie » laisse place à « assiduité » qui n'a strictement rien à voir avec le contexte de notre phrase mais qui dans un autre contexte aurait très bien pu être un synonyme envisageable. 
Correspondance
La dernière étape de notre approche consiste à comparer chaque phrase d'une source candidate avec les mots porteurs de sens de chaque segment du texte en cours d'analyse ainsi qu'avec leurs mots de substitution possibles contenus dans le tableau défini précédemment. On appellera seuil de correspondance le nombre de concepts communs à partir duquel on peut estimer qu'une phrase est la reformulation d'une autre. S'il y a plus de concepts pertinents communs entre deux phrases que le seuil de correspondance défini, c'est sans doute que l'une est une reformulation de l'autre.
Plusieurs algorithmes mettant en oeuvre cette méthode ont été développés, certains plus robustes que d'autres face aux changements de genre, de nombre, de casse typographique ou bien d'ordre des mots (e.g. phrase passée de la forme active à la forme passive et vice versa). L'efficacité de la détection dépend de l'algorithme choisi, du seuil de correspondance défini, et du nombre et de la pertinence des « synonymes » disponibles dans le dictionnaire chargé.
Nous proposons trois algorithmes, trois implémentations différentes de l'approche décrite précédemment.
-un premier (tableau 2 -A) qui compare la présence des concepts dans l'ordre et tels qu'ils sont présents dans les phrases. Il ne supporte donc ni le changement de casse typographique, ni la dérivation et la flexion ; -un second (tableau 2 -B) qui compare également la présence des concepts dans l'ordre des phrases mais en comparant leurs lemmes en minuscules, il supporte donc le changement de casse typographique, la dérivation et le changement de genre et de nombre ;
-un troisième (tableau 2 -C), plus naïf, qui reprend le principe du précédent, en comparant cette fois la présence des concepts dans les deux phrases sans prendre l'ordre en compte. Il est ainsi robuste aux reformulations non paraphrastiques de type mise à la forme passive. Le tableau 2 résume les différentes variations de la langue supportées par chaque algorithme. Considérons maintenant la phrase : « La célèbre équation d'Einstein E = mc 2 exprime le phénomène suivant : une importante quantité d'énergie est apparue et un peu de la masse a disparu. » ainsi que sa reformulation : « Ce peu de masse disparue crée une grande quantité d'énergie comme le démontre la fameuse formule d'Albert Einstein E=mc2. » Si on opère la comparaison de type C sur ces deux phrases, on retrouve bien, malgré le changement de vocabulaire et d'ordre des mots, la correspondance de nos concepts, ici en gras.
A noter l'importance du seuil de correspondance, il y a dans cette exemple 11 concepts identiques, avec un seuil de correspondance de 11 ou moins, la phrase est reconnue comme reformulation, alors qu'avec un seuil de correspondance supérieur ce ne sera plus le cas.
Évaluation et tests 4.1 La base de tests et protocole
La base de tests est composée de 150 textes, représentant chacun un passage d'un document, allant de plus de 100 mots pour le plus petit à environ 9000 mots pour le plus grand. Cela représente 400 comparaisons de textes deux à deux. Afin de tester correctement les performances des algorithmes évalués, aussi bien des paraphrases que des reformulations plus complexes sont présentes dans le corpus, ainsi que des textes « pièges » traitant du même sujet et donc employant le même vocabulaire mais n'étant pas pour autant des reformulations d'un autre texte du corpus.
Ci-dessous la répartition des types de textes présents dans le corpus : -10 différents chapitres tirés d'un même roman ; -20 chapitres de la bible (deux traductions différentes pour 10 chapitres) ; -25 textes de Wikipédia (différentes versions à différentes dates de 10 articles) ; -35 extraits de travaux d'élèves (avec leurs sources provenant du Web) ; Les extraits de travaux d'élèves proviennent pour la plupart de rapports et mémoires scientifiques ou économiques. L'intégralité des textes sont en français.
Résultats
Dans un premier temps, on compare les trois méthodes décrites précédemment, leur efficacité et leur temps moyen d'exécution respectifs étant différents selon le seuil utilisé, on détermine d'abord le seuil optimal pour chacune d'entre elles. Le tableau 3 représente le rapport précision/rappel des trois algorithmes allant du seuil 1 à 7. Un seuillage de 4 semble mieux convenir aux algorithmes A et B, tandis qu'un seuillage de 5 semble idéal pour l'algorithme C. Prenant en compte la F-mesure et privilégiant le rappel plutôt que la précision, l'algorithme C se montre être le plus efficace sur la base de tests. Le tableau 4 représente le temps d'exécution de la procédure (segmentation, extraction de mots porteurs de sens, chargement du thésaurus et comparaison) des trois algorithmes en utilisant leur meilleur seuillage en fonction du nombre moyen de mots contenus dans les deux textes à comparer (moyenne du nombre de mots des deux textes). La méthode C s'avère être la plus rapide (200 secondes en moyenne pour un texte d'environ 4000 mots contre 250 pour la méthode A et 212 secondes pour la méthode B) en plus d'avoir un meilleur rapport préci-sion/rappel, respectivement 0.745 et 0.807, car malgré le fait qu'elle soit utilisée avec un seuil plus grand (5 contre 4 pour les deux autres implémentations) et qu'elle fasse donc forcément un plus long parcours, elle ne vérifie pas l'ordre des mots et néglige donc des permutations et suppressions de tableau. On remarque néanmoins une précision générale assez basse due aux faux positifs générés par les propriétés paradigmatiques des mots contenus dans le thésaurus.
Le tableau 5 compare la méthode retenue (l'algorithme C avec un seuil de 5) avec une mé-thode d'alignement basée sur la méthode de Bannard et Callison-Burch (2005) mais appliquée sur un corpus mono-lingue. Ces deux approches possèdent des performances similaires face à la détection de « copier/coller », environ 84% d'efficacité, en revanche notre méthode montre de biens meilleurs résultats sur la détection des reformulations non paraphrastiques (un rappel de 0.80 contre 0.24 pour une méthode avec alignement). TAB. 5 -Evaluation de notre méthode par rapport à une méthode à alignement en fonction des types de similitudes à détecter.
Conclusions
La méthode retenue montre des résultats similaires aux méthodes avec alignement sur la détection de copies exactes et de paraphrases et se montre beaucoup plus robuste face aux reformulations. Néanmoins, sa précision est fortement impactée par le thesaurus utilisé, qui peut engendrer des faux positifs pour les raisons évoquées dans la partie 3.2 Extraction de mots clefs, et la segmentation, qui peut être faussée par du texte enrichi (tableau, liste à puces). Nous conviendrons également que cette technique est plutôt coûteuse en temps et en ressources (chargement du thesaurus en mémoire).
Un seuil adaptatif évoluant en fonction de la taille des phrases pourra également être mis en place dans de futurs travaux. Pour des phrases standards comportant entre 8 et 15 mots, il sera préférable de fixer le seuil à 5, en revanche si la phrase excède la vingtaine de mots, il faudra définir le seuil entre 10 et 12 mots communs.
Au final, cette approche reste naïve et gourmande aussi bien en temps qu'en ressources, néanmoins elle permet de détecter des reformulations jusque là impossibles à détecter avec des méthodes conventionnelles à alignement et constitue donc une alternative intéressante. Elle est à privilégier pour la détection de reformulations non paraphrastiques.

Introduction
La plupart des logiciels anti-plagiat se concentrent sur une détection extrinsèque de plagiat, c'est-à-dire sur le fait de trouver des similitudes entre un document et un corpus de sources probables. Or ce système est inutile si le document ayant été plagié ne se trouve pas dans le corpus fouillé. Néanmoins, il existe un autre type de détection, la détection intrinsèque qui exploite des données extraites de l'intérieur même du document. La détection d'auteurs par étude du style d'écriture du document est la forme de détection de plagiat intrinsèque la plus répandue. Cette approche diverge selon les travaux car elle soulève plusieurs problèmes, allant du découpage du texte de façon pertinente, au choix et à la collecte des données stylistiques à surveiller, en passant par la manière de découper et de classer les différents passages du document par auteur. C'est sur ce dernier point que l'article va essentiellement se concentrer.
La détection d'auteur 2.1 La notion de stylométrie
La stylométrie ou l'étude stylométrique d'un texte est une analyse à mi chemin entre une analyse linguistique et statistique. Elle exploite des variables stylométriques, qui sont des caractéristiques linguistiques du texte, afin d'établir des statistiques sur le document étudié. Effectuer l'analyse stylométrique d'un document consiste à surveiller les variations du style d'écriture du document en surveillant l'évolution des variables stylométriques au sein de celuici afin d'en détecter les irrégularités et ainsi pouvoir déterminer si certains passages, appelés phases stylistiques, sortent de la norme par rapport à la majorité du texte.
État de l'art
Dès le XIX e siècle, Mendenhall (1887) suggère qu'en analysant des caractéristiques internes d'un texte on peut en reconnaître l'auteur. Depuis, les techniques d'études stylomé-triques de document ont fait d'importantes avancées et de nombreuses recherches (Stein et Eissen, 2007;Layton et al., 2013;Jayapal et Goswami, 2013) appliquent cette découverte à la détection de plagiat. Certaines de ces recherches se concentrent sur l'extraction et la surveillance des données stylométriques les plus pertinentes. Stein et Eissen (2007) ainsi que Zamani et al. (2014)    (Cheng, 1995), un algorithme multidimensionnel des k-moyennes non paramétrique.
Segmentation
Dans un premier temps, l'idée est de segmenter le document. Il est important que chaque segment conserve un sens afin d'être autonome et donc d'être potentiellement écrit par une personne différente. Une segmentation en unité de sens est donc à privilégier. S'appuyant sur le travail de Zechner et al. (2009), c'est une segmentation pseudo sémantique qui a été retenue : un découpage par phrase d'une taille minimale (en mots). Le seuil a été fixé à 15 mots, taille moyenne des phrases dans la langue française.
Extraction de la stylométrie
La seconde étape du processus consiste à extraire la stylométrie de chaque segment. Pour ce faire, il faut au préalable détecter la langue de chaque segment au moyen d'un module im-plémentant la technique de catégorisation de texte à base de n-grammes de Cavnar et Trenkle (1994). Ensuite, l'étiqueteur morphosyntaxique TreeTagger (Schmid, 1994)  
Construction des courbes
Une fois la segmentation et les calculs stylométriques opérés, on obtient donc plusieurs valeurs par segment (i.e. une valeur par variable stylométrique). Une suite de valeurs brutes sans cohérence n'étant pas exploitable, on représente la stylométrie du document sous la forme de courbes, avec en abscisse, la position des segments (la ligne de vie du document) et en ordonnée, les valeurs des variables stylométriques observées. Ceci a pour avantage, en plus de permettre une représentation visuelle, de faciliter la comparaison et la manipulation des valeurs entre elles, les algorithmes de manipulation de courbes étant courant.
Il est possible que le style d'un même auteur varie énormément au fil d'un même texte. La fatigue ou la maturité lors de longs écrits peuvent entraîner du bruit ou des variations brusques. On convient alors qu'un lissage est nécessaire. C'est le lissage par la moyenne glissante sans pondération (Chou, 1975) qui a été utilisé dans cet article.
Regroupement
Il reste à déterminer les phases stylistiques de façon automatique. Un algorithme d'apprentissage automatique non supervisé (i.e. sans intervention humaine) est idéal dans ce cas de figure qui s'apparente au clustering car il faut déterminer à quel auteur (i.e. à quel cluster) chaque donnée s'apparente. Sachant que le nombre d'auteurs et donc de clusters n'est pas connu à l'avance, c'est le Mean Shift multidimensionnel (Cheng, 1995) qui se dégage. En effet, cet algorithme permet de clustériser un ensemble de points sans connaître à l'avance le nombre k de clusters. L'idée dans notre cas est de déterminer le nombre k à partir d'un seuil. On dé-finit alors empiriquement un nombre k de départ assez grand, admettons 10 et un seuil, entre 2% et 15% en fonction de la moyenne de la variable stylométrique observée (seuil adaptatif). Tant qu'il existe deux clusters voisins avec une différence de stylométrie inférieure au seuil, on relance un KMeans avec k = k ? 1. Une fois toutes nos phases identifiées et k définitif, s'il existe deux clusters (non voisins cette fois-ci étant donné que les voisins ont déjà été réunifiés) avec une différence de stylométrie inférieure au seuil, on en déduit qu'ils sont du même auteur.
On prend en considération plusieurs variables stylométriques en même temps, tout comme le fait van Halteren (2004). L'idée est de surveiller plusieurs variables stylométriques afin qu'elles se « complètent » mutuellement. On augmente ainsi le taux de certitude de l'existence d'une zone par le fait qu'une zone est définie comme telle si la majorité des courbes fléchissent de telle façon à la dessiner. De plus, la zone de flexion retenue est maintenant désignée par la moyenne des zones de flexion de toutes les courbes surveillées, ceci réduisant considérable-ment l'erreur d'approximation et rendant plus sûr notre prise de décision. Pour exemple, sur la FIG. 1 -Mean Shift sur plusieurs variables stylométrique.
figure 1 chaque zone de flexion des courbes est représentée par une ligne verticale pointillée de la même couleur que la courbe dont elle dépend. Les lignes pointillées noires plus épaisses représentent les découpages retenus (les moyennes des trois flexions des trois courbes).
Pour faciliter l'observation des écarts et des flexions, les courbes sur cette figure ont été normalisées (mises à la même échelle), leurs valeurs stylométriques sont donc faussées.
Évaluation et tests 4.1 La base de tests et protocole
La base de tests est composée de 500 textes contenant en moyenne 7 000 mots. Les textes sont constitués d'un (l'intégralité du texte) à cinq passages, chaque passage étant potentiellement écrit par un auteur différent. Un texte peut contenir plusieurs passages écrits par un même auteur. On recense en totalité dans la base, une dizaine d'auteurs différents. La langue prédomi-nante au sein des textes est le français, cependant pour tester l'adaptabilité et le plurilinguisme du système de nombreux passages sont en anglais ou en italien. Afin de tester correctement la procédure évaluée, des passages traitant du même sujet et donc employant le même vocabulaire ont été utilisés dans le but de tromper la stylométrie extraite. De plus, l'intégralité des textes est annotée, de telle sorte à savoir précisément de quel mot à quel mot les textes sont écrits par un auteur ou par un autre.
Résultats
Notre procédure présente une précision de 0.89 et un rappel de 0.34. Il est néanmoins important d'étudier plus en détails les limites de cette procédure et de nuancer un rappel si faible. La figure 2 est un diagramme à bulles représentant les performances du découpage stylomé-trique. L'axe des abscisses représente la taille en segment de la phase stylistique concernée et celui des ordonnées l'écart moyen de la variable stylométrique observé entre cette phase et ses voisines, son unité est notée us pour unité stylométrique. Les bulles représentent les différents 
Conclusions
Notre approche montre des résultats exploitables lorsque les phases stylométriques à identifier ne sont pas trop importantes (n'excèdent pas 190 segments soit environ 4000 mots) et lorsque la différence de stylométrie est suffisamment grande (supérieur à 0.20 us). En revanche dans tous autres cas, les limites de notre approche se font ressentir. Pour palier ces problèmes, un seuil adaptatif pourra être défini en fonction du type de variable stylométrique surveillée. De plus, avec du recul, nous convenons qu'un Mean Shift n'était sans doute pas la meilleure option de clustering. Dans la suite de nos travaux nous implémenterons d'autres classifieurs (hiérarchique, DBSCAN, etc.).
Pour conclure, bien que perfectible, cette approche permet de détecter différents styles d'écriture au sein d'un même texte et notre contribution malgré ses limites permet bien de regrouper automatiquement les phases stylistiques par auteur.

Introduction
Le risque chimique ou alimentaire se manifeste lorsque les produits chimiques sont dangereux pour la santé et consommation humaine ou animale, et pour l'environnement. Si certains produits et substances sont maintenant clairement identifiés comme dangereux (e.g. l'amiante, l'arsenic, le plomb), nos connaissances actuelles sur d'autres substances sont moins complètes. Nous nous intéressons en particulier au risque alimentaire (e.g. l'arsenic, les nitrates, la listeria, la dioxine) et au risque chimique (e.g. le bisphénol A, les phtalates). Ces substances entrent souvent dans la composition de produits courants et peuvent avoir l'effet nuisible sur l'organisme humain. Le contrôle sur la commercialisation de ces substances est effectué par des organismes sanitaires dédiés, comme EFSA (European Food Safety Authority) ou ANSES (Agence nationale de sécurité sanitaire de l'alimentation, de l'environnement et du travail). Les experts se retrouvent face à une littérature scientifique abondante et doivent l'étudier pour avoir une base solide pour la prise de décisions. L'objectif de notre travail consiste à proposer une aide automatique pour l'analyse de la littérature scientifique afin de détecter les phrases indicatives du risque induit par ces substances. Nous abordons cette tâche comme une problématique de catégorisation : les phrases des textes doivent être catégorisées dans les classes du risque. Nous présentons les données (section 2) et approches utilisées (sections 3 et 4). Nous discutons ensuite les résultats obtenus et concluons avec les pistes pour les travaux futurs (section 5).
Notre objectif est de catégoriser les phrases des corpus dans les classes de risque. L'é-valuation est effectuée par rapport aux données de référence. Une liste de mots vides et des ressources linguistiques sont aussi utilisées. Le travail est effectué avec le matériel en anglais.
Corpus. Les corpus proviennent de la littérature scientifique, qui est le matériel typique utilisé par les experts. Le corpus du risque chimique (80 000 occ.) contient le rapport sur le bisphénol A (EFSA Panel, 2010). Le corpus du risque alimentaire (>240 000 occ.) a été constitué à partir de 115 documents officiels publiés entre 2000 et 2010 sur une dizaine de substances, comme l'arsenic, la dioxine ou le nitrate (Blanchemanche et al., 2013). Trois sections (introduction, conclusion et résumé) sont traitées car elles comportent les résultats principaux.
Classifications du risque. Les classifications du risque (alimentaire (Blanchemanche et al., 2013) et chimique (Maxim et van der Sluijs, 2014)) sont structurées hiérarchiquement et décrivent différents aspects révélateurs de la nocivité des substances chimiques :
-significativité des résultats (The Panel concluded that the current NOAEL for BPA (5 mg/kg b.w./day) would be sufficiently low to exclude any concern for this effect) ; -hypothèse scientifique (Despite this lack of evidence, the possibility of poultry and egg consumption as an exposure route to HPAIV remains a concern to food safety experts). Le risque est présent lorsque la nocivité des substances est apparente dans la littérature scientifique, ou lorsque les expériences présentées montrent des imprécisions et incertitudes.
Ressources linguistiques. Des ressources linguistiques sont utilisées avec l'approche par apprentissage supervisé pour enrichir l'annotation. Nous supposons que ces différentes expressions, souvent liées à la notion d'incertitude, sont indicatrices de la notion du risque chimique :
-l'incertitude (e.g. possible, should, may, usually) indique des doutes existant au sujet des résultats obtenus expérimentalement, leur interprétation, etc. ; -la négation (e.g. no, neither, lack, absent, missing) indique que de tels résultats n'ont pas été observés, que l'étude ne respecte pas les normes, etc. ; -les limitations (e.g. only, shortcoming, insufficient) indiquent des limites, comme la taille insuffisante de l'échantillon traité, le faible nombre de tests ou de doses testées, etc. ; -l'approximation (e.g., approximately, commonly, estimated) indique d'autres insuffisances liées aux valeurs imprécises de substances, d'échantillons, de doses, etc. Avec la recherche d'information, nous utilisons des ressources pour l'extension de requêtes :
-101 805 paires de synonymes provenant de la langue générale (Fellbaum, 1998) et spé-cialisée (Grabar et Hamon, 2010), -des clusters de mots générés avec des méthodes distributionnelles à partir des corpus (Brown et al., 1992;Liang, 2005). Données de référence. Les données de référence sont obtenues grâce à l'annotation par des spécialistes en évaluation du risque. Un expert a annoté 425 phrases couvrant 55 classes du risque chimique. Plusieurs experts ont participé dans l'annotation du corpus du risque alimentaire et fournissent des données de référence pour 657 phrases monoclasses couvrant 27 classes et 389 phrases multiclasses, pour un total de 1 046 phrases annotées. Plusieurs des classes contiennent très peu de phrases annotées et nous gardons celles qui fournissent un nombre suffisant d'exemples (minimum de 10 pour le risque alimentaire, 5 pour le risque chimique).
Liste de mots vides. La liste de mots vides contient 176 mots (e.g., & about again all almost and any by do to etc). Cette liste contient essentiellement des mots grammaticaux.
3 Approche par apprentissage supervisé Méthode. Nous utilisons différents algorithmes de la plateforme Weka (Witten et Frank, 2005) avec le paramétrage par défaut. Les phrases sont l'unité de travail. Nous visons la dé-tection de phrases liées au risque : (1) de manière générale G pour détecter les phrases relatives au risque ; (2) de manière précise D pour associer ces phrases aux classes de risque. Les descripteurs sont fournis par l'annotation sémantique et linguistique : forms (les formes de mots comme elles apparaissent dans le corpus), lemmas (mots lemmatisés), lf (combinaison de formes et de lemmes), tag (les étiquettes morpho-syntaxiques des formes (e.g. noms, verbes, adjectifs)), lft (combinaison de formes, lemmes et étiquettes morpho-syntaxiques), stag (étiquettes sémantiques de mots (e.g. incertitude, négation, limitations)), all (combinaison de tous les descripteurs). Les descripteurs sont pondérés de trois manières : freq (fréquence brute des descripteurs), norm (fréquence normalisée par la taille du corpus), tfidf (pondération tfidf (Salton et Buckley, 1987)). Nous effectuons une validation croisée. Les mesures d'évaluation sont la précision, le rappel et la F-mesure (moyenne harmonique de la précision et du rappel). Résultats. Les résultats présentés sont obtenus avec J48 (Quinlan, 1993). Avec l'expéri-ence G, les performances avec le risque alimentaire (autour de 0,8) sont meilleures que celles du risque chimique (0,61-0,64). Les performances sont assez stables avec les différents descripteurs et pondérations. L'exploitation de formes, d'étiquettes sémantiques et les différentes combinaisons de descripteurs donnent des résultats légèrement supérieurs. Bien que très simplistes, les étiquettes morpho-syntaxiques (e.g. noms, verbes, adjectifs) sont assez efficaces sur les deux corpus. Les étiquettes sémantiques seules (stag) sont parmi les plus efficaces pour détecter le risque chimique, mais montrent une F-mesure assez faible pour le risque alimentaire. À la figure 1, nous présentons l'expérience D avec les descripteurs lft (formes, lemmes et étiquettes morpho-syntaxiques). Les résultats sont élevés avec les classes du risque alimentaire et deux classes du risque chimique (Facteur d'incertitude et Hypothèses scientifiques). Le tfidf donne de meilleurs résultats dans la plupart des cas, mais la pondération norm est aussi compétitive. Les descripteurs lft fournissent de meilleurs résultats que les autres descripteurs. Les résultats sont meilleurs avec le risque alimentaire, où il existe plus de données d'apprentissage.
Approche de recherche d'information
Méthode. Nous considérons les libellés des classes comme les requêtes et les phrases des corpus comme les réponses potentielles à ces requêtes. Nous exploitons le système de recherche d'information Indri (Strohman et al., 2005), qui utilise un modèle probabiliste basé sur le champ aléatoire de Markov et offre plusieurs fonctionnalités, comme par exemple :
-la racinisation (Porter (Porter, 1980) et de Krovetz (Krovetz, 1993)) réduit un mot à sa racine (e.g., suppression de pluriels et de chaînes finales comme -ment et -ique) ; -le et booléen (band) permet de combiner plusieurs mots clés ; -les fenêtres ordonnées ou non ordonnées permettent de spécifier l'ordre des mots clés ; -la pondération (tfidf (Salton et Buckley, 1987) et okapi (Robertson et al., 1998)) permet de relativiser le poids des mots-clés ; -la pondération des synonymes (wsyn) permet d'indiquer l'importance des mots clés. Pour l'expansion des requêtes, nous retenons les mots supplémentaires des ressources linguistiques (synonymes et clusters) si ces mots montrent au moins 0,3 % de précision. L'évaluation est effectuée avec plusieurs mesures : précision, rappel, F-mesure et MAP (Mean Average Precision), cette dernière prenant en compte l'ordre des réponses. Pour la baseline, les mots des libellés de classes sont utilisés, sans la racinisation ni l'expansion de requêtes. Résultats. Le tableau 1 indique la MAP et la F-mesure de différentes expériences : baseline, utilisation de raciniseurs, pondération des mots clés et des clusters. Nous obtenons de meilleurs résultats avec les libellés du risque chimique, car ils sont plus explicites. La F-mesure est en général plus élevée que la MAP. Les résultats sont améliorés avec la racinisation, la pondération tfidf et okapi, et les clusters. Plusieurs autres expériences n'ont pas été concluantes (e.g. exploitation des définitions, pondération des synonymes, fenêtres ordonnées des mots clés des requêtes, et booléen). Krovetz, la pondération et les clusters fournissent les meilleurs ré-sultats (figure 2). Les raciniseurs améliorent le rappel et donc les performances globales, tandis que l'utilisation de la pondération des mots clés (okapi ou tfidf) améliore surtout les valeurs de la MAP : les phrases retournées sont alors les mêmes, mais leur ordre devient plus correct. Les ressources linguistiques supplémentaires sont favorables pour certaines classes. Elles permettent surtout d'améliorer le rappel.
Discussion et Conclusion
L'apprentissage supervisé est plus performant que la recherche d'information, tandis que cette dernière, étant moins supervisée, permet de traiter un plus grand nombre de classes. La recherche d'information permet aussi de varier plus facilement les paramètres selon que l'on voudrait privilégier la précision ou le rappel. La pondération montre toujours un effet favorable. Dans une expérience similaire avec le risque alimentaire, des résultats comparables aux nôtres sont obtenus (Blanchemanche et al., 2013). Notons que nous avons aussi testé une approche non supervisée à base de règles, qui montre des résultats très faibles : rappel quasinul pour une précision entre 0,5 et 0,6. Il existe plusieurs possibilités pour combiner les deux approches testées : combinaison des sorties pour augmenter le rappel ; le vote des approches pour améliorer la précision ; l'utilisation des noeuds décisionnels des modèles d'apprentissage supervisé pour l'extension de requêtes ; l'exploitation des sorties de recherche d'information et du système à base de règles par l'apprentissage supervisé.
En conclusion, nous utilisons l'apprentissage supervisé et la recherche d'information pour détecter des phrases relatives au risque induit par les substances chimiques. Nous abordons la tâche comme une problématique de catégorisation : les phrases des textes doivent être caté-gorisées dans les classes de risque. Deux corpus et deux classifications du risque sont utilisés. Les résultats par apprentissage automatique sont les plus performants. Les résultats indiquent aussi que l'expression de l'incertitude linguistique (e.g., likely, should, assume) est associée avec la notion du risque chimique. Dans les travaux futurs, nous allons tester d'autres paramètres pour améliorer les performances des approches testées et nous allons combiner les résultats de ces approches de différentes manières. Ces résultats peuvent être utilisés par les experts travaillant sur la gestion du risque pour la prise de décisions et évalués par eux.
Remerciements. Ce travail est soutenu par le projet PNRPE DICO-Risk.
Références
Blanchemanche, S., A. Rona-Tas, A. Duroy, et C. Martin (2013). Empirical ontology of scientific uncertainty : Expression of uncertainty in food risk analysis. In Society for Social Studies of Science, pp. 1-27.

Introduction
Ces dernières années, nous assistons à la sémantisation des données statiques et dynamiques (flux de données). Toutefois, vu la spécificité de ces derniers ni les technologies du web sémantique ni celles des Systèmes de Gestions de Flux de Données (SGFD) ne peuvent les traiter. Pour ce faire, les chercheurs proposent aujourd'hui de nouveaux systèmes tels que C-SPARQL (Barbieri et al., 2010), CQELS (Phuoc) et SPARQL Stream (Calbimonte et al.). Lorsque le débit du flux en entrée de ces systèmes dépasse les seuils supportés, deux solutions existent : 1-Allouer au système autant de ressources que nécessaires (Hoeksema et Kotoulas, 2011)  manière, nous préservons le niveau sémantique de l'information et protégeons la cohérence des données du graphe en mémoire.
Conclusion
Nous avons proposé dans cet article une approche orientée graphe pour la réduction de charge des systèmes de traitement de flux de données sémantiques. Notre approche, permet d'améliorer la qualité des résultats des requêtes des systèmes de traitement de flux de données sémantiques, en protégeant la sémantique et la cohérence des données de ces flux, contrairement à une application naïve de l'approche orientée triplet RDF utilisée jusqu'à présent.

Introduction à Sélection basée sur le Degré de Pertinence
Les librairies digitales sont actuellement très répandues. Elles renferment des quantités d'informations énormes et nécessitent des mécanismes efficaces d'indexation et de manipulation. Les moteurs de recherche du type général ne peuvent pas les indexer car ils exigent que l'information qu'ils manipulent soit composée d'entités indépendantes. Dans le besoin de traiter rapidement et efficacement les requêtes, des méthodes basées des approches diffé-rentes ont été inventées. On rencontre alors, des méthodes se basant sur les réseaux bayésiens comme CORI Callan et al. (1995), d'autres méthodes qui se basent sur les statistiques TF*IDF. Il existe aussi des méthodes qui se basent sur le modèle de langage et la pseudo-pertinence. Ces méthodes utilisent des résultats déjà obtenus pour de réponses futures. Puisque le modèle centralisé souffre du problème de passage à l'échelle, certaines méthodes ont été mises pour tourner sur les systèmes pair-à-pair. La méthode CORI a été une source d'inspiration et a été utilisée comme moyen de classification dans beaucoup de travaux. Cette méthode fonctionne sur un système bayésien pour localiser des réponses probables aux utilisateurs. La fonction de score donnée dépend de certains paramètres obtenus à partir d'expérimentations sur des datasets. Ce paramétrage fait que CORI est devenue instable. Ces paramètres doivent être réajustés pour chaque nouvelle collection. Afin de réduire le nombre de collections interrogées, Abbaci et al. (2002) présente la méthode CS. Celle-ci définit ndoc le nombre de documents à retourner et tient compte uniquement des deux premiers termes lors de l'évaluation des requêtes longues. Bien que l'objectif de réduction de flux est atteint, CS produit des faux positifs et faux négatifs importants à cause des restrictions imposées. Soit un système distribué où un serveur appelé courtier est lié à un ensemble de serveurs. Le courtier détient un index Terme/Serveur qui indique pour chaque terme t i la liste des serveurs qui le manipulent. Chaque serveur S i est responsable d'une collection de documents c i et manipule un index Terme/Documents. Cet index définit pour chaque terme t i la liste des documents où il figure. Par cette définition, le courtier sélectionne de façon déterministe le sous-ensemble de serveurs pertinents. Ces index permettent de réduire la charge du système. Un document est jugé pertinent s'il partage au moins un terme avec la requête. Plus un document partage de termes avec la requête plus son degré de pertinence s'élève, induisant ainsi que le score d'une Sélection basée sur la pertinence
collection est proportionnel aux nombre de documents pertinents qu'elle contient. Sur cette définition, pour une requête q, le score d'une collection c i se calcule selon la fonction SDP suivante :
T F ti est la fréquence du terme t dans la collection c i . L'expérimentation des trois méthodes sur le dataset Reuters21578, sur un système distribué. La figure Fig. 1 présente la comparaison entre les trois méthodes en fonction du recall. Nous avons réalisé des expéri-mentations intensives en faisant varier le Top-k. Nous remarquons que les valeurs pour cette métrique sont plus grandes dans SDP que dans les autres méthodes. CS (CS2 pour ndoc=2, CS3 pour ndoc=3, CS5 pour ndoc=5) a retourné un recall plus faible. C'est certainement à cause du ndoc qui influence la recherche. Avec ndoc=2 et 3 ; le recall n'atteint pas 1 c-à-dire il existe des documents pertinents et rares où le système n'arrive pas à les sélectionner. CORI c'est placé au-dessus de CS.

Introduction
Avec les avancées technologiques en terme d'acquisition des données scientifiques (images satellitaires, capteurs, etc.), les scientifiques s'intéressent de plus en plus à des applications importantes en terme de surveillance et suivi de l'environnement. Les données collectées sont généralement hétérogènes, multiéchelles, spatiales et temporelles (série temporelle d'images satellites, aériennes, modèles numériques de terrain, nature du sol ...) et sont destinées à comprendre et prédire des phénomènes résultant de processus complexes et d'origine pluridisciplinaire (données climatiques, géologiques, ...). L'explosion de cette information spatiale, temporelle et des systèmes d'informations géographiques nécessitent l'investissement dans des méthodes d'extraction de connaissances et nous nous intéressons à celles qui reposent sur la détection de motifs locaux comme, par exemple, la découverte de motifs séquentiels (Agrawal et Srikant, 1995;Mannila et al., 1997;Masseglia et al., 1998) ou de motifs plus complexes comme des sous-graphes Inokuchi et al. (2000) ou des sous-arbres Zaki (2002). Nos besoins concernent l'étude spatiale et temporelle des évolutions d'objets et de leurs interactions. Les objets peuvent être caractérisés par plusieurs attributs et leurs évolutions que l'on appelle parfois dynamique se décrivent par les évolutions des attributs, par leur emplacement géographique, leur existence (apparition/disparition) et leur structure topologique (fusion/division). Pour certaines applications, nous pouvons transformer la base de données spatio-temporelles dans une base de données transactionnelles Hai et al. (2012) ou dans une base de séquences pour les analyser. Cependant, ces transformations peuvent s'avérer très fastidieuse et les résultats peuvent être difficilement interprétables. Des domaines de motifs plus sophistiqués et applicables à l'étude de phénomènes spatio-temporels ont donc été proposés. Ainsi, plusieurs travaux se sont intéressés à l'extraction de motifs dans des graphes étiquetés Inokuchi et al. (2000). Quelques travaux ont été menés dernièrement sur des graphes attribués (Fukuzaki et al., 2010;Miyoshi et al., 2009;Desmier et al., 2013). Les difficultés dans la fouille de graphes attribués résident dans l'explosion combinatoire de l'exploration de l'espace de recherche. En effet, cette espace de recherche porte à la fois sur les combinaisons de graphes et les combinaisons d'attributs. Dans un travail présenté dans (Sanhes et al. (2013a,b)), Sanhes et al. ont proposé de travailler à la modélisation de données spatio-temporelles dans des DAG attribués, autrement dit un unique graphe orienté acyclique attribué (a-DAG) (cf. Figure 1) : les sommets sont des objets spatiaux caractérisés par un ensemble d'attributs ou caractéristiques et les arcs dénotent la proximité spatio-temporelle entre ces objets (par exemple le voisinage spatial entre deux objets de deux pas de temps consécutifs). Le but est de trouver les transitions ou cheminements de caractéristiques pouvant montrer une tendance attendue ou surprenante, expliquer un phénomène particulier, ce qui revient à chercher dans un a-DAG les chemins fréquents d'attributs. On trouve quelques travaux s'attaquant à la fouille de graphes orientés FIGURE 1: Exemple de a-DAG construit sur des objets représentés dans des images temporelles acycliques mais étiquetés (et non pas attribués) tels que Chen et al. (2004);Termier et al. (2007). Ces méthodes recherchent des sous-graphes dans un ensemble de graphes, et de plus les sommets sont plutôt labélisés ou considérés comme labélisés et non attribués. Dans notre cas, on est en présence d'un seul graphe orienté acyclique et attribué (a-DAG) ce qui pose des problèmes très différents. Le domaine de motif proposé pour la première fois dans Sanhes et al. (2013b) est appelé domaine des chemins pondérés dans un a-DAG. Lorsque nous avons voulu travailler à une implémentation efficace de l'algorithme d'extraction de ce type de motif, le seul à notre connaissance qui calcule des motifs dans des DAG attribués, nous avons étudié de près ses propriétés et nous avons découvert qu'il était juste mais incomplet. Nous présentons ici un nouvel algorithme permettant de réaliser l'extraction de tels motifs de façon complète. Non seulement nous proposons une correction du premier algorithme mais aussi nous étudions des optimisations nécessaires au passage à l'échelle en introduisant des structures de données complémentaires comme un graphe de motifs. Nous montrons que la performance de l'extraction est améliorée de plusieurs ordres de magnitude sur des jeux de données artificiels et nous l'appliquons aussi à des données réelles pour motiver qualitativement l'usage des chemins pondérés.
Dans la section suivante, nous présenterons les concepts et définitions nécessaires à la compréhension de l'algorithme. En Section 3, nous prouvons l'incomplétude de l'algorithme existant. Nous proposons une solution de complétude et une optimisation basée sur une structure de graphe de motifs en Section 4. Nous montrerons les performances de l'algorithme complet et optimisé sur des jeux de données artificielles en Section 5. Et enfin, nous conclurons en Section 6. 
Un chemin P est une séquence d'itemsets P = P 1 2 · · · n tel qu'il existe un chemin O = v 1 2 · · · n dans le graphe où P i est inclus dans l'ensemble des items de v i (notion à différentier de la définition classique d'un chemin dans un graphe). On dit que alors que O est une occurrence du chemin P et l'ensemble des occurrences de P est noté occu G (P ).
Par exemple dans le graphe de la figure 2 les occurrences du chemin de taille 3 ah sont 2 3 6 , 2 3 8 , 2 4 7 , 2 5 7 , et 5 7 8 . Un chemin pondéré P est un chemin où un poids est associé à chaque arc P i i+1 constituant P . Ce poids correspond au nombre d'occurrences distinctes de P i i+1 dans le graphe. Pour l'exemple précédent, le chemin ah et ses occurrences permettent de construire le motif pondéré : ah 4 cd 5 i. En effet, le nombre d'occurrences de ah dans occur G (P ) st 4, et le nombre d'occurrences de cd dans occur G (P ) est 5. Une telle représentation permet de voir que l'itemset ah apparaît 4 fois avant l'apparition du chemin cd et que l'itemset i apparaît 5 fois après l'apparition du chemin ah Dorénavant, ? G (P i i+1 ) désignera le poids de l'arc entre les itemsets
Relation d'inclusion L'opérateur d'inclusion sur un couple de chemins pondérés est défini de la manière suivante :
Autrement dit, P est inclus dans P' s'il existe une sous-séquence Q de P' tel que les itemsets de P sont inclus un à un dans ceux de Q avec les mêmes poids au niveau des arcs. On dit que P est un sur-chemin ou super-chemin pondéré de P . A partir de la mesure proposée par Bringmann et Nijssen (2008), nous définissons le support d'un chemin pondéré P, noté ?(P ), comme étant le poids minimal de ses arcs.
Chemin pondéré condensé Un chemin pondéré P est un condensé s'il n'admet aucun surchemin pondéré. Pour simplifier, nous appellerons motif un chemin pondéré et les occurrences d'un motif seront tout simplement des chemins.
Cette méthode permet bien l'extraction des motifs condensés de manière juste mais ne les génère pas tous. En effet, toutes les graines condensées forment bien des motifs condensés mais un motif condensé de taille supérieure à 2 peut contenir des graines non condensées. Effectivement, il existe des motifs condensés au sens de l'inclusion qui peuvent être formés par certains motifs de taille 2 qui ne sont pas générés par la première étape.
Pour illustrer l'incomplétude de l'algorithme, nous montrons un contre-exemple sur le graphe de la figure 3. Dans ce a-DAG, les graines générées par la première étape de l'algorithme ne permettent pas de construire le motif condensé a 1 bc 1 de.
:a 2 :a
Condensés représentés dans le a-DAG :
• chemins de tailles 2 : a de où ? ne vaut plus 2 car toutes les occurrences de P ne sont pas utilisées : 2 4 n'est pas relié à de. Dans ce cas on parle d'extension avec perte d'occurrences. Par conséquent il faut déterminer les occurrences utilisées et mettre à jour les différents poids (?) ainsi que les itemsets du motif. En réalité avec la séparation des 2 étapes, l'information structurelle est perdue pendant le parcours en profondeur.
Algorithme complet et optimisé
Dans ce paragraphe, nous proposerons une solution permettant de corriger la complétude et nous proposerons par la même occasion une version optimisée utilisant une structure de graphe permettant de stocker les motifs que l'on appellera graphe de motifs.
... ?n I n un motif. Nous pouvons étendre P sans perte d'occurrences s'il existe un itemset I n+1 fermé fréquent dans l'ensemble des sommets accessibles par V n (nous appellerons I n+1 un fermé fréquent local à V n ), tels que V n+1 supporte I n+1 et qu'il existe au moins un arc de chaque sommet de V n vers V n+1 , c'est-à-dire que toutes les occurrences de P sont conservées. De manière analogue, nous pouvons étendre P avec perte d'occurrences lorsque toutes les occurrences de P ne sont pas utilisées lors de l'extension. Dans ce cas nous obtenons un motif P = I 1 ? 1
... 
Stratégie de l'algorithme complet
À partir des notions introduites précédemment, nous pouvons présenter la stratégie géné-rale de l'algorithme complet (cf. algorithme 1). Cet algorithme est basé sur un parcours en profondeur de l'espace de recherche pour étendre les motifs condensés.
En partant de l'ensemble des sommets du graphe, l'algorithme effectue un parcours en profondeur dans l'espace de recherche pour étendre le motif condensé P initialisé à ?. La ligne 1 exprime le cas d'arrêt de l'algorithme : l'ensemble des sommets destinations V P est vide. Le parcours en profondeur se fait aux lignes 2 et 3. L'extension de P se fait avec l'itemset Y fermé fréquent par rapport aux sommets de V P (ligne 4). Nous notons P le motif ainsi étendu. Lorsqu'il s'agit d'une extension avec perte d'occurrences, il est nécessaire de mettre à jour les occurrences du motif (ligne 5). Ce nouveau motif P est potentiellement un motif ou un sous-motif condensé. Nous supprimons les motifs qui sont inclus dans P , et insérons P dans C l'ensemble des motifs condensés (lignes 7 et 8). Puis nous continuons le parcours (ligne 9).
La complexité de l'algorithme ne permet pas le passage à l'échelle à cause de nombreux tests coûteux d'inclusion de motifs pour vérifier sa maximalité. Nous proposons ci-dessous une implémentation optimisée basée sur une structure de graphe pour stocker les motifs. Cette nouvelle structure va permettre d'éviter les tests trop coûteux de comparaison entre motifs.
Algorithme 1 : DepthFirstMining
Entrées : P motif courant (? à l'appel initial) V P ens. des sommets destinations de P (V P = V G à l'appel initial) C ens. des motifs condensés (? à l'appel initial) Mettre à jour les occurrences de P 6
Supprimer dans C les motifs Q inclus dans P 7
Insérer P dans C.  
Implémentation optimisée
Nous allons nous servir de la maximalité des chemins pondérés condensés recherchés pour optimiser l'algorithme qui se traduit par la maximalité des itemsets (itemsets fermés) du chemin et sa taille. Les tests d'inclusions de l'algorithme se font sur les itemsets et sur les chemins. Pour éviter ces tests nous allons définir une structure de graphe permettant le stockage des motifs trouvés au fur et à mesure du parcours de l'espace de recherche. Cette structure est appelée graphe de motifs. • V m ? P(V ) est l'ensemble des sommets • E m ? P(V ) × P(V ) est l'ensemble des arcs • ? m : la fonction d'attributs définie par : V m ?? P(I) V ?? X où X représente l'itemset maximal caractérisant les sommets de V . Réciproquement, on associe à un itemset X l'ensemble des sommets noté V X supportant l'itemset X. Un sommet du graphe G m est identifié par un ensemble de sommets du graphe G. Un motif condensé est alors un chemin c = V 1 n de G m de longueur maximale (cf. figure 5), i.e. V 1 est une source (pas d'arc incident) et V n est un puits (pas d'arc sortant).
Procédure cherCondRec(X : itemset, V X : ens. de sommets, min_sup : entier) sommets_a_remonter : var. globale contenant les sommets à backtracker
BdT (E + (V X )) la base de données transactionnelles construites à partir de ...
?n?1 I n , c'est une séquence d'itemsets et chaque itemsets I i du motif P représente un sommet dans le graphe des motifs qui n'est autre que V Ii ensemble des sommets du graphe a-DAG contenant l'itemsets I. Le motif P est alors identifié de manière unique par la séquence V I1 In dans le graphe des motifs. Au moment de la construction du graphe des motifs et à l'insertion d'un nouveau sommet V i dans le graphe des motifs, il suffit de vérifier s'il est déjà présent dans le graphe des motifs alors il a déjà été parcouru sinon il est inséré et le parcours de l'espace de recherche continue. L'algorithme final se déroule en 2 grandes étapes suivantes : -Recherche des motifs condensés par un parcours en profondeur de l'espace de recherche (procédure 2). Les motifs sont stockés dans le graphe des motifs. Pendant la recherche, les sommets pour lesquels il y a eu extension avec perte d'occurrences sont marqués pour être traités par la phase de backtracking.
-Phase de backtracking sur les sommets marqués (pour lesquels il y a eu extension avec perte d'occurrences) pour mettre à jour les occurrences des motifs (cf. procédure 3). La première étape fait appel à la procédure cherCondRec qui effectue un parcours en profondeur de l'espace de recherche. Cette procédure étend récursivement les motifs condensés au fur et à mesure de leur construction dans G m . A une étape de la construction du motif P , soit V X le sommet à étendre dans le graphe des motifs G m supportant l'itemset X, on calcule V X utile ensemble de tous les sommets ayant au moins un arc sortant de V X . On calcule tous les items accessibles par X, on obtient une base de données transactionnelles dont les transactions sont les arcs sortants et les items sont les items accessibles par X (ligne 3 et 4). Pour chaque itemset maximal Y dans cette base transactionnelle, on va étendre V X par V Y . Deux cas se présentent :
• V X = V X utile : tous les sommets de V X sont utilisés pour l'extension, il y a extension sans perte. Il suffit alors de créer le sommet V Y et le relier à V X par un arc dans G m .
• V X = V X utile : l'extension est réalisé avec perte d'occurrences, on duplique le motif P en remplaçant le sommet V X par V X utile (à marquer pour être traiter dans la phase de Backtracking). On insère un nouveau sommet V Y et on crée un arc entre
Procédure backtrackRec(V X utile : sommet du graphe des motifs, V toBacktrack : sommet du graphe des motifs, min_sup : entier)
Soit V i utile ensemble des sommets de V i ayant au moins un arc sortant vers V X utile .
4
Insérer V i utile ? > V X utile dans le graphe des motifs G m 5 // Lors de l'insertion, mise à jour des attributs. backtrackRec(V i utile , V i )// backtracking vers le haut 6 L'étape de backtracking décrite par la procédure récursive backtrackRec retraite chaque sommet marqué en visitant la branche du motif dans le sens inverse pour mettre à jour les occurrences des motifs et les informations tels que les poids et les itemsets. La figure 6 montre le déroulement de l'algorithme sur l'exemple du graphe de la figure 5. Les arcs du graphe de motifs sont en bleu. Les arcs en rouge définissent le cas d'extension avec perte d'occurrences, les arcs en vert montrent la phase de backtracking avec la mise à jour des poids et des itemsets.
Expérimentations et résultats
Nous avons appliqué la méthode sur des jeux de données artificielles pour montrer la performance que nous avons comparé avec la méthode incomplète. Pour montrer l'intérêt de ce nouveau domaine de motifs, nous avons utilisé un jeu de données réelles pour le problème de suivi du phénomène de l'érosion.
Dans un premier temps, nous avons créé artificiellement trois jeux de données afin d'observer l'impact de la taille des a-DAG sur les performances notés « V20K, E60K » pour un a-DAG   données artificiel ressemble plus aux jeux de données tirés d'une application spatio-temporelle.

Introduction
La quantité d'information dans le Web a augmenté ces dix dernières années. Ce phénomène a favorisé la progression de la recherche dans le domaine des systèmes de recommandation. Les systèmes de recommandation consistent en un filtrage de l'information dans le but de ne présenter aux utilisateurs que les éléments qui sont susceptibles de l'intéresser, quel que soit le domaine. Les éléments à recommander sont également appelés items et peuvent être de différents types : des produits, services, informations, etc. Les systèmes de recommandation se doivent de sélectionner les informations les plus intéressantes en fonction du but recherché, tout en conciliant nouveauté, surprise et pertinence. Un système de recommandation se base sur des caractéristiques de références acquises de manière automatisée selon plusieurs méthodes différentes. Les caractéristiques de références peuvent provenir de : -L'item (l'objet à recommander) lui-même, on parle alors « d'approche basée sur le contenu » (ou content-based approach) Balabanovi´cBalabanovi´c et Shoham (1997). Le filtrage basé sur le contenu calcule la similarité entre les objets afin de trouver l'objet le plus semblable aux goûts de l'utilisateur. Dans ce cas, l'utilisateur se voit recommander des items similaires à ceux qu'il a préférés dans le passé. -L'utilisateur et l'environnement social, on parle alors « d'approche de filtrage collaboratif » (ou collaborative filtering). Le principe du filtrage collaboratif Breese et al. (1998)   Koren et al. (2009). Pour remplir leurs fonctions, les technologies de recommandation font aujourd'hui face à des défis scientifiques majeurs. Comment intégrer l'hétérogénéité des sources d'information pour modéliser les préférences, comment prendre en compte le contexte, comment traiter efficacement ces masses d'information, quels types d'interfaces faut-il considérer ? Par ailleurs, les deux approches citées précédemment présentent des inconvénients principalement liés au démarrage à froid et à la montée en charge du système d'où la nécessité de mettre en place des algorithmes performants et robustes. Ceci est l'objectif de cette étude en vue d'amélio-rer la qualité des systèmes de recommandation en introduisant de la sémantique aux données et en distribuant les traitements afin de minimiser les temps de calcul. La sémantique est ici représentée par une ontologie du domaine (domaines des films pour les expérimentations).
Architecture globale
Afin de fournir une généricité dans le domaine d'application, un passage à l'échelle et une recommandation précise, nous proposons un système à trois couches : une couche de pré-analyse, une couche sémantique et une couche de recommandation.
Le module de pré-analyse met en oeuvre un filtre de comptage afin d'étudier en profondeur l'intérêt implicite des utilisateurs : le filtre permet de compter le nombre de fois qu'une valeur d'un attribut figure parmi les items évalués par les utilisateurs, pour cela nous utilisons un filtre de Bloom. Un filtre de Bloom Broder et Mitzenmacher (2004) est un tableau de bits qui permet de tester d'une manière rapide l'appartenance d'un élément à un certain ensemble. Le FBC, décrit dans Broder et Mitzenmacher (2004) est une extension du filtre de Bloom standard qui fournit la possibilité de supprimer des éléments du filtre. Le vecteur de bits y est remplacé par un tableau d'entiers, où chaque position est utilisée comme compteur. L'insertion d'un élément est réalisée en incrémentant de 1 les entiers aux positions renvoyées par les fonctions de hachage. Le retrait est réalisé en décrémentant de 1 ces entiers. La question d'appartenance d'un élément au filtre est traitée en regardant si tous les entiers aux positions renvoyées par les k fonctions de hachage sont strictement positifs. Nous proposons de se baser sur l'ontologie du domaine afin d'extraire les attributs des items. Ensuite, nous utilisons les filtres de bloom avec compteur afin de stocker l'intérêt implicite des utilisateurs dans les attributs des éléments. Ceci se fait en suivant les étapes : (1) Pour chaque utilisateur, nous créons un filtre de bloom avec compteur vide, (2) pour chaque élément noté par cet utilisateur, nous extrayons ses attributs et enfin (3) nous insérons ces attributs dans le filtre . Ainsi, le filtre contient tous les attributs des items précédemment notés par l'utilisateur.
Le module sémantique exploite l'ensemble des données ainsi que l'ontologie dans le but de définir les relations entre les utilisateurs, les items et les attributs. Ceci se traduit par la transformation sémantique des notes des utilisateurs. Nous nous intéressons tout d'abord au nombre d'occurrence des attributs qui ont été notés par un utilisateur. Nous appelons cette occurrence « la fréquence d'apparition » ou « coïncidence » : cette valeur correspond au nombre de fois que les valeurs des attributs se répètent dans les items notés par l'utilisateur. Cette valeur est extraite à partir des filtres de bloom avec compteurs.
La deuxième étape consiste à calculer la valeur sémantique (SV) en se basant sur la fré-quence d'apparition. L'équation utilisée est la suivante (1).
Avec F le nombre total des attributs, N u le nombre total des items notés par l'utilisateur "u". C j est la fréquence d'apparition de l'attribut j dans l'ensemble des items qui ont été notés par l'utilisateur et W j étant le poids calculé à partir de la phase de la sélection des attributs par une analyse des composantes principales.
E[r u, * ] est la moyenne des notes de l'utilisateur et r u,i est la valeur du rating initial donnée à l'item "i". L'utilisation de N permet de normaliser l'équation sémantique. Cette équation a l'intérêt de pouvoir prendre en compte des valeurs positives et/ou négatives comme note.
L'équation sémantique peut être appliquée à deux niveaux dans la recommandation. D'une part, nous pouvons appliquer cette équation à toutes les notes disponibles dans la base de données initiale, ce qui permet de mieux expliquer l'intérêt des utilisateurs pour les caractéristiques définissant les items notés (ajouter du sens à la note). D'autre part, nous pouvons faire le choix d'appliquer l'équation sémantique à la sortie de la recommandation. Supposons que le module de recommandation renvoie un résultat des top K items (les K items les plus pertinents) pour un utilisateur donné, avec une estimation de la note pour ces top K. Ces notes seront transformées en une note sémantique suivant l'équation (1) et les items proposés seront réordonnés en conséquence en top K', K' pouvant être inférieur ou égal à K.
Enfin, le module de recommandation utilise une technique de filtrage collaboratif basée sur une méthode de factorisation de la matrice pour générer des recommandations précises. Nous avons fait le choix d'utiliser la factorisation de matrice car cette technique a montré son efficacité comme méthode de filtrage collaboratif pour la recommandation Koren et Bell (2011 Nous avons testé le module sémantique au travers des deux approches : -Application du module sémantique au jeu de données (Semantic dataset). Dans cette approche, l'application du module sémantique (que nous appelons "sémantisation") porte sur les données en entrée du système de recommandation. Il s'agit donc de traiter l'ensemble du jeu de données, avant son analyse par le système de recommandation et le filtrage collaboratif. Cette approche nous permet de retrouver des objets non pris en compte a priori. Cependant, puisque le module sémantique doit analyser tout le jeu de données, le temps de calcul peut être grandement augmenté par rapport à une analyse non sémantique. -Application du module sémantique au résultat de la recommandation Top K (semantic top K). Dans cette première approche, nous cherchons à "sémantiser" les données en sortie du système de recommandation. Le système de recommandation fournit classiquement une liste de K objets, ordonnés par ordre décroissant de préférence. La sé-mantisation réordonne ces objets, et fournit une nouvelle liste plus pertinente. Cette approche est extrêmement légère, et permet d'améliorer la recommandation sans (trop de) perte de temps. De plus, avec cette approche, les paramètres de pondérations peuvent être personnalisés par l'utilisateur plutôt que de considérer l'ensemble du jeu de données. Néanmoins, une analyse a priori des données de l'utilisateur est nécessaire avant de personnaliser les paramètres de pondérations. Dans la section suivante, nous présentons comment ces deux approches modifient les recommandations, et la qualité des résultats obtenus.
F-Mesure
Cette métrique est généralement utilisée dans l'évaluation des systèmes de recommandations. Cette métrique n'évalue pas la qualité de la prédiction des notes, mais la pertinence des items qui sont proposés aux utilisateurs. La F-mesure est une façon courante de combiner le rappel et la précision dans une seule métrique afin de faciliter la comparaison. le rappel étant la probabilité qu'un item choisi soit pertinent et la précision calcule la probabilité qu'un item pertinent soit choisi. tel que nous pouvons le constater dans la figure 1, nos approches donnent
FIG. 2 -ILS in "genre" attribute.
de meilleurs résultats que la technique de matrice de factorisation (SVD++) sans sémantique. l'amélioration est plus prononcée pour le cas de la" sémantisation" du jeu de données.
ILS
ILS (Intra-List Similarity), appelée également ILD (Intra-List Diversity) mesure la diversité/similarité entre les items dans la liste des top-k présentée à l'utilisateur. Un bon système de recommandation doit trouver l'équilibre entre ces deux concepts diversité et similarité. En effet, des items trop diversifiés peut provoquer une confusion chez l'utilisateur, alors que recommander toujours les mêmes items peut ennuyer celui-ci. Le figure 2 représente cette mesure en se concentrant sur le attribut genres des films dans le top-k. Des valeurs élevées correspondent à une grande similarité. Ainsi, nous pouvons constater que notre approche permet de retourner des items plus similaires dans le top-k. Ceci est du au fait que nous prenons en compte l'intérêt pour les attributs afin d'identifier les items susceptibles d'intéresser l'utilisateur.
Conclusion
Nous avons proposé un système de recommandation qui repose sur deux concepts : relations sémantiques entre les données manipulées et un filtrage collaboratif basé sur la factorisation des matrices. Dans le but d'améliorer la pertinence des recommandations, nous avons étudié en profondeur l'intérêt implicite des utilisateurs pour les attributs des items. pour cela, nous appliquons une équation sémantique permettant de modifier les notes initiales des utilisateurs pour refléter leur intérêt pour les items.
Notre système de recommandation opère en plusieurs étapes : Nous comptons le nombre de fois qu'un attribut figure dans les items notés par les utilisateurs. Pour cela, nous nous sommes appuyés sur l'utilisation d'un filtre de bloom avec compteur. Ensuite, après passage par le module sémantique (transformation des notes des utilisateurs en appliquant l'équation sémantique) , les recommandations sont générées en utilisant une technique de matrice de factorisation (SVD++).
L'approche proposée dans ce papier a montré un intérêt pour la recommandation de films en utilisant le jeu de données MovieLens combiné à une ontologie de films, peuplé par les données de IMDB. Nous avons fait le choix de travailler avec ce jeu de données car il est disponible et public et c est celui qui est le plus utilisé dans les expérimentations autour des systèmes de recommandation. Toutefois, nous avons conçu notre approche indépendamment du jeu de données que nous avions utilisé. Notre approche peut être utilisée comme une boite noire nécessitant de se connecter à une base de données des ratings disponibles, mais aussi à l'ontologie du domaine de l'application. Nous envisageons de tester notre approche sur des jeux de données provenant d'autres applications telles que la recommandation nutritionnelle ou de tourisme. Nous envisageons également d'intégrer dans notre approche la prise en compte des notes négatives (attributs non aimés par l'utilisateur).

Introduction
Traditionnellement, les données hydrométriques se présentent sous la forme de séries temporelles, représentant des mesures effectuées régulièrement par des stations : ces mesures peuvent concerner différents aspects comme les hauteurs et les débits de l'eau dans les cours d'eau, les quantités de précipitations, etc. Comme ces mesures sont souvent prélevées par un réseau distribué de capteurs, le problème des données manquantes est inévitable. Allant d'une simple valeur manquante à une longue plage de valeurs manquantes, les lacunes peuvent avoir des causes multiples : dysfonctionnement des capteurs, maintenance des stations de mesure, erreurs humaines, etc. (Harvey et al. (2010)).
Le réseau hydrométrique au Luxembourg fournit un bon cas d'utilisation. Il est constitué de différentes stations hydrométriques permettant de mesurer notamment les débits des cours d'eau. Les mesures sont ensuite fréquemment utilisées dans les modèles numériques de prévi-sion hydrologique ou pour calculer des statistiques sur les écoulements (e.g. temps de retour des crues ou des sécheresses).
En conséquence, lorsque certaines séries de mesures présentent beaucoup de lacunes (par exemple : les données de la station de HallerBach au Luxembourg, Figure 1), cela pose de nombreux problèmes et il est nécessaire d'apporter un soin très particulier à combler ces lacunes avec une bonne précision.
Afin de combler ces lacunes, les méthodes classiques d'analyse de données ont été appliquées dans le domaine hydrologique (Salas (1980)), et des travaux récents tentent de fournir des solutions toujours plus efficaces (Harvey et al. (2010); Mwale et al. (2012)  Kotsiantis (2013)). -Les réseaux de neurones artificiels ont récemment été utilisés, notamment via des perceptrons (Tfwala et al. (2013)) ou des cartes auto-adaptatives (Mwale et al. (2012)). -L'algorithme espérance-maximisation (EM) est souvent utilisé pour reconstruire des données manquantes (Van Hulse et Khoshgoftaar (2008)). -Enfin, différentes techniques de prédiction de séries temporelles peuvent être utilisées suivant les caractéristiques des séries (ARMA, ARIMA, etc.). Habituellement, les experts en hydrologie se servent de divers scripts pour corriger les données (R, MATLAB). Ainsi est-il important pour eux de pouvoir disposer d'un outil interactif pour à la fois intégrer les diverses sources de données, visualiser les séries temporelles, et enfin choisir le mode d'estimation de valeurs manquantes le plus adapté.
2 gapIT : un outil pour estimer les données manquantes 2.1 Technologie gapIT est une application développée en JAVA et basée sur le logiciel d'analyse et de traitement de données Cadral (Pinheiro et al. (2014) 
Inspection et caractérisation des valeurs manquantes
Les valeurs manquantes ne se corrigent pas de la même manière selon le contexte (taille des trous, saison durant laquelle les valeurs sont manquantes, probabilité qu'une crue soit en cours, etc.) (Gyau-Boakye et Schultz (1994)  Premièrement, la phase de sélection des stations de référence est critique, car elle permet à l'expert de choisir les séries temporelles qui serviront à compléter les séries temporelles lacunaires. Pour ce faire, l'outil propose plusieurs approches complémentaires :
-la sélection des stations les plus proches géographiquement ; -la sélection des stations se trouvant sur le même cours d'eau (en amont et/ou en aval) ; -la sélection des stations ayant les séries temporelles les plus similaires sur la période concernée ; la similarité est calculée en utilisant la déformation temporelle dynamique (Dynamic Time Warping) (Berndt et Clifford (1994)). Deuxièmement, l'outil propose diverses méthodes d'estimation : interpolation, régressions multiples, arbres de régressions, réseaux de neurones (perceptron multi-couches), algorithme des plus proches voisins, etc.
Troisièmement, l'utilisateur peut ensuite appliquer via le logiciel la méthode d'estimation sélectionnée en se basant sur les séries temporelles choisies pour évaluer les valeurs man-quantes. Or, il est important de déterminer la précision des estimations produites. Pour ce faire, des trous fictifs sont créés dans la fenêtre de temps proche du trou réel à remplir (par exemple : un trou avant, un trou après, élargissement du trou en cours d'examen ou à un endroit choisi par l'utilisateur). Ensuite, les mesures suivantes sont calculées : l'erreur absolue moyenne (MAE), la racine carrée de l'erreur quadratique moyenne (RMSE) et le coefficient Nash-Sutcliffe car c'est un indicateur très commun en hydrologie (Nash et Sutcliffe (1970)).
Pour finir, l'outil est capable de calculer automatiquement la configuration optimale (stations de référence et algorithme). Dans ce cas, l'utilisateur garde la possibilité de modifier la configuration à son gré de manière à obtenir un nouveau résultat plus proche de ses attentes.
3 Exemple : les débits des cours d'eau au Luxembourg gapIT a été utilisé pour estimer les débits d'écoulement manquants pour des stations sélec-tionnées au Luxembourg, sur la période allant du 1er janvier 2007 au 31 décembre 2013. Le jeu de données utilisé correspond à des mesures effectuées toutes les quinze minutes dans 24 stations. Afin de tester l'efficacité de l'outil, un ensemble de trous fictifs a été créé pour ces stations. Pour obtenir un ensemble représentatif, les trous générés sont de différentes tailles, se situent durant différentes saisons, etc. Ensuite, pour chacun de ces trous fictifs, toutes les techniques d'estimation proposées par l'outil ont été testées, et pour chacun des cas, les taux d'erreur ont été mesurés (MAE, RMSE, Nash-Sutcliffe).
Ainsi, nous avons constaté que les réseaux de neurones et les arbres de régression permettent d'obtenir les taux d'erreur les plus faibles. De plus, si l'on considère le meilleur ré-sultat concernant chaque trou, alors on voit que les taux d'erreur sont globalement très faibles. Cela signifie que pour ces cas, une estimation correcte est possible en utilisant les données pré-sentes (Figure 4). En revanche, dans un certain nombre de cas, les meilleurs taux d'erreurs sont élevés. Après analyse, il s'avère que pour les stations concernées, il n'existe pas suffisamment de stations assez proches, similaires ou dépendantes afin de créer une estimation assez précise. 

Contexte
La recrudescence des documents textuels disponibles sur le web incite de plus en plus travaux à l'exploitation de ces données de manières automatiques. Pour faire interagir ces données entre elles de manière efficace, il faut développer des moyens basés non seulement sur la ressemblance syntaxique mais également sur la correspondance sémantique.
GEOLSemantics est une entreprise qui propose une solution logicielle de traitement linguistique basée sur une analyse linguistique profonde. Le but est d'extraire automatiquement, d'un ensemble de textes, des connaissances structurées, localisées dans le temps et l'espace. Pour représenter ces connaissances, nous avons opté pour les technologies du web sémantique. Nous représentons nos extractions sous forme de triplets RDF et exploitons une ontologie pour apporter de la cohérence. Cette approche permet de relier les résultats de nos extractions aux connaissances du Linked Open Data, tels que Dbpedia et Geonames.
Lors de l'analyse linguistique, il arrive que l'information traitée contienne des imperfections. Dans notre travail, nous intéressons à l'incertitude. Notre première contribution porte sur une catégorisation de l'incertitude lors des différentes phases d'extraction. Notre seconde contribution se situe au niveau de la représentation de l'incertitude dans le graphe RDF.
2 Acquisition de l'information avec incertitude L'analyse des textes comporte plusieurs étapes distinctes allant du simple découpage du texte en mots à la représentation de son contenu. Parmi ces étapes, nous retrouvons : (i) l'analyse syntaxique, il s'agit de la mise en évidence des structures d'agencement des catégories grammaticales, afin d'en découvrir les relations formelles ou fonctionnelles. (ii) l'analyse sé-mantique, l'objectif principal de cette analyse est de déterminer le sens des mots des phrases. (iii) l'extraction de connaissances permet de mettre en évidence des entités nommées et des relations relatives à un concept particulier. Grâce à des déclencheurs qui indiquent qu'une relation relative à un concept peut être présente et extraite. Un déclencheur correspond généra-lement à un concept présent dans l'ontologie, ce qui permet de guider la règle d'extraction par la suite. (iv) la mise en cohérence permet de consolider les connaissances extraites notamment le regroupement des entités nommées, la résolution des dates relatives. Cette étape peut être Gestion de l'incertitude. suivi par un enrichissement à partir des données du Linked Open Data. Cependant, la fiabilité de l'information est très souvent remise en cause. Notre démarche est de considérer le cycle de vie de la connaissance depuis son acquisition jusqu'à son stockage dans la base de connaissances pour cela, nous identifions trois catégories : Pré-extraction de la connaissance : il s'agira lors de cette étape de considérer les modalités de publication de l'information à savoir : la date et le lieu de publication, la fiabilité accordée à la source, qu'il s'agisse de l'auteur ou de l'organisme de publication... Pendant l'extraction de la connaissance : l'incertitude pourra concerner aussi bien l'information véhiculée que la règle d'extraction à appliquer. Post-extraction de la connaissance : l'incertitude peut intervenir au niveau des règles de mise en cohérence ou bien au niveau du choix de la base de référence.
Le formalisme de représentation de connaissances choisie est le RDF tout en nous basons sur une ontologie développée pour prendre en compte les concepts relatifs à un domaine particulier. Notre approche consiste à considérer l'incertitude comme une connaissance à part entière telle que le décrit l'ontologie suivante. La classe Uncertainty, nous permet de modéliser l'incertitude. Elle est décrite par trois proprié-tés : weight : une propriété littérale pour quantifier l'incertitude identifiée, hasUncertainProp : une propriété objet qui servira d'intermédiaire entre le domaine initial de la propriété et la propriété en question, isUncertain : propriété objet qui aura pour co-domain le top-concept, cela veut dire que tout concept de l'ontologie pourra être visé par une incertitude. Cette ontologie est indépendante de tout domaine d'application. Dès lors, elle peut être ajoutée à toute autre ontologie voulant prendre en compte l'incertitude.
Conclusion et perspectives
Dans cet article nous nous intéressons au traitement de l'information incertaine dans le cadre d'une extraction de connaissances à partir de texte. Le traitement repose sur les technologies du web sémantique pour permettre de faire le lien avec les données du Linked Open Data. Notre démarche consiste à identifier les différentes situations où une incertitude remettant en cause la validité de l'information peut subsister. Nous proposons une ontologie pour modéliser l'information incertaine et la représenter au format RDF. Nous travaillons actuellement sur développement d'un ensemble de patterns pouvant faciliter l'interrogation du graphe RDF prenant en compte notre représentation de l'incertitude. Nous prévoyons par la suite de développer un raisonneur basé sur le formalisme des logiques possibilistes afin de permettre l'inférence sur les données incertaines.
Summary
The knowledge representation area needs some methods that allow to detect and handle uncertainty. Indeed, a lot of text hold information whose the veracity can be called into question. These information should be managed efficiently in order to represent the knowledge in an explicit way. As first step, we have identified the different forms of uncertainty during a knowledge extraction process, then we have introduce an RDF representation for these kind of knowledge based on an ontologie that we developped for this issue.

Introduction
L'utilisation d'ontologies s'est montrée très efficace dans bien des domaines. La taille et la dynamique des domaines considérés demande toutefois l'exploitation combinée de plusieurs ontologies, d'où la nécessité d'établir des correspondances sémantiques, ou mappings (Euzenat et Shvaiko, 2007), entre ontologies. Ainsi, la qualité des résultats produits par les systèmes utilisant des ontologies dépend de la validité des mappings entre ontologies, ce qui oblige des experts du domaine à réviser leur définition lorsque les ontologies évoluent. Si cette maintenance peut être effectuée manuellement sur de petits ensembles de mappings, une approche plus automatique est nécessaire lorsqu'ils sont volumineux, comme dans le domaine médical. L'existence de mappings erronés est souvent dûe à l'évolution des ontologies, les erreurs d'alignement mises à part (Dos . Il est alors fondamental de comprendre l'évolution des ontologies pour pouvoir agir sur les mappings afin de garantir leur validité. Ce faisant, nous avons proposé un ensemble de patrons de changement permettant de caractériser l'évolution des concepts d'une ontologie en analysant les changements dans la définition des concepts . Nous avons également observé sur des jeux de données réelles le comportement des mappings dans le temps, ce qui nous a permis d'identifier un ensemble d'actions d'adaptation pouvant s'appliquer aux mappings pour les faire évoluer (Dos . L'objet de cet article formalise, sous forme d'heuristiques, le lien entre les patrons de changement et les actions d'adaptation pour faire évoluer les mappings lorsque les ontologies liées évoluent. Aprés avoir introduit les concepts de notre approche (Section 2), en particulier les patrons de changement et les actions d'adaptation, nous présentons les heuristiques proposées (Section 3) et le cadre expérimental emprunté au domaine biomédical pour les évaluer (Section 4) et les discuter par rapport à l'existant (Section 5) avant de conclure (Section 6).
Préliminaires
Nous présentons ici les notions et notations utilisées pour définir notre approche. Soit O t , semT ype) relie deux concepts c s ? C X et c t ? C Y par la relation sémantique semT ype, telle que semT ype ? {?, <, >, ?} (cf. tableau 1). 
TAB. 1 -Notations pour la formalisation des heuristiques
Patrons de changement caractérisant l'évolution d'ontologies
Nos travaux ont montré que l'évolution des ontologies rend nécessaire d'adapter les mappings. Nous pensons qu'une compréhension précise de cette évolution va nous renseigner sur la façon d'adapter les mappings au cours du temps. Pour caractériser l'évolution des ontologies, nous avons proposé un ensemble de patrons de changement . Contrairement à ceux de la littérature (Djedidi et Aufaure, 2009), (Javed et al., 2013), (Gröner et al., 2010), nos patrons considèrent les changements syntaxiques et sémantiques au niveau des attributs des concepts. Ce choix a été motivé par les résultats expérimentaux obtenus montrant que la définition des mappings repose sur certaines valeurs d'attributs (Dos .
Les Une Copie Totale caractérise le changement à travers lequel une valeur d'attribut devient également la valeur d'un attribut d'un autre concept. Par exemple, un attribut a 1 d'un concept c 1 a pour valeur "portal systemic encephalopathy" au temps j. Au temps j + 1, a 1 a la même valeur, mais un attribut a 2 d'un concept c 2 aura également cette valeur.
Une Copie Partielle est une copie d'une partie de la valeur d'un attribut. Un attribut a 1 d'un concept c 1 a la valeur "familial hyperchylomicromenia" au temps j. Au temps j + 1, a 1 garde cette valeur, mais un attribut a 2 aura "familial chylomicromenia" comme nouvelle valeur.
Un Transfert Total correspond au transfert de la totalité de la valeur d'un attribut à un autre attribut. Contrairement au cas TC, la valeur originale de l'attribut n'est pas conservée.
Un Transfert Partiel définit le transfert d'un partie de la valeur d'un attribut. Par exemple, un attribut a 1 peut valoir "eye swelling" au temps j. Au temps j + 1, cette valeur sera supprimée partiellement de a 1 mais un attribut a 2 vaudra "head swelling" (i.e., le terme "swelling" est déplacé de a 1 vers a 2 entre j et j + 1).
Les Patrons de changement sémantiques (SCP) s'intéressent à l'évolution de la séman-tique de la valeur des attributs au cours du temps. Nos observations ont montré qu'à travers leurs évolutions successives, les concepts pouvaient devenir plus généraux, plus spécifiques ou rester équivalents, modifiant ainsi la relation sémantique des mappings. Les 4 SCP que nous proposons sont : Equivalent (EQV), Plus Spécifique (MSP), Moins Spécifique (LSP) et Recouvrement Partiel (PTM).
Equivalent stipule que les changements syntaxiques au niveau de la valeur de l'attribut ne modifient pas sa sémantique. Par exemple, la valeur d'un attribut peut passer de "Diabetes type 1" à "Diabetes type I" sans en affecter le sens.
Plus Spécifique identifie un changement rendant un concept plus spécifique que sa nouvelle version. Le changement menant de "kappa light chain disease" à "kappa chain disease" rend le premier plus spécifique du fait de la suppression du qualificatif "light". Moins Spécifique décrit l'effet inverse.
Recouvrement Partiel identifie un changement au niveau de la sémantique ne pouvant être caractérisé par les autres SCP. Considérons la valeur originale "focal atelectasis" et son évolu-tion "helical atelectasis". Ces deux valeurs font toutes deux référence à la notion de "atelectasis", mais ne peuvent être déclarées ni équivalentes, ni plus ou moins spécifiques. ModSemTypeM(m st , semT ype) consiste en la modification de semT ype (la relation sé-mantique) à cause des modifications sur les concepts sources (c s ) et/ou cibles (c t ).
Les actions d'adaptation de mappings
NoAction(m st ) est appliquée lorsque les modifications sur les concepts sources et/ou cibles n'entraînent pas de changements sur la sémantique des mappings.
Heuristiques d'adaptation des mappings
Nous présentons dans cette section des heuristiques indiquant sous quelles conditions adapter les mappings afin qu'ils restent valides dans le temps. Ces heuristiques ont été établies expérimentalement à partir de l'observation de l'évolution des mappings entre des jeux de données réelles et de l'analyse de l'impact des patrons de changements sur leur évolution. Des heuristiques ont été définies pour chaque type d'adaptation.
Heuristiques pour les mappings de type Move et Derive
Soit Cand l'ensemble des concepts dits candidats regroupant les concepts du contexte de c s en j + 1 pour lesquels il existe un changement de type LCP entre un de leurs attributs et un attribut de c s . Soit la fonction topA(c s , c t , n) retournant les n attributs expliquant le mieux le mapping entre c s et c t . Soit la fonction SLCP (a 1 , a 2 ) retournant VRAI s'il existe un changement de type LCP entre a 1 et a 2 et FAUX sinon. Soit la fonction SCP (a 1 , a 2 ) retournant les changements de type SCP entre a 1 et a 2 ou ? si aucun changement de type SCP n'a été identifié.
MoveM. Expérimentalement, nous avons observé que l'adaptation d'un mapping de type MoveM correspondait à l'existence de changements de type LCP entre attributs. Plus précisé-ment, nous avons observé que lorsque l'adaptation du mapping est de type MoveM, il n'existe qu'un seul attribut de c s au temps j expliquant le mapping avec un changement de type LCP avec un attribut du contexte de c s au temps j + 1. De ce fait, nous appliquons l'adaptation de type MoveM lorsqu'il n'existe qu'un seul concept candidat avec un changement de type LCP  Figure 1). Intuitivement, cela signifie que le mapping suit l'évolution des attributs qui l'expliquent.
Soit les concepts '128829008' "Acute myeloid leukemia, 11q23 abnormalities (morphologic abnormality)" et 'C6924' "Acute Myeloid Leukemia with 11q23 MLL Abnormalities" issus de SNOMED CT et de NCI. Deux changements de type LCP se sont produits lors de l'évolu-tion de 'C6924'. Un transfert total s'est produit sur l'attribut qui expliquait le mieux le mapping ("Acute Myeloid Leukemia with 11q23 Abnormalities"). Une copie totale a eu lieu pour un autre attribut qui expliquait aussi le mapping. Ces deux changements concernent le même concept candidat 'C82403' "Acute Myeloid Leukemia with t 9 11 p22 q23 MLLT3-MLL" dans la nouvelle version de l'ontologie, mais deux attributs différents expliquant le mapping. Suite à l'évolution de 'C6924', le mapping relie dorénavant les concepts '128829008' et 'C82403'.
DeriveM. De façon similaire à l'action MoveM, DeriveM est appliquée lorsque plusieurs changements entre attributs de type LCP sont reconnus suite à une évolution. Le mapping original est préservé et c s existe dans la nouvelle version de l'ontologie. Le fait que plusieurs changements de type LCP concernent les attributs pertinents de c s et qu'il existe donc plusieurs concepts candidats conduit à la création de nouveaux mappings entre ces candidats et c t (cf. Figure 2). Formellement, nous définissons cette heuristique de la façon suivante :
En guise d'illustration, considérons le mapping 'plus spécifique que (<) entre '41452004' -"Uterus acollis (disorder)" dans SNOMED CT et '752.3' -"Other anomalies of uterus" dans ICD-9-CM. L'analyse de l'évolution de 752.3 permet d'observer plusieurs changements de type LCP concernant des concepts candidats différents et également plusieurs changements de type SCP. Par example, il y a une copie totale (TC) de l'attribut "Other anomalies of uterus" expliquant le mapping vers un attribut de '752.33' qui fait apparaître une équivalence. En plus, l'attribut "Bicornuate uterus" est totalement transféré (TT) dans '752.34' faisant également apparaître une relation d'équivalence. L'action d'adaptation à réaliser, selon notre heuristique, est DeriveM en l'appliquant aux deux concepts candidats concernés par les changements de type LCP et en gardant la même relation sémantique que la relation du mapping original.
3.2 Heuristiques associées à la modification de relation sémantique L'application de l'action ModSTR dépend des changements de types SCP trouvés entre attributs. Nous avons identifié deux scénarios différents. Le premier concerne la modification de la relation du mapping original m 0 st alors que c s ne change pas (cf. Equation 3). Le second scénario concerne la modification de la relation du mapping original suite à un MoveM ou DeriveM (cf. Equation 4). Dans le premier cas, la nouvelle relation sémantique lie le concept source (en terme de contenu) au temps j + 1 et le concept cible alors que dans le second cas, c s est remplacé par un concept candidat (un concept appartenant au contexte de c s ). Le type de la relation sémantique après évolution est obtenu en combinant le type de la relation du mapping original semT ype et le type d'un changement de type SCP détecté entre attributs. Soit la fonction getSemType qui fournit la relation sémantique obtenue en combinant le type d'un mapping original semType (x) avec des relations identifiées par des patrons de type SCP, SCPs (y). Par example, si la relation sémantique entre c 
Voici les heuristiques associées au premier et deuxième scénario :
st ? {?, <, >, ?}, semT
ct ? {?, <, >, ?}, semT , nous sélectionnons les concepts candidats au temps j + 1 à partir du contexte de c s au temps j. Les expérimentations à partir desquelles cette heuristique a été proposée ont montré que la similarité entre les attributs expliquant un mapping et les attributs des concepts du contexte de c s après évolution était très faible lorsque le mapping était supprimé (Dos . Ceci a conduit à introduire une condition portant sur la similarité dans l'heuristique proposée.
No action. L'heuristique pour NoAction considère que des patrons de type LCP et SCP ne sont pas observables (cf. Equation 6). Elle s'applique dans des cas où les changements portant sur un concept lié par un mapping n'ont pas d'effet sur les attributs expliquant ce mapping, ou lorsque la similarité avec de nouveaux attributs du contexte est faible (Dos .
Evaluation
Cette section porte sur l'évaluation des heuristiques proposées. Nous étudions si les changements identifés dans une ontologie conduisent à des adaptations correctes (i.e. MAAs) des mappings affectés. Cette évaluation a été réalisée sur plusieurs versions d'ontologies biomédi-cales (SNOMED CT et ICD-9-CM) et plusieurs versions de mappings associés.
Protocole d'expérimentation
4. Deux types de mesure, représentées respectivement par and sont proposées pour évaluer de manière rigoureuse les actions proposées (cf. Résultats obtenus dans le tableau 2). Cette distinction est bien appropriée à l'évaluation des actions MoveM, DeriveM and ModSTR. Le symbole exprime la précision, le rappel et la F-mesure lorsque les actions proposées sont correctes par rapport aux actions attendues, en plus du concept candidat ou de la relation sémantique. Le symbole correspond aux cas où seul le type d'action est correct, le concept candidat ou la relation sont erronés (par exemple, MoveM ou DeriveM est proposé mais pas avec le bon concept candidat à j+1). La mesure est plus contrainte, elle pourrait conduire à des valeurs plus faibles. Ainsi, si nous observons un MoveM avec un concept c obs du contexte et si notre mécanisme propose un MoveM avec le même concept c obs , nous choisissons de mesurer la précison, le rappel et la F-mesure de (action d'adaptation, concept lié et relation sémantique exacts). En revanche, si seule l'action MoveM est correcte, nous faisons le calcul correspondant à Nous procédons de la même façon pour ModSTR, en considérant en plus de l'action d'adaptation du mapping, le type de la relation sémantique.
5. Enfin, nous effectuons un calcul global qui calcule la précision, le rappel et la F-mesure pour chaque jeu de données, indépendamment des types d'action d'adaptation.
Résultats et discussion
Les résultats de l'adaptation des mappings basés sur nos heuristiques sont présentés dans la Mapping Move. Pour ce type d'action, les résultats varient selon que l'évolution concerne ICD-9-CM ou SNOMED CT uniquement pour la précision. Ainsi, nos heuristiques peuvent générer des adaptations plus ou moins correctes selon les jeux de données. Les différences de résultats entre et étant très faibles, on remarque que, la plupart du temps, lorsque notre système propose un MoveM, c'est avec un bon concept candidat du contexte.
Mapping Derive. Les résultats obtenus sont plus élevés que ceux de Move pour SNOMED CT. L'action DeriveM est complexe car l'action propose plusieurs DeriveM par mapping. Aucune action DeriveM n'a été observée pour ICD-9-CM. Là encore, les conditions d'application des heuristiques sont différentes selon les jeux de données. L'explication peut provenir du processus de maintenance, de matching ou de la granularité des ontologies alignées.
Modification de la relation sémantique. L'évaluation de cette heuristique a été difficile car les mappings dont nous disposions contenaient peu de relations différentes (la plupart était des équivalences). Néanmoins, les résultats montrent que les nouvelles relations proposées quand une action de type ModSemTypeM est détectée, sont pertinentes (surtout pour SNOMED CT). Ce type d'adaptation est généralement combiné avec un MoveM ou un DeriveM ce qui oblige à choisir la bonne action avant de changer la relation. Parfois, les changements dans l'ontologie ne sont pas l'unique raison pour changer la relation. Cela influe négativement sur nos résultats. Enfin, considérer la nouvelle version des mappings comme référence peut aussi poser problème. C'est un ensemble de mappings qui a subi des évolutions mais ce n'est pas à proprement parler un jeu de données de référence. Une relation sémantique proposée peut être considérée comme fausse par notre processus d'évaluation alors qu'elle peut être correcte d'un point de vue sémantique. L'intervention d'un expert serait nécessaire pour y remédier.
Mapping Remove. La précision et le rappel sont relativement bons. Ils ne varient que légèrement selon l'ontologie. La F-mesure minimale est de 0.54. La prise en compte du retrait de l'attribut expliquant le mieux le mapping dans l'heuristique semble être une bonne décision.
Application d'aucune action. Ce type d'adaptation couvre le plus grand nombre de cas que ce soit en valeur absolue (5892 attendus dans SNOMED CT et 3139 dans ICD-9-CM) ou relative. Les résultats montrent une grande efficacité des heuristiques. On constate que, bien que les concepts source ou cible évoluent, si les attributs expliquant les mappings qui les concernent restent inchangés, ces mappings n'évolueront pas non plus. Les très bons ré-sultats obtenus avec l'application de cette heuristique permettent aux experts de se concentrer sur l'étude d'une toute petite partie des mappings (en comparaison à l'ensemble initial), ces mappings étant plus difficiles à adapter et pouvant nécessiter une intervention humaine.
Résultats globaux. Nous avons analysé les résultats en combinant tous les types d'adaptation pour avoir une idée de la qualité du processus général d'adaptation. La F-mesure est élevée (un min de 0.85 dans ICD-9-CM). L'action d'adaptation NoAction influe beaucoup sur ces résultats. Néanmoins les résultats sont globalement acceptables et prometteurs même si certaines actions sont difficiles à appliquer. Ceci montre que les conditions d'application des heuristiques que nous proposons sont adaptées. L'amélioration des résultats liés à certaines actions nécessiterait de rechercher quels autres éléments influencent les types d'adaptation et d'étudier comment les prendre en compte dans les heuristiques.
Etat de l'art
Nous distinguons trois principaux types d'approches pour la maintenance des mappings due à l'évolution d'ontologies. La première consiste à identifier et réparer les mappings invalides (Meilicke et al., 2008)  (Ivanova et Lambrix, 2013). Ces approches effectuent des raisonnements logiques pour identifier les mappings produisant des incohérences logiques. Ce mécanisme ne s'applique que sur des ontologies formelles, qui n'existent pas toujours.
Les travaux s'inscrivant dans la deuxième catégorie reposent sur des techniques de ré-alignement total ou partiel d'ontologies. Si le premier type d'approche ne considère aucune information provenant de l'évolution, le second recoupe l'ensemble des concepts modifiés au niveau des ontologies avec ceux impliqués dans des mappings pour ne réaligner que ce sousensemble de concepts (Khattak et al., 2012). Ces approches sont coûteuses en temps de calcul et de validation lorsque les ontologies sont volumineuses (Shvaiko et Euzenat, 2013).
La troisième catégorie, inspirée du monde des bases de données (Velegrakis et al., 2003), fait référence à des approches qui profitent au maximum des informations provenant de l'évo-lution d'ontologies pour éviter de les réaligner totalement. Tang et Tang (2010) ont proposé une méthode visant à trouver l'impact minimal de la propagation des changements au niveau d'une ontologie. Martins et Silva (2009) suggèrent d'adapter les mappings en procédant de la même façon que lors de la modification des concepts de l'ontologie qui évolue. Cependant, dans leur approche, les mappings ne sont adaptés que lorsque les concepts sont supprimés. L'originalité de nos travaux, par rapport à cet état de l'art, réside dans : (i) l'importance donnée aux modifications des ontologies sous-jacentes, (ii) à leur caractérisation et (iii) à la prise en compte du changement au niveau de la relation sémantique des mappings, ce dernier point étant souvent négligé dans les approches existantes qui ne considèrent qu'un seul type de relation (Dos Reis et al., 2012).
Conclusion
Dans cet article, nous avons proposé un ensemble d'heuristiques guidant l'adaptation des mappings entre ontologies. Ces dernières formalisent le lien existant entre les changements identifiés au niveau des éléments ontologiques et les actions d'adaptation à appliquer sur les composants des mappings pour préserver leur validité. L'approche décrite a été validée expé-rimentalement sur des données réelles du monde biomédical. Le manque de données de ré-férence nécessite l'implication d'experts du domaine mais le travail réalisé permet de réduire considérablement le temps nécessaire aux experts pour valider les adaptations proposées.

Introduction
Qui n'a pas dit, un jour, en écoutant la radio : "Mais ça ressemble à Supertramp ou à une musique de Chopin ou à une musique baroque" ? Sur la base d'un court morceau écouté on peut en effet identifier directement l'auteur ou le placer dans une catégorie même si on ne connait pas forcément le morceau. Si c'est un chanteur, on le reconnait facilement au timbre de sa voix, pour un morceau de musique classique, l'interprétation peut varier et on détecte plutôt la ligne musicale. Pour les documents textuels, ce problème d'authentification d'auteur présumé est récurrent, et la fouille de texte peut s'avérer très utile. Ainsi, par exemple pour authentifier une élégie de Shakespeare en 1995 1 des techniques telles que le comptage exclusif des mots et la prise en compte de mots rares ont été employés avec succès (Foster (1996)). Le champ littéraire n'est cependant pas le seul concerné. Le problème d'authentification d'un auteur apparait aussi dans bien d'autres applications, comme dans le domaine juridique par exemple pour l'authentification d'un testament ou dans le cadre des investigations anticriminelles ou antiterroristes pour identifier la provenance d'une demande de rançon ou de posts émis sur des forums de discussion du Dark Web (Abbasi et Chen (2005)). Le marketing peut également être intéressé par le profiling des auteurs des blogs ou des commentaires sur le Web.
Dans le cas de textes écrits, on peut plus généralement distinguer trois variétés de problèmes liés à la détermination d'un auteur inconnu :
-l'extraction de profil (Author Profiling) : il s'agit d'indiquer à partir d'un texte des éléments du profil de son auteur comme, par exemple, la tranche d'âge et le genre ( Rangel et al. (2013)) ou l'appartenance à une catégorie particulière comme celle des criminels potentiels ) -la reconnaissance de l'auteur (Author Verification ou Author Recognition) : il s'agit de vérifier parmi une liste des auteurs possibles lequel est le bon -l'identification d'un auteur : il s'agit de décider si un texte donné a été écrit par l'auteur d'un autre groupe de documents Ainsi, dans ces trois problèmes, on s'interroge sur l'auteur d'un document écrit, et dans ces trois cas pour répondre, il est nécessaire de représenter de façon appropriée le document à explorer et de pouvoir le comparer avec d'autres. Toutefois, nous pensons qu'il est illusoire de rechercher une empreinte d'un auteur sur un texte qu'on pourrait comparer avec des empreintes extraites d'autres textes et qui serait unique au même titre qu'une empreinte digitale. Nous pensons qu'il faut utiliser divers espaces de représentation pour les textes à analyser selon la langue d'origine ou encore le genre ou la qualité du document. Dans cet article, où nous nous intéressons plus spécifiquement à des problèmes d'identification d'auteurs à partir de documents rédigés dans diverses langues et de différents types (textes littéraires courts ou longs, articles de presse ou publications, blogs) nous avons exploré différents modes de représentation des documents. Nous avons ensuite proposé de formaliser l'identification d'auteurs comme un problème de classement que nous avons résolu de trois façons : à l'aide d'un algorithme original de comptage de similarité (DCM), puis avec deux autres méthodes qui exploitent ce comptage, par une technique de vote (DCM-voting), par apprentissage automatique (DCM-classifier).
Notre article est organisé de la manière suivante : après la section 2 consacrée aux travaux relatifs à l'identification d'auteurs, nous définissons plus formellement le problème dans la section 3, puis nous décrivons les trois méthodes proposées pour le résoudre dans la section 4. La section 5 présentera les résultats des expériences réalisées afin d'évaluer l'intérêt de ces approches et de les comparer à celles de l'état de l'art. Des conclusions seront présentées dans la dernière section.
Etat de l'art
L'identification d'auteur peut être définie comme un problème de classification de textes : Etant donné un ensemble, grand ou réduit à un seul élément, de documents d'un même auteur, il faut déterminer si un nouveau document a été écrit par le même auteur que les autres ".
Il s'agit donc d'un problème de classement supervisé binaire dont la réponse attendue est binaire ("oui" ou "non") ou une probabilité d'appartenance à l'ensemble de documents fournis. Toutefois, une des spécificités de ce problème de classement est que seuls des exemples d'une des deux classes sont donnés : les documents rédigés par l'auteur, mais la seconde classe n'est pas explicitée. De plus, parfois le nombre d'exemples positifs est réduit à un seul document, ce qui rend la tâche particulièrement difficile.
Pour pallier l'absence d'exemples négatifs, on peut essayer d'en produire. C'est la voie explorée par différents auteurs parmi lesquels figurent (Seidman (2013)) qui construisent une classe d'"imposteurs" choisis aléatoirement sur la base des dix mots les plus fréquents figurant dans les documents disponibles pour remplir la classe du "non". D'autres auteurs, comme Zhang et al. (2014) et Halvani et al. (2013), transforment ce problème de classification à deux classes en un problème avec plusieurs classes, soit en rajoutant des classes extérieures, soit en transformant la classes initiale en plusieurs. Les même auteurs (Halvani et al. (2013)) augmentent la taille de la classe des documents connus quand celle-ci est réduite à un seul document. Ainsi, ces approches permettent de revenir à un problème classique de classement supervisé mais, lors de la construction des exemples négatifs, elles sont confrontées au risque de choisir des documents trop proches ou trop éloignés des documents déjà fournis.
Outre la question des données disponibles pour résoudre le problème, l'identification d'auteurs est ensuite confrontée à deux autres questions classiques en fouille de texte : comment représenter les documents et, une fois l'espace de représentation choisi, quelles méthodes appliquer pour résoudre le problème de classement ?
Comme nous l'avons déjà remarqué, l'identification d'auteurs peut être réalisée à partir de documents très différents : méls (de Vel et al. (2001); Chaurasia et Kumar (2010)), programmes, parties des oeuvres littéraires ou parties des documents de la vie de tous les jours, texte plat (Zhang et al. (2014)), extraits de chat (Inches et Crestani (2013)) ou séquences de commandes Unix (Szymanski et Zhang (2004)). Le choix des caractéristiques examinées, on parle des caractéristiques stylométriques, dépend du type de document, parfois de la langue et aussi de la qualité du texte initial. Les caractéristiques dites "spécifiques" aux applications portent plutôt sur le comptage des tabulations et autres séparateurs ou l'analyse des caracté-ristiques spécifiques, telles que la position des parenthèses et des crochets fermants pour les programmes, et des lignes vides pour les méls. Les caractéristiques sémantiques sont prises en compte plutôt pour des textes issus du web (forum et chat), comme, par exemple, l'usage des abréviations ou des mots démonstratifs fréquents ("well") ou des transcriptions concentrées des expressions orales ("sse u"). On peut également considérer des caractéristiques syntaxiques comme des fautes d'orthographe ou les abréviations. Si on se place dans un cadre générique (authentification d'auteur dans diverses langues et dans divers genres), on utilise plutôt des caractéristiques de type caractère ou mot, ou suites de caractères ou de mots (n-grams) (Chaurasia et Kumar (2010); Szymanski et Zhang (2004)). On peut aussi avoir recours à un étiqueteur lexical et syntaxique mais son usage augmente considérablement le temps de traitement (Juola et Stamatos (2013)) et les résultats vont dépendre de sa qualité (Vilariño et al. (2013)). Lorsque le choix de ces caractéristiques est fait, les documents peuvent être transformés en vecteurs en utilisant, le plus souvent, tf-idf comme pondération ou uniquement la fréquence. Ensuite, selon la représentation du texte adoptée, on peut comparer les documents à l'aide de fonctions "classiques" de similarité telles que le cosinus, la corrélation, moins souvent des mesures de compression de données comme la Fast Compression Distance (Cerra et al. (2014)) ou la Common N-Gram dissimilarity (Layton et al. (2013)).
Pour ce qui concerne la résolution du problème de classement lui-même, on peut appliquer des méthodes "classiques" telles que les k plus proches voisins (k-NN) (Zhang et al. (2014); Ghaeini (2013); Halvani et al. (2013)) ou les SVM (Vilariño et al. (2013)). Certains auteurs (Dam (2013); Layton et al. (2013) ;Jankowska et Milios (2013)) proposent des méthodes basées sur le choix d'un seuil ou d'un vote et des formules de calcul de l'éloignement entre le document d'auteur inconnu et les autres. Les différences entre ces approches résident dans la phase de prétraitement, dans l'extraction des caractéristiques et dans le choix du seuil et de la fonction de dissimilarité.
Par rapport aux travaux antérieurs, notre contribution se situe dans un cadre plus large avec l'objectif de proposer une méthodologie générique, applicable à des collections très différentes tant par le genre des documents que par le langage. Ceci nécessite la mise en place d'une approche permettant de choisir automatiquement la représentation textuelle la mieux adaptée pour un corpus donné.
Définition du problème et représentation des documents
Le problème d'identification d'auteur peut être défini de la façon suivante. Etant donné un corpus composé de documents d'un même type (mel, blog, roman, code, etc.) écrits dans un même langage (anglais, français, Java, etc.) on dispose pour chaque problème p d'un ou plusieurs documents A p du corpus qui ont été rédigés par un même auteur et d'un document u p dont l'auteur est inconnu. L'objectif est de déterminer si u p a été écrit ou non par le même auteur que les documents de A p . Si on dispose d'un échantillon d'apprentissage, autrement dit d'un ensemble de problèmes P tel que pour chaque problème p ? P on sait si le document inconnu u p a été rédigé ou non par le même auteur que les documents associés A p , alors on peut formaliser le problème comme un problème de classement supervisé binaire et le résoudre à l'aide de méthodes d'apprentissage automatique. La difficulté consiste alors à déterminer d'une part un ou des espaces de représentation des documents appropriés et d'autre part à construire à partir de ces représentations des facteurs descriptifs des documents inconnus permettant de prédire efficacement si chaque document u p a été ou non produit par le même auteur que les documents de A p qui lui sont associés. Parmi les modèles les plus connus et les plus utilisés pour représenter des documents figure le modèle tf-idf introduit par Salton et al. (1975). Un document d est représenté par un vecteur (w 1 , . . . , w j , . . . , w |T | ) tel que le poids w j du terme t j dans d correspond au produit de la fréquence tf j du terme t j dans d par le pouvoir discriminant idf (j) de t j . Ce modèle est très efficace notamment pour identifier des termes (caractères, mots ou séquences de mots ou caractères correspondant à des n-grams) qui sont fréquents dans un document et rares dans les autres. Mais, comme nous l'avons souligné en introduction, d'autres caractéristiques peuvent être prises en compte pour représenter les documents. De plus, nous pensons qu'il n'existe pas un modèle de représentation universel adapté à tous les documents mais que le choix de cet espace de représentation doit dépendre du type de documents et du langage.
Ceci nous a conduit à considérer d'autres espaces de représentation indiqués dans le tableau 1. Outre le modèle tf-idf défini à partir des mots, avec élimination des mots outils à l'aide d'un dictionnaire (R5) ou en considérant leur fréquence (R4), des suites de mots ou de caractères (R1, R2, R3), nous avons introduit trois autres modèles de représentation (R6, R7 et R8) visant à caractériser le style d'écriture du document. Dans le modèle R6, la moyenne et l'écart type du nombre de mots par phrase sont associés au document. Le modèle R7 attribue à chaque document une mesure de diversité du vocabulaire définie comme le nombre de mots différents employés divisé par le nombre total d'occurrences de mots (i.e. la longueur du document). Le modèle R8 correspond au modèle de Salton dans lequel on considère les caractères de ponctuation au lieu des termes (mot ou caractère n-grams). Enfin, le modèle R678 correspond à la concaténation des trois modèles précédents : chaque document est représenté par un vecteur indiquant la moyenne par phrase des caractères de ponctuation " :" , " ;" , ",", la moyenne et l'écart type du nombre de mots par phrase et la diversité du vocabulaire. Ponctuation nombre moyen de signe de ponctuation par phrase caractères pris en compte : "," " ;" " :" "(" ")" " !" " ?" R678 Concaténation R6 + R7 + R8 TAB. 1: La liste de espaces de représentation considérés
DCM, DCM-voting et DCM-classifier
Ayant choisi un des espaces de représentation, on peut comparer les documents deux à deux à l'aide de mesures de similarité comme le cosinus et le coefficient de corrélation ou avec la distance euclidienne et appliquer une des trois méthodes (DCM, DCM-voting, DCMClassifier) que nous avons proposées pour résoudre le problème d'identification d'auteur. La première méthode DCM permet de traiter directement le problème d'identification p ? P , en considérant uniquement les similarités entre les vecteurs décrivant les documents suivant un des espaces. Les deux autres méthodes, basées sur DCM, permettent de combiner diffé-rentes représentations des documents, par une méthode de vote dans le cas de DCM-voting, à l'aide d'une méthode d'apprentissage supervisée nécessitant la construction d'attributs prédic-tifs pour DCM-classifier. Ces différentes méthodes sont décrites dans les sections suivantes.
Méthodes de comptage de similarités : DCM et DCM-voting
Etant donné un problème p ? P défini par un ensemble A p de documents rédigés par un même auteur et un document u p dont l'auteur est inconnu, représentés dans un même espace, et un seuil de décision ? , la méthode DCM, décrite par l'algorithme suivant, fournit en sortie la valeur True si l'auteur de u p est le même que celui des documents de A p ou la valeur False dans le cas contraire. Cette méthode exploite les similarités (ou distances) entre tous les documents disponibles. Elle consiste à assigner le document u p au même auteur que les documents de A p si la plupart d'entre eux sont plus proches de u p qu'ils ne le sont des autres documents de A p . Plus précisément la plus grande similarité de chaque document d x ? A p aux autres documents de A p est calculée puis comparée à la similarité de d x à u p . Si la première est inférieure à la seconde, un compteur est incrémenté. Après examen de tous les documents de A p (fin de la boucle for extérieure), il comptabilise la proportion de documents de A p qui sont les plus proches de u p que des autres de A p et si cette proportion est supérieure au seuil fixé ?, alors l'auteur du document inconnu u p est le même que celui des autres documents. Cette méthode présente l'avantage d'être simple et rapide à mettre en oeuvre et elle permet de traiter un problème d'identification p ? P indépendamment des autres. En revanche, elle n'exploite qu'un seul mode de représentation des documents.
Pour pallier ce défaut, on peut avoir recours à une méthode de vote DCM-voting consistant simplement à appliquer la méthode DCM en considérant plusieurs espaces de représentation des documents, de préférence en nombre impair, puis à affecter le document inconnu à la classe majoritairement retournée par les différentes exécutions. Cependant, comme nous l'avons souligné précédemment, tous les espaces de représentation ne sont pas équivalents et il serait souhaitable de pouvoir ajuster leur poids dans la décision finale ; ce qui est difficile à faire en pratique même pour un expert ; de même que le choix du seuil ?. Pour toutes ces raisons, nous proposons une autre méthode plus générale permettant d'exploiter simultanément plusieurs modes de représentation des documents et d'ajuster automatiquement, par apprentissage automatique, leur importance dans l'identification de la classe des documents inconnus.
La méthode DCM-classifier
Dans le cadre de l'apprentissage supervisé, on suppose que pour un sous-ensemble P A de problèmes de P , on sait en fait si les documents inconnus u p ont ou non été produits par le même auteur que les documents qui lui sont associés i.e. on dispose en plus de la classe class(u p ) des documents inconnus à savoir même auteur ou auteur différent. Ce sousensemble P A est décomposé en un échantillon d'apprentissage P a utilisé pour construire un modèle de décision et un échantillon test employé pour l'évaluer. La phase d'apprentissage permet de mettre en relation des facteurs descriptifs (ou attributs) des documents avec leur classe de façon à pouvoir ensuite identifier l'auteur d'un nouveau document dont la classe est inconnue uniquement à partir de ces facteurs descriptifs. Il est clair que la qualité du modèle dépend largement du pouvoir prédictif de ces facteurs que nous proposons de définir de la façon suivante.
Pour chaque espace de représentation R v , v ? {1, .., V }, chaque document u p est décrit par deux attributs count v (u p ) et mean v (u p ) respectivement définis à l'aide d'une mesure de similarité s par :
Un dernier attribut T OT count (u p ), basé sur tous les espaces de représentation est égale-ment calculé afin d'avoir une description plus synthétique. Il est défini par :
Ainsi lors de l'apprentissage, on considère les documents u p de chaque problème p de P a décrit par ces attributs descriptifs prédictifs et par leur classe réelle (
Compte tenu du caractère numérique de ces attributs descriptifs, plusieurs méthodes d'apprentissage supervisé peuvent alors être employées ( SVM, etc). Dans le cadre des expérimentations, nous avons privilégié les arbres de décision qui ont l'avantage d'intégrer une phase de sélection des attributs en fonction de leur pouvoir prédictif ; ce qui permet de favoriser selon la famille de problèmes considérés tel ou tel espace de représentation. De plus, ils permettent aussi d'ajuster automatiquement les paramètres du modèle. Les résultats obtenus sur chaque corpus des collections ev2013, app2014 et ev2014 ont été évalués à l'aide des indicateurs habituels de précision, de rappel et avec la mesure F 1.
Expérimentations et résultats
Le taux d'erreur indiqué par la mesure F 1 étant très synthétique, pour comparer les mé-thodes, on a également utilisé l'indicateur de performance AU C qui mesure l'aire de la courbe ROC (Davis et Goadrich (2006)).
Pour la collection ev2014, seuls les indicateurs de performances calculés par la plateforme du challenge pour chaque corpus sont disponibles : AU C, l'indicateur c@1, le produit des deux indicateurs et le temps d'exécution. L'indicateur c@1 permet de donner plus d'importance à une réponse correcte par rapport à l'absence de décision (i.e. une probabilité d'appartenance à la classe de 0.5). Cet indicateur est défini par :
où n est la taille du corpus, n c est le nombre de réponses correctes, n u le nombre de problèmes laissés sans décision.
Résultats sur la collection 2013
Pour la méthode DCM, nous avons utilisé la représentation R1 (caractère 8-grams) qui avait donné les meilleurs résultats sur la collection d'apprentissage de 2013 et fixé ? à |Ap| 2 alors que pour DCM-voting, nous avons privilégié les espaces de représentation R1, R2, R3, R4 et R678 (cf. Tableau 1). Pour les quatre espaces ayant trait aux mots ou aux n-grams caractères, les documents sont représentés sous format vectoriel avec la pondération tf ? idf .
La table 3 présente les résultats produits par les trois méthodes sur la collection eval13 ainsi que les résultats des gagnants de la compétition par corpus puis sur l'ensemble de la collection. La faible précision de DCM-classifier pour le corpus espagnol peut s'expliquer par le manque de problèmes pour cette langue (seulement quatre) dans le corpus d'apprentissage rendant difficile la construction d'un modèle performant. Les trois méthodes produisent des résultats satisfaisants cependant, DCM et DCM-voting sont limités aux problèmes contenant au moins deux textes connus. Si on compare les résultats obtenus par nos méthodes avec ceux des gagnants de la compétition par corpus, alors il n'y a que sur le corpus grec que la méthode DCM-classifier l'emporte avec un score de 85%. Par contre, sur l'ensemble de la collection, DCM-voting comme DCM-classifier obtiennent des résultats meilleurs ou équivalents à ceux du gagnant pour tous les critères d'évaluation (F1, précision et rappel) . TAB. 4: Résultats de la 10-cross validation de DCM-classifier sur app2014
Résultats sur les collections 2014
La collection 2014 contient un nombre plus élevé de problèmes et de types de document que la collection de 2013 et elle s'avère plus difficile à traiter puisque plus de la moitié des problèmes ont un seul document connu (|A| < 2) ; ce qui rend les méthodes DCM et DCMvoting inadaptées et inefficaces. Pour cette raison, seule DCM-classifier a été évaluée, d'abord sur la collection d'apprentissage en utilisant la technique de 10-validation croisée qui consiste à séparer le corpus à traiter en deux groupes, un pour entrainer le modèle et l'autre pour le tester, puis sur la collection d'évaluation dans le cadre du challenge.
Les résultats obtenus par validation croisée sur l'ensemble d'apprentissage sont présentés dans la table 4. Ils confirment les performances de la méthode DCM-classifier.
Le tableau 5 contient les résultats officiels obtenus lors de la compétition PAN14 in Author Identification, Stamatatos et al. (2014). Ils permettent de comparer notre méthode à celles des autres participants. DCM-classifier nous a permis d'être classé en deuxième position lors de la compétition. Elle fournit de bons résultats en un temps relativement court. Il convient de noter que les temps de traitement affichés par le gagnant de la compétition sont en moyenne supé-rieurs à trois heures alors que ceux de DCM-classifier sont de l'ordre de quelques secondes.
Un des avantages de la méthode DCM-classifier, basée sur les arbres de décision, est de mettre en évidence les caractéristiques qui permettent le mieux d'identifier l'auteur d'un do- 1/6 1% GR T OTcount 1/6 8% SP TAB. 6: Classement des attributs cument selon le type de corpus considéré. En effet, les documents sont décrits par des attributs calculés sur différents espaces de représentation mais l'apprentissage intègre une phase de sé-lection de ceux qui sont les plus discriminants. Ainsi, on peut en déduire pour chaque corpus l'importance de chaque attribut.
Les figures 1 et 2 présentent les différents espaces de représentation utilisés pour deux corpus de langues différentes, on voit que ces espaces sont très différents et que les poids rattachés le sont aussi. La table 6 indique de manière synthétique la liste des attributs les plus utilisés sur l'ensemble des corpus de la collection d'évaluation 2014. Ce résultat confirme l'intérêt de combiner plusieurs espaces de représentation pour résoudre le problème d'identification d'auteurs.

Introduction
Les systèmes de recommandation (SR), visent à recommander à des utilisateurs des ressources pertinentes pour eux. Le filtrage collaboratif (FC) (Resnick et al., 1994) est une des approches les plus populaires de la recommandation.
Bien que la qualité des recommandations fournies par le FC soit considérée comme satisfaisante en moyenne (Castagnos et al., 2013), certains utilisateurs ne reçoivent pas de recommandations de qualité. Le manque de données sur ces utilisateurs est une des raisons possibles expliquant cette mauvaise qualité. Ce problème est appelé démarrage à froid (Schein et al., 2001). Parmi les autres raisons évoquées dans l'état de l'art se trouve la trop grande différence des préférences de ces utilisateurs, par rapport à celles des autres (Haydar et al., 2012). C'est sur cette raison que nous nous focalisons dans cet article. En effet, le filtrage collaboratif suppose une cohérence entre les préférences des utilisateurs ; ces utilisateurs ne respectant pas ce critère, il semble normal qu'ils se voient proposer des recommandations de mauvaise qualité. Ces utilisateurs peuvent aussi être considérés comme des données aberrantes, ou outliers. Nous choisissons de les appeler des utilisateurs atypiques.
Notre objectif ici est d'identifier ces utilisateurs atypiques. Nous proposons, dans ce travail préliminaire, plusieurs mesures permettant de les identifier, en exploitant uniquement leurs préférences.
La section 2 se focalise sur les systèmes de recommandation et l'atypisme. Dans la section 3, nous proposons des mesures d'identification des utilisateurs atypiques. Ensuite, nous pré-sentons les expérimentations menées pour valider ces mesures et nous concluons notre travail.
État de l'art
La recommandation sociale, ou filtrage collaboratif (FC), exploite les préférences d'utilisateurs (en général des notes sur des ressources) pour estimer des préférences inconnues. L'approche à base de mémoire, et notamment les k plus proches voisins (knn), exploite les similarités de préférences entre utilisateurs. Bien que simple à mettre en oeuvre, intégrant dynamiquement les nouvelles préférences et fournissant des recommandations de qualité, cette approche ne passe pas à l'échelle. L'approche à base de modèle souffre moins du problème de passage à l'échelle. La technique de factorisation de matrices (Hu et al., 2008), la plus répan-due, forme un sous-espace de caractéristiques latentes, dans lequel utilisateurs et ressources sont représentés, qui permet d'estimer les préférences inconnues.
Dans la littérature, les utilisateurs que nous appelons atypiques sont nommés déviants, anormaux, etc. (Del Prete et Capra, 2010) et la définition qui en est faite varie légèrement. Les travaux dédiés à leur identification sont peu nombreux. La mesure d'anormalité (Del Prete et Capra, 2010;Haydar et al., 2012) aussi appelée coefficient de déviance, déviance, etc., est la plus utilisée pour les identifier. Elle représente la propension qu'a un utilisateur à noter différemment des autres. Elle exploite l'écart entre les notes d'un utilisateur sur des ressources et la note moyenne sur ces ressources (équation (1)).
où n u,r est la note que l'utilisateur u a donné à la ressource r, n r la note moyenne sur r, R u l'ensemble des ressources notées par u et u leur nombre. Les utilisateurs dont l'anormalité est très élevée sont considérés comme atypiques. Bien que peu complexe, cette mesure ne tient pas compte du comportement propre à chaque utilisateur et les ressources sur lesquelles les utilisateurs ne sont pas unanimes vont injustement augmenter l'anormalité. (Bellogín et al., 2011) définit un indicateur de clarté qui identifie les utilisateurs ambigus (instables) dans leur notation, basé sur la mesure de l'entropie. Il a l'inconvénient d'identifier comme instables des utilisateurs dont les préférences évoluent ou dont les préférences diffèrent en fonction des domaines des ressources. Cette mesure ne nous paraît pas adéquate car l'approche sociale peut leur proposer des recommandations de bonne qualité. (Bellogín et al., 2011;Haydar et al., 2012;Griffith et al., 2012) identifient un lien entre l'erreur commise sur chaque utilisateur et ses caractéristiques (nombre de notes, de voisins, etc.). (Haydar et al., 2012), forme des clusters d'utilisateurs et identifie un cluster d'atypiques : des utilisateurs avec une forte erreur et un fort indice d'anormalité. Cependant, nous pensons que les atypiques ne sont pas toujours similaires entre eux (ce qui en ferait d'ailleurs des utilisateurs non atypiques au sens de la recommandation sociale), le clustering échouera probablement à former des clusters d'atypiques. C'est dans ce sens que va le travail présenté dans (Ghazanfar et Prugel-Bennett, 2011), qui clusterise des utilisateurs et propose de considérer comme atypiques les utilisateurs qui ne sont proches du centre d'aucun des clusters formés.
L'identification d'utilisateurs atypiques peut être associée à l'identification de données aberrantes (outliers) : un outlier est une donnée qui dévie tellement des autres données que cela laisse penser qu'elle a été générée par un mécanisme différent. Les méthodes statistiques et le clustering sont également très utilisées dans le domaine de la détection d'outliers (Aggarwal, 2013).
Nouvelles mesures d'identification d'utilisateurs atypiques
Partant des travaux de la littérature, nous proposons de nouvelles mesures permettant l'identification d'utilisateurs atypiques.
CorrKMax -Nous pensons que l'approche knn échoue sur les utilisateurs n'ayant pas suffisamment d'utilisateurs similaires. CorrKM ax représente la similarité moyenne qu'a un utilisateur u avec ses k utilisateurs les plus similaires (équation (2)).
où CorrP earson(u, v) est la corrélation de Pearson entre u et v. V (u) représente les k utilisateurs les plus similaires à u. Nous pensons que les utilisateurs associés à une faible valeur de CorrKM ax(u) recevront des recommandations de mauvaise qualité.
Anormalité CR (Anormalité avec Controverse sur les Ressources) -Cette mesure repose sur la mesure d'anormalité de l'état de l'art. Elle suppose que l'écart sur une ressource controversée n'a pas le même sens qu'un même écart sur une ressource consensuelle, ce qui n'est pas considéré par la mesure d'anormalité de l'état de l'art. Nous proposons de pondérer les notes par le degré de controverse de la ressource, fonction de l'écart-type de ses notes. L'Anormalite CR d'un utilisateur u est présentée dans l'équation (3).
où contr(r) est la controverse associée à une ressource r. Cet indice est basé sur l'écart-type normalisé des notes sur r. Où contr(r) = 1 ? ?r??min ?max??min , avec ? r est l'écart-type des notes de r. ? min et ? max sont respectivement le plus petit et le plus grand écart-type de notes possibles parmi les ressources.
Le calcul d'Anormalité CR est d'une complexité comparable à celle de l'anormalité de l'état de l'art. Elle peut donc être calculée fréquemment et ainsi prendre en compte les nouvelles préférences des utilisateurs.
Anormalite CRU (Anormalité avec Controverse sur les Ressources et profil Utilisateur) -Ni Anormalité ni Anormalité CR ne tiennent compte du comportement général de l'utilisateur auquel elles s'appliquent. Un utilisateur sévère peut être identifié comme atypique alors qu'il n'est atypique que dans sa manière de noter, et non dans ses préférences et qu'il recevra probablement des recommandations de qualité. Pour éviter ce biais, nous proposons de centrer les notes de chaque utilisateur par rapport à sa moyenne de notes. L'Anormalité d'un utilisateur u, notée Anormalité CRU (u) est calculée selon l'équation (4) :
où n Cr représente la moyenne des notes centrées des utilisateurs sur r, contr C (r) est l'indice de controverse associé à r, calculé à partir de l'écart-type des notes sur la ressource, centrées par rapport aux utilisateurs. Le calcul de Anormalité CRU (u) est certes plus coûteux en temps que Anormalité CR (u), mais devrait permettre une identification plus précise des utilisateurs atypiques. Notons que ces deux dernières mesures sont indépendantes de l'approche de recommandation utilisées (knn ou factorisation de matrices), à l'opposé de la mesure CorrKM ax.
Expérimentations
Dans cette section, nous évaluons les mesures d'identification des utilisateurs atypiques que nous proposons, en comparaison avec celles de l'état de l'art.
Nous utilisons le corpus de données de l'état de l'art MovieLens, composé de 100 000 notes (de 1 à 5) de 943 utilisateurs sur 1 682 films (ressources). Une division du corpus en 2 sous-corpus de 80% (pour l'apprentissage) et 20% (pour le test) est effectuée. L'état de l'art souligne que les utilisateurs pour lesquels le système manque de données (démarrage à froid) reçoivent de mauvaises recommandations. De façon à ne pas biaiser notre évaluation, nous écartons du corpus les utilisateurs associés à du démarrage à froid : ceux avec moins de 20 notes dans le corpus d'apprentissage (Schickel-Zuber et Faltings, 2006). Le corpus est alors réduit à 821 utilisateurs. La mesure Anormalité de l'état de l'art présente une corrélation de 0,453 avec la RMSE. Cette corrélation significative confirme le lien existant entre l'anormalité d'un utilisateur et l'erreur commise par une approche knn : plus un individu est anormal (atypique), plus l'erreur commise sera élevée. A l'opposé, moins il est anormal, moins l'erreur sera élevée.
Corrélations entre les mesures et l'erreur de recommandation
Anormalité CR augmente la corrélation de 11% (0,504). La controverse associée aux ressources permet donc d'améliorer l'estimation de la qualité des recommandations fournies aux utilisateurs. Anormalité CRU prend également en compte le profil de l'utilisateur. Une corréla-tion de 0,546 est obtenue (amélioration supplémentaire de 8%, et donc de 20% par rapport à l'état de l'art). La prise en compte des particularités de notation des utilisateurs permet donc d'améliorer l'estimation de la qualité des prédictions fournies aux utilisateurs.
La faible corrélation de la mesure CorrKMax (-0,22) indique que, contrairement à notre intuition, la qualité des voisins d'un utilisateur n'est pas corrélée à la qualité des recommandations dans le cadre d'une approche de recommandation knn.
Erreur en prédiction sur les utilisateurs atypiques
Une corrélation représente le lien entre deux variables sur un ensemble d'observations. Cependant, il est possible qu'un lien existe sur une seule partie des observations, ce qui ne sera pas reflété par la corrélation. Ici, nous nous intéressons uniquement aux utilisateurs qualifiés d'atypiques, c'est-à-dire à ceux ayant des valeurs d'anormalité les plus extrêmes. Par consé-quent, dans la suite des expérimentations, nous nous intéressons uniquement à la répartition FIG. 1 -Répartition de la RMSE des utilisateurs atypiques avec l'approche knn des erreurs observées sur les utilisateurs considérés comme atypiques. Pour représenter la ré-partition de ces erreurs, nous exploitons les quartiles et la médiane de ces erreurs, sur les 4 mesures d'anormalité. Plus les erreurs sont élevées, plus nous pouvons considérer que la mesure est de qualité. Nous comparons ces répartitions à celle associée à l'ensemble total des utilisateurs, dénommé Complet (Figure 1).
Nous utilisons un pourcentage d'utilisateurs atypiques fixe, car les mesures n'ont pas des valeurs d'anormalité comparables. Nous avons fixé exprimentalement ce seuil à 6% des utilisateurs, cela correspond à environ 50 utilisateurs parmi les 821 utilisateurs.
L'erreur médiane sur l'ensemble des utilisateurs (Complet) est de 0, 82. Celle de Anormalité est de 1, 26, ce qui correspond à une augmentation de l'erreur de plus de 50%, elle est d'ailleurs équivalente au troisième quartile de l'ensemble complet. Cependant, 25% des utilisateurs qualifiés d'atypiques ont une RMSE plus faible que la RMSE médiane sur l'ensemble des utilisateurs. Par conséquent, Anormalité semble sélectionner un grand nombre d'utilisateurs dont les recommandations sont de bonne qualité. Anormalité CR et Anormalité CRU pré-sentent de meilleurs résultats qu'Anormalité. Anormalité CRU est la plus performante : plus de 75% des utilisateurs sélectionnés ont une RMSE supérieure à 1,25 : 75% des utilisateurs Anormaux CRU font partie des 25% de l'ensemble complet des utilisateurs à recevoir les moins bonnes recommandations. Enfin, environ 50% des utilisateurs sélectionnés avec CorrKM ax reçoivent de bonnes recommandations, ce qui confirme les premières conclusions obtenues avec la corrélation.
Nous pouvons conclure que Anormalité CRU identifie de façon fiable les utilisateurs atypiques : ceux recevant des recommandations de mauvaise qualité avec une approche knn.
Conclusion et perspectives
Notre objectif dans ce premier travail était d'identifier, en recommandation sociale, les utilisateurs qui reçoivent des recommandations de mauvaise qualité, avant que des recommandations ne leur soient proposées. Nous avons fait l'hypothèse que ces utilisateurs avaient des préférences différentes des autres utilisateurs : des utilisateurs atypiques. Nous avons proposé plusieurs mesures exploitant la similarité de préférence avec les autres utilisateurs, l'écart des notes par rapport aux autres, le consensus de notation sur les ressources et le profil de nota-tion des utilisateurs. Nous avons montré que, sur un corpus de l'état de l'art, ces informations permettaient de prédire fiablement la mauvaise qualité des recommandations faites à un utilisateur. Nous pouvons donc conclure que les utilisateurs présentant des préférences différentes des autres utilisateurs reçoivent effectivement des recommandations de mauvaise qualité. Ces mesures peuvent donc être utilisées pour anticiper une mauvaise recommandation et la suite de ce travail portera naturellement sur la prise en compte des préférences atypiques des utilisateurs pour leur fournir des recommandations de qualité.

Introduction
Depuis les débuts du TALN, la compréhension de texte fait l'objet d'un suivi particulier de plusieurs recherches. C'est en faveur du développement rapide de la tâche d'extraction d'information que la tâche de REN s'est manifestée. Elle consiste à rechercher les expressions réfé-rentielles (Ehrmann (2008)), qui recouvrent classiquement les noms désignant des personnes, des lieux, des organisations, des expressions temporelles et celles numériques, mais peuvent aussi se rapporter à des notions plus techniques comme les maladies. Dès la campagne MUC-6, la tâche de REN s'est ainsi polarisée sur trois types d'entités (Grishman et Sundheim (1996)), à savoir : ENAMEX (personnes, organisations et lieux), TIMEX (expressions temporelles), NUMEX (expressions numériques). Cette première définition a été étendue dans la campagne CoNLL (Tjong Kim Sang et De Meulder, 2003) où 4 classes ont été normalisées : personnes, organisations, lieux, Divers. Dans les campagnes d'évaluation ESTER2 (Galliano et al., 2009) 8 catégories ont été normalisées à savoir personnes, fonctions,organisations, lieux, productions humaines, dates, montants et événements.
Travaux Connexes
Auparavant, la visibilité de la langue amazighe au Maroc était quasiment nulle. Récem-ment, et grâce aux revendications qui se sont faites à l'aide de l'IRCAM 1 , elle a été soumise à un processus de codification et de standardisation. Face à l'augmentation vertigineuse des informations en langue amazighe, disponibles librement sur le Web, plusieurs recherches ont été entamées dans ce sens. Il y en a celles qui se concentrent sur la reconnaissance optique des caractères (OCR) (Es-Saady et al., 2012) et celles qui se focalisent sur le TALN que nous pouvons classer en deux grandes catégories : (1) ressources informatiques, y compris des études sur la construction des corpus amazighe (Boulaknadel et Ataa Allah, 2011) et (2) les outils du TAL qui ont été réalisés comme le concordancier (Boulaknadel et Ataa Allah, 2010), l'analyseur morphologique (Nejme et al., 2013a,b) et (Ataa Allah et Boulaknadel, 2010). Quant au domaine de la REN, il a acquis un certain intérêt à travers les travaux réalisés de Talha et Boulaknadel (Talha et al., 2014b,a;Boulaknadel et al., 2014).
Aperçu général de notre approche
On distingue traditionnellement trois grandes approches : Approches symboliques qui reposent sur l'utilisation de grammaire formelle construite par la main. Approches statistiques qui permettent d'apprendre, des modèles d'analyse de textes sur de large corpus annoté auparavant, et ensuite établir automatiquement une base de connaissances à l'aide de plusieurs modèles numériques comme le CRF, SVM, etc. Au-delà de ces deux approches, il existe une autre qualifiée d'hybride qui représente un arrangement entre ses antécédents. Dans notre contribution, nous proposons un système fondé sur une approche symbolique, vu la non disponibilité d'un large corpus, où le repérage s'effectue en se basant sur un ensemble de gazetteers et de règles qu'on a construit manuellement tout en exploitant le principe de transducteurs à états finis disponibles sous GATE. 
Architecture du système
Notre système de repérage d'entités nommées permet l'identification des bornes des EN, ainsi que leur catégorisation dans des classes prédéfinies. Son architecture, détaillée sur la figure 1, comporte 3 modules qui effectuent un traitement séquentiel immédiat des données : 
Prétraitement Morphologique
Manipuler des textes écrits en langue amazighe nécessite une analyse préliminaire qui consiste à : La suppression des espaces supplémentaires existants entre les mots et l'élimi-nation de tous les mots non-amazighes figurant dans le corpus. Notre analyse comprend deux phases :
-La segmentation du texte amazighe en des phrases.
-L'identification des entités linguistiques de base « tokenisation ». Ces deux phases citées au dessus sont implémentées en utilisant, respectivement, les modules de GATE : le « Sentence Splitter » et le « Tokeniser ».   
Constitution des gazetteers
Évaluation
Discussion
Les entités nommées qui n'ont pas été identifiées, correspondent soit à des entités qui ne font pas partie de nos ressources, soit à des entités qui font partie de nos ressources, mais sont ambiguës. Certaines entités sont cependant ambiguës pour cause d'homographie, ou encore le cas d'entités poly-référentielles, une même entité nommée peut convenir à plusieurs classes. La prépondérance des entités mal classées implique un manque d'information que ce soit au niveau du contexte syntaxique ou de la présence des indices externes, qui, en plus de déter-mination des mots d'arrêt qui permettent de décider ou s'arrêter, augmente les probabilités d'erreurs de délimitation. Une analyse approfondie a conduit aux constats suivants :
-Enrichir nos gazetteers (Personne, Organisation, Localisation, DATE, NUM).
-Effectuer un traitement syntaxique supplémentaire afin de mieux saisir la structure syntaxique des phrases amazighes avant d'effectuer le repérage des entités nommées. -Étendre le nombre de règles linguistiques pour chaque classe d'entité nommée.
Conclusions et perspectives
Dans cet article nous avons proposé un système de repérage des entités nommées amazighes à base de règle. L'évaluation du système montre que les résultats obtenus sont assez encourageants et nous invitent à explorer de nouveaux modes de repérage d'entités nommées, afin de tirer le meilleur parti de notre approche et affiner le repérage des entités nommées amazighes.
Références
Ataa Allah, F. et S. Boulaknadel (2010). Light morphology processing for amazighe language.
In proceeding of the Workshop on Language Resources and Human Language Technology for Semitic Languages, Volume 17.

Introduction
Lorsque l'on cherche à comparer deux documents, on recherche tout élément présent dans l'un qui est également présent dans l'autre, ces éléments sont dénommés "similitudes". Les plus évidentes à voir à l'oeil humain sont les similitudes exactes, les parties copiées d'un document directement dans l'autre. Cependant, reproduire informatiquement cette capacité humaine est une opération délicate. Ce procédé est souvent gourmand en temps, car passant par une comparaison mot à mot afin d'identifier les séquences de mots identiques dans les deux textes. De ce fait, des méthodes beaucoup moins gourmandes ont vu le jour. Basées sur un système de n-grammes, elles extraient des séquences de n mots se suivant d'un texte et en cherche la présence dans l'autre. C'est dans l'optique de proposer une alternative à ces méthodes que nous allons décrire dans cet article une nouvelle approche de construction de séquences communes.
Après avoir présenté l'état de l'art, nous décrirons dans un premier temps le processus d'intersection des deux textes, ensuite, la phase de construction des plus longues séquences communes et pour finir, nous présenterons l'évaluation de notre approche en la comparant aux méthodes naïves de comparaison mot à mot et à la méthode classique n-grammes.
2 Le « copier/coller » 2.1 Le phénomène « copier/coller » Le « copier/coller » touche particulièrement les étudiants, en Europe, 34,5% (Guibert et Michaut, 2011) d'entre eux auraient déjà recopié tout ou partie d'un document pour le présen-ter comme travail personnel. Cette fréquence rejoint celle de travaux américains (Park, 2003) estimant à environ 30% la proportion d'étudiants ayant produit un travail reprenant des phrases d'Internet sans en citer la source. Une étude européenne (Gibney, 2006) révèle que près d'un étudiant français sur deux (46%) a déjà fait usage du plagiat pendant son cursus, contre environ un tiers des étudiants anglais et 10% des étudiants allemands. Ces résultats, qui paraissent déjà impressionnants, pourraient pourtant encore être sous-évalués. En effet, toujours selon la même étude, 40% des étudiants ne comprennent pas ce que signifie réellement le plagiat et n'assimilent pas le « copier/coller » à de la tricherie. La recherche de « copier/coller » entre deux textes joue donc un rôle essentiel dans la prévention du plagiat et la protection du droit d'auteur.
État de l'art
Les « copier/coller » sont en théorie les similitudes textuelles les plus facilement repé-rables et identifiables. En effet, la détection de celles-ci équivaut à comparer l'égalité entre deux textes. Pour effectuer cette recherche automatiquement on est obligé de procéder à une comparaison mot à mot. Cette opération, étant beaucoup trop chronophage pour être intégrée dans des solutions à but commercial ou hébergées en ligne, comme des services anti-plagiat, des techniques alternatives ont dû être mises au point.
Les méthodes les plus efficaces restent les méthodes classiques dites n-grammes (Torrejón et Ramos, 2013), qui consistent à construire puis comparer à partir de textes, des séquences de n éléments pouvant être des syllabes, des mots, des entités nommées, etc. La recherche de Barron-Cedeño et Rosso (2009) prouve qu'en prenant des "n-words" (séquence de n mots se suivant) de petites tailles, deux ou trois par exemple, les résultats sont bien meilleurs qu'en utilisant des longues séquences avec un n important. Sur le même principe mais plus originale, on peut citer la méthode de Stamatatos (2009), utilisant des n-grammes mais lors d'une détection intrinsèque, c'est-à-dire sans utilisation de document externe, on ne cherche pas des similitudes avec d'autres documents mais on étudie l'intérieur même du document analysé pour y repérer des irrégularités, des zones suspectes. Les n-grammes les plus pertinents ne sont pas toujours des séquences de mots, comme en atteste le travail de Shrestha et Solorio (2013), des n-grammes de mots vides (stop words) et d'entités nommées sont également utilisés pour dé-tecter des parties de textes similaires entre deux documents. Toutefois, les méthodes les plus répandues sont les méthodes "fingerprint", créant une empreinte du document pour la comparer avec celle d'autres documents. La plupart de ces méthodes (Kent et Salim, 2010) utilisent également des n-grammes pour construire l'empreinte des documents.
Les méthodes "fingerprint" divisent la plupart du temps le document en grammes de longueur n, ainsi les empreintes de deux documents peuvent être comparées et les points (i.e. grammes) concordants, identifiés comme étant des passages identiques dans les textes. Certaines méthodes de "fingerprint" (Stein et Eissen, 2006, 2007Lyon et al., 2001) vont audelà de la recherche de similitudes exactes et introduisent la notion de « similarités proches » pouvant ainsi détecter les paraphrases. Toujours dans cette optique, des recherches plus ré-centes (Simac-Lejeune, 2013; Kong et al., 2013) ne se contentent pas de comparer des mots ou groupes de mots d'un document à un autre mais tentent d'établir une corrélation « sémantique » entre deux documents par une approche utilisant des mots-clefs.
3 Notre approche 3.1 Intersection de deux textes L'idée de cette première étape est d'effectuer une intersection de deux textes, afin d'obtenir un tableau des mots présents dans les deux textes tout en conservant la position qu'ils ont dans l'un des deux. La procédure utilisée durant cette étude est la suivante :
1. passage en minuscule des deux textes à comparer ; 2. transformation en tableaux de ces deux phrases en segmentant sur les espaces et les caractères de ponctuation (lemmatisation) (chaque tableau représente une phrase et chaque cellule d'un tableau contient un lemme de la phrase à laquelle il correspond) ; 3. intersection des deux tableaux créés en conservant les offsets (positions) des mots du premier tableau et donc de la première phrase.
Construction de séquences maximales communes
La seconde et dernière étape consiste à construire, à partir du tableau obtenu à l'étape précédente, les séquences d'un minimum de n mots se suivant dans le premier texte, se suivant donc dans le tableau et étant également présentes dans le second texte. Le seuil n est le nombre de mots se suivant à partir duquel on peut déterminer qu'une séquence est la copie d'une autre et qu'elle n'est pas due au hasard. Nous pourrions dès lors nous poser la question : à partir de combien de mots se suivant une séquence peut être considérée comme réellement copiée ? En effet, il existe des séquences de trois mots ou plus, suffisamment fréquentes dans la langue, pour fausser la comparaison, comme les séquences « il était une fois » ou « nulle par ailleurs ». Cependant, les résultats des travaux de Barron-Cedeño et Rosso (2009) démontrent que sur de larges textes, il est tout aussi efficace de fixer un n petit, à deux ou trois par exemple.
La procédure de construction des séquences est la suivante :
1. on déplace une fenêtre de glissement de n éléments dans le tableau en fonction du seuil n choisi afin de constituer des "n-words" se suivant donc forcément dans le premier texte ; 2. pour chaque "n-word" constitué, on vérifie son existence dans le second texte ; 3. tant qu'une correspondance est trouvée et que la séquence existe bien dans les deux textes, on construit la séquence de taille n + 1 en y concaténant le mot suivant du tableau ; 4. dès que la séquence ne s'y trouve plus, on récupère la séquence maximale commune (la séquence essayée précédemment avant que le test échoue) et on recommence depuis l'étape 1 en déplaçant la fenêtre de glissement sur le mot suivant et en reprenant le n initial.
Évaluation et tests 4.1 La base de tests et protocole
La base de tests est composée de 200 textes, allant de 100 mots à environ 20 000 mots (avec une moyenne de 1500 mots), représentant 500 comparaisons de textes deux à deux annotés manuellement afin de savoir quel passage est réellement la copie d'un autre. Pour tester correctement les performances des algorithmes évalués, la base comporte aussi bien des passages entièrement copiés que des paraphrases ou des reformulations plus complexes, ainsi que des textes « pièges » traitant du même sujet et donc employant le même vocabulaire mais n'étant pas pour autant un « copier/coller » ou une reformulation quelconque d'un autre texte présent dans le corpus. L'intégralité de ces textes est en français. Ci-dessous la répartition des comparaisons :
-120 comparaisons effectuées afin de détecter des textes entièrement « copier/coller » de façon exact ; -80 comparaisons afin de détecter des textes entièrement paraphrasés ou reformulés ; -200 comparaisons entre des textes ne comportant que quelques passages rigoureusement identique (copier/coller) ; -100 comparaisons entre deux textes ne comportant que quelques passages « similaires » (paraphrases ou reformulations de phrases et/ou paragraphes). Ces comparaisons sont réparties entre des travaux d'élèves (mémoires financiers et scientifiques) avec leurs sources, différentes versions à différentes dates d'un même article de Wikipédia et des extraits du corpus de la PAN-CLEF 2014 en matière d'alignement de textes.
Résultats
Les résultats obtenus sur le corpus de test, par la méthode naïve de comparaison mot à mot, la méthode classique des n-grammes et notre méthode, sont représentées dans le tableau 1. La méthode n-grammes évaluée est celle décrite dans l'article de Barron  (2009) disant que prendre un n de petite taille augmente l'efficacité de détection, sachant que prendre des bigrams favorise le rappel, tandis que prendre un n supérieur favorise la précision. Ce phénomène s'explique par le fait que prendre un petit n forme des séquences courtes, on ne manque ainsi aucune correspondance mais on favorise les faux positifs, baissant alors la précision. En revanche prendre un n plus important construit des séquences plus longues, réduisant ainsi la correspondance de chaînes et donc le rappel mais augmentant le taux de certitude des concordances et donc la précision. Cet article ne pose pas la question d'optimisation de la détection en fonction du n choisi, on fixe donc n = 2 pour la suite de notre évaluation.
On constate dans le tableau 1 que notre méthode donne de meilleur résultat que celle des n-grammes (0.76 de précision contre 0.72 et 1 de rappel contre 0.78 avec n = 2 pour les deux méthodes). Toutefois, on peut voir dans le tableau 2 qu'en moyenne elle est 15% moins rapide et 30% plus coûteuse en mémoire que la méthode n-grammes (en allouant 6. 
Conclusions
Notre approche montre donc des résultats supérieurs aux méthodes n-grammes classiques, dans le sens où elle recherche une séquence de taille minimale n et agrandit si possible la séquence trouvée afin d'obtenir une séquence maximale commune. En revanche, elle est tout aussi dépendante du nombre n choisi que les méthodes n-grammes. Son temps d'exécution et son usage de la mémoire restent supérieurs à ceux des méthodes n-grammes bien que nettement inférieurs à ceux de la méthode mot à mot.
Pour des travaux futurs, nous envisageons de confronter nos résultats à des méthodes ngrammes plus sophistiquées comme celles décrites dans l'article de Shrestha et Solorio (2013).
Pour conclure, bien que moins rapide, notre méthode montre une précision équivalente aux méthodes n-grammes tout en proposant un rappel nettement supérieur.
Summary
Plagiarism detection most commonly use the most naive phase of similarities search, the detection of copy and paste. In this paper, we propose an alternative method to the standard verbatim comparison approach. The idea is to carry out an intersection of two texts to get a table of common words and to keep only the maximum sequences of consecutive words in one of the texts which also exists in the other. We show that this method is faster and less expensive in memory that commonly used scan texts methods. The goal is to detect identical passages between two texts faster than verbatim comparison methods, while operating more efficient than the n-grams.

Introduction
Le BiClustering consiste à réaliser un clustering simultanément sur les observations et les variables. Govaert et al ont introduit une adaptation de l'algorithme k-means au biclustering nommée "Croeuc" qui permet de découvrir tous les biclusters en même temps. Dans Labiod et Nadif (2011), les auteurs ont proposés une approche de factorisation CUNMTF, qui généra-lise le concept de la NMF Lee et Seung (1999). D'autres modèles probabiliste de biclustering sont proposés dans Govaert et Nadif (2008). Le Biclustering a de nombreuses applications et devient un challenge de plus en plus important avec l'augmentation des volumes de données. Cependant les bons algorithmes de clustering sont encore extrêmement utiles, il est donc néces-saire de les adapter aux nouvelles architectures massivement distribuées utilisant le paradigme MapReduce.
Le paradigme MapReduce Dean et Ghemawat ( Dans ce papier nous proposons une nouvelle approche globale de biclustering basé sur les cartes auto-organisatrices et le calcul distribué. Le modèle de biclustering BiTM (Biclustering using Topological Maps) a deja donnée lieu à une publication dans Chaibi et al. (2014). Dans ce papier nous proposons une adaptation de ces travaux a l'architecture MapReduce avec une implémentation de BiTM sous Spark, une technologie open source de calcul distribué incluant plusieurs paradigmes de programmation. Les principales problématiques abordées dans ce papier sont la minimisation de la fonction et la taille des données en entrée et en sortie des fonctions primitive (Map et Reduce) d'un algorithme de biclustering topologique.

Introduction
Les tweets sont des messages courts ne dépassant pas 140 caractères. Cette contrainte impose l'utilisation d'un vocabulaire particulier pour les rédiger et donc elle rend indispensable de connaitre leurs contextes pour les comprendre. Pour ces raisons, nous allons nous concentrer sur la tâche de contextualisation des tweets attribuée à INEX2014 1 . Les participants devaient fournir un contexte, pour permettre aux lecteurs de bien comprendre le tweet en utilisant un système de recherche d'information SRI et système de résumé automatique SRA. Dans cet article, nous proposons une nouvelle approche de contextualisation de tweets basée sur les règles d'association inter-termes.
Cet article est organisé comme suit : Dans la section 2, nous détaillons notre nouvelle approche. la section 3 sera consacrée aux différentes expériences menées , finalement nous conclurons dans la section 4. 
Approche proposée
Conclusion
Dans cet article, nous avons décrit une nouvelle approche de contextualisation de tweet basée sur règles d'association inter-termes. Les résultats ont confirmé que la synergie entre les règles d'association entre termes et l'expansion de tweets est fructueuse. Dans un travail en cours, nous proposons d'ajouter une phase de désambiguïsation pour réduire le bruit dans nos résultats. 
Summary
Tweets are short messages that do not exceed 140 characters. Since they must be written respecting this limitation, a particular vocabulary is used. To make them understandable to a reader, it is therefore necessary to know their context. In this paper, we describe our approach for the tweet contextualization. This approach allows the extension of the tweet's vocabulary by a set of thematically related words using mining association rules between terms.

Introduction
L'objectif des systèmes de recommandation est de prédire les choix et les préférences individuelles en fonction des comportements et des préférences observées. Le filtrage collaboratif est la technique la plus utilisée par les systèmes de recommandation. Il consiste à comparer les données d'un utilisateur avec des données similaires d'autres utilisateurs, basée sur les habitudes d'achat et de navigation (Goldberg et al., 1992). Il permet aux commerçants de fournir des recommendations aux clients pour de futurs achats. Dans la suite, les données sont représentées par une matrice U de taille (n × p) où chaque ligne représente un utilisateur, les colonnes représentent des items, et chaque cellule (u ij ) de U est la note attribuée par un utilisateur i pour un item j. Les notes (u ij ) peuvent être binaires, ou réelles et dans ce cas U est appelée matrice réelle de notations. La matrice U peut être obtenue de manière explicite (en gardant les évaluations fournies par les utilisateurs pour des articles donnés) ou de manière implicite (en considérant qu'un utilisateur préfère implicitement acheter ou pas les éléments présentés sur des pages Web visitées).
Dans le filtrage collaboratif (désormais désigné par FC), plusieurs approches sont utilisées. Les techniques de FC actuelles telles que celles basées sur la corrélation entre utilisateurs (Bobadilla et al., 2013) ou sur la factorisation matricielle (Koren, 2009;Sarwar et al., 2000;Delporte et al., 2014) sont couramment utilisées, mais nécessitent un temps de calcul très coûteux et ne peuvent être déployées en ligne. Dans ce contexte, la classification croisée ou co-clustering, qui consiste à regrouper simultanément les utilisateurs et les items, est une bonne solution. Elle est particulièrement appropriée dans les systèmes de de recommendation. Il est ainsi, par exemple, intéressant de disposer de groupes d'utilisateurs appréciant un groupe de films. Dans (George et Merugu, 2005), les auteurs ont proposé une approche de FC basé sur un algorithme de classification croisée pondérée (COCLUST) qui implique le regroupement simultané des utilisateurs et des articles. Malheureusement, dans cette approche la prise en compte des données manquantes n'est pas appropriée, conduisant ainsi à une faible qualité de recommandation. Nous proposons donc de faire un meilleur usage de cet algorithme par une prise en compte plus efficace des données manquantes. D'autre part, en exploitant le potentiel des résultats de la classification croisée, nous développons un outil interactif de visualisation et d'interprétation simultanée des groupes d'utilisateurs et des groupes d'items.
Le reste du papier est organisé comme suit. La section 2 présente le système de FC basé sur la classification croisée (COCLUST). Les sections 3 et 4 fournissent des détails sur nos approches de gestion des notes manquantes et de visualisation. La section 5 démontre l'efficacité des approches proposées sur des données réelles. Enfin, la section 6 conclut et présente les directions pour des recherches futures.
Notation. Soit U la matrice des notes, une classification croisée en K × L co-clusters (blocs ou sous-matrices résultant d'une classification croisée) par COCLUST conduit à une partition de l'ensemble des utilisateurs en K classes et une partition de l'ensemble des items en L classes. Notons Z = (z ik ) la matrice de classification binaire de taille (n × K) dé-finie par z ik = 1 si l'utilisateur i appartient à la k i` eme classe et 0 sinon. De la même manière notons W = (w j ) la matrice de classification binaire de taille (p × L) définie par w j = 1 si l'item j appartient à la i` eme classe et 0 sinon. Par commodité, nous utiliserons également z = (z 1 , . . . , z n ) avec z i ? {1, . . . , K} (respectivement w = (w 1 , . . . , w p ) avec w j ? {1, . . . , L} ; le vecteur des étiquettes des items). Enfin, nous utiliserons les indices i, j, k et pour désigner implicitement les lignes (utilisateurs), les colonnes (items), les classes en ligne (classes d'utilisateurs) et les classes en colonnes (classes des items) respectivement.
2 Classification croisée par COCLUST Partant de l'algorithme weighted Bregman co-clustering (Banerjee et al., 2004), les auteurs dans (George et Merugu, 2005) ont proposé de s'attaquer au problème de recommandation moyennant une reconstitution des données observées suivant une classification croisée donnée. Plus précisément, à partir de la réorganisation en co-clusters obtenus par l'algorithme COCLUST, les auteurs proposent une matrice d'approximation de U sparse par une matricêmatricê U = (ˆ u ij ) non sparse où chaque cellule est définie de la manière suivante :
avec u k , u k. , u .. , u i , u j sont respectivement les moyennes calculées sur l'ensemble des valeurs observées dans le co-cluster (k, dans la classe des utilisateurs k, dans la classe des items, pour chaque utilisateur et pour chaque item. Notons donc que dans cette formulationû formulationˆformulationû ij dépend de i, j, k et Sachant que les partitions Z and W sont inconnues, le critère à minimiser par COCLUST est le suivant :
où M = (m ij ) est une matrice binaire de taille (n × p) où m ij = 1 si u ij est observé et m ij = 0 si u ij est manquant. Une solution (optimum local) de ce problème peut être obtenue par une minimisation alternée ; sachant Z puis sachant W (Banerjee et al., 2004) jusqu'à la convergence (Algorithm 1). A la convergence la prédiction est obtenue en utilisant (1).
Algorithm 1 Training based on Co-clustering.
. , u i and u j ; ( ? k, i and j) 2. Update Z :
Comme indiqué précedemment les estimations au cours des itérations de COCLUST sont basées uniquement sur les données observées. Malheureusement et étant donné que le taux des données manquantes est très élevé (la sparsité de certaines matrices peut être de l'ordre de 99 %), les prédictions sont biaisées impliquant des qualités de recommandation discutables. D'autre part, George et Merugu (2005) ont proposé de remplacer la moyenne d'un co-cluster vide (qui ne contient aucune note observée) par la moyenne globale. Cette stratégie peut fortement perturber la qualité de la classification croisée, et de plus elle ne garantit pas la convergence de COCLUST, comme le montre figure 1. Pour plus d'explications, nous avons rapporté  
Gestion des notes manquantes dans le FC
Pour surmonter le problème des données manquantes dans le FC, deux approches sont principalement utilisées. La première consiste à travailler uniquement sur les valeurs observées, et la deuxième consiste à utiliser les procédures d'imputation. L'imputation par la moyenne est la plus couramment utilisée, elle consiste à remplacer les notes manquantes d'un item/utilisateur par la moyenne de ses notes observées.
Ces approches peuvent être efficaces, si peu de valeurs sont manquantes, et que le mé-canismes des données manquantes est Missing completely at random ou Missing at random (Little et Rubin, 2002). Malheureusement le taux de notes manquantes dans le filtrage collaboratif est très élevé, ce qui rend ces approches inefficaces dans ce contexte. En effet, elles peuvent conduire à des estimations fortement biaisées, ce qui impacte négativement la qualité des recommandations. Pour illustrer ce propos, nous avons rapporté dans figure 3 un exemple d'une matrice utilisateur-item, avant (figure 3a) et après l'imputation par les moyennes des items (figure 3b). Si nous voulons ordonner les items en fonction des préférences des utilisateurs, l'ordre le plus fiable serait : i1, i4, i2, i3 (tels que i1 est l'élément le plus apprécié). En revanche si nous utilisons la matrice après imputation pour trier ces éléments de la même manière, nous obtiendrons l'ordre suivant : i2, i4, i1, i3 qui est absurde, puisque i1 arrive seulement en troisième position et i2 arrive en première position. Cela est dû aux estimations fortement biaisées des moyennes des utilisateurs i2 et i4. Dans ce qui suit, nous allons présenter une nouvelle méthode d'imputation basée sur la version en ligne de l'algorithme kmeans sphé-rique (OSPK-means) (Zhong, 2005). Notre approche repose sur les deux étapes principales, 1) Partitionner l'ensemble des utilisateurs en k classes, en utilisant l'algorithme OSPK-means et en tenant compte des valeurs manquantes, 2) Estimer les notes manquantes, en se basant sur les résultats de la classification. Et enfin remplacer celles-ci dans la matrice U. Ci-dessous, nous décrivons de manière détaillée les différentes étapes de notre approche :
Etape de Classification
Dans le but de partitionner l'ensemble des utilisateurs en k groupes, nous proposons les procédures suivantes : Initialisation : Dans la version initiale de OSPK-means, l'initialisation se fait par un tirage aléatoire de K centres initiaux parmi l'ensemble des utilisateurs. Cependant cette stratégie n'est pas efficace dans notre cas. En effet la probabilité de choisir un utilisateur avec très peu de notes observées, comme un centre de gravité initial est élevée. D'autre part, sélectionner les centres initiaux uniquement parmi l'ensemble des utilisateurs ayant noté beaucoup d'items, permettrait seulement la détection de certains groupes. Afin de surmonter ces difficultés, nous proposons la procédure d'initialisation suivante :
1. Générer une partition aléatoire des utilisateurs en k classes.
2. Estimer les centres initiaux comme suit : soit µ kj la j i` eme composante du centre k alors :
; sinon où S est un seuil proportionnel à la taille de la classe k, et peut être défini par l'utilisateur. Intuitivement, cette stratégie permet d'estimer la j i` eme composante du centre de la k i` eme classe à partir des données disponibles, mais seulement s'il y a suffisamment de notes observées pour dans cette classe. En revanche, quand peu de valeurs sont observées pour une composante j l'estimation de celle-ci est pénalisée, en divisant par le seuil S. Etape de mise à jour : Lorsque l'utilisateur choisi dans l'étape d'affectation ne dispose pas de suffisamment de notes-observées, l'assignation de celui-ci n'est pas fiable. Par conséquent le centre correspondant ne doit pas être déplacé dans le sens de cet utilisateur. Pour résoudre ce problème, nous introduisons une fonction binaire (h(u) ? {0, 1}) qui annule la mise à jour dans ce cas. L'Algorithme 2 fournit plus de détails sur cette étape de classification.
Algorithm 2 Classification.
Input : n normalized users u i ( i = 1) in R p , K : number of user clusters, ? : learning rate, B : number of batch iterations ; Output : K Centroids µ k in R p , and z = (z 1 , . . . , z n ) ; Steps : 1. Random initialization of the partition z ; 2. Estimation of initial centroids :
; otherwize. for b = 1 to B do for i = 1 to n do 3. Assignment : for each user u i , compute z i :
z i +?h(ui)ui ; t = t + 1 ; end for end for
Estimation des notes manquantes
Dans cette étape les notes manquantes sont estimées, en se basant sur les résultats de la classification. Cependant, une pondération des notes s'avère encore nécessaire. Nous avons choisi d'accorder plus d'importance aux utilisateurs representant le mieux leur classe d'appartenance en pondérant par cos(u i , µ k ) tout en atténuant l'effet des utilisateurs qui ne sont pas en accord avec la préférence globale pour un item au sein de leur classe d'appartenance à l'aide de p(u ij ). Soit u a un utilisateur actif, k = z a , la note pour pour un item j prend la forme suivante :
où r med est la note médiane (r med = 3 if u ij ? {1, 2, 3, 4, 5}, p(u ij ? r med ) est la probabilité qu'un item j soit apprécié au sein d'un groupe k, tel que :
Dans la section suivante, nous proposons d'exploiter les résultats de classification croisée, dans le but de fournir aux utilisateurs une représentation interactive basée sur des graphes bipartis. Cette dernière permet non seulement de faciliter l'interprétation des résultats, mais aussi de donner un sens aux préférences des utilisateurs dans le contexte du filtrage collaboratif.
Visualisation des résultats de la classification croisée
Il y a très peu de travaux qui se sont intéressés à l'aspect visualisation dans le contexte des systèmes de recommandation. En effet ces systèmes sont souvent évalués pour leur capacité à faire de bonnes recommandations, mais leur fonctionnement reste abstrait pour les utilisateurs. Parmi les quelques travaux de visualisation on peut citer la méthode de visualisation des données du FC (Mei et Shelton, 2006) qui consiste à représenter les utilisateurs à côté des items qu'ils aiment, sur le même espace euclidien. On peut aussi citer PeerChooser (Smyth et al., 2008) qui est un système de FC interactif qui permet de visualiser sous forme de graphe les interactions entre un utilisateur actif et son voisinage, tout en offrant la possibilité de modifier ce dérnier. Il existe aussi des travaux qui proposent de visualiser sur un plan à deux dimensions la liste d'items à recommander pour un utilisateur actif, en utilisant les techniques classiques telles que l'ACP, MDS, SOM.
Dans ce travail nous proposons une nouvelle approche de visualisation dans le contexte des systèmes de recommandations. Contrairement aux autres méthodes citées ci-dessus, notre approche est globale, c'est à dire qu'elle ne se focalise pas uniquement sur l'utilisateur actif. Elle exploite la dualité inhérente de la classification croisée pour mieux mettre en évidence les affinités entre certains types de groupes d'utilisateurs et certains types de produits. Plus pré-cisément, nous proposons de représenter les relations de préférences entre des groupes d'utilisateurs et groupes d'items, au moyen des graphes bipartis. Notre approche peut être décrite comme suit : 1) Classifier la matrice utilisateur-item en K classes d'utilisateurs et L classes d'items. Dans nos expérimentations nous avons utilisé COCLUST, après la gestion des données manquantes, présentée dans la section précédente, 2) Construire une matrice résumant les résultats de la classification croisée, dans laquelle chaque groupe de de lignes et chaque groupe de colonnes est représenté par les utilisateurs et les items les plus populaires (qui ont le plus de votes) respectivement, 3) Calculer la relation de préférence entre chaque groupe d'utilisateurs et chaque groupe d'items, à l'aide de la formule (4), 4) Construire le graphe biparti, étape décrite en détail dans la partie expérimentale.
Soit U = (u ij ) la matrice résumée de l'étape 2, avec n utilisateurs et p items. Et soit E = (e ij ) une matrice binaire de n × p, tel que e ij = 1 si l'utilisateur i aime l'item j et e ij = 0 sinon. Alors la corrélation entre la k 
Intuitivement la corrélation (de préférence) 4, entre un groupe d'utilisateurs k et un certain groupe d'items représente la proportion des items populaires dans la i` eme classe ayant été appréciée par les utilisateurs les plus populaires de la classe k. La section suivante présente les résultats expérimentaux démontrant l'efficacité des approches proposées.
Algorithm 3 Bipartite procedure.
Input : U, K and L ; Output : C : correlation matrix between clusters ; Steps : 1. Compute (Z, W) into K row clusters and L column clusters ; 2. Compute U with the relevant users and items. for k = 1 to K do for l = 1 to L do 4. Compute C = (c k ) the correlation matrix between clusters, by using (4) end for end for 5. Build the bipartite graph
Résultats expérimentaux
Dans nos expériences, nous avons choisi les deux jeux de données de MovieLens 1 (ML-100K et ML-1M) qui sont beaucoup utilisés dans le domaine. L'échantillon ML-1M est constitué de 6040 utilisateurs, 3952 films, et de 1 million de notes observées. L'ensemble ML-100K contient 100,000 notes fournies par 943 utilisateurs pour 1664 films. La proportion des notes observées dans ce dernier est seulement de 6, 4%. Les évaluations des utilisateurs (u ij ) appartiennent à l'intervalle : [1; 5], et les notes manquantes sont codées par : NA. Les données MovieLens fournissent également certaines informations démographiques sur les utilisateurs, telles que : le sexe, l'âge, la profession, code postal ; et des informations de base sur les films tels que : le titre, le genre, la date de sortie, etc. A noter qu'un film peut être de plusieurs genres à la fois. Nous proposons dans la suite de réaliser la comparaison des courbes ROC et de la F-measure, des systèmes de FC suivants : COCLUST, le FC incrémental basé sur la décompo-sition en valeurs singulières SVDCF (Sarwar et al., 2002), et COCLUST++ (COCLUST après la gestion des valeurs manquantes). Ces comparaisons sont réalisées sous recommenderlab (Hahsler, 2011), que nous avons combiné avec le langage C pour implémenter les différentes méthodes ci-dessus. Les courbes de figure. 4a sont construites en faisant varier le nombre d'items à recommander de 1 à 40. Les deux figures 4a et 4b montrent une amélioration significative des performances de COCLUST, grâce à la gestion des données manquantes que nous proposons. On remarque aussi, une faible qualité des recommandations pour SVDCF, qui est due à une gestion des données manquantes inappropriée. En effet dans cette dernière approche (Sarwar et al., 2002), les notes manquantes sont remplacées par les moyennes des items dont les estimations sont fortement biaisées. En d'autres termes cette imputation favorise les items avec très peu de notes observées, comme illustré dans la section 3 (figure 3). La figure 4d montre que même avec l'étape d'imputation COCLUST++ reste plus rapide que SVDCF.
En ce qui concerne les possibilités de visualisation exploitant la classification croisée, la figure 5 montre un exemple de graphe biparti, qui est construit comme suit 1) Classification de l'ensemble ML-100k, en 6 classes utilisateurs et 8 classes d'items, en utilisant COCLUST++, 2) Calculer les corrélations entre les groupes d'utilisateurs et d'items, via la formule (4), 3) Construire le graphe biparti où les rectangles de gauche représentent des groupes d'utilisateurs, tandis que ceux de droite des groupes d'items. Seuls les liens qui correspondent à de fortes corrélations sont représentés. Pour chaque groupe d'utilisateurs, les deux professions les plus populaires sont présentées, de même les deux genres les plus populaires dans chaque classe  
Conclusion
Dans ce papier nous avons proposé une meilleure exploitation du potentiel de la classification croisée dans les systèmes de FC. Pour ce faire, nous avons développé une nouvelle stratégie pour une gestion efficace des données manquantes. Nous avons ensuite proposé une nouvelle approche interactive basée sur des graphes bipartis, permettant d'interpréter et de comprendre les résultats de la classification croisée dans le contexte du FC. Les résultats expé-rimentaux montrent une amélioration importante des performances de la classification croisée dans le FC, grâce à une meilleure gestion des notes manquantes. Nous avons aussi montré, comment les représentations interactives basées sur des graphes bipartis peuvent aider les uti-

Introduction
Dans le contexte d'une fouille exploratoire, le recours à des techniques de réduction de dimensionnalité permet classiquement de contourner la difficulté de représenter des résultats de clustering réalisés sur des données à haute dimensionnalité (HD). Les étiquettes de clusters, associées par exemple à des couleurs catégorielles, peuvent alors être appliquées aux points d'un nuage 2D ou 3D.
Les techniques de réduction de dimensionnalité sont susceptibles d'introduire des artéfacts de déchirement et de recollement (Aupetit, 2007). Les algorithmes de clustering ne sont pas sujets à ces artéfacts, mais peuvent mener à des résultats sous-optimaux, ou avoir été mal paramétrés. L'objet de cet article est de proposer un outil interactif de fouille visuelle combinant le meilleur de ces deux approches. Il utilise une projection 2D obtenue par t-SNE, une technique de réduction de dimensionnalité non-supervisée (van der Maaten et Hinton, 2008). Nous ne proposons pas un algorithme de clustering per se, mais plutôt une manière itérative d'amélio-rer conjointement un clustering initial calculé de manière non-supervisée dans un espace HD, et une représentation 2D associée.
Une présentation générale de notre outil est proposée en section 2. Les clusters sont amendés grâce à des techniques de diffusion d'étiquettes, présentées en section 3. Réciproquement, l'adaptation de la projection 2D aux clusters est évoquée dans la section 4. Les exemples donnés en section 5 et tout au long de cet article utilisent le jeu de données COIL-20 (Nene et al., FIG. 1 -Diagramme résumant la logique de l'outil. 1996). Il contient 1440 images, réparties en 10 classes. Les images sont décrites par les intensités de leurs 1024 pixels sur une échelle de gris.
Présentation de l'outil
La logique de l'outil est résumée dans la figure 1. Il est paramétré par un jeu de données HD (i.e., > 3), et par un clustering non-supervisé réalisé dans l'espace HD.
Une projection 2D initiale est calculée de manière non-supervisée par l'algorithme itératif t-SNE. Elle est matérialisée par un nuage de points, dont les colorations sont associées aux étiquettes de clusters via un ensemble de couleurs catégorielles. À partir d'une projection et d'un clustering donnés, l'utilisateur peut déclencher les actions suivantes :
-Sélection d'une restriction : optionnellement, l'utilisateur peut limiter le rayon de son action à un ensemble de clusters sélectionnés directement en cliquant la légende (voir Figure 2e). -Sélection de pivots : en cliquant sur un des points dans le nuage (voir Figure 2d), l'utilisateur définit un pivot pour la diffusion d'une nouvelle étiquette de cluster. Un inspecteur interactif est à sa disposition pour associer une sémantique à chaque pivot (voir Figure 2c). -Diffusion d'étiquettes : les étiquettes des pivots sélectionnés sont diffusées en utilisant la proximité 2D entre éléments. -Modification des dissimilarités : la répartition en clusters peut être utilisée pour influencer les dissimilarités sous-jacentes à l'algorithme t-SNE. L'utilisateur peut paramétrer le niveau de cet impact. Les actions de l'utilisateur modifient la visualisation et les clusters, dont le nouvel état peut servir d'entrée à une nouvelle itération du diagramme en figure 1. Des itérations sont effectuées jusqu'à ce que l'utilisateur soit satisfait du résultat. fastidieuses, nous proposons une diffusion semi-automatique basée sur la sélection de pivots par l'utilisateur. La diffusion peut être calculée selon deux algorithmes issus de la littérature : -Propagation probabiliste : les étiquettes peuvent métaphoriquement sauter de manière probabiliste depuis les pivots associés, puis d'élément en élément. Ce processus converge, et le résultat peut être obtenu sous une forme analytique impliquant de simples produits de matrices (Zhu et Ghahramani, 2002). -Coupes de l'arbre de couverture minimal : l'arbre de couverture minimal d'un graphe peut être calculé grâce à l'algorithme de Kruskal (Kruskal, 1956). Des coupes dans cet arbre isolent des composantes connexes du graphe.
Diffusion d'étiquettes
Ces opérations sont réalisées relativement à la distribution visuelle des éléments ; les distances 2D entre éléments dans la projection sont donc utilisées dans les algorithmes.
L'utilisateur commence par sélectionner un ou plusieurs pivots dans la restriction en cours (qui peut englober tous les éléments si aucun cluster n'a été sélectionné dans la légende). Ces derniers peuvent être vus comme des prototypes de clusters, existants et à redécouper, ou à créer. Selon ses préférences, l'utilisateur peut alors les propager exhaustivement, ou paramé-trer interactivement la coupe de l'arbre de couverture minimal pour isoler des composantes connexes. Dans ce dernier cas, il peut aussi choisir de regrouper les composantes sans pivot dans un cluster résiduel, ou laisser leur étiquette telle qu'avant l'interaction.
Modification des dissimilarités
Plutôt que d'utiliser la distribution 2D des éléments pour modifier les clusters comme dans la section 3, l'utilisateur peut utiliser la répartition en clusters pour modifier la projection 2D, de manière par exemple à renforcer la séparation des clusters.
Considérons le graphe complet entre les éléments dans la restriction en cours, pondéré par les dissimilarités dans l'espace HD. Nous voulons utiliser l'information portée par les clusters pour amender les dissimilarités HD entre éléments. Pour préserver la structure interne des clusters, nous proposons de restreindre la modification au sous-graphe multipartite induit par les clusters. La fonction cumulative normalisée de la distribution Beta est alors appliquée aux poids d'arêtes associés :
Les bornes a et b permettent d'adapter la transformation aux valeurs de dissimilarité, e.g., à la valeur maximale observée dans la restriction, ou à la cohésion interne des clusters. Les changements trop disruptifs sont ainsi évités. L'utilisateur peut alors paramétrer interactivement un rapprochement (respectivement un éloignement) des clusters en augmentant le paramètre ? (respectivement ?).
L'algorithme t-SNE se base sur des dissimilarités HD entre les éléments pour estimer leurs positions dans le nuage de points. Classiquement, ces dissimilarités sont initialisées par une distance Euclidienne dans l'espace HD. Dans l'outil, les dissimilarités peuvent être modifiées dynamiquement.
L'algorithme t-SNE peut être interprété comme une variante d'algorithme force et ressort. La métaphore physique suivie par cette classe d'algorithmes convertit des changements discontinus des forces en présence en mouvements continus. Ainsi, la discontinuité obtenue à l'application de l'équation (1) est convertie en mouvements continus, facilitant leur suivi par un utilisateur. Après une modification de dissimilarités, ce dernier peut alors suivre le changement progressif induit par son action.
Les dissimilarités HD demeurent latentes à la projection 2D, et ne peuvent pas être observées directement dans la visualisation. Pour pallier cette limitation, nous avons incorporé l'outil ProxiViz (Heulot et al., 2012), qui permet de mapper interactivement les dissimilarités sur le diagramme de Voronoï du nuage de points (voir Figure 3). L'utilisateur dispose ainsi d'une information plus complète avant de procéder à ses modifications. Ceci peut par exemple permettre de prendre en compte d'éventuels artéfacts de projection (Aupetit, 2007).
Exemples
Au cours de ses manipulations avec l'outil, l'utilisateur est confronté à la situation de la figure 4a. Le cluster vert, identifié par l'algorithme de clustering dans l'espace HD, est éclaté en 3 composantes dans la visualisation. L'utilisateur souhaite les regrouper, en respectant le voisinage des composantes dans la projection.
Il commence par vérifier la pertinence d'un tel regroupement en utilisant l'outil ProxiViz. Les composantes du cluster vert sont bruitées par les clusters violet et orange. Il commence FIG. 3 -Le survol du nuage de points déclenche l'outil ProxiViz. Les dissimilarités HD par rapport au point de la cellule survolée sont mappées sur une échelle de gris, et colorent les cellules de Voronoï des éléments respectifs. Les étiquettes de clusters sont rappelées en colorant le contour des points, ainsi que la cellule du point survolé.
FIG. 4 -a)
Le cluster vert est réparti en 3 composantes, bruité par les clusters violet et orange. Un pivot est sélectionné pour chaque composante. b) L'exécution d'itérations de t-SNE sur la restriction en cours ne réunit que partiellement le cluster. c) Après mise à jour des dissimilarités, le cluster est effectivement regroupé. donc par définir une restriction à ces 3 clusters, et sélectionne des pivots pour isoler les composantes (voir Section 3). Les composantes sont ensuite réunies simplement en éditant la légende interactive.
En déclenchant t-SNE sur la restriction en cours, le cluster est partiellement réuni (voir Figure 4b). L'utilisateur influence leur rapprochement en mettant à jour les dissimilarités (voir Section 4 et Figure 4c).
Conclusion
Dans cet article, nous avons exposé notre outil de clustering visuel et interactif. Les techniques de diffusion d'étiquettes et de modification des dissimilarités permettent d'enrichir mutuellement une projection 2D et un clustering itérativement mis à jour. La métaphore physique suivie par le nuage de points et les moyens de contrôle offerts par l'interface permettent à l'utilisateur de suivre le changement progressif induit par ses actions. Nous avons illustré l'intérêt de l'approche au travers d'exemples.

Introduction
La découverte de motifs locaux introduite par Agrawal et Srikant (1994) consiste à extraire des informations pertinentes décrivant une portion des données. Evaluer et garantir la qualité des motifs extraits demeure une problématique très ouverte malgré le nombre important de propositions (Giacometti et al., 2013). Chacune de ces propositions repose explicitement ou implicitement sur une mesure d'intérêt dont la qualité dépend de la complexité du modèle sous-jacent et de son ajustement aux données. Le modèle repose en général sur des fondements statistiques dont la complexité et la compréhension sont bien connues. A l'inverse, l'ajustement aux données reste une notion difficile à appréhender. Pourtant, c'est probablement cette notion qui distingue la fouille de données des statistiques traditionnelles. L'ajustement aux données est souvent connoté négativement et synonyme de sur-apprentissage par rapport aux données. De notre point de vue, l'ajustement aux données n'est pas un biais d'apprentissage mais un moyen pour lever certaines hypothèses sur le modèle en les remplaçant par des mesures sur les données. Nous proposons d'étudier l'ajustement aux données à travers les interrelations entre motifs lors de l'évaluation d'une mesure d'intérêt ou d'une contrainte d'extraction.
La qualité d'une mesure repose sur sa capacité à isoler un motif singulier qui dévie des autres motifs communs. Pour cette raison, une mesure se doit de mettre en relation le motif évalué avec d'autres motifs, dits motifs liés. Par exemple, la confiance de la règle d'association X ? Y met en relation la fréquence de X ? Y (motif évalué) par rapport à la fréquence de X (motif lié). La qualité de la règle augmente avec la fréquence de X ? Y tandis qu'elle diminue si la fréquence de X augmente (lorsque les autres fréquences restent constantes). Ces variations de la confiance sont en partie conformes aux deux axiomes formulés par PiatetskyShapiro (1991). De manière intéressante, ces axiomes permettent d'étudier formellement le comportement des mesures d'intérêt dédiées à l'évaluation des règles d'association. Dans cet article, nous proposons de généraliser ce principe en introduisant la notion de motif lié pour s'attaquer à l'évaluation de n'importe quelle méthode de découverte de motifs.
L'objectif de ce travail est de formaliser la qualité et la sémantique des mesures d'intérêt et contraintes en analysant les interrelations entre les motifs nécessaires à l'évaluation de chaque motif extrait. Ce travail s'inscrit dans la lignée des travaux sur l'analyse des propriétés formelles vérifiées par les mesures d'intérêt Piatetsky-Shapiro (1991); Tan et al. (2004); Geng et Hamilton (2006); Lenca et al. (2008); Hämäläinen et al. (2010) en les étendant aux contraintes d'extraction. Nous formaliserons l'interrelation entre motifs en introduisant l'ensemble de motifs liés. Cet ensemble regroupe tous les motifs susceptibles d'impacter l'évaluation d'un motif donné. Nous distinguerons les motifs liés positivement qui permettent d'accroître la mesure d'intérêt de ceux qui la font décroître, i.e., les motifs liés négativement. Nous formulerons alors trois axiomes que devraient satisfaire une mesure d'intérêt ou contrainte. Chacun de ces axiomes impose des contraintes topologiques que doivent respecter les deux ensembles de motifs liés. Enfin, nous proposerons des critères d'analyse de la complexité et de la sémantique d'une mesure d'intérêt ou contrainte. Nous introduirons finalement la complexité en évaluation qui repose sur la cardinalité de l'ensemble des motifs liés.
Travaux relatifs
A notre connaissance, très peu de travaux se sont intéressés à l'interrelation entre les motifs lors de l'évaluation d'une mesure d'intérêt ou d'une contrainte. De manière plus générale, l'évaluation de la qualité des méthodes de découverte de motifs est une tâche ardue et peu étudiée.
Protocoles expérimentaux
La plupart des méthodes d'évaluation ou d'analyse de la qualité concernant la découverte de motifs repose sur des protocoles expérimentaux où l'objectif est de vérifier la conformité du résultat avec un étalon-or. Dans un contexte supervisé, il est possible d'exploiter directement la variable cible comme référence. Ensuite, le cadre précision/rappel, l'analyse ROC (Fawcett, 2006), la validation croisée (Kohavi, 1995), etc sont utilisés pour évaluer l'écart entre les motifs extraits et la référence. L'évaluation de la qualité des méthodes de découverte de motifs dans un contexte non-supervisé s'avère bien plus difficile comme le rappellent de Lin et Chalupsky (2004). En effet, la validation des motifs extraits ne peut pas s'appuyer sur un étalon-or explicite. Plusieurs stratégies sont alors mises en oeuvre pour obtenir un succédané de cet étalon-or.
Premièrement, il est possible de s'appuyer sur la connaissance d'experts d'un domaine (Carvalho et al., 2005). Avec un cas d'utilisation, des experts du domaine sont sollicités pour juger la justesse des motifs découverts et ainsi, évaluer la méthode d'extraction. La stratégie de redécouverte teste si un processus parvient à retrouver les connaissances bien établies dans un certain domaine. Deuxièmement, il est parfois possible de construire l'étalon-or (Gupta et al., 2008;Zimmermann, 2013). Par exemple, Gupta et al. (2008) propose un protocole d'évaluation quantitative des algorithmes d'extraction de motifs approximés fréquents : (1) l'extraction des motifs fréquents dans un jeu de données classique, (2) l'ajout de bruit dans ce jeu de données, (3) l'extraction des motifs approximés fréquents et enfin, (4) la comparaison des véritables motifs fréquents avec les motifs approximés fréquents. Ici, l'étape 1 explicite l'étalon-or. Enfin, l'hypothèse nulle peut parfois être construite expérimentalement. Gionis et al. (2007)  Ces protocoles expérimentaux sont clairement utiles, mais néanmoins ils souffrent de plusieurs limites. Tous ces protocoles expérimentaux reposent sur l'évaluation de la collection de motifs extraits. Ils ne peuvent donc fournir qu'un résultat a posteriori, i.e., après l'implémen-tation d'un prototype et de son application sur un jeu de données. L'évaluation est forcément dépendante du jeu de données considéré et les résultats sont difficilement généralisables à n'importe quel jeu de données, de n'importe quel domaine.
Outils formels
Plusieurs outils formels ont été proposés dans la littérature pour analyser qualitativement les méthodes de découverte de motifs. Premièrement, la taille des représentations condensées est souvent utilisée comme une mesure objective d'évaluation de leur intérêt (Calders et al., 2004). Par exemple, une représentation condensée fondée sur les motifs fermés est toujours plus compacte qu'une représentation condensée fondée sur les motifs libres. Les motifs fermés sont donc jugés comme plus intéressants. Cependant, les représentations condensées les plus compactes ne sont pas forcément les plus utilisées. Par exemple, les itemsets non-dérivables (NDI) sont rarement utilisés malgré leur taux de compression impressionnant. La sémantique complexe des NDI expliquerait cette impopularité pour certaines personnes.  (2010) les ont étendus aux motifs ensemblistes. A notre connaissance, de tels axiomes n'ont jamais été appliqués à des contraintes ou des algorithmes de construction de modèles. De plus, ils se sont essentiellement concentrés sur les mesures dédiées à la recherche de corrélations. Comment généraliser ces axiomes à n'importe quelle mesure d'intérêt ou contrainte ?
Enfin, à notre connaissance seuls deux travaux ont étudié l'interrelation entre les motifs lors de l'évaluation d'une mesure d'intérêt. Crémilleux et Soulet (2008) ont défini informellement la notion de contrainte globale. Il s'agit de contraintes dont l'évaluation met en correspondance plusieurs motifs. Giacometti et al. (2011) ont ensuite formalisé cette notion de contrainte globale en utilisant une algèbre relationnelle étendue spécifiquement pour la découverte de motifs. Notre cadre propose une définition formelle plus générale et plus précise, mais surtout permet de mieux analyser l'interrelation entre les motifs lors de l'évaluation. En particulier, nous ré-pondrons aux deux questions énoncées ci-avant.
3 Formalisation de l'interrelation entre motifs Fu et al., 2000) Bouncer and Picker Algorithme de sélection avec différentes heuristiques (Bringmann et Zimmermann, 2009) TAB. 1 -Définition de plusieurs méthodes de découverte de motifs fondées sur la fréquence
Méthode de découverte de motifs
Dans cet article, nous modélisons une méthode de découverte de motifs par une mesure
-le langage L correspond à l'ensemble des parties de I (i.e., L = 2 I ) 1 et -? est l'ensemble de tous les jeux de données possibles sachant qu'un jeu de données est un multi-ensemble de L.
Si le jeu de données visé est clair, M (X, D) est simplement noté M (X). Sans perte de géné-ralité, nous considérons que l'intérêt d'un motif X augmente avec M (X) i.e., si le motif X est plus intéressant que
Notre modélisation d'une méthode de découverte de motifs par une mesure d'intérêt M est suffisamment souple pour englober les principaux outils de la littérature :
-Evaluation par mesure d'intérêt : l'évaluation par mesure d'intérêt consiste à affecter un score ou un rang à chaque motif reflétant sa qualité (e.g., la all-confidence proposée par Omiecinski (2003)). -Extraction sous contraintes : l'extraction sous contraintes consiste à extraire tous les motifs satisfaisant un prédicat de sélection qui détermine la pertinence d'un motif. Par exemple, il est courant d'utiliser un seuil minimal sur une mesure d'intérêt pour filtrer les motifs (e.g., la contrainte de support minimal (Agrawal et Srikant, 1994) 
Notion de motifs liés
La qualité d'une méthode de découverte de motifs repose sur sa capacité à isoler un motif singulier qui dévie des autres motifs communs. Pour cette raison, cette méthode doit mettre en relation le motif évalué avec d'autres motifs, dits liés. Pour analyser cette interrelation entre motifs, nous proposons de déterminer son ensemble de motifs liés en identifiant l'impact de chacun de ces motifs sur le motif évalué. L'impact d'un motif Y sur le motif évalué X peut se mesurer en observant s'il existe deux jeux de données D et D quasi-équivalents où la seule variation de Y modifie l'évaluation de X. Typiquement, la all-confidence d'un itemset X introduite par Omiecinski (2003) correspond à la plus petite confiance des règles d'association Y ? Z incluses dans X. Il est bien connu que la all-confidence peut être réécrite comme le ratio entre la fréquence de X et la fréquence maximale de ses items : f req(X)/ max i?X f req({i}). Dans ce cas, quand X est évalué, les motifs liés de X sont luimême et tous ses items. En effet, augmenter la fréquence d'un item peut faire décroître la allconfidence de X. A l'inverse, augmenter la fréquence de X augmente aussi sa all-confidence. Cet exemple conduit à deux observations d'importance :
1. Il y a deux catégories de motifs liés : ceux qui peuvent améliorer la qualité du motif évalué (ici, X) et ceux qui peuvent la détériorer (ici, les items qui constituent X).
2. Les motifs liés impactent la mesure d'intérêt qu'on analyse (ici, la all-confidence) via une autre mesure d'intérêt élémentaire (ici, la fréquence).
Suivant ces deux observations, nous définissions formellement la notion de motif lié en nous appuyant sur la définition d'équivalence avec exception :  Illustrons la définition 2 avec la all-confidence. Comme la all-confidence ne peut croître qu'avec la fréquence de X, all-conf + f req (X) est égal à {X}. Nous verrons qu'il est courant voire souhaitable que le motif évalué soit aussi un motif lié. Pour l'ensemble des motifs liés négativement, nous obtenons que all-conf ? f req (X) = {{i}|i ? X} car seuls l'augmentation de la fréquence d'un item de X peut faire diminuer all-conf (X). Le tableau 2 donne d'autres exemples d'ensembles de motifs liés. Une force de la notion de motifs liés est de bien identifier les motifs « réellement » impliqués dans l'évaluation. Par exemple, la définition des motifs libres donnée dans le tableau 1 implique tous les sous-ensembles de X (avec le ?Y ? X). Pourtant, seuls les sous-ensembles directs sont des motifs liés.
Dans le tableau 2, les mesures, contraintes, algorithme de construction de modèles choisis reposent exclusivement sur la fréquence (comme mesure m qui implique les motifs liés). Il est à noter que les définitions de motifs libres/fermés auraient pu être présentées avec d'autres mesures élémentaires (e.g., fréquence disjonctive ou fonction d'agrégat). Les bordures néga-tive et positive pourraient être analysées avec d'autres mesures suivant la contrainte monotone ou anti-monotone considérée (Mannila et Toivonen, 1997). De même, la notion de top-k motif est pertinente avec d'autres mesures d'intérêt que la fréquence. En changeant la mesure élé-mentaire m introduite dans la définition 2, l'analyse des interrelations entre motifs se ferait de manière analogue.
où k = |X| et n = |I| ; singletons correspond à {{i}|i ? X} ; sous-ensembles directs correspond à {X \ {i}|i ? X} ; sur-ensembles directs correspond à {X ? {i}|i ? I \ X} ; sous-ensembles correspond à 2 X \ {X} ; treillis correspond à L \ {X}.
TAB. 2 -Analyse des méthodes suivant leurs ensembles de motifs liés
Axiomes de qualité
En s'inspirant de ce qui a été fait pour les mesures d'intérêt dédiées aux règles d'association, cette section énonce trois axiomes que devraient satisfaire une mesure ou une contrainte idéale. Le tableau 2 illustre la satisfaction ou non des axiomes par les différentes mesures et contraintes.
Réflexivité
Revenons sur l'exemple de la all-confidence. Nous avons constaté que l'augmentation de la fréquence de certains motifs avait un impact sur la all-confidence de X. Donc la fréquence est une mesure élémentaire d'importance pour la all-confidence. Par ailleurs, il est souvent considéré que l'intérêt d'un motif augmente avec sa fréquence. Il paraît donc naturel qu'augmenter la fréquence de X augmente également la all-confidence de X. Plus généralement, si l'intérêt d'un motif X augmente avec la mesure élémentaire m, l'intérêt de ce motif X selon la mesure d'intérêt M devrait également augmenter lorsque m(X) croît. Comme all-conf + f req (X) = {X}, la all-confidence est bien une mesure réflexive par rapport à la fréquence. A l'inverse, l'extraction des motifs libres n'est pas réflexive par rapport à la fréquence puisque f ree + f req (X) = {X \ {i}|i ? X} n'inclut pas X. Tandis qu'un motif est jugé plus intéressant quand sa fréquence augmente, il a moins de chance d'être libre (car sa fréquence sera plus proche de celle de ses sous-ensembles). Par conséquent, la contrainte de liberté pourrait même être qualifiée d'irréflexive par rapport à la fréquence (i.e., X ? f ree ? f req (X)). Nous reviendrons dessus dans la sous-section suivante. L'axiome 1 est une généralisation de plusieurs propositions de la littérature où la mesure m est restreinte au support. Pour les règles d'associations, Piatetsky-Shapiro (1991)  
Exclusivité
Pour être facilement compréhensible par l'utilisateur final, le comportement d'une mesure M doit toujours rester le même vis-à-vis de chaque motif lié. En d'autres termes, un motif lié ne devrait pas permettre d'augmenter la mesure M dans certains cas, et de la diminuer dans d'autres.
Axiome 2 (Exclusive) Une mesure d'intérêt M est exclusive par rapport à m ssi aucun motif est à la fois lié positivement et négativement à un motif donné pour m :
Cet axiome est largement vérifié par les méthodes de la littérature comme le montre le tableau 2 (colonne A2). Par exemple, comme all-conf + f req (X) ? all-conf ? f req (X) = ?, la allconfidence est exclusive par rapport à la fréquence. A l'inverse, l'extraction des motifs libres fréquents n'est pas exclusive puisque X appartient à la fois à f ree&f req + f req (X) à cause de la contrainte de fréquence et à f ree&f req ? f req (X) à cause de la contrainte de liberté. Le non-respect de l'axiome 2 complexifie la lecture d'une méthode d'extraction puisqu'il devient nécessaire de se reporter au jeu de données ou à d'autres motifs pour comprendre les motifs extraits. Par exemple, lors de l'extraction des motifs libres fréquents, un motif X peut ne pas être extrait soit si sa fréquence est trop basse, soit si sa fréquence est trop élevée. A l'inverse, pour les motifs fermés fréquents, un motif n'est pas extrait si sa fréquence est trop faible (aussi bien si le motif est non-fréquent ou non-fermé). Nous estimons donc que la violation de l'axiome 2 pourrait expliquer en partie l'échec des motifs NDI même s'ils constituent une représentation condensée extrêmement compacte.
La combinaison des axiomes 1 et 2 (notée A1+2 dans le tableau 2) implique naturellement que X ? M ? m (X). Si une mesure M viole cette propriété, M est dite irréflexive selon m. L'extraction des motifs libres et celle de la bordure négative des motifs fréquents sont irréflexives.
Exhaustivité
La pertinence d'un motif est d'autant plus forte que son intérêt dépend de la variation de nombreux autres motifs. Pour cette raison, tous les motifs du langage L devraient avoir un impact sur la mesure M . En d'autres termes, l'ensemble des motifs liés devrait idéalement être égal à la totalité du langage L.
Axiome 3 (Exhaustive) Une mesure d'intérêt M est exhaustive par rapport à m ssi tous les motifs du langage sont liés à tout motif pour m :
Cet axiome exprime que l'interrelation entre les motifs lors de l'évaluation d'une mesure M devrait concerner tous les motifs. Chaque variation du jeu de données mesurable à travers m(X) devrait avoir une incidence sur l'évaluation de M . Bien sûr, la plupart des méthodes d'extraction (mesures ou contraintes de la littérature) ne satisfont pas cet axiome. Cependant, nous pensons que cet axiome donne la direction à suivre et nous revenons longuement dans la section suivante sur la forme de l'ensemble de motifs liés.
Complexité et sémantique
En pratique, l'axiome 3 est peu vérifié. Néanmoins, les méthodes d'extraction proposées tendent plus ou moins à le satisfaire. Cette section propose d'étudier deux grandes caractéris-tiques de l'ensemble des motifs liés à savoir sa taille et sa forme.
Complexité en évaluation
Suivant l'axiome 3, nous affirmons que la pertinence d'un motif pour une mesure élémen-taire m est encore plus forte lorsque sa pertinence dépend de la variation de la pertinence de nombreux autres motifs selon m. Par conséquent, l'intérêt d'une mesure M (au sens de sa globalité) selon m se mesure avec la taille de l'ensemble de motifs liés : De manière similaire, on peut déterminer que la complexité en évalua-tion de la productivité est exponentielle par rapport à la taille du motif évalué puisque tous les sous-ensembles sont impliqués dans l'évaluation de cette contrainte. Suivant la complexité en évaluation, on dira donc que la productivité est plus intéressante (car plus globale) que la all-confidence car les interrelations sont plus nombreuses.
A notre connaissance, la complexité en évaluation est le premier indicateur pour mesurer l'interrelation entre les motifs lors de l'évaluation d'une mesure d'intérêt. Cette complexité permet de comparer plusieurs mesures d'intérêt entre elles. La colonne |M ± f req (X)| du tableau 2 indique la complexité en évaluation des mesures et contraintes définies dans le tableau 1. Il se dégage clairement 3 grandes classes correspondant à 3 complexités en évaluation : constant, linéaire et exponentiel.
Bien que le tableau 1 ne comporte qu'un échantillon restreint de la découverte de motifs, la complexité en évaluation des méthodes d'extraction de motifs semble avoir augmenté durant ces deux dernières décennies. Au-delà de l'intérêt des motifs extraits, nous pensons que la complexité en évaluation reflète aussi la difficulté algorithmique à les extraire. Ainsi, l'amé-lioration des techniques d'extraction pourrait expliquer cette augmentation de la qualité des motifs extraits.
TAB. 3 -Liens entre l'ensemble des motifs liés, la sémantique et la complexité
Sémantique de l'ensemble de motifs liés
Contrainte globale Le tableau 3 schématise les principales topologies observées notamment au sein du tableau 2 en les organisant en trois grandes classes de complexité évoquées dans la section précédente. Ces classes font écho à la notion de contrainte globale introduite par Crémilleux et Soulet (2008) puis définie formellement par Giacometti et al. (2011). Il s'agit des prédicats de sélection dont la complexité est au moins linéaire :
Propriété 1 (Contrainte globale) Une contrainte q : L ? {1, 0} est globale ssi il existe une mesure élémentaire m telle que la complexité en évaluation de q selon m est au moins linéaire. Propriété 2 (Webb et Vreeken (2013); Hämäläinen et al. (2010)) Une mesure d'intérêt M se comportant bien doit vérifier ?X ? L : 2 X \ {X} ? M ? f req (X) Non-redondance Toutes les méthodes de découverte de motifs visant à réduire les redondances exploitent les sous-et/ou sur-ensembles du motif évalué. La majorité des représenta-tions condensées s'appuient exclusivement sur les sous-ensembles ou sur-ensembles directs. Modèle Nous avons constaté que tous les algorithmes de construction de modèles ont leurs motifs liés qui couvrent l'intégralité du treillis comme c'est le cas pour Bouncer and Picker (Bringmann et Zimmermann, 2009). Les modèles sont souvent vus comme une amélioration des représentations condensées. La complexité en évaluation confirme que les modèles sont plus intéressants que les représentations condensées. La complexité des motifs top-k fréquents se rapproche de celle des modèles sans toutefois l'atteindre.
2. Dans ce contexte, « un bon comportement » signifie que les corrélations doivent être évaluées plus favorablement que les non-corrélations.
Conclusion
Cet article a introduit la notion de motifs liés qui nous semble centrale pour analyser l'interrelation des motifs pour les méthodes de découverte de motifs. Une force de notre approche est son large spectre d'application qui va au-delà des mesures d'intérêt pour traiter aussi bien l'extraction sous contraintes que la construction de modèles. Pour la première fois, des axiomes de qualité concernent la problématique de la non-redondance. L'introduction de la complexité en évaluation permet de dépasser le stade qualitatif pour mieux comparer plusieurs méthodes.
Plusieurs axes de progression subsistent au sein de notre cadre. La définition actuelle des motifs liés repose sur une notion d'équivalence entre jeux de données où seule une mesure élé-mentaire m est impliquée dans l'évaluation de M ; comment tenir compte qu'une autre mesure m peut potentiellement impacter M en parallèle ? Une réflexion sur la définition de mesure élémentaire et des interrelations entre mesures élémentaires est nécessaire pour répondre à cette question. Par ailleurs, d'autres axiomes importants sur les mesures d'intérêts mériteraient d'être étendus en s'appuyant sur la notion de motifs liés. Bien que l'incidence de la mesure élémentaire m dans l'évaluation de M soit cruciale sur la sémantique des motifs liés, nous n'avons pas encore étudié les implications des propriétés de m sur celles de M .

Introduction et contexte
Le problème de la prédiction de liens tel qu'il est formulé dans l'article de Liben-Nowell et al. (2007) peut être compris comme une tâche de classification binaire. Des outils classiques d'apprentissage tels que les arbres de classification, les SVM ou les réseaux de neurones ont été utilisés pour le résoudre sur des réseaux biologiques et de collaboration (Pujari et al. (2012)). Cependant, ces méthodes ne permettent pas à l'utilisateur de faire varier le nombre de prédic-tions selon ses besoins. Pour ce faire, il est possible de calculer un score pour chaque paire de noeuds, corrélé à la probabilité d'existence d'un lien entre ces noeuds, on obtient alors un classement, et l'utilisateur effectue la prédiction en sélectionnant les T paires les mieux classées. Le score peut être basé sur la structure connue du réseau, mais également sur d'autres sources d'information : par exemple les attributs des noeuds, la dynamique des contacts ou la localisation géographique (Scellato et al. (2011)). Pour combiner les informations capturées par différents scores, on utilise des méthodologies d'apprentissage de classements. Parmi les méthodes à disposition, certaines sont non-supervisées et peuvent être vues comme des mé-thodes de consensus, telles que celles décrites dans Dwork et al. (2001). Il existe également des méthodes supervisées : une solution consiste à se ramener à un problème de classification en effectuant une transformation deux-à-deux, plutôt que de considérer des éléments à ordonner, on examine des couples d'éléments dont on cherche à dire lequel doit être classé audessus de l'autre (Herbrich et al. (1999)). Malheureusement, cette méthode n'est pas adaptée à de grands réseaux où le nombre d'éléments à classer est élevé. Plus généralement, la plupart des méthodes d'apprentissage de classements ont été créées pour des tâches de recherche d'information, où l'on souhaite une précision élevée sur un petit nombre d'éléments (e.g., Burges et al. (2011)). Nous souhaitons ici au contraire pouvoir fixer le nombre de prédictions, quitte à perdre en précision, pour prédire des liens non-observés dans de grands réseaux sociaux, et nous définissons dans ce but une méthodologie simple mais efficace d'apprentissage supervisé. 2 Classements non-supervisés Scores de classement. Le but de ce travail est de montrer comment notre méthode permet de combiner des informations issues de différentes sources, nous ne présentons ici que quelques caractéristiques structurelles permettant d'associer à chaque paire de noeuds un score à partir duquel est construit un classement. On trouve dans la littérature beaucoup d'indices, nous en avons choisi quelques classiques et proposons de les généraliser à des réseaux pondérés. Certaines de ces caractéristiques sont dites locales car elles ne considèrent que des paires de noeuds à distance au plus 2. On appelle N (i) l'ensemble des voisins du sommet i : -nombre de voisins communs (CN) :
Données
D'autres caractéristiques sont dites globales car elles sont calculées sur l'ensemble de la structure du réseau et permettent de classer des paires de noeuds distantes :
-indice de Katz (Katz) : calculé à l'aide du nombre de chemins de longueur l de i à j (au sens d'un multigraphe pour un réseau pondéré), noté ? ij (l), selon l'expression : p.w(k, k )/W (k) et revenant en i avec une probabilité 1 ? p, l'indice est la probabilité pour que ce marcheur soit en j dans l'état stationnaire de la marche.
-attachement préférentiel (PA), basé sur l'observation dans les réseaux sociaux que les noeuds de fort degré tendent à créer plus de nouveaux liens : s PAw (i, j) = W (i).W (j) 2 . Enfin nous utilisons une méthode pour agréger des classements dans le but d'améliorer la qualité de la prédiction. Il s'agit de la méthode de Borda, initialement définie pour obtenir un consensus dans un système de vote. On affecte à chaque paire un score correspondant à la somme sur l'ensemble des classements des nombres de paires moins bien classées, soit :
Résultats. Certaines métriques usuelles d'évaluation des problèmes de classification, comme la courbe ROC, ne sont pas adaptées au problème, en raison de la forte asymétrie des classes. En effet, les graphes étant peu denses, on s'attend à ce que le taux de faux positif soit élevé dans une large gamme de prédictions et rende l'observation de la courbe ROC peu instructive. Comme nous souhaitons pouvoir ajuster le nombre de prédictions pour trouver un bon compromis entre précision (Pr) et rappel (Rc), nous visualisons les résultats à l'aide du F-score et des courbes Pr-Rc.
Nous traçons sur la Figure 1 les résultats obtenus sur le graphe d'apprentissage G learn pour prédire les liens A 2 ? A 2 en utilisant certains des scores définis précédemment (pas la totalité pour une question de lisibilité). On voit que l'allure des courbes de F-score varient significativement d'un indice à un autre, ce qui indique que chaque score permet de détecter des liens différents, ce dont nous chercherons à tirer parti dans RankMerging. Comme on peut s'y attendre pour une méthode de consensus, la méthode de Borda améliore les performances de la classification, en particulier la précision sur la prédiction des paires les mieux classées. Étant donnée la difficulté de la tâche, la précision est peu élevée en moyenne : lorsque Rc est plus grand que 0.06, Pr est inférieur à 0.3 pour toutes les méthodes. Nous n'utilisons que des scores structurels, rendant improbable la prédiction de liens entre paires de sommets éloignées dans le graphe, le rappel est donc limité à des valeurs relativement faibles, car augmenter le nombre de prédictions diminuerait drastiquement la précision.
Méthode RankMerging
Les différentes méthodes de classements précédemment citées ne sont pas sensibles aux mêmes types d'information structurelle. Nous décrivons une méthode générique d'apprentissage supervisé qui permet d'agréger les classements en tirant partie de la complémentarité des sources d'information. Celle-ci ne nécessite pas qu'une paire soit bien classée selon chaque critère, comme pour une règle de consensus, mais qu'elle soit bien classée selon au moins un. La procédure est dénommée RankMerging, une implémentation et un guide utilisateur sont à disposition sur http://lioneltabourier.fr/program.html.
2. Katz et RWR sont calculés à l'aide de sommes infinies, que nous approximons à l'aide des quatre premiers termes afin de réduire le coût du calcul. La forte asymétrie des classes tend à diminuer les performances de l'indice PA, nous limitons la prédiction avec cet indice aux paires de sommets distantes d'au plus 3.
3. Pour que cette méthode ne biaise pas en faveur des prédicteurs qui classent un grand nombre d'éléments, on considère qu'une paire classée dans r? mais pas dans r ? est aussi classée dans r ? , avec un rang équivalent à toutes les autres paires non-classées et en-dessous de toute paire classée. Pour plus de détails, voir Dwork et al. (2001). L'idée centrale est de déterminer à chaque pas de l'algorithme le classement qui prédit le nombre le plus élevé de tp dans les prochaines étapes. Dans ce but nous définissons une fenêtre W i pour chaque classement : c'est l'ensemble des g liens non-prédits classés à partir du rang ? i dans r i 4 . On appelle qualité ? i le nombre de tp dans W i . À chaque étape, le classement r i dont la fenêtre a la qualité la plus élevée est sélectionné (en cas d'égalité, on choisit aléatoirement), et on ajoute alors la paire classée au rang ? i de r i au classement de sortie de la phase d'apprentissage (r L M ). On met à jour ? i et le curseur de fin de la fenêtre, de manière à ce que chaque W i contienne toujours exactement g paires, puis on met à jour les qualités ? i . Au cours du processus, on enregistre à chaque pas les valeurs des curseurs ? 1 ...? ? , qui indiquent la contribution de chaque classement au classement agrégé. Cette procédure est itérée jusqu'à ce que le classement agrégé r L M contienne le nombre de paires T fixé par l'utilisateur. Cet algorithme présente l'intérêt majeur de ne parcourir qu'une seule fois chaque classement, soit une complexité en O(?N ) pour ? classements de taille N (on note que N ? T ).
La phase de test consiste à agréger les classements obtenus sur le réseau G test en utilisant les paramètres ? 1 ...? ? appris durant la première phase. L'implémentation pratique est simple : à chaque étape on regarde quel classement a été choisi à l'étape correspondante de l'apprentissage, et on sélectionne la paire la mieux classée du classement correspondant pour le graphe de test. Si celle-ci n'est pas déjà dans le classement agrégé de la phase de test (r Protocole expérimental et résultats. Nous évaluons les performances de RankMerging en les comparant à des techniques existantes. En premier lieu, nous confrontons les résultats à la méthode non-supervisée de Borda, vue précédemment. Nous comparons aussi à des méthodes de classification supervisées, même si celles-ci ne sont pas conçues pour varier le nombre T de prédictions possibles 6 . Nous utilisons les implémentations proposées dans la boîte à outils Python scikit learn (scikit-learn.org) des méthodes suivantes : les k-plus proches voisins (NN), les arbres de classification (CT) et AdaBoost (AB), en faisant varier les paramètres de manière à obtenir plusieurs points dans l'espace Pr-Rc 7 . Suivant la description de la méthode faite précédemment, nous mesurons les ? i obtenus sur G learn pour découvrir les liens A 2 ? A 2 , puis nous les utilisons pour agréger les classements obtenus sur G test pour prédire les liens B ? B, en utilisant le facteur d'échelle f ? 1.5. La sélection des caractéristiques pertinentes n'est pas problématique ici. En effet, l'utilisateur peut agréger autant de classements qu'il le souhaite car, d'une part, l'addition d'un nouveau classement a un faible coût computationnel, et d'autre part, la méthode est conçue pour qu'un classement n'apportant pas d'information nouvelle soit simplement ignoré pendant l'agréga-tion. Le paramètre g de l'algorithme est ici fixé par une simple extrapolation : on mesure la valeur de g qui permet de maximiser la qualité de l'agrégation sur G learn , et on emploie la même pour l'agrégation sur G test (ici g = 200). Sur la Figure 2, nous traçons le F-score et la courbe précision-rappel obtenus pour RankMerging (g = 200), en agrégeant les classements des indices suivants : AA w , CN w , CN , Jacc w , Katz w (? = 0.01), PA w , RWR w (p = 0.8) et la méthode de Borda appliquée à ces sept indices. RankMerging permet d'améliorer les prédictions : la mesure de l'aire sous la courbe Pr-Rc indique une amélioration de 8.3% par rapport à la méthode de Borda 8 . On pouvait s'y attendre, dans la mesure où RankMerging est une méthode supervisée et utilise l'information contenue dans le consensus de Borda. En fait, la méthode est conçue pour que n'importe quel classement non-supervisé puisse être agrégé sans perte de performance. Nous vérifions cela en pratique en retirant un à un les classements et en constatant que l'amélioration ne fait que décroître, et ce quel que soit l'ordre dans lequel on retire les différents classements 9 . En ce qui concerne les méthodes de classifications supervisées, nous pouvons constater que leurs performances sont élevées uniquement pour un faible nombre de prédictions (inférieur à 2000), mais ces méthodes n'étant pas conçues pour faire varier le nombre de prédictions, elles produisent de très faibles performances en dehors de leur domaine optimal.

Introduction
Le problème de prédiction de séquences est un problème important en fouille de données, défini de la façon suivante. Soit un alphabet Z = {e 1 , e 2 , ..., e m } contenant un ensemble d'éléments (symboles). Une séquence est une suite d'éléments totalement ordonnée s = 1 , i 2 , ...i n où i k ? Z (1 ? k ? n). Un modèle de prédiction M est un modèle entraîné avec un ensemble de séquences d'entraînement. Une fois entraîné, le modèle peut être utilisé pour effectuer des prédictions. Une prédiction consiste, à prédire le prochain élément i n+1 d'une séquence 1 , i 2 , ...i n en utilisant le modèle M . La prédiction de séquences a des applications importantes dans une multitude de domaines tels que le préchargement de pages Web (Deshpande et Karypis, 2004;Padmanabhan et Mogul, 1996), la recommandation de produits de consommation, la prévision météorologique et la prédiction des tendances du marché boursier.
Un grand nombre de modèles de prédictions ont été proposés pour la prédiction de sé-quences. Un des modèle les plus connus est PPM (Prediction by Partial Matching) (Cleary et Witten, 1984). Ce modèle, basé sur la propriété de Markov, a engendré une multitude d'approches dérivées telles que Dependancy Graph (DG) (Padmanabhan et Mogul, 1996), All-korder-Markov (Pitkow et Pirolli, 1999) et Transition Directed Acyclic Graph (TDAG) (Laird et Saul, 1994). Bien que des propositions ont été faites pour réduire la complexité temporelle et spatiale de ces modèles (Begleiter et al., 2004), l'exactitude de leurs prédictions a subi peu d'amélioration. D'autre part, un certain nombre d'algorithmes de compression ont été adaptés pour la prédiction de séquences tels que LZ78 (Ziv et Lempel, 1978) et Active Lezi (Gopalratnam et Cook, 2007). De plus, des algorithmes d'apprentissage machine comme les réseaux de neurones et la découverte de règles d'association séquentielles ont été employés pour faire de la prédiction de séquences (Fournier-Viger et al., 2012;Sun et Giles, 2001). Néanmoins, ces modèles souffrent de limites importantes. Premièrement, la plupart d'entre eux partent de l'hypothèse Markovienne qu'un événement ne dépend que de son prédécesseur. Or, ce n'est pas le cas pour de nombreuses applications, ce qui nuit à l'exactitude des prédictions. Deuxièmement, tous ces modèles sont construits avec perte d'information par rapport aux séquences d'entraî-nement. Donc, ils n'utilisent pas toute l'information disponible dans les séquences d'entraîne-ment pour effectuer les prédictions.
Pour pallier ces limites, un modèle nommé Compact Prediction Tree (CPT) (Gueniche et al., 2013) a été récemment proposé. Il utilise une structure en arbre pour compresser les sé-quences d'entraînement sans perte ou avec une perte minime d'information. De plus, il emploie un algorithme de prédiction conçu pour tenir compte du bruit et de plusieurs événements anté-rieurs lors d'une prédiction plutôt que seulement le dernier. Il a été montré que ce modèle peut obtenir des prédictions jusqu'à 12 % plus exactes que PPM, DG et All-K-order-markov sur des jeux de données provenant de divers domaines, ce qui constitue un gain important. Néanmoins, une limite de CPT est sa complexité temporelle et spatiale élevée. Dans cet article, nous pallions ces problèmes en proposant trois stratégies pour réduire la taille et le temps de prédiction de CPT. De plus, nous présentons une comparaison expérimentale avec davantage de modèles de prédiction de la littérature : All-K-order Markov, DG, Lz78, PPM et TDAG. Les résultats expérimentaux sur 7 jeux de données réels montrent que le modèle résultant nommé CPT+ est jusqu'à 98 fois plus compact et est jusqu'à 4.5 fois plus rapide que CPT. De plus, CPT+ conserve une exactitude très élevée par rapport aux autres approches de la littérature.
Le reste de cet article est organisé de la façon suivante. La section 2 décrit brièvement le modèle CPT. Les sections 3 et 4 proposent respectivement de nouvelles stratégies pour réduire la taille du modèle CPT et ses temps de prédiction. La section 5 présente l'évaluation expérimentale avec plusieurs jeux de données et les principaux modèles de prédictions de la littérature. Finalement, la section 6 est dédiée à la conclusion et aux travaux futurs.
Le processus d'entraînement
Le processus d'entraînement génère trois structures distinctes à partir des séquences d'entraînement : (1) un Arbre de Prédiction (AP), (2) un Dictionnaire de Séquences (DS) et (3) un Index Inversé (II). Pendant l'entraînement, les séquences sont considérées les unes après les autres pour construire incrémentalement ces trois structures. À titre d'exemple, la figure 1 illustre la création des structures de CPT par insertion successive des séquences
(1) Insertion de ?í µí±¨, í µí±©, í µí±ª? . Chacun des noeuds de l'arbre représente un élément et chacune des séquences d'entraînement est représentée par un chemin partant de la racine de l'arbre et se terminant par un noeud interne de l'arbre ou une feuille. La construction de cet arbre a une basse complexité. Insérer une séquence de m éléments demande de parcourir/créer au plus m noeuds. La construction complète de l'arbre est O(n) où n est le nombre de séquences à insérer. Tout comme un arbre préfixe, cet arbre est une représentation compacte des séquences d'entraînement, car les séquences partageant un préfixe commun partagent un chemin dans l'arbre. Par exemple, à la figure 1, les séquences s 1 , s 2 et s 3 partagent le même chemin correspondant au préfixe B Dans le pire cas, le gain spatial offert par cette compression est nul, mais en pratique, tout dépendant de la densité et de la similarité des séquences du jeu de données utilisé, l'arbre peut offrir une réduction spatiale très importante allant jusqu'à 98% (Gueniche et al., 2013).
Le Dictionnaire de Séquences est une structure qui permet d'extraire chacune des sé-quences d'entraînement de l'arbre de prédiction. Lors de la construction du modèle CPT, un identifiant unique est assigné à chaque séquence. Il est égal à 1 pour la première séquence insérée (dénoté s 1 ) et est incrémenté d'un pour chaque séquence subséquente (s 2 , s 3 , ...). Le dictionnaire de séquences associe chaque identifiant de séquence s a à un pointeur vers un noeud de l'arbre. Ce noeud représente le dernier élément de la séquence s a dans l'arbre. Grâce à cette structure, il est possible de parcourir chaque séquence d'entraînement dans l'arbre de prédiction du dernier au premier élément.
L'Index Inversé permet d'identifier rapidement dans quelles séquences apparaît un ensemble d'éléments d'une séquence à prédire. L'index inversé contient un vecteur de bits v e pour chaque élément e de l'alphabet Z présent dans les séquences d'entraînement. Le k-ième bit d'un vecteur de bit v e prend la valeur 1 si l'élément e apparaît dans la séquence s k , sinon il prend la valeur 0. Par exemple, à la figure 1, le vecteur de bit de l'élément C après l'insertion des séquences s 1 , s 2 , s 3 , s 4 et s 5 est 10110, car C apparaît dans les séquences s 1 , s 3 et s 4 . L'index inversé est utilisé pour déterminer rapidement les séquences d'entraînement contenant un ensemble d'éléments d'une séquence à prédire. Cela est réalisé en faisant l'intersection des vecteurs de bits des éléments. Par exemple, déterminer l'ensemble des séquences contenant les éléments A et C est réalisé par l'opération 11101 ? 10110, donnant le résultat 10000, autrement dit {s 1 }. Grâce à l'index inversé, cette tâche est très rapide ; O(i) où i est le nombre d'éléments dans l'ensemble.
Le processus de prédiction
Le processus de prédiction de CPT utilise les trois structures décrites précédemment. Soit une séquence s = 1 , i 2 , ...i n de n éléments et y, un nombre entier représentant le nombre d'éléments de s à considérer pour faire une prédiction. Le suffixe de taille y de s dénoté P y (s) est défini comme étant P y (s) = n?x+1 , i n?x+2 ...i n La prédiction du prochain élément de s est effectuée de la façon suivante : CPT identifie tout d'abord les séquences similaires à P y (s), c.à.d. qui contiennent les derniers y éléments de P y (s) dans n'importe quel ordre et positions. Puis, pour chaque séquence similaire, CPT considère son conséquent. Le consé-quent d'une séquence u est la sous-séquence débutant après le dernier élément en commun avec P y (s) jusqu'à la fin de u. Chaque élément e dans un de ces conséquents est ensuite stocké dans une structure nommé Table de Compte (TC) avec son nombre d'occurrences (ce nombre est une estimation de la probabilité P (e|P y (s))). L'élément ayant le plus grand nombre d'occurrences est l'élément prédit par CPT. La mesure de similarité utilisée pour déterminer les séquences similaires est de nature stricte, mais est relâchée dynamiquement par le processus de prédiction, pour deux raisons. Premièrement, avec une mesure de similarité trop stricte, une séquence à prédire peut n'être similaire à aucune séquence d'entraînement, et donc aucune prédiction n'est possible. Deuxièmement, une mesure de similarité trop stricte ne permet pas de considérer qu'une séquence peut-être partiellement similaire à une autre. Or, dans les applications réelles, il y a souvent des éléments présents dans les séquences qui sont du bruit. Pour relâcher la mesure de similarité, CPT suppose qu'un ou plusieurs éléments présents dans le suffixe de la séquence à prédire sont du bruit et qu'ils peuvent être ignorés lors du calcul de similarité. Le calcul de similarité pour un suffixe P y (s) est fait par niveau, où à chaque niveau k = 1, 2, ..., |P y (s)| ? 1 toutes les sous-séquences de taille |P y (s)| ? k de P y (s) sont générées. Chacune des sous-séquence u est utilisée pour trouver les séquences similaires dans l'ensemble de séquences d'entraînement et pour mettre à jour la TC. Ce relâchement de la mesure de similarité se poursuit pour la séquence à prédire d'un niveau à l'autre tant que TC n'a pas été mise à jour un nombre minimum de fois.
Stratégies de compression de l'arbre de prédiction
Bien que CPT offre des prédictions plus exactes que les principaux modèles de prédiction de la littérature selon une étude antérieure (Gueniche et al., 2013), une limite importante de CPT est sa complexité spatiale. Il a été montré que la taille des structures de CPT est inférieure à All-k-order Markov, mais demeure nettement supérieures à d'autres modèles comme DG et PPM. L'arbre de prédiction étant la structure la plus imposante de CPT, nous proposons ci-après deux stratégies pour réduire sa taille.
Stratégie 1 : Compressions des Chaînes Fréquentes (CCF). Certaines répétitions peuvent être identifiées dans les séquences d'entraînement. Dépendamment du jeu de données, ces répé-titions peuvent être nombreuses et fréquentes. La compression des chaînes fréquentes consiste à identifier les sous-chaînes fréquentes d'éléments apparaissant dans les séquences d'entraîne-ment, puis à remplacer les sous-chaînes fréquentes par des éléments individuels.
Soit une séquence s = 1 , i 2 , ..., i n Une séquence c = m+1 , j m+2 , ..., j m+k est une sous-chaine de s, dénoté c s, si et seulement si 1 ? m ? m + k ? n. Pour un ensemble de séquences d'entraînement S, une sous-chaîne d est fréquente si |{t|t ? S ? d t}| > minsup pour un seuil minsup fixé par l'utilisateur.
La compression des chaînes fréquentes est effectuée pendant la phase d'entraînement de CPT en trois étapes : (1) identifier les chaînes fréquentes dans l'ensemble des séquences d'entraînement, (2) créer un nouvel élément dans l'alphabet Z pour représenter chaque sous-chaîne fréquente et (3) remplacer les sous-chaînes fréquentes par l'élément correspondant lors de la construction de l'arbre de prédiction de CPT. L'identification de séquences fréquentes dans un ensemble de séquences est un problème populaire en fouille de données, pour lequel un grand nombre d'algorithmes ont été proposés. Pour cette tâche, nous avons adapté un des algorithmes les plus performants nommé PrefixSpan (Pei et al., 2001), afin de ne découvrir que les séquences fréquentes d'éléments consé-cutifs (sous-chaînes). De plus, nous avons ajouté la contrainte que les sous-chaînes fréquentes doivent respecter des contraintes de longueur minimale minSize et maximale maxSize (deux paramètres).
Les sous-chaînes fréquentes identifiées sont stockées dans une nouvelle structure nommée Dictionnaire des chaînes fréquentes (DCF). Cette structure associe un nouvel élément non présent dans l'alphabet Z (dans les séquences d'entraînement) à chaque sous-chaîne fréquente. Le DCF permet de rapidement convertir une sous-chaîne en son élément correspondant et viceversa. Lors de l'insertion des séquences d'entraînement dans l'arbre de prédiction, le DCF est utilisé pour remplacer chaque sous-chaîne par son élément correspondant.
À titre d'exemple, l'illustration (1) de la figure 2 affiche la compression de l'arbre de pré-diction de l'illustration (5) de la figure 1 par la stratégie CCF. La sous chaîne fréquence B a été remplacée par un nouveau symbole x, réduisant le nombre de noeuds de l'arbre de pré-diction.
La stratégie de compression de séquences CCF a un effet seulement sur l'arbre de pré-diction où son nombre de noeuds et sa hauteur tendent à diminuer grandement. La stratégie CCF est transparente pour le processus de prédiction de CPT. En effet, lors de l'extraction de séquences similaires, les branches de l'arbre de prédiction sélectionnées sont décompressées à la volée par DCF. L'identification et le remplacement de branches simples sont faits en un seul parcours de l'arbre de prédiction. L'index inversé et le dictionnaire de séquences n'étant pas influencés par cette approche, le seul changement au processus de prédiction est la décompression dynamique des branches simples lorsque nécessaire. La complexité de ce remplacement est de O(n * (1 ? t)) où s est le nombre de séquence et t le taux de recouvrement de l'arbre, ce dernier est défini comme le "ratio" de noeuds qui partagent plusieurs séquences par le nombre total de noeuds dans l'arbre.
Stratégie de réduction des temps de prédiction
Stratégie 3 : Prédiction avec réduction du Bruit Amélioré (PBA). Tel qu'expliqué pré-cédemment, pour prédire le prochain élément s n + 1 d'une séquence s = 1 , i 2 , ..., i n CPT utilise le suffixe de taille y de s dénoté P y (s) (les y derniers éléments de s), où y est un paramètre propre à chaque jeu de donnée. CPT prédit le prochain élément de s en parcourant les séquences similaires à son suffixe P y (s). La recherche de séquences similaires est rapide (O(y)). Toutefois, le mécanisme de réduction du bruit lors des prédictions (décrit à la section 2) ne l'est pas, car il requiert de considérer non seulement P y (s) pour une prédiction, mais aussi toutes les sous-séquences de P y (s) de taille t > k. Plus y et k sont grands, plus le nombre de sous-séquences à considérer l'est aussi, et donc le temps de prédiction. Lors d'une tâche de prédiction, certains éléments dans une séquence à prédire peuvent être considérés comme du bruit si leur simple présence affecte de façon négative le résultat de la prédiction. La stratégie PBA se base sur l'hypothèse que le bruit observé dans une séquence est constitué des éléments ayant une faible fréquence, où la fréquence d'un élément est le nombre de séquences d'entraînement contenant l'élément. Pour cette raison, PBA enlève seulement les éléments qui ont une faible fréquence pendant la phase de prédiction. Puisque la définition du bruit de CPT+ est plus restrictive que celle de CPT, un moins grand nombre de sous-séquences sont considé-rées. Cette réduction à un impact positif et tangible sur les temps de calculs tel que présentés dans notre évaluation expérimentale (section 5). Le pseudo-code illustrant la stratégie PBA est présenté ci-après (Algorithme 1). L'algorithme prend en paramètres le préfixe P y (s) à prédire, les autres structures de CPT, un taux de bruit et un nombre minimum de mise à jour à faire à la TC pour faire une prédiction. Le taux de bruit représente le pourcentage d'éléments dans une séquence qui doivent être considérés comme du bruit ; un taux de bruit de 0 indique que les séquence n'ont pas de bruit alors qu'un taux de bruit de 0.4 signifie que 40% des éléments d'une séquence pourrait être du bruit. PBA est récursive de nature et considère un nombre minimal de sous-séquences dérivées de P y (s) pour faire une prédiction. Le bruit est d'abord retiré de chaque sous-séquence. Puis la TC est mise à jour. Lorsque le nombre minimal de mise à jour est atteint, une prédiction est faite comme dans CPT en utilisant la TC. La stratégie PBA est une généralisation de la stratégie de réduction du bruit utilisée par CPT. En effet, selon les paramètres utilisés, il est possible de reproduire le fonctionnement original de CPT. Les trois contributions principales apportées par PBA sont l'imposition d'un nombre minimal de mise à jour de la TC pour faire une prédiction, la définition du bruit basé sur la fréquence d'un élément et la réduction relative du bruit par rapport à la longueur de la séquence.
Algorithme 1 : L'algorithme de prédiction avec PBA input : PS : le suffixe P s, CPT : les structures de CPT, TB : le taux de bruit output : Seq : un ou plusieurs éléments prédits file.ajouter(PS); while nombreMiseAjour < minNombreMiseAJourTC ? file.nonVide() do suffixe = file.prochain(); elementsBruit = selectionnerElementsMoinsFrequents(TB); foreach elementBruit ? elementsBruit do suffixeSansBruit = copierSuffixeSansBruit(suffixe, elementBruit); if suf f ixeSansBruit.length > 1 then file.ajouter(suffixeSansBruit); end mettreAJourCountTable(CPT.countTable, suffixeSansBruit); nombreMiseAjour++; end retourne faireUnePrediction(CPT.countTable); end
Évaluation expérimentale
Nous avons effectué une série d'expériences pour comparer la performance de CPT+, CPT et les principaux modèles de prédiction de la littérature All-K-order Markov, DG, Lz78, PPM et TDAG. Pour implémenter CPT+, nous avons obtenu et modifié le code source proposé dans l'article original de CPT (Gueniche et al., 2013). Pour permettre la reproduction des expériences, le code source des modèles et jeux de données sont fournis à l'adresse http: //goo.gl/LE4uYO. Tous les modèles sont implémentés en Java 8. Les expériences ont été réalisées sur une machine dotée d'un processeur deux coeurs Intel i5 de 4ème génération avec 8 Go de mémoire vive et un SSD en SATA 600. Tous les modèles de prédiction utilisés ont été configurés empiriquement pour tenter de donner des valeurs optimales à chacun de leurs paramètres. PPM et LZ78 n'ont pas de paramètres, DG et AKOM ont respectivement une fenêtre de 4 et un ordre de 5, finalement, par soucis d'espace, TDAG à une hauteur maximale de 6. CPT à 4 paramètres et CPT+ en a 8, leurs valeurs sont elles aussi déterminées via une exploration expérimentale de l'espace de valeurs possibles. Ces valeurs sont accessible dans les fichiers sources du projet. Les paramètres propres à l'expérience se limitent à la longueur minimale et maximale des séquences utilisées, la taille du suffixe à considérer pour une séquence à prédire et la quantité d'éléments à prédire pour chacune des séquences.
Des jeux de données ayant des caractéristiques variées ont été utilisés (cf. Table 1) : sé-quences courtes/longues, séquences denses/éparses, petit/grands alphabets et divers types de données. Les jeux de données BMS, Kosarak, MSNBC et FIFA consistent en des séquences de pages Web visitées par des utilisateurs sur un site Web. Dans ce scénario, les modèles de pré-diction sont appliqués pour prédire la prochaine page Web que visitera chaque utilisateur. Le jeu de données SIGN est un ensemble de phrases exprimées en langage des signes, transcrites à partir de vidéos. Bible Word et Bible Char sont deux jeux de données qui proviennent de la Bible, livre religieux, le premier est l'ensemble des phrases découpées en mots et le second est l'ensemble des phrases découpées en caractères.
Pour l'évaluation des prédictions des modèles, une prédiction est soit un succès, un échec, ou une abstention (si un modèle ne peut effectuer une prédiction). Deux mesures sont utilisées. La couverture est le nombre d'abstentions divisé par le nombre de séquences à prédire. L'exactitude (alias précision) le nombre de succès divisé par le nombre de séquences à prédire.
Nom
Nombre Expérience 1 : comparaisons des optimisations. Dans cette première expérience, nous avons tout d'abord évalué les améliorations spatiales présentées à la section 3 en terme de taux de compression et de temps de calcul à l'entraînement. Les autres mesures de performance telles que le temps de prédiction, la couverture et l'exactitude ne sont pas affectées par la compression de l'arbre de prédiction. Pour un arbre de prédiction A avec s noeuds avant compression et s2 noeuds après compression, le taux de compression tc a de A est défini comme tc = 1 ? (s2/s), et est compris entre 0.0 et 1.0 non inclusivement. Plus la valeur est haute, plus la compression est importante. Les deux stratégies de compression sont évaluées d'abord individuellement (dénotées CCF et CBS) puis en conjonction (dénoté CPT+). Toute compression permet d'obtenir un gain spatial au prix d'un coût temporel. La figure 3 présente cette relation pour chacune des stratégies de compression. Les résultats présentés à la figure 3 montrent que le taux de compression de l'arbre varie selon le jeu de données de 58.90% à 98.65%. CCF offre un taux de compression moyen de 48.55% avec un faible écart type de 6.7%. alors que CBS à un taux de compression moyen de 77.87% avec un écart type beaucoup plus prononcé de 15.9%. L'efficacité de CBS est dépen-dante au jeu de données ; dans le cas de MSNBC, qui est le jeu de données le moins affecté par les stratégies de compression, la faible cardinalité de son alphabet permet à MSNBC d'être naturellement compressé grâce au fort recouvrement des branches de son arbre de prédiction. En effet, MSNBC ne possède que 17 éléments uniques et même si la taille moyenne des sé-quences ressemble à celle des autres jeux de données, la taille de son arbre avant compression est très petite. Le jeu de données où les stratégies de compression CCF et CBS sont les plus effectives est SIGN. SIGN a un très faible nombre de séquences, mais chacune d'elle est très longue (en moyenne 93 éléments). Ces caractéristiques font en sorte que son arbre de prédic-tion a un faible taux de recouvrement et donc une importante partie de ses noeuds n'ont qu'un seul fils ; ce qui rend ce jeu de données un candidat idéal pour la stratégie CBS. CBS offre un taux de compression de 98.60 % pour SIGN.
La figure 3 présente également les temps d'entraînement engendrés par les deux stratégies de compression de CPT, CBS et CCF. La mesure utilisée est un facteur multiplicatif du temps d'entraînement. Par exemple, un facteur de x pour CBS signifie que CBS a eu une phase d'entraînement x fois plus longue. Pour tous les jeux de données à l'exception de SIGN, CBS est plus rapide que CCF. Il est intéressant d'observer que le temps pris par la combinaison des deux stratégies de compression n'est pas simplement une addition de leur coût d'entraînement.
CBS et CCF sont appliqués indépendamment à CPT et pourtant l'utilisation de CBS réduit les temps de calcul de CCF grâce à une diminution du nombre de branches qui ont besoin d'être compressées.
Nous avons également évalué le gain en temps de prédiction et l'exactitude (précision) obtenue en appliquant la stratégie PBA. La figure 4 (gauche) illustre les temps de prédiction de CPT+ (avec CBA), et ceux de CPT. Les gains temporels sont importants pour la plupart des jeux de données notamment pour SIGN et MSNBC où les temps d'entraînement sont jusqu'à 4.5 fois moindres. Pour les jeux de données Bible Word et FIFA, les temps de prédiction sont plus élevés pour obtenir un gain en exactitude comme le montre la figure 4 (droite). L'effet de CBA sur l'exactitude des prédictions est positif pour tous les jeux de données sauf MSNBC. Cette amélioration s'élève jusqu'à 5.47% dans le cas de Bible Word. CBA se montre donc une stratégie effective pour à la fois réduire les temps de prédiction et augmenter l'exactitude des prédictions. Expérience 2 : Mise à l'échelle. Nous avons également comparé la complexité spatiale de CPT+ (avec ses deux stratégies de compression) avec celle de CPT et All-K-order Markov, DG, Lz78, PPM et TDAG, en termes de mise à l'échelle par rapport au nombre de séquences. Les deux seuls jeux de données utilisés sont FIFA et Kosarak à cause de leur grand nombre de séquences (573,060 et 638,811 respectivement). L'accroissement du nombre de séquences dans cette expérience est quadratique et s'arrête à 128,000 séquences dû aux énormes temps de calcul requis pour réaliser chaque expérience. La figure 5 présente les résultats. Le taux de compression de CPT+ tend à baisser très légèrement avec l'accroissement du nombre de sé-quences, ce phénomène est causé par un recouvrement de plus en plus important des branches dans l'arbre de prédiction ; car la taille de l'alphabet étant constante, plus de séquences sont utilisées et plus de branches s'unifient. Les modèles DG et PPM ont une croissance linéaire, car ils sont basés sur la taille de l'alphabet et indirectement sur le nombre de séquences d'entraînement. Les autres modèles ont tous une croissance beaucoup plus importante que DG et PPM, notamment TDAG et LZ78.
Expérience 3 : Comparaison avec les autres modèles de prédiction Dans l'expérience 1, nous avons comparé l'exactitude des prédictions de CPT+ avec celle de CPT afin d'évaluer la contribution de la stratégie PBA. Dans cette expérience, nous effectuons une comparaison de l'exactitude celle des autres principaux modèles de prédiction de la littérature All-K-order Markov, DG, Lz78, PPM et TDAG, sur les mêmes jeux de données. Il est à noter que nous ajoutons dans cette comparaison deux modèles de prédictions (Lz78 et TDAG) qui n'ont pas été utilisés dans l'article original proposant CPT. 
Conclusion
Dans cet article, nous avons présenté trois stratégies pour réduire la taille et le temps de prédiction de CPT, nommées CCF (Compression des Chaînes Fréquences), CBS (Compression des Branches Simples) et PBA (Préduction avec réduction du Bruit Améliorée). Les résultats expérimentaux sur 7 jeux de données réels ont montré que le modèle résultant nommé CPT+ est jusqu'à 98 fois plus compact que CPT, et que cette compression demeure lorsque le nombre de séquence augmente. En termes de temps d'exécution, CPT+ s'est montré jusqu'à 4.5 fois plus rapide que CPT. Finalement, CPT+ s'est montré comme étant le modèle offrant les pré-dictions généralement les plus exactes dans une comparaison avec les principaux modèles de la littérature CPT, All-K-order Markov, DG, Lz78, PPM et TDAG.
Comme travaux futurs, nous adapterons CPT+ pour la prédiction de séquences dans le contexte d'un flux infini de séquences. CPT+, de par sa nature incrémentale, pourrait être adapté à ce problème.

Introduction
Ce travail s'inscrit dans le cadre de l'apprentissage supervisé et plus précisément des systèmes de classification à base de règles floues (Ishibuchi et al. (1992). Ces systèmes ont la spécificité d'être facilement interprétables grâce à l'utilisation de termes linguistiques.
Dans ces systèmes, les règles floues peuvent être fournies par un expert humain. Comme l'acquisition des connaissances humaines est une tâche complexe, plusieurs travaux se sont consacrés à l'automatisation de la construction des règles à partir des données numériques (Ishibuchi et al., 1992), (Dehzangi et al., 2007). Cette construction comprend deux phases : une partition floue de l'espace des entrées puis la construction d'une règle floue pour chaque sous-espace flou issu de cette partition. Dans ces systèmes, un nombre élevé d'attributs conduit à une explosion du nombre de règles générées, ce qui entraîne une dégradation de la compréhensibilité des systèmes, et affecte le temps de réponse nécessaire aussi bien à la phase d'apprentissage qu'à la phase de classification.
De ce fait, l'optimisation du nombre de règles floues ainsi que du nombre d'antécédents paraît comme une clé pour améliorer les systèmes de classification à base de règles floues. Dans ce cadre, plusieurs approches ont été proposées dans la littérature. On peut citer l'approche de sélection des règles pertinentes par algorithme génétique (Ishibuchi et al., 1995) ou par le concept d'oubli . Une autre approche consiste à réduire le nombre d'attributs par une sélection des attributs les plus significatifs (Lee et al., 2001).
Dans ce papier, nous nous intéressons à la technique de regroupement des attributs dans les prémisses des règles. Dans cette approche, initialement introduite dans un cadre non flou par Borgi (1999), les attributs prédictifs sont regroupés en blocs, les attributs de chaque bloc sont traités séparément et apparaissent ensemble dans une même prémisse. Une première extension de ce travail dans un cadre flou, pour la génération de règles dans les systèmes d'infé-rence floue, a été réalisée par Soua et al. (2012). Cette approche de regroupement des attributs, nommée SIFCO, présente l'avantage de décomposer le problème d'apprentissage en des sousproblèmes de complexité inférieure, et de réduire ainsi le nombre de règles générées. De plus, cette approche permet d'obtenir des règles plus intelligibles car de taille réduite.
Le regroupement d'attributs dans (Borgi, 1999) et (Soua et al., 2012) se fait par recherche de corrélation linéaire : les attributs linéairement corrélés sont regroupés et traités séparément. Dans cet article, nous proposons une méthode qui se base sur le concept des règles d'association (RA) introduit par Agrawal et al. (1993). Les RA vont nous permettre de déterminer les attributs "liés" ou "associés" qui seront regroupés dans les mêmes règles.
L'article est organisé comme suit : dans la partie 2, nous présentons les systèmes de classification à base de règles floues. Nous décrivons, dans la partie 3, le principe de regroupement des attributs comme présenté dans (Borgi, 1999) et (Soua et al., 2012). Notre approche de regroupement des attributs par RA est décrite dans la partie 4 et les résultats des tests expéri-mentaux sont présentés dans la partie 5. Nous concluons l'article en présentant les principales perspectives de ce travail.
Apprentissage à base de règles floues
On se place dans le cadre des problèmes de classification supervisée dont le but est d'affecter une classe à un objet décrit par des variables descriptives (des attributs). Nous nous intéressons au système de classification floue proposé dans (Ishibuchi et al., 1992). Afin de simplifier les notations, nous désignons ce système par l'acronyme SIF. Deux phases sont à distinguer dans ce système : la phase d'apprentissage dans laquelle on construit le modèle de classement à partir des données d'apprentissage, et la phase de classification qui sert à associer une classe à un objet inconnu en utilisant ce modèle.
Phase d'apprentissage
La méthode de génération des règles floues que nous adoptons correspond à l'utilisation d'une grille floue simple, proposée par Ishibuchi et al. (1992). Pour illustrer cette approche, nous supposons, par souci de clarté, que notre problème d'apprentissage est un problème bidimensionnel (2 attributs : X 1 et X 2 ). Les m exemples d'apprentissage considérés sont notés E p = (X p1 , X p2 ) ; p = 1, 2, . . . , m , ils appartiennent chacun à l'une des C classes : y 1 , y 2 , . . . , y C . Il est à noter que dans les SIF les attributs considérés sont numériques ; chacun des attributs X 1 et X 2 est partitionné en k sous-ensembles flous {A 1 , A 2 , . . . , A k } où chaque sous-ensemble A i est défini par une fonction d'appartenance triangulaire symétrique. Un exemple de grille floue simple est présenté dans Fig 
-y k ij est la conclusion de la règle, elle correspond à l'une des C classes -CF k ij est le degré de certitude de la règle, il traduit sa validité.
La conclusion et le degré de certitude de chaque règle sont déterminés comme suit :
1. Pour chaque classe y t , calculer la somme des compatibilités des exemples d'apprentissage appartenant à cette classe, par rapport à la prémisse de la règle :
Trouver la classe y a qui a la plus grande valeur de compatibilité
3. Déterminer le degré de certitude CF
Dans les travaux portant sur la construction de règles de classification floues, les attributs ne sont pas nécessairement partitionnés en un même nombre de sous-ensembles flous. Plusieurs types de grilles floues ont été étudiés comme par exemple la grille floue rectangulaire .
Phase de classification
Dans cette phase, le système décide, à partir de la base de règles générées, notée S R , de la classe y a à associer à un individu E = (X 1 , X
2 ) de classe inconnue. 1. Pour chaque classe y t ; t = 1, 2, . . . , C, calculer ? yt par :
2. Trouver la classe y a qui maximise ? yt :
FIG. 1 -Grille floue simple.
Regroupement d'attributs
L'approche de regroupement d'attributs se base sur le concept des ensembles d'apprentissage artificiel, qui repose sur la combinaison des décisions de plusieurs apprenants pour améliorer l'exécution du système global (Valentini et Masulli, 2002). L'idée de ce concept est de répartir l'information -qui peut correspondre aux exemples d'apprentissage, aux attributs descriptifs ou encore aux classes -entre plusieurs apprenants, chaque apprenant réalise la phase d'apprentissage sur l'information qui lui a été fournie, et les opinions "individuelles" des différents apprenants sont ensuite combinées pour atteindre une décision finale. Dans notre cas, l'information à répartir correspond aux attributs descriptifs : chaque classifieur utilise un sous-ensemble des attributs initiaux et construit une base de règles locale, puis les différentes bases locales obtenues sont combinées pour former le modèle final (voir Fig. 2).
Cette approche, vérifiée expérimentalement dans (Soua et al., 2012) et (Borgi, 1999), permet de garantir une réduction conséquente du nombre de règles sans trop altérer les taux de bonnes classifications. Pour un problème de n attributs et k sous ensembles flous pour chaque attribut, le nombre de règles générés par les SIF, noté N R SIF , vaut k n . Lorsqu'on découpe le problème d'apprentissage en g sous-problèmes et on applique sur chacun d'eux la même démarche de génération de règles que les SIF, on obtient un nombre de règles N R regrp égal à :
où n i est le nombre d'attributs liés dans le i ème groupe g i . Il a été démontré dans (Soua et al., 2012) que si les groupes d'attributs, issus de l'approche de regroupement, forment une partition de l'ensemble des attributs de départ (
Par conséquent :
FIG. 2 -Approche de génération de règles par regroupement des attributs (Soua et al., 2012).
Approche proposée : regroupement des attributs par RA
Notre contribution réside au niveau de la méthode de regroupement d'attributs ; nous proposons une nouvelle méthode n'utilisant pas la recherche de corrélation linéaire, mais qui se base sur le concept des règles d'association (Agrawal et al., 1993). Les algorithmes d'extraction des RA déterminent les associations intéressantes entre les attributs en analysant leurs apparitions simultanées dans les enregistrements de la base de données. Cette méthode peut être très intéressante pour les bases de données pour lesquelles il n'existe aucune relation de type corrélation linéaire entre les attributs.
Généralement, les algorithmes d'extraction des RA déterminent les associations entre des variables de type booléen. Comme les données traitées dans les SIF sont quantitatives, il est nécessaire de commencer par les transformer en des valeurs booléennes, puis d'appliquer le concept des RA sur ces valeurs. Ensuite, à partir des associations trouvées entre ces valeurs booléennes, nous déduisons les associations entre les attributs de départ. Enfin et dans le but de garantir une réduction du nombre de règles, nous nous proposons de filtrer les groupes d'attributs associés de manière à obtenir une partition de l'ensemble des attributs de départ. Nous décrivons dans ce qui suit ces différentes étapes.
Génération des itemsets fréquents : liaisons locales entre attributs
L'existence d'une liaison entre deux variables dépend de la réponse à la question : est-ce que la connaissance des valeurs de l'une permet de prédire les valeurs de l'autre ? Le concept des RA répond à cette question en associant les valeurs qui apparaissent souvent ensemble dans les transactions de la base de données considérée.
Les RA ont été introduites par Agrawal et al. (1993)  -la génération des itemsets fréquents (tous les itemsets ayant un support supérieur à un seuil prédéfini minSupp). -la génération des règles d'association à partir de ces itemsets fréquents ; une RA doit avoir une confiance supérieure à un seuil prédéfini par l'utilisateur minConf .
Dans ce travail, nous nous intéressons au premier sous-problème et nous cherchons à déter-miner les groupes d'attributs liés. Pour déterminer les itemsets fréquents, plusieurs algorithmes ont été proposés (Agrawal et al., 1993)  (Agrawal et Srikant, 1994)  (Savasere et al., 1995). L'algorithme Apriori proposé dans (Agrawal et Srikant, 1994) est le plus connu et il est largement utilisé mais il ne traite que des données booléennes. Dans les problèmes courants, la majorité des données sont quantitatives et qualitatives et nécessitent des algorithmes applicables à ce type de données. Une extension de Apriori a été proposée par Srikant et Agrawal (1996) ; ils ont proposé de faire une correspondance entre des variables quantitatives ou qualitatives et des variables booléennes par le codage disjonctif complet. Pour une variable qualitative, chaque catégorie correspond à un élément booléen. Pour une variable quantitative, on discrétise l'attribut en des intervalles, puis on fait correspondre une variable booléenne à chaque intervalle.
Dans notre cas, les attributs étant continus, nous recourons au codage disjonctif complet des attributs. Le partitionnement des attributs se fait par une discrétisation régulière à intervalles égaux. Nous obtenons donc des intervalles que nous assimilons à des valeurs booléennes. Nous appliquons ensuite l'algorithme Apriori sur ces intervalles et obtenons ainsi des itemsets fré-quents ou des groupes d'intervalles liés.
Détermination des attributs liés : liaisons globales entre attributs
Dans l'étape précédente, nous avons déterminé les groupes d'intervalles liés. Notre but étant de faire un regroupement des attributs et non pas de leurs intervalles, on se propose de développer une procédure qui permet de déterminer la liaison entre un groupe d'attributs à partir des liaisons trouvées entre leurs intervalles.
Nous définissons pour cela une grille d'association qui représente les associations entre les valeurs (intervalles) d'un groupe d'attributs. Chaque axe de la grille concerne un attribut. La Fig. 3 présente 3 exemples de grille avec deux attributs X 1 et X 2 ; X 1 est décomposé en 6 valeurs (val de valeurs des attributs X 1 et X 2 . Quand deux valeurs forment un itemset fréquent, la case correspondante à leur intersection est grisée : on appelle cette case une région liée.
La liaison entre deux valeurs de deux attributs n'entraîne pas forcément la liaison entre les deux attributs puisque d'une part, ces attributs peuvent avoir très peu de régions liées (exemple (2) de la Fig. 3) et d'autre part, le nombre de données dans ces régions peut être très faible par rapport au nombre total de données (exemple (3) de la Fig. 3).
Dire que, plus le nombre de régions liées est grand, plus l'association entre les attributs est forte, n'est pas toujours suffisant. En effet, le principe de RA détermine si une région est liée en analysant son support, et ce dernier reflète la densité de données, c.à.d. la fréquence d'apparition des données dans la région. Ainsi, une seule région liée peut entraîner une association plus significative que plusieurs régions liées si cette unique région a une densité plus importante que la densité totale de l'ensemble des autres régions liées. Nous proposons donc de prendre en compte aussi bien le nombre de régions liées que leurs densités. Pour cela, nous commençons par définir le poids d'une région, appelé aussi coefficient de pondération. Ce poids caractérise la densité de données dans cette région, ce qui revient à son support.
Pour les valeurs respectives val Nous nous inspirons ensuite du principe des RA généralisées où une taxonomie (Fig. 4) existe entre les variables. D'après (Srikant et Agrawal, 1995), les associations trouvées à un niveau donné peuvent remonter au niveau supérieur en sommant leurs supports, à condition qu'il n'y ait pas de recouvrement. Avec l'exemple de la Fig. 4, si les itemsets (Veste, Botte) et (Veste, Espadrille) sont extraits, alors il n'est pas possible de les généraliser à l'itemset de niveau supérieur (Vêtements, Chaussure) en sommant leurs supports, car Veste, Botte et Espadrille peuvent figurer dans une même transaction. Dans notre cas, les variables quantitatives sont partitionnées en des valeurs sous forme d'intervalles ; la présence de deux valeurs d'un seul attribut n'est donc pas possible dans le même enregistrement. En formant une taxonomie entre un attribut et ses intervalles (Fig. 5), et comme il n'y a pas de recouvrement entre les FIG. 4 -Exemple de taxonomie pris de Srikant et Agrawal (1995).
FIG. 5 -Taxonomie entre un attribut et ses valeurs.
intervalles, on peut calculer le degré d'association des attributs comme la somme des supports de leurs valeurs (intervalles) liées. En utilisant ce principe, et en ne comptabilisant que les régions liées, nous définissons un degré d'association entre deux attributs ou plus, par la somme des coefficients de pondération de leurs régions liées. Donc, pour deux attributs X 1 et X 2 , le degré d'association ? s'écrit : Dans le cas général d'un ensemble d'attributs X = {X n1 , X n2 , . . . , X n l }, le degré d'association de ces l attributs est :
-r i1i2...i l X est la région formée par les intervalles val
. . , k l sont respectivement les tailles des partitions de X n1 , X n2 , . . . , X n l .
Le degré ? est compris entre 0 et 1, on peut alors définir un seuil d'association ? min au delà duquel on considère que les attributs de l'ensemble X sont liés.
Choix des groupes d'attributs associés
La procédure présentée dans 4.1 et 4.2 est basée sur le principe de l'algorithme Apriori ; elle fournit donc tous les groupes d'attributs associés de différentes tailles. Il est à noter que ces groupes ne constituent pas forcément une partition de l'ensemble des attributs de départ : on peut avoir des relations d'inclusion entre deux groupes d'attributs, ou une intersection non vide. Afin de garantir une réduction du nombre de règles générées, nous nous proposons de sélectionner un ensemble de groupe d'attributs de manière à former une partition de l'ensemble des attributs de départ (voir partie 3). La sélection se base sur les deux critères suivants : 
Expérimentation
Notre système, baptisé SIFRA, utilise l'approche de regroupement des attributs dans le cadre des SIF (Ishibuchi et al., 1992) comme cela est fait dans SIFCO (Soua et al., 2012) mais avec une nouvelle méthode de regroupement des attributs, celle que nous avons proposée et qui se base sur le concept des règles d'association. Après avoir déterminé les groupes d'attributs associés, nous utilisons la démarche proposée par Ishibuchi et al. (1992) pour générer les règles floues (partie 2.1), pour chaque groupe d'attributs. La classification d'un objet inconnu se fait par la méthode de classification de Ishibuchi et al. (1992) (partie 2.2).
Nous avons testé notre système SIFRA sur des bases de données qui diffèrent par le nombre d'attributs, le nombre d'exemples et le nombre de classes (Tab. 1). Pour évaluer la capacité de généralisation de notre méthode, nous avons adopté la technique de validation croisée d'ordre 10 ( Kohavi, 1995). Dans le tableau 2, nous présentons le taux de bonne classification suivi entre parenthèses du nombre de règles générées. Les meilleurs taux de classification sont présentés en gras. Le terme "imp" fait référence à l'impossibilité de générer les règles floues à cause du nombre de règles très élevé (supérieur à 10 5 ). Pour la phase de regroupement des attributs, nous avons utilisé une discrétisation à intervalles égaux et avons fixé le nombre d'intervalles à 3. D'autres tailles de discrétisation ainsi que d'autres méthodes de discrétisation pourront être étudiées dans de prochains travaux. Au niveau de la phase d'apprentissage, nous avons utilisé une partition floue homogène et une partition floue supervisée. Pour la partition homogène, nous avons testé plusieurs valeurs de la taille de partition k. Comme dans SIFCO, la valeur de k qui permet d'obtenir le meilleur taux de bonne classification dépend fortement des données de la base. Pour la partition floue supervisée, nous avons adopté la méthode MDLP de Fayyad et Irani (1993 Nous présentons dans Tab. 2 une comparaison de notre méthode SIFRA avec les deux méthodes SIF et SIFCO. Chacune des 3 méthodes possède des paramètres d'entrée à définir, à savoir la taille de la partition foue k, le seuil et la méthode de corrélation pour SIFCO, les seuils minSupp et ? min pour SIFRA. Pour comparer la performance des 3 méthodes et pour simplifier la lecture des résultats, nous présentons dans Tab. 2, pour chaque méthode, le meilleur taux de bonne classification obtenu en faisant varier ses paramètres d'entrée. D'après Tab. 2, il est clair que notre approche SIFRA fournit des taux de bonne classification très satisfaisants comparée à la méthode SIF, et des taux similaires ou meilleurs comparée à SIFCO. Comparée aux SIF, notre approche permet d'améliorer la performance de classification et de diminuer notablement le nombre de règles (en particulier avec les bases Wine, Vehicle et Sonar pour lesquelles la génération des règles avec SIF est impossible vu l'explosion de leur nombre). Comparée à SIFCO, notre méthode donne le meilleur taux de classification pour la base Iris avec un nombre de règles plus élevé mais qui reste faible (33). Pour Lupus, Wine et Sonar, les mêmes taux ont été obtenus par SIFRA et SIFCO. Concernant la base Vehicle, notre approche améliore considérablement le taux de bonne classification (67.73% contre 54.97% avec SIFCO) mais avec un nombre de règles plus important. Ce différentiel du nombre de règles s'explique par le fait que les groupes d'attributs liés détectés par SIFRA (basé sur les RA) contiennent plus d'attributs que les groupes détectés par SIFCO (basé sur une recherche de corrélation linéaire) (équation 7). Avec ces données, les liaisons entre attributs déterminées par notre approche semblent donc être plus pertinentes que celles trouvées avec SIFCO. 6 Conclusion

introduction
Les méthodes à noyau utilisées en analyse exploratoire des données (K-PCA, K-LDA, K-CCA, etc.) ou pour traiter des tâches de classification ou de régression (machines à support vectoriel, SVM) nécessitent, dans leurs fondements, l'usage de noyaux définis (positifs ou né-gatifs). Pourtant, bon nombre d'études relativement récentes en fouille de données temporelles présentent des résultats produits par de telles méthodes exploitant des noyaux temporellement élastiques (NTE) non définis Haasdonk (2005)  Zhang et al. (2010) ou régularisés par des mé-thodes spectrales Narita et al. (2007). L'émergence de nouvelles méthodes de régularisation pour NTE offre aujourd'hui des alternatives à l'exploitation des noyaux élastiques non définis que nous nous proposons d'évaluer de manière comparative sur des jeux de données simples mais potentiellement explicites. D'une manière générale, les procédures de régulation ont été développées pour approximer des noyaux non définis par des noyaux définis (ou semi-définis). Les premières approches appliquent directement des transformations spectrales aux matrices de Gram issues des noyaux non définis. Ces méthodes Wu et al. (2005)  Chen et al. (2009) consistent à i) changer le signe des valeurs propres négatives ou décaler ces valeurs propres en utilisant la valeur de décalage minimal nécessaire pour rendre le spectre des valeurs propres TAB. 1 -Liste des noyaux analysés positif, et ii) reconstruire la matrice de Gram issue du noyau avec les vecteurs propres d'origine afin de produire une matrice semi-définie positive. D'autres approches sont basées sur la recherche de la matrice de corrélation (matrice symétrique positive semi-définie ayant une diagonale unitaire) la plus proche de la matrice de Gram issue du noyau non défini, la proximité étant prise au sens d'une norme (norme de Frobenius pondérée) Higham (2002).
Cependant ces procédures de convexification sont difficiles à interpréter géométriquement Graepel et al. (1998) et l'effet attendu du noyau d'origine non défini peut être, selon les études, soit perdu ou pour le moins atténué par ces méthodes agissant directement sur le spectre matriciel, soit encore minime voir négatif comparativement à l'exploitation directe de la matrice non régularisée Chen et Ye (2008). Dans le contexte de l'alignement de séquences ou de séries temporelles, des approches de régularisation plus directes pour les NTE consistent à remplacer les opérateurs min ou max par un opérateur de sommation ( ) dans les équations récursives qui définissent les distances élastiques. Il en résulte qu'au lieu de ne considérer que le meilleur chemin d'alignement possible entre deux séries temporelles, le noyau régularisé effectue la somme des couts (ou gains) de tous les chemins d'alignement possibles avec un mécanisme de pondération qui cherche à favoriser les bons alignements et à pénaliser les mauvais alignements. Ces principes ont été appliqués avec succès par Saigo et al. (2004) pour la mesure (non définie) de Smith et Waterman (1981) très utilisée pour la comparaison de séquences symboliques, et plus récemment pour la speudo distance Dynamic Time Warping (DTW, Velichko et Zagoruyko (1970), Sakoe et Chiba (1971)) Cuturi et al. (2007), .
Nous développons dans cet article, sur la base d'une analyse en composantes principales à noyau (K-PCA), une étude expérimentale permettant d'évaluer les noyaux listés en table 1 sur de tâches de classification (supervisée et non-supervisée) de séries temporelles dans des sous espaces de dimension réduite. En nous limitant à des ensembles de séries temporelles de taille fixe, nous proposons ainsi de comparer expérimentalement au travers d'une analyse K-PCA un noyau Gaussien construit à partir de la distance Euclidienne (noyau défini positif, non temporellement élastique, ce noyau servant de base de référence), un noyau Gaussien construit à partir de la pseudo distance DTW (noyau non défini en général, mais élastique), une version régularisée du noyau précédent basée sur la recherche de la matrice de corrélation la plus proche Higham (2002), et enfin le noyau DTW régularisé suivant la méthode proposée par , K DT W , et une version normalisée, K Faisant suite aux travaux de Cuturi et al. (2007), la technique de régularisation développée dans  s'attache à transformer les équations récursives définissant la DTW (Dynamic Time Warping) de manière à produire une mesure de similarité notée K DT W constituant un noyau défini positif, c'est-à-dire s'apparentant à un produit scalaire dans un espace de Hilbert à noyau reproduisant. K DT W se distingue de l'approche proposée par Cuturi et al. en prenant la forme d'un noyau de convolution tel que défini par Haussler (1999) tout en imposant une condition sur les coûts locaux d'alignement moins restrictive. Pour rappel, un noyau défini sur R est est une fonction continue symétrique K :
Definition
La définition récursive du noyau K DT W est la suivante : • ? ? R + est un paramètre d'ajustement qui permet de pondérer les contributions locales, i.e. les distances entre les positions localement alignées, et
Ainsi, fondamentalement, l'opérateur min (ou max) est remplacé par un opérateur de sommation et une fonction de corridor symétrique (la fonction h dans l'équation récursive cidessus) est introduite pour, éventuellement, limiter la sommation et et donc la complexité algorithmique. Enfin, un nouveau terme récursif nécessaire à la régularisation (K xx ) est ajouté, de telle sorte que la preuve de la propriété de positivité du noyau peut être comprise comme une conséquence directe du théorème de convolution d ' Haussler (1999).
Normalisation
Le noyau K DT W effectue la somme sur l'ensemble des chemins d'alignement possibles des produits des termes locaux d'alignement e
Pour les séries temporelles de grandes tailles, ces produits deviennent infimes et K DT W incalculable lorsque ? est trop faible. Ainsi, le domaine de variation de K DT W s'amenuise en convergeant vers 0 lorsque ? tend vers 0 sauf lorsque l'on compare deux séries temporelles identiques (la matrice de Gram correspondante souffre ainsi d'une dominance diagonale). Comme proposé dans , une manière de palier ce problème consiste à considérer le noyau normalisé : . Si l'on oublie la constante de proportionnalité, cela revient simplement à élever le noyau
, ce qui montre que˜Kque˜ que˜K DT W est lui aussi défini positif (Berg et al. (1984), Proposition 2.7). Une correction de dominance diagonale similaire (sous-polynomial, i.e. t < 1) a initialement été proposée dans Schölkopf et al. (2002). L'effet de ce type de normalisation, sur le jeu de données SwedishLeaf (c.f. Table 2) est illustré en figure 1. La distribution des valeurs de la matrice de Gram associée au noyau non normalisé K DT W (évalué avec ? = 1) présente une très forte accumulation autour des valeurs très faibles (pour ? = 1, K DT W M in = 2.1e ? 77 et K DT W M ax = 1.9e ? 06 sur le jeu de données testé), tandis que la distribution des valeurs de la matrice de Gram associée au noyau normalisé˜Knormalisé˜ normalisé˜K DT W = K t DT W est plus diffuse. Les valeurs du noyau sont par ailleurs bornées :  
Complexité algorithmique
La définition récursive précédente permet de montrer que la complexité algorithmique liée au calcul du noyau K DT W est O(n 2 ), où n est la longueur des deux séries temporelles mises en correspondance, et lorsqu'aucun corridor n'est spécifié. Cette complexité est ramenée à O(c.n) quand un corridor symétrique de taille c est exploité par le biais de la fonction symmétrique h.
L'analyse en composantes principales non-linéaire, encore appelée ACP à noyau ou Kernel-PCA, Schölkopf et al. (1998) peut être vue comme une généralisation de l'ACP classique : elle permet d'engendrer une réduction de dimensionnalité non linéaire du point de vue de l'espace de représentation initial des données. Le principe consiste à projeter, par le biais d'une fonction non-linéaire ?(.), les données initiales dans un espace en général de plus haute dimension de sorte que l'image de la variété (non linéaire) contenant les données initiales devienne plus facilement linéairement séparable dans le nouvel espace, appelé espace des caractéristiques. Il suffit alors d'effectuer une ACP classique dans cet espace linéaire pour obtenir une réduction de dimensionalité non linéaire dans l'espace des données initiales. Si l'on exploite un noyau K(., .) défini positif, celui-ci induit de manière implicite une fonction nonlinéaire dite de mapping ?(.) telle que ?x, y, K(x, y) =< ?(x)
T , ?(y) >. Cette fonction ?(.) n'a pas besoin d'être connue explicitement (on évoque ici l'astuce du noyau).
Algorithm 1 ACP non linéaire 1: Choix du noyau (défini positif) k 2: Construction de la matrice de Gram à partir des données :
..,m 3: Centrage de la matrice de Gram (on retire la moyenne des données projetées dans l'espace des caractéristiques) : 
L'algorithme 1 présente succinctement les étapes de l'ACP non-linéaire, qui, à partir du choix d'un noyau défini positif, extrait les valeurs et vecteurs propres de la matrice de Gram centrée associée, puis projette toute donnée (initiale ou de test) dans un espace des caracté-ristiques de dimension réduite (d). Il est clair que l'ACP non-linéaire nécessite que le noyau utilisé soit défini positif.  Keogh et al. (2006). Ils sont de tailles modestes pour permettre (éventuellement) une visualisation la plus explicite possible en faible dimension. Le nombre de catégories varie de 2 à 50 et la longueurs des séries varie de 60 à 463.
Pour les 13 jeux de données listés en Table 2, et les 5 noyaux listés en Table 1, une ACP non linéaire est pratiquée puis les données sont projetées dans l'espace des caracté-ristiques obtenu en faisant varier le nombre de vecteurs propres, c'est à dire la dimension de l'espace réduit. Par exemple, en figure 2 les projections des séries temporelles du jeu de données Gun_Point sont présentées dans un espace des caractéristiques de dimension 3 pour les noyaux Gaussien-Euclidien, Gaussien-DTW régularisé par matrice de corrélation la plus proche (MCPP), K DT W et K t DT W . La valeur du paramètre t, exposant du noyau K t DT W , est estimé directement à partir des données d'apprentissage en évaluant les valeurs extrêmes prises par le noyau K DT W non normalisé. A titre d'exemple, on considère le jeu de données Gun_Point, pour lequel a matrice de Gram évaluée sur le noyau Gaussien DTW n'est pas dé-finie. Comme le montre la figure 2, les projections dans le sous-espace des caractéristiques de dimension 3 sont très proches pour le noyau Gaussien DTW et sa version régularisée par matrice de corrélation la plus proche. La séparation des classes est, sur cet exemple, bien meilleure pour le noyau KDT W et sa version normalisée KDT W t . A l'issue de l'ACP non linéaire, nous proposons une expérience de classification supervisée basée sur la règle du plus proche voisin (1-PPV) et une expérience de clustering basée sur l'algorithme des K-moyennes (K correspondant ici au nombre effectif de catégories du jeux de données). Pour chaque jeux de données et pour chaque noyaux testés la classification supervisée et non supervisée sont effectuées dans le sous espace des caractéristiques défini par K-PCA en faisant varier la dimension du sous-espace de 1 à 20 (pour dix dimensions, cela correspond à une réduction dimensionnelle variant de 83% à 98% pour les jeux de données testés).
La qualité de la classification est ensuite évaluée à partir du taux d'erreur de classification estimé à partir d'une validation croisée en 5 sous-échantillons. La précision et l'information mutuelle normalisée (IM N ) sont utilisées pour évaluer la qualité d'une classification non supervisée sur des données La précision est définie comme la fraction des individus correctement étiquetés, étant donné une correspondance 1-vers-1 entre les vraies classes et les clusters dé-couverts. Si p dénote une permutation quelconque des indices des clusters {˜c{˜c i } proposés (ou des vraies classes {c j }), la précision est alors définie par : L'information mutuelle normalisée, IM N , entre la vraie classification C et celle prédite˜Cprédite˜ prédite˜C est définie par :  Par ailleurs, le noyau Gaussien-DTW et sa variante régularisée à partir de la matrice de corré-lation la plus proche conduisent à des résultats très similaires et sensiblement moins bons comparativement aux noyaux KDT W et KDT W t . La régularisation par matrice de corrélation la plus proche ne semble donc pas apporter de bénéfice significatif en terme de classification supervisée ou non supervisée sur ces jeux de données par rapport au noyau DTW non défini. Il obtientles taux d'erreur de classification les plus faibles sur 10 des 13 jeux de données (CBF est mieux classé par le noyau Gaussien-DTW, ECG200 par le noyau Gaussien-Euclidien et Lighting2 par le noyau Gaussien-DTW régularisé par matrice de corrélation la plus proche). Pour la classification non supervisée, K t DT W obtient également les meilleurs résultats d'après les mesures Précision et IMN pour 8 des 13 jeux de données. Pour cette tâche, K DT W est meilleur sur les jeux de données yoga et Gun_Point, tandis que le noyau Gaussien-Euclidien se distingue sur ECG200 et les noyaux Gaussien-DTW sur Lighting2. Sur les jeux de données pour lesquels K t DT W n'arrive pas en tête, ce noyau se positionne entre le noyau Gaussien-Euclidien et les noyaux Gaussien-DTW. Contrairement à la régularisation par matrice de corrélation la plus proche, le principe de régularisation mise en oeuvre dans K DT W et K t DT W modifie en profondeur la nature même de la fonction de similarité sous-jacente à la mesure DTW en apportant en général une meilleure capacité à séparer ou regrouper les séries temporelles dans des espaces de dimension réduite.
Résultats et analyse
Conclusion
Nous avons évalué expérimentalement la capacité de quelques noyaux (élastiques, nonélastiques, définis, non-définis) à classer des séries temporelles de manière supervisée ou non dans des espaces de caractéristiques à dimension réduite obtenus par ACP non linéaire. Les résultats présentés montrent que les approches récentes de régularisation de noyaux élastiques offrent des alternatives bien meilleures que les principes classiques de régularisation basés sur des approches spectrales portant directement sur les valeurs propres des matrices de Gram construites à partir des noyaux non définis. Le noyau DTW régularisé K DT W exploité dans cet article dans sa version normalisé K t DT W offre un bon compromis entre les noyaux définis non-élastiques (tel que le noyau Gaussien-Euclidien) et les noyaux non définis élastiques (tel que le noyau Gaussien-DTW). Non seulement celui-ci conserve une caractéristique d'élasti-cité temporelle tout en étant défini positif, mais il se marie également bien avec les approches à noyau tel que l'ACP non linéaire. Sa capacité à proposer des espaces de caractéristiques discriminantes en dimension réduite en font un outil en général efficace pour l'analyse exploratoire d'ensembles de séries temporelles. Ces résultats confirment et complètent ainsi ceux présentés par  et  dans le cadre d'une classification supervisée par machine à support vectoriel en apportant un éclairage sur la normalisation de ce type de noyau.

Introduction
À cause de l'explosion du nombre d'informations stockées et partagées sur Internet, et l'introduction de nouvelles technologies pour capturer ces données, l'analyse des données incertaines est devenue essentielle dans de nombreuses applications pour la prise de décision. Pour gérer et traiter l'incertitude des données, plusieurs modèles ont été proposés, ce qui a donné naissance à différents types de bases de données imparfaites. Nous pouvons citer les plus connues : les bases de données probabilistes présentées par Dalvi et Suciu (2007); Aggarwal et Yu (2009), les bases de données possibilistes introduites par Bosc et Pivert (2010) et les bases de données évidentielles basées sur la théorie de Dempster-Shafer proposées par Bell et al. (1996). L'utilisation des bases de données évidentielles offre plusieurs avantages à savoir : (i) Elles permettent de modéliser l'incertitude et l'imprécision des données ; et (ii) cela représente une généralisation des deux modèles ; probabiliste et possibiliste à la fois. Dans cet article, nous nous nous intéressons aux requêtes Skyline sur des données incertaines où l'incertitude est modélisée par la théorie de l'évidence, ce qui constitue un travail pionnier.
Le reste cet article est organisé comme suit. La section 2 contient un rappel sur l'opérateur Skyline, les notions de base de la théorie de l'évidence et les bases de données évidentielles. Dans la section 3, nous définissons formellement la relation de dominance et modélisons le Skyline évidentiel. Nos expérimentations sont données dans la section 4. Enfin, la section 5 conclut l'article.
Notions de base
Dans cette section, nous présentons d'abord les requêtes Skyline sur des données classiques Borzsonyi et al. (2001). Ensuite, nous présentons les notions de base de la théorie de l'évidence et les bases de données évidentielles.
Les requêtes Skyline
Pour simplifier, nous supposons que la valeur la plus élevée, est la plus préférée.  
Définition 1 (Relation de Dominance) Étant donnés deux objets
La théorie de l'évidence
La théorie de l'évidence a été introduite par Shafer (1976) dont le but est d'évaluer subjectivement la vérité d'une proposition. Cette théorie, aussi connue sous le nom de "théorie des fonctions de croyance" est une généralisation de la théorie bayésienne Dempster (1968). Elle représente un ensemble d'hypothèses désigné par le cadre de discernement.  
Les bases de données évidentielles (BDE)
Les BDE permettent de représenter les données manquantes, incertaines ou imprécises. 
Les requêtes Skyline en présence de données évidentielles
Dans cette section, nous introduisons la relation de dominance entre les objets dont l'incertitude est modélisée par la théorie des fonctions de croyance, par la suite, nous présentons la définition du Skyline évidentiel. Étant donné un ensemble d'objets O = {O 1 , O 2 , . . . , O n } défini sur un ensemble d'attributs A = {a 1 , a 2 , . . . , a d }, avec o i .a k représente l'ensemble des éléments focaux de l'objet o i et l'attribut a k ; par exemple 1 , o 1 .wl = { 16, 18}, 0.1 20}, 0.9 et o 1 .r = { 0.5 100}, 0.5 Le degré de croyance qu'une valeur incertaine de l'objet o i définie sur l'attribut a k est préférée ou égale à une autre valeur de l'objet o j , est donné par Bell et al. (1996) :
Dans notre exemple, nous avons bel(o 1 .wl ? o 3 .wl) = 0.3 · (0.1 + 0.9) + 0.7 · (0.1 + 0.9) = 1, et bel(o 1 .r ? o 3 .r) = 0.7 · 0.7 + 0.3 · 0.7 = 0.7.
Étant donnés deux objets
Le Tableau 2 présente les degrés de croyance que chaque objet en ligne, domine un autre objet en colonne.
Les objets o i et o j comparés sont supposés différents, ce qui fait que la relation ? suffit pour exprimer que o i domine o j (au sens de (2)). Remarquons que cette définition se réduit à la 
Par exemple, o 1 0.9-domine o 2 et o 4 . Mais, il ne 0.9-domine pas o 4 car bel(o 1 ? o 4 ) = 0.3 < 0.9. Intuitivement, un objet est dans le Skyline s'il n'est pas dominé par rapport à un certain seuil b. 
Définition 7 (b-dominant skyline) Le skyline de O, désigné par b-Sky O , comprend les objets dans O qui ne sont pas b-dominés par aucun autre objet, i.e., b-Sky
Théorème 1 Étant donnés deux seuils de croyance
Preuve 1 Supposons qu'il existe un objet
Le théorème 1 indique que la taille de b-dominant skyline est plus petite que celle de b ? -dominant skyline si b < b ? . Les utilisateurs ont donc la possibilité de contrôler la taille des objets que contient le Skyline évidentiel en faisant varier le seuil de croyance. obtenus. Dans chaque expérimentation., nous varions un seul paramètre, tandis que les autres paramètres prennent leurs valeurs par défaut. Le Tableau 3 montre ces paramètres et leurs symboles ; les valeurs par défaut sont en gras. La figure 1.a montre que la taille du skyline évidentiel augmente avec l'augmentation de n. Dans la figure 1.b, on montre que la taille du skyline évidentiel augmente aussi de façon significative avec l'augmentation de d. 
Étude Expérimentale
Conclusion
Dans cet article, nous avons abordé le problème des requêtes skyline dans le cadre des bases de données évidentielles et nous avons introduit un nouveau type de skyline. Notre étude expérimentale a démontré la faisabilité et la flexibilité du skyline évidentiel. Comme perspective, nous envisageons de développer des techniques de classement des objets retournés par l'opérateur skyline évidentiel.

Introduction
L'analyse en composantes principales (ACP, Jolliffe (1986)) est une des méthodes, si ce n'est la méthode, d'analyse exploratoire les plus couramment utilisées. Elle a été ré-interprétée sous un formalisme probabiliste par Tipping et Bishop (1999), montrant que les composantes principales pouvaient être estimées par maximum de vraisemblance dans le cadre d'un modèle à variables latentes. Avec l'avénement des données de grande dimension, la problématique consistant à sélectionner un petit nombre de variables d'intérêt parmi l'ensemble des variables disponibles est devenue primordiale. Un des soucis majeurs de l'ACP dans cette optique est que les composantes principales sont définies comme une combinaison linéaire de l'ensemble des variables initiales. Des versions parcimonieuses de l'ACP (Zou et al., 2004) ainsi que de sa version probabiliste (Guan et Dy, 2009) ont été proposées récemment. La version parcimonieuse de Zou et al. (2004) repose sur l'ajout d'une pénalisation de type 1 au problème des moindres carrés, qui nécessite le choix du coefficient de pénalisation de façon heuristique. Dans Guan et Dy (2009), une version sparse bayésienne de l'ACP probabiliste est proposée. Nous proposons dans ce travail une alternative fréquentiste utilisant un algorithme EM pour l'inférence. La procédure d'estimation obtenue à l'avantage d'être particulièrement simple, et ne nécessite pas le choix de loi a priori. Elle offre en outre la possibilité de considérer le problème du choix de la pénalité comme un problème de choix de modèles.
Analyse en composantes principales probabiliste
Soit y un vecteur aléatoire observé de dimension p, et x un vecteur aléatoire latent (non observé), de dimension d, relié à y par l'équation suivante :
où W est une matrice p × d, µ est le vecteur moyenne (supposé nul dans la suite, µ = 0), et ? N (0, ? 2 I). Conditionnellement au vecteur x, la distribution des vecteurs observés est :
En supposant x ? N (0, I), la distribution marginale du vecteur observé est :
Ce modèle est un modèle de type "factor analysis" (Bartholomew et al., 2011), popularisé par Tipping et Bishop (1999) sous le nom d'analyse en composantes principales probabiliste (Probabilistic Principal Component Analysis (PPCA)). En effet, une estimation par maximum de vraisemblance des paramètres du modèle à l'aide d'un algorithme EM (Dempster et al., 1977), considérant le vecteur latent x comme manquant, conduit à estimer les colonnes de W par les vecteurs propres de la matrice de covariance empirique, vecteurs qui ne sont rien d'autres que les axes principaux classiques. Soit y 1 , . . . , y n un échantillon i.i.d de vecteurs observés. L'algorithme EM consiste à maximiser de façon itérative la log-vraisemblance complétée par les données non observées x 1 , . . . , x n :
3 Une version parcimonieuse de l'analyse en composantes principales probabiliste
Dans ce travail, nous considérons une version parcimonieuse de l'analyse en composantes principales probabiliste. L'objectif est d'obtenir des axes principaux déterminés uniquement grâce à un nombre restreint de variables initiales, et ainsi faciliter leur interprétation. De plus, comme nous le verrons par la suite, l'approche probabiliste de l'ACP permet de sélectionner le paramètre de pénalité par des méthodes classiques de sélection de modèles.
Dans l'optique d'introduire de la parcimonie au sein de axes principaux, nous considérons une pénalité 1 sur les colonnes de la matrice W. La vraisemblance complétée à maximiser est alors la suivante :
où w = (w 1 , . . . , w pp ) t est la colonne de W et ? > 0 est le paramètre de pénalisation. L'algorithme EM est un algorithme itératif qui alterne deux étapes (E et M), décrites ci-après.
La q-ème itération de l'étape E consiste à calculer l'espérance de pen c sous la loi p(x|y, ? (q) ), où ? (q) = (W (q) , ? 2(q) ) est la valeur courante de l'estimation des paramètres du modèles :
est une constante indépendante des paramètres du modèle et où
De sorte à faciliter la maximisation, nous considérons l'approximation de la norme 1 par la forme quadratique suivante (Fan et Li, 2001) :
j w j . La maximisation de Q(?, ? (q) ) en fonction de W n'ayant pas de solution analytique, nous utilisons une approche alternative consistant à maximiser Q(?, ? (q) ) en fonction de chaque éléments de la matrice W successivement. On obtient alors, en dérivant Q(?, ? (q) ) par rapport à w j et en égalant à 0 :
Cette dérivation élément par élément ne conduit pas nécessairement au maximum de Q(?, ? (q) ), mais suffit pour faire augmenter la log-vraisemblance à chaque étape de l'algorithme. On obtient alors un algorithme GEM (Generalized EM) qui conserve les mêmes propriétés de convergence qu'un algorithme EM classique. L'estimateur de la variance résiduelle est quant à lui :
et s'avère être identique à celui de la version non sparse de l'analyse en composantes principales probabiliste (Tipping et Bishop, 1999).
4 Sélection de ? par l'heuristique de pente Dans Zou et al. (2004), le choix de la pénalité ? est réalisé de manière heuristique en se basant sur l'éboulis des valeurs propres. L'idée de la stratégie que nous proposons est d'estimer le modèle sur une grille de valeurs de ?, et d'utiliser un outil de sélection de modèles pour choisir le meilleur modèle. Les outils classiques de sélection de modèles sont par exemple les critères AIC (Akaike, 1974) et BIC (Schwarz, 1978), qui pénalisent la log-vraisemblancêvraisemblancê ?) de la façon suivante :
, où ? est le nombre de paramètres libres du modèles et n le nombre d'observations. La valeur de ? dépend directement de la valeur de ? puisqu'elle est égale au nombre d'éléments non nuls dans W plus un (pour la variance résiduelle). Même si ces critères sont largement utilisés et asymptotiquement consistants, ils sont aussi connus pour être plus efficaces sur simulations que sur données réelles.
Pour surmonter ce problème, Birgé et Massart (2007) ont récemment proposé une mé-thode dirigée par les données pour calibrer la pénalité des critères pénalisés, connue sous le nom d'heuristique de pente. L'heuristique de pente a été proposé initialement dans un cadre d'un modèle de régression gaussien homoscédastique, mais a ensuite été étendue à d'autres situations. Birgé et Massart (2007) ont démontré qu'il existait une pénalité minimale et que de considérer une pénalité égale au double de la pénalité minimale permettait d'approcher le modèle oracle en terme de risque. La pénalité minimale est en pratique estimée par la pente de la partie linéaire de la log-vraisemblance pen c ( ˆ ?) exprimée en fonction de la complexité du modèle. Le critère associé est alors défini par :
oùˆsoùˆ oùˆs est l'estimation de la pente de la partie linéaire de pen c ( ˆ ?). Une revue détaillée et des conseils d'implémentation sont donnés dans Baudry et al. (2012).
Illustrations numériques
Nous choisissons pour illustrer notre méthodologie un jeu de données classique issu de l'UCI machine learning repository : le jeux de données USPS. Le jeu original contient 7291 images représentant des chiffres manuscrits de 0 à 9. Chaque chiffre est une image en niveaux de gris de taille 16 × 16, représentée par un vecteur de dimension 256. Pour cette expérience, nous avons extrait un sous ensemble de 1756 images correspondant aux chiffres 3, 5 et 8. Nous réalisons sur ces données une ACP ainsi que l'ACP parcimonieuse que nous proposons. Pour cette dernière, nous fixons le nombre maximum d'itérations de l'algorithme EM à 500 et le seuil de convergence à 10
, et nous considérons une grille de valeurs de ? de 0 à 150, avec un pas de 1. La méthode de l'heuristique de pente (figure 1) conduit à choisir ? = 126. Dans un but illustratif, nous discutons ici les résultats concernant les deux premières composantes principales. La figure 2 représente la projection des 1756 images dans le premier plan principal de l'ACP ainsi que les deux premières composantes principales, tandis que la figure 3 propose la même représentation pour l'ACP parcimonieuse. Nous pouvons noter que les deux méthodes définissent un premier plan principal relativement discriminant vis-à-vis des trois types d'images. Tout l'intérêt de l'ACP parcimonieuse est que les composantes principales   

Introduction
Avec la récente explosion du nombre d'images et de données satellites disponibles, la conception de systèmes capables d'interpréter automatiquement de telles données est devenue un domaine florissant. En effet, les satellites modernes sont capables d'acquérir des images à très hautes résolutions (THR) avec une définition de plus en plus élevée sur un large domaine spectral. Or, les algorithmes capables de traiter un tel volume de données en un temps raisonnable sont pour le moment assez rares.
La segmentation de telles données d'imageries peut se faire en utilisant des algorithmes basés sur les champs de Markov, (Roth et Black, 2011). Les champs de Markov reposent sur la notion de voisinage pour modéliser les dépendances qui peuvent exister entre des données telles que des pixels adjacents, ou des super-pixels adjacents (groupes de pixels).
Dans ces modèle, on considère S = {s 1 , ..., s N }, s i ? 1..K un ensemble de variables aléatoires représentant les états (labels) des données. Ces états sont supposés liés dans l'espaces par des relations de voisinages et émettent des observations X = {x 1 , ..., x N } où les x i sont des vecteurs contenant les attributs de chaque donnée (RGB pour les modèles les plus simples). L'objectif est alors de déterminer la configuration idéale de S, c'est à dire de trouver les valeurs des s i afin d'obtenir une segmentation optimale.
Une des méthodes possibles pour résoudre ce problème est l'utilisation du couple ICM-EM, (Zhang et al., 2001). Le choix de l'algorithme ICM (Besag, 1986) vient du fait que cet algorithme a une complexité plus faible que celles des algorithmes plus récents utilisés pour les champs de Markov, ce qui est un atout non-négligeables pour traiter le volume important des données d'une image à très haute résolution. De plus, la plupart des données issues de telles images sont déjà pré-traitées et l'ICM est donc suffisant pour les segmenter. Il est également important de préciser qu'à ce jour, l'algorithme ICM est le seul à avoir été adapté pour pouvoir optimiser le modèle d'énergie contenant des informations sémantiques utiles dans le cadre des image satellites que nous avons proposé dans de précédent travaux (Sublime et al., 2014). L'objectif de l'algorithme ICM est d'optimiser itérativement une fonction d'énergie locale dérivant du logarithme de P (x|s, ? s )P (s). Bien que cet algorithme ait montré son efficacité pour résoudre ce problème, il a cependant plusieurs défauts tels que son critère d'arrêt basé sur l'évolution de l'énergie global et l'absence de garantie de convergence. En effet, cet algorithme essaye d'optimiser une fonction non-convexe, et il a été montré qu'après un processus d'optimisation relativement rapide de forme parabolique, l'algorithme ne se stabilisait pas toujours et pouvait même se mettre à diverger entrainant ainsi une détérioration des résultats (Zhang, 1989). Une des difficultés récurrente de la segmentation d'image dans le cadre non-supervisé est qu'il est difficile d'évaluer la qualité des résultats. Dans le cas de l'ICM, le critère d'arrêt repose sur l'énergie globale liée aux champs de Markov (somme des énergies locales) et sur l'hypothèse que cette énergie va se stabiliser. Le problème d'une telle approche est précisément que cette énergie globale ne se stabilise pas toujours, l'algorithme n'ayant pas de garantie de convergence, et que même dans le cas où elle se stabilise cette stabilisation n'intervient pas né-cessairement au moment où les résultats de la segmentation sont les meilleurs. De plus, en se basant sur l'énergie globale avec un nombre de données élevé, il y a un risque non négligeable d'"overflow" ou d'arrondi de cette énergie.
Dans cette article, nous proposons un nouveau critère d'arrêt pouvant être facilement calculé et qui repose sur un modèle d'énergie adapté aux images satellites à très haute résolution.
Algorithme proposé
On note V x le voisinage d'une observation x, et A = {a i,j } K×K la matrice de voisinage de l'image, où chaque a i,j est la probabilité de passer de l'état i à l'état j entre deux données voisines. A partir de ces notations nous utilisons comme modèle d'énergie locale le modèle défini lors de nos précédents travaux (Sublime et al., 2014) :
La fonction d'énergie décrite précédemment utilise une énergie locale dérivant de la loi normale dans laquelle µ s est la valeur moyenne associée à l'état s et ? s sa matrice de covariance. Le dernier membre de cette énergie qui décrit l'énergie de voisinage est une fonction positive calculée à partir des éléments de la matrice A. Le facteur ? x,v représente le poids de v en tant que voisin de la donnée x selon la proportion de frontière qu'il occupe. Ce modèle repose sur l'idée que des données voisines ayant des clusters différents ne sont pas nécessaire-ment incompatibles, et peut être vu comme une version relaxée du graph-cuts (Boykov et al., 2001).
La matrice de voisinage A de ce modèle d'énergie contient des informations sémantiques telles que les affinités des différents clusters ou leur compacité sur l'image. En effet, la diagonale de la matrice A contient la probabilité pour chaque cluster d'avoir un voisinage plus ou moins composé d'éléments du même cluster. Cette diagonale peut par conséquent être considérée comme un indicateur de compacité des clusters. Les éléments non-diagonaux quant à eux fournissent des informations sur les affinités des clusters.
Étant donné que l'objectif principal de l'utilisation des champs de Markov en segmentation d'image est d'obtenir des zones homogènes, nous avons décidé d'utiliser cette information de compacité contenue dans notre matrice de voisinage et d'en faire le nouveau critère d'arrêt de notre algorithme C-ICM : "Compactness-based Iterated Conditional Modes".
Il est en effet raisonnable de supposer qu'il faut arrêter l'algorithme lorsque la compacité des clusters cesse d'augmenter. Aussi, nous utilisons les variations de la trace de la matrice A comme nouveau critère d'arrêt. Ce critère présente plusieurs avantages : Les calculs sont faciles et plus rapides que ceux pour obtenir l'énergie globale, ce critère permet également de repérer les cas où un cluster commence à en absorber d'autres, ce qui arrive assez souvent avec l'ICM. Un tel cas de figure provoquerait rapidement la convergence de la trace de A vers 1. Dans l'algorithme (1), nous montrons comment nous avons adapté le framework EM-ICM pour notre critère d'arrêt : Comme on peut le voir sur la Figure (2), le critère d'arrêt basé sur l'énergie aurait conduit l'algorithme à s'arrêter après la 8ème itération. Or, sur la Figure (1) l'état de fusion des zones homogènes n'est pas encore assez avancé après l'itération 8 : La mer et une partie des bâtiments sont encore très fragmentés. Le critère de compacité aurait de son côté amené l'algorithme à s'arrêter après l'itération 43 ce qui aurait conduit à des zones nettement plus homogènes : routes, eau, gros bâtiments, etc.
On notera également que les deux critères se stabilisent définitivement après l'itération 47, itération après laquelle l'image n'évolue presque plus. Sur l'image prise après la 50ème itéra-tion de l'algorithme, on constate même le début assez marqué d'un phénomène de détérioration avec certains clusters qui ont commencé à déborder sur d'autres.
De cette première expérience, nous pouvons tirer les conclusions suivantes : Tout d'abord, elle confirme la difficulté évoquée dans notre introduction de trouver un critère d'arrêt idéal. En effet, le critère basé sur l'énergie aurait ici arrêté la segmentation trop tôt et l'énergie rebondit deux fois avant d'atteindre sa stabilisation finale. Ensuite, on voit que notre critère basé sur la compacité semble plus stable : il n'y a pas de rebond. Enfin, on notera que lorsque les deux critères semblent finalement se mettre d'accord pour arrêter la segmentation (itération 43 pour la compacité, et itération 47 pour la stabilisation définitive de l'énergie), on s'aperçoit que nous sommes déjà dangereusement proche de la zone à partir de laquelle la segmentation commence à se détériorer.
Données THR Strasbourg
Notre seconde expérience a été effectuée sur un jeu de données construit à partir d'une image satellite à très haute résolution de la ville de Strasbourg (Rougier et Puissant, 2014). Ce jeu de données pré-traitées utilise le modèle des super-pixels (agglomérats de pixels) avec des voisinages irréguliers : chaque super-pixel a entre 1 à 15 voisins. L'image est représentée sous forme de 187.058 super-pixels ayant chacun 27 attributs radio-métriques et géométriques. On constate à l'issue de cette expérience que notre indice de compacité tombe d'accord avec l'indice de qualité de clustering pour déterminer quand arrêter l'algorithme. On notera tout de même qu'il y a peu de différence en terme de qualité de résultats entre le moment d'arrêt décidé par le critère de compacité, et celui décidé par le critère classique d'énergie. Cependant, sur une image satellite à très haute résolution de cette taille, 2 itérations supplé-mentaires coûtent plusieurs minutes de calcul pour avoir dans le cas de cette expérience un résultat légèrement moins bon. Notre critère d'arrêt semble donc être ici un choix plus judicieux pour décider d'arrêter l'algorithme au bon moment et économiser du temps de calcul.
Conclusion
Dans cet article, nous avons proposé une amélioration de l'algorithme ICM pour la segmentation des images satellites à très haute résolution. Notre algorithme C-ICM introduit un nouveau critère d'arrêt basé sur un modèle d'énergie spécifique permettant d'avoir des informations sur les relations entre les différents clusters. Notre critère repose ainsi sur la compacité et l'homogénéité des clusters dans la segmentation plutôt que sur le traditionnel critère d'énergie globale. Nos expériences préliminaires ont montré des résultats intéressants qui pourraient mener à une amélioration globale de l'efficacité et à une vitesse accrue du traitement des

Introduction
Le modèle des graphes conceptuels (Sowa, 1984;Chein et Mugnier, 2009) permet de repré-senter des connaissances sous la forme d'un graphe étiqueté. Le modèle des graphes conceptuels utilise une représentation graphique visuelle des connaissances afin de faciliter la compré-hension pour les utilisateurs. La méthode d'interrogation du modèle est basée sur l'opération principale des graphes conceptuels, un homomorphisme de graphes appelé la projection : cette opération permet de déterminer si les connaissances exprimées dans un graphe conceptuel appelé graphe requête peuvent être déduites de celles exprimées dans la base de connaissances, représentée par un graphe conceptuel appelé graphe fait.
Les objectifs de ce modèle sont proches d'une partie de ceux des langages du Web sé-mantique tels que RDF, RDF-Schema ou OWL (Manola et al., 2004;Brickley et Guha, 2004;McGuinness et al., 2004) qui sont généralement interrogés en utilisant SPARQL (Garlik et al., 2013), une recommandation officielle du W3C disponible dans plusieurs outils. SPARQL offre plus de flexibilité par rapport aux graphes conceptuels dans l'interrogation d'une base de connaissances. D'une part, SPARQL permet d'exprimer une disjonction entre plusieurs parties d'une requête (SPARQL utilise le mot « union ») et d'identifier des parties comme obligatoires ou optionnelles. D'autre part, SPARQL permet d'interroger la base grâce à quatre types de requêtes : l'interrogation, la sélection, la description et la construction. La requête d'interrogation permet de savoir si la connaissance représentée par la requête est présente dans la base. La requête de sélection permet de trouver et extraire de la base des connaissances identifiées dans la requête comme importantes. La requête de description permet d'obtenir des informations sur des connaissances de la requête. La requête de construction permet de déduire de nouvelles connaissances à partir de celles contenues dans la base.
Cet article propose de combiner la simplicité de la représentation visuelle des graphes conceptuels avec la puissance du modèle d'interrogation du Web sémantique pour améliorer l'expression des requêtes des graphes conceptuels. La contribution de cet article est triple. D'abord, le graphe d'interrogation est introduit. La notion de graphe d'interrogation permet d'exprimer des conditions de disjonction -un « ou » entre deux de ses sous-graphes -et des conditions d'option -un sous-graphe est préféré mais non-nécessaire -dans un graphe conceptuel. Ensuite, le graphe d'interrogation nous permet de définir un langage d'interrogation pour les graphes conceptuels formé de quatre types de requêtes : requête d'interrogation, requête de sélection, requête de description et requête de construction. Une requête d'interrogation permet de savoir si le graphe de la requête est déductible du graphe fait. Une requête de sélection permet de trouver et extraire du graphe fait des sommets identifiés comme étant importants dans le graphe requête. Une requête de description permet d'obtenir des informations complémentaires du graphe fait liées à un sommet particulier du graphe requête. Une requête de construction permet de déduire de nouvelles connaissances en utilisant les connaissances extraites du graphe fait. Enfin, l'opération basique de calcul utilisée pour interroger et obtenir des réponses est introduite : la projection d'un graphe d'interrogation dans un graphe fait. Cette projection est définie en utilisant la projection classique d'un graphe conceptuel dans un graphe fait.
Aucun langage générique d'interrogation pour les graphes conceptuels n'a encore été proposé, mais différentes idées ont été mises en avant. De même que la requête de sélection, la possibilité d'identifier certains sommets dans un graphe requête pour facilement exploiter le résultat d'une projection a déjà été proposé dans Sowa (1984). Notre approche est différente de Sowa (1984) dans lequel les sommets sont simplement marqués puisque nous proposons de les nommer pour pouvoir facilement les identifier dans le résultat de la projection. Dans la communauté de SPARQL, Corby et Faron-Zucker (2007) propose une implémentation de la recherche de motifs de graphes de SPARQL en utilisant la projection classique des graphes conceptuels. Notre approche, à l'inverse, est de transposer les idées de SPARQL dans le modèle des graphes conceptuels.
L'article est organisé comme suit. La section 2 rappelle les bases du modèle des graphes conceptuels et de la projection. La section 3 présente le modèle des graphes d'interrogation. La section 4 présente la projection d'un graphe d'interrogation dans un graphe conceptuel fait ainsi que la requête d'interrogation, La section 5 présente la requête de sélection, La section 6 présente la requête de description, La section 7 présente la requête de construction. Enfin, la section 8 présente notre implémentation et donne quelques éléments pour comparer notre langage d'interrogation des graphes conceptuels avec SPARQL.
Modèle des graphes conceptuels
Le modèle des graphes conceptuels (Chein et Mugnier, 2009)  
Homme Femme aPourPère(Humain, Homme)
, ?) avec T C partiellement représenté par l'arbre de gauche, T R partiellement représenté par l'arbre de droite, et la signature ? de chaque relation est précisée à coté de chaque type de relation.
Exemple. La figure 1 montre un extrait du vocabulaire utilisé dans les exemples suivants.
Un graphe conceptuel est un multigraphe biparti défini sur un vocabulaire. Un des ensembles de sommets est appelé l'ensemble des sommets concepts, et l'autre ensemble est appelé l'ensemble des sommets relations, représentant les liens entre les concepts. Chaque sommet est étiqueté. Un sommet relation est étiqueté par un type de relations et un sommet concept est étiqueté par un couple formé d'un type de concepts et d'un marqueur. Si un concept c est le i-ème argument d'une relation r, alors il y a une arête entre c et r étiquetée par i, et le type de concepts de c doit respecter les contraintes de la signature de r. Un sommet concept individuel est référencé par un marqueur individuel de I. Un sommet concept générique est ré-férencé par le marqueur générique * . Nous considérons les graphes sous forme normale : deux sommets concepts différents ne peuvent pas être étiquetés par le même marqueur individuel.
Un graphe conceptuel défini sur V est un quadruplet G = (C, R, E, l) qui satisfait les conditions suivantes :
-(C, R, E) est un multigraphe fini, non-orienté et biparti. C est l'ensemble des sommets concepts, R est l'ensemble des sommets relations, E est l'ensemble des arêtes 
La projection est l'opération d'interrogation du modèle des graphes conceptuels. Soient un graphe requête G r et un graphe fait G f définis sur le même vocabulaire, G r se projette dans G f si les informations représentées par G r se déduisent de celles représentées par G f .
Les sommets de G f qui correspondent aux sommets de G r sont appelés les images des sommets de G r par ?.
Un bloc est défini comme un ensemble de sommets du graphe associé à un type de bloc. Tous les sommets concepts de l'ensemble doivent avoir leurs sommets relations voisins dans l'ensemble : ceci permet à un sommet concept du bloc d'être caractérisé par les relations auxquelles il est lié. Différents types de blocs permettent d'identifier la condition d'un bloc : l'Option (le bloc est dit optionnel), la Disjonction (le bloc est dit de disjonction) et le Standard (le bloc est dit standard).
Un bloc optionnel permet d'exprimer que la partie du graphe qu'il contient est facultative : on recherche dans le graphe fait un graphe avec la partie optionnelle, mais si elle n'est pas trouvée, un graphe privé de la partie optionnelle conviendra. Un bloc de disjonction est composé de blocs fils et exprime une disjonction entre ses blocs fils : on recherche dans le graphe fait un graphe avec seulement un des fils.
Un bloc du graphe G est un couple b = (S, T ) où -S ? C ? R est l'ensemble des sommets du bloc tel que :
?c ? S ? C, ?r ? R, si rc ? E alors r ? S -T ? {Standard, Option, Disjonction} est le type du bloc. Une arborescence est utilisée afin de structurer les blocs d'un graphe requête. Les blocs sont les sommets de l'arborescence. Les notions de bloc racine, bloc père et blocs fils sont définies grâce au vocabulaire lié aux arborescences. La structuration est hiérarchique, elle impose que la relation père-fils entre blocs vérifie l'inclusion des sommets du bloc fils dans ceux du bloc père et que si deux blocs ont un sommet en commun, alors un de ces blocs est un ancêtre de l'autre.
Standard Disjonction
Définition 5. Soit un graphe conceptuel G = (C, R, E, l).
Une structuration H de G est une arborescence (B, A, r) où la racine r est le bloc (C ? R, Standard), B étant l'ensemble des blocs de G qui forme l'ensemble des sommets de l'arborescence, et A l'ensemble des arcs. La structuration est telle que : H) est un couple où G est un graphe conceptuel et H est une structuration de G. 
Projection et requête d'interrogation
Les graphes d'interrogation sont utilisés comme base de construction pour chacun des quatre types de requêtes.
Un graphe d'interrogation peut être développé en un ensemble de graphes conceptuels en suivant les conditions d'option et de disjonction. Cet ensemble de graphes conceptuels, appelé l'ensemble des graphes développés du graphe d'interrogation, représente l'ensemble des graphes conceptuels dont on cherche à savoir s'ils sont déductibles du graphe fait. Cet ensemble permet de définir une projection d'un graphe d'interrogation dans un graphe fait comme étant une projection d'un graphe développé dans le graphe fait. Pour obtenir un graphe développé à partir d'un graphe d'interrogation, des opérations sont appliquées sur le graphe d'interrogation pour aboutir à un graphe ne contenant que des blocs standards. Les blocs optionnels seront retirés ou leur type sera changé en Standard. Le type des blocs de disjonction sera changé en Standard et seulement un et un seul de ses fils sera conservé.
Soit b ? B, un bloc optionnel. Le graphe G i dans lequel le type de b est changé en Standard est un développement
Le graphe G i dans lequel le type de b est changé en Standard et dans lequel tous les fils de b sauf un sont retirés est un développement de G i .
L'ensemble des graphes développés GD de G i est l'ensemble de tous les graphes conceptuels associés aux graphes d'interrogation ne contenant que des blocs standards obtenus grâce à une suite d'applications de l'opération de développement à partir de G i . Exemple. Les graphes conceptuels de la figure 4 forment l'ensemble des graphes développés
Pour définir la projection d'un graphe d'interrogation dans un graphe fait, on construit les projections des graphes développés dans le graphe fait. La projection d'un graphe développé est généralement une projection du graphe d'interrogation. L'exception est due à la sémantique de l'option, en effet, la partie optionnelle d'un graphe doit être présente dans la réponse si elle est présente dans le graphe fait : dans ce cas les projections qui ne contiendraient pas le bloc optionnel ne sont pas des projections du graphe d'interrogation. Notons que ces dernières projections sont prolongées 2 par la projection contenant les informations optionnelles. On définit donc l'ensemble des projections d'un graphe d'interrogation dans un graphe conceptuel comme l'ensemble des projections des graphes développés du graphe d'interrogation privé des projections prolongées.
L'ensemble des projections du graphe d'interrogation G i dans G f est : 
Requête de sélection
Dans une requête d'interrogation, l'image de certains sommets peut être plus importante que l'image des autres sommets de la requête. Cet article propose un type de requête dans lequel on peut identifier les sommets jugés importants et dont on veut retenir les images par la projection du graphe d'interrogation : la requête de sélection.
Une requête de sélection est donc un couple composé d'un graphe d'interrogation et d'une fonction de nommage qui associe à des noms à sélectionner, des sommets du graphe.
Définition 11. Soit un ensemble de noms N .
Une requête de sélection est un couple
Une réponse à une requête de sélection est une fonction qui associe à chaque nom de l'ensemble des noms à sélectionner de la requête, un sommet du graphe fait. Si le graphe d'interrogation se projette dans le graphe fait, les noms à sélectionner seront associés avec l'image dans le graphe fait, si elle existe, du sommet du graphe d'interrogation auxquels ils sont liés. Un sommet peut ne pas avoir d'image par la projection, dans ce cas, le nom n'est associé à aucun sommet. 
Une réponse à la requête de sélection R s est une fonction partielle de N dans C f ? R f associant à chaque élément n de N le sommet ?(select(n)) de G f s'il existe.
Exemple. La figure 7 montre une requête de sélection R s = [G i , select], où la fonction de nommage select, définie sur N = {?prénom, ?surnom}, est représentée sur G i . La requête permet de connaître le prénom, et s'il existe le surnom, de tous les humains du graphe fait. Les deux réponses à l'exécution de R s dans G f (figure 5) sont présentées en partie droite de la figure 7.
Requête de description
La requête de sélection permet d'obtenir les images des sommets qui intéressent l'utilisateur, la requête de description permet, elle, d'obtenir non seulement les images des sommets qui intéressent l'utilisateur, mais aussi leur description. La description d'un sommet est formée par les sommets voisins qui apportent des informations sur ce sommet. Une requête de description est un couple composé d'un graphe d'interrogation et d'une fonction de nommage qui à un nom de l'ensemble des noms à décrire, associe un sommet du graphe. Étant donné que seuls les concepts peuvent être décrits, l'ensemble à décrire ne doit contenir que des noms se référant à des sommets concepts.
Définition 13. Soit un ensemble de noms N .
Une requête de description est un couple
Une réponse à une requête de description est un sous-graphe du graphe fait dans lequel figurent les images des concepts à décrire, les relations qui sont liées à ces concepts par une arête étiquetée par « 1 », et les concepts liés à ces relations. Seules les relations reliées par une arête étiquetée par « 1 » sont sélectionnées puisque se sont les relations dont le concept à décrire est le sujet. 
Une réponse à la requête de description R d est un sous-graphe
Exemple. La figure 8 montre une requête de description R d = [G i , desc], où la fonction de nommage desc, définie sur N = {?père}, est représentée sur G i . La requête permet de décrire tous les humains qui sont pères. Les trois réponses à l'exécution de R d dans G f (figure 5) sont présentées en partie droite de la figure 8 : H2 et H3 sont pères, mais H2 est père à la fois de H1 et de F1.
Requête de construction
Une requête de construction est composée de deux graphes : un graphe d'interrogation, appelé graphe condition, et un graphe conceptuel, appelé graphe modèle,. Le principe d'une requête de construction est d'une part d'extraire des informations du graphe fait à l'aide du graphe condition, qui doit se projeter dans le graphe fait, et d'autre part d'exploiter ces informations afin de construire un nouveau graphe conceptuel, appelé graphe réponse, à partir du graphe modèle. Une requête de construction peut être vue comme une règle (Salvat, 1998) dont la partie condition est définie par le graphe condition et la partie conclusion est obtenue par le graphe modèle. Pour lier les sommets du graphe condition à ceux du graphe modèle, on utilise une fonction lien qui associe à un sommet du graphe condition, un sommet du graphe modèle. Un sommet du graphe condition qui est lié à un sommet du graphe modèle doit avoir la même étiquette que ce dernier. Notons qu'en utilisant l'extension des types conjonctifs (Chein et Mugnier, 2004), il serait possible que deux sommets liés ne possèdent pas la même étiquette.
est un graphe conceptuel appelé graphe modèle, tels que G et G m sont définis sur le même vocabulaire, lien est une fonction partielle définie comme suit : lien :
Une réponse à une requête de construction est un graphe réponse construit sur le modèle du graphe modèle comme suit. Les sommets du graphe modèle liés aux sommets du graphe condition sont étiquetés par les images des sommets du graphe condition dans le graphe fait. Les sommets qui ne sont pas liés dans le graphe modèle gardent leurs étiquettes. Il se peut que des sommets du graphe condition ne possèdent pas d'image par la projection, les sommets liés au graphe modèle sont alors retirés du graphe réponse, ainsi que toutes les relations éventuel-lement connectées à ces sommets. Ce cas se présente lorsqu'un sommet du graphe modèle est lié à un sommet du graphe condition qui appartient à un bloc option, ou de disjonction.
Définition 16. Soient un graphe conceptuel G f = (C f , R f , E f , l f ), une requête de construction R c = [G i , G m , lien] avec G i = (G, H) où G = (C, R, E, l) et avec G m = (C m , R m , E m , l m ), une projection ? de G i dans G f , et l'ensemble S ? des sommets de G qui ont une image dans G f par ?.
Une réponse à la requête de construction R c est un graphe conceptuel appelé graphe ré-ponse G r = (C r , R r , E r , l r ), tel que : -C r = C m \ {c = lien(c ) ? C m |c ? C, c / ? S ? } -R r = (R m \ {r = lien(r ) ? R m |r ? R, r / ? S ? }) \ {r ? R m |rc ? E m , c / ? C r } -E r = {nn ? E m |n, n ? C r ? R r } -l r est définie de la façon suivante ?s ? C r ? R r ? E r :
si ?s ? C ? R tel que s = lien(s ), l r (s) = l f (?(s )) sinon, l r (s) = l m (s)
Exemple. La figure 9 montre la requête R c = [G i , G m , lien] où la fonction lien est représentée directement sur le graphe via les pointillés. Cette requête peut s'apparenter à une règle qui dit que si un humain a pour parent un autre humain, alors ce dernier est un parent du premier, mais si cet humain n'a pas de parent, personne n'est son parent. Les quatre réponses à l'exécution de R c dans G f (figure 5) sont présentées en partie droite de la figure 9.
8 Conclusion

Introduction
Les réseaux sociaux sont l'objet d'une recherche intense depuis plusieurs années (Carrington et al., 2005;Newman et al., 2006;Scott et Carrington, 2011). Leur étude donne lieu à différentes questions concernant leur évolution, qu'il s'agisse d'analyser comment les interactions se sont mises en place, ou alors de comprendre l'état du système qu'ils décrivent. Parmi ces interrogations, l'étude des phénomènes de propagation dans les réseaux a suscité un intérêt soutenu au sein de la communauté, multipliant les domaines d'applications, allant de la sociologie (Granovetter, 1978;Macy, 1991) à l'épidémiologie (Hethcote, 2000;Dodds et Watts, 2005;Bertuzzo et al., 2010) en passant par la publicité virale et le placement de produits (Domingos et Richardson, 2001;Chen et al., 2010).
Nous nous intéressons dans cet article à l'étude de la propagation dans les réseaux sociaux. Notre objectif est de proposer une méthodologie permettant de comparer des modèles préexistants et documentés de propagation. Le grand nombre et les différentes variations de ces derniers offrent un assortiment de solutions compliquant le choix d'un modèle particulier. Afin de faciliter cette tâche, il convient de pouvoir comparer effectivement les modèles et non seulement les résultats finaux obtenus suite à leur application.
Cette ambition rejoint Kempe et al. (2003) qui proposent une généralisation de différents types de modèles de propagation. Ces résultats permettent de voir les modèles dans un cadre entièrement mathématique où chacun des algorithmes devient une solution à un problème d'optimisation commun. A l'inverse, nous adoptons une perspective résolument algorithmique dont l'objectif est de venir en appui à une approche exploratoire.
Il n'existe à notre connaissance pas de formalisme unifiant toutes les approches permettant d'effectuer une comparaison des modèles, de leur formulation, leur complexité ou leurs performances. La première contribution de cet article est donc de proposer un tel cadre unificateur basé sur un formalisme solide : la réécriture de graphes.
La propagation est généralement vue comme un phénomène global au réseau bien qu'elle émerge en réalité de la somme d'une multitude d'évènements y agissant localement. La plupart des modèles consistent donc en un ensemble de règles décrivant les situations dans lesquelles une entité peut influencer ses voisins. Bien que chacun de ces évènements soit décrit localement et succinctement, l'application répétée de transformations locales permet de faire émer-ger le comportement du modèle au niveau global. Dans ce formalisme, un modèle correspond alors à un ensemble de règles de transformation couplé à une stratégie qui régule et ordonne l'application de ces mêmes règles.
Les modèles auxquels nous nous intéressons par la suite considèrent un réseau social dont la topologie est fixée. Les règles décrivent alors comment évoluent les états des sommets du ré-seau. Dans leurs travaux, Kejžar et al. (2008) se sont intéressés, de façon similaire, à l'évolution du caractère topologique d'un réseau social. Partant d'un réseau pré-existant, les auteurs proposent une série de règles permettant de modifier les connexions entre les acteurs du réseau, autorisant ainsi la création ou suppression de liens. Leur travail rejoint donc notre approche consistant à exploiter la réécriture comme mécanisme pour exprimer leurs modèles d'évo-lution des réseaux. Cependant, leur article est davantage orienté vers l'analyse des résultats asymptotiques probabilistes sur l'évolution de la taille et la densité des réseaux ainsi produits. L'intérêt de notre approche basée sur une description commune des modèles tient égale-ment à la possibilité d'étudier et de comparer ceux-ci de manière expérimentale. La plupart des travaux s'intéressent à l'objectif atteint au terme d'une propagation (couverture du réseau, vitesse de propagation, etc.), en revanche, il est plus difficile d'établir des résultats expliquant comment se déroule la propagation et pourquoi cet objectif est atteint. L'utilisation d'un formalisme commun nous permet, au contraire, la réalisation de ce type d'investigation.
De plus, cette méthodologie prend un sens particulier lorsque l'étude des modèles se fait de manière visuelle et interactive. En manipulant le modèle (en lançant des simulations, en isolant une règle, etc.), l'utilisateur est à même de développer une connaissance du modèle ainsi que de mesurer et suivre son comportement au fil du déroulement. Pour ces raisons, nous présentons une plate-forme de visualisation analytique -exploitant une version étendue de PORGY (Pinaud et al., 2012) (voir Fig. 1) -pour, simultanément, construire les réseaux et règles de réécriture, simuler la propagation selon différentes stratégies (i.e. les modèles) et comparer les traces d'exécution de ces dernières à l'aide de divers critères.
L'article introduit d'abord la terminologie propre aux modèles de propagation des réseaux et décrit deux modèles particuliers (section 2). Ces modèles sont ensuite exprimés à l'aide de règles de réécriture illustrant ainsi le pouvoir d'expression et l'utilisabilité du formalisme (section 3). Nous montrons enfin comment la plate-forme de visualisation peut être utilisée pour étudier les modèles et exhiber leurs différences (section 4).
FIG. 1: Interface de PORGY :
(1) le réseau social sur lequel on applique la propagation ; (2) édition d'une règle ; (3) portion de l'arbre de dérivation, conservant une trace complète des calculs réalisés (le graphe (1) représente un des sommet de celui-ci) ; (4) courbe montrant l'évolution du nombre de sommets actifs ; (5) autre représentation de l'arbre de dérivation ; (6) éditeur de stratégies.
Modélisation de la propagation dans les réseaux sociaux
Un réseau social (Brandes et Wagner, 2003)   (2010)) construits au fil du déroulement de la propagation et qui peuvent être utilisés pour mesurer l'influence d'un utilisateur sur ses voisins ou représenter sa tolérance à la réalisation d'une action (plus un utilisateur est sollicité, plus il sera enclin à s'activer ou inversement).
Face à cette diversité, nous nous limitons dans la suite de l'article à illustrer la faisabilité de notre approche sur deux modèles représentatifs : un modèle à cascades indépendantes (IC, Kempe et al. (2003)) utilisé comme base pour de nombreux cas, et un modèle à seuils linéaires (LT, Goyal et al. (2010)) qui exploite un principe d'activation non probabiliste contrairement au modèle précédent :
Le modèle à cascades indépendantes IC. Ce modèle comporte de nombreuses variations (e.g. Gomez-Rodriguez et al. (2010); Watts (2002)) permettant, par exemple, la propagation d'opinions divergentes dans un même réseau (Chen et al., 2011). Nous le décrivons sous une forme basique, telle que proposée par Kempe et al. (2003).
Soit un sous-ensemble de sommets A 0 ? V activés au temps t = 0 et les probabilités p v,w , définies pour toutes paires de sommets voisins {v, w} pour représenter l'influence de v ? A 0 sur w / ? A 0 . Une série de nouveaux ensembles de sommets activés A t+1 est calculé à partir de A t . Pour chacun des sommets dans A t , on visite les voisins w ? N (v) qui n'ont pas déjà été activés (mais qui peuvent déjà avoir été visités) ; en d'autres mots, w ? N (v) \ ? t i=0 A i . Un sommet w peut alors devenir actif avec une probabilité p v,w , auquel cas il est ajouté à A t+1 . L'algorithme s'arrête lorsque A t+k est vide (pour k ? 0).
Modèle à seuil linéaire LT. Ce modèle suit un déroulement différent du précédent. Il le rejoint cependant en ce qu'un sommet ne change plus d'état dès lors qu'il est activé. On suppose donné, soit aléatoirement, soit appris selon un historique d'actions connues, les probabilités p v,w . Chaque sommet w ? V est aussi équipé d'un seuil ? w . Soit S w l'ensemble des voisins du sommet w qui sont activés. On détermine l'ensemble A t+1 en calculant pour chaque sommet w non encore activé la valeur d'influence jointe p w (S) = 1 ? v?Sw (1 ? p v,w ). Le sommet w devient ainsi actif dès que l'influence de ses voisins excède son seuil d'activation, c'est à dire lorsque p w (S) ? ? w . 
Réécriture de graphes
Les éléments de base du calcul de la réécriture sont des sommets du graphe, additionnellement équipés de ports, auxquels les arêtes vont se connecter. Plus généralement, les sommets, ports et arêtes vont avoir des propriétés associées à une valeur (par exemple la probabilité p v,w ou le nom donné à un port) qui permettront de les distinguer entre eux, une combinaison spécifique des ces propriétés pouvant être identifiée comme un état.
Une règle de réécriture consiste en un couple L ? R où L et R sont eux-mêmes des graphes (souvent petits). L et R sont respectivement appelés les membres gauche et droit de la règle. L'application d'une règle sur un graphe G se fait en localisant dans G un sousgraphe H isomorphe à L et en le "remplaçant" par R. La notion d'isomorphisme doit toutefois être étendue pour tenir compte des états des sommets, des ports et des arêtes. La règle doit également, le cas échéant, préciser comment traiter les arêtes incidentes aux ports de H qui ne sont pas mentionnées dans L. De plus, une règle peut aussi calculer les valeurs de plusieurs propriétés de R en fonction de celles de H.
Des exemples de règles sont donnés figure 2. La règle 2a concerne une paire de sommets, l'un dans l'état activé (vert), l'autre étant non activé (rouge), connectés par une arête allant du port In du sommet activé vers le port Out du sommet non activé. La règle maintient la connexion entre les sommets et modifie l'état du sommet rouge, le faisant passer dans l'état violet signifiant que le sommet a été visité (ses propriétés ont été lues et il est possible de tenter de l'activer). La règle 2b ne concerne qu'un seul sommet ; son application est donc potentiellement réalisable sur tout sommet du graphe à condition qu'il soit dans le même état (ici visité, symbolisé par la couleur violette). La règle consiste simplement à modifier l'état du sommet, le faisant passer de l'état violet à l'état vert (activé).
Le défi consiste la plupart du temps à savoir prédire le comportement de la réécriture ré-pétée de règles sur un graphe. En effet, l'exécution des règles n'est pas déterministe puisque leur ordre d'application n'est, a priori, pas précisé, mais aussi parce qu'elles peuvent être appliquées sur de multiples instances H du membre gauche L de la règle. Il devient alors intéressant de savoir si le calcul de la réécriture converge et s'il est confluent.
Dans cet optique, il peut également être tentant de chercher à conditionner l'ordre d'application des règles afin de guider le comportement du calcul. A cette fin, il est possible de définir une stratégie d'application des règles. Une stratégie permet de choisir un ensemble des règles à appliquer, préciser leur ordre, le nombre de répétitions, et l'endroit où celles-ci peuvent être ap-pliquées. Pour plus de détails sur le langage de stratégies utilisé par la plate-forme PORGY, sa formalisation et ses propriétés en tant que langage formel, le lecteur pourra consulter l'article de Fernandez et al. (2014).
Traduction des modèles de propagation
Le premier défi qui se pose à nous est de pouvoir donner, pour chacun des modèles présen-tés dans la section 2, un ensemble de règles et une stratégie d'application de ces dernières qui permet d'émuler le fonctionnement du modèle.
Notre démarche de traduction d'un modèle de propagation quelconque en une série de règles de réécriture et leur stratégie d'application est aisément généralisable. Celle-ci a pu être appliquée à tous les modèles rencontrés dans la bibliographie étudiée. Pour la clarté de la discussion concernant les étapes à suivre, nous présentons uniquement la traduction du modèle de propagation à cascades indépendantes (section 2). Ce dernier illustre tout à fait les opérations à réaliser et toute procédure de traduction d'autres modèles suit un déroulement similaire.
Le motif (membre gauche d'une règle) principal à rechercher pour faire évoluer la propagation consiste à identifier un couple de sommets voisins dont l'un est activé et l'autre ne l'est pas. Il est ainsi nécessaire de conserver pour chaque sommet son état actuel. De manière similaire, chaque arête devra préserver les probabilités d'influence p v,w et p w,v que ses extrémités v et w pourront imposer l'une sur l'autre lorsque l'un des sommets s'activera.
La stratégie employée consiste, pour chaque sommet non actif, à calculer puis stocker l'influence de ses voisins actifs en ne conservant que la valeur pour le sommet qui a l'influence la plus forte. Le sommet non actif passe alors dans l'état visité (règle de la figure 2a). Le parcours du voisinage est, de cette manière, contrôlé par la stratégie tandis que les actions à effectuer sur le réseau sont contrôlées par les règles. La procédure décrite ci-dessus forme une stratégie qui sera répétée tant qu'un sommet actif peut influencer un de ses voisins (il reste une arête non marquée qui permet d'appliquer la règle 2a). Chaque application de règle entraîne l'ajout d'un sommet sur l'arbre de dérivation et d'une arête (de couleur violette) pour montrer la succession des opérations. Les points de départ et d'arrivée d'une stratégie (enchaînement de plusieurs règles) sont eux représentés par une arête verte (Fig. 1). Cet arbre peut rapidement atteindre une taille conséquente, rendant la lisibilité difficile. Nous pouvons néanmoins le filtrer et ne conserver que les arêtes vertes et les sommets correspondants (Fig. 4).
Visualisation analytique et comparaison des modèles
Nous détaillons dans cette partie comment la plate-forme de visualisation PORGY (Pinaud et al., 2012) est utilisée pour comparer deux applications des modèles de propagation présentés au début de cet article. Nous avons utilisé le modèle de Wang et al. (2006) pour générer un réseau social aléatoire de 300 sommets. Le réseau obtenu a 597 arêtes. Les conditions de départ sont identiques pour les deux modèles : même ensemble initial de sommets activés et même distribution de probabilités d'influence entre les sommets. Notre objectif n'est pas de montrer que tel modèle de propagation est meilleur que tel autre (ceci nécessiterait de nombreuses simulations pour calculer les résultats moyens sur les modèles probabilistes) mais plutôt de comprendre comment les modèles fonctionnent 1 . Les applications successives de la stratégie décrite précédemment permettent aux sommets actifs de transmettre l'information ou l'action représentant le sujet de la propagation à leur voisins. Chaque exécution de règle va créer un état intermédiaire du graphe d'origine qui sera conservé dans la trace de la propagation (passage des différents sommets de non visité à visité puis potentiellement actif ). Cet historique va pouvoir être exploité pour étudier et comparer le graphe à un instant donné ou pour reconstituer et suivre le chemin emprunté par le processus d'activation des sommets.
Un arbre de dérivation (voir fig.1) est ainsi créé et maintenu pour fournir toutes ces informations. En visualisant des états successifs, nous pouvons observer cette progression. La figure 3 présente quelques vignettes d'une vue de type Small-Multiples, qui est une partie du graphe analysé, et montre l'évolution de l'état de ses sommets. Les différents temps t représentent les applications successives des stratégies (un sommet non visité à t peut donc se retrouver activé à t + 1). L'arbre de dérivation nous permet immédiatement de montrer quel est le modèle qui nécessite le moins d'étapes de calcul ou le moins de lancements de stratégie avant d'arriver à terme car sa branche est la plus courte (figure 4). Nous avons ainsi une première approximation de la complexité en temps des algorithmes.
Lier la profondeur de l'arbre (donc le temps) avec d'autres mesures nous permet de considérer l'évolution de différents paramètres tout au long de la propagation. Nous pouvons, par exemple, aborder la notion de vitesse de propagation, valeur indiquant l'évolution du nombre de sommets actifs en fonction du temps. La figure 4 présente l'évolution de cette valeur pour une exécution des modèles à cascades indépendantes (partie droite, courbe du haut) et à seuils linéaires (courbe du bas). Les courbes présentées n'ont pas les mêmes échelles puisqu'elles sont calculées indépendamment pour chacun des modèles. Malgré ceci, nous pouvons observer que le modèle à cascades va parvenir à activer environ 80% des sommets contre seulement 18% pour celui à seuils (pour un même nombre d'étapes de réécritures), démontrant le fort impact sur les performances des modèles, dans un premier temps, des valeurs utilisées pour l'initialisation des probabilités d'influence, et dans un second temps, du choix de l'ensemble de sommets initialement activés. Nous avons utilisé une loupe (fonctionnalité de PORGY) sur le haut de chaque axe pour rendre les valeurs lisibles. Nous pouvons aussi noter qu'après la première application de la stratégie, le nombre de sommets activés est très proche pour chacun des modèles, les différences apparaissant et se confirmant par la suite. Il peut aussi être intéressant de voir l'état du graphe quand le nombre de sommets activés atteint un seuil. Puisque les différentes vues sont liées, la sélection d'un sommet/arête (en bleu sur la figure 4) de l'arbre de dérivation entraîne sa sélection dans la courbe et vice-versa. De manière similaire, la sélection d'un sommet lors d'une étape de la propagation sera immédia-tement répercutée à l'ensemble des étapes contenant ce même élément, rendant la sélection visible même sur les sommets de l'arbre de dérivation. D'après la méthodologie employée par l'application PORGY, tant qu'un élément n'est pas modifié par une règle, il n'est jamais changé. En conséquence, la sélection d'un sommet d'intérêt dans l'un des graphes intermé-diaires représentant le réseau permet de savoir directement quand cet élément a changé d'état, surtout si l'on travaille sur la version complète de l'arbre de dérivation (montrant le détail des applications de règles).
Finalement, le type de mesure évoluant selon le temps peut être généralisé à d'autres propriétés des modèles de propagation. La notion de sommet visité, introduite précédemment, peut présenter un intérêt, dans le cas où un message doit seulement être vu et non nécessai-rement redistribué ou propagé par les utilisateurs. Cette vitesse de connaissance du contenu de la propagation est observable de manière similaire à la vitesse de propagation. De plus, en considérant ces deux mesures, nous pouvons en proposer une troisième exprimant l'efficacité d'une propagation, calculable grâce au rapport entre le nombre de sommets activés à un instant t et ceux visités/influencés au moment précédent t ? 1.
Conclusion et travaux futurs
Nous avons présenté un formalisme basé sur la réécriture de graphes vu comme un langage commun à l'expression de tous les modèles de propagation sur réseaux. Lorsque la propagation n'entraîne pas de modifications de la topologie du graphe, le modèle consiste en un ensemble réduit de règles gérant les transitions d'état des sommets du graphe. Le stockage des états des sommets, les règles et le langage de stratégie qui pilote leur application facilite la gestion du caractère probabiliste des modèles.
Nous envisageons d'étendre notre étude à un panel plus large de modèles de propagation afin de démontrer le caractère "universel" de notre approche. Cela exige aussi de pouvoir multiplier les simulations sur des réseaux de tailles conséquentes. Cet aspect pose un défi en raison de la complexité liée à la recherche de motifs correspondant aux membres gauches des règles -d'abord parce que nous faisons face à un problème NP-Complet (isomorphisme de sous-graphes), mais aussi à cause de l'explosion combinatoire qu'elle engendre et qui doit être gérée à l'aide du langage de stratégie (d'application des règles).
La formulation des modèles à l'aide de réécritures offre une possibilité nouvelle qui permettra de combiner propagation dans le réseau et évolution de la topologie du réseau sur lequel la propagation a lieu. Là encore, un langage de stratégie facilitera la gestion de l'application simultanée ou alternée de ces deux types de transformations. Il n'existe pas, à notre connaissance, de tels modèles. Nous espérons ainsi pouvoir proposer des modèles réalistes d'évolution de réseaux, dont le réalisme tiendrait à la fois aux caractères structurels des réseaux produits, mais aussi à leur qualité en terme de circulation de l'information.
can only be made at the cost of describing models based on a common formalism and independant from them. We propose to use graph rewriting to formally describe the propagation mechanisms as local transformation rules applied according to a strategy. This approach makes complete sense when supported by a visual analytics framework dedicated to graph rewriting. The paper first presents several models and illustrates them through selected simulations. We then show how our visual analytics framework allows to interactively manipulate models, and underline their differences based on measures computed on simulation traces.

Introduction
L'évolution d'ontologie est un sujet posé avec l'apparition des méthodologies de construction d'ontologies. Il s'est avéré indispensable de penser à maintenir et faire évoluer les ontologies, après leur construction, afin d'assurer leur réutilisation et leur continuité. Ce besoin s'est rapidement développé avec la prolifération des ontologies et leur large utilisation. À titre d'exemple, depuis Janvier 2010, 59 versions de l'ontologie Gene Ontology 1 (une des plus fameuses ontologies) ont été publiées à raison d'une version par mois. Ainsi, afin de définir et gérer le processus d'évolution, plusieurs méthodologies ont été proposées dans la littérature (Klein, 2004;Stojanovic, 2004;Djedidi et Aufaure, 2010;Khattak et al., 2013). Les premiers travaux ont pensé à définir ce qu'est une évolution d'ontologie. D'où la définition proposée par (Stojanovic, 2004) : "l'évolution d'ontologie est l'adaptation, dans le temps, d'une ontologie aux besoins de changement et la propagation cohérente des changements aux artefacts dépen-dants". Cette définition a ouvert le débat sur la signification des changements ontologiques, leurs formalisations et leurs types (Klein, 2004;Stojanovic, 2004). Ainsi, un changement ontologique est une modification d'une ou plusieurs entités ontologiques (classe, propriété, axiome, individus, etc.). Il peut viser la modification de la structure de l'ontologie (ex. ajout de classe, ajout de propriété) et on parle dans ce cas de l'enrichissement d'ontologie. Il peut viser égale-ment l'ajout d'individus et on parle alors du peuplement d'ontologie. Les changements ontologiques sont souvent de trois types (Stojanovic, 2004) : 1) les changements élémentaires qui représentent une opération primitive et non décomposable qui affecte une seule entité ontologique (ex. renommer une classe) ; 2) les changements composés (composites) qui affectent une entité ontologique et ses voisins (ex. suppression d'une classe) ; 3) les changements complexes qui expriment un enchainement de plusieurs changements élémentaires et/ou composés (ex. fusion de classes). En effet, les changements composés et complexes sont des changements utiles et demandés par l'utilisateur. Ils englobent plusieurs modifications en une seule opéra-tion, ce qui lui permet d'adapter son ontologie d'une manière plus facile sans se perdre dans les détails des changements élémentaires (Klein et Noy, 2003). Cependant, la définition et la formalisation de ces changements sont des tâches non triviales comme leur application peut affecter la cohérence de l'ontologie. Deux types de cohérence sont généralement distingués dans la littérature : 1) la cohérence conceptuelle qui se réfère aux règles structurelles et contraintes du langage de représentation de l'ontologie (ex. inexistence de concepts isolés) ; 2) la cohé-rence sémantique qui se réfère à la cohérence logique de l'ontologie dans le sens où elle ne doit pas comporter des contradictions logiques (ex. ne pas avoir des relations contradictoires entre deux concepts). En effet, la préservation de la consistance de l'ontologie et la résolution des incohérences résultantes de l'application des changements ontologiques sont encore des problématiques insuffisamment étudiées. Ainsi, nous proposons dans cet article une nouvelle formalisation des changements ontologiques composés et complexes permettant : 1) d'éviter les inconsistances d'une manière a priori en utilisant les concepts des grammaires de graphes typés ; 2) de réduire le nombre de changements élémentaires constituant les changements composés/complexes. Les changements étudiés traitent à la fois le niveau structurel de l'ontologie (l'enrichissement de l'ontologie) et aussi le niveau assertionnel (le peuplement d'ontologie).
Le reste de l'article sera organisé comme suit : la section 2 présente un tour d'horizon sur les principales approches d'évolution d'ontologie. La section 3 introduit les concepts de base des grammaires de graphes. La section 4 propose une nouvelle formalisation des changements ontologiques composés et complexes. Enfin, une conclusion synthétise le travail présenté et donne les perspectives envisagées.
État de l'art
De nombreuses approches ont été proposées dans la littérature pour définir et implémenter le processus d'évolution d'ontologies. Le Tableau 1 présente certaines approches tout en préci-sant les langages utilisés, l'implémentation, la gestion des inconsistances et leurs spécificités. Ainsi, nous pouvons observer que différents langages ont été étudiés : KAON (Stojanovic, 2004), RDF (Luong et Dieng-Kuntz, 2007), OWL (Klein, 2004;Djedidi et Aufaure, 2010), etc. En se basant sur ces langages, plusieurs changements ontologiques ont été définis et différentes classifications de ces changements ont été proposées (Stojanovic, 2004;Klein, 2004). Ainsi, certains travaux se sont intéressés à l'étude des changements élémentaires (Mahfoudh et al., 2013). D'autres ont traité également les changements composés et complexes (Djedidi et Aufaure, 2010;Javed et al., 2013;Liu et al., 2014). Des travaux se sont focalisés sur l'enrichissement d'ontologies (Klein, 2004). D'autres ont étudié aussi le peuplement d'ontologies (Luong et Dieng-Kuntz, 2007;Djedidi et Aufaure, 2010;Mahfoudh et al., 2013). La résolution des inconsistances est encore insuffisamment étudié. En effet, certaines approches ont ignoré cet axe comme ils se sont intéressés à d'autres problématiques, comme par exemple la gestion des versions des ontologies (Hartung et al., 2013). D'autres travaux se sont focalisés plutôt sur l'identification des inconsistances sans les résoudre (Gueffaz et al., 2012). Certains chercheurs se sont intéressés également par la résolution des inconsistances (Djedidi et Aufaure, 2010;Luong et Dieng-Kuntz, 2007;Javed et al., 2013). Cependant, les approches proposées admettant un processus a postériori de traitement des inconsistances qui nécessite l'utilisation d'une ressource externe (tel qu'un raisonneur) afin de vérifier la consistance de l'ontologie évoluée. Afin d'éviter l'utilisation d'un raisonneur externe, Mahfoudh et al. (2013)  -Approche basée sur les patrons de conception.
-Évaluation de la qualité de l'ontologie évoluée.
-Approche nécessitant des activités lourdes. (Gueffaz et al., 2012) OWL DL Prototype -Identification des inconsistances en utilisant le checker NuSMV.
-Approche d'évolution d'ontologies, CLOCk (Change Log Ontology Checker) basée sur le modèle checking.
-Approche nécessitant la transformation des ontologies OWL au langage NuSMV. (Hartung et al., 2013) OBO 
Grammaires de Graphes Typés
Les grammaires de graphes, également appelées réécriture de graphes, sont un formalisme mathématique pour représenter et gérer les graphes. Elles permettent la modification de graphes via des règles de réécriture tout en précisant quand et comment faire les changements. Grâce aux concepts et outils qu'elles proposent, les grammaires de graphes sont utilisées dans plusieurs branches de l'informatique, comme par exemple la modélisation des systèmes logiciels et la théorie des langages formels (Ehrig et al., 1996). Elles ont récemment été introduites dans le domaine des ontologies, ce qui a donné naissance à des travaux traitant de la formalisation des ontologies modulaires (d'Aquin et al., 2007), la représentation des graphes RDF (Resource Description Framework) (Braatz et Brandt, 2010), la fusion d'ontologies (Mahfoudh et al., 2014a), etc. Dans ce qui suit, nous dressons un tour d'horizon des définitions de base concernant les fondements théoriques de la réécriture de graphes. Graphe. Un graphe G(N, E) est une structure composée par un ensemble de noeuds (N ), d'arêtes (E) et d'une application s : E ? N × N qui attache les noeuds source/destination à chaque arête. Graphe attribué. Un graphe attribué est un graphe étendu par un ensemble d'attributs A, une fonction d'attribution att : N ? E ? P(A), P représentant l'ensemble des parties, et une fonction d'évaluation val : A ? V . Ainsi, chaque noeud ou arête peut avoir un ensemble d'attributs (P(A)) dont les valeurs seront données par val.
avec N T correspond aux types des noeuds et E T aux types des arêtes. Grammaires de graphes typés. Une grammaire de graphe typé est une structure mathéma-tique définie par T GG = (G, T G, P ) avec :
-G est un graphe initial, appelé aussi graphe hôte ; -T G est un graphe type précisant le type de l'information représentée dans le graphe hôte (type des noeuds et des arêtes) ; -P un ensemble de règles de réécriture, appelées aussi règles de production ou de transformations de graphes. Une règle de réécriture r est une paire de graphes pattern (LHS, RHS) avec : 1) LHS (Left Hand Side) représente la pré-condition de la règle de réécri-ture et décrit la structure qu'il faut trouver dans un graphe G pour pouvoir appliquer la règle ; 2) RHS (Right Hand Side) représente la post-condition de la règle de réécriture et doit remplacer LHS dans G. Les règles peuvent également avoir des conditions supplémentaires appelées N AC (Negative Application Conditions). Ce sont des graphes pattern définissant des conditions ne devant pas être vérifiées pour que la règle de réécriture puisse être appliquée. La transformation de graphe consiste ainsi à définir comment un graphe G peut être transformé en un nouveau graphe G . Cette transformation peut être réalisée selon deux types d'approches (Rozenberg, 1999) : les approches ensemblistes (Node replacement, Edge replacement, etc.) et les approches algébriques. Dans ce travail, nous utilisons les approches algébriques basées sur le concept de pushout de la théorie des catégories (Ehrig et al., 1973). Pushout. Soient trois objets de la catégorie des graphes :
. A partir de là, deux variantes sont proposées pour la réécriture des graphes : le Simple pushout SPO (Löwe, 1993) et le Double poushout DPO (Ehrig, 1979). Dans ce travail, seule l'approche SPO a été considérée car elle se voit plus générale et permet l'application des différents changements ontologiques (Mahfoudh et al., 2014b). Ainsi, appliquer une règle de réécriture à un graphe initial G, selon la méthode SPO, revient à : 1) trouver un morphisme (m) permettant d'identifier un sous-graphe de graphe G qui correspond (match) avec la partie LHS (m : LHS ? G) ; 2) appliquer la règle de réécriture sur le sous-graphe en le remplaçant par m(RHS) et supprimant les arêtes suspendues, i.e. les arêtes qui ont une extrémité non liée à un noeud. Ainsi, d'une manière générale, nous avons SP O(G, LHS, RHS) = G .
Formalisation des changements ontologiques 4.1 Modèle de transformation de graphes
Afin de représenter les ontologies avec le formalisme de grammaires de graphes, nous considérons une ontologie comme un graphe hôte G possédant une relation de typage avec le graphe type (T G), où T G représente le méta-modèle de l'ontologie. Pour être conforme aux standards, c'est OWL qui a été retenu comme méta-modèle d'ontologies. Ainsi, les types des noeuds considérés sont : N T = {Class(C), P roperty(P ), ObjectP roperty(OP ), DataP roperty(DP ),-Individual(I), DataT ype(D), Restriction(R)}.
Les types des arêtes correspondent aux axiomes utilisés pour relier les différentes entités :
Les changements ontologiques sont formalisés par un ensemble de règles de réécriture :
Dans cette définition étendue, CHD correspond à l'ensemble des changements dérivés ajoutés à un changement ontologique pour corriger ses éventuelles inconsistances. La Figure 1 montre une représentation et une application de la règle de réécriture du changement ontologique AddIndividual. Elle permet d'ajouter un individu "Pascal" tout en spé-cifiant son type, la classe "Person". La règle assure, grâce au N AC, la non redondance de données, i.e. elle empêche l'application du changement dans le cas où l'individu existe déjà dans l'ontologie. 
Formalisation des changements ontologiques
Cette section présente notre formalisation des changements ontologiques composés et complexes par les grammaires de graphes typés. Avant de détailler cette formalisation, une brève introduction des changements élémentaires est indispensable pour mieux comprendre le reste de l'article. Ainsi, les changements élémentaires englobent les changements de renommage, l'ajout et la suppression de certains concepts. Ils n'affectent qu'une seule entité ontologique bien qu'ils dépendent d'autres entités. Le Tableau 2 présente quelques changements élémen-taires adressés dans notre travail et les concepts dont ils sont dépendant. À noter que les N ACs des règles de réécriture sont déduites à partir de ces interdépendances. Un exemple du changement élémentaire AddIndividual est déjà présenté dans la section 4.1. 
Changements ontologiques composés
Les changements ontologiques composés, appelés aussi composites, affectent une entité ontologique et ses voisins. Ils sont alors formés par plusieurs règles de réécriture : une règle présentant le changement souhaité par l'utilisateur (changement principal) et les autres règles présentant les changements dérivés (CHD) ajoutés pour préserver la consistance de l'ontologie. En effet, l'ordre des règles de réécriture est primordial dans la plupart des changements. Le Tableau 3 présente l'interdépendance entre ces changements organisés dans une matrice de changements. La valeur d'un élément de matrice (i, j) indique que l'application d'un changement relié à une ligne i implique l'application du changement de la colonne j. Ainsi, le changement ontologique RemoveCardinalityRestriction(C, OP ) permet de supprimer une CardinalityRestriction définie sur une classe C et une objectProperty OP . Il est formalisé par deux règles de réécriture. La première représente le changement dérivé RemoveAssertionObjectP roperty qui supprime toutes les assertions définies sur OP . La deuxième règle définit la règle de réécriture principale assurant la suppression de la restriction.
Le changement ontologique RemoveObjectP roperty(OP ) supprime une objectProperty OP et toutes ses dépendances de l'ontologie. La Figure 2 présente les six règles de réécriture définissant ce changement. Ainsi, les cinq premières règles décrivent les changements déri-vés (CHD) devant être appliqués pour préserver la consistance de l'ontologie. La dernière règle présente la règle de réécriture principale. Ainsi, les restrictions définies sur la propriété OP doivent être supprimées en appliquant les règles suivantes : RemoveAllV aluesRestriction(OP ), RemoveSomeV aluesRestriction(OP ), RemoveHasV alueRestriction(OP ) et RemoveCardinalityRestriction(OP ). Toutes les ObjectP ropertyAssertion qui réfé-rencent l'objectProperty OP doivent également être supprimées. 
Changements ontologiques complexes
Le Tableau 4 présente l'ensemble des changements complexes abordé dans ce travail et les changements dont ils sont composés.
Comme exemples de changements complexes, nous présentons les changements P ullU pClass, M ergeClass et SplitClass. Ainsi, le changement P ullU pClass(C, C p ) permet de monter une classe C dans sa hiérarchie de classes et l'attacher aux parents de sa super-classe précédente C p . Ceci implique que la classe C n'est plus la subClass de la classe C p et n'infère plus ses propriétés. La Figure 3 présente la règle de réécriture définissant ce changement. Ainsi, le changement dérivé RemoveObjectP ropertyAssertion vérifie si la classe C possède des individus qui partagent une objectP ropertyAssertion sur les propriétés de la classe C p . Dans ce cas, toutes les assertions doivent être supprimées. Le changement RemoveDataP ropertyAssertion supprime toutes les dataP ropertyAssertion définies sur les individus de la classe C et les dataProperties liées à la classe C p .  Le changement M ergeClasses(C 1 , C 2 , C N ew ) fusionne deux classes C 1 et C 2 déjà existantes dans l'ontologie en une nouvelle classe (C N ew ). Il nécessite l'application des règles de réécriture AddClass(C N ew ), RemoveClass(C 1 ) et RemoveClass(C 2 ). Cependant, pour préserver la consistance de l'ontologie, avant de supprimer C 1 et C 2 , toutes leurs propriétés et axiomes doivent être attachés à C N ew . Formellement : 1)
AddSubClass(C N ew , C j ), 2) répéter le processus pour C 2 , 3) ?C i ? C(O) · C i ? C 1 appliquer AddEquivalentClasses(C i , C N ew ), 4) répéter le processus pour C 2 , etc.
Le changement SplitClass(C, C N ew1 , C N ew2 ) divise une classe (C) déjà existante dans l'ontologie en deux nouvelles classes C N ew1 et C N ew2 . Il nécessite alors l'application des règles de réécriture AddClass(C N ew1 ), AddClass(C N ew2 ) et RemoveClass(C). Comme le changement M ergeClasses, le changement SplitClass nécessite, avant la suppression de la classe C, d'attacher toutes ses propriétés et axiomes aux classes C N ew1 et C N ew2 .
Discussion : Le formalisme des grammaires de graphes offre une représentation simple des changements ontologiques. Le Tableau 5 montre deux exemples de changements AddObject-P roperty et P ullDownClass, représentés à la fois par le formalisme proposé et le travail de (Djedidi et Aufaure, 2010). Dans Djedidi et Aufaure (2010), ces changements sont considé-rés, respectivement, comme composés et complexes. Le premier changement est composé par trois changements élémentaires et le deuxième par deux. De plus, ils nécessitent, comme tous les autres changements ontologiques, l'utilisation du raisonneur Pellet pour identifier d'une manière a posteriori les inconsistances. Dans notre travail, ces changements sont considérés comme élémentaires puisqu'ils ne sont composés que d'une seule règle de réécriture. De plus, pour préserver la consistance de l'ontologie, les inconsistances sont gérées d'une manière a priori grâce à l'utilisation des Negatives Applications Conditions (N AC).
Changement ontologique (Djedidi et Aufaure, 2010) Formalisme proposé AddObjectP roperty(OP, C1, C2)
Le changement est composé de trois changements élémen-taires : 1.
AddObjectP roperty-(OP ), 2. AddDomain(OP, C1) 3. AddRange(OP, C2).
-Le changement est formalisé par une seule règle de réécriture et évite la non-redondance de données. Le changement est composé par deux changements élémen-taires : 1. AddSubClass(C1, C2) 2.
RemoveSubClass-(C1, Cp)avec la classe Cp est la super-classe de C1 et C2.
-Le changement est formalisé par une seule règle de réécriture et évite la contradiction des axiomes. 
Conclusion
Nous avons présenté dans cet article une nouvelle formalisation des changements ontologiques composés et complexes basée sur les grammaires de graphes typés et l'approche algé-brique Simple Pushout (SPO) de transformation de graphes. L'utilisation de l'approche SPO offre plusieurs avantages. En particulier, elle permet de définir simplement et formellement les règles de réécriture correspondant aux changements ontologiques. Elle assure le contrôle des transformations de graphes en évitant les incohérences d'une manière a priori. De plus, elle réduit le nombre de changements élémentaires nécessaires pour appliquer les changements complexes et composés. À noter que la formalisation des changements a été implémentée à l'aide de l'outil AGG (Attributed Graph Grammar) et testée sur des ontologies de taille ré-duite. En effet, l'étape la plus coûteuse en temps et en ressources est la reconnaissance du graphe LHS à partir du graphe hôte. Vu que la plupart des changements possèdent des LHS de taille réduite, il s'est avéré que le temps d'exécution est assez limité. À titre d'exemple, pour un graphe d'une ontologie composé de 21 noeuds, l'exécution du changement complexe SplitClass(C, C N ew1 , C N ew2 ) a pris seulement 700 millisecondes (avec un LHS composé de 37 noeuds). Pour mieux évaluer la performance de notre approche, nous travaillons actuellement sur l'évaluation de l'influence de la taille de LHS sur les ontologies de grande taille.

Introduction
Que l'on suggère à l'utilisateur d'accéder à une information, de s'inscrire à une newsletter, de commenter un service, ou d'acheter un produit, le Web Usage Mining est indispensable à l'objectif d'adaptabilité de l'offre technologique. La propension d'un utilisateur en ligne à réa-liser une action suggérée dépend en effet de sa réaction face aux modes de sollicitation et ses réactions sont, pour une part déterminante, déclenchées par son expérience en cours. L'expé-rience utilisateur correspond "aux réponses et aux perceptions d'une personne qui résultent de l'usage ou de l'anticipation de l'usage d'un produit, d'un service ou d'un système" 1 . Dès lors on comprend aisément l'enjeu communicationnel de l'analyse automatique de l'expérience de l'utilisateur. Le web 3.0 se réfléchit d'ores et déjà dans une logique one to one avec l'individualisation de la communication sur le web comme élément central.
Les questions ouvertes, par exemple sur la manière d'analyser cette expérience utilisateur, sont nécessairement interdisciplinaires. La première partie de cet article définit une méthode d'analyse sémiotique pour y répondre. C'est ensuite la spécification de cette méthode à travers la proposition de nouveaux descripteurs sémiotiques qui est développée. La dernière partie définit le mode d'apprentissage automatique pertinent pour la détection de la propension d'un individu à réaliser une action suggérée.
Détermination des données
Contrairement à la majorité des études sur les profils utilisateur, notre recherche porte sur un utilisateur quelconque 2 exposé à n'importe quelle sollicitation, ce qui détermine l'utilisation des « attitudes implicites » comme indicateur de l'expérience. Les "jugements et attitudes implicites" sont des indicateurs, issus de récentes études socio-cognitives (cf. Courbet et Fourquet-Courbet (2014)), particulièrement pertinents dans le cadre de notre recherche car définissent des actes non analysables par l'utilisateur et parfois même non perçus, mais révéla-teurs de l'expérience vécue. Nous n'avons donc pas besoin de connaître au préalable l'utilisateur qui ne saurait pas, à titre d'exemple, identifier précisément si et/ou pourquoi il clique sur l'image plutôt que sur le texte pour accéder à l'information désirée.
L'exploitation des données implicites produites par l'utilisateur permet d'estimer ses attitudes implicites. Les données explicites archivées sont inexistantes pour un utilisateur quelconque et les données explicites liées au déclaratif sont considérées trop intrusives (cf. Oard et Kim (2001)). La sollicitation de l'utilisateur, pour obtenir ces informations, constitue un frein avéré dans le processus de séduction à l'oeuvre dans les techniques de communication.
L'analyse de ces données a pour objectif de déduire un comportement à partir des interactions de l'utilisateur avec le système. Les interactions sont identifiées par : la durée de lecture (temps passé par l'utilisateur sur un écran, soit le temps passé entre deux actions), le mouvement de souris, le nombre de clics de la souris, la durée de défilement de l'ascenseur, le défilement avec souris, le nombre de clics sur l'ascenseur, le défilement avec les touches du clavier, la sélection du texte (cf. Tchuente (2013)).
Se pose alors la question de la méthode permettant de détecter des attitudes implicites à partir de l'exploitation de données implicites. Pour y répondre, nous définissons une catégorie sémiotique d'analyse du comportement prenant en considération la portée communicationnelle inconsciente des actions réalisées. Les catégories de comportement, définies dans l'état de l'art (cf. Oard et Kim (2001)), sont constituées d'actants : examiner, référencer, retenir, annoter, créer. Nous cherchons à remplacer ces catégories d'actants par des catégories d'expérience de l'en acte. Notre contribution se situe dans l'utilisation d'une catégorie sémiotique d'analyse du comportement : le style perceptif. Tel que défini par Pignier (2012), le style perceptif est l'expression d'une esthésie, une manière de percevoir le monde, l'autre et les choses, reliée à l'exercice d'une sensibilité. C'est l'expérience de l'utilisateur que nous cherchons à définir, sa façon de communiquer implicitement en interagissant avec le système. Là où la catégorisation classique d'un profil utilisateur s'intéresse aux caractéristiques d'un individu (genre, situation socio-démographique, centres d'intérêt), nous nous intéressons à son hexis numérique, son être impliqué dans l'espace du web et agissant en fonction de lui. Ainsi, le contexte est né-cessairement inclus dans le style perceptif. Qu'il soit acteur adjuvant ou opposant, il influe sur l'expérience de l'utilisateur. Il s'agit de s'approcher du schéma de la communication interpersonnelle qui permet, dans la vie réelle, d'adapter débit de parole, gestualité, tonalité à l'interlocuteur, en fonction de l'observation de ses réactions. La détection du style perceptif poursuit in fine cet objectif d'adaptabilité de la réponse à l'expérience du co-énonciateur à un moment donné.
Afin de définir le style perceptif, nous proposons une structuration des données implicites de navigation. Le style perceptif se caractérisant par les expériences médiées par les interfaces, 2. L'utilisateur quelconque représente l'utilisateur non identifié versus utilisateur identifié.
le web contient donc intrinsèquement les informations nécessaires à sa détection. La réflexion se pose au niveau de la production de métadonnées (des descripteurs) caractérisant les données implicites. Le niveau morphosyntaxique de l'analyse sémantique (comme par exemple pour TypWeb dans Beaudouin et al. (2002)) est transféré au niveau sémiotique de l'analyse. En effet, l'exploration qualitative des sites web nécessite l'utilisation de descripteurs contenant les possibilités d'interaction et de contexte.
Nous proposons d'intégrer ici, comme niveau d'analyse, le sème connotatif, qui est défini comme étant un sème connotant l'effet produit par l'interaction entre l'utilisateur et l'élément. À titre d'exemple, à la valeur sémantique de l'élément "pop-up", nous associons les valeurs sé-miotiques intrusif et surgissant. Intrusif et surgissant connotent l'effet produit par l'interaction avec "pop-up". À la valeur sémantique "sommaire sous forme de listes à faire défiler", nous associons la valeur sémiotique cartographique. L'effet cartographique renvoie à l'expérience de la vue d'ensemble. L'ensemble des sèmes ainsi produit forme les descripteurs pour l'annotation des sites web. Le niveau sème connotatif contient l'expérience possible (soit l'effet produit par l'interaction) mais sa capacité à qualifier un style dépend du schéma actantiel (l'action effective de l'utilisateur). Le schéma actantiel de Greimas (1966)  Le niveau sème connotatif est donc pondéré par les actions suivantes sur les éléments : lecture ou visualisation (déterminée elle même par le temps passé), cliquer pour accéder, cliquer pour fermer, défiler, surligner, cliquer pour partager, inscrire, commenter, annoter. Le sème connotatif peut alors soit être positivé ou contrarié. Le sème connotatif intrusif relié à l'apparition d'une pop-up pour l'inscription à une newsletter, sera positivé par l'action "inscrire" (par exemple inscrire son mail), mais sera contrarié par l'action "cliquer pour fermer". On définit alors la paire de sèmes suivante : intrusif et non intrusif, qui sera niée en dehors des actions contrariantes ou positivantes qui lui sont associées. Il s'agit ensuite d'envisager les structures qui se dessinent au niveau sémiotique. En prenant en considération la chronologie des interactions et leur interopérabilité -simultanéité, succession, opposition, exclusion -et en définissant la dernière action de l'utilisateur comme finalité, le contexte sémiotique est ainsi défini. L'interaction formulée dans la phrase suivante : « L'utilisateur en ligne ferme quasi instantanément la pop-up newsletter puis fait défiler le sommaire dans son intégralité avant d'accéder à la page information » est retranscrite dans le contexte d'une annotation sémiotique par : « Un style perceptif non intrusif successivement cartographique pour aboutir à : accéder à la page désirée ». La création d'une ontologie appliquée à la détection du style perceptif s'avère ainsi nécessaire pour définir les règles et articulations d'une annotation sémiotique. Celle-ci permet ainsi d'exploiter les données implicites, pour faire émerger les attitudes implicites détermi-nantes de l'expérience de l'utilisateur, que nous caractérisons par son style perceptif.
Descripteurs sémiotiques proposés
L'étude sémiotique présentée dans la section précédente nous a amené à définir des descripteurs sémiotiques que nous présentons dans ce qui suit. Pour rappel, une session de navigation consiste en une succession de L écrans visités (L étant variable d'une session à une autre), en ne tenant compte que des écrans sur lesquels l'utilisateur a passé un temps supérieur à un certain seuil, ainsi que ceux qu'il ne subit pas par défaut au chargement de la page.
Il s'agit alors de caractériser le contenu sémiotique de ces écrans, en introduisant des descripteurs qui encodent les informations liées aux styles perceptifs. Comme nous l'avons mentionné dans la section 2, ces informations se trouvent au niveau des éléments qui composent l'écran, et avec lesquels l'utilisateur interagit pendant sa navigation. A noter que cette notion d'interaction est spécifique à chaque type d'éléments : certains ne seront pris en compte dans l'analyse que si l'utilisateur les survole avec la souris, alors que pour d'autres, l'affichage par défaut sur l'écran visité est suffisant.
Nous proposons ainsi d'associer à chaque élément de l'écran un certain nombre de sèmes qui le caractérisent. Un dictionnaire de N sèmes est donc défini (sèmes positifs et négatifs), et une annotation sémiotique des pages du site permet d'attribuer à chaque élément des labels sémiotiques issus de ce dictionnaire. Cette annotation se fait manuellement sur un certain nombre de pages caractéristiques du site, et est ensuite généralisée de manière automatique aux autres pages en attribuant les mêmes labels sémiotiques aux éléments appartenant à une même catégorie, en se basant par exemple sur les attributs HTML. Les descripteurs sémiotiques caractérisant un écran E sont ensuite calculés à partir de ces vecteurs binaires, en mesurant la fréquence de chaque modalité du dictionnaire de sèmes :
Afin d'éviter que deux écrans ayant un contenu sémiotique similaire et appartenant à des pages sémantiquement éloignées ne soient représentés avec des indicateurs identiques, nous proposons de concaténer le vecteur desc sémiotique (E) avec un descripteur sémantique classique décrivant le contenu de la page à laquelle appartient l'écran.
Pour ce faire, un dictionnaire de M labels sémantiques est ainsi défini, et les différentes pages du site sont annotées avec ces labels. De nombreuses méthodes d'annotation automatique, issues notamment des domaines du Web sémantique et du Web Content Mining, existent dans l'état de l'art (Charrad et al. (2008) par exemple).
Ainsi, chaque écran d'une page donnée sera représenté par un vecteur de description de dimension N × M , composé d'un premier vecteur qui est propre à cet écran, et d'un second qui est commun à tous les écrans d'une même page. Cette représentation permet à la fois de lever la limitation décrite dans le paragraphe précédent, et d'encoder indirectement la suite des pages visitées (comme dans une approche de Web Usage Mining classique) à travers les variations des vecteurs sémantiques à chaque fois que l'utilisateur accède à une nouvelle page.
Traitement des descripteurs
Nous nous intéressons dans cette section au choix d'un modèle d'apprentissage adapté au traitement des descripteurs introduits dans la section précédente. Nous avons opté pour un modèle neuronal, et ce principalement pour deux raisons : (i) l'aspect temps-réel des applications visées, pour lequel les modèles neuronaux sont particulièrement adaptés de part le fait que la quasi-totalité de la complexité est reportée sur la phase d'apprentissage, et (ii) l'optimalité de ce types de modèles en terme de performances, qui a été démontrée dans de nombreuses études comparatives récentes (parmi lesquelles nous pouvons citer celle de Bengio et Delalleau (2011)).
Vue la nature séquentielle des données traitées (une session de navigation étant représentée par une séquence de vecteurs, de dimension N × M chacun, et de longueur variable), nous avons opté pour un modèle neuronal récurrent. Ce dernier (initialement introduit dans Williams et Zipser (1995) et ayant connu plusieurs évolutions depuis) est analogue à un Perceptron multi-couches classique, mais dans lequel des connexions récurrentes sont rajoutées au niveau des couches cachées (c'est à dire les couches intermédiaires entre l'entrée et la sortie). Ce modèle récurrent est entraîné par une version modifiée de l'algorithme de rétro-propagation du gradient (dans laquelle les connexions récurrentes sont prises en compte), en ciblant les sorties désirées selon l'application visée. Ces sorties peuvent correspondre par exemple à la réponse à une recommandation de produit sur un site de e-commerce, à la réaction à une sollicitation sur un site de collecte de dons, et plus généralement à n'importe quel objectif mesurable lié au comportement de navigation de l'utilisateur.

Introduction
Les données massives, appelées communément "big data", impactent directement le processus ETL (Extracting-Transforming-Loading) vu que celui-ci est le premier composant du système décisionnel confronté à ces données. Peu de travaux ont traité sur la problématique des données massives dans le processus ETL. Liu et al. (2011) ont proposé une approche parallèle/distribuée appelée ETLMR consistant à améliorer les performances de la phase de transformation (T) et de chargement (L) de l'ETL et ce en adoptant, pour chacune des deux phases, des stratégies de distribution appropriées. Les expérimentations de Misra et al. (2013) ont montré que le paradigme MapReduce est prometteur et que les solutions ETL basées sur des frameworks open source tel que Apache Hadoop sont plus performantes et moins couteuses par rapport aux solutions ETL commercialisées. Contrairement aux travaux de Liu et al. (2011), ceux de Misra et al. (2013) considèrent la phase d'extraction (E) de l'ETL très couteuse ; celle-ci a été traitée dans un environnement parallèle/distribué selon le paradigme MapReduce. (Liu et al., 2012) est une démonstration du prototype ETLMR. Dans (Liu et al., 2014), les auteurs proposent une plateforme CloudETL basée sur Apache Hadoop et Apache Hive où les performances ont été nettement améliorées par rapport à celles d'ETLMR (Liu et al., 2011). Les plateformes ETLMR (Liu et al., 2011) et CloudETL (Liu et al., 2014) sont basées sur du code Python, et par conséquent celles-ci sont destinées aux informaticiens dévelop-peurs de solutions ETL parallèles/distribuées. Dans le but de vulgariser ce type de plateformes et les rendre accessibles aux utilisateurs finaux, nous proposons, dans ce papier, une plateforme baptisée P-ETL (Parallel-ETL) développée sous l'environnement Apache Hadoop
1
. Le paramétrage d'un processus se fait, de bout-en-bout, sur une interface unique structurée en trois onglets, chacun concerne une phase du processus (E, T, L). Le même paramétrage peut s'effectuer dans un fichier XML pour un traitement en batch. Nous avons adapté le schéma classique de l'ETL dans l'environnement MapReduce. Ainsi, P-ETL procède en cinq phases : (E)xtracting, (P)artitioning, (T)ransforming, (R)educing et (L)oading ; au lieu d'ETL, on parle plutôt d'EPTRL.
Les bases de P-ETL
Nous présentons dans cette section les bases et les principes fondamentaux de P-ETL en exposant les techniques de partitionnement supportées, l'adaptation des phases Map et Reduce aux spécificités de l'ETL et nous terminons par l'architecture globale de P-ETL.
Partitionnement des données
Dans le but de distribuer/paralléliser le processus ETL, les données sources doivent être elles aussi distribuées pour permettre à plusieurs tâches de s'exécuter de façon parallèle où chacune traite sa propre partition de données. P-ETL offre trois types de partitionnement. Afin d'assurer une charge plus ou moins équitable entre les différentes tâches parallèles, le choix du type de partitionnement est important. La présence d'un taux élevé, dans une partition de données, de tuples avec des valeurs creuses implique une charge faible en termes de traitement pour la tâche. En effet, les tuples en question seront rejetés par un filtre tel que NOT NULL.
Simple : étant donné un volume de données source v, le type de partitionnement simple génère des partitions égales selon l'équation 1 où nb_part étant le nombre de partitions.
Round Robin (RR) : Avec la technique Round Robin, l'affectation d'un tuple depuis le volume source v vers une partition de données p est basée sur l'équation 2. rang (tuple) étant le rang du tuple dans le volume v et nb_part étant le nombre de partitions.
Round Robin par Bloc (RRB) : Cette technique est similaire à Round Robin. Dans le but d'accélérer le partitionnement, un bloc de tuples est affecté à la partition, plutôt qu'une affectation tuple par tuple.
Les mappers et les reducers
Dans la plateforme P-ETL, les primitives map() et reduce() ont été adaptées aux spécificités de l'ETL. Le rôle assigné à un mapper est la normalisation des données (nettoyage, filtrage, conversion, ...). Le mapper traite chaque row dans un tunnel de transformations (T 1 , T 2 ...) où chaque T i est chargé d'une opération particulière telles que le nettoyage, filtrage, projection, conversion et concaténation. La figure 1  
Architecture de P-ETL
La plateforme P-ETL est organisée en cinq modules : (E)xtracting, (P)artitioning, (T)ransforming, (R)educing et (L)oading (Figure 3). Après extraction (E), les données sources sont chargées dans le système de fichier distribué de Hadoop (HDFS). Ensuite, un partitionnement logique des données (P) s'effectue selon le choix de l'utilisateur final (Simple, RR, RRB). Les partitions des données ainsi générées seront soumises au processus MapReduce. Chaque mapper est en charge de transformer (T) les données de sa partition (nettoyage, filtrage, conversion, ...).
Les fonctions de fusion et d'agrégation des données sont différées pour être exécutées dans la phase Reduce (R). A ce niveau, les données deviennent pertinentes et peuvent alors être chargées (L) dans l'entrepôt de données.
FIG. 3 -Architecture de P-ETL.
Paramétrage d'un processus dans P-ETL
Le paramétrage d'un processus se fait sur une interface unique organisée en trois onglets (Extract, Transform, Load) dédiés au processus lui-même (workflow), plus une partie "Advanced Parameters" réservée pour la configuration de l'environnement parallèle/distribué de Apache Hadoop (Figure 4). Pour la configuration du processus, l'utilisateur doit commencer par l'onglet "Extract". Les paramètres disponibles sur les deux autres onglets "Transform" et "Load" dépendent du premier, principalement du format de la source, sa structure et son emplacement. En revanche, la partie "Advanced Parameters" peut être utilisée indépendamment des trois onglets.
FIG. 4 -Interface de configuration P-ETL.
Dans l'onglet "Extract", l'utilisateur doit, en premier lieu, localiser les données sources. Le format pivot des données est le fichier csv qui est adopté par toutes les plateformes MapReduce vu sa légèreté (absence de méta-données). Afin d'accélérer le chargement des données sources dans le système HDFS, l'utilisateur pourra activer la compression de celles-ci. Ensuite, l'utilisateur doit choisir un type de partitionnement des données sources (simple, Round Robin, Round Robin by block) ainsi que le nombre de partitions (égal au nombre de mappers). Enfin, il doit paramétrer le mode de lecture du mapreader à partir des partitions de données (Ligne par ligne, nombre de lignes, taille en KO). La figure 4 montre l'onglet "Transform" qui fournit à l'utilisateur une liste de fonctions de transformation et d'agrégation. Chaque fonction insérée doit être paramétrée (entrées, conditions, expressions, ...). Le tunnel de transformation ainsi constitué s'exécutera dans l'ordre d'insertion des fonctions. Enfin, l'onglet "Load" permet la configuration du chargement des données préparées dans l'entrepôt de données et comporte la destination des données (entrepôt de données, magasin de données, cube de données), la compression des données avant leur chargement dans le système HDFS ainsi que le caractère séparateur du fichier csv cible. Concernant le paramétrage de l'environnement parallèle/distribué dans Apache Hadoop (taille d'un bloc HDFS, nombre de noeuds impliqué dans le processus, taille mémoire réservée à un noeud et à une tâche, compression des résultats des mappers avant de les soumettre aux reducers, ....), celui-ci se fait dans la partie "Advanced Parameters".
Expérimentation
Pour évaluer P-ETL, nous avons installé un cluster de 19 machines, chacune possède un processeur intel-Core TMi3-3220 CPU@3.30 GHZ x 4 processor, 4GO RAM et 500 GO d'espace disque. Le réseau local (LAN) est un Ethernet 100 Mbps. Selon la configuration matérielle présentée ci-dessus, l'environnement Hadoop permet d'affecter, au maximum, deux tâches parallèles à un même noeud. Ainsi, deux tâches parallèles sont équivalentes à un noeud dans ce qui suit.
Données de test :
Nous avons développé un programme qui génère des données synthé-tiques relatives aux renseignements des étudiants. Les expérimentations ont été réalisées sur des jeux de données allant de 244 * 10 6 à 7, 317 * 10 9 tuples. Nous exposons, dans ce qui suit, l'expérimentation réalisée sur un fichier etudiant.csv de 7, 317 * 10 9 lignes où chacune a une taille de 44 octets. Ce fichier contient les attributs suivants : Matricule , Date d'inscription, Cycle (Licence, Master, Doctorat), Spécialité, Bourse et Sport. Le processus ETL paramétré pour l'expérimentation se présente en quatre fonctions. La première tâche est la projection qui consiste à exclure les attributs Bourse et Sport. La deuxième tâche du processus est une restriction qui filtre les tuples et rejette tous ceux présentant une valeur Null dans l'un des attributs : Date d'inscription, Cycle, et Spécialité. La troisième tâche est Year() qui extrait l'année à partir de la date d'inscription. Enfin, la quatrième tâche est une fonction d'agrégation COUNT() qui compte le nombre d'étudiants inscrits durant la même année, dans le même cycle et la même spécialité. Il est à noter que durant l'exécution du processus, lorsque P-ETL rencontre une agrégation, comme COUNT() dans ce cas, celle-ci sera différée pour être exécutée dans la phase Reduce.
Résultats : Comme le montre le tableau 1, l'augmentation des tâches parallèles améliore de manière significative les performances du processus. Nous remarquons que le gain de temps entre 24 et 30 tâches est très intéressant (50 mn). En revanche, nous constatons une régression du gain entre 30 et 38 tâches (6 mn). Nous pourrons conclure qu'au delà d'un certain seuil en termes de tâches parallèles, l'amélioration des performances des processus sous P-ETL ne devient plus significative. 
Conclusion
Le processus ETL est considéré aujourd'hui comme étant le coeur du système décisionnel puisque toutes les données destinées pour l'analyse transitent par celui-ci. Afin de faire face aux données massives, nous l'avons adapté selon le paradigme MapReduce pour permettre son exécution dans un environnement parallèle et distribué. P-ETL est basé sur une interface de paramétrage conviviale afin de rendre la plateforme accessible aux utilisateurs finaux. Les résultats des expérimentations montrent une meilleure scalabilité de P-ETL face à des volumes de données importants lorsque la taille du cluster augmente.

Introduction
Dans le cadre de l'édition de manuscrits anciens, le rôle de l'éditeur consiste à reconstruire le plus fidèlement possible le manuscrit original à partir des différentes versions du texte disponible. Pour cela, l'éditeur classe les différentes versions du texte afin d'obtenir un arbre généalogique de cette filiation que l'on nomme stemma codicum (cf. Fig1).
FIG. 1 -Stemma de De Nuptiis Philologiae et Mercurii établi par Danuta Shanzer.
La reconstruction généalogique par un arbre suppose que chaque copiste n'a utilisé qu'un seul manuscrit pour réaliser son exemplaire (filiation unique). Malheureusement, il arrive qu'un exemplaire ait été copié, non à partir d'une seule source, mais sur plusieurs manuscrits existants. On parle alors de contamination ou de corruption.
Nous proposons dans cet article, de représenter une tradition contaminée à l'aide une construction pyramidale basée sur la notion de manuscrits intermédiaires.
Une des modélisations philologiques utilisées par les éditeurs pour reconstruire le stemma codicum est du à Don Quentin (1926). Elle permet de retrouver, à partir du corpus, des triplets de manuscrits dont l'un est l'intermédiaire des deux autres et que nous nommons T3M. Nous utilisons alors ces triplets T3M pour bâtir une pyramide généalogique.
Les triplets T3M sont déterminés par un indice qui est nul. L'expérience montre néanmoins que le nombre de triplets T3M obtenus est très faible. Dans des données réelles, la contamination, les erreurs de saisie, etc., empêchent de respecter strictement les conditions de Don Quentin. Nous allons donc être amené à relâcher les conditions strictes de Don Quentin pour créer des triplets T3M-souples où l'on impose à l'indice, non plus d'être nul, mais d'être infé-rieur à un seuil, seuil déterminé par l'éditeur en fonction du corpus.
Pour construire une pyramide à partir des triplets, définissons quatre propriétés : -l'intermédiarité permet à un ensemble de triplets T3M de respecter la transitivité ; -La couverture impose que tous les manuscrits appartiennent au moins à un triplet T3M afin de pouvoir les situer sur la pyramide finale. -La compatibilité consiste pour un ensemble de triplets T3M à être représentable sur une pyramide (cf. Defays (1979)  
Summary
In this paper we present a new codicum stemma visualization method. Don Quentin's modeling is usec to classify the textual tradition. We supplement the genealogical editor's information of betweenness triplets obtained directly from the corpus. A pyramid depicting the family codicum stemma is then constructed on the basis of information obtained by the triplets

Introduction
L'olfaction, ou la capacité de percevoir des odeurs, est le résultat d'un phénomène complexe : une molécule s'associe à un récepteur de la cavité nasale, et provoque l'émission d'un signal transmis au cerveau qui fait ressentir l'odeur associée [Sezille et Bensafi (2013)-Meierhenrich et al. (2005]. Si les phénomènes qui caractérisent les sens de l'ouïe et de la vue sont bien connus, la perception olfactive n'est, encore aujourd'hui, toujours pas comprise dans sa globalité. Cependant, on dispose de nombreux atlas (comme celui d'Arctander (1969)) qui renseignent les qualités perçues par l'humain pour des milliers de molécules odorantes : des experts senteurs associent à des milliers de molécules odorantes des qualités d'odeurs (fruité, boisé, huileux, etc : un vocabulaire bien défini et consensuel). On dispose également maintenant d'outils capables de calculer des milliers de propriétés physico-chimiques de molécules 1 . Il a alors pu être montré que ces propriétés déterminent la (les) qualité(s) d'une odeur perçue [Khan et al. (2007) -Kaeppler et Mueller (2013)]. Ce lien entre le monde physico-chimique et le monde du percept olfactif a été mis en évidence à l'aide de méthodes d'analyse en composantes principales démontrant, à partir de données, la corrélation existante entre ces deux mondes. Les neuroscientifiques ont donc maintenant besoin de méthodes descriptives afin de comprendre les liens entre propriétés physicochimiques et qualités.
La découverte de régularités (ou descriptions) qui distinguent un groupe d'objets selon un label cible (souvent appelé label de classe), est un problème qui a fédéré diverses communautés en intelligence artificielle, fouille de données, apprentissage statistique, etc. En particulier, la découverte supervisée de règles descriptives de type description ?? label est étudiée sous divers formalismes : découverte de sous-groupes, fouille de motifs émergents, ensembles contrastés, hypothèses, etc. (Novak et al. (2009)). Dans tous les cas, nous faisons face à un ensemble d'objets associés à des descriptions (dont l'ensemble forme un ensemble partiellement ordonné), et ces objets sont liés à un ou plusieurs labels de classe.
Dans cet article, on s'intéresse à la découverte de sous-groupes (subgroup discovery), introduite par Klösgen (1996) et Wrobel (1997. Étant donné un ensemble d'objets décrits par un ensemble d'attributs, et chacun associé à un (ou plusieurs) label(s) de classe, un sousgroupe est un sous-ensemble d'objets statistiquement intéressant par sa taille et ses singularités au sein de l'ensemble d'objets initial vis à vis d'un ou plusieurs labels cibles. En fait, il existe deux familles principales de méthodes. La première (Wrobel, 1997) vise à trouver des règles de type description ? label où le conséquent est un unique label. La seconde, la fouille de modèles exceptionnels (exceptional model mining, EMM) introduite par Leman et al. (2008), vise à trouver des sous-groupes dont la répartition d'apparition de tous les labels diffèrent grandement dans le sous-groupe comparé à toute la population, i.e. de la forme description ? {(label 1 , valeur 1 ), ..., (label k , valeur k )} où k est le nombre de labels de l'attribut cible. Dans les deux cas, on veut optimiser une mesure de qualité pour distinguer au mieux le sous-groupe en fonction du label, ou d'une distribution des labels dans le sous-groupe (i.e. le modèle).
En olfaction cependant, une molécule est associée à une ou plusieurs qualités d'odeurs : aucune des approches existantes ne permet de se focaliser sur un sous-ensemble de labels de cardinalité arbitraire. Effectivement, ces approches permettent soit de caractériser un seul label de classe par des sous-groupes, soit de trouver des sous-groupes qui caractérisent tous les labels de classes à la fois. Alors, d'une part, un sous-groupe effectue une caractérisation trop locale, trop spécifique et d'autre part la caractérisation est beaucoup trop globale.
Nous cherchons alors à découvrir des sous-groupes comme des règles descriptives de type description ? {label 1 , label 2 , ..., label l } où l << k. Pour cela, nous proposons une nouvelle méthode appelée ElMM (Exceptional local Model Mining) qui généralise à la fois la méthode de sous-groupes classiques ainsi que EMM. Nous montrerons alors que les sousgroupes extraits sont plus caractéristiques de peu de qualités à la fois, et donc aussi plus faciles à interpréter par l'expert en olfaction.
La suite de cet article est organisée comme suit. Tout d'abord, nous introduisons les deux principales méthodes de découverte de sous-groupes en section 2 (subgroup discovery et exceptionnal model mining). Nous montrons alors les limites de ces deux types d'approche avant d'introduire en section 3 notre nouvelle méthode : la découverte de modèles exceptionnels locaux (exceptional local model mining). Un algorithme de découverte est présenté en section 4 et appliqué à des données issues des domaines de la neuroscience et de l'olfaction (section 5). 2) présente ce jeu de données dans le cas où la fonction class n'associe à chaque molécule qu'un seul label de C -mono-qualité-(resp. un sous-ensemble de labels -multi-qualités-).
Un sous-groupe peut être représenté formellement de manière duale soit en intension soit en extension, c'est-à-dire, soit par une description dans un langage donné mettant en oeuvre des restrictions sur le domaine de valeurs des attributs, soit par l'ensemble d'objets qu'il décrit. Il existe plusieurs langages de description possibles, basés sur différents types de connecteurs logiques (conjonctions, disjonctions, ou encore négations), dont certains sont très expressifs (voir par exemple Galbrun et Kimmig (2014)). Dans la suite nous utiliserons un langage basé uniquement sur des conjonctions.
Définition 2 (Sous-groupe). On note d = 1 , . . . , f |A| la description d'un sous-groupe où chaque f i est une restriction sur le domaine de l'attribut a i ? A (à un sous-ensemble du domaine de a i s'il est nominal, ou à un intervalle s'il est numérique). Chaque restriction peut être assimilée soit à un ensemble (dans le cas d'une restriction sur un attribut nominal), soit à un intervalle dont les bornes appartiennent à Dom(a i ) (dans le cas d'un attribut numérique).
Relation d'ordre partiel entre les sous-groupes. 
Exemple (suite). On a d 1 = W ? 151.28, 23 ? nAT avec pour support l'ensemble des molécules {24, 48, 82, 1633} : la molécule 1 ne vérifie pas la restriction sur l'attribut nAT alors que la molécule 60 ne vérifie pas celle sur M W . Pour plus de lisibilité, lorsque l'on ne précise pas une restriction f i dans une description d cela signifie qu'aucune restriction effective n'est appliquée sur l'attribut a i dans d. La description d 2 = W ? 151.28, 23 ? nAT, 10 ? nC est une spécialisation de d 1 car d 2 comporte les mêmes restrictions que d 1 plus une restriction sur un autre attribut. Réciproquement, d 1 est une généralisation de d 2 .
Étant donné un jeu de données, il y a potentiellement 2 |O| sous-groupes, il est donc néces-saire de n'en sélectionner qu'une partie en fonction de leur intérêt. Pour cela, les différentes approches de l'état de l'art utilisent une mesure de qualité qui évalue la singularité du groupe au sein de la population par rapport à une cible, c'est-à-dire l'attribut de classe. La mesure de qualité est choisie en fonction du type de données, mais aussi en fonction de l'attribut de classe et de la finalité de l'application. Il existe deux approches pour la découverte de sous-groupes : l'approche que l'on va définir comme classique (Wrobel, 1997), et l'approche d'Exceptional Model Mining (EMM) introduite par Leman et al. (2008).
Dans la première, chaque objet n'est associé qu'à un et un seul label de l'attribut de classe, c'est-à-dire ?o ? O, class(o) = c avec c ? Dom(C), et la mesure de qualité permet de mettre en évidence la singularité d'un sous-groupe relativement à un seul label de C. Pour un sous-groupe de description d, une mesure généralement utilisée relativement au label l est :
les proportions d'objets du sous-groupe et du jeu de données entier possédant la classe l. Cette mesure est une généralisation de la mesure WRAcc (? = 1) qui prend en compte à la fois la taille du sous-groupe et aussi sa singularité dont le rapport entre les deux est pondéré par un facteur ?.
Dans le cas d'EMM, un objet est associé à un sous-ensemble de labels de classe, c'est-à-dire, ?o ? O, class(o) ? Dom(C). La mesure de qualité utilisée dans ce cas permet de mettre en évidence la singularité d'un sous-groupe relativement à tous les labels de C à la fois. Une mesure possible est la somme des divergences de Kullback-Leibler pour tous les labels de classe entre les objets du sous-groupe et ceux du jeu de données entier :
Exemple (suite). Avec la description d 1 = W ? 151.28, 23 ? nAT dans la Table 1, en utilisant la mesure de l'équation (1) avec
(1/2?1/3) = 2/3. Dans la Table 2, en utilisant la mesure de la formule de l'équation (2), on a W KL(d 1 ) = 4/6 × ((2/4 log 2 3/4) + (3/4 log 2 3/2) + (3/4 log 2 3/2)) = 0.45.
Découverte de sous-groupes, limites et problème. Étant donnés D(O, A, C, class), minSupp, ? et k l'objectif est de récupérer l'ensemble des k-meilleurs sous-groupes au regard de la mesure de qualité ? choisie où la taille du support du sous-groupe est supérieure ou égale à minSupp. Pour notre domaine d'application de l'olfaction, les approches existantes (décou-verte de sous-groupes classique et EMM) ne permettent pas de répondre à la problématique posée, à savoir la caractérisation de sous-ensemble de qualités d'odeurs. Effectivement, ces approches permettent soit de caractériser un seul label de classe, c'est-à-dire une qualité olfactive, par sous-groupe, soit de trouver des sous-groupes qui caractérisent tous les labels de classes à la fois avec EMM. D'une part un sous-groupe effectue une caractérisation trop locale et spécifique, d'autre part la caractérisation est trop globale. Nous introduisons dans la suite une nouvelle méthode qui généralise ces deux approches en permettant de caractériser par un sous-groupe un sous-ensemble L de taille quelconque de labels de classe.
Découverte de modèles exceptionnels locaux : ElMM
Soit D(O, A, C, class) un jeu de données conforme à la Définition 1, avec Dom(C) = {l 1 , . . . , l k }. Étant donnée une mesure de qualité ?, notre méthode ElMM recherche des sousgroupes de la forme (d, L) où d est la description d'un sous-groupe et L ? Dom(C) est un sous-ensemble de labels de la classe C à caractériser. Cette méthode correspond au cas général de la découverte de sous-groupes. Effectivement, si on fixe pour tous les sous-groupes que L ? Dom(C) on se ramène au cas de la découverte de sous-groupes classique dans lequel un sous-groupe ne caractérise qu'un label de classe. De plus si L = Dom(C) alors on bascule dans le cas d'EMM où chaque sous-groupe doit caractériser tous les labels de classe à la fois. ElMM permet donc de caractériser des sous-ensembles de labels de classe par des sous-groupes appelés sous-groupes locaux. 
La contrainte (i) permet de ne considérer que les sous-groupes dont le support est supérieur ou égal à un seuil minSupp, évitant ainsi d'obtenir des sous-groupes de trop petite taille qui n'auraient alors aucun intérêt et facilitant l'exploration. La contrainte (ii) permet d'interagir sur le langage de la description en restreignant le nombre maximal de restrictions effectives par description à un seuil maxDesc (|d| est le nombre de restrictions effectives de d). De manière similaire, la contrainte (iii) permet de limiter le nombre de labels à discriminer dans L.
Mesure de qualité. La mesure de qualité utilisée dans EMM dont la formule a été donnée dans l'équation 2 peut être généralisée pour ElMM pour ne considérer qu'un sous-ensemble L ? Dom(C) de labels de classe et non plus l'ensemble complet de labels à la fois :
Cependant, cette mesure de qualité ne correspond pas à l'objectif de notre contexte applicatif car elle ne quantifie pas les labels de L ensemble, c'est-à-dire de manière conjointe, lorsqu'ils sont associés conjointement aux objets. Nous cherchons à caractériser l'ensemble des objets cohérents qui possèdent tous les labels de L, et non pas un sous-ensemble de L. Pour cela, nous nous sommes tournés vers une mesure de qualité usuellement utilisée en classification supervisée : la F 1 -Mesure. Cette mesure nous permet dans notre cas de quantifier la pureté d'un sousgroupe vis à vis des labels à caractériser L, i.e. les objets du support de la description du sousgroupe doivent être le plus possible associés à L (la précision) et les objets associés à L dans D doivent être au maximum inclus dans le support du sous-groupe (le rappel). La F 1 -Mesure se base sur le rappel et la précision d'un sous-groupe vis à vis du sous-ensemble L à caractériser. Pour un sous-groupe local (d, L), on note :
On remarque alors que la F 1 -Mesure est comprise entre 0 et 1 puisque la précision et le rappel sont compris aussi entre 0 et 1. Plus la valeur de F 1 (d, L) est proche de 1 plus le sous-groupe (d, L) caractérise spécifiquement le sous-ensemble de labels L.
Exemple. Afin d'illustrer la méthode ElMM, nous reprenons l'exemple de la Table 2. Soit le sous groupe (d, L) avec d 1 = W ? 151.28, 23 ? nAT et L = {M iel, V anillé} le sous-ensemble de labels de classe à caractériser, en appliquant la formule de l'équation 3 afin de calculer la mesure de qualité par la F 1 -Mesure on trouve : 
Découverte de sous-groupes locaux avec ELMMUT
Dans cette section nous présentons l'algorithme ELMMUT qui répond au problème d'Exceptional local Model Mining (ElMM). Tout d'abord, nous caractérisons l'espace de recherche des sous-groupes. Ensuite, nous décrivons la manière de parcourir cet espace pour produire les sous-groupes en illustrant le pseudo-code de ELMMUT.
Espace de recherche. L'espace de recherche correspond à l'ensemble de tous les sous-groupes locaux, partiellement ordonnés. Un sous-groupe local
Ainsi, l'espace de recherche correspond à un treillis dans lequel chaque sous-groupe local est un noeud et le lien entre deux noeuds dénote que le noeud de niveau i+1 est une spécialisation du noeud de niveau i par ajout d'une nouvelle classe à caractériser, ou par spécialisation d'une restriction de la description. L'élément le plus général du treillis correspond au sous-groupe local vide que l'on note ( ?) en omettant les f i car aucune restriction n'est effectuée pour tout attribut a i :
Parcours heuristique de l'espace de recherche. L'algorithme ELMMUT effectue un parcours en profondeur de l'arbre de recherche en partant du plus général (le sous-groupe local vide à la racine de l'arbre) vers le plus spécifique. Le principe algorithmique est donné dans l'Algorithme 1. Pour chaque sous-groupe d'un noeud de l'arbre de recherche, ELMMUT essaie de le spécialiser par une extension de description ou de labels tant que la mesure de qualité est améliorée (fonction Spécialiser) (Galbrun et Kimmig, 2014). Cependant, il existe dans le pire des cas |Dom(C)| + (|A| × n(n + 1)/2) possibilités pour spécialiser un sous-groupe, puisque on peut effectuer jusqu'à |Dom(C)| extensions de labels et |A| extensions de description pour lesquelles on peut construire n(n + 1)/2 intervalles possibles (si l'attribut possède n valeurs différentes). Afin de pallier à ce problème d'espace de recherche, nous nous sommes tournés vers une approche heuristique utilisée dans la découverte de sous-groupes et dans la fouille de redescriptions, il s'agit d'une approche de type "beam-search" (recherche par faisceau) (Lowerre, 1976). Cette approche permet d'explorer seulement une partie des branches de l'arbre de recherche : à chaque spécialisation, seulement une partie des possibilités (au maximum beamW idth) de spécialisation du sous-groupe va être analysée (cf. ligne 11 de Spécialiser). Optimisation des intervalles à la volée. Pour les attributs numériques, une simple discrétisa-tion en prétraitement n'est pas suffisante. Cette approche est cependant utilisée dans une partie des expérimentations afin de pouvoir se comparer équitablement à l'algorithme de référence pour EMM (DSSD Diverse Subgroup Set Discovery introduit par van Leeuwen et Knobbe (2012)), qui utilise une telle discrétisation. Afin d'obtenir des résultats les meilleurs possibles, le choix des bornes de l'intervalle pour un attribut a ? A doit se faire à la volée pour tenir compte des spécificités d'un sous-groupe particulier. Le choix des bornes de l'intervalle est alors déterminant. Tester toutes les possibilités d'intervalles n'est pas envisageable car cette méthode peut s'avérer beaucoup trop gourmande en ressources (complexité théorique d'ordre n 2 pour n valeurs différentes). Afin de pallier ce problème, nous avons adopté une méthode de discrétisation proche de celle de Fayyad et Irani (1993), introduite dans la découverte de sous-groupes par Grosskreutz et Rüping (2009). La Figure 1 présente la répartition des valeurs prises par les objets du support d'un sous-groupe local (d, L) pour l'attribut a. Pour optimiser la mesure il faut éliminer du support du sous-groupe un maximum d'objets qui ne sont pas associés au sous-ensemble de labels L à caractériser. Soit S = (d, L) un sous-groupe, et a ? A un attribut à partir duquel on veut étendre d, on note {p 1 , . . . , p |a| } l'ensemble ordonné (p 1 < p 2 < · · · < p |a| ) des |a| ? |supp(S)| valeurs différentes prises par l'ensemble des objets de S pour l'attribut a. On dit alors qu'une valeur p i des valeurs prises par a est prometteuse si le nombre d'objets de supp(S) associés à L possédant la valeur p i pour a est supérieur ou égal au nombre d'objets de supp(S) non associés à L possédant la valeur p i . Sinon, on dit qu'elle est non-prometteuse. Ainsi, une valeur p i de a correspond à une borne inférieure potentielle si p i est prometteuse et p i?1 est non-prometteuse. De plus une valeur p i de a correspond à une borne supérieure potentielle si p i est prometteuse et p i+1 est non-prometteuse. Ensuite il suffit de tester tous les intervalles possibles en prenant tous les couples (borne inférieure potentielle, borne supérieure potentielle) et de choisir le meilleur.
Expérimentations
Jeu de données
Nous disposons d'un atlas Arctander (1969), première base de données olfactive établie, qui sert de référence pour les neuroscientifiques. Il met en oeuvre 1 689 molécules différentes décrites par 1 704 propriétés physicochimiques numériques (leur volume, leur poids, le nombre d'atomes de carbone qu'elles contiennent, etc...) et sont associées à leur(s) qualité(s) olfactive(s) évaluée(s) par des experts. Les possibles discussions quant à l'obtention de cet atlas, et notamment pour les qualités olfactives, ne sont pas abordées dans ce papier comme il s'agit d'un problème traité par les neuroscientifiques en amont. L'atlas étant clairement multi-labels, on associe chaque molécule à un sous-ensemble de qualités olfactives. En moyenne, chaque molécule est associée à 2.88 qualités olfactives.
A partir de cet atlas, nous avons construit deux jeux de données différents. Dans le premier jeu de données D 1 , on ne considère que 43 attributs (propriétés physicochimiques) de l'atlas Arctender, alors que dans le second jeu de données D 2 on en considère 243. La sélection des 43 attributs de D 1 a été faite sur recommandation de l'expert qui assure que ces attributs doivent être déterminants pour la caractérisation de qualités d'odeurs. Les 243 attributs du jeu de données D 2 ont quant à eux été sélectionnés par une analyse de non-corrélation des attributs.
Résultats quantitatifs
Tout d'abord afin de juger de la performance de l'approche que nous avons mise en place, considérons l'aspect quantitatif des résultats sur le jeu de données d'olfaction. Nous avons exécuté les expérimentations sur une machine avec 8Go de RAM et un processeur cadencé à 3.10GHz. Les résultats quantitatifs obtenus ont été réalisés sur les deux jeux de données Algorithm 1 ELMMUT.
Entrée : O, A, C, class, ?, k, beamW idth, minSupp, maxDescr, maxLab Sortie : L'ensemble des sous-groupes locaux R 1: R ? ? 2: for all c ? Dom(C) do 3:
Vérifier et Mettre à jour les contraintes pour ( {c}) 5:
for all a ? A do 6:
f ? Choisir restriction sur a 7:
Vérifier et Mettre à jour les contraintes pour ( {c}) 8:
Ajouter ( {c}) à T emp 9:
end for 10:
T emp ? Conserver les k-meilleurs sous-groupes locaux de T emp 11:
for all (d, L) ? T emp do 12:
Ajouter Spécialiser(d, L) à R 13:
end for 14: end for
Ajouter (d, L ? {c}) à T emp 5: end for 6: for all a ? liste des attributs candidats de (d, L) : A Cand do 7:
f ? Choisir restriction sur a 8:
Vérifier et Mettre à jour les contraintes pour
Ajouter (d ? {f }, L) à T emp 10: end for 11: T emp ? Conserver les beamW idth meilleurs sous-groupes locaux de T emp 12: for all (d, L) ? T emp do 13: Figure 2 présente les différents temps d'exécution de notre approche pour la version de l'algorithme sans la discrétisation à la volée pour les attributs (une discrétisation par effectifs égaux est réalisée a priori pour chaque attribut numérique, comme cela est fait par la méthode EMM). Les trois courbes sont relatives au jeu de données D 1 . On remarque que plus on augmente la taille maximale autorisée pour la description (maxDescr) ou pour le sous-ensemble de labels à caractériser (maxLab), plus le temps d'exécution est long ce qui est tout à fait compréhensible puisque l'algorithme cherche à étendre le plus possible les sousgroupes tant que la mesure de qualité est améliorée. On remarque cependant qu'à partir de maxDescr = 15 le temps d'exécution est sensiblement semblable ce qui signifie que même si la taille maximale autorisée augmente les sous-groupes ont une description dont la taille ne va pas au-delà d'un certain seuil : la mesure ne peut plus être améliorée en les étendant. Ce résultat semble être causé à la fois par le paramètre minSupp et par le jeu de données. Effectivement, plus les descriptions sont étendues plus le support a une taille qui tend à diminuer, et puisque l'algorithme est déterministe, avec ce jeu de données, on ne peut excéder une description de taille 15. De même pour la taille maximale du sous-ensemble de labels à caractériser, à partir de 2 ou 3 le temps d'exécution reste le même, ce qui concorde avec le fait qu'en moyenne une molécule est associée à 2.88 qualités olfactives (au-delà de n > 3 qualités olfactives, le nombre de molécules partageant l'ensemble de ces mêmes n qualités olfactives est trop faible et la contrainte du support minimale n'est pas respectée). La Figure 3 présente l'impact du jeu de données et de la discrétisation à la volée via notre technique. Clairement, le nombre d'attributs est un facteur crucial pour l'algorithme ELMMUT, on observe la présence d'un facteur 10 entre le temps d'exécution sur D 1 avec 43 attributs et celui sur D 2 avec 243. L'utilisation de la discrétisation à la volée ne semble pas passer à l'échelle lorsque l'on augmente la taille des descriptions : à partir d'une valeur de 15 pour maxDescr l'exécution dure plus de 12 heures et a donc été avortée. Nous prévoyons des techniques d'optimisation dans le futur.
Résultats qualitatifs
L'interprétation des résultats est un point central dans le cadre de notre application. Les règles descriptives que nous avons mises en place doivent être capables d'informer et d'aiguiller les neuroscientifiques dans leur recherche. Notre approche, ElMM, en ne caractérisant qu'un sous-ensemble de labels de classe permet alors de correspondre au cas pratique à savoir qu'une molécule ne possède en moyenne que 2.88 qualités olfactives. En observant la Figure 4 qui présente la distribution des qualités au sein du jeu de données entier et d'un sous-groupe obtenu par la méthode EMM, on s'aperçoit clairement que l'interprétation d'un tel résultat est très difficile. On constate des différences entre les distributions du sous-groupe et du jeu de données initial mais cette différence est présente sur beaucoup trop de qualités olfactives à la fois et ainsi l'interprétation d'un tel résultat pour la déduction d'une règle descriptive est infaisable pour un neuroscientifique. La Table 3 présente les 5 meilleurs sous-groupes (du point de vue de la mesure F 1 ) obtenus après suppression des motifs redondants (on utilise ici la même méthode que Galbrun et Kimmig (2014)). Ces sous-groupes sont issus de la base de données D 1 lorsque la discrétisation à la volée est activée avec maxDescr = 10, maxLab = 2 et minSupp = 30. Seulement un sous-groupe caractérisant plusieurs labels de classe (Floral et Balsamique) est présent, avec une mesure de 0.33 et un support de 38. Sa description contient 9 restrictions. Des sous-groupes ont aussi des descriptions plus courtes. La taille des supports est variable. De plus, dans le jeu de données D 2 , lorsque la discrétisation à la volée est désactivée et que maxDescr = 15, maxLab = 3 et minSupp = 30, on obtient 74.6% de sous-groupes dont le sous-ensemble de labels est de taille 1, 22.9% de taille 2 et 2.5% de taille 3. 
Conclusion
Nous avons présenté la découverte de motifs exceptionnels locaux, une nouvelle méthode de fouille de règles descriptives qui généralise les approches existantes, pour caractériser spé-cifiquement un sous-ensemble de labels de classe. Nous l'avons appliquée au cas concret de l'olfaction afin de mettre en évidence les liens existant entre les propriétés physicochimiques d'une molécule et ses qualités olfactives. Le pouvoir d'interprétation des résultats et l'information qu'ils véhiculent, permettent d'entrevoir une évolution de la connaissance à propos du phénomène complexe qu'est l'olfaction. De nombreuses expérimentations restent à faire et nous envisageons une exploration interactive inspirée par Galbrun et Miettinen (2012).
Références Arctander, S. (1969). Perfume and flavor chemicals :(aroma chemicals), Volume 2. Allured Publishing Corporation.

Introduction
La fouille de séries temporelles (FST) a récemment focalisé l'attention des chercheurs en fouille de données en raison de l'augmentation de la disponibilité de données comportant une dimension temporelle. Les algorithmes de FST tels que la classification/regroupement des sé-ries temporelles, l'extraction de motifs ou la recherche de similarités nécessitent une mesure de distance entre séries temporelles. Le calcul de ces distances repose principalement sur la classique distance euclidienne ou l'alignement temporel dynamique (DTW -Dynamic Time Warping) qui peuvent conduire à des temps de calcul trop importants pour s'attaquer à de longues séries ou à des bases de séries volumineuses. Aussi, de nombreuses représentations approchées des séries temporelles ont émergé au cours de la dernière décennie. La représenta-tion symbolique est une technique pour approximer les séries temporelles. L'algorithme SAX proposé par Lin et al. (2003) est un des plus utilisés pour la symbolisation. C'est une technique très simple, permettant de symboliser par segments les séries temporelles sans nécessiter d'information a priori. Lin et al. (2003) ont montré que SAX possède de bonnes performances pour la FST. Elle ne permet néanmoins pas de prendre en compte les informations de tendance dans les segments de séries temporelles. Plusieurs extensions de la représentation SAX ont été proposées pour pallier ce manque (Esmael et al. (2012); Lkhagva et al. (2006);Zalewski et al. (2012)). Cependant l'information sur la pente dans ces travaux est soit très simplifiée, soit ne tient pas compte du fait que la distribution des valeurs de pente dépend de la taille des segments. De plus, ces méthodes augmentent significativement la taille de la représenation symbolique associée et les résultats n'en tiennent pas toujours compte.
Nous avons proposé dans Malinowski et al. (2013), une nouvelle représentation symbolique 1d-SAX pour les séries temporelles, basée sur la quantification de la régression linéaire des segments de la série. Il est montré que cette nouvelle représentation approche plus préci-sément les données d'origine que la représentation SAX pour une même quantité de symboles disponibles, et qu'elle permet d'améliorer les performances en termes de recherche efficace de plus proche voisins dans des bases de série. Dans cet article, nous rappelons les grands principes de 1d-SAX, et montrons dans la dernière section que cette technique peut-être utilisée avantageusement (en termes de compromis complexité/stockage/performance) pour de la classification de séries temporelles d'images satellites.
2. Calculer la régression linéaire de la série temporelle sur chaque segment, 3. Quantifier les couples (moyenne, pente) de la régression linéaire en un symbole choisi dans un alphabet de taille N .
Dans l'étape 2, l'algorithme calcule la régression linéaire de chaque segment produit dans l'étape 1, puis cette régression est quantifiée dans un alphabet fini (étape 3). La régression linéaire est calculée selon l'estimation des moindres carrés :
. La régression linéaire de V sur T est la fonction l(x) = sx + b qui minimise la distance euclidienne entre l et V sur T . Elle est entièrement déterminée par les deux valeurs s et b. s représente la pente de l et b la valeur de l pour x = 0 :
où T et V représentent respectivement la moyenne des valeurs de V et de T . Dans ce qui suit, nous avons choisi de représenter une régression linéaire de V sur les instants T par la valeur de la pente s, et la valeur moyenne du segment a. a est défini par l'équation a = s × (t 1 + t L )/2 + b. À l'issue de l'étape 2, la série temporelle est représentée par des couples (s, a) pour chaque segment résultant de la segmentation de la série. Il faut ensuite quantifier ces couples dans un alphabet de N symboles. À cette fin, les deux valeurs d'un couple sont quantifiées séparément puis combinées en un symbole. Avec la même hypothèse de gaussiannité de la série temporelle, les propriétés statistiques de la régression linéaire garantissent que les distributions des valeurs de moyenne et des valeurs de pente sont gaussiennes de moyenne 0, de variance 1 pour les valeurs de moyenne et de variance ? 2 L , une fonction décrois-sante de L, pour les valeurs de pente. Selon ces propriétés, la quantification de la moyenne et de la pente peut se faire de la même manière que pour SAX. Les valeurs des moyennes sont quantifiées sur N a niveaux selon les (N a ? 1) quantiles de la distribution gaussienne N (0, 1), tandis que les valeurs des pentes sont quantifiées sur N s niveaux selon les (N s ? 1) quantiles de la distribution gaussienne N 0, ? 2 L
. Le choix du paramètre ? 2 L est important. À partir de l'analyse de l'impact de ? L sur de nombreuses séries temporelles de distribution gaussienne, ? 2 L = 0.03/L semble un bon compromis. Pour une même nombre de niveaux N = N a × N s , la représentation 1d-SAX permet différentes configurations, suivant le nombre de niveaux affectés respectivement à la moyenne et à la pente. Par exemple, une représentation symbolique sur 64 niveaux peut être répartie en 32 pour la moyenne et 2 pour la pente ou à 16 pour la moyenne et 4 pour la pente, etc.
Interrogation asymétrique d'une base de séries
Nous avons appliqué cette nouvelle représentation des séries temporelles au problème de la recherche du plus proche voisin (1-PPV). L'objectif de cette application est le suivant. Étant donné une base de données D contenant #D séries temporelles et une série requête q, nous souhaitons trouver les séries temporelles de D les plus proches de q. Nous supposons dans la suite de cet article que la série requête q et toutes les séries temporelles de la base de données sont de même longueur. La méthode naïve consiste à calculer la distance entre la requête et toutes les séries de la base de données et à retourner la série la plus similaire à q. Le nombre de distances à calculer est donc #D. Nous pouvons tirer parti de la représentation symbolique afin d'accélérer la recherche dans de grandes bases de données de séries temporelles. La représentation SAX a par exemple été utilisée pour indexer et interroger des bases contenant des téraoctets de séries temporelles (Shieh et Keogh (2008)). Nous définissons dans cette section un schéma d'interrogation asymétrique pour la recherche approchée de séries temporelles dans une base de données. Le terme asymétrique signifie que les requêtes ne sont pas quantifiées pour éviter d'avoir une double erreur de quantification (quand les requêtes et les séries de la base de données sont quantifiées). Jégou et al. (2011) ont montré que l'interrogation asymétrique améliore la précision de l'approximation pour la recherche de vecteurs. Nous proposons une méthode basée sur cette même idée pour la recherche de séries temporelles dans une base de données. Nous supposons que D contient les séries temporelles, ainsi que leur représentation symbolique (1d-SAX) pour un ensemble de paramètres donnés (L, N a , N s ). Cet ensemble de paramètres définit complètement les N = N a × N s symboles s 1 , . . . , s N utilisés pour quantifier la série. L'algorithme de recherche du 1-PPV de la requête q est le suivant :
1. Découper q en segments de longueur L : q = q 1 , . . . , q w 2. Calculer les distances euclidiennes entre chaque segment de q et les symboles s j , 1 ? j ? N . Ces valeurs sont stockées dans une table
. ED représente la distance euclidienne.
Pour toute série
obtenue par simple accès à la table A et sommation sur w éléments.
Une fois ces étapes effectuées, les distances entre q et toutes les séries temporelles de D sont disponibles pour identifier les plus proches voisins de q. Le nombre ? q d'opérations arithmétiques élémentaires nécessaires pour satisfaire une requête est :? q = (3L ? 1) × w × N + (w ? 1) × #D, où la partie gauche est le coût de l'étape 2 ci-dessus et la partie droite celui de l'étape 3. Dans le cas de la méthode naïve le nombre d'opérations élémentaires est (3Lw ? 1) × #D. Le coût du calcul dans le schéma de recherche approchée est plus faible, en particulier pour de grandes bases de données où N #D. La figure 1 compare les taux de classifications correctes obtenus en gardant les séries temporelles telles quelles (et en utilisant la distance euclidienne pour les comparer), et en les symbolisant par les techniques SAX et 1d-SAX. L'axe des abscisses représente le paramètre k de la classification par k plus proches voisins. On peut constater que les taux de classification correctes obtenus par la méthode 1d-SAX sont globalement supérieurs à ceux obtenus avec SAX. De plus, ils sont assez proches de ceux obtenus avec la distance euclidienne (méthode beaucoup plus coûteuse en temps et en espace de stockage des images).
Expérimentations et évaluations
Conclusion
Dans cet article, nous avons proposé une nouvelle représentation symbolique des séries temporelles. Cette représentation est basée sur la quantification de la régression linéaire des segments constituant la série. Les symboles prennent en compte des informations sur la moyenne et la pente des segments de la série temporelle. Dans toutes nos expérimentations, nous

Introduction
La représentation sac-de-mots des documents (abrégée ici en BoW, Bag-of-Words) est très largement utilisée en recherche d'information (RI) et en traitement automatique des langues (TAL). Elle permet d'associer à un texte un descripteur unique basé sur l'ensemble des motsformes qu'il contient. Cependant, cette représentation est parfois trop grossière pour certaines tâches. Plusieurs représentations alternatives ont été imaginées selon les cas et les informations disponibles. Les similarités entre objets complexes (graphes, arbres...) ont été extensivement étudiés (Bunke, 2000, inter alia), mais sont rarement utilisées en RI à cause de leur coût calculatoire. C'est pourquoi, dans beaucoup de cas, les travaux gardent une structure de données identique à celle des sacs-de-mots (même si ce sont des morphèmes, des n-grammes ou des syntagmes et non plus des mots qui sont manipulés). Dans cet article, nous nous intéressons à une extension simple de la représentation classique en sac-de-mots dans laquelle un objet est décrit par un multiensemble de sac-de-mots. Cette représentation en sac-de-sacs-de-mots (BoBoW, Bag-of-Bags-of-Words garde certaines propriétés calculatoires des BoW, mais nécessite de savoir comment agréger les résultats obtenus entre sacs-de-sacs. Dans son travail séminal en RI, Wilkinson (1994) l'utilise pour comparer une requête aux différentes portions d'un document et combiner les résultats, soit sur les similarités soit sur les rangs. Mais les quelques fonctions d'agrégation testées obtiennent des résultats inférieurs à un système vectoriel classique. En revanche, cette représentation a été utilisée avec succès dans des cadres particuliers en TAL (Ebadat et al., 2012) et en image (Kondor et Jebara, 2003). Elle est aussi à rapprocher des travaux sur la recherche d'information structurée (Luk et al., 2002)  
La comparaison de deux documents nécessite le calcul de toutes les combinaisons de similarités entre les vecteurs des deux documents, comme illustré en figure 1.
Il faut noter que la représentation en sacs-de-sacs implique une plus grande complexité mé-moire et calculatoire. Il faut en effet stocker plusieurs sous-documents pour un document ; ces sous-documents contiennent certes moins de mots, mais la somme de leur empreinte mémoire est supérieure à celle du document considéré dans son ensemble. Cette consommation supplé-mentaire dépend en pratique du nombre moyen de sacs par documents et de la façon dont ils sont stockés. En ce qui concerne le temps de calcul, la complexité est aussi plus grande. Au moment de la recherche, le calcul exhaustif du score d'un document est en O(n * m * d) avec n le nombre de sacs de la requête, m celui du document et d la complexité du calcul de ?.
Propriétés des fonctions d'agrégation pour le modèle vectoriel
Les mesures similarités entre deux sacs-de-sacs-de-mots agrègent les résultats de la similarité mineure ? pour toutes les combinaisons possibles vecteur à vecteur. Deux façons simples pour ce faire ont été proposées (Haussler, 1999) :
Plus récemment, une mesure plus générale a été proposée (Gosselin et al., 2007) : 
Beaucoup de façons d'agréger les similarités mineures peuvent être imaginées. Il convient cependant de s'interroger sur les propriétés attendues de ces agrégations. Nous en listons ici quelques unes qui nous semblent essentielles ou d'autres souhaitables pour que la représen-tation en sacs-de-sacs garde une sémantique correcte. En RI, cette sémantique doit assurer l'ordonnancement des documents par proximité avec la requête. Les modèles usuels traduisent cette proximité selon une réponse graduelle, de manière à induire un ordre total entre les documents. L'agrégation des similarités mineures se doit donc de conserver au mieux cette propriété. Pour simplifier les notations, nous considérons l'agrégation comme une fonction, notée Aggreg, prenant en paramètre les similarité mineures notées a, b, c....
Associativité Cette propriété traduit le fait que le résultat d'une agrégation soit considérée de même nature qu'une similarité mineure et puisse être à son tour utilisée pour une agrégation. Cela nous permet de généraliser les propriétés ci-dessous, définies sur des relations binaires, à des fonctions n-aires : Aggreg(a, b, c) = Aggreg(a, Aggreg(b, c)) = Aggreg (Aggreg(a, b), c). Ainsi, on peut définir la fonction PowerScalar avec deux arguments :
Comme nous l'avons dit, aucun ordre n'est à considérer pour les sacs-desacs ; on souhaite donc avoir une fonction commutative : Aggreg(a, b) = Aggreg(b, a)
Monotonie croissante. Une autre propriété essentielle est la monotonie de l'agrégation (croissante si la similarité mineure l'est) en fonction des similarités mineures :
Certaines autres propriétés ne sont pas nécessaires pour que la métrique résultante ait bien le comportement attendu, mais peuvent être recherchées pour espérer de bons résultats.
Élément neutre. Pour ne pas favoriser les documents contenant beaucoup de sacs sans liens avec la requête, il est souhaitable que la mesure d'agrégation ait un élément neutre qui soit le minimum de la similarité mineure. C'est le cas avec les fonctions proposées en section 2.2 dont l'élément neutre est 0, avec une similarité mineure basée sur le produit scalaire : Aggreg(a, 0) = a Continuité. La continuité de la fonction d'agrégation en fonction de toutes ses variables (similarités mineures) n'est pas non plus une condition nécessaire pour la sémantique de l'agré-gation. Cependant, une telle continuité permet évidemment un comportement plus facilement interprétable et prévisible.   (Hull, 1993).
Dans ces expériences et les suivantes rapportées dans cet article, les textes que nous traitons doivent être découpés en sous-documents, chacun étant représenté par un sac. Selon le formatage disponible (textes organisés en paragraphes ou non, etc.) et l'application visée, plusieurs options peuvent être explorées. Dans le cadre de nos évaluations, nous adoptons un découpage en phrase (détectées via les marques de ponctuations. Les requêtes sont également représentées en sacs de sacs : un sous-document correspond ici aussi à une phrase, s'il y en a, ou au plus à un champ (titre, corps...).
Adaptations des pondérations
Selon le modèle adopté, il peut être nécessaire d'adapter les pondérations utilisées. La plupart des fonctions de pondérations de RI font intervenir des valeurs calculées sur le document (IDF, longueur du document DL...). Le passage à une unité plus petite pose la question du calcul de ces valeurs. Nous rapportons ci-dessous une des expériences menées pour tester deux stratégies : dans un cas, les variables problématiques (IDF, DL...) sont calculées classiquement sur le document, et dans l'autre cas, sur le sous-document (le DL est alors la longueur du sous-document considéré, l'IDF est la fréquence document inverse dans l'ensemble des sousdocuments de la collection). Pour ces expériences préliminaires, nous utilisons la collection INIST, nous fixons Sum-Sum comme mesure d'agrégation. La similarité mineure est le modèle Okapi-BM25 (Robertson et al., 1998), classiquement utilisé en RI, avec les constantes fixées à leur valeur par défaut : k 1 = 2, k 3 = 1000 et b = 0.75.
Les résultats sont donnés dans le tableau 1, avec la représentation sac- 
Résultats
Nous évaluons les systèmes de RI BoBoW avec les paramètres décrits dans les soussections précédentes selon les fonctions d'agrégation présentées en section 2.2. Le tableau 2 présente les résultats obtenus respectivement en utilisant un modèle Okapi-BM25 (avec les paramètres IDF et DL calculés à l'échelle du document). Nous faisons apparaître les différences par rapport à la référence ; celles non statistiquement significatives sont en italiques.
On remarque que la représentation en sacs-de-sacs de mots obtient dans la plupart des cas des performances au moins équivalentes à celles du sac-de-mots classique, alors même que nous n'en avons optimisé aucun des paramètres. Cela montre le potentiel intéressant de ce type de représentations. À ce titre, les bons résultats de l'agrégation PowerScalar recoupent les constatations faites dans d'autres contextes (Ebadat et al., 2012;Gosselin et al., 2007). À l'inverse, le fait que Max-Max fonctionne moins bien que les autres agrégations, comme dans les tentatives de (Wilkinson, 1994), est dû au caractère non archimédien de cette fonction (la pertinence repose sur la proximité entre un seul sous-document du document et de la requête).
Conclusion
L'objectif de cet article était d'étudier les conditions nécessaires à l'utilisation des repré-sentations en sac-de-sacs-de-mots, en s'intéressant notamment aux fonctions d'agrégation au coeur de cette approche. Nous avons mis en lumière quelques unes des propriétés souhaitables de ces fonctions, que nous avons illustrées dans un cadre vectoriel classique. Les expérimenta-tions menées ont mis en exergue l'importance du choix de la fonction, en fonction notamment de son comportement seuillant ou non. Bien que notre but n'était pas d'optimiser un système

Introduction
Les ontologies sont nées d'un besoin de standardisation des vocabulaires ressenti dans de nombreux domaines et connaissent, depuis quelques années, un succès qui a en partie été porté par l'explosion du web de données (cf. Heath et Bizer (2011)), dont la promesse est le partage à grande échelle de données ouvertes et liées. Le web d'aujourd'hui est donc non seulement une source inépuisable d'informations, mais il est devenu, à travers le web de données, un vecteur de développement et de diffusion incontournable pour les ingénieurs de la connaissance et les fournisseurs de données.
Participer au web de données suppose d'être capable de s'interconnecter avec les données et ontologies déjà présentes et disponibles. Publier une nouvelle ontologie nécessite donc au préalable de la relier avec les "bonnes" ontologies publiées sur le web de données comportant des concepts similaires dans des domaines similaires. Relier deux ontologies consiste en fait à les aligner, i.e. trouver des correspondances entre les entités (concepts, propriétés ou instances) des deux ontologies. L'alignement d'ontologies est un domaine en plein essor qui a donné lieu à de nombreux travaux de recherche. Nous pouvons notamment citer Shvaiko et Euzenat (2013), Bernstein et al. (2011), Rahm (2011) ou encore Euzenat et Shvaiko (2007). Ces travaux ont permis de définir des formalismes et des outils, qui sont régulièrement évalués dans le cadre de la campagne d'évaluation de l'OAEI Nous proposons dans cet article une méthode d'alignement d'une ontologie source avec des ontologies cibles déjà publiées sur le web de données et liées entre elles. Ces ontologies peuvent en fait être des thésaurus, des ontologies ou encore des ressources termino-ontologique qui peuvent être exprimées dans différents langages de représentation. Notre méthode repose sur un principe de raffinage d'alignements qui exploite des ontologies liées sur le web de données. Cette méthode prend en entrée un ensemble d'alignements qui peuvent être générés en utilisant différentes approches d'alignement.
Nous proposons d'illustrer la méthode proposée par un retour d'expérience sur l'alignement d'une ontologie dans le domaine des sciences du vivant et de l'environnement. Dans ce domaine, plusieurs thésaurus ont été créés au niveau international et publiés sur le web de données pour faire face au besoin de standardisation des vocabulaires. Les deux plus importants sont actuellement AGROVOC 2 et NALT 3 . AGROVOC a été créé dans les années 1980 par la FAO (Food and Agriculture Organization of the United Nations) comme un thésaurus structuré multilingue pour les domaines de l'agriculture, de la sylviculture, de la pêche, de l'alimentation et de domaines apparentés (comme l'environnement). Il est actuellement disponible en 19 langues, avec une moyenne d'environ 40 000 termes dans chaque langue (cf. Caracciolo et al. (2012)  Caracciolo et al. (2012)). Dans ce papier, nous nous intéressons à l'alignement d'une ressource termino-ontologique naRyQ (n-ary Relations between Quantitative experimental data) non encore publiée (cf. (Buche et al., 2013)) avec AGROVOC et NALT. naRyQ contient environ 1 100 concepts structurés en plusieurs sous-domaines, tels que les produits alimentaires, les microorganismes et les emballages.
Nous présentons, dans la section 2, notre méthode d'alignement d'une ontologie avec des ontologies liées et, dans la section 3, les résultats de notre expérimentation dans le domaine des sciences du vivant et de l'environnement. Enfin, nous concluons dans la section 4. 
Alignement des variantes des ontologies
Le processus d'alignement d'ontologies prend en entrée deux ontologies et produit en sortie un ensemble de correspondances entre les entités de ces ontologies. Selon Euzenat et Shvaiko (2007), ce processus est défini comme suit :  
On remarquera que le nombre total de processus d'alignement lancés pour obtenir l'alignement de l'ontologie source O s avec chacune des deux ontologies cibles O 
Deuxième étape : raffinage des correspondances trouvées
La deuxième étape consiste à raffiner les ensembles d'ensembles de correspondances
. Ces deux ensembles d'ensembles qui résultent de la concaténation des résultats de plusieurs processus d'alignement lancés sur plusieurs variantes d'ontologies permettent d'obtenir de nombreuses correspondances (qui laisse présager une bonne couverture), mais aussi beaucoup de bruit, i.e. des mauvaises correspondances, qu'il convient de réduire.
Nous proposons deux méthodes de raffinage pour améliorer la qualité des correspondances trouvées lors de la première étape : la première méthode permet de supprimer des correspondances considérées comme ambiguës et donc potentiellement éronnées (section 2.2.1) ; la deuxième méthode permet d'identifier les correspondances considérées comme potentiellement correctes (section 2.2.2).
Suppression des correspondances ambiguës
Nous distinguons trois types d'ambiguïté entre correspondances. Le premier type d'ambiguïté concerne les correspondances obtenues à partir d'une même méthode d'alignement lancée sur différentes variantes des ontologies source et cible, correspondances qui ont la même entité source, la même entité cible et la même relation. Nous proposons de lever les ambiguïtés de type 1 en ne conservant que la correspondance ayant le degré de confiance le plus élevé. 
sont ambiguës selon le type 1 si :
L'ensemble des ensembles de correspondances non ambigüs selon le type 1 est :
Le deuxième type d'ambiguïté identifié entre des correspondances correspond au cas où une entité d'une ontologie source O s est alignée, par la relation d'équivalence, avec deux entités distinctes d'une ontologie cible O c . Nous proposons dans ce cas de ne conserver que la correspondance la plus pertinente, i.e. celle qui a a priori le degré de confiance le plus élevé. Cependant lorsque ces correspondances n'ont pas été générées par la même méthode d'alignement, leur degré de confiance ne sont pas comparables. Nous proposons de calculer une mesure de similarité sim, indépendante des méthodes d'alignement utilisées, pour les deux correspondances à comparer, qui peut, par exemple, s'appuyer sur les mesures de similarité syntaxiques usuelles implémentées dans l'Alignment API (David et al., 2011). 
c . L'ensemble des ensembles de correspondances non ambiguës de type 2 est :
Le troisième type d'ambiguïté identifié entre des correspondances correspond au cas où deux entités distinctes d'une ontologie source O s sont alignées, par la même relation, avec une même entité d'une ontologie cible O c . Nous proposons dans ce cas de ne conserver que la correspondance la plus pertinente, i.e. celle avec la mesure de similarité sim la plus élevée. 
Identification des correspondances potentiellement correctes
Lorsque des redondances apparaissent entre des correspondances qui ont été générées à partir d'au moins deux méthodes d'alignement distinctes, nous faisons l'hypothèse que ces correspondances peuvent être considérées comme ayant plus de "chance" d'être bonnes. Nous les conserverons dans un ensemble distinct, noté recT C Os?Oc pour ensemble de recouvrement, afin de les présenter à l'utilisateur comme des correspondances potentiellement correctes.   
Nous considérons dans la section 3 que l'ensemble des correspondances potentiellement correctes entre une ontologie source O s et une ontologie cible O c est l'ensemble obtenu par l'union des deux ensembles de recouvrement définis ci-dessus, noté U * ,2 * ,3 * recT Os?Oc = C Os?Oc ? LOD C Os?Oc , dans lequel ont été supprimées les ambiguïtés de type 1, 2 et 3.
Expérimentation
Nous illustrons dans cette section la méthode présentée dans la section 2 pour aligner une ontologie source naRyQ , présentée dans la section 3.1, avec chacune des deux ontologies cibles, AGROVOC et NALT. L'alignement de l'ontologie naRyQ avec AGROVOC est noté naRyQ ? AGROVOC et l'alignement de l'ontologie naRyQ avec NALT est noté naRyQ ? NALT.
LOD pour Linked Open Data en anglais
3.1 L'ontologie source naRyQ L'ontologie naRyQ (n-ary Relations between Quantitative experimental data) a été définie pour représenter des relations n-aires entre des données quantitatives expérimentales (Buche et al., 2013). Les spécificités de cette ontologie sont les suivantes : (i) c'est une ressource termino-ontologique qui est un modèle hybride ; (ii) les labels sont disponibles en français et en anglais ; (iii) elle est représentée en OWL DL et SKOS ; (iv) la composante conceptuelle contient environ 1 100 concepts structurés en plusieurs sous-domaines, les plus importants en effectif étant les produits alimentaires (? 460 concepts), les microorganismes (? 180 concepts) et les emballages (? 150 concepts).
Production des alignements de références
Pour évaluer la qualité des alignements produits et pour comparer entre eux les résultats des processus d'alignement, nous utilisons les mesures de précision et de rappel adaptées à l'alignement d'ontologies (Euzenat et Shvaiko, 2007). Ces mesures s'appuient sur une comparaison avec un alignement de référence, R. La production d'un alignement complet n'étant pas envisageable car elle demanderait de nombreuses personnes, du temps et une expertise pointue, nous avons construit deux alignements, partiels, notés R AGROVOC pour l'alignement naRyQ ? AGROVOC, et R NALT pour l'alignement naRyQ ? NALT. Dans la suite, nous n'étu-dierons que la relation d'équivalence ?.
Pour chaque ontologie et pour tout concept, nous avons extrait ses labels (e.g. skos :prefLabel, skos :altLabel, rdfs :label, rdfs :comment) en anglais ou en français ainsi que des éléments de structure (e.g. skos :broader, rdfs :subClassOf). Un premier alignement a été produit en utilisant SMOA (A String Metric for Ontology Alignment) (Stoilos et al., 2005), une similarité syntaxique destinée à l'alignement d'ontologie. Cet alignement a permis de générer 1 453 correspondances, qui ont ensuite été validées par deux experts en double aveugle et ont été réconciliées, i.e. les experts se sont a posteriori mis d'accord. Cette validation a été réalisée en quatre heures, en utilisant un outil de visualisation spécifiquement développé pour cette tâche.
Afin d'améliorer les premiers alignements produits R AGROVOC et R NALT , nous les avons enrichis en exploitant les résultats d'alignement obtenus avec la méthode proposée dans ce papier. Les alignements ainsi enrichis, notés R  (David, 2007). Quelque soit l'outil utilisé, nous avons, dans la suite, fait l'hypothèse qu'une correspondance est jugée acceptable si elle est obtenue avec un degré de confiance supérieur ou égal à 0.5, seuil qui a été obtenu empiriquement suite à de nombreux tests.
Nous avons retenus les variantes d'ontologies suivantes, en ne retenant pour AGROVOC que les labels en français et en anglais et pour NALT que les labels en anglais :
Remarque 3.1 La variante naRyQ OW L a été obtenue en gardant la structure de la composante conceptuelle et en transformant les skos :prefLabel et skos :altLabel en rdfs :label. La variante naRyQ SKOS a été obtenue en gardant les labels de la composante terminologique et en transformant la hiérarchie conceptuelle en éléments de hiérarchie SKOS.   
Évaluation de naRyQ ? NALT
Le tableau 3 présente l'évaluation des différents ensembles de correspondances produits durant la phase de raffinage par rapport à l'ensemble d'alignement de référence partiel, R 
Discussion
Comme nous pouvons l'observer dans les tableaux 2 et 3 et comme nous pouvions nous y attendre, (1) l'augmentation de l'ensemble d'alignements permet d'améliorer les valeurs de rappel (au détriment de la précision) pour la plupart des ensembles produits (meilleur rappel avec agr C * ), et, (2) en combinant les différentes méthodes de raffinage, nous obtenons de meilleurs résultats en termes de précision (ensemble U * ,2 * ,3 * ). En comparant ces résultats avec les meilleurs scores obtenus par les outils d'alignement (cf. tableau 1), dans le cas de AGROVOC, notre approche obtient des performances similaires en termes de F-mesure, tandis que dans le cas de NALT une augmentation de la F-mesure est observée. De plus, notre approche surpasse les meilleurs résultats individuels en rappel pour AGROVOC et en précision pour NALT. Ce qui représente, dans l'ensemble, des résultats très encourageants.
Notre approche d'exploitation des alignements existants sur le web de données pour raffiner des résultats d'alignement est une piste prometteuse. D'autres travaux exploitent également le web de données pour aider à la tâche d'alignement. Pernelle et Sais (2011) proposent une approche qui combine à la fois la découverte de liens entre les données du web de données et l'alignement entre les concepts de deux ontologies. Parundekar et al. (2012) exploitent les liens entre différents sources de données du web de donnés pour aligner deux ontologies.
La plupart des outils d'alignement utilisent des stratégies pour combiner différentes mé-thodes d'alignement de base (i.e., lexicale, structurale, etc.) au sein d'un processus d'alignement et pour filtrer leurs résultats (seuil, agrégation pondérée, règles, etc.) (cf. Euzenat et Shvaiko (2007)). Nous nous intéressons ici à la fois au raffinage des ensembles d'alignements produits par différents outils d'alignements et à la discrimination de l'ensemble de correspondances trouvées. Dans le premier cas, nous avons identifié trois types d'ambiguïté à résoudre pour raffiner l'ensemble de correspondances en supprimant un certain nombre. Dans le deuxième cas, nous proposons deux méthodes originales pour discriminer l'ensemble des correspondances. La première méthode consiste à considérer la redondance entre des correspondances trouvées dans au moins deux processus d'alignement issus de méthodes distinctes comme gage de validité (i.e. une correspondance apparaissant dans le résultat d'au moins deux processus d'alignement est susceptible d'être correcte). La deuxième méthode consiste à exploiter les alignements définis sur le web de données pour appuyer la validité de certaines correspondances (i.e. les correspondances permettant d'aligner une même entité à deux entités distinctes mais liées sur le web de données sont susceptibles d'être correctes). D'autres travaux (Mochol et Jentzsch, 2008;Steyskal et Polleres, 2013) ont, comme nous, eu l'idée d'utiliser des outils existants et de combiner leurs résultats pour aligner des ontologies. Tandis que Mochol et Jentzsch (2008) utilisent un ensemble de règles pour sélectionner le meilleur outil, Steyskal et Polleres (2013) proposent une méthode itérative basée sur le vote où, à chaque tour, les correspondances acceptées par la majorité des outils sont considérées comme valides. Cependant, ces travaux n'exploitent pas des alignements sur le web de données.
Un autre point original de notre approche réside dans l'exploitation de différentes variantes qui permet de prendre en compte les spécificités des ontologies à lier qui peuvent être des ontologies, des thésaurus, des RTO et qui peuvent être exprimées dans différents langages de représentation avec différents niveaux d'expressivité. Cela nous donne la possibilité de couvrir un panel large et hétérogène de ressources.
Conclusion et persectives
Nous avons proposé une méthode originale d'alignement d'ontologies qui permet de relever l'un des défis des alignements d'ontologies cité dans Shvaiko et Euzenat (2013) : « Matching with background knowledge ». Notre méthode permet, d'une part, d'obtenir de nombreuses correspondances en utilisant et combinant des méthodes existantes pour aligner des ontologies, des thésaurus et des RTO exprimés dans différents langages. Elle permet, d'autre part, de discriminer les correspondances obtenues en en supprimant certaines ambiguës, et,

Introduction
La numérisation de documents administratifs est un enjeu économique et écologique prioritaire dans le contexte sociétal actuel. Le défi n'est plus la numérisation du document, mais l'extraction des informations qu'ils contiennent. Cet article présente une nouvelle approche d'annotation automatique de documents administratifs (certificat d'assurance, acte de naissance, etc.) qui utilise le logo contenu dans les documents comme élément d'apprentissage. Le logo est un élément graphique riche de sens (Duthil et al., 2013) auquel il est possible de rattacher de multiples informations (secteur d'activité, etc.). L'objectif ne se limite donc pas à la reconnaissance d'un logo dans un document (élément graphique) mais également aux aspects sémantiques connexes. Cet article est organisé de la manière suivante : la section 2 présente un état de l'art des méthodes existantes de classification et d'extraction de logo pour l'annotation. La section 3 présente notre approche d'annotation automatique de documents administratifs. La section 4 est consacrée aux expériences et la section 5 conclue cet article et donne quelques perspectives.
État de l'art
La détection et l'extraction de symboles ou de logos est un sujet de recherche actif de ces deux dernières décennies, comme l'atteste les très nombreuses publications réalisées dans les conférences ICDAR ou GREC depuis 1995. Ces approches ont tout d'abord cherché à exploiter des images binaires, pour ensuite évoluer vers des images de documents en couleurs (Ahmed, 2008;Nourbakhsh et al., 2011). Nous proposons un bref résumé des méthodes dédiées aux logos ci-dessous.
En 2008, Zeggari (Ahmed, 2008) développé un algorithme d'extraction de logo reposant sur deux propriétés principales des logos : leur compacité spatiale et leur uniformité colorimétrique. Tout d'abord, le contenu de l'image est simplifié et transformé à partir d'opérateur morphologiques pour uniformiser les logos appartenant à une même classe. Puis, la densité spatiale et chromatique des régions composant chaque logo sont calculés.
Une approche intéressante, proposée en 2012 par Sahbi et al (Sahbi et al., 2012), vise à définir un noyau sur la "similarité liée au contexte", qui intègre le contexte spatial de caracté-ristiques locales. Les points d'intérêts sont détectés à l'aide du détecteur SIFT, et le contexte est décrit à l'aide d'un shape-context. Une fonction mesurant la similarité est alors définie en s'appuyant sur trois critères principaux : la fidélité, le contexte et l'entropie du terme, pour rechercher des points d'intérêts similaires selon ces critères.
La plupart de ces méthodes présentent des restrictions particulières,soit elles reposent sur des heuristiques à priori, ou alors sont très coûteuses en temps de calcul. De plus, hormis la dernière approche citée, toutes les autres reposent sur des critères bas-niveau (descripteurs radiométriques) sans intégrer le contexte ni conceptualiser le contenu d'une image. L'objectif de cet article est de proposer une méthode applicable à la fois sur les images en couleurs et en noir et blanc, avec des documents de bonne qualité ou bruités, et capable d'intégrer un apprentissage incrémental (apprentissage de nouveau logo à la volée). Enfin, nous proposons une méthode qui combine des traitements d'images et des techniques de fouille de texte afin d'annoter un document par un ensemble de mots clés caractéristiques des différents concepts contenus dans le documents.
L'approche
Notre approche d'annotation de documents administratifs est composée de quatre étapes. La première étape consiste à extraire automatiquement les zones saillantesdu document afin d'extraire et d'identifier le(s) logo(s) contenu(s) dans le document. La seconde étape permet la construction automatique d'un corpus d'apprentissage en utilisant les vignette identifiées précédemment. La troisième étape sert d'apprentissage du vocabulaire relatif au logo. La dernière étape correspond à l'annotation du document par le lexique de descripteurs appris lors de l'étape précédente.
Extraction de logo
Un logo est une région particulière d'un document qui peut-être définie par les trois proprié-tés suivantes (Duthil et al., 2013) : Région visuellement saillante par rapport à son contexte, récurrent visuellement (même image dans différent contextes), élément graphique créé par l'Homme pour l'Homme. L'objectif de cette première étape est donc de rechercher et d'extraire des zones saillantes dans un document, zones qui seront analysées par la suite pour identifier les logos potentiellement présents. Pour réaliser cette étape, nous nous sommes appuyés sur l'approche proposée par (Perreira Da Silva et Courboulay, 2012) pour l'analyse visuel du document. Ce modèle hybride permet d'étudier l'évolution temporelle du focus attentionel. Le système visuel est inspiré des modèles d'attention de Itti et al. (1998) et Frintrop (2005. La scène visuelle est décomposée en différentes caractéristiques selon une approche multiréso-lution et ces caractéristiques sont calculées à partir de filtres numériques. Le système génère, pour chacune des caractéristiques prises en compte (intensité, couleur et orientation), un certain nombre de cartes représentant les éléments les plus saillants. À partir de chacune des vignettes identifiées par le processus de saillance visuelle, un processus d'identification des logos est effectué. Chaque vignette fait l'objet d'une requête sous forme d'image sur un moteur de recherche web (Google Images) pour identifier le nom du logo. Si la vignette n'est pas reconnue, elle ne sera plus considérée dans la suite du processus (fouille de texte), sinon, le nom du logo est conservé et il sera ensuite utilisé lors de la phase d'apprentissage des descripteurs sous forme de "mot germe"(c.f. section 3.2.2).
Fouille de texte
Le processus de fouille de texte est composé des étapes 2 et 3. Cette section présente la mé-thode de construction automatique du corpus d'apprentissage et d'apprentissage automatique des descripteurs. L'approche Synopsis proposée par Duthil et al. ((Duthil et al., 2012)) entre dans ce cadre. D'une part, Synopsis nous permet de construire automatiquement un corpus d'apprentissage à partir de "mots germes", et d'autre part, l'approche nous permet un apprentissage automatique des descripteurs.
Construction du corpus d'apprentissage
La construction du corpus d'apprentissage a pour objectif d'obtenir des documents web qui ont un contenu similaire à la vignette requête. À chaque vignette saillante est associé un corpus de documents. Plus formellement, à chaque vignette q, q variant de 1 à k, k étant le nombre de vignettes identifiées, un corpus Doc q de n documents est associé tel que Doc q = doc q n , n = 1 . . . n q .
Apprentissage des descripteurs
L'objectif de l'apprentissage est de construire un lexique de descripteurs textuels (mots) décrivant sémantiquement chacun des logos. À chaque logo est associé un lexique L q . Pour faire le lien entre les éléments graphiques contenus dans le document et les différents concepts auxquels ils font référence, nous utilisons le corpus d'apprentissage constitué à l'étape 2. L'approche Synospis est principalement basée sur deux éléments clés : la notion de fenêtre et la notion de classe/anti-classe. La fenêtre permet d'effectuer un apprentissage des descripteurs tout en assurant leur cohérence sémantique avec le mot germe (Duthil et al., 2012). La notion de classe/anti-classe permet de filtrer le bruit web. Les noms communs et les noms propres sont les deux classes grammaticales apprises car elles sont reconnues comme porteuses de sens.
Plus formellement une fenêtre de taille sz centrée sur un mot germe g pour un document doc est définie par F (g, sz, doc) = {m ? doc/d Le principe général est de calculer la représentativité (Duthil et al., 2012) d'un mot M dans chacune des deux classes (fréquence d'apparition normalisée ?(M ) dans la classe (mots présents dans les fenêtres) et dans l'anti-classe ?(M ) (mots en dehors des fenêtres). La repré-sentativité d'un mot M dans chacune des classes est défini tel que :
F (?, sz, doc))| À partir de la représentativité d'un descripteur dans chacune des classes, il devient possible de déterminer la proximité sémantique du descripteur M considéré en appliquant une formule de discrimination f tel que cela est proposé dans (Duthil et al., 2012). Un score Sc(M ) est alors attribué à chaque descripteur. Chaque descripteur constitue une entrée du lexique L q propre au logo q considéré. Le score d'un descripteur est calculé tel que :
Annotation des documents
L'étape d'annotation consiste à rattacher à un document l'ensemble des concepts qu'il contient (Lexique de mots précédemment construits). Annoter sémantiquement un document au format image revient à rechercher, et à identifier, les logos qu'il contient afin de lui rattacher les lexiques associés (sémantique). Annoter un document à partir de son contenu textuel (résul-tat d'OCR) consiste à identifier les concepts contenus dans le document. L'approche Synopsis permet d'identifier (segmentation), à partir d'un lexique, les zones du document qui traitent du concept (logo) considéré. L'approche utilise une fenêtre glissante (Duthil et al., 2012) centrée sur les noms communs pour identifier les segments de textes pertinents. Cette méthode nous permet également de connaître l'intensité ( (Duthil et al., 2012)) du discours. À chaque document est associé un fichier xml qui contient un ensemble d'informations sémantiques : lexiques associés (identifiant du lexique correspondant au nom du logo), intensité du discours et l'importance du discours pour chacun des concepts (lexiques) qui ont été rattachés.
Expérimentations
Dans cette section nous évaluons la qualité de l'apprentissage sur un corpus de 1766 documents administratifs dans un contexte de classification. Ce corpus est composé de 4 classes de documents : acte de mariage (A-M), certificat d'assurance (C-A), relevé d'identité bancaire (RIB) et certificat de naissance (C-N). Le tableau 1 montre la répartition de ces quatre classes (Nombre de logos différents dans la classe et nombre de document de la classe). Cette base confidentielle provient d'un des leaders mondiaux de la dématérialisation documentaire. Chaque document contient un des 196 logos identifiés dans le corpus. Les documents sont scannés en 200 dpi noir et blanc. Nous utilisons les indicateurs classiques de mesure pour éva-luer la classification : Précision, Rappel. La Précision est calculée en considérant les erreurs d'identification du logo : le système identifie un logo qui n'est pas le bon, le lexique associé ne correspond donc pas au logo à identifier. Le Rappel est calculé en utilisant le nombre de logos correctement identifié par le système. Nous utilisons durant tout le processus comme moteur de recherche web Google Images. Nos résultats sont résumés dans le tableau 2.
Les résultats montre la pertinence du système. Les différences de résultats entre chacune des classes s'expliquent par la qualité des documents. Cependant, les résultats sont remarquables, nous obtenons un Rappel de 80,6 et une précision de 100 toutes classes confondues. La Précision (100) met en évidence la robustesse de l'approche. 

Introduction
Le monde des musées aujourd'hui connaît un engouement sans précédent qui a vu plus de 31 millions de visiteurs se précipiter en 2012 dans les musées nationaux français (NuitDesMusées, 2013). Par ailleurs, et depuis plus d'une dizaine d'années, les ministères et les services publiques des différents pays accordent de plus en plus d'importance à l'ouverture et à la réuti-lisation de leurs données collectées ou produites au niveau de leurs différents établissements. Or, avec le temps ces données s'accumulent et deviennent très difficiles à stocker, à traiter et à analyser. D'où la nécessité de s'orienter vers de nouvelles solutions et paradigmes de programmation afin de faire face à ces difficultés. Nous proposons dans ce travail un algorithme basé sur le paradigme MapReduce (Dean et Ghemawat, 2008) qui permet, à partir des données ouvertes du Ministère français de la communication et de la culture, de déterminer le degré d'accessibilité des personnes handicapées (tout type d'handicape) aux musées nationaux français. L'algorithme définit, à partir de ces données brutes et hétérogènes collectées, une note pour chaque musée selon son degré d'accessibilité, et retourne un classement final des musées par commune et par région de France. Notre algorithme de classement présente plusieurs avantages notamment en terme de rapidité de traitement de grandes quantités de données puisqu'il ne s'exécute pas de manière classique (mono-poste) mais distribué sur un cluster d'ordinateurs. Nous avons implémenté notre algorithme sur une plateforme Hadoop multi-noeuds.
Jeu de données
L'algorithme que nous présentons dans ce papier est appliqué aux données publiques 1 ré-coltées à partir de la grande manifestation annuelle : "La nuit européenne des musées". Elles concernent plus de 3000 musées en Europe dont plus de 1200 établissements français labellisés "Musées de France" par le Ministère de la communication et de la culture. Ces données sont fournies sous le format de fichier .csv, elles regroupent des informations (plus de 48 attributs avec la dernière mise à jour) sur chaque établissement tel que : Le nom du musée, adresse, commune, etc. Mais aussi des données sur différents types d'accessibilités aux personnes handicapés offerts tel que : accès-handicape-moteur, accès-handicape-visuel, accès-handicape-auditif, accès-handicape-intellectuel et acces-handicap-langueDesSignes. Ces dernières sont des données binaires, le but est de traiter ces données là afin d'extraire une note d'accessibilité pour chaque musée, et au final agréger les résultats obtenus géographiquement par ville et par région de France. 
Summary
Based on official data from the French Ministry of Communication and Culture, we propose in this paper a parallel algorithm as a solution to extract and process these data sets in order to define a ranking of national museums and galleries according to the accessibility degrees for people with disabilities.

, Christine Froidevaux
(1), (3) (1) LRI, CNRS UMR 8623, Université Paris-Sud, 91405 Orsay Résumé. Les fonctions biologiques dans la cellule mettent en jeu des interactions 3D entre protéines et ARN. Les avancées des techniques exérimentales restent insuffisantes pour de nombreuse applications. Il faut alors pouvoir pré-dire in silico les interactions protéine-ARN. Dans ce contexte, nos travaux sont focalisés sur la construction de fonctions de score permettant d'ordonner les solutions générées par le programme d'amarrage protéine-ARN RosettaDock. La méthodologie d'évaluation utilisée par RosettaDock impose de trouver une fonction de score s'exprimant comme une combinaison linéaire de mesures physicochimiques. Avec une approche d'apprentissage supervisé par algorithme géné-tique, nous avons appris différentes fonctions de score en imposant des contraintes sur la nature des poids recherchés. Les résultats obtenus montrent l'importance de la signification des poids à apprendre et de l'espace de recherche associé.
Introduction
La plupart des mécanismes cellulaires mettent en jeu des complexes protéine-ARN. La compréhension de leurs fonctions dans un but thérapeutique ne peut se faire que par une connaissance fine des mécanismes moléculaires. Même si plus d'un millier de structures 3D de complexes protéine-ARN sont disponibles dans la Protein Data Bank 1 , base de données de référence des structures 3D, la résolution expérimentale reste longue et coûteuse, parfois même impossible. Les travaux présentés dans cet article sont focalisés sur l'amélioration d'une des approches de référence dans le domaine de la prédiction de l'amarrage (docking) de structures 3D in silico : RosettaDock (Gray et al. (2003)). L'objectif de ces approches est de modéliser la protéine et l'ARN et d'en prédire les assemblages 3D les plus probables. De nombreuses méthodes, dont RosettaDock, fonctionnent en deux phases imbricables l'une dans l'autre : (1) génération d'un large ensemble de candidats 2 et (2) évaluation de ces candidats pour ne retenir que les plus plausibles. La "qualité" des candidats est évaluée avec une fonction de score dédiée à la problématique de l'amarrage (Lensink et Wodak (2010)). La complexité des objets étudiés rend quasiment impossible l'obtention d'un candidat en tous points identique à la solution obtenue expérimentalement. Afin de déterminer pour chaque candidat s'il est acceptable, la mesure RMSD (Root Mean Square Deviation) est calculée entre ce candidat et la solution. Tous les candidats ayant un RMSD ? 5 Å sont considérés comme des solutions acceptables (des presque natifs), les autres candidats étant appelés leurres.
Les travaux présentés dans cet article concernent la construction d'une fonction de score permettant de trier les candidats générés. Il a déjà été montré que les techniques d'apprentissage se prêtaient bien à ce genre de problème Bernauer et al. (2007); Bourquard et al. (2011) . Nous avons choisi d'adapter l'outil RosettaDock pour l'amarrage protéine-ARN en créant une fonction de score spécifique. Les performances obtenues par RosettaDock, dans le cadre de la compétition internationale d'amarrage CAPRI 3 , font de cet outil un logiciel très performant pour l'amarrage protéine-protéine.
La problématique de l'amarrage protéine-ARN est assez récente dans CAPRI (Lensink et Wodak (2010); Pons et al. (2010)) et il n'existe pas encore de consensus sur la nature des fonctions de score à utiliser. Dans sa version actuelle, RosettaDock ne dispose pas d'une fonction de score dédiée. Les fonctions de score de RosettaDock sont de la forme suivante : f (X) = i w i x i où X représente le candidat à évaluer, w i les poids de chaque attribut et x i les attributs physico-chimiques. Les attributs sont tels que des poids à valeurs positives sont biologiquement interprétables mais la restriction des poids aux valeurs positives impose une contrainte forte sur l'espace de recherche des poids optimaux. Nous avons donc relâché cette contrainte en autorisant les poids à évoluer dans les trois intervalles suivants :
. La recherche de ces poids est effectuée par un algorithme génétique présenté dans la section 3.
Données utilisées
Pour ce travail, nous avons utilisé un jeu de données de 120 complexes binaires (une protéine et un ARN) de référence issus de la PRIDB 3 Protocole expérimental L'utilisation de RosettaDock impose que les fonctions de score recherchées sont des combinaisons linéaires des attributs physico-chimiques présentés dans la section 2. Plusieurs formes d'apprentissage ont été testées pour optimiser les poids des différents attributs : régression linéaire, régression logistique et SVM. Les meilleurs résultats ont été obtenus avec une approche par régression logistique dont les poids sont optimisés par l'algorithme génétique (RO-GER (Sebag et al. (2003)) adapté à la régression logistique). La fonction à optimiser est l'aire sous la courbe ROC (ROC-AUC). 100 000 itérations avec µ = 10 (nombre de parents) et ? = 80 (nombre d'enfants) sont effectuées.
Afin d'évaluer les performances de notre approche, nous avons mis en place une variante du cadre classique d'évaluation Leave-one-out. Au niveau de la phase d'apprentissage, nous pouvons parfaitement mélanger les candidats issus de plusieurs couples protéine-ARN diffé-rents car nous cherchons à apprendre des poids valides pour tous les couples protéine-ARN. Par contre, pour tester l'efficacité d'une fonction de score, il est impératif de travailler sur un ensemble de candidats issus d'un seul et unique couple protéine-ARN. Nous avons donc mis en place une validation de type Leave-one-pdb-out, où pdb se réfère à la structure native issue de la PRIDB.
Sachant que le jeu de données de référence contient 120 structures natives, nous avons effectué 120 apprentissages à partir de 119 × 10 000 candidats. Pour ne pas biaiser la phase d'apprentissage nous avons échantillonné les jeux d'apprentissage qui contiennent 30 presque natifs et 30 leurres par complexe, soit 3 570 candidats de chaque classe par jeu. Puis, chaque fonction apprise a été évaluée sur les 10 000 candidats associés à la structure native écartée pour le test.
Les résultats obtenus pour les trois fonctions de scores apprises, avec des contraintes différentes sur l'espace de recherche des poids, sont présentés sous deux angles : l'analyse "classique" des performances en ROC-AUC et l'analyse plus "biologique" des résultats en nous focalisant sur le gain de performance par rapport au cadre d'évaluation CAPRI.
Résultats
5
Nous présentons les résultats obtenus pour les quatre fonctions de score suivantes : POS, la fonction de score à poids dans [0 ; 1] ; NEG à poids dans [?1 ; 0] ; ALL dans [?1 ; 1] et ROS, la fonction par défaut de RosettaDock. La figure 1 montre les résultats obtenus pour les 4 fonctions de score évaluées. On remarque que ROS ne permet pas de trier correctement des candidats (AU C < 0, 5) alors que POS le permet. C'est la fonction de score la plus performante (AU C = 0, 80 ± 0, 02). POS présente aussi une courbe ROC de forte pente à l'origine, indiquant que l'enrichissement en presque natifs dans les premiers résultats est importante. Les autres fonctions de scores ont des AUC nettement inférieures. La connaissance de l'interprétation physico-chimique des descripteurs qui incite à ne chercher que des poids positifs fait augmenter drastiquement la performance. Avec ALL, les minimima locaux de l'intervalle [?1 ; 0] sont si importants qu'ils restreignent les solutions à ce dernier intervalle, ne permettant pas d'obtenir les meilleures solutions dans [0 ; 1]. Le score d'enrichissement défini par Tsai et al. (2003) et noté SE, représente la proportion des candidats se trouvant à la fois dans les 10 % premiers candidats en score et en RMSD. C'est un critère d'évaluation courant pour les expériences à grande échelle de prédiction de structures 3D. Sa valeur varie de de 0 à 10 avec 10 pour une fonction de score extrayant parfaitement les 10 % meilleurs candidats en RMSD, notés top10% RM SD et 1 pour une fonction de score triant aléatoirement. On considère habituellement qu'un score supérieur à 5 est preuve de performances très intéressantes dans ce domaine.
On observe des scores d'enrichissement supérieurs à 6 pour 27 structures natives avec POS, aucune avec les autres fonctions. Ils sont aussi supérieurs à 4 pour 54 structures natives avec POS et pour 4 avec les autres fonctions. Cela confirme la performance plus importante de POS. De façon similaire, lorsque la performance est dégradée, elle l'est moins avec POS qu'avec les autres. En effet, il y a 6 structures natives qui ont un score d'enrichissement inférieur à 1 avec POS, alors qu'il y en a 69 pour les autres fonctions. La comparaison des scores d'enrichissement montre qu'il n'y a pas de structure native pour laquelle POS a un score d'enrichissement inférieur de plus de 1 aux deux autres fonctions de score dédiées. Il y en a 6 pour lesquelles la fonction de score par défaut est meilleure de plus de 1. L'évaluation des fonctions de score sur le critère du score d'enrichissement corrobore les résultats obtenus en AUC et permet de parvenir à la même conclusion.
Le score d'enrichissement représente donc bien la capacité à obtenir des structures plausibles en premier. Cela est particulièrement visible sur les structures 3D. La figure 2 présente un exemple caractéristique. Les résultats obtenus grâce à POS sont de très bonne qualité et très resserrés dans l'espace par rapport aux leurres. L'épitope (zone d'interaction) est bien (mieux) caractérisé pour les deux partenaires après sélection des meilleurs candidats.
Pour avoir un critère d´évaluation proche de celui des expérimentalistes, on utilise le nombre de presque natifs du topN . C'est le nombre de presque natifs obtenus dans les N premiers candidats après tri. Pour nous placer dans l'objectif CAPRI, N est fixé à 10 candidats.
Dans 92 cas sur 120, POS prédit au moins un presque natif de plus que la valeur attendue sous l'hypothèse d'une distribution uniforme des candidats. Par contre, les fonctions apprises échouent en proposant une structure de moins dans 11 cas, contrairement aux 21   
Conclusion et perspectives
Dans cette étude, nous avons montré qu'une technique d'apprentissage classique, la régres-sion logistique, réalisée à l'aide d'un algorithme génétique pour l'apprentissage des poids, se prête bien à un problème d'optimisation de poids pour les fonctions de score pour l'amarrage. Nous avons aussi mis en évidence que la connaissance des données physico-chimiques qui impose a priori des contraintes sur l'intervalle de recherche des poids, est ici essentielle car l'intervalle de recherche ne peut être déterminé de façon automatique. Par rapport aux trois autres, la fonction de score à poids positifs permet d'obtenir de très bonnes performances. La nature chimique fine des interactions pour chaque exemple pour lequel les performances de la fonction de score sont dégradées devra encore être analysée. Il semble aussi clair, étant donnés les résultats de cette étude et au vu des précédentes études réalisées sur ce même type de données, qu'une fonction de score linéaire des paramètres, bien que conforme au modèle physico-chimique sous-jacent, ne permet pas une classification optimale. Une approche par filtres collaboratifs pourrait, par exemple, donner de bien meilleurs résultats. En se plaçant dans un contexte réaliste d'évaluation CAPRI, la comparaison entre les structures natives du nombre attendu de presque natifs dans le top10 a montré que la nature des données ne permet pas de travailler sur des jeux de données équilibrés. Utiliser le critère ROC-AUC plutôt que le top10 pour définir la fonction à optimiser a permis d'obtenir des résultats comparables entre structures natives malgré la différence d'équilibre des jeux de données entre structures natives. Pour conclure, ces premiers travaux nous permettent d'envisager d'intégrer la fonction

Introduction
Du fait de l'accroissement continu des capacités de stockage, la capture et le traitement des données ont profondément évolué durant ces dernières décennies. Il est désormais courant de traiter des données comprenant un très grand nombre de variables et les volumes considérés sont tels qu'il n'est plus forcément envisageable de pouvoir les charger intégralement : on se tourne alors vers leur traitement en ligne durant lequel on ne voit les données qu'une seule fois. Dans ce contexte, on considère le problème de classification supervisée où Y est une variable catégorielle à prédire prenant J modalités C 1 , . . . , C J et X = (X 1 , . . . , X K ) l'ensemble des K variables explicatives, numériques ou catégorielles. On s'intéresse à la famille des prédicteurs de type Bayésien naïf. L'hypothèse d'indépendance des variables explicatives conditionnellement à la variable cible rend les modèles directement calculables à partir des estimations conditionnelles univariées de chaque variable explicative. Pour une instance n, la probabilité de prédire la classe cible C conditionnellement aux valeurs des variables explicatives se calcule alors selon la formule :
On considère ici qu'une estimation des probabilités a priori P (Y = C j ) et des probabilités conditionnelles p(x k |C j ) sont disponibles. Dans notre cas, ces probabilités seront estimées dans les expérimentations par discrétisation ou groupage univarié MODL (cf. ). Ces probabilités univariées étant connues, un prédicteur Bayésien naïf pondéré est décrit entièrement par son vecteur de poids des variables W = (w 1 , w 2 , . . . , w K ). Au sein de cette famille de prédicteurs, on peut distinguer : -les prédicteurs avec des poids à valeur booléenne. En parcourant l'ensemble des combinaisons possibles de valeurs pour le vecteur de poids, on peut calculer le prédicteur MAP à savoir le prédicteur qui maximise la vraisemblance des données d'apprentissage. Cependant, quand le nombre de variables est élevé un tel parcours exhaustif devient impossible et l'on doit se résoudre à un parcours sous-optimal de l'espace {0, 1}
K . -les prédicteurs avec des poids continus dans [0, 1] K . De tels prédicteurs peuvent être obtenus par moyennage de modèles du type précédent avec une pondération proportionnelle à la probabilité a posteriori du modèle (Hoeting et al., 1999) ou à leur taux de compression . Cependant, pour des bases comprenant un très grand nombre de variables, on observe que les modèles obtenus par moyennage conservent un très grand nombre de variables ce qui rend les modèles à la fois plus coûteux à calculer et à déployer et moins interprétables. Dans cet article on s'intéresse à l'estimation directe du vecteur des poids par optimisation de la log vraisemblance régularisée dans [0, 1] K . L'attente principale est d'obtenir via cette approche des modèles robustes comprenant moins de variables, à performances équivalentes. Des travaux préliminaires (Guigourès et Boullé, 2011) ont montré l'intérêt d'une telle estimation directe des poids. La suite de l'article est organisée de la façon suivante : la régularisation parcimonieuse proposée est présentée en section 2 et la mise en place d'un algorithme de gradient en ligne, anytime et à budget limité pour l'optimisation du critère régularisé en section 3. Plusieurs expérimentations sont présentées en section 4 avant un bilan et la présentation de nos perspectives pour ces travaux.
Construction d'un critère régularisé
Étant donné un jeu de données D N = (x n , y n ) N n=1 , on cherche à minimiser sa logvraisemblance négative qui s'écrit :
Vu comme un problème classique d'optimisation, la régularisation de la log vraisemblance est opérée par l'ajout d'un terme de régularisation (ou terme d'a priori) qui exprime les contraintes que nous souhaiterions imposer au vecteur de poids W . Le critère régularisé est donc un critère de la forme :
n=1 où ll désigne la log vraisemblance, f la fonction de régularisation et ? le poids de la régulari-sation. Plusieurs objectifs ont guidé notre choix pour la fonction de régularisation : 
Ce coefficient est supposé connu en amont de l'optimisation. Si aucune connaissance n'est disponible, ce coefficient est fixé à 1. Il peut être utilisé pour intégrer des préfé-rences "métier" entre les variables. Dans notre cas, on y décrit le coût de préparation de la variable, i.e. le coût de discrétisation pour une variable numérique et le coût de groupage pour une variable catégorielle décrits respectivement équations (2.4), resp. (2.7) de . 3. Sa cohérence avec le critère régularisé du prédicteur MODL Bayésien naïf avec sé-lection binaire des variables . Pour que les deux critères coïncident pour ? = 1 et w k à valeurs binaires, on utilise finalement le terme de régularisation :
3 Algorithme d'optimisation : descente de gradient par mini-lots avec perturbation à voisinage variable
qui sont toutes des quantités constantes dans ce problème d'optimisation. Le critère régularisé à minimiser s'écrit alors :
On se donne la contrainte que w soit à valeurs dans 
Calcul de ? t+1 ; Calcul de la valeur du critère sur l'historique de taille N : CR D t,N (w t+1 ) ; if amélioration du critère then mémorisation de la meilleure valeur : w * = w t+1 ; else incrémentation du compteur des dégradations successives ; end end Algorithme 1 : Algorithme de descente de gradient projeté par mini-lots (DGML) dérivée partielle :
Le gradient ?CR D N (w t ) est le vecteur de ces dérivées partielles pour ? = 1, . . . , K. On s'est intéressé à sa minimisation par un algorithme de type descente de gradient projeté (Bertsekas, 1976) c'est à dire un algorithme de type descente de gradient pour lequel, à chaque itération, le vecteur w obtenu est projeté sur [0, 1] K . Plusieurs objectifs ont guidé notre choix d'algorithme :
1. algorithme en ligne : que la structure de l'algorithme soit adaptée au traitement d'un flux de données et qu'il ne nécessite donc pas le traitement de la base dans son intégralité ; 2. algorithme anytime : que l'algorithme soit interruptible et qu'il soit en mesure de retourner la meilleure optimisation étant donné un temps de calcul budgété au préalable. Dans l'algorithme de gradient projeté de type batch, on procède itérativement en mettant à jour le vecteur de poids à chaque itération t selon le gradient calculé sur toutes les instances pondéré par un pas. Si on note w t le vecteur de poids obtenu à l'itération t, la mise à jour à l'itération t + 1 s'effectue selon l'équation :  . Cette approche batch suppose que l'on dispose de l'intégralité de la base pour être en mesure de débuter l'optimisation. Dans sa variante stochastique, la mise à jour se fait en intégrant le gradient calculé sur une seule instance. La descente de gradient peut alors se révéler chaotique si la variance du gradient d'une instance à l'autre est élevée. Souhaitant adopter une approche en ligne nous avons retenu une variante à la croisée de ces deux approches (batch et stochastique) à savoir l'approche par mini-lots (Dekel et al., 2012) qui consiste à orienter la descente dans le sens des gradients calculés sur des paquets successifs de données de taille que nous noterons L. Afin que les chemins de descente soient comparables lorsque la taille des lots varie, le gradient utilisé est rapporté à la taille L des mini-lots. L'algorithme de descente de gradient par mini-lots adopté est résumé dans l'algorithme 1.
La valeur optimale du pas ? t a fait l'objet de nombreuses recherches conduisant à des algorithmes plus ou moins coûteux en temps de calcul. Nous avons opté pour la méthode Rprop (Riedmiller et Braun, 1993) : le calcul du pas est spécifique à chaque composante du vecteur c'est à dire que ? est un vecteur de pas de dimension K et que chaque composante de ce vecteur est multiplié par un facteur plus grand, resp. plus petit que 1, si la dérivée partielle change, resp. ne change pas, de signe d'une itération à l'autre. En terme de complexité algorithmique, chaque itération nécessite l'évaluation du critère sur un échantillon de taille N soit une complexité en O(K * N ). Pour L = N , on retrouve l'algorithme batch classique et pour L = 1, le gradient stochastique.
Etant donné la non-convexité du critère à optimiser, il présente en général de nombreux minima locaux vers lesquels une telle descente de gradient peut converger. Il est alors courant de lancer plusieurs descentes de gradient avec des initialisations aléatoires distinctes (approche multi-start) en espérant que l'un de ces chemins de descente conduise au minimum global du critère. Afin de rendre l'optimisation efficace et de ne pas perdre de temps de calcul au début de chacune de ces descentes, il est également possible de perturber la solution obtenue au bout d'un certain nombre d'itérations afin de "sortir" de l'éventuelle cuvette contenant un minimum local. En rendant variable la taille du voisinage dans lequel on perturbe la solution, on se donne les moyens de sortir des minima locaux (approche Variable Neighborhood Search (Hansen et Mladenovic, 2001)). Cette approche notée DGML-VNS est décrite dans l'algorithme 2. On peut remarquer que, pour un voisinage recouvrant intégralement [0, 1] K , l'algorithme DGML-VNS revient à une structure multi-start avec initialisation aléatoire. D'autre part, précisons que le tirage aléatoire peut conduire à une valeur non nulle pour un poids mis à zéro à l'issue du start précédent. Une variable peut donc ré-apparaître en cours de lecture du flux.
L'algorithme DGML-VNS est anytime dans le sens où une estimation du minimum du critère est disponible à la fin de la première descente de gradient et qu'elle est ensuite améliorée en fonction du budget disponible en temps de calcul mais interruptible à tout moment. Sa complexité totale est en O(T * K * N ) où T est le nombre total d'itérations autorisées. Ce paramétrage par T permet de limiter le budget maximal utilisé par l'algorithme.
Expérimentations
Les premières expérimentations ont pour objectif d'évaluer la qualité de l'optimisation obtenue avec l'algorithme DGML-VNS en fonction de la taille L des mini-lots présentés et du nombre total d'itérations autorisées T . Pour étudier la qualité intrinsèque de l'optimisation indépendamment des performances statistiques du prédicteur associé, nous avons pour cela fixé le poids ? de la régularisation à 0 ce qui revient à optimiser directement la vraisemblance non régularisé. La seconde partie des expérimentations traite des performances statistiques du classifieur obtenu par optimisation du critère régularisé (? = 0). Pour l'ensemble des expérimentations les paramètres suivants de l'algorithme DGML sont fixés aux valeurs suivantes :
avec une multiplication par 0.5, resp. 1.2, en cas de changement, resp. non changement, de signe entre deux gradient successifs -Max = 100 le nombre maximal d'itérations (i.e. le nombre de mini-lots présentés). On a vérifié que ce nombre n'avait jamais été atteint pour les 36 bases testées. -Tol = 5 le nombre de dégradations successives autorisées. On considère qu'il y a amélioration du critère pour une amélioration d'au moins = 10  Toutes les expérimentations ont été menées en 10-fold-cross-validation sur les 36 bases de l'UCI décrites dans le tableau 1. Dans la présentation des résultats 'SNB' désigne la performance d'un classifieur de Bayes moyenné à l'aide du taux de compression . 
Expérimentations sur la qualité de l'optimisation
FIG. 1 -Taux de compression moyen en Train et en Test pour 36 bases de l'UCI
Tout d'abord, nous avons étudié les performances de l'algorithme DGML, c'est à dire de l'algorithme de descente de gradient projeté sans post-optimisation, en fonction de la taille des mini-lots L. Nous avons choisi comme indicateur de la qualité de l'optimisation le taux q q q q q q q q q q q q q q q q q 1600 q 1200 q Critère q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q L=batch L=1000 L=100 0 20 40 60 80 400 600 800 Itération FIG. 2 -Chemins de convergence du critère selon la taille des mini-lots (DGML) au cours des itérations pour la base Phoneme de compression qui mesure le logarithme négatif de la vraisemblance du modèle, normalisé par l'entropie de Shannon. Plus ce taux est proche de 1, plus la vraisemblance du modèle est élevée. Pour des modèles moins performants que le modèle aléatoire, le taux de compression est négatif. La valeur du taux sur les données d'apprentissage est donc un bon indicateur de la qualité de l'optimisation obtenue étant donné que le critère non régularisé est réduit à la log vraisemblance négative. La figure 1 présente le taux de compression obtenu en Train et en Test et moyenné sur les 36 bases de l'UCI pour différentes tailles de mini-lots L = 100, 1000, N . Dans le dernier cas, le choix L = N revient à un algorithme de type batch. Les taux de compression obtenus en Train et en Test pour le SNB MODL  servent ici de référence. Les résultats obtenus indiquent que, plus la taille des mini-lots est petite, plus la qualité de l'optimisation se dégrade. D'autre part, les résultats obtenus pour L = 1000 et L = N sont très proches. Le taux de compression en Train est significativement meilleur en batch que pour L = 1000 pour 8 des 36 bases. La figure 2 présente à titre illustratif la série des valeurs prises par le critère au cours de l'optimisation selon la valeur de L = 100, 1000, N pour la base Phoneme. Sur l'ensemble des 36 bases la convergence est plus rapide mais plus chaotique lorsque la taille des mini-lots diminue.
Nous avons ensuite comparé la qualité de l'optimisation pour l'algorithme DGML sans post-optimisation d'une part et pour un algorithme DGML post-optimisé d'autre part. Plusieurs types de post-optimisations ont été testées : par multi-start (DGML-MS) ou par perturbation aléatoire à voisinage variable (DGML-VNS). Pour avoir une complexité du même ordre de grandeur que celle de l'algorithme de pré-traitement univarié MODL, à savoir O(K * N * log (K * N )), on a fixé le nombre total d'itérations autorisées T proportionnel à log (K * N ). Plus précisément, on a choisi T = log (K * N ) * 2 PostOptiLevel où PostOptiLevel est un entier qui permet de régler le niveau de post-optimisation souhaité. Pour chacun de ces deux types de post-optimisation, on a étudié l'influence de la valeur du niveau d'optimisation OptiLevel = 3, 4, 5. Dans la mesure où l'algorithme post-optimisé mémorise au fur et à mesure la meilleure solution rencontrée, la post-optimisation ne peut qu'améliorer le taux de compression en Train. On a donc dans un premier temps mesuré si cette amélioration était significative ou pas. Pour une post-optimisation de type MS, le taux de compression en Train est amélioré de manière significative pour 7, 16, 18 des 36 bases avec un niveau d'optimisation respectivement de 3, 4, 5. Pour une post-optimisation de type VNS, le taux de compression en Train est amélioré de manière significative pour 18, 19, 23 des 36 bases avec un niveau d'optimisation respectivement de 3, 4, 5. La post-optimisation de type VNS semble donc préférable à la post-optimisation MS : l'exploration guidée par un voisinage de taille variable à partir du meilleur minima rencontré permet une recherche plus fructueuse des autres minimas qu'une exploration purement aléatoire. La figure 3 permet d'illustrer ce phénomène de "gaspillage" d'itérations en mode MS au début q q q q q q q q q q q q q q q q q q q q q q q q q q q q q MS VNS q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q de chaque nouveau lancement pour la base. Les expérimentations présentées dans cette section ont mis en évidence : l'effet de la taille des mini-lots sur la qualité de l'optimisation. Plus cette taille est élevée meilleure est la qualité de l'optimisation ; l'intérêt de la post-optimisation VNS comparé notamment à une postoptimisation MS. Nous retenons donc pour la suite des expérimentations un algorithme de type DGML-VNS avec une taille de mini-lots fixée à L = 1000 et un niveau d'optimisation PostOptiLevel = 5.
Performances du classifieur régularisé
Nous présentons les performances du classifieur en fonction du paramétrage pour le poids de la régularisation ? et l'exposant p de la fonction |w k | p . Trois valeurs ont été testées pour ? = 0.01, 0.1, 0.5 et pour p = 0.5, 1, 2. Les performances en terme d'AUC des neuf prédicteurs régularisés associés à ces valeurs sont présentées figure 4 avec pour référence les performances du prédicteur non régularisé obtenu pour ? = 0 et du prédicteur SNB.
Pour la valeur la plus élevée du poids de régularisation, à savoir ? = 0.5 (en violet sur la figure), les performances en terme d'AUC sont dégradées par rapport aux performances obtenues sans régularisation (en rond rouge sur la figure) quelle que soit la valeur de p. En re- D'autre part, pour étudier la parcimonie des prédicteurs obtenus, la figure 5 présente le nombre de variables retenues et la somme de leur poids. On voit tout d'abord que plus p est faible plus le nombre de poids non nuls est faible également. La régularisation quadratique est celle qui conduit aux prédicteurs les moins parcimonieux. Parmi la régularisation en valeur absolue (p = 1) et celle en racine carrée (p = 0.5), c'est la seconde qui permet une réduction la plus importante du nombre de variables retenues. Concernant la somme des poids des variables retenues, tous les types de régularisation permettent de réduire en moyenne la somme des poids. D'autre part, à poids ? donné, la régularisation quadratique a un impact moins important sur la réduction de la somme des poids que les deux autres régularisations dont les performances sont très proches pour cet indicateur. En prenant en compte ces deux aspects de performance statistique et de parcimonie, le compromis p = 1 et ? = 0.1 semble le plus favorable. Sans dégrader les performances du prédicteur non régularisé, il permet de réduire de façon importante le nombre de variables sélectionnées ce qui rend le prédicteur plus interprétable et moins complexe à déployer.
Conclusion
Nous avons proposé une régularisation parcimonieuse de la log vraisemblance d'un classifieur Bayésien naïf pondéré. Nous avons décrit et expérimenté un algorithme de descente de gradient intégrant en ligne des mini-lots de données et optimisant les poids de ce classifieur par exploration plus ou moins poussée de la solution optimale courante en fonction d'un budget donné. Les expérimentations menées ont permis de montrer l'intérêt de l'introduction de ces 

Introduction et état de l'art
Une structure de dépendances d'une phrase traduit une hiérarchie syntaxique des mots, et permet d'en inférer une sémantique. Les applications liées aux structures de dépendance sont multiples, on peut citer entre autres la modélisation de langages, la reconnaissance d'implications textuelles, les moteurs de question/réponse, l'extraction d'information, l'induction d'ontologies lexicales et la traduction automatique.
Une structure de dépendances d'une phrase (cf. Fig. 1 à gauche) est un arbre dont les noeuds sont les mots, ou tokens, de la phrase. Un des mots est désigné comme la racine de l'arbre (en général un verbe), à laquelle sont attachés des sous-arbres couvrant des portions contigües de la phrase. Un arbre de dépendances est constitué de relations orientées entre un mot syntaxiquement plus fort (tête) et un mot plus faible (dépendant). Le modèle de dépendances est un compromis intéressant entre l'analyse syntaxique classique complète et une représentation "sac de mots".
Les modèles d'apprentissage supervisé de dépendances exigent un nombre important d'exemples annotés à la main. Ce travail, très long et fastidieux, demande une expertise de linguiste essentielle et doit être remanié profondément à chaque nouveau type de texte analysé. La quantité de textes annotés en dépendances est faible comparée à la variété des types de textes disponibles sur le Web. Nous proposons une approche non supervisée ne demandant qu'une connaissance très superficielle de la langue et du type de texte. Nous nous plaçons dans le cadre de l'Apprentissage Non Supervisé de Dépendances (ANSD). Le corpus journalistique américain Penn Treebank propose une version "dépendances" du corpus, donnant les structures de dépendances des phrases. Klein et Manning (2004) furent les premiers à obtenir par ANSD, sur les phrases de moins de 10 mots de ce corpus, des résultats meilleurs que le simple attachement de chaque mot à son voisin de droite, ou que des arbres aléatoires. Ils ont nommé leur modèle Dependency Model of Valence (DMV). L'entrée de cet algorithme est constituée des étiquettes grammaticales des mots des phrases. C'est un modèle génératif dans lequel la racine de la phrase est générée et ensuite, chaque nouvelle tête génère récursivement ses dépendants gauches et droits. La catégorie du dépendant est déterminée en fonction de la tête et de la direction (gauche ou droite). L'apprentissage des probabilités du modèle, dont celles des types de dépendance favorisées, est effectué par une procédure classique de maximisation de l'espérance (EM), basée sur des probabilités a priori calibrées à la main suivant des critères linguistiques.
Ce modèle est riche et intéressant, mais l'initialisation des paramètres du modèle est un problème complexe et essentiel. Elle demande à la fois de l'innovation technique de la part d'un expert en apprentissage et des connaissances linguistiques poussées de la part d'un expert dans la construction syntaxique de la langue étudiée.
Apprentissage de grammaire hors contexte
Nous proposons une démarche incluant un apprentissage de grammaire hors contexte probabiliste (PCFG) par l'algorithme Inside-Outside (Lari et Young, 1990), puis une analyse des phrases par une version probabiliste de CYK (Jurafsky et Martin, 2009)  Inside-Outside est un algorithme génératif, que l'on peut considérer comme une extension des modèles de Markov cachés (HMM) permettant d'apprendre des grammaires hors contexte probabilistes. Alors que les HMM apprennent les probabilités des règles de dérivation par des calculs sur les sous-séquences précédant et suivant une position t, Inside-Outside se base sur des calculs de sous-séquences à l'intérieur et à l'extérieur de deux positions t 1 et t 2 .
CYK probabiliste est un algorithme d'analyse qui choisit parmi toutes les analyses autorisées par les règles de la grammaire, celle qui est la plus probable.
Le formalisme. L'originalité de notre méthode réside dans le choix d'une grammaire simple permettant d'exprimer des dépendances entre les mots d'une phrase. Par exemple, dans la phrase "il a obtenu ce qu'il souhaitait", "obtenu" est un dominant dont dépendent à gauche "il" et "a" et à droite "ce", dominant lui-même "qu'il souhaitait". L'arbre de dépendances de référence est représenté en Figure 1 (gauche). Notre modèle associe à chaque mot (représenté par son étiquette grammaticale) sa qualité de dominant ou dominé par rapport à ses voisins. Pour analyser une phrase, le modèle combine ensuite, par un jeu de symboles, les groupes de mots jusqu'à ce que chaque mot trouve sa place dans l'arbre de dépendances (cf. -Un non terminal en majuscule va dominer le non terminal en minuscule auquel il est associé dans une règle de dérivation. Par exemple G ? G d exprime qu'un dominant gauche peut se décomposer en un dominant gauche et un dominé droit. -Un non terminal gauche g (resp. G) est associé par la gauche à D (resp. d). nt ? G d ou nt ? g D. Le sens que l'on donne aux non terminaux interdit de nombreuses règles, et permet de limiter la taille de la grammaire tout en gardant une signification de grammaire de dépendances latéralisée (la position relative, gauche ou droite, des mots importe). Les règles de Chomsky du premier type (nt ? nt nt) sont des règles exprimant la construction interne des phrases, nous parlons de règles de structure. Les règles de Chomsky de second type (nt ? terminal) sont celles par lesquelles on transmet l'information qu'une catégorie peut (ou non) dominer ses voisins de gauche ou de droite. Par exemple, nous interdirons systématiquement pour le français les règles nt ? DET pour tout nt = g car un déterminant est toujours dominé par un nom dont il placé à la gauche.
Les variantes. En fonction de la structure de la langue considérée, les règles de structure de la phrase peuvent ne pas correspondre à la forme intrinsèque des phrases. La grammaire présentée est nommée 4bin car elle contient, en plus du symbole de départ, 4 non terminaux (D, G, d et g) et les règles de structure de la phrase sont écrites de façon binaire, suivant la forme normale de Chomsky.
La signification de ces quatre non terminaux témoigne d'une différenciation essentielle entre les rôles des catégories grammaticales qui domineraient à gauche ou à droite. Pour le français, dans de nombreux cas cette différenciation est pertinente. On peut cependant rencontrer des situations où cette différenciation n'est pas pertinente. Par exemple, deux adjectifs qualifiant le même nom, l'un à sa gauche, l'autre à sa droite, sont dominés par le nom. Nous avons donc envisagé une version 3bin ne comportant que trois non terminaux (en plus de S). Nous avons conservé dans cette variante la latéralisation des dominés g et d, mais n'avons gardé qu'un non terminal dominant N non latéralisé. 
TAB. 1 -Différentes variantes de la grammaire formelle DGdg (MAJ désigne les non terminaux majuscules (G ou D)).
Le choix de ces deux formes (4bin et 3bin) implique une vision "binaire" des découpages de la phrase en groupes de mots, imposée par la forme normale de Chomsky. Nous pouvons cependant nous affranchir de cette contrainte par la traduction des règles ternaires en règles binaires et envisager ainsi une structure ternaire (4ter) suggérée par les phrases du type : sujet (à gauche), verbe (au centre) et complément (à droite).
Dans la continuité de cette idée, en conservant les rôles latéralisés des dominants D et G, mais en permettant aussi une domination centrale non latéralisée, nous avons introduit un nouveau symbole N (pour neutre) dans les variantes 5ter et 5ter+ qui se distinguent par le fait que l'on interdit (5ter) ou que l'on autorise (5ter+) l'utilisation récursive du symbole N , conduisant à des structures plus complexes dans le dernier cas. Le Tableau 1 résume les différentes variantes proposées.
Le calibrage. Tous les modèles d'ANSD sont calibrés en fonction de la langue du corpus. Ce calibrage consiste à ne sélectionner que les règles du type nt ? terminal qui ont linguistiquement un sens. Par exemple, en français, un déterminant dépendra toujours d'un nom situé à sa droite ; c'est pourquoi parmi les règles nt ? DET seule la règle g ? DET sera conservée. Dans les expérimentations à suivre, le calibrage a été réalisé en observant pour chaque langue quelques arbres donnés comme référence dans les treebanks de dépendances.
Expérimentations et résultats
Le French Treebank (Abeillé et Barrier, 2004) donne la structure en constituants (groupes nominaux, verbaux. . .) ainsi que les fonctions syntaxiques (sujet, objet. . .) de nombreuses phrases issues d'articles du journal Le Monde. Depuis 2009, ce treebank a été converti en arbres de dépendances (Candito et al., 2010). Nous avons comparé les arbres appris par notre modèle à ceux donnés en référence dans le treebank de Candito et al. (2010). Pour la comparaison, nous avons utilisé le score UAS (Unlabeled Attachment Score) qui calcule, pour un ensemble de phrases, le nombre de dépendances correctes (sans les ponctuations).
Les scores sont différents en fonction des variantes de la grammaire DGdg. Nous obtenons pour 3bin : 29.8%, pour 4bin : 32.9%, pour 4ter : 42.1%, pour 5ter : 37.4% et 5ter+ : 42.2 % ; à titre de comparaison nous obtenons un score de 14.2% pour des arbres générés aléatoirement. On observe que les deux variantes (4ter et 5ter+), qui autorisent des règles ternaires récursives, avec un groupe de mots central dominant et deux groupes latéraux dominés, engendrent des scores pratiquement identiques, nettement supérieurs à ceux obtenus pour les autres variantes. Cela suggèrerait que la structure sous-jacente de ces phrases journalistiques, assez sophistiquées, est mieux capturée par un modèle plus complexe. À notre connaissance, nous sommes les premiers à traiter cette tâche d'ANSD pour le français.
Celle-ci ayant été largement abordée pour l'anglais, puis dans d'autres langues à partir de la conférence CONLL 2006 (Buchholz et Marsi, 2006), nous avons confronté notre modèle à la référence DMV. Le Tableau 2 résume les résultats obtenus, ainsi que la variante ayant engendré le meilleur score. Les résultats pouvent différer beaucoup d'une variante à une autre, celle-ci doit être judicieusement choisie en fonction de la langue et du type de texte. TAB. 2 -Les meilleurs scores UAS obtenus comparés aux références soft-EM données dans Spitkovsky et al. (2011). Les treebank de dépendances des différentes langues proviennent : pour l'Anglais : Marcus et al. (1993), pour le français : Candito et al. (2010); Abeillé et Barrier (2004), les autres langues étant celles de CONLL 2006, Buchholz et Marsi (2006.
Discussion et conclusion
Les temps d'apprentissage dépendent fortement du volume des données et faiblement du nombre de catégories. Le petit nombre de règles de structures de la grammaire permet cependant un apprentissage raisonnable en temps, voire très rapide sur de petits corpus. Une fois la grammaire apprise, l'analyse est quasiment instantanée (quelques secondes pour des milliers de phrases). Ceci argue de la souplesse de notre modèle et de la rapidité de sa mise en oeuvre. Celui-ci est donc portable et performant, relativement à DMV. D'autres tests ont révélé que les résultats peuvent encore être améliorés en considérant des catégories grammaticales plus fines (morpho-syntaxe). Le temps d'apprentissage s'en ressent nécessairement.
Pour améliorer encore notre modèle, nous envisageons d'y intégrer des informations lexicales pour que deux séquences de catégories identiques puissent, en fonction du vocabulaire, être interprétées en arbres de dépendances différents.

Introduction
Selon Klein (2001), la fusion d'ontologies est "la création d'une nouvelle ontologie à partir de deux ou plusieurs ontologies existantes avec des parties qui se chevauchent". La création de la nouvelle ontologie est généralement une tâche difficile et requiert un cadre formel capable de contrôler les différentes étapes de construction. Cet article propose ainsi de lever ce verrou scientifique par l'utilisation des grammaires de graphes typés (T GG) basées sur les approches algébriques. Les T GG sont un formalisme mathématique de représentation et de manipulation des graphes. Elles sont composées d'un graphe type (T G), un graphe hôte (G) et d'un ensemble de règles de production (P ) appelées aussi règles de réécriture. Ces règles sont définies par une paire de graphes : 1) LHS (Left Hand Side) représente les pré-conditions de la règle ; 2) RHS (Right Hand Side) représente les post-conditions et doit remplacer LHS dans G. Les règles peuvent également avoir des conditions supplémentaires appelées N AC (Negative Application Conditions). La transformation de graphe consiste ainsi à définir comment un graphe G peut être transformé en un nouveau graphe G . Pour cela, il doit exister un morphisme qui remplace LHS par RHS pour obtenir G . Différentes approches ont été proposées pour appliquer ce remplacement. Dans ce travail, nous utilisons l'approche algébrique Simple pushout SPO (Löwe, 1993). Ainsi, appliquer une règle de réécriture à un graphe G, selon la méthode SPO, revient à : 1) trouver le LHS dans G ; 2) supprimer de
Dans notre travail (Mahfoudh et al., 2013), nous avons utilisé les T GG pour la formalisation et l'implémentation des changements ontologiques. Elles nous ont permis, grâce à leurs 
Les ontologies et les grammaires de graphes typés
En adoptant le formalisme de grammaire de graphes typés sur les ontologies, on obtient : -G est le graphe hôte qui représente une ontologie (voir l'exemple de la Figure 1). -T G est le graphe type qui représente le méta-modèle de l'ontologie. Dans le cadre de cet article, c'est le méta-modèle d'OWL qui a été retenu. Ainsi, les types des noeuds sont les classes (C), individus (I), propriétés (P = DP ? OP avec DP sont les datatype properties et OP sont les object properties). Les types des arêtes sont les axiomes (A). Les restrictions (R) nécessitent à la fois les noeuds et les arêtes. -P sont les règles de réécriture correspondantes aux changements ontologiques (ex.
AddClass, RemoveP roperty). Un changement ontologique CH = (N ame, N ACs, LHS, RHS, CHDs) avec : 1) N ame précise le nom du changement ; 2) N ACs défi-nissent les conditions qui ne doivent pas être satisfaites pour pouvoir appliquer le changement ontologique ; 3) LHS représente les pré-conditions du changement ; 4) RHS définit les post-conditions du changement ; 5) CHDs sont les changements dérivés, c'est à dire les règles de réécriture additionnelles qui sont attachées au CH pour corriger ses éventuelles inconsistances. Plus de détails et exemples des changements ontologiques sont présentés dans (Mahfoudh et al., 2013). Nous décrivons dans ce qui suit notre approche de fusion des ontologies. A noter que l'approche a été implémentée en se basant sur l'API AGG (Algebraic Graph Grammar).
Recherche de similarité
Pour identifier la similarité entre les ontologies, nous nous sommes basés sur la distance de Levenshtein pour détecter les correspondances syntaxiques et l'ontologie linguistique WordNet pour identifier les correspondances sémantiques. Le processus de recherche de similarité prend ainsi deux ontologies et génère l'ensemble de correspondances suivantes : 1) CN est l'ensemble des noeuds communs ; 2) EN est l'ensemble des noeuds équivalents ; 3) SN est l'ensemble des noeuds partageant une relation sémantique. Les relations de subsomption (IsaN ) sont identifiées manuellement. En considérant l'exemple des ontologies O 1 et O 2 , on aura : 1) CN = {"Automobile", "F iat, "BM W "} ; 2) EN = {("has_owner", "hasOwner")} ; 3) SN = {("P erson", "Individual")}. 4) IsaN = {("German_Car", "European_Car"), ("Italian_Car", "European_Car"), ("M ercedes", "German_Car")}.
Fusion des ontologies
Le processus de fusion des ontologies avec l'approche SPO passe par trois étapes. La première vise à minimiser la différence entre les deux ontologies. Elle remplace ainsi les entités de l'ontologie O 1 par leurs équivalents de O 2 et ceci en appliquant la règle de réécriture RenameN ode (N i , N j ) avec N i est un noeud de O 1 et N j est un noeud de O 2 . On aura alors pour ce SPO : 1) le graphe hôte est l'ontologie O 1 ; 2) le LHS est le graphe constitué de l'ensemble des noeuds {N i ? EN } ; 3) le RHS est le graphe formé par {N j ? EN }. L'étape 2 consiste à créer l'ontologie (CO) qui est le sous-graphe commun entre les deux ontologies. Elle est composée par les noeuds communs (CN ) et les arêtes qu'ils partagent. La dernière étape fusionne les ontologies avec la règle de réécriture M ergeGraph(CO, O 2 ). Cette règle a comme graphe hôte l'ontologie O des axiomes est contrôlé par les N ACs pour ne pas altérer la consistance de l'ontologie. Ainsi la règle AddSubClass (C 1 , C 2 ) est définie par :
-NACs : 1) C 1 C 2 , condition pour éviter la redondance ; 2) C 2 C 1 , la relation de subsomption ne peut pas être symétrique ; 3) C 1 ¬C 2 , les classes qui partagent une relation de subsomption ne peuvent pas être disjointes ; 4)
, s'il existe dans l'ontologie une classe C i qui est à la fois la subClass de la classe C 2 et la superClass de C 1 , alors, C 1 est déjà la subClass de C 2 et ce lien de subsomption ne doit pas être rajouté ; 5)
, les classes qui partagent une relation de subsomption ne peuvent pas avoir des subClasses disjoints. -LHS : {C 1 , C 2 }, les classes doivent exister dans l'ontologie.
-RHS : (C 1 C 2 ), l'axiome doit être ajouté à l'ontologie.
-CHD : ?.
Conclusion et futurs travaux
Nous avons présenté dans cet article une approche formelle de fusion d'ontologies basée sur les grammaires de graphes typés. L'approche proposée a été implémentée avec l'outil AGG qui permet de réaliser les SPOs requis pour la fusion. Comme perspectives de ce travail, nous envisageons l'étude de différents conflits qui peuvent affecter le résultat de fusion et la manière de les résoudre.

Introduction
Ces dernières années, plusieurs travaux se sont intéressés à la fouille de graphes pour modéliser des phénomènes réels. Récemment, on trouve des travaux sur les graphes dynamiques (Pei et al. (2005); Borgwardt et al. (2006); Bilgin et Yener (2006); Robardet (2009) ;Rossi et al. (2013)) qui ont permis la modélisation de l'évolution d'objets au cours du temps ainsi que leurs relations. Dans ce type de méthodes, on analyse essentiellement les évolutions structurelles. Par exemple dans (Robardet (2009)), les auteurs étudient l'évolution de sous-graphes en considérant des opérations tels que le découpage, le regroupement, la suppression ou la création de quasi-cliques. De plus en plus de travaux de fouille de graphes s'orientent vers les graphes attribués (e.g., Moser et al. (2009) ;Silva et al. (2012); Mougel et al. (2012a)) qui sont des graphes dynamiques dont les sommets sont décrits par des attributs. Ces derniers ont permis l'étude de plusieurs domaines d'applications notamment les réseaux biologiques (Fukuzaki et al. (2010); Mougel et al. (2012b)). Mais à notre connaissance peu de travaux traitent la fouille de graphe dynamique dont le nombre de sommets évolue dans le temps.
Dans cet article, nous proposons une approche permettant d'étudier les graphes attribués dynamiques et dont le nombre de sommets varient dans le temps. Dans ce type d'approche on considère une séquence temporelle de graphes attribués dont le nombre de sommets peut varier dans le temps. Un domaine d'application pour lequel une telle approche est particulièrement intéressante est l'analyse d'objets dans une séquence d'images satellites à différentes résolu-tions. En effet, il est très difficile de constituer une longue séquence temporelle d'images à la même résolution notamment des images à très haute résolution qui sont coûteuses en acquisition.
Pour traiter ce problème, nous proposons un nouveau modèle de données basé sur une séquence de graphes attribuées 1 nommé Graph Attribué Multi-résolution (GAM). Un GAM est une séquence de graphes tels que les sommets entre deux temps consécutifs peuvent être reliés par des arêtes temporelles, les arêtes reliant des sommets à un temps donnée sont appelées arêtes structurelles. A partir de ce modèle, nous proposons de rechercher des motifs nommés Graphes Multi-résolutions Homogènes (GMH). Un GMH est formé par une collection de sous-graphes connexes et homogènes nommés Graphe Connexe Homogène (GCH). La propriété d'homogénéité assure que le graphe est formé par des sommets partageant des propriétés similaires. La contrainte structurelle de connexité permet de trouver des graphes sans forme définie a priori. Dans le cadre de l'analyse d'images, ces deux conditions sont basées sur les hypothèses suivantes : (1) un GCH représente un segment de l'image correspondant à un objet réel, (2) un objet segmenté dans une image est généralement décrit par des pixels ayant des propriétés similaires (e.g., rouge, vert, bleu) et (3) la forme des objets segmentés n'est pas connue à l'avance mais reste connexe.
2 Contexte et définition des motifs
La fonction AtbV associe un ensemble de valeurs d'attributs à chaque sommet.
Le nombre de pas de temps dans G (i.e., |V|) est noté T G . L'ensemble de tous les sommets de G est noté V G , i.e., V G = ? i V i . Le domaine de tous les attributs de la collection A t ? A est noté D t , i.e., D t = ? A?At Dom(A). Le sous-graphe de G induit par l'ensemble de sommets
Nous définissons une fonction Hmg associant l'ensemble des valeurs d'attributs partagés par un ensemble de sommets et la fonction inverse V ert. Etant donné un ensemble de sommets V et un ensemble de valeurs d'attributs A,
Nous proposons une nouvelle classe de motifs nommée Graphes Homogènes Multi résolu-tions (GMH). Un GMH est formé par une collection de sous-graphes connexes et homogènes nommés Graphe Connexe Homogène (GCH).
seuils définis par l'utilisateur. Un ensemble de sommets H tel que H ? V t ? V est un Graphe Connexe Homogène (GCH) si et seulement si (1) tous les sommets de H partagent au moins h · |A t | valeurs d'attributs en commun avec les autres sommets de H, i.e., |Hmg(H)| >= h·|A t | ; (3) il existe un chemin passant par des arêtes structurelles entre chaque paires de sommets de H ; (2) la collection contient au moins |H| / |V t | ? s sommets ; et (4) il n'existe pas de sommets v ? V G n'appartenant pas à H tel que H ? {v} vérifie les conditions précédentes.
La première condition assure l'homogénéité de l'ensemble des sommets par rapport aux attributs. La condition (2) s'intéresse à la structure du graphe. Dans le contexte de l'analyse d'images, la contrainte de connexité permet de trouver des régions de forme arbitraire. La condition (3) permet de filtrer les petits graphes qui peuvent ne pas être intéressant pour les experts. Enfin, la condition (4) assure la maximalité des GCH par rapport à l'inclusion.
Un GCH décrit une partie d'un GAM pour un temps donné. Afin d'étudier l'évolution des GCH nous proposons de grouper les GCH connectés entre des pas de temps consécutifs.
|?(H1)| ? ? , avec ?(H) l'ensemble des sommets connectés par des arêtes temporelles par des sommets de H. I.e.,
. . , H n est une chaîne connectée temporellement si et seulement si ?i ? {1, . . . , n ? 1}, H i est connecté temporellement à H i+1 .
Définition 2.4 (Graphe Multi-résolution Homogènes) Une collection P de GCH est un Graphe Multi-résolution Homogène (GMH) ssi. (1) pour chaque paire H 1 , H 2 ? P , il existe un GCH H ? P et deux chaînes connectées temporellement S 1 et S 2 formées uniquement par des GCH de P tel que (2) pour tout GCH H, si H n'appartient pas à P alors il n'existe pas de GCH de P connecté temporellement à H.
La condition (1) assure que chaque paire de GCH d'un GMH est connectée de manière transitive par la relation de connectivité temporelle. La condition (2) assure la maximalité du motif dans le sens qu'il n'existe pas de GCH n'appartenant pas au motif et pouvant lui être ajouté sans violer la condition (1). D'après cette propriété pour calculer la collection des GCH à un temps t un algorithme naîf peut énumérer tous les ensembles non vides de valeurs d'attributs X ? D t . Si |X| / |D t | ? h alors la collection des composantes connexes dans le graphe G[V ert(X)] satisfaisant la condition de taille minimale est formée uniquement par des GCH. Cependant cette approche nécessite l'énumération de 2 |Dt| ? 1 ensembles de valeurs d'attributs pour chaque temps. Les propriétés suivantes permettent de réduire l'espace de recherche.
Méthode d'extraction
Propriété 3.1 Soit G un GAM et V ? V t ? V un ensemble de sommets. Uniquement les sommets de V appartenant à une composante connexe de G[V ] ayant au moins s×|V t | sommets peuvent former un GCH. Description des algorithmes. L'algorithme principal se déroule en deux parties. La première partie calcule la collection des GCH notée H t pour chaque temps t ? {1, . . . , T G } en appelant l'algorithme 1. La deuxième partie de cet algorithme réalise l'extension temporelle. Pour chaque temps t ? {1, . . . , T G ? 1}, les GCH de H t qui n'ont pas été précédemment traités sont utilisés pour construire un nouveau GMH noté P . Si un GCH H n'a pas été précédemment énuméré, la fonction ?(H) renvoie ?, sinon le motif auquel il appartient. Enfin, P est étendu en utilisant les GCH au temps consécutif en appelant l'algorithme 2 .
L'algorithme 1 calcule la collection des GCH pour un temps donné. Le test effectué à la ligne 1 vérifie si l'ensemble de sommets actuellement énuméré est homogène. Si c'est le cas, la collection de GCH est mise à jour aux lignes 2 ou 3. Le test de la ligne 2 permet de traiter le cas particulier où tous les sommets à un temps donné sont homogènes. Lors des appels récusrifs, l'ensemble de sommets V est nécessairement connexe donc la collection peut directement être mise à jour (ligne 3). D'après la condition de maximalité, si l'ensemble des sommets est homogène, l'énumération de la branche peut s'arréter, sinon l'énumération continue aux lignes 5 à 10. Le filtrage effectué à la ligne 5 retire de l'ensemble des valeurs attributs énumérées A cand les valeurs d'attributs partagées par tous les sommets de V cand . L'énuméra-tion des valeurs d'attributs restantes est effectuée ensuite, ainsi que le calcul des composantes connexes correspondantes (ligne 8, fonction CC). Pour chaque composante connexe satisfaisant la condition de taille minimale, l'algorithme est appelé de manière recursive.
Sélectionner un élément a de A cand et le retirer
L'algorithme 2 effectue le regroupement temporelle des GCH précédement extraits. étant donné un GCH H, la première ligne de l'algorithme calcule les sommets V suiv connectés à H par des arêtes temporelles. La ligne 2 énumère pour chaque GCH H du temps consécutif ceux qui sont connectés temporellement à H. Le motif P en cours de construction est ensuite mis à jour avec H (ligne 3). Si H appartenait déjà à un motif, les deux motifs sont regroupés (lignes 5 et 6), sinon, un appel récursif à l'algorithme est effectué à partir de H (ligne 8).
FIG. 1 -Exemple de motif. Les zones coloriées correspondent aux GCH.
Résultats expérimentaux
Le jeu de données a été construit à partir d'images satellites à différentes résolutions (30 mètres et 10 mètres le pixel). Huit images ont été utilisées de la même région. Nous avons utilisés six attributs (discrétisés) dont trois correspondent à la radiomètre rouge, vert et proche infrarouge. Les trois autres propriétés sont des indices calculés à partir des attributs précé-dents : l'indice de rougeur, l'indice de brillance, et l'indice normalisé de végétation (NDVI). Dans le graphe, un sommet correspond à un pixel et les arêtes structurelles au 4 voisinages d'un pixel (i.e., sans considérer les diagonales). Une arête temporelle connecte deux sommets correspondant aux pixels situés dans la même zone entre des temps consécutifs. Le GAM construit contient 4 graphes avec 10 890 000 de sommets et 4 graphes avec 1 210 000 de sommets.
Dans ce jeu de données, nous recherchons des motifs homogènes sur la moitié des attributs (i.e., h = 0.5) et ayant une taille relativement petite, correspondant à une superficie d'environ 0.45 km 2 (i.e., s = 2 × 10
?6
). Plus précisément les GCH regroupés partagent au moins 80 % de leurs sommets avec un autre GCH du motif (i.e., ? = 0.8). En utilisant ces seuils, 24 motifs ont été extraits en 5 minutes. Parmi ces motifs, nous présentons sur la figure 1 un motif représentant une zone qui évolue de la même manière correspondant à l'activité minière. De manière intéressante, on peut noter que regrouper les motifs avec la contrainte de connectivité temporelle permet de bien couvrir la zone.

Introduction
Le stockage et la distribution à travers les média numériques, et plus particulièrement internet, de données visuelles atteignent des proportions gigantesques. Ceci est accéléré par la banalisation des outils de capture et d'éditions de données numériques telles que la vidéo et l'audio. Cette masse de données de différentes modalités représente une information capitale pour les étapes d'identification d'événements et de décision. Il est donc nécessaire de développer des solutions automatiques pour analyser ces contenus numériques. Une des problématiques qui suscitent beaucoup d'intérêt depuis quelques années est la détection et la reconnaissance des actions humaines dans les séquences vidéo. On appelle action tout évènement caractérisé par des mouvements ou de comportements anormaux que l'on rencontre par exemple dans les flux de vidéo surveillance Bouttefroy et al. (2010). La détection d'actions trouve de nombreuses applications telles que l'indexation, la vidéo surveillance Hu et al. (2004) ou le résumé de vidéos Zhou et al. (2008), pour ne citer que quelques-unes. Dans le contexte de la reconnaissance de l'action, la représentation des descripteurs vidéo au moyen d'un dictionnaire de mots visuels, est un domaine de recherche très actif, Willamowski et al. (2004). L'idée de base d'un DMV est de grouper un ensemble d'objets, par exemple des descripteurs visuels, en groupes de sorte que les objets de même type soient dans un même groupe (cluster). Ré-cemment, l'algorithme des K-moyennes a été largement utilisé pour construire les DMV en raison de ses hautes performances et de sa simplicité. Chaque vidéo est ensuite représentée par une distribution de mots visuels. Ces distributions servent de paramètres d'entrée dans le processus d'apprentissage à l'issue duquel une classification des actions est obtenue. Cependant, dans une telle approche, la difficulté réside souvent dans la recherche de liens plausibles entre ces entités perceptuelles et l'interprétation de la scène dans le contexte considéré. Il est donc important de trouver le moyen de définir des descripteurs plus riches en information et surtout corrélés aux actions que l'on souhaite identifier et classer.
Afin d'extraire les descripteurs de vidéo, Mojarrad et al. (2008), ont utilisé des descripteurs relatifs à des régions du corps humain, moyennant quelques hypothèses souvent difficiles à satisfaire. Afin, d'éviter ces problèmes, Dollár et al. (2005) ont choisi d'extraire des descripteurs locaux en détectant les cuboïdes locaux, cette méthode produit des mots visuels basés sur la quantification en suivant le même principe que le DMV proposé par Csurka et al. (2004). Dans la même veine, Laptev et Lindeberg (2006) ont proposé le descripteur STIP (Points d'intérêts spatio-temporels) pour détecter les cuboïdes. Néanmoins, les limites des méthodes mentionnées ci-dessus concernent non seulement la difficulté de trouver la taille optimale du "cuboïde", mais aussi le temps de calcul élevé. Pour surmonter ces problèmes, nous proposons un descripteur spatio-temporel basé sur le descripteur local SURF proposé dans Bay et al. (2006). Ce descripteur est ensuite étendu à un espace spatio-temporel 3D. Nous montrons expérimentalement l'efficacité de cette contribution pour la détection des actions humaines dans la base réaliste "UCF sport" proposée par Rodriguez et al. (2008).
Approche proposée pour la reconnaissance d'actions
Les points d'intérêt ST-SURF sont localisés à l'aide du détecteur fast-hessien proposé par Beaudet (1978) ensuite extraits à partir de l'intégralité des vidéos de la base d'aprentissage. Les ST-SURF extraits sont regroupés en utilisant l'algorithme de clustering des K-moyennes. Les clips vidéo sont représentés sous forme d'histogrammes de distributions de mots visuels. Enfin, l'étape d'apprentissage est réalisée à l'aide d'une machine à vecteurs de supports (SVM).
Extention du descripteur SURF dans le domaine temporel
L'extension du descripteur SURF dans le domaine temporel est effectuée en estimant le flot optique proposé par Sun et al. (2010). Ces deniers ont montré que les algorithmes, de calcul du flot optique, fondés sur une étape de filtrage médian permettent d'obtenir un flot optique stable sur un voisinage important, Sun et al. (2010).
Dans cet article, un point d'intérêt IP = (x, y, t) est défini par sa position (x, y) à un instant t . Dans la trame (t + n). Si cet IP effectue un déplacement u suivant la direction x et v suivant celle de y. IP devient, IP (t + n) = (x + u, y + v, t + n). Dans toutes nos expériences, sauf mention du contraire, nous supposons qu'en raison de la segmentation de la vidéo en ensemble de trames (ETr), selon la méthode de Megrhi et al. (2013), les trajectoires des vecteurs de mouvement sont stables et parallèles. Les points d'intérêts tels que u = v = 0 seront négligés. L'ensemble des trames forment un volume dans l'espace. Ce volume est un parallélipipède. Ainsi, tout au long des trames du parallélépipède, la direction 3D (u, v, n) représente la direction du mouvement de IP . Le vecteur de mouvement est calculé pour chaque IP . Notre contribution consiste en l'utilisation de l'orientation du mouvement et de sa position afin de caractériser le mouvement, au lieu d'utiliser le vecteur de direction (u, v, n) généré par le calcul du flot optique. Nous supposons que le vecteur de mouvement dans l'espace 3D peut être défini comme l'intersection de deux plans perpendiculaires respectivement au plan (x, t) et le plan (t, y). Pour extraire l'orientation du IP , nous projetons le vecteur de mouvement sur les plans (x, t) et (t, y) de l'ETr pour définir un angle pour chaque projection, le premier angle ? x entre le flot optique et le plan (t, x), l'angle ? y entre le plan (t, y) et le vecteur mouvement. 
avec t max , x max et y max sont les dimensions du Parallélipipède, t max varie en fonction du nombre de trames segmentées. Dans ce qui suit, D x et D y décrivent les distances de déplace-ment d'un point d'intérêt donné. La figure 1, illustre la projection du centre du parallélipipède sur les plans (t, x) et (t, y).
FIG. 1 -La projection du vecteur mouvement sur les plans adjacents.
Extraction du ST-SURF
La génération du nouveau descripteur ST-SURF est réalisée par la fusion par concaténa-tion du descripteur local SURF, de dimension 64-D et les 4 paramètres décrivant la position et l'orientation de chaque SURF. Le ST-SURF est donc un vecteur de 68-D. Les ST-SURF générés sont quantifiés en mots visuels en utilisant l'algorithme des k-moyennes. Chaque sé-quence vidéo est alors représentée par l'histogramme de fréquence des mots visuels. Les histogrammes des occurrences de mots visuels qui en résultent sont utilisés comme entrées du classifieur SVM.
Résultats expérimentaux
Le tableau 1, comporte les Meilleures Moyennes de Précisions (MMP) pour les différents ensembles détecteurs/descripteurs de l'état de l'art ainsi que la Précision Moyenne reportée en utilisant le détecteur Hessien (PMH). Hu et al. (2004) ont obtenu une précision de 77,4% en utilisant le descripteur HOG et 82,6% en utilisant le HOF. En effet les descripteurs de mouvement local, donnés par les histogrammes des flots optiques(HOF), caractérisent mieux l'action que les histogrammes du gradient orienté (HOG) qui décrivent l'apparence locale. L'utilisation de la combinaison HOG/HOF n'améliore pas la précision de la reconnaissance d'action. En effet, un taux de 81,6% a été reporté par Hu et al. (2004), ceci s'explique par le fait que les HOG sont moins précis pour caractériser l'information temporelle. L'extension du HOG dans le domaine temporel a permis à Wang et al. (2009) d'atteindre 85% en utilisant HOG3D/Gabor. L'orientation spatiale de ce descripteur décrit les informations de l'apparence et l'orientation temporelle extraite renseigne sur la la vitesse du mouvement. La précision du ST-SURF reste en dessous des résultats réalisés par Laptev et al. avec 85 % en utilisant la combinaison HOG3D/Gabor. Ainsi, nous adoptons l'hypothèse que celà pourrait être dû à la génération de différents DMV et l'utilisation de différents détecteurs de points d'intérêt. En utilisant la combinaison détecteur Hessien/SURF, Le ST-SURF que nous proposons donne les meilleurs résultats PMH et atteint 80,7 % de précision surpassant tous les descripteurs locaux spatio-temporels de l'état de l'art. En effet, ce descripteur est une combinaison de l'information spatiale, donnée par le SURF, et l'information temporelle dérivée du flot optique. TAB. 1 -Précision moyenne pour différentes combinaisons de détecteurs/descripteurs appliquées sur la base UCF sport.
La matrice de confusion relative à la base "UCF sport" est donnée dans le tableau 2. Nous notons que le descripteur proposé donne des résultats satisfaisants dans des vidéos réalistes. Nous soulignons que les précisions les plus faibles sont obtenues par les actions « skate » et « ride », car les mouvements de ces actions sont horizontaux. Le résultat s'améliore au fur et à mesure que les actions contiennent des mouvements verticaux comme « walk », « kick » et « lift » qui présentent des mouvements de rotation importants. Les précisions entre « dive » et « swing » sont trop proches ceci est dû à la ressemblance entre ces deux actions. L'ensemble de nos résultats prouvent que notre méthode est équivalente à l'état de l'art, et montre des performances satisfaisantes sinon meilleures que d'autres méthodes utilisant la même configuration.
TAB. 2 -Matrice de confusion de la reconnaissance d'actions de la base UCF en utilisant le ST-SURF.
Conclusion
Dans cet article, nous avons proposé un nouveau descripteur spatio-temporel basé sur l'extension du descripteur local SURF vers le domaine temporel. L'extraction du descripteur consiste à détecter des IPs et les projeter dans un espace 3D basé sur une exploitation originale de l'orientation du flot optique et de sa position. Les descripteurs extraits sont intégrés dans un DMV, pour finalement être classés en neuf actions réalistes de la base "UCF sport". En outre, le ST-SURF proposé démontre des performances de reconnaissance prometteuses sur cette base avec une précision d'environ 80,7 %. En effet, le reparamétrage du flot-optique a permis de décrire l'orientation de la trajectoire de la région d'intérêt ainsi que sa position dans un volume spatio-temporel. L'exploitation de l'information relative à l'orientaion garantit l'invariance par rotation du ST-SURF. La position de la région d'intérêt est extraite afin d'améliorer et optimiser la classification pour plus de précision. Ainsi, la classification sera plus robuste aux décalages de pixels qui peuvent aboutir à plus de mots visuels. Ainsi, en utilisant la position de la région d'intérêt à partir du flot optique (au lieu des coordonnées du point d'intérêts) nous obtenons un DMV plus compact. Enfin, les résultats obtenus démontrent la viabilité de notre approche et prouve que nous sommes déjà équivalents aux performances données par l'état de l'art. Nous imaginons de nombreuses perspectives pour l'avenir, la plus importante est d'appliquer la même méthode sur des vidéos contenant des actions plus complexes. Nous prévoyons également d'améliorer notre ST-SURF et envisageons de le combiner avec d'autres descripteurs de bas niveau et de différentes modalités.
Références
Bay, H., T. Tuytelaars, et L. Van Gool (2006 

Introduction
Les sources de production d'énergie autonomes intermittentes, de type photovoltaïque, connaissent un développement important dans les îles subtropicales. Un projet a été mis en place pour améliorer la capacité à prédire la production d'énergie d'une installation photovoltaïque grâce à un réseau de capteurs intelligents. Les données disponibles concernent 956 journées, du 2008-12-21 au 2012-03-21, sur lesquelles ont été mesurés les cumuls horaires du rayonnement solaire journalier de 9H jusqu'à 17h. Nous présentons deux stratégies (Bessafi et al., 2013) de classification des jours selon leurs rayonnements solaires puis une méthode de prédiction du flux solaire basée sur les résultats des classifications précédentes
Classification
Le rayonnement solaire peut-être décomposé en trois flux : le flux global F Global , diffus F Dif f us et direct F Direct = F Global ? F Dif f us . Nous définissons l'indice de fraction directe noté k b = F Direct /F Global pour représenter le rayonnement solaire journalier. Lorsque cet indice est proche de 1, le flux direct est proche du flux global et on est en présence d'une journée ensoleillée ; inversement, lorsque l'indice est proche de 0, la journée est nuageuse (Figure 1). On note I l'ensemble de n journées, T l'ensemble de p heures et K le nombre de classes. Dans la suite, les indices i, t décriront respectivement I, T et k = 1 . . . K. Dans la première approche, une journée d i par le vecteur des indices k b horaires
La première démarche pour classer l'ensemble des journées combine trois méthodes éprou-vées d'analyse des données : -l'Analyse en Composantes Principales pour réduire la dimension des données. Afin de trouver un nombre optimal de classes pour la partition, -la Classification Hiérarchique Ascendante de Ward (CAH) est appliquée sur un ensemble pertinent de composantes principales. -la qualité de la partition obtenue par la CAH est ensuite améliorée en appliquant la mé-thode k-means. La librairie FactoMineR (Lê et al., 2008) qui implémente cette stratégie dans le logiciel R a été utilisée. Dans la seconde approche, une journée d i est caractérisée par trois composantes
?t 2 ) t l'accélération. Pour classifier les journées, un indice global de dissimilarité est défini sur les paires des journées La plus récente méthode pour la détermination de poids optimaux est la méthode CARD (Clustering and Aggregation of Relational Data) de (Frigui et al., 2007) qui introduit une estimation des pondérations pour chaque matrice des dissimilarités. Nous proposons une nouvelle méthode (De Carvalho et al., 2012) qui détermine simultanément un ensemble de pondérations optimales et une classification des objets décrits par plusieurs matrices de dissimilarités.
Soient P = (C 1 , . . . , C K ) une partition de E, un ensemble de matrices de dissimilarités D t définies sur E et une matrice ? = (? kt ) k,t où ? kt est la pondération associée à la dissimilarité
Le problème de classification s'énonce comme la recherche du triplet optimal (P
L'algorithme proposé comporte trois étapes.
-Étape 1 : construction de la meilleure partition en K Classes -Étape 2 : calcul de la meilleure matrice de pondération -Étape 3 : recherche le meilleur vecteur de K prototypes L'algorithme démarre avec un vecteur de prototypes tiré au hasard et toutes les pondérations égales à 1 et alterne ces trois étapes jusqu'à la convergence. L'application de la première stratégie classique de classification sur la composante position a déterminé une partition P 1 a 5 classes des journées. Pour étudier l'influence des composants vélocité et accélération, nous appliquons la seconde stratégie qui calcule les pondérations ? t,k pour chaque classe et pour chaque dissimilarité D t et détermine la partition P 2 . Les courbes des moyennes des classes des partitions P 1 et P 2 sont similaires (figure 2). La similitude des deux partitions est confirmée par l'analyse du tableau 1 de confusion entre 
Prédiction
L'objectif du projet est de proposer des outils de prédiction, à travers un site web, du rayonnement solaire horaire dans une journée. L'intérêt de ces prédictions est de permettre d'anticiper un pic ou une chute de la production d'électricité pour l'heure à venir. Dans la suite, F désignera un des indicateurs de flux F Global , F Dif f us , K b . Notons F (i, t) le flux à prévoir à l'heure t d'une journée d i et F (i, t) la valeur estimée. Le problème de prédiction se formule comme la recherche d'une fonction f telle que F (i, t) = f (F (i, 1 : (t ? 1))). La classification précédente a mis en évidence l'existence de plusieurs régimes de flux solaire journalier (figure 2). Elle suggère de rechercher des modèles de prévision horaire par classe qui seraient plus appropriés qu'un modèle unique.
Si P = {C k } k est une partition des jours, le modèle de prévision locale s'écrit :
. La mise en oeuvre de cette approche nécessite le choix d'une partition P , d'une fonction d'affectation 1 C k d'une observation à une classe et d'un modèle de prévision par classe f k . Cart-Regression (Breiman et al., 1984) est un exemple de méthode qui adopte cette approche locale. Son modèle de prévision est
. La moyenne du flux F (t) dans la classe C k est l'estimation de la valeur du flux pour les journées de cette classe. Des partitions homogènes relatives à la variable à prédire F (t) sont déterminées de manière récursive et des arbres de décision calculés sur les variables prédictives permettent d'affecter une journée aux classes de ces partitions.
La méthode de prévision globale (Regr-Globale) que nous avons utilisée s'appuie sur un modèle linéaire simple :
. Plusieurs types de coefficients ont été essayés, comme a(t) = 0, b(t) =
MoyF (t)
MoyF (t?1) ainsi que d'autres statistiques la médiane, le troisième quartile. La régression linéaire simple a été retenue car elle a donné le meilleur score de prévision sur les ensembles de test. Ce modèle de régression linéaire simple a été aussi choisi pour l'approche locale. Ce choix a été motivé par la propriété physique de la persistance du flux horaire, la contrainte du projet d'avoir un système de prédiction efficace en ligne et les études préalables sur la sélection de variables discriminantes pour la régression. Le modèle de prévision de la régression locale (Regr-Locale) proposée s'écrit
, b(t, k) sont les coefficients estimés pour la classe C k d'une partition P de l'échantillon d'apprentissage des jours.
Pour cette méthode, une partition unique des jours caractérisés par ses flux horaires est dé-terminée par la première méthodologie de classification précédente. La qualité d'une méthode de prédiction est mesurée par le rapport des normes Le tableau 2(a) donne les valeurs de ? EQM des flux relatifs à des partitions calculées sur des échantillons d'apprentissage pour des nombres de classes différents. De manière logique, on constate que plus le nombre de classes K augmente plus la qualité de la prédiction s'amé-liore (l'indice ? EQM décroit). Pour K = n, on a ? EQM = 0 car à un individu correspond une classe, la prédiction est parfaite mais ne présente pas d'intérêt (sur-apprentissage). Sur les ensembles de test, la classe d'appartenance d'une journée est inconnue. A l'heure t, une journée est affectée à la classe la plus proche selon sa distance aux centres de gravité des classes calculée à partir de F (1 : t ? 1). Le modèle de régression de la classe d'affectation est ensuite choisi pour l'estimation de F (t).
Le tableau 2(b) donne les scores moyens, sur les ensembles de test, des méthodes CartRegression, Regr-Globale et Regr-Locale. On constate que la qualité moyenne de prévision de

Introduction
Pour les données hors-ligne, des méthodes d'extractions de connaissances performantes et éprouvées depuis plusieurs années existent. Différents types de classifieurs ont été proposés : plus proches voisins, bayésien naïf, SVM, arbre de décision, système à base de règles... Mais avec l'apparition de nouvelles applications comme les réseaux sociaux, la publicité en-ligne, les données du Web... la quantité de données et leurs disponibilités ont changé. Les données auparavant facilement disponibles et pouvant tenir en mémoire (données hors-ligne) sont devenus massives et visibles une seule fois (flux de données). La plupart des classifieurs, prévus pour fonctionner hors-ligne, ne peuvent généralement pas s'appliquer directement sur un flux de données.
Depuis les années 2000, l'extraction de connaissances sur flux de données est devenue un sujet de recherche à part entière. De nombreux travaux traitant cette nouvelle problématique ont été proposés (Salperwyck et Lemaire, 2011;Gama, 2010). Parmi les solutions aux problèmes de l'apprentissage en-ligne sur flux de données, les algorithmes d'apprentissage incrémentaux sont l'une des techniques les plus utilisées. Ces algorithmes sont capables de mettre à jour leur modèle à partir d'un seul nouvel exemple. Cependant la plupart d'entre eux, bien qu'étant incrémentaux, ne sont pas capables de traiter des flux de données car leur complexité n'est pas linéaire.
Dans cet article, on s'intéresse plus particulièrement à l'un des classifieurs les plus utilisés dans l'état de l'art pour réaliser une classification supervisée en ligne : le classifieur naïf de Bayes. Nous modifions ce classifieur de manière à réaliser un apprentissage en ligne pour flux de données. Ce classifieur ne nécessite en entrée que des probabilités conditionnelles P (X i |C) (où X i représente une variable explicative et C une classe du problème de classification) et sa complexité en prédiction est très faible, ce qui le rend adapté aux flux.
Néanmoins, dans le cadre de l'apprentissage hors-ligne, il a été prouvé qu'en sélectionnant les variables (Koller et Sahami, 1996;Langley et Sage, 1994) et/ou en pondérant les variables (Hoeting et al., 1999) on obtient des résultats sensiblement meilleurs. De plus Boullé dans (Boullé, 2006b) a montré le lien entre pondération des variables et moyennage de plusieurs classifieurs naïf de Bayes dans le sens où, à la fin de l'apprentissage, les deux processus produisent des modèles similaires. Ces modèles se différencient du classifieur naïf de Bayes par l'ajout d'une pondération sur chaque variable. Ces poids peuvent être optimisés directement comme cela a déjà été réalisé dans (Guigourès et Boullé, 2011) mais de manière hors-ligne.
Le présent article présente une nouvelle méthode pour estimer incrémentalement les poids d'un classifieur Naïf de Bayes Pondéré (NBP) dans le cadre des flux de données. Cette méthode utilise un modèle graphique proche d'un réseau de neurones artificiels. Le plan de cet article est le suivant : notre modèle graphique ainsi que la méthode permettant d'apprendre les poids à attribuer aux variables explicatives sont présentés au cours de la section 2. La section 3 décrit comment les estimations des probabilités conditionnelles à la classe (P (X i |C)), utilisées en entrée du modèle graphique, sont estimées. La section 4 présente une étude expérimentale de notre classifieur Naïf de Bayes Pondéré (NBP) entraîné incrémentalement sur les bases de données ayant servies au "large scale learning challenge". Enfin, la dernière section conclut cet article.
2 Classifieur Naïf Bayésien Pondéré incrémental 2.1 Introduction : le classifieur Naïf de Bayes (NB) et le classifieur Naïf de Bayes Moyenné (NBM)
Le classifieur Bayésien naïf est une méthode d'apprentissage supervisé qui repose sur une hypothèse simplificatrice forte : les variables X i sont indépendantes conditionnellement à la classe à prédire. Cette hypothèse naïve ne permet pas de modéliser les interactions entre différentes variables. Cependant, sur de nombreux problèmes réels, cette limitation n'a que peu d'impact (Hand et Yu, 2001;Langley et al., 1992). L'idée de départ de ce classifieur vient de la formule de Bayes : P (C|X) =
. La probabilité conditionnelle jointe P (X|C) étant difficilement estimable on utilise la version naïve (appelée par la suite NB) de ce classi-fieur. La probabilité de la classe devient dans ce cas :
où j est l'indice de la classe (j ? {1, ..., K}), i l'indice de la variable explicative et k une classe d'intérêt. La classe prédite est celle qui maximise la probabilité conditionnelle P (C k |X). Les probabilités P (X i |C k ) peuvent être estimées par intervalle à l'aide d'une discrétisation pour les variables numériques. Pour les variables catégorielles, cette estimation peut se faire directement si la variable prend peu de valeurs différentes ou après un groupage dans le cas contraire. Le dénominateur de l'équation 1 normalise le résultat tel que k P (C k |X) = 1. Un des avantages de ce classifieur dans le contexte des flux de données réside en sa faible complexité en déploiement, complexité qui ne dépend que du nombre de variables utilisées. L'état de l'art montre toutefois que le classifieur bayésien naïf peut être amélioré de deux manières : (i) en sélectionnant les variables (Koller et Sahami, 1996;Langley et Sage, 1994) ; (ii) en pondérant les variables (Hoeting et al., 1999) ce qui est proche d'un moyennage de modèles bayésiens (BMA = Bayesian Model Averaging) (Hoeting et al., 1999) ; ces deux processus, sélection-pondération, pouvant être mélangés de manière itérative. Le classifieur bayésien naïf moyenné résultant est similaire au classifieur bayésien naïf mais ajoute une pondération par variable tel que :
où chaque variable explicative i est pondérée par un poids w i dans l'intervalle [0, 1]). L'approche revient à un moyennage de modèles et en possède les qualités. Le moyennage de modèles vise à combiner la prédiction d'un ensemble de classifieurs de façon à améliorer les performances prédictives. Ce principe a été appliqué avec succès dans le cas du bagging (Breiman, 1996) qui exploite un ensemble de classifieurs appris sur une partie des exemples. Dans ces approches, le classifieur moyenné résultant procède par vote des classifieurs élé-mentaires pour effectuer sa prédiction. A l'opposé des approches de type bagging, où chaque classifieur élémentaire se voit attribuer le même poids, le moyennage de modèles bayésiens (BMA = Bayesian Model Averaging) (Hoeting et al., 1999) pondère les classifieurs selon leur probabilité a posteriori.
L'approche proposée : Naïf Bayésien Pondéré incrémental (NBP)
Lorsque l'on se place dans le cadre de l'apprentissage hors-ligne, les poids du classifieur Naïf de Bayes Moyenné peuvent être estimés de différentes manières : (i) par moyennage de modèles (Hoeting et al., 1999) ; (ii) par moyennage de modèles avec optimisation des poids basée sur un critère MDL (Minimum Description Length) (Boullé, 2006b) ; (iii) par optimisation directe des poids par une descente de gradient (Guigourès et Boullé, 2011). Toutefois toutes ces méthodes fonctionnent en chargeant toutes les données en mémoire et nécessitent de les relire plusieurs fois. L'approche proposée dans cet article optimise directement les poids du classifieur et est capable de fonctionner sur flux de données.
FIG. 1 -Modèle graphique pour l'optimisation des poids du classifieur bayésien moyenné.
La première étape consiste à créer un modèle graphique (voir Figure 1)  (Whittaker, 1990) dédié à l'optimisation des poids. Il permet de réécrire l'équation 2 sous la forme d'un modèle graphique où le classifieur bayésien naïf pondéré reçoit un poids par variable et par classe tel que présenté dans l'équation 3. Les poids que nous cherchons à optimiser sont donc plus nombreux dans ce modèle graphique. En effet le poids n'est plus seulement associé à la variable, mais à la variable conditionnellement à la classe, soit : (i) w ik le poids associé à la variable i et à la classe k et (ii) b k le biais lié à la classe k. Ce biais correspond à l'estimation de la probabilité P (C) et peut donc varier au cours du temps.
La première couche du modèle graphique est une couche linéaire réalisant une somme pondérée H k pour chaque classe k , tel que
La seconde couche du modèle graphique est un Sof tmax tel que :
. Finalement le modèle graphique proposé, dans le cas où les entrées sont les logs des estimation conditionnelles aux classes (log(p(X i |C k ), ?i, k), donne en sortie les valeurs P k (?k) telles que :
c'est à dire des P k = P (C k |X). Les variables positionnées en entrée de ce modèle sont issues des résumés univariés construits sur le flux. Ceux-ci seront présentés dans la section suivante (section 3).
L'optimisation des poids est réalisée à l'aide d'une descente de gradient stochastique pour une fonction de coût donnée. Pour un exemple donné X, la règle de modification des poids est :
où coût X est la fonction de coût appliquée à l'exemple X et ?coût X ?wij la dérivée de la fonction de coût vis à vis des paramètres du modèle, ici les poids w ij . Le calcul de cette dérivée (détaillé dans l'annexe de cet article) aboutit à :
où T k désigne les valeurs de probabilité désirées (target) et P k les sorties obtenues. Il ne reste ensuite plus qu'à inclure la partie couche linéaire de notre modèle graphique pour avoir les dérivées partielles ?coût ?w ik . La modification des poids a donc une complexité calculatoire très faible.
La méthode de descente de gradient en-ligne utilisée dans cet article est celle utilisée habituellement pour réaliser une rétropropagation et 3 principaux paramètres (Lecun et al., 1998) sont à prendre en compte : (i) la fonction de coût ; (ii) le nombre d'itérations ; (iii) le pas d'apprentissage.
Dans le cadre de la classification supervisée le meilleur choix pour la fonction de coût, du fait que les sorties du modèle graphique à apprendre prennent uniquement deux valeurs {0, 1}, est le log vraisemblance (Bishop, 1995), qui optimise log(P (C k |X)). Le nombre d'ité-rations est dans notre cas égal à 1 du fait que le modèle est mis à jour après chaque exemple et seulement une fois. Etant donné que l'apprentissage est réalisé sur un flux de données, nous choisissons de n'effectuer qu'une itération par exemple du flux, de ne pas utiliser ni d'early stopping (Prechelt, 1997) ni d'ensembles de validation (Amari et al., 1997). Finalement le seul paramètre à ajuster est le pas d'apprentissage. Une valeur trop faible aboutit à une convergence longue pour atteindre le minimum de la fonction de coût, alors qu'un pas trop grand ne permet pas d'atteindre ce minimum. Dans le cas d'un apprentissage hors-ligne il est possible de régler sa valeur par une méthode de validation croisée mais dans le cas de l'apprentissage sur flux, ou en une passe, ceci n'est pas envisageable. Pour les expérimentations de cet article un pas fixe (? = 10 ?4 ) a été choisi. Cependant si la présence de dérive de concept est suspectée il peut être intéressant d'avoir un pas d'apprentissage adaptatif (Kuncheva et Plumpton, 2008).
Estimation des densités conditionnelles
Cette section présente comment sont estimées les probabilités conditionnelles P (X i |C k ) qui doivent être placées à l'entrée du modèle graphique présenté au cours de la section précé-dente. Les méthodes d'estimation sont présentées ci-dessous brièvement n'étant pas la contribution majeure de cet article.
Pour nos expérimentations trois estimations sont utilisées (voir Figure 2) pour calculer P (X i |C k ) pour chaque variable numérique explicative i et pour chaque classe k : (i) une méthode de discrétisation à deux niveaux basée sur des statistiques d'ordre tel que décrite dans  (ii) une méthode de discrétisation à deux niveaux « cPiD » qui est une version modifiée de la méthode PiD (Gama et Pinto, 2006) (iii) une approximation gaussienne. L'approche peut être la même dans le cas des variables catégorielles (non détaillée dans cet article) en mettant dans le premier niveau l'approche count-min sketch (Cormode et Muthukrishnan, 2005) et dans le deuxième niveau la méthode de groupage MODL. Le lecteur intéressé pourra trouver un état de l'art et davantage de détails sur les techniques d'estimation de densités conditionnelles dans le chapitre 3 de (Salperwyck, 2012). Dans cet article le nombre de tuples utilisé est égal à 100 correspondant donc à une estimation des centiles. Le réglage de la valeur du nombre de tuples est discuté dans (Salperwyck, 2012). cPid et GkClass, décrits ci-dessous, sont deux méthodes permettant d'obtenir les quantiles.
3.1 cPid (Gama et Pinto, 2006) ont proposé une méthode de discrétisation à deux niveaux pour une variable numérique. Le premier niveau est un mélange entre les méthodes « Equal Width » et « Equal Frequency » (détaillé dans (Gama et Pinto, 2006) p. 663). Le premier niveau est actualisé de manière incrémentale et nécessite d'avoir plus d'intervalles que le second niveau. Le second niveau utilise l'information contenue dans le premier niveau pour construire une deuxième discrétisation. De nombreuses méthodes peuvent être utilisées pour le second niveau : Equal Width, Equal Frequency, Entropy, Kmoyenne... L'avantage de PiD est d'avoir un premier niveau très rapide et purement incrémental. Dans cPiD nous apportons une modification afin d'avoir une mémoire constante. L'augmentation de la consommation mémoire de la méthode PiD est due à la création de nouveaux intervalles. En effet si un intervalle devient trop peuplé alors il est divisé en deux intervalles contenant chacun la moitié des individus. Notre modification consiste, suite à la division d'un intervalle, à fusionner les deux intervalles consécutifs dont la somme des comptes est la plus faible. Ainsi le nombre d'intervalles stockés reste toujours le même. Aucune comparaison n'est réalisée dans cet article entre PiD et cPiD car notre intérêt se porte sur une utilisation à mémoire constante des méthodes.
GKClass
Cet algorithme proposé dans (Greenwald et Khanna, 2001) est un algorithme destiné à calculer les quantiles en utilisant une mémoire de O( 1 log( )) dans le pire cas, avec N le nombre d'éléments observés et l'erreur souhaitée. Cette méthode ne requiert pas de connaitre au préalable la taille N du flux et est insensible à l'ordre d'arrivée des exemples. Un des avantages de cette méthode est que, selon le besoin, on peut soit définir l'erreur maximale souhaitée, soit la mémoire maximale à utiliser. Dans le première cas l'erreur maximale est fixée et le résumé consomme autant de mémoire que nécessaire pour que l'erreur maximale ne soit pas dépassée. Dans le deuxième cas on fixe une quantité de mémoire maximale et on l'utilise au mieux pour minimiser l'erreur. Nous avons adapté cet algorithme pour qu'il stocke directement les comptes par classe. Le second niveau utilise la méthode de discrétisation supervisée MODL (Boullé, 2006a).
Approximation Gaussienne (AG)
Cette méthode suppose que la distribution des données se rapproche d'une loi normale que l'on va chercher à approximer. Pour cela il suffit de ne conserver que les trois valeurs par classe : la moyenne µ, l'écart type (ou la variance ?) et le nombre n d'éléments qui définissent cette gaussienne. Le maintien de ces trois valeurs peut se faire de manière incrémentale et donc cette méthode est parfaitement adaptée à une utilisation en-ligne ou sur les flux et ne comporte qu'un seul niveau. Elle sera utilisée comme référence dans le cadre de nos expérimentations du fait que les bases de données du « Large Scale Learning » ont été générées à l'aide de générateurs gaussiens. L'indicateur d'évaluation retenu est la précision (« accuracy ») c'est à dire T P +T N T P +T N +F P +F N où T P , T N , F P et F N sont respectivement le nombre de vrai-positifs, vrai-négatifs, faux-positifs et faux-négatifs.
Expérimentations
Protocole
Les expérimentations pour ce classifieur sont réalisées sur les bases du challenge Large Scale Learning 1 2 , proposé par le réseau d'excellence PASCAL. Toutes ces bases contiennent 500 000 exemples étiquetés, ce qui peut est considéré comme suffisant pour évaluer un algorithme en-ligne. Nous utilisons les bases alpha, beta, delta et gamma, qui possèdent 500 variables numériques et les bases epsilon et zeta, qui en contiennent 2000. Les jeux de données sont séparés en ensemble de test/apprentissage. Les 100 000 premiers exemples sont pris comme ensemble de test et les autres comme ensemble d'apprentissage.
1. http://largescale.ml.tu-berlin.de/about/ 2. http://jmlr.csail.mit.edu/papers/topic/large_scale_learning.html
Résultats
Pour la première partie des expérimentations, un classifieur Naïf de Bayes (NB) sans pondération est utilisé. Il utilise les estimations de densités conditionnelles aux classes issues des méthodes décrites dans la section précédente. Les résultats sont présentés dans le tableau 1 et montrent que l'estimation des probabilités conditionnelles est précise pour les trois méthodes du fait que le classifieur naïf de Bayes obtient de bons résultats avec chacune d'entre elles. Malgré le fait que les données du challenge aient été générées à l'aide d'un générateur gaussien les deux autres méthodes, cPid et GKClass, obtiennent des résultats similaires à la méthode basée sur l'approximation Gaussienne (AG). Entre les deux méthodes cPid et GKClass, le résumé GKClass, apporte des garanties précision / mémoire utilisée, a des résultats comparable à cPid et ne fait pas d'hypothèse sur la nature de la distribution des données. De ce fait il a été choisi pour la suite des expérimentations.
Alpha
Beta Delta # examples 40 000 100 000 380 000 40 000 100 000 380 000 40 000 100 000 380 000 TAB. 1 -Précision du classifieur naïf de Bayes sans pondération utilisant GKClass, cPiD et l'approximation gaussienne pour calculer les probabilités conditionnelles.
Grâce aux résultats présentés dans le tableau 1 nous savons que l'estimation des densités conditionnelles est précise. Par conséquent nous pouvons à présent évaluer le comportement de notre classifieur Naïf de Bayes Pondéré (NBP). Les résultats comparent quatre classifieurs :
1. un classifieur Naïf de Bayes (NB) entraîné hors-ligne et utilisant la discrétisation MODL (Boullé, 2006a) et toutes les données chargées en mémoire ;
2. un classifieur Naïf de Bayes Moyenné (NBM) entraîné hors-ligne et utilisant la discréti-sation MODL (Boullé, 2006a) et l'algorithme décrit dans (Boullé, 2006b) pour calculer les poids des variables explicatives -cette méthode est l'une des meilleures de l'état de l'art (Guyon et al., 2009) ; 3. un classifieur Naïf de Bayes (NB) entraîné en-ligne et utilisant la méthode de discrétisa-tion à deux niveaux qui utilise GKClass au niveau 1 et la discrétisation MODL au niveau 2 ;
4. notre classifieur Naïf de Bayes Pondéré (NBP) entraîné en-ligne et dont les poids sont estimés à l'aide de notre méthode basée sur un modèle graphique. La méthode de discrétisation utilise GKClass comme niveau 1 et la discrétisation MODL comme niveau 2.
La 
Conclusion
Les résultats de notre version en-ligne du classifieur naïf de Bayes sont prometteurs. Ses performances sont meilleures que celles de la version en-ligne non pondérées et proche de la version pondérée hors-ligne. Cependant nos résultats pourraient encore être améliorés dans de prochains travaux. Notre première piste d'amélioration serait d'utiliser les résumés GKClass comme des « mini-batch » (Cotter et al., 2011) et de réaliser plusieurs itérations pour accélérer la descente de gradient. Notre seconde proposition serait d'avoir un pas adaptatif pour la descente de gradient : rapide au début de l'apprentissage puis plus lent par la suite, ou de prendre en compte le taux d'erreur comme dans (Kuncheva et Plumpton, 2008).
Le pas d'apprentissage pourrait aussi être contrôlé par une méthode de détection de changement de concept afin de ré-augmenter le pas dès qu'une détection a lieu et donc de ré-apprendre plus rapidement. Il faudrait de plus mettre à jour les résumés suite à la détection afin d'avoir des estimations correspondant au nouveau concept. De nombreuses méthodes de détection existent mais afin de rester cohérent avec notre approche la grille MODL  pourrait être utilisée pour détecter les changements de distribution.
Annexe -Calcul de la dérivée de la fonction de coût
Cette annexe explicite le calcul de la dérivée de la fonction de coût pour le classifieur bayésien moyenné avec optimisation des poids par descente de gradient.
Le modèle graphique permet d'avoir directement en sortie la valeur des P (C k |X). Le but étant de maximiser la vraisemblance il suffit alors de minimiser le log vraisemblance. On décompose tout d'abord la partie softmax en considérant que chaque sortie de la fonction sof tmax, avant la phase de normalisation, peut être vue comme étant la succession de deux étapes : une phase d'activation suivi d'une fonction recevant la valeur de l'activation. Ici la fonction d'activation peut être vue comme étant O k = f (H k ) = exp(H k ) et la sortie de la partie softmax de notre modèle graphique est :
La fonction de coût étant le log vraisemblance, il faut considérer deux cas : (i) soit on désire apprendre pour la valeur 1 ; (ii) soit on désire apprendre la valeur 0. On pose pour la suite :
Dans le cas où l'on désire obtenir la valeur 1 en remplaçant (6) dans (7) :
D'où on obtient finalement :
Dans le cas où l'on désire obtenir la valeur 0 l'effet de l'erreur est uniquement transmis par la normalisation issue de la fonction sof tmax (la dérivée de la fonction d'erreur vis-à-vis d'une unité de sortie pour laquelle la sortie désirée est 0 est nulle). On obtient à l'aide de calculs similaires :
On conclut alors que :
où T k désigne les valeurs de probabilité désirées (target) et P k la probabilité estimée par le modèle graphique.
Il ne reste ensuite plus qu'à inclure la partie couche linéaire de notre modèle graphique pour avoir les dérivées partielles ?Coût ?w ik .
Summary
A naive Bayes classifier is a simple probabilistic classifier based on applying Bayes' theorem with naive independence assumption. The explanatory variables (X i ) are assumed to be independent from the target variable (C). Despite this strong assumption this classifier has proved to be very effective on many real applications and is often used on data stream for supervised classification. The naive Bayes classifier simply relies on the estimation of the univariate conditional probabilities P (X i |C). This estimation can be provided on a data stream using a "supervised quantiles summary". The literature shows that the naive Bayes classifier can be improved (i) using a variable selection method (ii) weighting the explanatory variables. Most of these methods are related to off-line learning and need to store all the data in memory and/or require reading more than once each example. Therefore they cannot be used on data stream. This paper presents a new method based on a graphical model which computes the weights on the input variables using a stochastic estimation. The method is incremental and produces a Weighted Naive Bayes classifier for data stream. This method will be compared to classical naive Bayes classifier on the Large Scale Learning challenge datasets.

Introduction
De nombreux services pour la télévision requièrent une segmentation et un étiquetage corrects du flux (corpus thématiques issus d'archives, TV à la demande...). Il faut ainsi disposer d'un guide de programme complet, documentant aussi les inter-programmes, et précis à l'image près. Un tel guide est malheureusement rarement disponible auprès des chaînes. Calculer ce guide de programme est le but de la structuration automatique des flux TV. Plusieurs approches ont été présentées dans la littérature pour ce faire. Qu'elles exploitent des méta-données (Poli, 2008) ou des indices audio et vidéo (Naturel et Gros, 2008;Manson et Berrani, 2010;Ibrahim et Gros, 2011), toutes reposent sur une étape de classification supervisée nécessitant des connaissances a priori mais requièrent trop d'annotation manuelle pour être facilement utilisables en pratique. Par ailleurs, l'utilisateur doit définir les classes pertinentes pour un flux.
Dans cet article, nous proposons de réduire drastiquement l'intervention a priori de l'utilisateur en passant à une classification non supervisée. Le rôle résiduel de l'utilisateur serait alors d'étiqueter les classes qui émergent ainsi des données, plusieurs classes pouvant bien entendu partager la même étiquette. À l'image du célèbre K-MEANS, les techniques de clustering reposent sur une représentation simple des données, et une notion de distance entre ces repré-sentations est également fournie par l'utilisateur (Jain, 2010). Ce sont ces deux points que nous cherchons à éviter. Depuis quelques années, certains travaux ont tenté de mettre à profit les capacités discrimantes des techniques d'apprentissage supervisé dans un cadre non-supervisé.
Leur principe commun est de déduire une similarité à partir de classifications identiques répé-tées sur des problèmes d'apprentissage factices (Shi et Horvath, 2005;Claveau et Ncibi, 2013). Elles permettent de ne pas avoir à expliciter la notion de distance entre données, mais reposent toujours sur une représentation classique des données sous forme attributs-valeurs qui n'est pas adaptée à la nature multi-relationnelle de nos données. Dans la veine de ces derniers travaux, nous proposons donc une technique de clustering capable de manipuler ces données. Plus précisément, nous détournons la programmation logique inductive (PLI) -technique d'apprentissage supervisé dont l'expressivité permet de représenter naturellement ce type de données -pour fonctionner dans ce cadre non supervisé.
Programmation logique inductive et données relationnelles
Que ce soit dans un cadre supervisé ou non supervisé, il est usuel de décrire les données à manipuler sous une forme propositionnelle, dite attribut-valeur, ou encore vectorielle. Les objets doivent alors tous avoir le même nombre d'attributs, et les attributs sont considérés indépendamment (les relations ne sont pas exploitées). Dans notre cas, les objets à manipuler sont des segments correspondant à des programmes ou des inter-programmes, dont beaucoup (e.g. les publicités) sont répétés plusieurs fois (appelées ci-après occurrences).  ton et De Raedt, 1994, pour une présentation détaillée). La PLI est fondée sur la logique des prédicats : les données d'apprentissage sont décrites en Prolog et le classifieur est un ensemble de clauses de Horn. C'est cette expressivité qui permet de décrire les problèmes multi-relationnels. La figure 1 montre un bref extrait de description d'une émission dans B en Prolog standard. On y voit la façon simple de décrire les relations entre les différentes occurrences grâce aux prédicats binaires next_occ/2 et next_in_stream/2. Dans B sont aussi données les définitions de prédicats pouvant être utiles pour inférer les règles de H. Dans l'extrait précé-dent se trouvent les définitions du prédicat prev_occ/2, qui met en relation deux occurrences de la même émissions diffusées l'une après l'autre, et du prédicat interval/3 qui indique l'intervalle de temps entre deux occurrences de deux émissions.
Le  3 Du supervisé au non supervisé 3.1 Principes L'idée principale de notre approche est de déduire une distance (ou une similarité) à partir de classifications répétées de deux émissions pour des tâches d'apprentissage aléatoire en PLI. Les émissions couvertes souvent par les mêmes clauses inférées seront supposées proches. L'algorithme 1 donne un aperçu global de la démarche. Comme pour le bagging (Breiman, 1996), la classification est répétée un grand nombre de fois en faisant varier les différents paramètres d'apprentissage : les exemples (étape 3 qui divise les données en exemples positifs E + train et en un ensemble E OoB dit out-of-bag utilisé ensuite), les contre-exemples (étape 4), le langage d'hypothèse (étape 5). À chaque itération, un compte des paires d'émissions (x i , x j ) couvertes par les mêmes clauses (on parle de co-couvertures) est tenu à jour dans la matrice M co-couv en tenant compte des couvertures (une règle très discriminante "rapporte plus"). La dernière étape relève simplement de l'emploi d'une technique de clustering opérant sur cette matrice de co-couvertures, considérées comme des mesures de similarité. Dans nos expérimen-tations présentées dans la section suivante, nous utilisons le Markov Clustering (van Dongen, 2000). L'avantage par rapport au K-MEANS/K-MEDOIDS est de ne pas nécessiter de fixer a priori le nombre de clusters attendus, et d'éviter le problème de l'initialisation de ces clusters. Inférence :
for all clause h l parmi H do 8:
for all paire e i , e j de E OoB telle que B, h e i , e j do 9:
La stratégie au coeur de cette approche est donc de varier les biais d'apprentissage à chaque itération. Le premier de ces biais est bien sûr l'ensemble d'exemples utilisés. Pour nos expériences, nous utilisons un dixième des exemples positifs tirés aléatoirement à chaque ité-ration. C'est sur les 90 % restants que sont appliquées les règles inférées pour trouver les co-couvertures. La génération des exemples négatifs est un point important de l'algorithme. Il s'agit dans notre cas d'inventer des émissions, avec leurs différentes occurrences et leurs caractéristiques. Ces exemples doivent être suffisamment réalistes pour produire des tâches d'apprentissage dont la difficulté assure la pertinence des règles trouvées et donc des co-couvertures produites. Pour générer ces contre-exemples, nous copions des parties de descriptions d'émis-sions piochées aléatoirement dans B. Le format des règles autorisées, c'est-à-dire le langage d'hypothèse L H est lui aussi différent à chaque tour. En pratique, tous les modes des pré-dicats possibles sont décrits à l'initialisation de l'algorithme, et un sous-ensemble (un tiers) est ensuite choisi aléatoirement à chaque itération. Ces apprentissages sur des tâches supervisées factices conférent, par leur variété, des propriétés importantes à la similarité obtenue. Celle-ci mélange ainsi naturellement des descriptions complexes, opère par construction une sélection de caractéristiques, prend en compte les redondances des descripteurs, ignore ceux de mauvaise qualité, et elle est robuste aux données aberrantes.
Validation expérimentale
Évaluer une tâche de découverte de connaissances comme le clustering est toujours délicat, puisqu'elle suppose l'existence d'une vérité terrain dont on souhaite se passer. Les données que nous utilisons pour nos expériences celles de Naturel et Gros (2008)  Nous présentons en figure 3 les résultats du clustering relationnel après 1 000 itérations ainsi que plusieurs baselines fondés sur une représentation classique des données, c'est-à-dire sous forme attribut-valeur. Les attributs utilisés pour décrire une émission sont les suivants : nombre d'occurrences, durée moyenne, intervalle minimal entre deux occurrences, intervalle maximal, intervalle moyen, nombre maximal d'occurrence sur une plage de 24h, durée entre la première et la dernière occurrence, présence ou non de toutes les occurrences dans la même journée et nombre moyen d'émissions uniques apparaissant avant ou après les occurrences. Les algorithmes baseline sont le K-MEANS, EM, CobWeb, tels qu'implémentés dans WEKA (Hall et al., 2009) ; pour chacun, nous ne rapportons que les résultats des meilleures configurations en terme d'ARI. Nous indiquons aussi les résultats de notre système avec cette description attribut-valeur (i.e. sans les prédicats relationnels de L H ). Quelle que soit la mesure d'éva-luation, notre technique de clustering offre de bien meilleurs résultats que les autres ; l'apport de la représentation apparaît clairement. Les clusters obtenus sont néanmoins différents en nombre et en contenu des classes attendues en vérité terrain. Une analyse des différences montre que la classe bande-annonce est difficile à capturer (elle est distribuée sur plusieurs clusters) ; d'autres cas problématiques sont causées par des émissions aux bornes de notre corpus ou pour lesquelles les trois semaines ne sont pas suffisantes pour identifier les schémas de récurrences.
L'examen des règles inférées à chaque itération permet aussi une validation indirecte de notre approche puisqu'elles exploitent bien l'aspect multi-relationnel de nos données. C'est le cas de la règle suivante qui couvre ainsi les émissions diffusées à intervalle régulier : 
Conclusions
La méthode de clustering que nous avons proposé nous permet bien de tirer au mieux profit de la nature particulières de nos données. Elle offre ainsi un moyen d'obtenir une notion de distance même dans des espaces de description riches et non métriques. Bien sûr, bien qu'il

Introduction
Les données contenant une information temporelle constituent un défi pour le processus de découverte de connaissances (Yang et Wu, 2006). Les données temporelles sont complexes dans le sens où un objet de la base est décrit par une ou plusieurs séquences d'éléments ordonnés dans le temps. Selon la nature des éléments temporels (catégoriels ou numériques, ponctuels ou continus dans le temps), il existe une grande diversité de méthodes d'extraction de connaissances (Mörchen, 2007). Ici, nous nous intéressons aux données de séquences d'évènements catégoriels et ponctuels, où chaque évènement d'une séquence est associé à un temps t, et que nous appelons simplement séquences d'évènements temporels. La fouille de séquences d'évènements temporels trouve des applications dans de nombreux domaines : e.g., dans le domaine médical, Patnaik et al. (2011) explore des bases de dossiers médicaux électro-niques de patients à la recherche de motifs d'évènements temporels fréquents ; dans le domaine du Web, Masseglia et al. (2008) et Saleh et Masseglia (2011) extraient des comportements fré-quents d'utilisateurs par période de temps ; en sciences sociales, Studer et al. (2010) cherche à grouper des individus selon leur parcours de vie. La majeure partie des efforts de recherche s'est focalisée sur l'extraction de motifs fréquents dans les données de séquences d'évène-ments temporels (ou TAS pour "Temporally-Annotated Sequences", voir e.g., (Giannotti et al., 2006)). Dans cet article, nous nous intéressons au problème de clustering de séquences : le but est de créer des groupes de séquences qui partagent des caractéristiques similaires. Dans la plupart des méthodes de l'état de l'art, il est nécessaire de définir une mesure de (dis)similarité entre séquences ainsi que le nombre de clusters à trouver et d'autres paramètres : e.g., Studer et al. (2010) utilisent l'approche des k-medoids couplée à une distance basée sur les opérations d'insertion-suppression et de substitution (dont les coûts sont à définir) de transitions dans une séquence. Le paramétrage de telle méthode est souvent complexe et peut dépendre du domaine d'application et de la quantité de données dont on dispose. Le choix d'un clustering trop fin (un grand nombre de clusters) ne garantit pas que les clusters sont statistiquement valides et peut mener au sur-apprentissage alors qu'un clustering grossier (peu de clusters) nous apporte une information peu précise sur la structure sous-jacente des données et nous offre ainsi un ré-sumé trop général des données. De plus, la dimension temporelle des séquences d'évènements temporels est primordiale et doit être prise en compte pour grouper des séquences similaires, i.e. qui suivent la même distribution d'évènements au cours du temps.
Notre contribution est la suivante. Nous proposons KHC, une méthode de co-clustering de séquences d'évènements temporels basée sur les modèles en grille (Bondu et al., 2013) : le coclustering (Dhillon et al., 2003;Nadif et Govaert, 2010) consiste à partitionner simultanément et de manière cohérente les trois dimensions de la base de séquences d'évènements temporels ; ici, les séquences sont partitionnées en clusters, ainsi que les évènements, et le temps est discrétisé en intervalles. Nous en déduisons une mesure de dissimilarité entre clusters nous permettant d'accéder par classification hiérarchique ascendante à la granularité nécessaire à l'analyse. En section 3, nous validons expérimentalement la méthode sur des données synthé-tiques et réelles et proposons des indicateurs utiles à l'interprétation des clusters révélés par la méthode.
Séquences temporelles, modèles en grilles et clustering
Contexte et notations. Une séquence s d'évènements temporels de taille k > 0 est un ensemble d'observations ordonnées s i = (t i1 , e i1 ), (t i2 , e i2 ), . . . , (t i k i , e i k i ) , tel que ?j, 1 ? j ? i ki , t j ? R+ et e j ? E avec E un ensemble non-ordonné d'évènements catégoriels. Une base de données de séquences temporelles est simplement un ensemble de séquences temporelles ainsi définies D = {s 1 , . . . , s n }. Nous proposons de représenter un ensemble de séquences temporelles par une base de données à trois variables (ou dimensions) : S pour les identifiants de séquences, T pour la variable temps et E pour la variable évènement. Dans la suite, un objet (s, t, e) de D sera appelé un point de la base. Cette représentation tridimensionnelle des données se prête bien à l'usage des modèles en grilles (Bondu et al., 2013) pour le clustering -plus précisément nous utilisons le cadre de travail MODL (Minimum Optimized Description Length) et la méthode de coclustering KHC 1 déjà instanciée dans le cas des données fonctionnelles (Boullé, 2012). Le but est de partitionner les variables catégorielles (identifiants de séquences et évènements) et de discrétiser la variable numérique "temps". Le résultat est une grille tridimensionnelle dont les cellules sont définies par un groupe d'identifiants de séquence, un groupe d'évènements et un intervalle de temps. Le meilleur modèle M * (i.e., la grille optimale) est la grille la plus probable connaissant les données. Pour obtenir le modèle de grille optimal M Bayesien, noté cost. Le critère cost établit un compromis entre la précision et la robustesse du modèle en grille et est défini comme suit : Critère d'évaluation. Un modèle de grille (pour le coclustering de séquences temporelles) est optimal s'il minimise le critère cost :
où n est le nombre de séquences, a le nombre d'évènements de E, N le nombre total d'évè-nements temporels (i.e., le nombre de points de la base), k S (resp. k E , k T ) le nombre de clusters de séquences (resp. le nombre de clusters d'évènements, le nombre d'intervalles de temps), k = k S k E k T le nombre de cellules de la grille, N i S (resp. N j T , N i E , N i S j T i E ) est le nombre cumulé de points du cluster de séquences i S (resp. dans l'intervalle de temps j T , du cluster d'évènements i E , de la cellule (i S , j T , i E ) de la grille), n i S (resp. n i E ) le nombre de séquences dans le cluster i S (resp. le nombre de valeurs d'évènements dans le cluster i E ), et enfin n S i (resp. n E i ) le nombre de points de la séquence i (resp. le nombre de points ayant pour valeur d'évènement i). Notons que B(n, k S ) est le nombre de divisions de n éléments en k S sous-ensembles et B(a, k E ) est défini de manière similaire. Les deux premières lignes correspondent à la probabilité a priori du modèle et constituent le terme de régularisation du modèle : les modèles complexes (beaucoup de clusters pour les variables catégorielles et/ou beaucoup d'intervalles pour la variable numérique) seront pénalisés. Les deux dernières lignes correspondent à la vraisemblance du modèle : les modèles les plus proches des données seront préférés ; le cas extrême avec un point par cellule aura une vraisemblance maximale, mais une probabilité a priori très faible et donc une valeur de cost très forte. Une grille avec une faible valeur de cost indique une forte probabilité p(M | D) de la grille connaissant les données. En termes de théorie de l'information, le logarithme négatif de probabilités s'interprète comme une longueur de codage. Ainsi, selon le principe MDL (Minimum Description Length), le critère cost peut s'interpréter comme la longueur de codage du 2. L'uniformité se situe à chaque étage de la hiérarchie : le prior n'est donc pas uniforme ; dans ce cas, l'approche MAP n'est pas une simple maximisation de la vraisemblance. modèle de grille plus la longueur de codage des données connaissant le modèle ; et une faible valeur de cost indique aussi une forte compression des données en utilisant le modèle M .
Le critère cost est optimisé en suivant une stratégie gloutonne ascendante : (i) on part de la grille au grain le plus fin, (ii) on considère toutes les fusions possibles entre groupes de valeurs ou intervalles, et (iii) on réalise la meilleure fusion si le critère cost décroit après fusion. Ce processus est réitéré tant qu'il y a amélioration du critère. La grille obtenue constitue une estimation de la densité jointe des séquences et des dimensions des évènements temporels (i.e., des trois variables S, T et E). Notons que KHC est libre de tout paramètre utilisateur (i.e., nous n'avons pas à choisir le nombre de clusters de séquences ou d'évènements, ni le nombre d'intervalles de temps) ; de plus sa complexité en temps est sub-quadratique : ?(N ? N log N ) où N est le nombre de points de la base -pour les détails complémentaires voir (Boullé, 2012).
Mesure de dissimilarité et simplification de la structure de grille. Bien qu'optimale, la grille générée par KHC peut s'avérer trop fine pour une analyse directe par un utilisateur, e.g., plusieurs dizaines de clusters de séquences peuvent être générés. Nous proposons une méthode de simplification de la grille par fusions successives de clusters ou d'intervalles, en choisissant la fusion qui dégrade le moins la qualité de la grille. Pour ce faire, nous introduisons une mesure de dissimilarité entre deux clusters (ou intervalles) qui caracté-rise l'impact de la fusion sur le critère cost. Soient c .1 et c .2 deux clusters d'une dimension de la grille M (i.e., deux groupes de valeurs d'identifiants de séquences ou d'évènements, ou deux intervalles contigus de temps). Soit M c.1?c.2 le modèle de grille après avoir fusionné c .1 et c .2 . La dissimilarité ?(c .1 , c .2 ) entre deux clusters est définie comme la différence du critère cost après et avant fusion :
Ainsi, si l'on fusionne les clusters qui minimisent ?, nous obtenons la grille sub-optimale M (avec un grain plus grossier, i.e., simplifiée) qui dégrade le moins le critère cost et donc avec une perte d'information minimale par rapport à la grille avant fusion. Le taux d'information de la nouvelle grille M est défini par
où M ? est le modèle nul, i.e., la grille dont aucune dimension n'est partitionnée. En construisant ainsi une hiérarchie ascendante des clusters, en partant de M * et au pire jusqu'à M ? , l'utilisateur pourra s'arrêter au niveau de grain voulu et nécessaire pour une analyse en contrô-lant le nombre de clusters ou le pourcentage d'information gardée. Notons que les fusions s'effectuent indistinctement sur toutes les dimensions en fonction de ?.
Validation expérimentale
Dans cette section nous proposons des expériences sur données simulées afin de démontrer l'efficacité de la méthode en termes de pertinence pour retrouver les motifs simulés dans les données ainsi qu'en terme de temps de calcul pour des données allant jusqu'au million de points. Nous rapportons aussi les résultats de la méthode sur un jeu de données réelles. Les expériences sont réalisées sur un PC de bureau cadencé à 3,8GHz avec 2Go de RAM.
Données simulées
Exemple à 2 motifs. Considérons deux motifs M 1 et M 2 définis sur le domaine de valeurs de temps T = [0, 1000] ? R + et l'ensemble d'évènements E = {a, b, c, d, e, f, g, h, i, j, k, l} tels que :
=]600; 1000] alors e ? E  . Nous calculons la valeur de l'indice de Rand ajusté (ARI) pour chaque grille générée pour évaluer la concordance entre les clusters de séquences trouvés par KHC et les deux motifs sous-jacents. Les résultats sont rapportés dans les figures 1(abc). Nous observons que pour des petits sous-ensembles de données de D, il n'y a pas assez de points pour que KHC découvre de motifs significatifs : aucun cluster de séquences n'est découvert pour N ? 64 (i.e., en moyenne 3 points par séquences). Pour CM = 10 (10 séquences par motifs en figure 1(a)), à partir de N = 128 points (soit en moyenne seulement 6 points par séquence), ARI = 1 et les deux motifs sous-jacents sont découverts. Nous remarquons aussi que pour un niveau de bruit ? ? 0.1, N = 128 points suffisent encore à trouver les deux clusters de séquences, puis plus le bruit augmente, plus le nombre de points nécessaires à la découverte des deux motifs augmente. Enfin, augmenter le nombre de points jusqu'à 2 20 ne provoque pas de sur-apprentissage, la valeur de ARI est stable à 1. Les mêmes observations tiennent lorsque CM = 50 ou CM = 100 ; nous observons aussi que plus il y a de courbes par motifs (i.e. plus CM est grand), plus il faut de points pour découvrir les deux motifs. Temps de calcul. La figure 2 rapporte les temps de calcul des différentes versions de bases de données à 2 motifs pour CM = 10, 50, 100, en fonction du nombre de points. D'une manière générale, on observe que le temps de calcul augmente, comme attendu, avec le nombre de points d'apprentissage, mais aussi avec CM et le niveau de bruit. Retenons aussi que pour la base la plus difficile, i.e., N = 2
20
, CM = 100, (soit en moyenne 5200 points par séquence) et ? = 0.5, KHC retrouve les motifs recherchés en moins de 1h30. Ici, le temps de calcul dépend du nombre de valeurs de temps différentes prises dans T (potentiellement 2 20 ), car lors de la discrétisation de T , KHC explore toutes les coupures possibles entre deux valeurs de temps présentes dans les données ; nous avons réalisé les mêmes expériences avec des valeurs de temps entières prises dans T : dans ce cas, pour N = 2 20 , CM = 100 et ? = 0.5, KHC trouve les motifs recherchés en 13 minutes. points, CM = 10 et ? = 0.5. Nous proposons 3 visualisations différentes basées sur la fréquence des cellules, l'information mutuelle et le contraste des cellules -chacune d'entre elles apportant une information différente sur les clusters de séquences découverts. Bien que KHC génère des grilles tridimensionnelles, la dimension S étant partitionnée en deux clusters de séquences, nous proposons des visualisations sur les deux autres dimensions pour chaque cluster de séquences. Nous pré-sentons ces visualisations dans la figure 3. Visualisation de la fréquence. En figures 3M 1 (a) et 3M 2 (a), la plus classique des visualisations consiste à représenter le nombre de points par cellule, i.e. N i S j T i E pour la cellule (i S , j T , i E ). Nous apercevons déjà les cellules les plus fréquentes qui correspondent à la définition des mo-tifs sous-jacents malgré le niveau de bruit ? = 0.5. Visualisation de l'information mutuelle. Pour un cluster de séquences c i S , l'information mutuelle entre les variables T ? M et E ? M issues du partitionnement ? M des variables temps et évènement généré par le modèle de grille M , est défini comme suit :
Ainsi, les M I i1i2 représentent la contribution de la cellule c i1i2 à l'information mutuelle. Fréquence Information mutuelle Contraste
FIG. 3 -Visualisation de la fréquence, de l'information mutuelle conditionnelle à un cluster de séquences et du contraste pour les 2 clusters trouvés par KHC correspondant aux 2 motifs sousjacents M 1 et M 2 . En abscisses, la discrétisation du temps en 7 intervalles et en ordonnées, la partition des évènements en 4 groupes : E = {a, b, c} ? {d, e, f } ? {g, h, i} ? {j, k, l}.
Visualisation du contraste. Pour le couple de variables partitionnées à visualiser (T ? M , E ? M ), le contraste entre un contexte, i.e., un cluster de séquences c i S et le reste des clusters de sé-quences {c i } i =i S est défini comme suit :
Comme précédemment, le signe des M I i1i2;i S qualifiera le contraste entre c i S et le reste des données (i.e., les clusters de séquences {c i } i =i S ). Les valeurs de M I i1i2;i S sont rapportées dans les figures 3M 1 (c) et 3M 2 (c). Prenons le cluster de séquences de c M1 . Les cellules blanches indiquent qu'il n'y a pas de contraste à cet endroit entre c M1 et le reste des données (ici, c M2 ) : par exemple la cellule ([0; 100], {g, h, i}), malgré le bruit, n'est pas caractéristique de c M1 ; en effet, la probabilité du groupe d'évènements {g, h, i}) dans l'intervalle de temps ([0; 100] n'est pas significativement différent selon qu'on se trouve dans c M1 ou c M2 . De même la cellule ([401; 500], {d, e, f }) présente un contraste nul puisqu'elle est commune aux deux motifs sous-jacents. Les cellules rouges indiquent ce qui caractérise c M1 par rapport à c M2 : dans ces cellules, la probabilité de points est bien supérieure pour c M1 que pour c M2 . Les cellules bleues indiquent un contraste négatif : la probabilité de points y est plus faible pour c M1 que pour c M2 .
Données réelles
A partir de la base de données DBLP (Ley, 2009), nous considérons tous les auteurs qui ont publié des articles parus dans les actes de neuf conférences dont la thématique première est les bases de données et/ou la fouille de données (CIKM, VLDB, SIGMOD, ICDE, ICDM, KDD, SDM, PAKDD, PKDD). Pour chaque auteur, nous considérons l'année de publication et l'évènement lié à la publication (i.e., le nom de la conférence). Nous constituons ainsi une base de données de séquences d'évènements temporels à trois dimensions (auteur, année, évè-nement). Les points de la base sont dupliqués lorsqu'un auteur a publié plusieurs fois dans la même conférence la même année.
La base D ainsi constituée est composée de plus 
c j ?X M c j =c où P X M (c) est la probabilité d'avoir un point avec une valeur du cluster c, c \ v est le cluster c duquel on a retiré la valeur v, c j ? v est le cluster c j auquel on a rajouté la valeur v et M |c \ v, c j ? v le modèle de grille M qui a subi les modifications précitées. Intuitivement, une valeur v i est représentative d'un cluster c et dite typique, si elle est proche de c et très différente (en moyenne) des autres clusters c j = c. Aussi, pour un cluster d'auteurs, les auteurs qui ont peu publié (1 ou 2 fois, voir figure 5) sont souvent moins typiques que ceux qui sont les plus prolifiques, puisque si on les déplace vers un autre cluster, la différence de cost (dans la formule de la typicité) sera faible. Dans la figure 6, nous présentons pour chacun des quatre clusters d'auteurs (un par ligne), les auteurs les plus typiques (colonne 1), ainsi que la fréquence de publications (colonne 2) et le contraste (colonne 3) par une grille à deux dimensions Année × Conférence. 
Conclusion & discussion
Nous avons proposé une méthode de clustering et d'analyse de séquences temporelles basée sur les modèles en grille. Les identifiants de séquence sont groupés en clusters, ainsi que les évènements, et la dimension temporelle est discrétisée en intervalles -le tout forme ainsi une grille tridimensionnelle (ou tri-clustering). Obtenir la grille optimale (au sens Bayésien) ne nécessite aucun paramétrage utilisateur. Pour exploiter la grille, nous avons proposé (i) une mesure de dissimilarité entre clusters afin de sélectionner le grain de la grille tout en contrôlant la perte d'information, (ii), un critère (la typicité) pour identifier les valeurs les plus représen-tatives d'un cluster, (iii) ainsi que deux critères basés sur l'information mutuelle pour caracté-riser, interpréter et visualiser les clusters trouvés. Nos différentes propositions ont été validées sur des données simulées ainsi que sur des données réelles issues de DBLP. Notons qu'une étude du comportement asymptotique des indicateurs proposés a été réalisée et qu'une étude complète des trajectoires antenne-antenne d'utilisateurs du mobile -que l'on peut voir comme des séquences d'évènements -a été menée à l'échelle d'un pays (voir (Guigourès, 2013)).

Introduction
Internet fournit une gigantesque base de connaissances qui permet aujourd'hui d'obtenir des informations sur tous les sujets du monde. C'est par exemple le cas des sites dédiés aux problèmes médicaux, pour lesquels les utilisateurs peuvent aisément soumettre des problèmes de santé et être au centre d'espaces de discussion. Si certains sites se refusent à tout diagnostic en ligne, d'autres en revanche font souvent étalage de situations individuelles difficiles qui conduisent bien souvent à l'énoncé de diagnostics par des non-professionnels, dont certains évoquant les mots "cancers" ou "tumeurs", ce qui peut avoir un impact psychologique fort sur les visiteurs à la recherche d'informations.
Dans ce travail, nous étudions ce phénomène et montrons que quel que soit le syndrome recherché, les résultats conduisent toujours à l'énoncé des mots "cancer" ou "tumeur". Nous générons pour cela des syndromes en associant des symptômes à différentes parties du corps et analysons les résultats du moteur de recherche et le réseau de pages web sous-jacent (Watts, 2004). Des travaux proches ont par exemple été menés par M. Godwin sur les forums de type Usenet (Godwin, 1994), pour montrer que plus une discussion s'étend dans le temps et plus la probabilité d'y trouver une comparaison impliquant des analogies extrêmes s'approche de un.
La Section 2 détaille notre objectif et la méthodologie suivie. La Section 3 présente les résultats expérimentaux. La Section 4 conclut l'article et présente nos travaux futurs.
Objectif et méthodologie
Notre objectif est d'étudier, pour une recherche donnée, la probabilité qu'apparaissent les mots "cancer" ou "tumeur" en analysant les résultats renvoyés par Google. Deux études sont ainsi menées : (i) La première vise à évaluer le nombre de cliques nécessaires à l'apparition de ces termes si les résultats sont consultés dans leur ordre d'apparition. (ii) La seconde, plus générale, vise à dépasser l'aspect temporel de la navigation, afin d'évaluer la probabilité que ces termes apparaissent quelle que soit la chronologie de consultation des résultats. Nous analysons pour cela l'ensemble du réseau de pages web sous-jacent et évaluons la probabilité de présence des termes sur deux niveaux.
La méthodologie que nous avons adoptée est la suivante. Nous générons un ensemble de syndromes en associant 5 symptômes à 10 parties du corps humain :
-Symptômes : Douleur, Irritation, Tache, Grosseur, Saignement -Parties du corps : Tête, Yeux, Nez, Langue, Ventre, Bras, Main, Sexe, Jambe, Pied 50 requêtes sont ainsi obtenues par combinaisons de symptômes et de parties du corps. Nous effectuons ensuite une recherche automatique de ces syndromes à l'aide du moteur de recherche Google et analysons, selon les approches (i) et (ii), les sites renvoyés. Dans cette première approche du travail, nous nous intéressons uniquement à la présence des termes et ne prenons pas en compte le contexte dans lequel ils apparaissent. On peut en effet supposer que leur seule présence peut avoir un impact psychologique fort chez l'utilisateur qui se sait malade.
Résultats expérimentaux 3.1 Nombre de cliques
Dans l'hypothèse où l'utilisateur parcourt les résultats les uns après les autres, la question est de savoir combien de cliques seront nécessaires pour parvenir à une page affichant les mots "cancer" ou "tumeur". En analysant l'ensemble des résultats renvoyés par le moteur de recherche, nous avons identifié le nombre de cliques nécessaires pour atteindre ces termes. La Figure 1 montre la distribution du nombre de cliques obtenus avec les différentes requêtes. 
FIG. 1 -Distribution du nombre de cliques nécessaires pour les différentes requêtes
Nous pouvons observer que la majeure partie des requêtes effectuées conduit à des sites présentant les mots "cancer" ou "tumeur" en très peu de cliques. En moyenne, quel que soit le syndrome, 2.28 cliques sont nécessaires pour que l'utilisateur soit dirigé vers une page affichant ces termes. Si ce nombre parait très faible, il s'explique par le fait que les sites placés en tête de liste sont souvent des sites à très grandes audiences, affichant énormément d'informations tels que des menus, des forums ou des sujets connexes, augmentant ainsi la probabilité que les mots recherchés y soient présents.
Vers une analyse réseau
Pour aller plus loin dans notre étude, nous nous sommes intéressés dans un second temps à la probabilité qu'apparaissent les mots "cancer" ou "tumeur" quel que soit l'ordre de consultation des résultats. Pour cela, nous collectons l'ensemble des résultats renvoyés et générons le réseau de pages web sous-jacent (Barabasi, 2002;Borner et al., 2007) sur deux niveaux (résultats directs et page directement liées). La figure 2 montre un exemple de ce réseau.
FIG. 2 -Exemple du réseau de pages web généré à partir de la recherche "douleur tête"
Ainsi, en générant et analysant ce réseau pour chacune des requêtes, nous avons calculé la probabilité de présence des mots "cancer" ou "tumeur" aux deux niveaux du réseau, c'est-à-dire le pourcentage de pages présentant au moins l'un des termes à chacun des niveaux. La Figure 3 montre la distribution de cette probabilité (a) au niveau 1 et (b) au niveau 2.
Comme attendu, on observe qu'un fort pourcentage de requêtes a donné lieu à des taux de probabilité de présence relativement élevés au premier niveau du réseau (cf Figure 3(a)). La moyenne des résultats obtenus pour ce premier niveau est de 0.47. Ainsi, quels que soient les syndromes recherchés, la probabilité qu'apparaissent les mots "cancer" ou "tumeur" dès les premiers résultats renvoyés par le moteur de recherche est de 0, 47.
L'observation la plus intéressante concerne les résultats obtenus pour le niveau 2 du réseau (cf Figure 3(b)). Nous pouvons en effet observer que globalement, la probabilité que soient pré-sents les mots "cancer" ou "tumeur" ne varie pas radicalement au second niveau. La moyenne des résultats obtenus pour ce second niveau est de 0.39. Quels que soient les syndromes recherchés, la probabilité que soient présents les mots "cancer" ou "tumeur" dans les pages citées par les résultats du moteur de recherche est de 0.39. Nous pouvons expliquer cette différence par le fait que les pages web font souvent référence à des sites qui ne sont pas nécessairement liés sémantiquement au sujet abordé. Les liens publicitaires, les liens vers les sponsors ou les 
Conclusion et travaux futurs
Nous avons montré que quel que soit le syndrome recherché sur Internet, il existe toujours dans le réseau de pages web sous-jacent des pages énonçant les mots "cancer" ou "tumeur". Dans ce travail préliminaire, nous avons considéré uniquement la présence du mot sans prendre en compte l'environnement dans lequel il intervient. L'étude pourra être complétée à la fois en tenant compte du contexte et en éliminant les cas de faux positifs qui peuvent être déclenchés par de nombreux éléments parasites sur les pages.

Introduction
Le clustering permet l'exploration d'ensembles de données en les résumant sous la forme de groupes homogènes plus facilement caractérisables et interprétables. Récemment, de nouveaux algorithmes ont été proposés pour répondre aux problèmes de traitement des grands volumes ou des flux de données (Aggarwal et al., 2003;Cao et al., 2006;Philipp Kranen et Seidl, 2011). Ces méthodes reposent généralement sur des algorithmes qui ne réalisent qu'une seule passe sur les données initiales. Ceux-ci ne sont malheureusement applicables qu'à des données vectorielles, pour lesquelles des structures incrémentales de description des clusters existent (Zhang et al., 1996).
Dans ce travail, nous nous intéresssons au cas général où les données ne sont pas nécessai-rement vectorielles, et où il n'est donc pas possible d'utiliser de telles structures. Une solution consiste à résumer chaque cluster par un sous-ensemble des données qui le compose, possiblement un seul point, appelé médoide, qui est le plus similaire aux autres données du cluster. Le problème est que la détermination des médoides, et donc l'affectation d'un nouveau point aux clusters existants dans un contexte incrémental, possède une complexité quadratique avec le nombre de données. Cela n'étant pas envisageable dans des cas d'usage réels, les algorithmes implémentent généralement des mécanismes d'échantillonnage pour réduire le coût des calculs de l'appartenance à un cluster. Cependant, échantillonner implique l'introduction d'une incertitude dans la représentation du cluster, qui, dans le cas d'un algorithme en une passe, peut se traduire par une erreur d'affectation d'un point à un cluster, et qui est aggravé par le fait que cette donnée peut ensuite être, à tort, utilisée pour représenter le cluster auquel elle appartient.
Dans ce papier, nous proposons une méthode stochastique d'affectation d'un point à un cluster, qui s'inspire des principes des inégalités de concentration en mettant en oeuvre des bornes théoriques qui vont estimer la distance réelle d'un point à chaque cluster et ainsi gérer l'incertitude liée à l'échantillonnage. Nous comparons ici trois bornes théoriques : Bernstein, Hoeffding et Student. Nous proposons également de réduire artificiellement ces bornes à l'aide d'un pourcentage pour en accélérer la convergence, mais au prix d'erreurs plus nombreuses.
Les résultats expérimentaux sur des jeux de donnés artificiels ou réels issus du répertoire UCI Machine Learning Repository visent ici à évaluer l'accélération du processus de clustering en une passe tout en comptabilisant les erreurs d'affectation par rapport à un algorithme exhaustif, mais ne s'intéressent pas à évaluer la qualité de la partition obtenue en tant que telle par les indices habituels (Rand, erreur de confusion). Nos résultats montrent que les meilleurs performances sont obtenues par la borne de Bernstein qui offre dans tous les cas le meilleur ratio "nombre de comparaisons entre données par nombre d'erreurs observées". Les expéri-mentations montrent par ailleurs que la réduction des bornes théoriques permet d'en améliorer les performances en pratique.
Cet article est organisé comme suit : la section 2 présente un état de l'art succinct des principales méthodes d'échantillonnage utilisées pour l'accélération du clustering de données non vectorielles et leurs limites. La section 3 décrit les principes généraux de notre méthode de sélection de cluster et décrit les différentes bornes théoriques ainsi que leurs variantes ré-duites. Ensuite, la section 4 présente les résultats comparatifs expérimentaux entre la méthode exhaustive, qui est considérée comme la vérité terrain, et les approches basées sur les bornes théoriques et réduites sur l'ensemble des bases de tests. Finalement, la section 5 présente les conclusions et les perspectives de ce travail.
Méthodes d'échantillonnage pour le clustering de données non vectorielles
Deux approches principales existent pour le traitement de données non vectorielles (Hammer et Hasenfuss, 2007) : les approches basées sur des médoides qui limitent les coordonnées des centres des clusters à des exemples du jeu de données, et les approches basées sur des données relationnelles qui travaillent directement à partir des matrices de distances ou de (dis)similarité. Dans les deux cas, des méthodes d'échantillonnage ont été proposées pour réduire la complexité quadratique de leur résolution.
Ainsi, l'algorithme CLARA (Kaufman et Rousseeuw, 1990) réduit la complexité en échan-tillonnant alétoirement l'ensemble du jeu de données. D'autres méthodes, comme CLARANS (Ester et al., 1995) ou CURE (Guha et al., 1998) limitent la recherche des candidats mé-doides aux voisins des médoides actuels, tout comme la méthode floue Linearized Fuzzy CMedoids (Krishnapuram et al., 1999) qui utilise pour cela les degrés d'appartenance des points aux clusters. D'autres méthodes, comme l'algorithme Leader Ant (Labroche, 2006) remplace la détermination exhaustive du médoide par un nombre fixé réduit de comparaisons aléatoires avec chaque cluster.
Dans (Zhu et al., 2012), les auteurs présentent des mécanismes d'accélération pour les données relationnelles de type approximation de Nyström, visant à réduire la dimension de la matrice de distance, ou de type "patch processing", ne considérant qu'un échantillon carré de la matrice de distances à la fois pour traiter de grands volumes de données.
Cependant, pour toutes ces méthodes, le taux d'échantillonnage est un paramètre, qui est non seulement difficile à déterminer a priori car il manque de sens, mais il est également le même tout au long du processus de clustering indépendamment par exemple de la taille des clusters ou de la complexité de leur forme. Pour éviter cela, (Domingos et al., 2001) proposent une variante de k-means, limitée aux données vectorielles, qui utilise une borne de Hoeffding (Maron et Moore, 1994) pour minimiser le nombre de données nécessaires à la détermi-nation des centres de chaque cluster, tout en garantissant que l'erreur commise reste bornée pour un taux d'erreur fixé.
Similairement, l'idée de ce papier est de proposer un cadre général pour accélérer les algorithmes de clustering en une passe, dans le cas de données non nécessairement numériques, à l'aide d'un mécanisme de mise en compétition des clusters reposant sur des bornes théoriques, qui va permettre de gérer l'incertitude sur les distances issue de l'échantillonnage.
3 Accélération des algorithmes de clustering en une passe 3.1 Sélection des clusters dans les algorithmes en une passe Dans le cas particulier des algorithmes de clustering en une seule passe, les données sont considérées séquentiellement et l'affectation à un cluster dépend uniquement de l'estimation de la distance de cette donnée aux clusters existants. Bien sûr, l'ordre des données a une influence directe sur la partition produite par ces méthodes. Généralement, celles-ci sont paramétrées à l'aide d'un seuil de distance qui est utilisé pour décider si une donnée est suffisamment proche des clusters existants pour en intégrer un ou bien si elle doit initier son propre cluster. Lorsque plusieurs (éventuellement tous) les clusters sont éligibles, le problème revient à déterminer le cluster qui optimise le mieux la fonction objectif de l'algorithme de clustering. Ce problème est encore plus compliqué lorsque la distance est estimée à partir d'un échantillon des données de chaque cluster comme dans ce travail.
Nous proposons un mécanisme de sélection des clusters inspiré du mécanisme de compétition ou "racing" introduit par (Heidrich-Meisner et Igel, 2009) qui peut s'appliquer au problème de clustering pour adapter automatiquement la taille de l'échantillon nécessaire à l'affectation d'un point à un cluster sur la base d'une erreur maximale tolérée d'affectation. Ce faisant, nous pouvons simultanément accélérer les algorithmes de clustering en une passe, en limitant le nombre de calculs de distances entre une nouvelle donnée et les données déjà classées, et également garantir une borne supérieure sur l'erreur d'affectation par rapport à un algorithme exhaustif qui réaliserait toutes les comparaisons possibles, sous l'hypothèse que toutes les comparaisons sont indépendantes. En pratique, comme le montre nos expérimenta-tions, et bien que l'hypothèse d'indépendance ne soit pas nécessairement vérifiée, cela conduit à réduire drastiquement le nombre de comparaisons nécessaires tout en limitant les erreurs à des niveaux très faibles.
Méthode statistique de compétition (racing) entre clusters
La technique de "racing" est un outil qui permet de prendre une décision dans le cas d'une incertitude résultant d'au moins deux variables aléatoires ayant un recoupement partiel de leur intervalle de confiance, étant donné un taux d'erreur fixé. L'idée du racing est que la comparaison entre deux (ou plus) variables aléatoires peut être affinée lorsque plus de réalisations des variables aléatoires sont observées. Avec peu de réalisations, la variance est très large et la plupart des distributions attachées aux variables aléatoires se recoupent. Lorsque plus d'observations sont réalisées, la variance décroît et il existe un moment où les distributions se séparent (exception faite de distributions exactement identiques ou s'il n'y a pas assez de données à échantillonner pour chaque variable aléatoire). Après un certain nombre d'observations, il devient possible de prendre une décision sur la relation des deux variables aléatoires (plus grand / plus petit en fonction de l'objectif du problème) avec un certain niveau de confiance, qui correspond à la borne supérieure de l'erreur qui est tolérée. Sur la base des relations estimées (plus petit / plus grand) les mauvais clusters candidats sont éliminés plus tôt, même si la variance demeure assez grande. Cela conduit à la concentration des efforts de calcul sur les meilleurs candidats (Horvitz et Zilberstein, 2001;Beyer et Sendhoff, 2007a,b).
Plus formellement, dans notre algorithme de clustering en une seule passe, nous représen-tons la distance estimée entre le point actuel et chaque cluster i par une variable aléatoire X i . Comme déjà indiqué, nous faisons par la suite l'hypothèse naïve que les X i sont indépendantes. La distance entre la donnée et le cluster est supposée comprise entre deux bornes a et b, à partir desquelles il est possible de calculer une étendue (ou "range" en anglais) R = |a ? b|. Nous supposons également que nous connaissons un seuil de confiance noté 1 ? p, et où p ? [0, 1] est la probabilité d'erreur. Nous proposons ci-après trois méthodes principales pour estimer les bornes de l'intervalle de confiance associé à chaque variable aléatoire. De plus, nous proposons de modifier l'expression théorique des bornes en introduisant un facteur de réduction r ? [0, 1] pour les rendre plus strictes au besoin. Il est intéressant de noter que si r = 1, on retrouve l'expression exacte des bornes théoriques.
La première borne est la borne de Hoeffding (Maron et Moore, 1994) :
où X i,n représente la moyenne empirique des distances au cluster X i après n comparaisons et est définie comme suit :
, où E(X i ) désigne la distance réelle au cluster X i , et l'erreur de probabilité p indique les chances que la distance réelle soit hors des bornes.
La seconde borne est la borne plus récente de Bernstein (Heidrich-Meisner et Igel, 2009) qui repose sur l'écart-type empirique
n Bien que la borne de Bernstein soit connue pour être plus stricte que la borne de Hoeffding (Audibert et al., 2007;Mnih et al., 2008) et doit donc conduire à accélérer davantage le processus de compétition entre clusters, nous proposons de comparer le comportement des deux sur nos jeux de test.
Cependant, comme le montre les équations précédentes, une des limitations potentielles des bornes de Hoeffding et Bernstein est que l'étendue R est un paramètre nécessaire au calcul des valeurs de ces bornes. Même si les espaces de description des données sont souvent bornés, ce qui permet de déduire la valeur du paramètre R, dans de nombreux cas il n'est pas possible de connaître cette valeur a priori, ou il est peut-être trop complexe de le calculer exhaustivement, ce qui ferait perdre le gain de notre approche par ailleurs. Pour toutes ces raisons et également pour accélérer les calculs en resserrant la borne en la contraignant plus, nous proposons d'éva-luer également la borne de Student qui est indépendante de l'étendue des données, et qui fait l'hypothèse que les distances aux clusters suivent une loi normale de variance inconnue.
désigne l'estimateur non biaisé de la variance de la distance au cluster X i .
Implémentation de la méthode de compétition
Algorithme 1 Algorithme de clustering en une passe avec compétition (X, D, T ) Entrée : X : jeu de données, D : matrice de distances, T : seuil de distance pour la construction d'un nouveau cluster Sortie : P : partition de sortie du jeu de données X 1: initialiser l'ensemble des clusters C = ? 2: Pour Tout x ? X Faire 3:
déterminer le meilleur cluster c w pour x en utilisant le mécanisme de compétition de l'algorithme 2 4: affecter x à c w ssi X cw ? T 5: Fin Pour 6: Retourner la partition calculée à partir de C L'algorithme 1 détaille le schéma global de notre méthode de clustering : à chaque itéra-tion, un nouvel objet est considéré et les clusters existants sont mis en compétition par le biais de l'algorithme 2. La compétition permet de filtrer graduellement l'ensemble des clusters candidats, jusqu'à ce qu'il ne reste plus qu'un seul candidat, ou bien qu'il n'y ait plus de données pour affiner la prise de décision. En effet, à chaque fois que la borne inférieure sur la distance empirique à un cluster est plus grande que la borne supérieure du meilleur cluster actuel, il est supprimé de la compétition. À l'opposé, si la borne supérieure d'un cluster est plus petite que la borne inférieure du vainqueur actuel, celui-ci le remplace et devient le nouveau meilleur cluster. Enfin, lorsqu'il n'y a pas de différences singitificatives entre les clusters restants, le vainqueur final de la compétition est celui qui minimise sa distance empirique moyenne avec la donnée. À l'issue de la compétition, l'objet est affecté au cluster vainqueur c w si sa distance X cw est inférieure à un seuil T passé en argument. Dans le cas contraire, la donnée construit un nouveau cluster.
Algorithme 2 Algorithme de compétition entre clusters (x, C, D) Entrée : x : objet du jeu de données X, C : ensemble des clusters existants, D : matrice de distances Sortie : c w : indice du cluster qui remporte la compétition 1: initialiser le cluster vainqueur c w = ? 2: Tant Que la compétition n'est pas finie Faire
3:
Pour Tout clusters c ? C Faire 4:
selectionner aléatoirement une nouvelle donnée x c dans le cluster c
5:
mettre à jour la distance moyenne empirique pour le cluster c avec la distance D(x, x c ) ainsi que les bornes [inf c , sup c ] pour le cluster c
6:
Si sup c < sup cw ou c w == ? Alors
7:
mettre à jour le cluster vainqueur c w = c 8:
Fin Si
9:
Fin Pour 10:
supprimer tous les clusters c ? C encore en compétition tels que inf c > sup cw
11:
Si |C| < 2 Alors
12:
Retourner c w
13:
Fin Si 14: Fin Tant Que 15: Si la compétition se termine sans différence significative entre les clusters de C Alors
16:
Retourner le cluster c ? C qui minimise la distance moyenne empirique 17: Fin Si 4 Résultats expérimentaux Nous présentons ici les résultats comparatifs entre la méthode de détermination exhaustive du cluster le plus proche et notre méthode de compétition basée sur des bornes théoriques. Nous discutons ensuite l'influence des paramètres de la méthode sur son efficacité.
Protocole expérimental
Les résultats sont présentés pour différentes valeurs du facteur de réduction r comprises entre 0 et 1 (quand r = 1 on se trouve dans le cas de la borne théorique originale), pour une probabilité d'erreur fixée pour l'ensemble des tests à 0.1 et une valeur du range R exacte. La valeur du seuil qui détermine si un nouveau cluster doit être construit ou non est estimée, similairement à ce qui a été proposé pour l'algorithme Leader Ant (Labroche, 2006), comme la moyenne des distances calculée sur un échantillon aléatoire d'une taille égale à 10% de l'effectif total du jeu de données. Enfin, du fait du calcul de la partition de manière exhaustive, seulement 5 tests ont été réalisés pour chaque jeu de données et chaque valeur du facteur de réduction de borne r.  (Asuncion et Newman, 2007).
Évaluation de la qualité des résultats : l'objectif de notre évaluation n'est pas de détermi-ner la qualité de la partition de manière classique (indice de Rand . . .), car, dans notre cas, la référence est donnée par la qualité de la partition de la méthode exhaustive. Nos expérimenta-tions visent donc à montrer l'impact de notre méthode selon deux dimensions principalement : l'accélération par la réduction du nombre de comparaisons et le nombre d'erreurs par rapport à la méthode exhaustive. Nous n'évaluons pas, dans ce travail, les temps de calcul des diffé-rentes méthodes pour mesurer l'accélération, car nous souhaitons nous affranchir des optimisations d'implémentation liées au langage choisi (Java) et du contexte d'exécution (bibliothèques liées).
De façon à pouvoir comparer identiquement au cours du temps nos méthodes basées sur les bornes, s'affranchir des éventuelles erreurs précédentes, et également déterminer à quel moment surviennent les erreurs d'affectation par rapport à la méthode exhaustive, nos expé-rimentations sont basées sur une mesure d'erreur avec correction de l'affectation à chaque nouvelle donnée traitée. Ainsi, pour chaque point des jeux de données, notre protocole déter-mine le cluster idéal à l'aide de la méthode exhaustive puis prédit le cluster qui serait choisi pour une affectation avec chacune des bornes. En cas de différence entre le cluster prédit et le cluster idéal, une erreur est comptabilisée pour la borne concernée. Dans tous les cas, le point est affecté au cluster idéal.
Jeux de données : du fait de l'utilisation de la méthode exhaustive qui a une complexité quadratique comme référence pour notre mesure d'erreur, nos tests ont été effectués sur des jeux de données d'une taille intermédiaire, permettant de finir le calcul en un temps raisonnable, tout en donnant une idée du comportement des méthodes sur de grands jeux de données (notamment l'accélération). De plus, de façon à varier les difficultés, les tests ont été conduits d'une part sur des données artificielles générées à l'aide de distributions de données normales avec un recouvrement plus ou moins important entre les groupes, et d'autre part sur des données réelles issues du UCI Machine Learning Repository (Asuncion et Newman, 2007). Le détail des jeux de données retenus est présenté dans le tableau 1 qui indique pour chacun son nombre d'objets (n), sa dimensionalité (nombre d'attributs n att ) et le nombre de clusters k attendus. comme le montre la figure 1. La borne de Bernstein permet d'obtenir dans tous les cas de meilleurs résultats que la borne de Hoeffding à la fois en terme de nombre de comparaisons et de nombre d'erreurs. Cela est probablement dû à l'utilisation de la variance empirique qui permet de resserer un peu plus la borne de Bernstein par rapport à celle de Hoeffding. La force de cette amélioration dépend du jeu de données mais l'exemple de la figure 1 est représen-tatif avec un nombre de comparaisons inférieur d'environ 30% pour Bernstein par rapport à Hoeffding.
Résultats comparatifs
La borne de Student est plus contrainte que les précédentes car elle fait l'hypothèse d'une distribution normale des données. Elle réalise ainsi beaucoup moins de comparaisons en géné-ral, mais au prix d'un nombre d'erreurs beaucoup plus élevé. En effet, sur nos jeux de données de tests, les bornes de Bernstein et Hoeffding, dès lors que le facteur de réduction r ? 0.25 ne génèrent quasiment plus aucune erreur par rapport à la méthode exhaustive, alors que la borne de Student, même non réduite (r = 1) commet des erreurs (voir la colonne de droite de la figure 1).
Enfin, l'accélération augmente avec le nombre d'itérations. Plus le nombre de données déjà traitées augmente, meilleures sont les estimations, et donc plus les clusters candidats sont rapidement éliminés de la compétition, ce qui accélère le processus. Enfin, d'autres tests non rapportés dans cet article, suggèrent que le mécanisme d'accélération est également possible et bénéfique pour de petits jeux de données (comme Iris par exemple).
Discussion autour du paramétrage
L'accélération basée sur les bornes théoriques présentées dans la section 3 admet comme paramètre la probabilité d'erreur p. Du point de vue du problème de clustering, cette erreur p indique qu'il y a une probabilité non nulle qu'un mauvais cluster soit retenu. L'erreur d'affectation est donc supposée être inférieure à cette probabilité p. En pratique, comme le montre la figure 2-Haut, sur nos données de test nous observons que l'augmentation de la probabilité d'erreur réduit le nombre de comparaisons. En revanche, seule la borne de Student voit son erreur d'affectation augmenter, les bornes de Berstein et Hoeffding ne générant pas d'erreurs avec la borne originale (r = 1). Ce résultat doit encore être étudié, mais nous pensons pour le moment que cela est dû au fait que les bornes de Bernstein et Hoeffding sont lâches et se retrouvent par conséquent souvent dans un cas de décision ambigue entre plusieurs clusters candidats, l'affectation se faisant alors généralement au cluster correct sans garantie, mais après avoir éliminé plusieurs candidats.
Les bornes de Bernstein et Hoeffding nécessitent également de connaître à l'avance l'éten-due (ou "range" R) des distances entre données. La figure 2-Bas illustre le comportement observé sur nos jeux de données à l'aide du jeu Art 1 lorsque l'étendue R est multiplié par des puissances de 2. On observe dans ce cas que la borne de Bernstein résiste mieux à une surestimation de l'étendue que la borne de Hoeffding, mais que dans tous les cas, il est toujours possible d'accélérer les calculs même avec une étendue 2 fois supérieure à ce qui était prévue initialement.
FIG. 2 -Haut : analyse de l'influence de la probabilité d'erreur p. Haut-Gauche : exemple représentatif avec la borne de Bernstein pour le jeu de données Art 6 , qui montre l'influence de la probabilité d'erreur sur le nombre de comparaisons. Haut-Droite : influence de la probabilité d'erreur p sur le nombre d'erreurs dans le cas de la borne de Student. Bas : influence de la sur-estimation de l'étendue des distances ("range") R en fonction des bornes de Bernstein (gauche) et Hoeffding (droite) pour le jeu de données Art 1 .
Conclusion et perspectives
Ce papier présente une nouvelle méthode de clustering en une passe pour des données non vectorielles, qui repose sur le principe des inégalités de concentration pour définir un méca-nisme de compétition (ou "racing") qui estime la distance d'un nouveau point aux clusters tout en minimisant le nombre de comparaisons nécessaires. Trois bornes, Bernstein, Hoeffding et Student, sont comparées ainsi qu'une version réduite de chacune d'entre elles. Nos résultats montrent que notre algorithme permet de réduire drastiquement le nombre de comparaisons né-cessaires par rapport à une méthode de clustering en une passe exhaustive en pratique, bien que les garanties théoriques des bornes ne puissent être assurées du fait de la possible dépendance des observations. Le facteur de réduction permet d'améliorer encore les résultats observés, notamment pour les bornes de Bernstein et Hoeffding qui sont, par construction, plus lâches que la borne de Student. Nous observons également que cette accélération augmente avec le nombre de données déjà classées, ce qui nous laisse à penser que notre méthode est particulièrement adaptée pour le traitement de grands jeux de données. En conclusion, d'un point de vue général, la borne de Bernstein offre le meilleur compromis entre l'accélération (meilleur que Hoeffding) et le nombre d'erreurs (meilleur que Hoeffding et Student).
Dans le futur, de nouveaux tests doivent être conduits sans la correction des erreurs à partir de la "vérité terrain" de l'approche exhaustive, de façon à évaluer la qualité des partitions ainsi que l'impact des erreurs d'affectation sur les affectations suivantes. Ces tests pourront être conduits sur des données plus grandes, comme des données d'usage sur Internet qui sont produites continuellement et pour lesquelles il n'est pas possible d'utiliser les algorithmes incrémentaux classiques limités aux données numériques. D'autres modèles statistiques (Ustatistics) et d'autres bornes (borne de Serfling) pourront également être envisagés pour obtenir des garanties théoriques.
A plus long terme on pourra s'intéresser à l'utilisation de l'information d'ambiguité, c'est-à-dire les cas où il n'est pas possible de différencier les clusters sur la base des bornes estimées, dans un contexte incrémental pour déterminer notamment : quand un cluster doit être créé, quand plusieurs clusters peuvent être fusionnés ou plus généralement pour identifier les points les moins importants d'un cluster dont l'importance peut en conséquence être réduite.
Summary
Single-pass incremental clustering relies on the efficient assignment of each new data point to one of the existing clusters. In the general case, where it is not necessarily possible to represent the clusters by a mean, the exhaustive assignment of a point a cluster has a quadratic complexity in term of the number of data objects. This paper proposes a novel stochastic assignment method that minimizes the number of comparisons between the new data and each cluster to guaranty, given an acceptable error rate, that the point is assigned to its nearest cluster. Several theoretical bounds are considered (Bernstein, Hoeffding and Student) and compared in this paper. Results observed on artificial and real data sets show that Berntein bound give the overall best results (especially when it is reduced) as it provides the best acceleration of the clustering while maintaining a very low number of errors.

Introduction
Les méthodes de recherche de motifs spatiaux sont utilisées couramment pour construire des caractérisations de données spatiales (voir Selmaoui-Folcher et al. (2013)). Ces approches s'appuient sur des représentations de l'espace tels que des graphes de voisinage ou des chemins construits sur des courbes fractales (par ex. chemins de Hibert-Peano, voir Mari et Le Ber (2006)). Les graphes de voisinages contiennent une information spatiale riche, mais ils sont plus complexes à fouiller, tandis que les chemins sont faciles à fouiller, mais ils réduisent l'information spatiale disponible.
Dans ce travail, nous confrontons ces deux représentations vis-à-vis de la caractérisation d'un parcellaire agricole. En particulier, nous cherchons à savoir si l'approximation par un chemin fractal permet de conserver une bonne caractérisation de l'organisation spatiale des parcelles agricoles en vue de l'application d'une méthode de recherche de motifs. Cette question soulève deux difficultés. La première est l'absence d'étiquetage des données qui permettrait d'évaluer une représentation sur une tâche de classification. Nous nous plaçons donc dans un contexte non-supervisé. La deuxième difficulté porte sur la comparaison des caractérisations obtenues, qui sont de natures différentes. Nous ne pouvons pas nous appuyer comme classiquement sur des calculs de corrélation (par ex. par une matrice de confusion) entre les localisations des motifs dans les chemins et dans les graphes.
Pour résoudre ces difficultés, nous utilisons des « sacs de noeuds » , inspirés des sacs de mots introduits dans le contexte de l'analyse de texte (Salton et al. (1975) Un graphe S G = E G , ? est construit à partir des données selon la méthode décrite par Guyet (2010). Un noeud v ? V est construit pour chaque parcelle, représentée par son barycentre. Chaque noeud v ? V est associé à une occupation du sol. Un arc e ? E G ? V × V lie deux parcelles voisines, c'est-à-dire connexes ou séparées par un faible espace (séparation par une route ou imprécision géométrique des données). Les arcs ne sont pas étiquetés.
Une méthode de calcul du chemin de Hibert-Peano Adaptatif (CHA) a été décrit par Quinqueton et Berthod (1981). Cette méthode a été utilisée par Da Silva (2013) pour extraire les structures spatiales de linéaires agricoles. Elle utilise un ensemble de points spatialement distribués (ici les barycentres des parcelles), qui sont parcourus de manière déterministe. Le chemin est ensuite simplifié pour se ramener à une succession de parcelles, qui est transcrite sous la forme d'une séquence d'occupations du sol. Pour l'unification des notations, un CHA peut être décrit de la même façon qu'un graphe, par S CHA = E CHA , ? avec un noeud par item de la séquence et un arc pour deux items successifs. On peut noter que la séquence S CHA obtenue sur les mêmes données n'est pas un sous-graphe de S G .
Pour atténuer l'influence des paramètres de construction des CHA, nous construisons plusieurs chemins 1) en faisant varier aléatoirement les limites de la cellule initiale autour des limites géographiques qui définissent une zone (3 cadrages aléatoires initiaux) et 2) en géné-rant les chemins pour les 4 directions principales.
La caractérisation des régions par SdN débute par l'énumération de toutes les sous-structures d'une représentation de l'espace. Soit S = E, ? une représentation de l'espace, chemin de Hilbert adaptatif ou graphe. Une sous-structure de S est un triplet , E , ? où
, il existe e ? E tq e soit un arc entre v et u. Dans le cas des CHA, on s'intéresse à des sous-séquences de taille fixe w s . Dans le cas des graphes de voisinage, on s'intéresse à des sous-graphes contenant exactement w g arcs. L'énumération de toutes les structures est possible en temps output-polynomial (Bonzini et Pozzi (2007)), sans seuil de fréquence à fixer. Pour les sous-graphes, nous utilisons l'outil TGE de Uno (2005). Le « sac de noeuds » (SdN) d'une sous-structure s S , noté SdN (s S ) ? 2 O , est un vecteur de présence / absence des types d'occupations du sol dans la sous-structure s S . Nous avons préféré ne conserver que l'information de présence / absence à la place d'un dénombrement pour éviter la multiplication combinatoire des sacs de noeuds.
Comparaison des représentations sur les parcellaires
Le parcellaire a été décomposé en 14 sous-zones aux caractéristiques variées : les zones les plus au nord sont caractérisées par des grands champs de céréales, tandis que les zones au sud correspondent à un secteur bocager constitué de petites parcelles de prairies. La décomposition T. Guyet et al. réduit le nombre de parcelles à prendre en compte pour obtenir des résultats plus rapidement et permet d'étudier le caractère spécifique des motifs d'une zone par rapport à une autre ou le comportement des deux méthodes en relation avec les caractéristiques des zones.
Le nombre total de sacs (de 1 à 4 occupations du sol) distincts s'élève à 475 pour les graphes (toutes zones comprises), 206 pour les chemins : le rapport moyen du nombre de sacs différents trouvés par les graphes et par les chemins dans les différentes zones s'élève à 1,9. Le nombre moyen d'éléments dans les sacs issus des graphes s'élève à 879 (toutes zones confondues), et à 28 pour les sacs issus des chemins.
En considérant ensemble tous les sacs obtenus sur les 14 zones par le CHA d'une part et par le graphe d'autre part, on obtient un indice de corrélation (Spearman) I r (graphes, chemins) = 0, 712, ce qui indique une forte corrélation ; pour le test du ? 2 (avec distribution sous H 0 simulée à cause des faibles valeurs), on obtient I ? (graphes, chemins) = 30094, valeur de p < 0, 0005, soit des distributions très différentes. On observe les mêmes résultats sur les zones prises séparément. Les proportions des sacs estimées par les deux mé-thodes sont donc différentes mais corrélées.
Pour expliciter ces distributions différentes, on s'intéresse maintenant aux sacs oubliés par les CHA. On peut calculer le nombre d'éléments par sac à partir duquel un sac présent dans le graphe disparaît dans le chemin (moyenne sur les 14 zones 1215, 6 ± 855, 9) et le rapporter au nombre maximum d'éléments dans les sacs issus des graphes (81307, 0 ± 64722, 3) ou au nombre moyen (5891 ± 6322, 74). Le premier taux s'élève à 2,1%, le second à 44,6%. Ces chiffres dépendent fortement du nombre de parcelles dans chaque zone et de la diversité des occupations représentées.
Si on regarde plus précisément la répartition des sacs perdus, on observe qu'il s'agit la plupart du temps de « petits » sacs, comptant moins de 100 éléments. Au delà on peut séparer des zones plutôt homogènes -peu de sacs perdus et peu remplis -et des zones plutôt hétérogènes -sacs plus nombreux et plus remplis. Les premières rassemblent des zones de petites parcelles de bocage (prairies très majoritaires) et des zones de grands parcelles cultivées (céréales et maïs majoritaires) : les sacs perdus par la méthode CHA représentent des occupations et des voisinages très minoritaires. Les deuxièmes sont plus diversifiées en taille de parcelles et occupations du sol : les sacs perdus peuvent correspondre à des voisinages relativement fréquents même si non majoritaires.
Discussion et conclusion
La comparaison d'une analyse fondée sur les données extraites par un chemin et d'une analyse sur les données complètes a été réalisée dans le cadre des modèles de Markov en analyse d'images par Benmiloud et Pieczynski (1995). Pour ces modèles, l'analyse fondée sur un chemin, bien que moins ajustée à la réalité des données, s'est montrée pertinente et acceptable en termes de rapidité.
Pour l'étude que nous avons menée, nous aboutissons à une conclusion similaire tout en mettant en évidence certains manquements de la méthode fondée sur le CHA, qui conduit à oublier les motifs rares mis en évidence par la méthode appuyée sur le graphe de voisinage.
Finalement, si on s'intéresse à des voisinages fréquents et à des zones relativement homogènes, la recherche de motifs par linéarisation de l'espace s'avère pertinente et efficace. Le fait que les différences se trouvent principalement sur les motifs rares laisse également espé-

Introduction
Le Web 2.0 favorise le développement des sites collaboratifs, où les utilisateurs échangent des connaissances, se structurent en communautés, développent des codes, des usages et une sémantique qui leurs sont propres. Dans le cadre de la recherche traditionnelle de fouille de la connaissance, cette évolution en relative autonomie peut se révéler problématique : il n'existe aucune garantie que cette dernière se structure autour d'une sémantique qui soit en adéquation avec les bases de connaissance de référence traditionnelles. La pertinence des conclusions peut alors ne pas ou peu refléter l'évolution réelle du comportement des utilisateurs et de la sémantique de leurs échanges.
Nous proposons une méthode pour construire notre propre compréhension des contributions des utilisateurs, basée uniquement sur les données de celles-ci, afin d'extraire la séman-tique des utilisateurs. Nous évaluons cette approche par la mesure d'une valeur de confiance. Nous effectuons notre analyse dans le contexte des recettes de cuisine, dont les sites de partage communautaires sont nombreux et très populaires sur le Web français comme mondial.
État de l'art
La recette de cuisine est un type de données particulier, composé d'un ensemble d'ingré-dients et de procédures d'exécution. Ce type de données est exploité par de nombreux systèmes de recommandation. Le Cooking Assistant (Sobecki et al., 2006) définit un système de recommandation démographique basé sur une inférence à logique floue, efficace pour fournir une réponse globale à un besoin général. Mais la généralisation des caractéristiques conduit à une recommandation également généralisée. Pour prendre en compte la spécificité des ingrédients, Freyne et Berkovosky utilisent la relation de composition qui existe entre ingrédients et recettes pour propager des évaluations et déterminer un comportement utilisateur (Freyne et al., 2011). Cela nécessite néanmoins une phase constante de normalisation, un travail d'expert consistant à vérifier ou annoter les ingrédients afin qu'ils correspondent à une liste de référence.
Les recettes de cuisine ont également été traitées par des approches de raisonnement à partir de cas. Le système CHEF (Hammond, 1986) est un système d'adaptation par la critique, qui permet de prendre en compte la spécificité du type de données qu'est l'ingrédient, en relevant les problèmes découlant d'une substitution. En revanche, une importante phase d'apprentissage est requise. Le système MIKAS (Khan et Hoffmann, 2003) propose de contourner ce besoin par un recours à l'expert. Cette aspect de la transmission de connaissance de l'expert au système par l'expérience plutôt que par le déclaratif est vu comme plus efficace et plus adapté aux conditions réelles. Il ne permet toutefois pas une évaluation indépendante des contenus, car dépendant des connaissances propres de l'expert.
Extraction de l'information et structure a priori
La première étape consiste à extraire l'information depuis des lignes d'ingrédients librement saisies. Pour exploiter ces données, nous définissons la structure a priori comme étant : quantité -unité -ingrédient. En fonction de l'existence d'une valeur dans les champs quantité et unité nous obtenons les classes 1, 2, 3 et 5 du tableau 1, la classe 5 regroupant les lignes où il a été impossible d'extraire de l'information pour les champs quantité et unité. Cette classe est ensuite ventilée par une phase d'apprentissage en deux étapes :
-Recherche des incohérences dans les éléments identifiés : la présence d'un ingrédient complexe en classe 1 permet de mettre en évidence une erreur de détection dans les autres classes. Par exemple, la ligne « 500g de corned beef » permet d'identifier la ligne « corned beef » comme ingredient seul, corrigeant la première identification de « corned » comme quantificateur 1 . -Pour toutes les lignes de la classe 5 restantes, nous cherchons à les faire correspondre aux cas précédemment rencontrés. Dans le cas du jeu de données Marmiton, la phase d'apprentissage réduit la classe 5 de 20 à 1,9 % de la population (figure 1).
Évaluation de la confiance des ingrédients et des recettes
Ordonnant nos ingrédients en fonction de leur fréquence, nous observons sur la fonction cumul des ingrédients un effet longue traîne, phénomène commun à bon nombre de sites sociaux à usages libres. Eu égard à la distribution en loi de puissance de nos données sociales, nous appliquons à notre modèle le principe de Pareto, où 80 % des effets sont le produit de 20 % des causes. Dans le cas où les a ingrédients contenus dans 80 % des lignes représentent moins de 20 % des ingrédients les plus fréquents, nous définissons b = 5a ingrédients comme ensemble représentatif des ingrédients. La valeur de confiance C i est alors attribuée à chaque 1. Une unité sans présence de quantité est appelée quantificateur. 
La figure 2 illustre la confiance ainsi calculée des ingrédients de Marmiton. La valeur de confiance par recette C x est égale à :
où I x est l'ensemble des ingrédients de la recette x et u(i, x) est l'unité associée à l'ingrédient i dans la recette x. µ u(i,x),i est alors la fréquence de l'unité u(i, x) dans les lignes contenant l'ingrédient i et C i la confiance de l'ingrédient i. 
Conclusion et travaux futurs
Nous avons présenté une méthode pour évaluer la confiance d'une publication utilisateur, comme étant la probabilité qu'un autre utilisateur en saisisse la sémantique. Notre approche est indépendante de toute base de connaissance externe, afin de raisonner directement sur les termes manipulés. Cette méthode présente l'avantage de ne pas être dépendant de la langue, ni de souffrir des problèmes de pertinence ou de couverture relatifs aux bases de connaissance. Nous projetons d'exporter la connaissance extraite des contributions utilisateurs, ce qui permettra de définir sans apport extérieur l'ontologie du système analysé, ou d'enrichir une base extérieure. Enfin, l'application de méthode de partitionnement de fouille de données, guidées par nos mesures de confiance, permettra d'évaluer une structure interne de la sémantique du système et des relations déductibles qui existent entre les différents ingrédients (proximité) ou recettes (variantes, alternatives).

Introduction
Un entrepôt de données est une base de données dédiée à l'analyse en ligne pour l'aide à la prise de décision. Grâce aux opérateurs OLAP, l'utilisateur peut extraire des cubes de données correspondants à des contextes d'analyse (Inmon, 1992). Dans un SGBDR, la construction d'un cube OLAP nécessite le calcul d'agrégats à partir des n-upelts stockés en lignes. Ce type de stockage est pénalisé par un coût de jointure important, ce qui implique un temps élevé pour l'extraction et le traitement des données. Par ailleurs, l'architecture orientée colonnes offre un mode de stockage et une technique de traitement plus adéquats au processus analytique en réduisant considérablement les accès au disque (Matei, 2010). Cependant, les SGBD orientés colonnes ne disposent pas d'opérateurs de construction de cube OLAP. L'objectif de ce travail est de proposer une nouvelle approche de construction de cube OLAP. Cette approche utilise un système de stockage et de traitement orienté colonnes pour réduire le temps d'extraction de données et une nouvelle méthode de calcul de cube OLAP. Nous avons implémenté cette approche sous l'SGBDR orienté colonnes MonetDB 2 Construction de cube OLAP L'idée sous-jacente est d'extraire les données constituant le cube une seule fois de l'entrepôt de données stocké en colonnes, et d'appliquer ensuite un ensemble de traitements pour calculer tous les agrégats possibles, à différents niveaux de granularité. Une fois le cube est calculé, il est matérialisé suivant l'architecture orientée colonnes pour permettre à l'analyse OLAP de bénéficier des avantages de cette architecture quant à la manipulation du cube (Slice, Dice, Drill-Down, Drill-Up, ..).
L'approche que nous proposons s'appuie sur une architecture orientée colonnes pour l'entreposage des données et un un nouveau algorithme de calcul de cube OLAP qui parcourt le treillis de cuboïdes et exploite la relation originelle. A la différence des algorithmes de la deuxième approche de construction de cube OLAP dans les bases de données orientées lignes, où le treillis de cuboïdes est créé au fur et à mesure suivant un plan d'exécution, notre méthode crée le treillis de cuboides global par un pré-traitement et le croise avec les données extraites de l'entrepôt (relation originelle). Cela permet de diminuer le nombre d'opération de calcul. Plus précisément, cette approche est exécutée en trois phases :
Phase 1 : Extraction des données Cette phase consiste à extraire les données à partir de l'entrepôt de données. Le résultat de cette phase est une relation R composée de valeurs de dimensions et de la mesure qui satisfont les prédicats de la requête. Avec un entrepôt de données implémenté suivant l'approche orientée colonnes, cette phase est plus performante qu'avec un entrepôt de données implémenté suivant l'approche orientée lignes. En effet, le mécanisme de stockage de l'architecture orientée colonnes permet un accès rapide aux données des différentes colonnes et diminue considéra-blement le coût de jointure comparé à celui de l'architecture orientée lignes.
Phase 2 : Construction du treillis de cuboïdes Cette phase consiste à construire le treillis de cuboïdes et définir les combinaisons possibles du cube à calculer.
Phase 3 : Calcul et matérialisation du cube Cette phase consiste à parcourir le treillis de cuboïdes pour calculer le cube en fonction de la relation obtenue de la première phase, ainsi, le cube est matérialisé colonne par colonne
Expérimentations
Nous avons implémenté le modèle benchmark en étoile (SSBM) K. Dehdouh et al.
Calcul du cube OLAP : Cette expérimentation consiste à comparer le temps de construction du cube OLAP en utilisant les deux fonctions ; Group by CUBE et ROLLUP, selon notre approche avec celui de l'approche classique (orientée lignes). Nous exécutons quatre requêtes décisionnelles de construction de cube OLAP avec un nombre de dimensions qui augmente progressivement. Les résultats obtenus sont présentés dans la figure 1.
Manipulation du cube OLAP : Cette expérimentation est consacrée à l'évaluation du temps relatif à la manipulation du cube OLAP matérialisé selon notre approche avec celui de l'approche classique. Pour cela, nous avons construit, selon les deux approches, un cube OLAP à trois dimensions et qui répond à la requête décisionnelle suivante : Quelle est la somme des revenus des ventes par année, par marque de produit et par région des clients. Nous avons ensuite exécuté des opérations de forage (slice et dice) sur le cube. Les résultats que nous avons obtenus sont présentés dans la figure 2. 
Résultats des expérimentations
Summary
This paper presents a new method of calculating the OLAP cube. The results obtained from experiments carried out has shown that our approach significantly optimizes the OLAP cube building time and reduced the response time of cube manipulation, compared to the traditional approach.

Introduction
Le besoin d'incorporer les préférences aux requêtes dans les technologies de l'information est un verrou crucial pour une grande variété d'applications allant de l'e-commerce aux moteurs de recherche personnalisés. Un utilisateur accédant à un système d'information peut avoir à reformuler plusieurs fois sa requête pour éliminer les résultats insatisfaisants et cheminer vers le résultat attendu. En particulier, cette expérience est très fréquente avec les recherches sur le Web en raison d'une abondance d'information et surtout, de l'hétérogénéité des utilisateurs. Une observation cruciale alors est que « différents utilisateurs considèreront comme pertinent des résultats différents » car leurs préférences divergent.
Cependant, la construction manuelle de modèles de préférences par l'utilisateur reste à la fois complexe et consommatrice de temps. De ce fait, l'apprentissage automatique de ses préférences, en s'appuyant sur les interactions entre lui et le système, joue un rôle critique dans de nombreuses applications. Ces interactions appelées, feedbacks utilisateurs implicites, ont une forme souvent rudimentaire indiquant si un utilisateur a réalisé une action particulière sur un objet (par exemple, le temps passé sur un objet). Ces feedbacks implicites sont souvent ambigus et d'une granularité peu fine mais ils sont plus faciles à obtenir en abondance dans un système réel. Dans cet article, nous faisons l'hypothèse que nous disposons d'un ensemble d'objets et d'un ensemble de paires de préférences sur ces objets, c.-à-d., tel objet est préféré à tel autre. Notre objectif sera de construire un modèle de préférences à partir de ces données même si de telles préférences sont moins subtiles que des notes et peuvent parfois contenir des inconsistances.
Dans cet article, nous présentons une approche de construction de profils de préférences contextuelles basée sur l'extraction de motifs séquentiels, nommée Sprex (Sequence-patternbased preference rule extraction). Cette approche se constitue de trois composants principaux : un extracteur de motifs séquentiels pour l'extraction des règles de préférence contextuelle, une fonction de modélisation qui permet de trier et de sélectionner un sous-ensemble de règles de préférence contextuelle afin de créer un profil, et une fonction de préférence qui permettent de prédire la préférence de l'utilisateur en utilisant le profil construit. L'approche Sprex étend l'expressivité des règles de préférence contextuelle par rapport à Agrawal et al. (2006). Auparavant, de telles règles permettaient seulement de préférer un item par rapport à un autre. Notre proposition permet de préférer un ensemble d'items par rapport à un autre. Ce gain en expressivité des règles permet de construire des profils enrichis plus proches des préférences de l'utilisateur. En outre, la réutilisation des outils d'extraction de motifs séquentiels fréquents permet à l'approche Sprex de générer les profils de préférences avec une très grande efficacité. L'étude expérimentale montre la rapidité de l'approche mais aussi son efficacité en terme de prédiction.
L'article est organisé de la manière suivante. La section 2 définit les notions préliminaires. La section 3 introduit les travaux antérieurs associés à notre problématique. La section 4 pré-sente l'approche Sprex. Pour cela, nous définissons formellement la notion de séquences de préférence et ensuite introduisons globalement notre approche ; enfin, nous présentons égale-ment les méthodes Sprex-Build et Sprex-Predict qui composent l'approche Sprex. La section 5 rapporte les expérimentations que nous avons menées sur des jeux de données réels, avant de conclure et de présenter les perspectives associées dans la section 6.
Définitions préliminaires et problématique
Nous définissons les notions préliminaires liées à notre problématique et à notre approche dans cette section.
Soit R = {R 1 , R 2 , . . . , R n } un ensemble d'attributs. Pour chaque attribut R i ? R, nous notons le domaine de valeurs de
Ri?R dom(R i ) l'ensemble de toutes les valeurs d'attributs, chaque valeur i ? I est un item. Soit T = {i 1 , i 2 , . . . , i n } une transaction, chaque sous-ensemble I ? T est un itemset. Une base de transactions est un ensemble de transactions, chacune associée à un identifiant unique. Une paire de transactions, notée 1 , T 2 est un vecteur de deux transactions tel que 1 , T 2 = 2 , T 1 Dans cet article, nous modélisons les préférences de l'utilisateur comme un ordre partiel sur les itemsets tel que X Y indique que l'utilisateur préfère l'itemset X à l'itemset Y . Définition 1 (Règle de préférence contextuelle étendue). Une préférence contextuelle est une règle de la forme C ? X Y décrivant que l'utilisateur préfère l'itemset X à l'itemset Y si le contexte, représenté par l'itemset C, est observé.
Cette définition étend la notion de règle de préférence contextuelle utilisée par Agrawal et al. (2006) et de Amo et al. (2012. En effet, la définition usuelle se limite à des itemsets de taille 1 pour X et Y . En d'autres termes, l'expressivité de nos règles de préférences contextuelles est plus forte. Exemple 1. La règle de préférence {viande} ? {carotte} {riz} exprime qu'un utilisateur préfère des carottes au riz pour accompagner de la viande, et la règle de préférence {viande, samedi} ? {carotte, vin} {riz, soda} précise que le samedi, cet utilisateur préfère accompagner sa viande avec des carottes et du vin plutôt que du riz et du soda.
Intuitivement deux transactions sont comparables suivant la règle C ? X Y si les deux contiennent C et si une seule des deux contient X et l'autre Y . De manière formelle, étant données une règle de préférence contextuelle r = C ? X Y et une paire de transactions
Une préférence de l'utilisateur est une paire de transactions U ? P qui spécifie que l'utilisateur préfère T à U , également noté T U . Soit D une base de transactions décri-vant des objets (p. ex. des films), une base de préférences de l'utilisateur P ? D × D est un ensemble de paires de transactions correspondant à un échantillon des préférences de l'utilisateur sur les objets de D. À partir d'une base de préférences utilisateurs P, nous définissons le support d'une règle de préférence contextuelle r, noté supp P (r), comme le nombre de paires de transactions qui supportent r ; nous définissions également la confiance de la règle r, notée conf P (r), comme le ratio du nombre de paires de transactions qui supportent r sur le nombre total de paires de transactions qui supportent ou contredisent r. On a alors supp P (r) = |{p ? P | r + p}| et conf P (r) = |{p ? P | r + p}| / |{p ? P | (r
Définition 2 (Règle de préférence contextuelle minimale). Une règle de préférence contextuelle r = C ? X Y est minimale par rapport à une base de préférences utilisateurs P si et seulement s'il n'existe aucune règle r
Un modèle de préférences sur une base de préférences utilisateurs P est un ensemble trié de règles de préférences contextuelles minimales, noté M P . Étant donnée une base de préférences d'un utilisateur, on dit qu'un modèle de préférence est un profil de préférences de cet utilisateur. Le problème de la construction de profil d'un utilisateur est ainsi de construire un modèle de préférences à partir d'une base de préférences de l'utilisateur. 
Travaux antérieurs
Représentation séquentielle des préférences
Soit I l'ensemble de tous les items, un item de préférence est une paire L:i où L ? {C, P, N} est un label (parmi Contexte, Préferé et Non-préféré) et i ? I est un item. Étant données deux transactions T et U telles que T U , l'itemset contextuel est un ensemble d'items de pré-férence {C:c 1 , C:c 2 , . . . , C:c k } où {c 1 , c 2 , . . . , c k } ? T ? U ; l'itemset préféré est l'ensemble {P:x 1 , P:x 2 , . . . , P:x m } où {x 1 , x 2 , . . . , x m } ? T \ (T ? U ) ; l'itemset non-préféré est l'ensemble {N:y 1 , N:y 2 , . . . , N:y n } où {y 1 , y 2 , . . . , y n } ? U \ (T ? U ). Les itemsets contextuel, préféré et non-préféré sont appelés les itemsets de préférence. Nous définissons trois fonctions de label {? C , ? P , ? N }. Chacune génère un itemset de préférence à partir d'un itemset usuel : par exemple, si I = {i 1 , i 2 , . . . , i n }, alors ? C (I) = {C:i 1 , C:i 2 , . . . , C:i n }, etc.
De manière plus simple, on note
. Nous appelons alors la séquence C P P N N une séquence de préférence, il s'agit d'une liste ordonnée des itemsets de préférence sur les itemsets C, P et N qui indique la préférence de la transaction T = C ? P sur la transaction U = C ? N . Étant donnée une base de préférences de l'utilisateur P, il existe une base de séquences de préférence, notée P S , dont chaque séquence (associée à un identificateur unique) correspond à une préférence unique p ? P. . Etant donnée une base de séquences de préférence P S , le support de la séquence de préférence s, dénoté par supp P S (s), est le nombre de séquences de préférence de P S qui contiennent s : supp P S (s) = |{s ? P S | s s }|. Etant donné un seuil minimal de support ?, une séquence s est alors dite fréquente si supp P S (s) ? ?, on parle de séquence fréquente de préférence.
Soit r = C ? X Y une règle de préférence contextuelle, où C = {c 1 , c 2 , . . . , c k }, X = {x 1 , x 2 , . . . , x m } et Y = {y 1 , y 2 , . . . , y n }, alors la règle r peut être réécrite en une séquence de préférence C X P Y N Propriété 1. Avec la paire de transactions U où T U représentée sous la forme d'une séquence de préférence C I P I N on obtient les relations suivantes :
Ainsi, étant donnée une base de séquences de préférence P S construite à partir des pré-férences utilisateurs P, alors le support de la règle r = C ? X Y par rapport à P est équivalent au support de la séquence de préférence C X P Y N par rapport à P S . De plus, le nombre de paires de transactions de P qui contredisent la règle r est équivalent au nombre de séquences de préférence de P S qui contiennent la séquence C Y P X N
Phase de construction : Sprex-Build
La construction du modèle de préférences Sprex-Build est décrite par l'algorithme 1 où un modèle de préférence M P (c.-à-d., le profil de l'utilisateur) est généré à partir d'une base de préférences utilisateurs P avec un seuil minimal de support ?, un seuil minimal de confiance ?, et une fonction de modélisation ?. Sprex-Build génère d'abord une base de séquences de préférence P S à partir des préférences utilisateurs P en utilisant la correspondance vue dans la section précédente (ligne 1 à 4). Ensuite, à la ligne 5, la collection des séquences fréquentes de préférence F est extraite depuis P S (selon un seuil ?) en utilisant n'importe quel algorithme d'extraction de motifs séquentiels (dans notre partie expérimentale, nous utiliserons l'approche PatternGrowth). Pour chaque séquence fréquente de préférence s de l'ensemble F, Sprex-Build calcule sa confiance conf P S (s) et l'ajoute au modèle M si conf P S (s) ? ?. Enfin, une fonction de modélisation ? définie par l'utilisateur est utilisée pour construire le modèle de préférence final.
Algorithme 1 : Sprex-Build
Entrées : Les préférences de l'utilisateur P, un seuil minimal de support ?, un seuil de confiance minimal ?, et une fonction de modélisation ?. Sorties : Modèle de règles de préférences contextuelles M P .
Comme précédemment décrit, étant donné une paire de transactions U une séquence de préférence C I P I N doit contenir au moins 2 itemsets de préférence et un maximum de 7. par contre, le motif séquentiel 1 , C:i 2 }} sera systématiquement ignoré car il ne sert à aucune règle pertinente.
Pour cette raison, l'extraction des séquences de préférence fréquentes est plus simple que l'extraction de tous les motifs séquentiels (Agrawal et Srikant, 1995;Pei et al., 2001). De plus, comme un modèle de préférence est constitué uniquement de règles de préférences minimales, le problème de construction se réduit même à l'extraction des séquences génératrices fréquentes introduites par Lo et al. (2008).
La méthode Sprex-Build utilise ensuite l'ensemble des motifs séquentiels extraits afin de générer un modèle (ligne 10) via une fonction de modélisation. Avant d'appliquer une fonction de modélisation, nous conservons pour chaque paire de transactions la meilleure règle qui la supporte. Cette notion de « meilleure » est modélisée par un ordre sur les règles déjà utilisé par Liu et al. (1998) pour la construction de classifieur. Ainsi, une règle est d'autant meilleure que sa confiance est élevée ; en cas d'égalité, on choisit alors celle dont le support est le plus grand. La fonction de modélisation effectue ensuite deux opérations : 1) sélection de règles de préférence contextuelle en respectant des contraintes définies par l'utilisateur (p. ex. le filtrage basé sur la confiance, le support, la composition, la structure ou la taille des règles) ; et 2) nouveau tri des règles sélectionnées en respectant un ordre défini par l'utilisateur en cas de besoin. Il s'agit donc de conserver des règles sûres et générales pour que le modèle final soit à la fois précis et concis.
Phase de prédiction : Sprex-Predict
L'utilisation par Sprex-Predict du modèle construit lors de la phase précédente pour effectuer une prédiction entre deux préférences est détaillée en dessous.
Étant donné un modèle M, une fonction de préférence ? retourne un score c entre 0 et 1 qui prédit entre les transactions T et U , celle qui est préférée par l'utilisateur en se référant au modèle M. Si c > 0.5, cela signifie que l'utilisateur préfère la transaction T à la transaction U et ainsi Sprex-Predict retourne la préférence T M U ; si c < 0.5, Sprex-Predict retourne la préférence U M T car l'utilisateur préfère la transaction U à la transaction T ; sinon, SprexPredict retourne T ? M U , qui signifie l'indécision de Sprex-Predict quant à la préférence de l'utilisateur entre T et U .
La fonction de préférence utilise les règles du modèle M afin de déterminer quelle est la transaction préférée entre T et U . Une fonction simple de préférence est d'utiliser la meilleure règle du profil r best qui permet de préférer T à U ou vice-et-versa. Si T est préférée à U selon r best , la fonction de préférence ? best retournera conf P (r best ) (une valeur > 0.5). À l'inverse, si U est préférée à T selon r best , la fonction de préférence ? best retournera 1 ? conf P (r b ) (une valeur < 0.5). Si aucune règle du profil ne s'applique, la fonction ? best retournera 0.5. À nouveau, nous utilisons un ordre défini sur la meilleure confiance, puis sur le meilleur support comme lors de la construction du modèle afin de choisir la meilleure règle.
Malheureusement, la fonction de préférence ? best est souvent indécise car il est peu fré-quent que deux transactions soient directement comparables par une règle du modèle, selon l'étude expérimentale menée dans de Amo et al. (2012). Au lieu de comparer directement les deux transactions avec ? best , nous organisons un vote par valeur pour prendre la décision « qui est le meilleur candidat ? ». Notre vote par valeur est un système de vote pour une élection à un siège dans lequel les électeurs E évalue les deux candidats en leur attribuant une valeur entre 0 et 1 en utilisant ? best . Les valeurs de chaque candidat sont additionnées, et celui ayant le plus haut score est le gagnant. Formellement, nous avons :
Dans les expérimentations, nous utiliserons exclusivement ? vote dont le rappel est toujours supérieur à ? best pour une précision comparable.
Évaluations expérimentales
Dans cette section, nous rapportons les évaluations expérimentales de notre approche Sprex sur les jeux de données réels Toutes nos expérimentations ont été effectuées sur un serveur de 16-Core 2.40GHz Intel Xeon avec 32 giga-octets de mémoire vive et avec le noyau Linux 2.6.32. L'extraction de motifs séquentiels est réalisée en utilisant le principe PatternGrowth proposé par Pei et al. (2001). Toutes les méthodes sont implémentées en C++ Template et compilées par LLVM-Clang++.
Le jeu de données concerné dans nos expérimentations est constitué des 800 156 notes attribuées par 6 040 utilisateurs sur 3 881 films. Chaque film constitue un ensemble d'attributs incluant un identifiant unique, l'année de sortie, les genres (multi-valeurs), les réalisateurs (multi-valeurs) et les principaux acteurs/actrices (multi-valeurs). Chaque utilisateur a évalué un certain nombre de films (de quelques uns à quelques milliers) avec des notes comprises entre 1 et 5 4 . Tous les films votés par un même utilisateur composent un jeu de données indépendant. Alors, soit D un jeu de films votés par un utilisateur, pour toute paire de films (m 1 , m 2 ) ? D, on a m 1 .rating > m2.rating impose m 1 m 2 . Ainsi, chaque jeu de films correspond à une base de préférences de l'utilisateur. La table 1 liste les 4 jeux de données testés dans nos expérimentations où chaque jeu est identifié par l'identifiant de l'utilisateur et contient plus de 500 films votés afin d'assurer une validation croisée en 5 blocs telle que chaque bloc de données contient du moins 100 films votés.
Dans une première étape, les motifs séquentiels sont extraits avec le support minimal variant entre 0,4% et 1,8% pour créer des profils bruts qui contiennent les règles de préférence TAB. 1 -Jeux de données réels sur les films votés.
FIG. 1 -Les temps moyens (en seconde) de construction de profils bruts par rapport à la variation du support minimal.
FIG. 2 -Les tailles moyennes de profils par rapport à la variation du support minimal avec la confiance minimale fixée à 75% (a-d) et à la variation de la confiance minimale avec le support minimal fixé à 0,5% (e-h).
FIG. 3 -Les précisions moyennes (a-d) et les rappels moyens (e-h) de profils par rapport à la variation du support minimal avec la confiance minimale fixée à 75%.
FIG. 4 -Les précisions moyennes (a-d) et les rappels moyens (e-h) de profils par rapport à la variation de la confiance minimale avec le support minimal fixé à 0,5%.
contextuelle minimales avec la confiance minimale fixée à 50% afin de faciliter les tests suivants. Ensuite, pour chaque profil brut, deux fonctions de modélisation, MaxRule et MinRule, sont appliquées pour construire des profils finaux. La fonction MaxRule permet de construire des profils en sélectionnant les règles les plus spécifiques parmi toutes les règles minimales gé-nérées précédemment, par contre la fonction MaxRule construit un profil en utilisant les règles moins expressives comme introduites dans les approches proposées par Agrawal et al. (2006) et par de Amo et al. (2012). Dans cette étape, la confiance minimale a varié entre 70% et 90%. Enfin, les résultats obtenus par SVMRank sont utilisés comme référence. La figure 1 montre les temps moyens de construction de profils bruts à partir des 4 jeux de films. En effet, avec le support minimal 0,4%, les constructions se terminent en 700 secondes dont les nombres moyens de règles par profil sont respectivement 20 934, 13 208, 12 020 et 17 699. Après avoir appliqué la fonction de modélisation, la taille des profils est significativement réduite comme le montre la figure 2.
Nous avons testé la qualité de prédiction de chaque profil construit en mesurant la précision et le rappel de la prédiction. Pour cet objectif, deux séries de tests ont été effectuées en variant le support minimal (la figure 3) et en variant la confiance minimale (la figure 4). Les courbes montrent que la qualité de la prédiction augmente avec l'expressivité des règles qui composent les profils. Par ailleurs, les courbes montrent également qu'il y a une corrélation forte entre la qualité de prédiction et la diminution des seuils de support et de confiance. La table 2 présente un exemple des règles de préférence contextuelle extraites à partir du jeu de films U0048. 
Conclusions
Dans cet article, nous avons présenté Sprex, une approche basée sur l'extraction de sé-quences fréquentes pour la construction de modèles de préférences. L'idée essentielle est de représenter les données d'apprentissage et les règles de préférence utilisateur sous la forme de triplets formant des séquences de préférence. De cette manière, nous avons pu tirer profits des travaux issus de l'extraction de motifs séquentiels fréquents et mettre en place une approche efficace. Par ailleurs, cette proposition a également l'avantage d'étendre l'expressivité des préférences extraites. Nous avons évalué notre approche sur une base de données filmographique du monde réel, et les résultats expérimentaux ont montré que les modèles de préférence construits sont relativement stables par rapport à la variation du seuil minimum de support.
Nos recherches futures vont s'orienter vers le développement de fonctions de modélisation et des fonctions de préférence afin d'améliorer la précision et le rappel de la prédiction des préférences de l'utilisateur. Enfin, nous pensons exploiter le potentiel des séquences au sein de l'approche Sprex pour introduire la notion de temps dans les règles de préférences.

Introduction
L'évolution et le croisement des disciplines de la fouille de données et du génie logiciel ouvrent de nouveaux horizons pour la compréhension et l'amélioration du logiciel et des pratiques de développement. A l'aube de ce domaine émergent de nombreuses questions restent cependant d'actualité du point de vue de la conduite de programmes de mesure logicielle, notamment relevées par Fenton (1994) et Kaner et Bond (2004). Nous proposons dans cet article quelques pistes pour répondre à ces problématiques et mettre en place un processus de mesure fiable et efficace.
Il est utile pour la définition de la notion de qualité de s'appuyer sur des modèles ou standards reconnus : du point de vue de la qualité produit, la référence de facto semble être l'ISO 9126 et son futur successeur, la série 250xx SQuaRE. La maturité du processus de développe-ment est adressée par des initiatives largement reconnues telle que le CMMi ou l'ISO 15504. Afin de clarifier la démarche de mesure, l'approche Goal-Question-Metric proposée par Basili et al. et reprise par Westfal Westfall et Road (2005) permet une approche plus rigoureuse, qui préserve l'efficacité de l'analyse et le sens de ses résultats. Le code source est le type d'artéfact le plus utilisé pour l'analyse de projets logiciels. Du point de vue de l'analyse statique (Louridas, 2006), les informations que l'on peut récupérer d'un code source sont les métriques, correspondant à la mesure de caractéristiques définies du logiciel (e.g. sa taille ou la complexité de son flot de contrôle), et les violations, correspondant au non-respect de bonnes pratiques ou de conventions de codage ou de nommage (e.g. l'obligation de clause default dans un switch). Ces informations sont fournies par des analyseurs tels que Checkstyle, PMD ou SQuORE (Baldassari, 2012).
La gestion de configuration contient l'ensemble des modifications faites sur l'arborescence du projet, avec des méta-informations sur l'auteur, la date ou l'intention des changements. Le positionnement dans l'arbre des versions est important, car les résultats ne seront pas les mêmes pour une version en cours de développement (développée sur le tronc) et pour une version en maintenance (développée sur une branche).
La gestion des tickets recense l'ensemble des demandes de changement faites sur le projet. Ce peuvent être des problèmes (bugs), de nouvelles fonctionnalités, ou de simples questions.
Les listes de diffusion sont les principaux moyens de communication utilisés au sein de projets logiciels. Il existe en général au moins deux listes, une dédiée au développement et l'autre aux questions utilisateur. La définition des métriques accessibles à partir des référentiels identifiés sur le projet doit être fiable -i.e. l'information recherchée est systématiquement présente et valide -et compréhensible pour garder la confiance des acteurs dans le processus.
L'implémentation du processus de collecte et d'analyse doit être transparente pour que les acteurs puissent se l'approprier, intégralement automatisée, et exécutée de manière régulière.
La présentation des informations est capitale. Dans certains cas une liste concise d'arté-facts est suffisante, alors que dans d'autres cas un graphique bien choisi sera plus adapté et délivrera en quelques secondes l'essentiel du message.
Mise en pratique avec Eclipse
Cette approche a été mise en oeuvre dans le cadre de Polarsys, un groupe de travail de la fondation Eclipse qui a pour but, entre autres, de proposer un cadre d'évaluation de la qualité des projets de la fondation. L'arbre de qualité montré en figure 2 montre les exigences identifiées pour Eclipse et Polarsys, et leur organisation en attributs de qualité.
FIG. 2 -Modèle de qualité proposé pour Polarsys.
Du point de vue de l'aide à la prise de décision, les axes de qualité choisis permettent d'évaluer la maturité du projet immédiatement, selon les critères propres à l'organisation. Par exemple le projet analysé sur la droite de la figure 2 montre une bonne qualité produit, mais une faible activité de communication et un petit nombre de participants, car l'essentiel des contributions est fait par une unique société. Cette caractéristique peu visible a priori en fait un risque pour la pérennité du projet, rapidement identifiable sur l'arbre de qualité. Du côté des équipes de développement, l'aide à l'amélioration de la qualité se fait au moyen de listes de violations et de points d'action, qui permettent d'identifier et d'améliorer les pratiques non acquises et fournissent des recommandations pragmatiques pour l'amélioration du code et du processus.
Conclusion
Le prototype développé dans le cadre du groupe de travail Polarsys a permis de définir un socle méthodologique commun et un ensemble de métriques issues de sources nouvelles pour travailler ensemble sur la notion de qualité, et a plus généralement permis de démontrer la faisabilité d'une telle solution. Le prototype a apporté la preuve que des connaissances pratiques pouvaient être extraites du projet pour l'évaluation et l'amélioration de la maturité. Ce travail a été présenté à la communauté Eclipse lors de la conférence EclipseCon France 2013 sise à Toulouse en Juin 2013, et son industrialisation est en cours pour une publication prévue en 2014. Il est également envisagé d'étendre la démarche de qualité initiée pour Polarsys aux projets Eclipse pour compléter les initiatives actuelles PMI et Dash.

Introduction
La classification supervisée de documents vise à déterminer la ou les catégories potentielles d'un document à partir de son contenu (les termes le composant). Dans un cadre supervisé, le processus se décompose généralement en 2 phases :
1. la phase d'apprentissage, qui vise à créer un modèle à partir d'un ensemble d'exemples étiquetés (documents dont la classe est connue), 2. la phase de classification, qui va déterminer la ou les catégories d'un document dont la classe est inconnue par application du modèle.
Bien entendu, la qualité du modèle dépend de la qualité et du nombre d'exemples disponibles. Ainsi plus il y a d'exemples, plus les observations seront fiables et plus le modèle sera précis et efficace.
Il peut cependant s'avérer intéressant de pouvoir élaborer un modèle de classification fiable à partir d'un faible nombre de descripteurs (Forman et Cohen, 2004). Par exemple, le dévelop-pement des réseaux sociaux, avec un nombre de plus en plus important de messages en temps réel mais d'une taille limitée (comme un tweet limité à 140 caractères), implique la mise à disposition d'outils capables de les classer rapidement avec un volume restreint de données. Dans ce contexte, l'extraction de descripteurs pertinents et discriminants représente un défi intéressant. De même, il peut arriver que le nombre d'exemples utiles à la création du modèle lors de la phase d'apprentissage soit lui même très limité. De plus, déterminer la classe d'un document est une opération longue qui, généralement, nécessite un expert du domaine. Certaines approches se fondent sur un nombre restreint d'exemples, les approches de classification semi-supervisées qui utilisent les documents non étiquetées pour compléter l'apprentissage supervisé ou encore d'apprentissage actif qui consiste à construire l'ensemble d'apprentissage du modèle de manière itérative, en interaction avec un expert humain. Certaines de ces méthodes appliquées à un faible nombre d'exemples sont présentées dans (Zeng et al., 2003;Lin et Cohen, 2010) mais elles impliquent d'avoir un grand nombre de documents à disposition pour améliorer le modèle ce qui n'est pas toujours possible.
La plupart des algorithmes actuels de classification supervisée de documents nécessitent un nombre suffisant d'exemples pour créer un classifieur performant et les performances dé-croissent en même temps que le nombre d'exemples diminue. Notre principale contribution est de proposer de nouvelles pondérations de descripteurs textuels pour traiter des jeux d'exemples de petite taille. Ces méthodes de pondérations ont été intégrées dans des approches classiques de classification (Class Feature Centroid et Naive Bayes).
L'article est organisé de la manière suivante : dans la section 2, nous discutons de l'intérêt de nouvelles pondérations. Des mesures permettant d'extraire les descripteurs les plus pertinents d'une classe sont présentées en section 3. L'intégration de ces mesures au sein d'algorithmes de classification en apprentissage supervisé est décrite dans la section 4. Les résul-tats expérimentaux, comprenant notamment une comparaison avec d'autres types d'approches, sont proposés en section 5. Enfin, la section 6 conclue et présente quelques perspectives.
Proposition
La classification supervisée de documents cherche à déterminer la catégorie (ou classe) d'un document à partir de son contenu. Considérons C = C 1 , C 2 , ..., C n un ensemble de n classes et
l'ensemble des documents de la classe j. Les documents sont représentés selon le modèle sac de mots (bag of words) de Salton (Salton et McGill, 1986) et tous les termes de tous les documents forment un dictionnaire (ensemble des termes apparaissant au moins une fois dans la collection de documents). L = t 1 , t 2 , ..., t |L| est un dictionnaire qui contient |L| termes où t i (1 ? i ? |L|) est un terme unique dans le dictionnaire. Chaque document est représenté par un vecteur pondéré de termes sans indication de position dans le document.
? ? D j = {w 1j , w 2j , ..., w |L|j } est la représentation vectorielle du document j où w ij est le poids (ex : Fréquence, Booléen,...) du terme t i pour le document j. Le poids w ij d'un terme dépend à la fois de son poids intra-classe et de son poids inter-classes. Le poids intra-classe permet d'évaluer l'importance du terme au sein de la classe (Est-ce un terme représentatif de la classe ?) alors que le poids inter-classes mesure pour le terme, le pouvoir discriminant de la classe par rapport aux autres classes (Est-ce que le terme représente toutes les classes ?). Des méthodes de pondérations sont présentées dans (Guan et al., 2009) et (Zhang et al., 2012). Ces pondérations, dérivées du T F -IDF , reposent à la fois sur la fréquence d'un terme dans les documents au sein d'une classe mais aussi sur la présence ou l'absence de ce terme au sein des autres classes. Nous pensons que deux situations posent des problèmes :
-En présence de classes composées d'un petit nombre de documents, l'impact de la fré-quence de document est trop importante. -En présence de classes sémantiquement proches puisqu'un terme est considéré comme représentant une classe même s'il n'apparaît qu'une seule fois dans la classe. Dans ce cas, le nombre d'occurrences du terme au sein des classes n'est pas pris en compte. De plus, ces pondérations ne sont appliquées que dans une approche de type Class-FeatureCentroid quand nous pensons qu'elles peuvent être pertinentes dans d'autres approches de classification en apprentissage supervisé.
Dans la section suivante, nous proposons de nouvelles pondérations pour répondre à ces problèmes.
Nouvelles pondérations
Nous présentons dans cette section de nouvelles méthodes de pondérations intra-classes (Section 3.2) et inter-classes (Section 3.3) dérivées du T F -IDF . Nous commençons par rappeler les principes de base du T F -IDF .
Les principes du T F -IDF
Le principe du T F -IDF est de donner un poids plus important aux termes les plus spéci-fiques d'un document (Salton et McGill, 1986). Le T F -IDF est une méthode de pondération éprouvée dont l'efficacité a été démontrée à de nombreuses reprises. Cette mesure repose sur le produit entre la fréquence du terme (T F : Term-frequency) et la fréquence inverse du document (IDF : Inverse Document Frequency). La fréquence du terme correspond au nombre d'occurrences d'un terme dans un document et représente le poids du terme au sein du document, aussi appelé poids intra-document. Pour un document d j et un terme t i , la fréquence du terme est calculée par :
T F i,j = n i,j k n k,j où n i,j correspond au nombre de fois où le terme t i apparait dans le document d j et où le dénominateur correspond au nombre total de termes du document d j . La fréquence inverse de document (IDF) mesure l'importance du terme dans le corpus. L'objectif est de donner un poids plus important aux termes qui apparaissent dans peu de documents. Il s'agit du poids inter-documents qui est calculé en considérant le logarithme de l'inverse de la fréquence de documents qui contiennent le terme dans le corpus :
avec |D|, le nombre de documents du corpus et |{d j : t i ? d j }| le nombre de documents qui contiennent le terme t i .
Le T F -IDF est obtenu en multipliant les poids intra-document et inter-documents du terme t i pour le document
Pour évaluer la représentativité d'un terme dans une classe et non dans un document, nous proposons une mesure adaptée sur les poids intra-classes et inter-classes d'un terme.
Poids Intra-classe
Dans un premier temps, nous proposons une méthode de pondération fondée sur la fré-quence de documents du terme dans la classe comme décrit dans (Guan et al., 2009), appelé inner-weight Df . Nous calculons le poids inner-weight Df ij du terme t i de la classe j selon la formule (1). Nous considérons que le terme le plus représentatif d'une classe n'est pas né-cessairement le terme le plus fréquemment utilisé dans la classe mais celui utilisé dans le plus grand nombre de documents de la classe. De plus, une telle mesure peut se révéler particuliè-rement pertinente pour le traitement de documents de longueurs déséquilibrées. En effet, dans ce cas, les descripteurs linguistiques présents dans les documents de plus grandes tailles auront un impact similaire aux termes présents dans les plus petits documents.
inner-weight
Avec : -DF j ti : Nombre de documents contenant le terme t i dans la classe C j -|d j | : Nombre de documents dans C j Néanmoins Inner-weight Df est confronté aux mêmes limites que celles présentées dans la Section 2 avec les classes composées d'un faible nombre de documents, à savoir l'impact de la fréquence de documents dans les classes les moins pourvues sera disproportionné par rapport aux classes les plus fournies. Ainsi, nous proposons une autre méthode de pondération fondée sur le terme plutôt que sur le document. Nous considérons la fréquence du terme plutôt que la fréquence du document. Cette méthode est appelée inner-weight T f . Nous définissons le poids inner-weight T f ij du terme t i de la classe j selon la formule (2).
Avec : -T F j ti : Nombre d'occurrences du terme t i dans la classe C j -|n j | : Nombre de termes total dans la classe C j Dans cette section, nous avons redéfini deux pondérations intra-classes appelées inner-weight
Dans la section suivante, nous nous intéressons à la pondération inter-classes.
Poids Inter-classes
Nous proposons une première méthode que nous nommons inter-weight class définie par la formule (3). Cette méthode, fondée sur le nombre de classes qui contiennent le terme, a été utilisée dans (Guan et al., 2009 inter-weight
Avec : -|d / ? C j | : Nombre de documents n'appartenant pas à la classe C j -|d : t i / ? C j | : Nombre de documents n'appartenant pas à la classe C j qui contient t i -|d| : Nombre de documents dans l'ensemble des classes -|d ? C j | : Nombre de documents de la classe C j -|d : t i | : Nombre de documents dans l'ensemble des classes contenant le terme t i -|d : t i ? C j | : Nombre de documents de la classe C j qui contient t i -En ajoutant 1, permet de prévenir le cas où t i est uniquement utilisé dans C j (quand |d :
Pour évaluer nos propositions de pondérations, nous les avons intégrées à des méthodes d'apprentissage supervisé détaillées en section 4.
4 Utilisation des mesures dans un contexte d'apprentissage supervisé SVM (Support Vector Machine) et Naive Bayes sont considérés parmi les algorithmes de classification supervisée les plus performants. Cependant, ils ne sont pas toujours adaptés en présence de faibles volumes de données (Kim et al., 2006). Nos nouvelles méthodes de pondération peuvent être utilisées dans des approches Naives Bayes ou Class-Features-Centroid qui présentent les avantages suivants : (i) leur facilité de mise en oeuvre les rendent bien adaptées pour la classification automatique de documents ; (ii) elles sont toutes deux fondées sur des descripteurs pondérés au niveau des classes, ce qui rend nos mesures bien adaptées à ce type d'algorithme ; (iii) enfin les modèles obtenus sont faciles à interpréter et à valider pour un utilisateur final.
Nouvelles mesures et classification Class-Feature-Centroid
L'approche Class-Feature-Centroid est un modèle récent présenté dans (Guan et al., 2009). Chaque classe est considérée selon le modèle vectoriel de Salton (Salton et McGill, 1986), sur la représentation en sac de mots. Chacune des classes est représentée par un vecteur de termes. ? ? C j = {w 1j , w 2j , ..., w |L|j } est la représentation de la classe j où w ij est le poids du terme t i pour la classe j. 
Nouvelles mesures et Naive Bayes
Nous avons aussi intégré nos pondérations dans une approche Naive Bayes. Le classifieur Naive Bayes est un classifieur de type probabiliste défini dans (Lewis, 1998) très utilisé pour la classification de texte car il donne de bons résultats malgré l'hypothèse rarement vérifiée d'indépendance conditionnelle à la classe des descripteurs. La probabilité qu'un document non étiqueté (d) composé de i termes t i appartienne à la classe C j est donnée par
Après avoir calculé C j = {w 1j , w 2j , ..., w |L|j } où w i,j est le poids du i eme terme de la classe C j , nous estimons la probabilité qu'un document non étiqueté d appartienne à la classe C j : P (d ? C j ) = P (C j ) i (w i,j + 1). Ajouter 1 permet de prévenir le cas où le terme n'apparait pas dans la classe, quand la probabilité vaut zéro. Les expérimentations sont appelées N b 
Algorithmes de comparaison
Dans la section suivante (section 5) nous comparons ces huit méthodes de classification supervisée (4 pondérations pour chacun des 2 algorithmes) avec différents algorithmes implé-mentés dans Weka 2 : -Deux implémentations de SVM, SMO, qui utilise un noyau polynomial (Platt, 1999) et LibSVM, qui utilise un noyau linéaire (Chang et Lin, 2011). -Une implémentation Naïve Bayes, DMNB (Su et al., 2008). -Un arbre de décision, LadTree, (Holmes et al., 2002). D'autres expérimentations ont été réalisées, toujours avec Weka (NaiveBayes (John et Langley, 1995), NaiveBayes Multinomial (McCallum et al., 1998), LibSVM avec fonction à base radial et LibSVM avec noyau polynomial (Chang et Lin, 2011), J48 et RepTree (Quinlan, 1993)). Les résultats étant moins satisfaisants, ils ne sont pas présentés dans cet article 3 . Chacun des algorithmes a été testé en validation croisée (3-fold cross validation) et les résultats indiqués ci-après sont la moyenne de ces 3 itérations. Le nombre de 3 itérations a été retenu afin d'avoir un nombre suffisant de données en apprentissage et test. Les différentes expérimentations fondées sur ces approches sont détaillées dans la section suivante.
Expérimentations
Protocole expérimental
Nous avons évalué nos propositions sur 2 corpus différents : -Le corpus Reuters-21578 4 , qui est fréquemment utilisé par la communauté pour évaluer la qualité des modèles, est un ensemble de dépêches écrites en anglais mises à disposition par l'agence Reuters. Les news sont regroupées dans différentes catégories comme par exemple "sucre", "huile" ou "or", etc. Les documents ont été étiquetés manuellement. pour le corpus Tweet. Le Tableau 1 présente les caractéristiques des 2 corpus après pré-traitement.
Résultats
La qualité des modèles est évaluée par la Précision, le Rappel et la F-mesure (moyenne harmonique de la Précision et du Rappel). Nous avons évalué la Micro-Moyenne (calcul global du Rappel et de la Précision sur l'ensemble des classes) et la Macro-Moyenne (calcul du Rappel et de la Précision pour chaque classe, puis calcul de la moyenne des classes (Nakache et Metais, 2005)).
Pour étudier le comportement de nos pondérations au sein des approches Naive Bayes et Class-Feature-Centroid et pour comparer nos résultats avec les autres algorithmes de classification, nous avons réalisé plusieurs expérimentations sur les corpus "Reuters-21578" et "Tweet". Nous avons étudié l'impact du nombre de termes, du nombre de documents et du nombre de classes sur les résultats de classification.
Conséquences du nombre de termes sur la classification
Tout d'abord, sur le corpus Tweet, nous avons fixé le nombre de classes (5) et de documents (1186) et décidé de supprimer aléatoirement des termes afin de diminuer le nombre de termes par document. Nous avons réalisé sept expérimentations résumées dans le Tableau 2.
Nous présentons l'évolution de la F-mesure en fonction des expérimentations dans le Tableau 3. Les Macro-Moyenne et Micro-Moyenne suivant la même tendance, nous ne reproduisons ici que la Micro-Moyenne. Ces expérimentations nous permettent de tirer 2 enseignements : (1) les nouvelles pondérations intégrées aux approches Naive Bayes et Class-FeatureCentroid donnent des résultats meilleurs que les algorithmes SVM et DMNB quand le nombre 6. Le Front National, dû au faible nombre d'élus actifs sous Twitter, était sous représenté dans le corpus.
TAB. 3 -Expérimentation 1 : évolution F-mesure (Micro-Moyenne) de termes est faible (expérimentations 6 et 7), (2) elles donnent de meilleurs résultats que les algorithmes LadTree et LibSVM dans tous les cas.
Conséquences du nombre de classes sur la classification
Ensuite, sur le corpus "Reuters-21578", nous nous intéressons à l'impact du nombre de classes. Nous fixons le nombre de documents par classe (50) et nous supprimons des classes pour passer de 28 à 2 classes. Nous avons réalisé 7 expérimentations, présentées dans le Tableau 4.
Comme nous pouvions le supposer, les algorithmes ont un meilleur comportement en pré-sence d'un nombre limité de classes. Les algorithmes suivent la même tendance et nous déci-dons de ne pas reproduire ici le détail des résultats 7 . Nous pouvons préciser que LadTree est plus impacté par un grand nombre de classes que les autres et que les nouvelles pondérations intégrées dans des approches Naive Bayes ou Class-Feature-Centroid donnent des résultats lé-gèrement meilleurs que ceux des autres approches (des observations similaires sont présentées ci-après). Il est aussi intéressant de noter que les résultats sont similaires sur un corpus de langue française (Tweet) et un corpus de langue anglaise (Reuters-21578). A partir de ces expérimentations, nous pouvons conclure que les méthodes de pondérations proposées dans cet article intégrées dans des approches Naive Bayes ou Class-Feature-Centroid (1) sont légèrement meilleures que les autres algorithmes (seulement devancé par LadTree) , (2) sont plus résistantes que la plupart des algorithmes quand le nombre de documents disponibles en apprentissage diminue fortement.
Conclusions et perspectives
La classification d'un faible volume de documents textuels reste une problématique d'actualité. En effet, même si le nombre de documents ne cesse de croître, il existe de plus en plus de domaines d'applications où nous devons rapidement et à partir d'un faible volume être capable de classer. Par exemple les tweets nécessitent, pour être suivis, de pouvoir classer en temps réel l'information disponible. Attendre d'avoir un nombre suffisant d'éléments n'est pas toujours possible et le décideur souhaite avoir rapidement les documents classés. Dans cet article, nous avons proposé de nouvelles mesures particulièrement adaptées aux faibles volumes de données (dû à un faible nombre de documents ou à un faible nombre de descripteurs). Nous avons également montré comment ces mesures pouvaient être prises en compte dans un cadre supervisé notamment via Naive Bayes et une approche basée sur les centroides. Les expérimentations menées sur un corpus de tweets et un benchmark plus traditionnel ont permis de montrer que nos nouvelles mesures ont un meilleur comportement que les autres approches lors de la manipulation d'un faible volume de documents. Simples à mettre en place, elles sont tout à fait adaptées à la manipulation de données évoluant rapidement comme les tweets. Notre proposition est fondée sur une extension de la mesure de T F -IDF . Nos travaux actuels évaluent la possibilité de prendre en compte d'autres mesures comme par exemple OKAP I BM 25 (Robertson et al., 1999) afin de mieux appréhender l'impact sur non seulement de petits volumes mais également des documents courts. Dans un contexte de classification, la proximité sémantique des classes est difficile à prendre en compte. Via nos propositions et notamment les mesures inter et intra classes, nous avons montré qu'elles sont adaptées aux faibles volumes. Nous souhaitons à présent proposer à l'utilisateur de pouvoir mieux pondérer ces mesures en fonction de la distribution et du volume de termes.

Biographie
Thomas Lebarbé est maître de conférences en informatique et sciences du langage à l'Université Stendhal -Grenoble 3. Depuis sept ans, il consacre ses recherches aux humanités numériques autour de fonds patrimoniaux (notamment les Manuscrits de Stendhal) sans perdre de vue la dimension linguistique des matériaux sur lesquels il travaille. Il a soutenu une habilitation à diriger des recherches promouvant l'interdisciplinarité intrinsèque et extrinsèque du traitement automatique des langues. Dans ses différentes fonctions, enseignant d'informatique dans une université de lettres, directeur adjoint de la Maison des Sciences de l'HommeAlpes et Chargé de Projet Humanités Numériques à l'université Stendhal, il revendique le dé-cloisonnement disciplinaire comme un vecteur de sérendipité et un moyen de questionner les

Introduction
De nos jours, les institutions étatiques tout comme les entreprises, s'appuient souvent sur l'opinion publique pour orienter leurs décisions stratégiques. L'analyse automatique d'opinions a ainsi connu une véritable envolée depuis l'apparition des réseaux sociaux tels que Twitter. Selon Pak et Paroubek (2010), une opinion peut être soit positive, négative, ou neutre, ce qui revient à un problème de classification en 3 classes. Un second point de vue, que nous adopterons, consiste à considérer qu'une opinion ne peut pas être neutre, seulement objective. Ainsi, le problème peut être décomposé en une évaluation de l'objectivité dans un premier temps, suivie dans le cas d'un texte subjectif d'une seconde étape de détection de la polarité. L'objectif de ces travaux, dans le cadre d'un projet européen de tourisme, est de présenter une méthode pour détecter la subjectivité puis la polarité d'un tweet anglais ou français. Dans la suite de cet article, nous présentons section 2 un état de l'art sur la détection d'opinions dans des textes. La section 3 décrit notre méthode de détection de subjectivité et son évaluation sur des tweets. La section 4 est dédiée à la détection de polarité. Enfin, la section 5 résume et analyse les résultats obtenus et propose des pistes pour nos travaux futurs.
La détection d'opinions dans des textes
La détection de subjectivité et de polarité sont des tâches similaires pouvant être résolues par le même type de méthodes. Il en existe 3 à l'heure actuelle : les méthodes symboliques, statistiques et hybrides.
Les méthodes symboliques créent un ensemble de règles de décision servant à classer un texte dans une catégorie. Ces règles peuvent être éditées de manière manuelle (très coûteux) ou semi-manuelle par un expert humain et sont directement appliquées aux textes à évaluer. Les règles définies peuvent être plus générales (moins coûteuses) en utilisant par exemple des formules telles que des calculs de valences Baccianella et Sebastiani (2010);¸Serban et Pécuchet (2012). Ces approches ne permettent néanmoins pas la détection d'opinions pour des structures linguistiques compliquées. Les méthodes statistiques s'appuient sur un corpus de textes transformés en vecteurs de caractéristiques encodant les textes (comme le nombre d'occurrences) afin d'en extraire des propriétés par apprentissage. Le clustering ne nécessite pas de corpus annoté mais si les caractéristiques sont mauvaises, les classes obtenues ne correspondent pas aux classes désirées Hatzivassiloglou et McKeown (1997). Les méthodes telles que SVM nécessitent un corpus annoté mais sont plus efficaces. Pang et Lee (2008) a démontré que la seule présence des mots suffit à obtenir de bonnes performances.
Les méthodes hybrides mélangent apprentissage statistique et édition de règles (le plus souvent manuelles). La sélection de motifs séquentiels les plus fréquents Serrano et al. (2012) peut ainsi être classée dans la catégorie des méthodes hybrides puisqu'elle est constituée d'une partie d'extraction automatique de motifs les plus fréquents et d'une partie de sélection manuelle des motifs les plus pertinents pour un contexte donné. Ces méthodes semblent efficaces à condition d'effectuer les bonnes combinaisons.
Nous utiliserons SVM pour détecter la polarité puisque c'est une méthode très efficace et que Go et al. (2009) propose une méthode de constitution automatique de corpus. Pour détecter la subjectivité, en revanche, ne disposant pas de ressource d'annotation manuelle, nous utiliserons une méthode hybride sans corpus annoté : les motifs séquentiels fréquents.
Détection de subjectivité par motifs séquentiels fréquents
Pour détecter la subjectivité, les motifs séquentiels les plus fréquents sont d'abord extraits des tweets et, parmi eux, nous avons ensuite sélectionné manuellement ceux permettant de détecter la subjectivité. Si l'un des motifs au moins peut s'appliquer à un tweet, alors celui-ci est considéré comme subjectif.
Trois concepts de base sont utilisés dans cette méthode : les items, les itemsets et les sé-quences. Les items synthétisent les différentes informations sur un mot (le mot lui-même, son lemme -en français -ou son stem -en anglais -et sa catégorie grammaticale). Pour diminuer le bruit induit par les urls et les hashtags, nous les avons remplacé par le terme « URL » et le terme « HASH » et avons rajouté une annotation au termes subjectifs en utilisant Sentiwordnet comme référence. Les itemsets sont des ensembles d'items. Enfin les séquences sont un ensemble d'itemsets représentant un tweet. Il est ensuite nécessaire de transformer un corpus de tweets subjectifs en un ensemble de séquences. Nous avons constitué un corpus en ne ré-cupérant que des tweets présentant un émoticône Go et al. (2009) (300 000 anglais et 300 000 français). Nous avons supprimé ceux ne possédant pas une entité nommée de type lieu touristique puisque le thème du projet est le tourisme. Nous obtenons 5 000 tweets pour l'anglais et 4 000 pour le français. Nous avons annoté nos propres corpus de test pour ne pas biaiser la validation en utilisant le corpus d'apprentissage puisque la méthode de Go et al. (2009) est naïve (700 pour le français et 800 pour l'anglais).
3 paramètres doivent être fixés pour affiner l'extraction de motifs : le nombre d'itemsets minimum et maximum à considérer pour générer les motifs (suffisamment grand pour obtenir des opinions et sans limite maximum), nous l'avons fixé à 3 pour ne pas couvrir les émotions ; le support, c'est à-dire la fréquence minimum d'apparition des motifs dans le corpus (suffisamment grand pour ne pas obtenir des règles trop précises mais suffisamment petit pour obtenir un nombre raisonnable de règles), nous l'avons fixé entre 100 et 200 pour obtenir 1000 motifs environ ; et enfin le gap (nombre d'itemsets à ignorer lors de la génération des motifs. Un gap de 0 correspond à des itemsets situés côte-à-côte dans le texte).
Nous obtenons les meilleures performances avec un gap de 2, avec une précision moyenne de 64% et un rappel moyen de 62% pour l'anglais et une précision moyenne de 65% et un rappel moyen de 66% pour le français. Barbosa et Feng (2010) ont comparé quatre méthodes de détection de subjectivité pour l'anglais en utilisant le taux d'erreur (nombre de textes mal classés sur nombre total de textes) comme métrique. Les performances se situent entre 18 et 32. Dans notre cas, nous obtenons 34 pour le français et 33 pour l'anglais ce qui reste acceptable.
Détection de polarité
Pour détecter la polarité, nous sommes partis d'une méthode SVM avec un noyau non linéaire RBF afin de classer des textes. Nous avons choisi la présence des mots comme caractéristiques du vecteur puisque Pang et Lee (2008) la recommande. Chaque terme est stemmé (pour l'anglais) ou lemmatisé (pour le français) et associé à sa catégorie grammaticale. Les URL et les hastags qui pourraient bruiter l'apprentissage sont supprimés, ainsi que les déter-minants. En suivant la méthode de Go et al. (2009) basée sur la polarité des émoticônes, nous sommes parvenus à créer un corpus annoté. Nous avons utilisé leur corpus de 1,6 millions de tweets pour l'anglais et avons constitué notre propre corpus français de 300 000 tweets. Pour valider la méthode, nous avons voulu éviter une validation croisée étant donnée que la méthode d'annotation est naïve. Nous avons ainsi annoté nos propres corpus de test (700 tweets français et 800 anglais). Il a ensuite été nécessaire de sélectionner les termes à conserver dans l'index. Nous avons commencé par conserver les 1 000 plus fréquents pour l'anglais et les 10 000 plus fréquents pour le français. En effectuant un rapide test, nous avons pu nous apercevoir qu'en augmentant ou en diminuant ce nombre, nous perdions en précision moyenne et en rappel moyen. Ensuite, en observant les tweets classés par le système, nous avons pu constater que la négation n'est généralement pas prise en compte. Nous proposons une méthode alternative aux n-grams, moins efficaces que la seule présence des motsPang et Vaithyanathan (2002), basée sur la position absolue des mots dans la phrase. Nous choisissons d'inscrire non plus la présence des mots dans l'index mais la position de chaque terme pondéré par la taille du tweet. Par exemple la phrase « it is time now » aura 0.25 pour it (position 1, tweet de taille 4), 0.5 pour is (position 2), 0.75 pour time (position 3) et 1 pour now (position 4). L'inconvénient est qu'un même mot peut apparaître plusieurs fois dans la phrase. Il devra donc apparaître autant de fois dans l'index.
En utilisant la seule présence des mots, nous obtenons 74% de précision moyenne et 73% de rappel moyen pour l'anglais et 62% de précision moyenne et 61% de rappel moyen pour le français. Avec la position absolue, les performances diminuent à 60% pour la précision et le rappel en anglais. En revanche, les performances françaises sont équivalents : 62.6% de précision et 57% de rappel. Compte-tenu des résultats, il est probable que l'information de position absolue soit trop précise et qu'il faille plutôt considérer une position relative. En comparant avec Go et al. (2009), ceux-ci ont obtenu 82% d'accuracy sur leur corpus de test anglais, dans notre cas, nous atteignons tout de même 73% sur un corpus de test différent.
Conclusion et perspectives
Dans cet article, nous avons proposé une méthode permettant de détecter la subjectivité et la polarité d'un tweet. Dans le cas de la détection de subjectivité les performances sont correctes mais inférieures à l'état de l'art puisque les motifs retournés présentent un bruit important dû aux émoticônes et à la ponctuation. Il sera donc nécessaire de les supprimer du corpus. De plus, les mots subjectifs ont tendance à disparaître dans les motifs remplacés par leur catégorie grammaticale, ce qui ne permet pas une sélection manuelle efficace. Il sera donc nécessaire de mettre en place une méthode pour que ces termes apparaissent dans les motifs. Enfin, dans le cas de la détection de polarité, nous avons obtenu des performances raisonnables bien que légèrement inférieures à ceux de la littérature, ce qui peut s'expliquer par une difficulté à sélec-tionner les paramètres optimaux du SVM. Une piste d'amélioration consisterait à supprimer de l'index les termes présents de manière équivalente dans les deux classes et d'ajouter les termes subjectifs non conservés car moins fréquents, en utilisant Sentiwordnet par exemple.

Introduction
De nombreux domaines d'application génèrent en permanence d'énormes quantités de données. L'un des défis essentiels auquel les approches de fouilles de flots doivent faire face est la détection de changement dans ces données. En effet, l'information disponible dans les flots change et évolue au fil du temps et les connaissances acquises au préalable peuvent s'avérer non représentatives des nouvelles données. Dans un contexte d'apprentissage, cela se traduit par le fait que des classes ou des concepts sous représentés (resp. surreprésentés) peuvent apparaître surreprésentés (resp. sous représentés) après une période plus longue. Savoir détecter le plus tôt possible les réels changements dans le flot permet alors de pouvoir réévaluer automatiquement les apprentissages précédents et surtout garantir que la connaissance extraite à un moment donné est vraiment représentative des données disponibles sur le flot. Dans cet article, nous proposons une nouvelle méthode de détection de changement définie pour traiter des donnés qualitatives : CDCStream (Change Detection in Categorical data Streams). L'une des originalité de notre approche est de pouvoir mettre en évidence de manière non supervisée les changements dans le flot des données en exploitant efficacement une partie de l'historique. Le modèle retenu de description du flot est sous la forme de lots de données : lorsque qu'un nouveau lot arrive, CDCStream construit un résumé informatif et calcule différents tests statistiques afin de vérifier si un changement a eu lieu dans la distribution des données.
Le reste de cet article est organisé de la manière suivante. CDCStream est décrit dans la section 2 et des expérimentations menées dans la section 3. Nous concluons dans la section 4.
L'approche CDCStream
Nous considérons une représentation classique de flots de données sous la forme d'un flot infini divisé en lots : S = {S 1 , S 2 , ..., S n , ...}. En outre, nous considérons que chaque exemple appartenant au flot est défini sur un ensemble d'attributs qualitatifs, i.e. chaque attribut X j est défini sur un ensemble discret de valeurs nominales. Tout d'abord nous résumons les données qualitatives à l'aide de la méthode DILCA proposée dans (Ienco et al., 2012) afin de résumer la distribution des données sous-jacente. Nous proposons ensuite de surveiller les statistiques extraites des lots en utilisant l'inégalité de Chebyshev (Aggarwal, 2007).
Le principe de DILCA est de regrouper des données qualitatives entre elles via un ensemble de matrices, une pour chaque attribut, où chaque matrice contient les distances apprises entre chaque paire de valeurs d'un attribut spécifique. Dans notre contexte, considérons l'ensemble F = {X 1 , X 2 , . . . , X m } de m attributs qualitatifs pour le lot S i . |X i | désigne la cardinalité de l'attribut X i . On note Y l'attribut cible, un attribut spécifique dans F pour lequel nous devons calculer les différentes distances. DILCA calcule une distance basée sur le contexte entre n'importe quelle paire de valeurs (y i , y j ) de l'attribut cible Y sur la base de la similitude entre les distributions de probabilité des y i et y j , compte tenu des attributs de contexte, appelée C(Y ) ? F \ Y . Pour chaque attribut de contexte X i il calcule la probabilité conditionnelle pour les deux valeurs y i et y j étant données les valeurs x k ? X i , puis il applique la distance euclidienne. La distance euclidienne est normalisée par le nombre total de valeurs considérées :
Pour sélectionner un ensemble pertinent et non redondant de caractéristiques, les auteurs de (Ienco et al., 2012) proposent d'adopter FCBF, une approche de sélection d'attributs initialement proposée par (Yu et Liu, 2003). A la fin du processus, DILCA renvoie un modèle de distance M = {M X l | l = 1, . . . , m}, où chaque M X l est la matrice contenant les distances entre toutes les paires de valeurs d'attribut X l , calculées en utilisant l'équation 1. Chaque matrice générée M X l présente des caractéristiques intéressantes : elle est symétrique, la diagonale contient forcément des 0 et chaque valeur est délimitée entre 0 et 1. En fait, cet ensemble de matrices constitue une information utile pour résumer la distribution sous-jacente. L'ensemble M de matrices peuvent alors être agrégées en une seule mesure par la formule suivante, qui ne tient compte que de la partie triangulaire supérieure de chaque matrice :
D. Ienco et al.
Cette formule correspond à un résumé de l'ensemble du lot en tenant compte à la fois de la corrélation entre les attributs et de la distribution des valeurs des attributs.
Pour déterminer si une distribution d'un lot s'écarte ou non d'une distribution initiale, nous utilisons des techniques de contrôles statistiques des processus (Gama et al., 2004) fondées sur l'inégalité de Chebyshev (Aggarwal, 2007). Ce choix ne nécessite aucune hypothèse sur les distributions des données et permet de montrer qu'une variable aléatoire prendra, avec une grande probabilité, une valeur relativement proche de son espérance :
Definition 1 (Inégalité de Chebyshev) Soit X une variable aléatoire avec espérance µ X et écart-type ? X . Alors, pour tout k ? R
L'inégalité nécessite de calculer la moyenne et l'écart-type et il n'est pas possible de stocker l'intégralité de ces valeurs. Nous proposons donc d'évaluer les changements à la volée et de ne remonter que les véritables modifications dans la distribution des données (en utilisant un mécanisme d'oubli) ou de ne faire remonter que les simples fluctuations sous forme d'alarmes.
Expérimentations
Nous comparons les résultats et le comportement de notre algorithme, CDCS., avec la méthode RSCD proposée dans (Cao et Huang, 2013) et reconnue comme la méthode la plus efficace pour détecter des changements dans des flots de données qualitatives. Nous comparons également notre approche avec la méthode supervisée, SDrif tC, de (Gama et al., 2004) qui possède un mécanisme de détection de changement et de variation en fonction de la précision du classifieur. Les expérimentations menées avec les jeux de données suivants : Electricity, Forest, Airlines, KDD99 ont été réalisées avec la plateforme MOA (Bifet et al., 2010). Dans le tableau 1, nous pouvons constater que l'approche supervisée n'est pas meilleure que les approches non supervisées. Ceci est un résultat intéressant dans la mesure où SDrif tC est la seule méthode qui exploite la précision afin de prendre une "décision de détection de changement" pour ré-apprendre le modèle sous jacent. Nous pouvons également constater que, la plupart du temps, le classifieur appris avec notre méthode obtient de bien meilleures performances que les autres approches non supervisées. Nous présentons également le comportement de CDCStream et RSCD dans le tableau 2 où nous mesurons le pourcentage des changements détectés par les deux méthodes pour différentes tailles de lots. Nous pouvons constater que les deux méthodes ont des comportements très différents. Par exemple, si nous analysons les résultats de Electricity, pour une taille de lot de 50 le nombre de changements est similaire. Par contre lorsque les lots grandissent les tendances sont très différentes. En particulier, la taille du lot pour b = 500 et b = 1 000 a un impact fort sur le pourcentage de changements découverts par RSCD.
Conclusion
Détecter les changements dans un flux de données qualitatives n'est pas simple. Dans cet article, nous avons présenté un nouvel algorithme qui extrait des résumés des différents lots et 

Introduction
Un des enjeux de l'amélioration du confort et de la sécurité des personnes dépendantes à domicile passe par la détection des anomalies dans le comportement.Pour cela on essaie d'établir un modèle personnalisé des activités de la vie quotidienne (AVQ), grâce l'utilisation de plateformes expérimentales (Smart Homes) équipées de divers capteurs domotiques telle la plateforme CASAS 1 . Cet outil permet la modélisation du comportement et le reconnaissance d'actions Cook et al. (2013)  Soulas et al. (2013), ou la détection de danger comme la chute, en combinaison avec des capteurs portés par la personne Lustrek et al. (2012).
Dans le cadre de notre étude nous nous intéressons à la survenue brutale d'un problème dans la vie quotidienne, et la dégradation lente du rythme de vie résultant d'une pathologie. Pour détecter ces changements nous prenons pour source d'information le temps de présence et d'inactivité dans certains lieux du logement, comme la chambre à coucher, la cuisine, le séjour, la salle de bain et les toilettes. Ces informations sont extraites des données fournies par les détecteurs de mouvement installés dans chaque pièce du logement.
L'article Botia et al. (2012) traite de la surveillance de personnes âgées dans leur logement, et propose une approche basée sur la mesure du temps d'inactivité entre deux déclenchements de capteurs de mouvement. La décision est prise en suivant des règles qui définissent les situations de danger, telle qu'une durée d'inactivité qui dépasse un certain seuil. Ces seuils sont déterminés par apprentissage pour chaque pièce du logement. Les résultats de l'étude de Botia montrent qu'il est possible de détecter un dépassement en fixant un seuil à partir d'une moyenne glissante des valeurs maximales de temps d'inactivité enregistrées au cours des jours précédents. Ils montrent aussi que la prise en compte des différentes jours de la semaine n'apporte pas d'amélioration. Par contre Botia ne tient pas compte de la variabilité de l'activité de la personne au cours de la journée et de la nuit.
Dans ce travail nous faisons l'hypothèse qu'il faut prendre en compte les plages horaires pour modéliser l'activité dans le logement. Nous réalisons une analyse des données issues de capteurs pour déterminer l'impact de la plage horaire sur l'activité dans le logement, et nous proposons une heuristique pour estimer les durées d'inactivité autorisées.  tivités, et encore plus difficile d'obtenir des informations sur une véritable situation de danger, autrement que par la simulation. Nous avons donc basé notre apprentissage des seuils sur la seule vérité terrain disponible, c'est à dire sans exemple d'anomalies. Nous avons estimé quel pourrait être à priori le nombre maximum de faux positifs à partir duquel les seuils de détec-tions devraient être calculés. L'apprentissage des seuils utilise les 28 premiers jours du dataset, la détection du nombre de dépassements des seuils est effectuée sur les 192 jours restants. Le seuil pour chaque zone et chaque tranche horaire est déterminé à partir de l'histogramme des durées d'inactivités, et il est tel que 99% des durées d'inactivité relevées sur la durée d'apprentissage lui soient inférieures. Le choix de 99% permet d'obtenir un nombre de fausses alarmes inférieur à une par jour en moyenne. Des essais avec différentes durées d'apprentissage (7 , 14, 21, 28 , 60, 90,et 120 jours ) montrent que les taux de fausses détection diminuent avec la durée d'apprentissage. Cependant dans la pratique pour un système réel fonctionnant sur ce principe, Il faut réduire la durée d'apprentissage afin d'activer la détection d'anomalies.
Extraction des caractéristiques
La figure 2 présente les seuils calculés sur une période d'analyse de 28 jours. Les couleurs choisies (vert, jaune, rouge) illustrent la sensibilité du système et la potentialité d'une situation de danger non détectable. En vert sont représentés les seuils de durée d'inactivité inférieurs ou égaux à 30 minutes, en jaune les seuils égaux à 1 heure, et en rouge les seuils supérieurs ou égaux à 2 heures. On constate que cette carte des seuils est très informative, dans le sens où les seuils calculés ont un grande variabilité en fonction de la plage horaire et de la zone, et qu'elle reflète la réalité des activités de la vie quotidienne. On remarque que certains seuils sont très élevés. En effet dans ces plages horaires il y a très peu d'activité dans les plages concernées, et les durées d'inactivité très longues ont beaucoup de poids sur le calcul du seuil.
Conclusion et perspective
Nous avons repris ici une méthode permettant de détecter une situation de danger en utilisant uniquement les informations provenant de capteurs de mouvement. Nous avons montré l'intérêt d'apprendre ces seuils en tenant compte des plages horaires et des zones dans lesquelles surviennent les différents évènements des capteurs. Le résultat de nos expérimenta-tions montre qu'un danger potentiel demeure dans certaines zones et à certains horaires. La précision peut être améliorée en prenant en compte la topologie du logement, afin d'éliminer tous les évènements incohérents qui se succèdent dans des zones non connexes. La grande variabilité des seuils en fonction des plages horaires nécessite de prendre en compte de manière fine le nombre de déclenchements, car si ce nombre est faible, l'estimation d'une espérance est fortement biaisée. Concernant l'évolution du comportement au cours du temps, il semble né-cessaire d'étudier par la suite l'évolution du modèle du comportement de la personne, à l'aide de dataset beaucoup plus longs, ou avec un système fonctionnant en situation réelle.
Summary
In the next decades elderly care will become a major concern. The information technologies can improve the comfort and security of the dependent persons at home. In this paper we propose a method for dangerous situations detection based on automatic thresholding of the inactivity durations of passive infrared motion detectors. Our contribution consists in learning the thresholds automatically, depending on the occupied room and the daily hour. The method is evaluated on real data. It allows the emergency services to operate quicker.

Introduction
L'explosion récente des technologies mobiles et des données géo-référencées a fait émerger un nouveau type de données, qualifiées de géographiques. De ce fait, une prolifération des systèmes d'informations géographiques (SIG) a vu le jour pour assurer une meilleure exploitation de ces informations. En fait, pour atteindre cet objective on doit disposer d'un ensemble d'informations complet et cohérent. Des moyens d'enrichissement ont été proposés pour aboutir à l'enrichissement des BDG à des coûts réduits. Dans ce contexte, on trouve à titre d'exemple MetaCarta (www.metacarta.com) et PIV (Lesbegueries et al., 2006) qui opèrent l'enrichissement en liant les documents textuels aux entités géographiques correspondantes. Ces travaux, ne proposent pas (ou peu) de moyens pour gérer les contenus textuels des documents. L'outil SDET (Mahmoudi et Faïz, 2010) est un autre moyen d'enrichissement qui cherche à exploiter le contenu textuel pour extraire l'essentiel sous forme d'un résumé. Le présent travail s'inscrit dans le contexte d'enrichissement des SIG tout en proposant une vue structurée des informations sous format de BDG. Concrètement, notre approche s'articule autour de deux grandes phases : une correspondance du texte en schéma conceptuel et une génération de la BDG à partir du schéma suite à son remplissage.
Notre approche
Le processus général de génération d'une structure sous forme de BDG à partir de texte vise un enrichissement de la BDG initialement incarnée dans le SIG. L'idée est de chercher un ensemble d'attributs pouvant compléter les données existantes. Notre approche se décompose
en deux grandes phases (cf. figure 1). La première phase de notre processus d'enrichissement consiste à la génération de la structure de la BDG à partir de textes. Cette phase est déclenchée par une analyse morphosyntaxique résultant en un texte taggé. Le texte prétraité subit par la suite une annotation permettant de repérer les phases cibles. Inspirée du modèle proposé dans (Gaio et Nguyen, 2011), nous avons pu identifier différentes formes syntaxiques de phrases renfermant des données géographiques. En adoptant la même représentation symbolique, nous avons dégagé le modèle présenté par la figure 2.
FIG. 2 -Modèles des phrases à détecter dans le texte (avec * signifie que le composant correspondant peut être présent zéro ou plusieurs fois). Les autres composants doivent apparaître au moins une fois.
Les phrases annotées seront injectées comme entrée de la phase d'extraction des concepts géographiques. Pour pouvoir dégager les éléments de base du modèle classes/relations nous nous sommes appuyés sur un ensemble de règles. Ces dernières sont dégagées empiriquement suite à l'étude de corpus textuels se rapportant aux données géographiques. Le tableau 1 décrit ces règles pour chaque élément du modèle.Une fois les données extraites, une étape de filtrage s'avère nécessaire pour la suppression des redondances et des noms propres au niveau des classes et des attributs. La tâche de suppression des redondances nécessite la détermination des synonymes de chaque élément du modèle. Ceci nous a amené à exploiter le thésaurus N. Hassini et al.
WordNet (Miller et al., 1990) pour accomplir cette tâche. Pour ce faire, nous avons appliqué un ensemble de règles illustrées par le tableau 1. La deuxième phase de notre approche consiste à fournir un corpus d'apprentissage annoté préalablement pour pouvoir dégager les règles d'annotation. Pour accomplir l'apprentissage nous avons adopté l'algorithme de SVM à marges inégales (Li et Shawe-Taylor, 2003). Par annotation nous entendons le repérage des données qui vont servir comme valeurs des attributs dont nous disposons. Les règles dégagées vont être appliquées au texte annoté à la première phase. Ceci résulte en un remplissage de la BDG préétablie. La base résultante et la BDG incarnée au SIG vont subir un appariement pour pouvoir ajouter tout attribut non présent dans la base initiale.
Implémentation
L'approche détaillée tout au long de ce papier a fait l'objet d'une implémentation qui a pu aboutir à l'outil GDB Generator. Etant donné que le but est d'enrichir les SIG, cet outil se présente comme un Plugin développé en Java et intégré dans le système OpenJump (http ://www. openjump. org). Si l'utilisateur a affaire à des informations non stockées dans la base initiale, il peut lancer notre outil. En backoffice l'outil s'appuie sur un corpus de textes relatifs à l'entité en question. En utilisant l'environnement d'édition visuel intégré à GATE (http ://www.gate.ac.uk), notre outil débute par l'annotation des éléments géographiques dans les textes. Ceci étant accompli en définissant des règles d'annotation en langage de grammaire JAPE (http ://gate.ac.uk/sale/tao/splitch8.htm).
Une fois extraits et filtrés, ces éléments forment le modèle conceptuel de la base à créer en adoptant le SGBD postgresql (www.postgresql.org). La détection des données qui serviront à remplir la base a été accomplie sous le système Gate en intégrant deux sessions de MachineLearning pour appliquer l'apprentissage au corpus préalablement annoté et l'application des règles d'annotation générées à notre corpus. La base créée servira à enrichir la base existante par des nouveaux attributs selon les besoins de l'utilisateur(cf. figure 3).
Conclusion
Nous avons présenté dans cet article une approche pour automatiser la création et le remplissage d'une BDG pour enrichir les SIG. La mise en oeuvre de notre approche a donné naissance à l'outil baptisé GDB Generator. Ce dernier a été intégré au SIG OpenJUMP. Comme perspectives, nous proposons d'enrichir nos règles d'annotation en prévoyant plus de formes syntaxiques. La prise en compte de la composante spatio-temporelle est prévue aussi comme extension de notre approche.

Introduction
Les réseaux sociaux sont dynamiques par nature. La détection de communautés a longtemps considéré uniquement une vue statique : la capture du réseau à un instant t. Récemment, des travaux sur la dynamique des communautés ont vu le jour. Certains auteurs essayent de suivre l'évolution des communautés durant plusieurs tranches de temps Palla et al. (2007), d'autre proposent de mettre à jour les communautés existantes en fonction des nouveaux évé-nements qui se produisent (ajout ou suppression de noeuds et/ou de liens) Nguyen et al. (2011). Enfin les derniers essayent de trouver des communautés consistantes sur plusieurs tranches de temps Aynaud et Guillaume (2011).
Un des problème non encore exploré dans la littérature sur la dynamique des communautés est la prédiction : connaissant l'évolution du réseau jusqu'au temps t, peut-on prédire les communautés au temps t + 1 ? Dans cet article, nous proposons une approche générale de pré-diction des communauté basée sur la prédiction des interactions dans les réseaux complexes. Dans cette approche, étant donné l'évolution du réseau jusqu'au temps t, les interactions sont prédites pour le temps t + 1 et les communautés sont ensuite calculées sur ce réseau prédit. L'hypothèse qui soutient cette démarche est la suivante : si on est capable de prédire l'évo-lution du réseau avec précision, alors on peut utiliser le réseau prédit pour d'autres tâches de prédiction (ici la prédiction des communautés).
Dans la suite de cet article, nous présentons d'abord la prédiction des interactions (section 2) puis nous présentons son évaluation et son application à la prédiction des communautés (section 3). Nous terminons par des conclusions et perspectives (section 4).
Prédiction des interactions
Le problème de prédiction des interactions peut être défini comme suit : étant donné un réseau dynamique G = (G 1 , ..., G n ) dont les tranches de temps sont non cumulatives (les liens correspondent aux interaction de la tranche de temps uniquement) quelle sera la structure du réseau (G n+1 ) correspondant à la tranche de temps n + 1 ? Ce problème peut être vu comme une généralisation de la prévision de lien : on ne se limite pas aux liens non existants mais on vérifie aussi que les liens existants resterons présents. De ce fait, les mêmes classes de méthodes peuvent être utilisées pour le résoudre. Dans ce qui suit nous présentons une solution basée sur la similarité et une solution par apprentissage supervisée. Dans les approches proposées, le temps joue un rôle important.
Modèle basés sur la similarité
Dans cette approche une fonction de similarité est définie et sa valeur est calculée pour chaque paire de noeuds potentielle. Un seuil est ensuite choisi pour décider de la création des interactions. La forme générale de la mesure de similarité proposée est :
Dans l'équation 1, les fonctions f et g et les paramètres ? et ? sont à définir. W est la matrice des poids. f est la fonction temporelle, elle permet de prendre en compte l'âge des interactions (donner plus d'importance aux relations récentes par exemple). g est la fonction de similarité topologique qui mesure la proximité dans le graphe social. h est la fonction de similarité entre les attributs (lorsqu'ils sont disponibles). Enfin neigh(i) est une fonction de voisinage (les voisins, les voisins et leurs voisins, la communauté par exemple).
Ce modèle est très intuitif. Cependant, il ne peut pas modéliser une large classe de relations possible entre les variables d'entrée et la variable à prédire. Pour cette raison, nous proposons dans la suite un modèle plus général basé sur l'apprentissage supervisé. 
Evaluation et discussion
Les jeux de données utilisés pour les évaluations sont DBLP et Facebook wall. DBLP est un réseau de collaborations entre auteurs indexés sur http://dblp.uni-trier.de/. Pour chaque tranche de temps (années), une interaction existe entre deux auteurs s'ils ont au moins une publication commune. Les liens sont pondérés par le nombre de publications communes. Le jeu de données Facebook wall (http://konect.uni-koblenz.de/networks/ facebook-wosn-wall) est un réseau construit à partir d'un sous-ensemble d'utilisateurs de la New Orleans. Pour chaque année, une interaction existe entre deux utilisateurs si l'un deux a publié sur le mur de l'autre. Les liens sont pondérés par le nombre de publications.
En raison du déséquilibre entre les classes, l'aire sous la courbe ROC (Receiver Operating Characteristic) notée AUC (Area Under Curve) est utilisée pour évaluer les performances des approches de prévision des interactions.
La table 1 présente les résultats de l'évaluation de la prédiction des interactions. Dans le jeu de données DBLP, on a à disposition les titres des articles et les noms des conférences ou journaux dans lesquels ils sont publiés. Pour chaque auteur et chaque année, on peut donc construire un vecteur TFIDF(Term Frequency Inverse Document Frequency) relatif aux mots contenus dans les titres de ses publications et les noms des conférences et/ou journaux dans lesquels il a publié. Ce sont ces vecteurs qui sont utilisés comme attributs.
Enfin, les communautés sont calculées en utilisant l'algorithme décrit dans Ngonmang et al. Les résultats de l'évaluation du jeu de données DBLP sont présentés sur la figure 3 (a). On peut constater que pour plus de 30% des noeuds on a une prédiction parfaite avec une valeur de N M I = 1. Plus de 50% des noeuds produisent un NMI supérieur à 0, 8 et enfin, plus de 70% des noeuds produisent un N M I > 0, 6. Un constat similaire peut être fait pour les résultats sur Facebook wall's présentés à la figure 3 (b).
Conclusions et perspectives
Récemment, de nombreux travaux sur la détection des communautés dans les réseaux dynamiques ont commencés. Un des problèmes encore non exploré est la prédiction des communautés. Dans cet article, nous avons dans un premier temps proposé des modèles pour la prédiction des interactions (un basé sur la similarité et l'autre par apprentissage supervisé). Ensuite, nous avons utilisé ces modèles pour prédire les communautés. Des tests sur des jeux de données réels montre la faisabilité de notre approche.
En perspective, nous pensons prendre en compte l'arrivée de nouveaux noeuds.
Remerciement
Ce travail est partiellement financé par le projet FUI français AMMICO. 
Summary
In this paper, we propose a general approach for communities prediction based on a machine learning model predicting interaction in social networks. In fact, we believe that if one is able to predict the structure of the network with a high precision, then one just need to compute the communities on this predicted network to have the prediction of the community structure. Evaluation on real datasets (DBLP and Facebook walls) shows the feasibility of the approach.

Introduction
La veille économique s'inscrit aujourd'hui pleinement dans la stratégie de développement des entreprises. Or, la quantité d'informations à leur disposition est considérable, rendant l'analyse complexe. L'entreprise partenaire de ces travaux publie quotidiennement des articles synthétisant des informations économiques émanant de différentes sources ou fruit d'une dé-marche d'investigation. Afin de les adresser au mieux aux lecteurs concernés, nous dévelop-pons un outil efficace de recommandation de ces articles d'actualités économiques régionales, reposant sur son adéquation avec les besoins des utilisateurs. Pour personnaliser la recommandation, une enquête a été menée auprès des clients avec l'appui des experts du domaines. Cela nous a permis d'identifier trois critères principaux sur lesquels nous nous appuyons : les Thèmes (principaux événements économiques), les Secteurs d'activité et la Localisation des informations. Les besoins des utilisateurs et le contenu informationnel des articles sont repré-sentés par une description sémantique des connaissances de ces trois critères au sein d'une ontologie. Dans cet article, nous nous intéressons à la distinction entre pertinence et similarité, qui sont souvent amalgamés. Nous proposons donc une nouvelle mesure, Relevancy Measure, nous permettant de définir la pertinence d'un article pour un profil donné en nous appuyant sur leurs descriptions ontologiques. Nous utilisons un système de recommandation basé sur le contenu avec une approche vectorielle. Cet article est organisé de la façon suivante : nous commençons par présenter la génération des vecteurs, puis nous introduisons les notions de similarité et de pertinence et définissons la mesure Relevancy Measure. Enfin, la section 4 propose une évaluation de nos algorithmes avant la présentation de nos conclusions.
Vectorisation
La description de chaque article et profil est contenue dans une base de connaissances ontologique. Afin d'utiliser le modèle vectoriel, il est nécessaire de transformer ces descriptions en vecteurs. Cette modélisation est bien moins expressive qu'une ontologie car les dimensions étant orthogonales dans le modèle vectoriel, tous les éléments de chaque vecteur sont considé-rés comme indépendants (Voorhees, 1994).
Génération des vecteurs : La mise en relation d'un article avec les critères qui lui sont associés est réalisée de façon semi-automatique via la plate-forme GATE (Cunningham, 2002). Les résultats de l'analyse du texte par cette plateforme sont vérifiés, corrigés et validés par les rédacteurs de chacun des articles puis remontés dans l'ontologie. La création des profils est réalisée manuellement lors de l'inscription des clients. Les vecteurs décrivant articles et profils contiennent les critères qui leur ont été associés dans la base de connaissances ontologique.
Expansion des vecteurs : D'après ce qui a été dit précédemment, les vecteurs de description des articles et profils ne contiennent que les instances de critères avec lesquels ils sont directement en relation dans la base de connaissances. Nous nous basons sur les apports de (IJntema et al., 2010), dont nous avons adapté la méthode dite d'expansion de vecteurs afin de conserver les connaissances de l'ontologie dans les vecteurs. Les instances de chaque critère sont organisées de façon hiérarchique dans la base de connaissances. Pour chaque instance ajoutée aux vecteurs, les instances parents y sont elles aussi ajoutées.
Pertinence et Relevancy : P ertinence(x, y) : I ×I ? [0, 1] est une fonction qui permet de mesurer le degré de pertinence d'un article x vis-à-vis d'un profil y. Cette mesure de pertinence doit aussi respecter les propriétés de positivité et réflexivité. La pertinence est une notion largement utilisée dans le domaine de la recherche d'informations. Dans notre cas, la pertinence n'est pas binaire. Un article peut plus ou moins correspondre au besoin d'informations d'un utilisateur, c'est pourquoi nous utilisons le modèle vectoriel pour l'estimer. Contrairement aux approches classiques confondant les notions de similarité et de pertinence (Salton, 1971), nous les distinguons. Par exemple, si un profil montre un intérêt pour la Côte d'Or, il est préférable de lui recommander un article plus précis, qui traite de Dijon, qu'un article plus général qui traite de la Bourgogne. Il faut donc conserver une pertinence forte dans le sens de la spécia-lisation et la baisser fortement dans le sens de la généralisation du critère. Afin de résoudre D. Werner et al.
ce problème, nous utilisons un vecteur intermédiaire. Le sous-vecteur ? ? s c est composé des instances communes entre le vecteur de l'article ? ? a c et celui du profil ? ? p c . Ainsi, nous définissions la pertinence pour un critère donné c de la façon suivante :
Avec S c le sous-ensemble commun d'éléments de l'ensemble d'instances en relation à la fois avec le profil I 2,c = 4, car nous considérons que la perte de précision du profil par rapport à l'article ne doit pas influencer plus de 20% du résultat. De plus, la perte de précision de l'article par rapport au profil doit influencer fortement le résultat, ici 80%. La pertinence globale Relevancy( ? ? a , ? ? p ) est la somme des mesures de pertinence pour chacun des critères, éventuellement pondérées. Cette mesure est utilisée dans notre prototype pour trier les résultats (articles) proposés à l'utilisateur en fonction de son profil :
Expérimentations
Nous avons comparé de deux façons différentes (évaluation binaire et d'ordre) les résultats de la recommandation d'articles via les méthodes, similarité cosinus (C), similarité cosinus avec vecteur étendus (B) et Relevancy Measure avec vecteur étendus (A), qui permet de gérer la différence de précision entre les profils et les articles. Pour nos évaluations, un ensemble de 10 profils et 70 articles a été choisi. Pour l'évaluation binaire, une sélection manuelle des articles pertinents a été réalisée pour chaque profil par des experts. Pour l'évaluation de l'ordre, un classement manuel des articles pertinents a été réalisé pour chaque profil par des experts. Dans les deux cas, le travail des experts est considéré comme la recommandation idéale, avec laquelle sont comparés les résultats des différents algorithmes.
Evaluation Binaire : Pour évaluer la recommandation binaire nous utilisons les mesures classiques en recherche d'informations, précision, rappel et F1-mesure, (Lewis et Gale, 1994) qui produisent des scores allant de 0 à 1. Tous les articles dont la corrélation avec le profil est supérieure à un seuil de 0,5 sont conservés. Les résultats de l'évaluation de la recommandation  Salton, G. (1971). The SMART retrieval system -experiments in automatic document processing.
Voorhees, E. M. (1994). Query expansion using lexical-semantic relations. In SIGIR '94, pp. 61-69. Springer London.
Summary
Today in the commercial and financial sectors, staying informed about economic news is crucial and complex because of the huge amount of information. To address this problem, we propose an innovative article recommender system based on a knowledge ontological model. We present also a novel method to evaluate the relevancy based on vector space model that we have perfected to overcome the mix up existing in models between the concepts of similarity and relevancy.

Introduction
Une grande quantité d'information textuelle sur la musique peut être extraite du Web. On y trouve notamment des données générées par les utitisateurs finaux (p. ex. : tags, critiques), des métadonnées (p. ex. : date de sortie, nom du parolier ) et, enfin, les paroles des chansons. Ces informations peuvent souvent être moissonnées au moyen des API qu'offre un nombre croissant de services musicaux sur le Web, de même qu'avec l'aide d'outils développés par la communauté de chercheurs dans le domaine de la recherche d'information musicale. Cependant, les paroles de chansons ont reçu relativement peu d'attention de la part des développeurs des systèmes de repérage pour la musique. Or, la recherche à partir des thèmes abordés dans les paroles peut être pertinente dans certains contextes, pour les chercheurs s'intéressant à la musique populaire ou pour toute personne souhaitant trouver une musique pour un événement particulier (mariage, funérailles). Nous avons donc construit un système d'exploration d'une collection de chansons à partir des paroles. Après la constitution du corpus à partir de données provenant de diverses sources sur le Web, nous avons utilisé des algorithmes de fouille de textes pour en détecter les structures thématiques puis développé une interface de visualisation afin de naviguer dans la collection. Dans cet article, nous expliquons comment les données ont été recueillies et décrivons les différents traitements ayant été appliqués. Nous présentons également l'interface de visualisation qui en résulte.
Travaux connexes
Étant donné l'abondance d'informations musicales disponibles en accès libre sur le Web, il n'est pas surprenant de constater qu'un grand nombre de chercheurs ont développé des outils afin de collecter ces informations afin de faciliter le repérage de la musique. Logan et al. (2004)   Kleedorfer et al. (2008) appliquent différentes techniques de fouille ainsi qu'une factorisation en matrices non-négatives (NMF) pour créer des clusters à partir des paroles, avec l'objectif de permettre l'exploration d'une collection de chansons. La visualisation de corpus musicaux a aussi intéressé des chercheurs : plusieurs ont développé des représentations visuelles statiques pour l'exploration de collections où l'espace de représentation est organisé à l'aide des cartes auto-organisatrices de Kohonen. Pour ce faire, la similarité entre chansons est calculée sur la base des fichiers audio (voir Islands of Music (Pampalk, 2001) et MusicRainbow (Pampalk et Goto, 2006)) ou des paroles dans le cas de SongWords, une application pour tablette tactile Baur et al. (2010). Cependant, l'analyse des paroles se limite uniquement à une approche par Tf * idf. En conséquence, il semble pertinent de travailler au développement d'interfaces pour l'exploration de collections de chansons à partir des paroles afin de répondre aux besoins décrits précédemment.
Traitement des données
Dans cette section, nous présentons la méthodologie utilisée pour le traitement des données.
Filtrages et prétraitement linguistiques
Différents prétraitements linguistiques sont effectués sur les données : conversion des majuscules en minuscules, retrait des chiffres et des nombres (numériques et textuels), des accents et des symboles. Afin d'éviter l'introduction de bruit dans le modèle, nous utilisons un antidictionnaire classique enrichi de termes extraits de la langue populaire du Québec et de France que l'on retrouve fréquemment dans les chansons (p. ex. : « té » (tu es), « chu » (je suis), « c'te » (cette)). Nous appliquons finalement un processus de lemmatisation simple afin de ré-duire considérablement les dimensions de l'espace tout en augmentant la fréquence des termes canoniques. Les premiers tests ont cependant montré que, malgré ces traitements, la taille du lexique était toujours importante. Nous présentons donc deux méthodes de sélection : à l'aide de l'analyse sémantique latente (LSA), afin de retenir uniquement les termes les plus repré-sentatifs, et en effectuant une sélection drastique consistant à ne retenir qu'un pourcentage des termes les plus fréquents.
Clustering des données
Après l'étape de prétraitement, nous appliquons un algorithme de K-moyennes afin d'identifier des clusters de chansons partageant des descripteurs similaires. L'évaluation des résultats d'algorithmes de clustering reste encore aujourd'hui une problématique ouverte importante, particulièrement lorsqu'il n'existe pas de référence. Nous avons évalué le clustering à l'aide de la mesure Silhouette (Rousseeuw, 1987), laquelle permet de mesurer la cohésion ainsi que la distinction des clusters. Nous avons fait varier le nombre de clusters, la mesure de distance (e : Euclidienne, b : Manhattan) ainsi que la méthode de sélection des descripteurs. La baseline est calculée en faisant un tirage aléatoire. Les résultats de l'évaluation (tableau 1) sont assez TAB. 1 -Évaluation du clustering à l'aide de la mesure Silhouette faibles, suggérant des clusters proches les uns des autres et une certaine confusion sur le plan de la classification des chansons. La mesure Silhouette ne permet cependant pas de prendre en compte les chevauchements thématiques ni des spécificités du traitement de données textuelles. La méthode de sélection par LSA présente néanmoins des résultats intéressants étant donné le faible nombre de descripteurs (environ une centaine). Compte tenu des résultats précédents, nous avons choisi pour la suite des expériences de ne retenir que 2,5 % des termes les plus fréquents, fixé à 5 le nombre de clusters et utilisé une distance euclidienne. Une fois l'étape de clustering terminée, nous utilisons le framework Gensim afin d'indexer chaque cluster sépa-rément et de transformer chaque sous-collection en un modèle LSA (Deerwester et al., 1990). Cette transformation permet de récupérer les mots les plus représentatifs pour chaque cluster, que nous appellerons par la suite les mots-clés thématiques, qui peuvent apparaître comme les thèmes associés à chaque cluster. Enfin, nous calculons la distance entre chaque chanson et ces mots-clés, ainsi qu'entre les chansons elles-mêmes avec un seuil minimum (0,4)  
Visualisation des données
Plusieurs outils ont été élaborés pour permettre la visualisation des réseaux, tel que Gephi (Bastian et al., 2009) ou Tulip (Auber, 2003). Dans le cadre de ce projet, nous avons utilisé Gephi, un logiciel libre flexible, puissant et particulièrement adapté pour mettre en lumière la structure des associations entre les noeuds d'un réseaux ou d'un graphe. Afin de produire une visualisation des données, nous avons transformé le modèle en un ensemble de noeuds (mots-clés thématiques, artistes et chansons) et de liens tels que :
• Chaque mot-clé thématique est connecté aux chansons du cluster avec un poids déter-miné à l'aide du cosinus.
• Chaque chanson est connectée aux autres chansons avec un poids déterminé à l'aide du cosinus (la connexion n'est retenue que si le poids est supérieur au seuil de 0,4).
• Chaque artiste est connecté à ses chansons ainsi qu'aux différents mots-clés thématiques en fonction du nombre d'occurrences de ceux-ci dans les chansons. Une fois les relations entre tous les éléments définies, une spatialisation est effectuée avec l'algorithme Force Atlas de Gephi. Le résultat est exporté par la suite vers l'interface Web.
Interface de visualisation
La version actuelle de l'interface contient 4 579 noeuds et 7 789 liens. Les points rouges représentent les mots-clés thématiques. La couleur des noeuds représentant chaque chanson ainsi que celle des liens est déterminée en fonction de son cluster d'appartenance. La sélec-tion d'une chanson ouvre un panneau latéral avec les métadonnées (le nom du chanteur, la couverture de l'album, etc.) ainsi qu'un nuage des termes les plus fréquents de cette chanson. Par ailleurs, les utilisateurs peuvent sélectionner n'importe quel noeud pour obtenir une vue détaillée ou encore zoomer sur une zone particulière. Il est également possible d'effectuer des recherches par mots-clés ou de visualiser chaque cluster séparément. Le système filtre alors la vue courante afin d'afficher le noeud correspondant ainsi que tous les autres noeuds auxquels il est lié. La figure 1 présente une vue générale de la visualisation. Comme mentionné précédem-ment, la collection a été divisée en 5 clusters. Le tableau 2 présente les mots-clés thématiques pour chacun des clusters : Notamment, on remarque que le premier cluster semble réunir des chansons qui invitent à danser, bouger et chanter. On y trouve entre autres les chansons « Tu vas au bal » et « Père Noël noir », deux chansons à la fois drôles et entraînantes de Renaud. Le deuxième cluster regroupe plutôt des chansons sur l'amour, par exemple la chanson « Mistral gagnant » de Renaud qui parle de l'amour d'un père pour sa fille. En revanche, les mots clés thématiques « revenir » et « regretter » du cluster bleu suggèrent des chansons à propos d'amours déçues ou imaginaires. On y retrouve ainsi la chanson « Me jette pas » de Renaud. 
Conclusion et perspectives
Dans cet article, nous avons présenté une méthodologie de fouille de textes permettant d'indexer et de visualiser une collection de chansons. Ce projet est basé sur l'hypothèse que l'analyse, l'organisation et la visualisation des paroles de chansons peuvent permettre aux utilisateurs de naviguer efficacement dans le contenu d'une grande collection musicale. Notre système trouve des applications dans une grande variété de contextes. Nous prévoyons ainsi proposer une visualisation des chansons par décennie et par pays afin de permettre aux chercheurs de comparer les thèmes principaux abordés dans les chansons en fonction de l'époque ou de l'origine de la chanson. En combinant l'analyse thématique des paroles au genre du chanteur, il serait possible aux chercheurs de répondre à une question telle que « Les thèmes abordés dans les chansons par les artistes masculins et féminins diffèrent-ils ? ». Nous prévoyons par ailleurs continuer à augmenter la taille du corpus et évaluer l'ergonomie de l'interface avec des chercheurs s'intéressant à la musique populaire.
Summary
In this paper, we present a text mining methodology and an information visualization interface that allows users to browse a large collection of French-language songs based on lyrics. We first harvested lyrics and metadata from various sources on the Web. After data preprocessing, we used clustering and Latent Semantic Analysis to identify a thematic structure and determine significant features. We then transformed the resulting model into a set of nodes and edges to obtain an interactive visualization system for the exploration of our song collection.

Introduction
Dans la plupart des pays développés, des bases de données géographiques (BD) sont initiées, même si certaines régions ne sont couvertes que partiellement. En particulier, des BD d'occupation du sol (OCS) à grande échelle sont en cours de réalisation, par agrégation de données existantes et leur assemblage ne permet pas une description complète du territoire. D'autre part, des images satellite de résolution sub-métrique couvrent avec une grande préci-sion géométrique de larges territoires et peuvent donc aider à compléter ces BD d'OCS. Les méthodes de classification supervisée sont largement utilisées pour résoudre ce genre de problème de télédétection (Mountrakis et al., 2011). Cependant, elles sont souvent peu robustes, présentent un fort taux de confusion, sont limitées à certaines thématiques. Enfin, elles né-cessitent une sélection manuelle des classes et des zones d'apprentissage afin de traiter le cas de classes composées de différentes apparences. Nous proposons ainsi ici une méthode d'inspection hiérarchique d'une BD existante, permettant d'apprendre indifféremment la (ou les) apparence(s) de chaque thème qui la constitue. Ces informations sont ensuite fusionnées à différents niveaux afin d'obtenir des résultats plus robustes.
Méthodologie
La structure hiérarchique des BD géographiques permet de faire ressortir trois niveaux possibles d'inspection : (1) le niveau objet, (2) le niveau thème, (3) le niveau BD. La BD initiale est projetée sur l'image, formant une carte qui lui est superposable et où les pixels sont étiquetés par un thème : un pixel de cette grille ne peut avoir qu'une seule étiquette. Cependant, il peut aussi n'appartenir à aucun thème ("non étiqueté") et leur étiquetage est l'objet de cette étude. Notre méthode est fondée sur le principe d'une inspection hiérarchique, de manière ascendante. D'abord, à chaque objet de la BD est associée une classification de toute l'image. Puis, les classifications sont fusionnées au niveau du thème et la décision finale d'éti-quetage est prise pour chaque pixel de l'image (Pal, 2008). Le premier niveau d'inspection permet d'apprendre l'apparence de chacun des objets. Il est composé de deux étapes : (1) une sélection, d'un sous-ensemble de pixels intérieurs et d'un extérieur qui permet de discriminer le mieux l'objet du reste de l'image et (2) une classification des pixels de toute l'image en deux classes (intérieur / extérieur). La sélection des sous-ensembles de pixels est fondée sur la maximisation du rappel d'une classification à deux classes des pixels de l'objet. L'étape de classification permet d'obtenir pour chaque objet de chaque thème de la BD, une carte d'appartenance au thème de l'objet. L'étape de fusion par thème permet de prendre en compte les différentes apparences d'un thème et de ne privilégier aucune apparence. À cette étape, toutes les classifications calculées au niveau de l'objet sont fusionnées par thème, afin d'obtenir une seule carte d'appartenance. On la considère comme la probabilité de chaque pixel de l'image d'appartenir au thème courant. Enfin, une décision d'étiquetage est prise pour chaque pixel de l'image en intégrant les résultats de toutes les cartes d'appartenance de chaque thème. Une nouvelle classification est obtenue, permettant d'associer à chaque pixel une étiquette de la BD initiale. Cette classification est accompagnée d'une mesure de confiance dans la classification, dérivant des mesures d'appartenance par thème, qui permet de définir des zones d'incertitudes pouvant être étudiées à posteriori par un opérateur.
Expérimentation
Une vérité terrain de qualité est difficile à obtenir sur des bases de données couplées à des images satellites, ainsi nous avons décidé de générer une image à partir d'une image réelle (image Pléiades à 50 cm de résolution), pour laquelle on connaît a priori le thème d'appartenance (par rapport à une nomenclature donnée). L'image reconstituée est composée d'échan-tillons d'une image Pléiades réelle (Fig. 1a), étiquetés en 9 thèmes eux-mêmes comprenant 10 objets (Fig. 1b). Les objets (100 pixels de côté) sont répartis sur une grille régulière. La diagonale de l'image correspond à une zone non étiquetée dans la BD initiale, elle est composée de pixels pouvant appartenir à différents thèmes présents ou non dans la BD. Les 9 thèmes sé-lectionnés sont présentés dans la figure 1c. Cela correspond à une nomenclature très détaillée (thèmes aux apparences très proches) et à notre connaissance, aucun article n'a jusqu'à présent tenté d'aborder un tel discernement. Afin de comparer notre méthode à l'existant, et d'étudier l'influence du nombre d'objets par thème de la BD, trois méthodes ont été appliquées sur diffé-rentes BD initiales (de 1 à 9 objets par thème) : (a) un SVM multi-classes, (b) les RF (Breiman, 2001) et (c) notre méthode. Les résultats des classifications pour la BD contenant 9 objets par thèmes sont visibles sur les figure 1d,e & f. La courbe de l'évolution du score de bonne classification (pourcentage de pixels bien classés) en fonction du nombre d'objets présents dans la BD simulée est présentée dans la figure 2. Les résultats de notre méthode montrent qu'avec seulement 10% des objets dans la base, un taux de bonne classification supérieur à 70% des pixels est obtenu. Ce score atteint les 80% dès que l'apprentissage est réalisé sur 30% des objets. Les trois classifications sont visuellement proches. Cependant, on peut noter que certains A. Gressin et al. thèmes sont confondus dans le cas des RF, alors qu'ils sont bien dissociés avec les SVM. Notre méthode donne des résultats moins bruités que les RF, mais plus confus que les SVM. L'évolu-tion du score de bonne classification en fonction du nombre d'objets dans la BD initiale, pour chacune des trois méthodes comparées, est montrée sur la figure 2. Les résultats de notre mé-thode sont de meilleure qualité que les RF, quel que soit le nombre d'objets dans la BD initiale. Les SVM obtiennent des résultats en moyenne légèrement supérieurs, mais sont moins stables que notre méthode (e.g. score de bonne classification inférieur pour trois objets), en particulier quand le nombre d'objets est faible. Cette instabilité peut être expliquée par le choix des pixels d'apprentissage par tirage aléatoire.  
Conclusion et perspectives
Dans cet article, nous avons présenté une méthode tirant profit d'une BD existante, pour apprendre les différentes apparences de chaque thème qui la compose. Ce processus a été appliqué avec succès à un jeu de données complexe simulé et comparé favorablement à plusieurs méthodes standards de classification supervisée. D'autre part, ce procédé a été développé dans un cadre plus général de détection de changements et de mise à jour de bases de données (Gressin et al., 2013) 

Introduction
Les graphes orientés sont des structures adaptées à la modélisation d'un grand nombre de données complexes présentes dans le monde réel. Les réseaux de régulation génétique, par exemple, sont des graphes orientés où les noeuds représentent des gènes et les arcs, des relations d'inhibition ou d'activation. Le Web est modélisé par un graphe orienté dans lequel les noeuds sont des pages et les arcs des liens hypertextes. La circulation des emails dans une organisation, les graphes sociaux dans lesquels un individu peut suivre les activités d'autres personnes, les réseaux de citations dans lesquels un article cite d'autres articles sont également représentés par des graphes orientés.
Du fait de ces nombreuses applications, ces structures sont beaucoup étudiées en théorie des graphes, et, plus récemment, dans le contexte de la fouille de données. La plupart des travaux portent sur des graphes non étiquetés ou dont les noeuds ne sont associés qu'à une unique étiquette. Cependant, dans les graphes modélisant des données réelles, les noeuds, qui représentent des objets, peuvent être associés à de nombreuses caractéristiques. Les graphes dans lesquels les noeuds sont annotés avec des ensembles d'attributs (ou itemsets) sont appelés graphes attribués et à l'heure actuelle, encore peu de travaux se consacrent à leur analyse.
L'un des problèmes phares abordés dans la fouille de graphes étiquetés concerne l'identification de sous-graphes qui apparaissent avec une fréquence minimum spécifiée par l'utilisateur. Ces motifs récurrents peuvent apporter un éclairage particulier sur la manière dont les noeuds sont organisés. Fouiller des graphes attribués est d'un grand intérêt, car cela permet d'identifier des motifs structurels, mais également, de mettre en évidence des liens entre les attributs des noeuds.
Deux raisons principales rendent la fouille des graphes attribués très difficile. En premier lieu, il est nécessaire de combiner de manière non triviale l'exploration de la structure du graphe avec l'identification d'itemsets fréquents associés aux noeuds. La seconde raison est que, comme pour la fouille des graphes étiquetés, les performances sont très impactées par la présence de sous-graphes isomorphes (Yan et Han, 2002). Comme indiqué par (Yan et Han, 2002), dans le cas de graphes creux avec des labels diversifiés, le nombre de sous-graphes isomorphes est la plupart du temps faible. Cela n'est plus le cas dans le cas des graphes attribués où la combinatoire sur les itemsets génère fréquemment un nombre important d'isomorphismes de sous-graphes.
Les contributions clés de notre travail sont les suivantes : i) nous présentons le problème de la fouille de structures fréquentes dans un ou plusieurs graphes attribués orientés, puis, ii) nous montrons comment l'identification des motifs fréquents peut être réalisée en adoptant une stratégie d'exploration de l'arbre couvrant de chaque noeud. iii) Nous définissons une nouvelle forme canonique adaptée au parcours de l'espace de recherche, iv) nous détail-lons la manière d'étendre les motifs avec des arcs réentrants et v) nous traitons de la prise en compte des cycles. vi) Nous analysons le cas particulier des motifs automorphes et nous montrons comment l'explosion combinatoire provoquée par l'isomorphisme de sous-graphes peut être considérablement limitée. Avant de conclure, vii) nous présentons les résultats obtenus en analysant plusieurs jeux de données artificiels et réels avec l'algorithme AADAGE (Automorphism Aware Directed Attributed Graph Explorer) que nous avons développé.
Etat de l'art
De nombreux algorithmes ont été proposés pour fouiller des graphes étiquetés. La quasitotalité des solutions proposées découlent de techniques développées pour la fouille d'itemsets et sont basées sur une même idée qui consiste à explorer l'espace des solutions en faisant grossir des sous-graphes candidats et en éliminant les motifs infréquents (Agrawal et al., 1993;Jiang et al., 2013). Là où les méthodes se différencient, c'est dans leur manière de parcourir l'espace de recherche et d'éviter la génération de solutions redondantes. En effet, contrairement à la fouille d'itemsets où il est facile de générer toutes les solutions de manière unique, dans la fouille de graphes, il est très fréquent de construire plusieurs fois le même motif.
L'une des méthodes privilégiées pour éviter d'explorer plusieurs fois une même partie de l'espace de recherche est d'ajouter des arcs et des noeuds aux motifs candidats de manière ordonnée et d'utiliser une représentation canonique des sous-graphes. L'idée sous-jacente à l'utilisation des formes canoniques est de n'étendre qu'une seule des solutions isomorphes. Pour éviter d'effectuer des tests coûteux d'isomorphisme complets, une représentation canonique adaptée à la manière d'étendre les sous-graphes est choisie. Cette forme canonique permet de décider rapidement si un motif a déjà été examiné et donc, ne doit pas être étendu. La solution la plus utilisée est de créer une séquence de tous les arcs contenus dans le sous-graphe dans l'ordre ou les arcs ont été ajoutés (Borgelt, 2007;Yan et Han, 2002). L'utilisation des formes canoniques, si elle permet d'éviter d'explorer plusieurs fois le même sous-graphe, ne dispense pas de la nécessité d'identifier toutes les manières de générer les motifs. Ainsi, si l'on ordonne les noeuds dans l'ordre lexicographique de leurs étiquettes, le motif de la figure 1(b), est canonique, mais il est présent 4 fois dans le graphe initial (a). Les motifs (c) et (d), qui sont eux aussi canoniques, peuvent être générés de 12 façons différentes (dans le graphe initial, il y a 12 manières possibles de choisir 2 noeuds b parmi les 4 fils étiquetés b du noeud a). Il est nécessaire de tenir compte de toutes ces formes car la configuration choisie conditionne les extensions futures du motif ; par exemple, la possibilité ou non d'étendre chacun des noeuds b avec un noeud étiqueté c. Les motifs qui possèdent de nombreux isomorphismes de sous-graphes avec le graphe analysé posent des difficultés à tous les algorithmes existants, car le problème d'isomorphismes de sous-graphes en lui même est NPcomplet. Les motifs (c) et (d), qui possèdent des automorphismes posent le plus de problèmes et contribuent de manière importante à la dégradation des performances des algorithmes, car le nombre d'isomorphismes de sous-graphes avec le graphe initial est augmenté.
Dans le cas des graphes étiquetés creux, ce nombre reste toutefois relativement raisonnable. Au contraire, dans les graphes attribués, puisque plusieurs attributs sont associés aux noeuds, la probabilité d'observer des sous-graphes partageant un ou plusieurs attributs identiques est grandement augmentée. Le problème devient alors très présent. Une manière de le contourner est d'éliminer du graphe les items les plus fréquents mais l'on se prive de tous les motifs contenant cet attribut, y compris certains qui peuvent potentiellement être intéressants.
Certaines études traitent spécifiquement de l'analyse des graphes dans lesquels les noeuds sont associés à des itemsets comme les travaux de Miyoshi et al. (2009) ou ceux de Fukuzaki et al. (2010). Les problèmes abordés dans ces études sont différents du notre dans le sens où ils traitent de l'analyse de graphes non orientés et ont pour but de rechercher des sous-graphes partageant les mêmes itemsets.
Dans notre travail, nous cherchons à identifier la présence récurrente de certains attributs associés à des noeuds connectés. Dans ses objectifs, notre travail est plus proche de la fouille de séquence ou d'arbres attribués. Dans une précédent étude Pasquier et al. (2013), nous avons travaillé sur l'identification de sous structures reflétant des évolutions d'itemsets. Cependant, l'algorithme que nous avions proposé ne s'appliquait que sur des jeux de données prenant la forme d'arbres attribués.
Concepts généraux et définition du problème
Dans cette section, nous introduisons les concepts et définitions nécessaires et présentons le problème de la fouille de graphes attribués orientés.
Préliminaires
Soit I = {i 1 , i 2 , .., i n } un ensemble d'items. Un itemset est un ensemble P ? I. Nous considérons que les items appartenant à un itemset sont triés selon l'ordre lexicographique et peuvent être accédés par leur index (p.ex. I 2 pour référencer le deuxième item de I).
Un graphe attribué orienté
La notation pow(I) représente l'ensemble des parties de I. Pour un arc (u, v) ? E, u est le parent de v et v est le fils de u. Il existe un cycle dans le graphe orienté si un chemin peut être trouvé entre un noeud et lui même. Un graphe attribué orienté est enraciné si l'on peut trouver un noeud v tel qu'il existe un chemin entre v et n'importe quel autre noeud du graphe.
Deux graphes attribués
Chaque graphe attribué G 1 qui est isomorphique à G définit un plongement (embedding en anglais) de G 1 dans G. A noter que, dans certains cas, G 1 peut être plongé plusieurs fois dans G en effectuant des permutations dans la mise en correspondance des noeuds.
Définition du problème
Les données à analyser sont disponibles soit sous la forme d'une base de données B de graphes orientés attribués, soit sous la forme d'un unique graphe orienté attribué G, non nécessairement connexe. Dans le premier cas, il est possible de calculer, pour chaque motif identifié P , une fréquence par transaction qui est donnée par le nombre de graphes dans B pour lesquels P est un sous-graphe attribué. Dans le second cas, nous utilisons la mesure de fréquence définie par Bringmann et Nijssen (2008) qui est égale au nombre minimal d'occurrences dans G de chacun des noeuds du motif. Nous pouvons avec cette mesure calculer une fréquence relative qui est donnée par la fréquence absolue divisée par le nombre de noeuds du graphe. Nous considérons qu'un motif est fréquent si sa fréquence est supérieure ou égale à un seuil minimum. Le problème consiste à énumérer tous les motifs fréquents présents dans un jeu de données.
La forme canonique que nous utilisons n'est pas basée sur une séquence d'arcs, mais sur une liste de noeuds dans l'ordre dans lequel ils ont été ajoutés à l'arbre couvrant du sousgraphe en effectuant un parcours en profondeur d'abord. Pour identifier de manière non ambiguë chaque arbre couvrant, il est suffisant d'identifier chaque noeud par ses attributs et sa profondeur. Comme nous devons également représenter les liens réentrants, nous utilisons un troisième attribut qui contient l'index du noeud pointé si celui-ci est déjà présent dans le motif.
Ordre portant sur les itemsets
Dans notre application, les noeuds sont associés à des itemsets et il convient de définir un ordre total sur les itemsets. Etant donné deux itemsets, I et
Ordre portant sur les noeuds
Le code d'un noeud est un triplet (d, Q, p) où d représente la profondeur du noeud dans l'arbre couvrant, Q représente l'ensemble des items associés au noeud et p vaut 0 si le noeud est utilisé pour la première fois ou est égal au numéro d'ordre du noeud si celui-ci est déjà présent dans le motif. Pour comparer deux noeuds, il suffit de comparer leurs codes sous forme de triplets. Soit 
Ordre portant sur les sous-graphes attribués
Étant donné un motif M de racine r(M ), nous définissons un code sur la racine de ce motif code(r(M )), obtenu en concaténant les codes des noeuds présents dans le motif dans l'ordre dans lequel ils ont été ajoutés. La forme canonique d'un sous-graphe attribué est déterminée simplement en sélectionnant le plus petit code.
Soit deux motifs M 1 et M 2 représentés respectivement par les codes
n 2 }, on dit que C 1 < C 2 ssi l'une des assertions suivantes est vraie : (i)
La figure 2 présente quatre motifs isomorphes d'un même graphe orienté attribué. Leurs codes sont respectivement représentés par les séquences suivantes :
code(r(M 1 )) = (0,a,0) (1,cd,0) (2,cd,0) (2,cd,0) (1,cd,4) (1,cd,0) (2,cd,0) code(r(M 2 )) = (0,a,0) (1,cd,0) (2,cd,0) (2,cd,0) (1,cd,3) (1,cd,0) (2,cd,0) code(r(M 3 )) = (0,a,0) (1,cd,0) (2,cd,0) (2,cd,0) (1,cd,0) (2,cd,0) (1,cd,4) code(r(M 4 )) = (0,a,0) (1,cd,0) (2,cd,0) (2,cd,0) (1,cd,0) (2,cd,0) (1,cd,3)
En utilisant l'ordre des triplets définie précédemment, on trouve que code(r(M 4 )) < code(r(M 3 )) < code(r(M 2 )) < code(r(M 1 )), donc le motif M 4 est sous forme canonique.
Parcours de l'espace de recherche
Tous les items présents dans le graphe de départ sont listés et constituent les motifs initiaux qui vont être progressivement étendus. L'arbre de recherche est construit en utilisant l'ordre défini précédemment sur les graphes orientés attribués, en effectuant un parcours en profondeur d'abord.
Enumération des arbres couvrants
L'utilisation de la stratégie définie par Pasquier et al. (2013) permet d'énumérer tous les arbres couvrants. La génération de nouveaux motifs est effectuée par extension des noeuds figurant dans le chemin le plus à droite du motif examiné (rightmost path extension) (Chi et al., 2004). Deux types d'extensions sont possibles : l'extension d'itemset permet d'ajouter un nouvel item à l'itemset associé au noeud terminal le plus à droite, l'extension de structure consiste à ajouter un fils à l'un des noeuds composant le chemin droit. La méthode est complète, mais peut générer des motifs redondants qui prennent la forme d'arbres isomorphes. A chaque génération, les motifs non canoniques sont écartés. L'inconvénient de cette approche est qu'elle ne permet de ne trouver que des motifs enracinés.
Identification des motifs canoniques et automorphes
Le code sur les graphes défini précédemment possède, comme la plupart des codes utilisés par les algorithmes de fouille de graphes, la propriété suivante : chaque préfixe d'un code canonique est lui même canonique (Huan et al., 2003;Yan et Han, 2002). L'extension d'un motif ne change pas l'ordre des triplets lors de la génération d'un motif canonique. Ainsi, chaque motif canonique ne peut être obtenu qu'en ajoutant un nouveau triplet ou en ajoutant un item à l'itemset associé au dernier triplet.
En utilisant la stratégie d'extension du chemin droit, il est possible de vérifier la canonicité d'un motif en ne testant que les noeuds appartenant au chemin droit. Soit les fonctions l et p qui, appliquées à un noeud retournent respectivement son dernier et son avant-dernier fils. Il est possible de déterminer la canonicité d'un motif de la manière suivante : pour tous les noeuds n composant le chemin droit, si ?p(n) et code(p(n)) ? code(l(n)), alors, le motif est sous forme canonique. Il est, de la même manière, assez aisé d'identifier les extensions qui produisent des motifs automorphes. S'il existe un noeud n dans le chemin droit du motif tel que n a plus d'un fils et code(p(n)) = code(l(n)), alors le motif possède des automorphismes. Sur un tel motif, on appelle noeud de séparation le noeud du chemin droit, le plus proche de la racine, qui possède plus d'un fils. Ainsi, les motifs illustrés en figures 1(c) et 1(d) possèdent des automorphismes. Dans ces deux figures, le noeud de séparation est le noeud a.
Prise en compte des liens réentrants
A partir de la stratégie d'énumération des arbres couvrants décrite précédemment, il est possible de définir une méthode permettant d'énumérer tous les sous-graphes. Pour cela, il faut identifier lorsqu'une extension structurelle ajoute un noeud qui est déjà présent dans le motif examiné. Lorsque ce cas se produit, le noeud est ajouté dans le motif comme s'il s'agissait d'une extension structurelle normale, mais l'on indique qu'il s'agit d'une réutilisation d'un noeud déjà présent en mémorisant l'index du noeud pointé. L'exploration de l'espace de recherche est stoppée à partir de ce noeud. En effet, du fait de notre stratégie d'exploration, nous sommes certains que tous les noeuds faisant partie du chemin le plus à droite du motif examiné sont étendus lors de la phase de génération en cours. Toutes les structures ne faisant pas partie du chemin le plus à droite ont quant à elles, déjà été complètement explorées. Concrètement, la réutilisation d'un noeud déjà présent est reflétée dans le code du graphe par l'ajout d'un triplet qui fait référence à un autre triplet. Cela ne génère aucun changement concernant les propriétés du code, la complétude de la méthode et la manière d'éliminer les formes non canoniques. 
Prise en compte des cycles
L'extension d'un motif peut donner lieu à la création d'un cycle si et seulement si le nouveau noeud ajouté pointe sur l'un des noeuds déjà présents dans le chemin droit. Lorsqu'un cycle est créé, on procède de la même manière que pour les autres liens réentrants : un nouveau noeud est ajouté avec l'index du noeud réutilisé, puis les extensions à partir de ce noeud sont stoppées. La complétude de la méthode n'est pas affectée par cette opération. Il y a cependant un problème au niveau de la redondance des solutions si le noeud réutilisé est la racine du motif car dans ce cas, de nombreux graphes isomorphes sont générés en débutant l'exploration à partir des autres noeuds appartenant au chemin droit.
Nous résolvons ce problème en générant tous les motifs ayant pour racine l'un des noeuds du chemin droit. En comparant les codes des motifs ainsi obtenus, nous en choisissons un comme motif canonique (le plus petit code) et ne continuons l'exploration que pour ce motif. Dans la figure 3, M 1 représente un motif trouvé dans le graphe G. Le noeud d, qui est à la racine du motif vient d'être ajouté ce qui crée un cycle. Le cycle est composé des 3 noeuds appartenant au chemin droit (d, c et bc). Le motif enraciné en d est connu, il reste donc à générer deux autres motifs en fixant arbitrairement leur racine à bc (motif M 2 ) et c (motif M 3 ). Le motif qui a le plus petit code est bien évidemment celui enraciné en bc donc nous stoppons ici l'exploration du motif M 1 .
6 Traitement des motifs automorphes L'utilisation de formes canoniques permet d'élaguer l'espace de recherche et cette optimisation est utilisée dans notre algorithme. Cependant, cela ne permet pas de résoudre le problème posé par la présence de motifs automorphes. Prenons par exemple le graphe orienté attribué représenté dans la figure 4(a). Ce graphe contient plusieurs sous-structures automorphes qui rendent l'élagage basé sur l'utilisation des formes canoniques moins opérant. Lorsque le motif illustré dans la figure 4(b) est analysé, il est conservé et étendu puisqu'il est sous forme canonique. Or, ce motif peut être obtenu de douze façons différentes. L'algorithme naïf consiste, pour chacune de ces formes, à étendre le motif en utilisant comme défini précédemment, l'extension d'itemset ou l'extension de structure.
L'extension d'itemset génère le motif illustré en (c). Ce motif, qui n'est pas sous forme canonique, sera écarté. L'extension de structure à partir du noeud b génère l'un des motifs décrits en (e). Dans tous les cas, le motif obtenu n'est pas sous forme canonique donc il sera écarté. La seule opération qui permet de générer un motif canonique est l'extension de structure à partir du noeud de séparation (d).
Lorsque l'on identifie un nouveau motif possédant des automorphismes, on sait que l'on ne pourra pas faire d'extension sur le dernier noeud, ni sur tous les noeuds situés après le noeud de séparation. En effet, pour un noeud de séparation n, on a code(p(n)) = code(l(n)) donc, n'importe quelle extension effectuée sur le motif de racine l(n) rendra son code inférieur à p(n), donc le motif ne sera plus canonique. Nous pouvons utiliser ce fait pour éviter de générer inutilement des motifs. Ceci est une première optimisation, mais son impact est limité.
La seconde optimisation consiste à supprimer certaines manière d'obtenir des motifs automorphes qui ne permettent pas de générer de nouveaux motifs canoniques. Dans la figure 1, par exemple, le motif (d), qui est obtenu de 12 façons différentes, ne peut être étendu que par l'ajout d'un troisième fils au noeud de séparation a. Quelle que soit la manière dont le motif (d) a été obtenu, les possibilités d'extension sont inchangées. Il est donc possible, dans ce cas, sans omettre de solution, de ne conserver qu'une seule mise en correspondance du motif ; n'importe laquelle.
Avec le graphe tel que présenté dans la figure 4(a), la situation est plus compliquée car il est possible, à partir du motif, d'ajouter un fils c au noeud a. Dans ce cas, il sera possible lors de la génération suivante, d'appliquer n'importe quel type d'extension sur tous les noeuds du chemin droit. Dans l'exemple, on pourrait en particulier ajouter e, f , g ou h en tant que fils du noeud c. Pour que tous les motifs possibles puissent être générés, il faut conserver les possibilités d'étendre le motif avec 2 noeuds bc parmi les 3 noeuds bc ayant un fils, soit 3 2 = 3 formes différentes. Dans un graphe réel, la situation est souvent beaucoup moins simple et déterminer quelles formes il est possible de supprimer n'est pas une tâche facile. Pour éviter d'effectuer des calculs portant sur la structure du graphe qui nuiraient aux performances de l'algorithme, on choisit de ne conserver qu'une seule forme parmi celles utilisant les mêmes noeuds. On passe ainsi à un nombre de formes déterminé par un nombre d'arrangements possibles à un nombre de formes basé sur le nombre de combinaisons. Cette solution n'est pas optimale mais elle est suffisante pour limiter l'explosion combinatoire. Sur une configuration en étoile dans laquelle un noeud possède n fils avec une même étiquette, il existe n i=1 n P k manières différentes de générer des motifs inclus. Après optimisation, ce nombre chute à n?1 i=0 (n ? i) n i .
Résultats expérimentaux
La méthode décrite dans l'article a été implémentée en C++ avec la STL. Les expéri-mentations ont été effectuées sur un ordinateur avec Ubuntu 13.04 basé sur un processeur Intel c TM i5-2400 @ 3.10GHz avec 12 Gb de mémoire. Tous les temps d'exécution incluent la phase de prétraitement et la sauvegarde des résultats.
Jeux de données artificiels
Un ensemble de 1000 graphes orientés comprenant chacun 1000 noeuds et 5000 arcs ont été générés avec le programme de Johnsonbaugh et Kalin (1991). Cet ensemble de graphes a servi de base à la création de cinq jeux de données de graphes orientés attribués, nommés A1 à A5, dans lesquels les noeuds ont été associés à des itemsets de taille variant de 1 à 5. Pour simuler le fait que, comme dans beaucoup de jeux de donnés réels, la plupart des attributs sont rares, mais un petit nombre est très fréquent, nous avons attribué à chaque item une valeur entière aléatoire distribuée suivant une loi puissance (concrètement, nous avons utilisé une distribution de Pareto de paramètre ? = 0.05).
Réseau de citations de PubMed Central
PubMed Central est une base de données bibliographique d'articles scientifiques dans le domaine des sciences de la vie. Les articles en Open Source sont au nombre de 322 526 mais seulement 58 728 d'entre eux citent au moins un autre article, lui même Open Source. Nous avons construit un graphe de citations où chaque noeud représente un article, chaque arc, un lien de citation et les attributs associés aux noeuds identifient les mots-clés de l'article. Le graphe est très creux puisqu'il ne contient que 60 012 arcs, cependant, certains attributs se retrouvent fréquemment associés à des sous-ensembles de noeuds connectés. 
Jeux de données Google+ et Twitter
Évaluation des performances
Les résultats de notre algorithme sont présentés dans la figure 5. La comparaison avec des algorithmes existants n'est possible que sur des graphes dans lesquels les noeuds ne sont associés qu'à une seule étiquette. Pour le jeu de données artificiel A1, le seul qui corresponde à un graphe étiqueté, une comparaison de AADAGE avec l'implémentations de gSpan réal-isées dans le cadre du projet ParSeMiS (Wörlein et al., 2005) (une implémentation qui permet de fouiller les sous-graphes orientés) est présentée ( figure 5(a)). Les deux algorithmes, qui sont paramétrés pour énumérer l'ensemble des sous-graphes orientés, enracinés et connectés, génèrent exactement les mêmes motifs. Pour les plus petites valeurs de support, la stratégie de filtrage des mises en correspondance permet de faire la différence avec gSpan. Les données Google+ et Twitter contiennent beaucoup de noeuds, beaucoup d'attributs et surtout quelques attributs très fréquents (par exemple, l'attribut "sexe = masculin", présent dans 52% des noeuds) dont la seule présence génère tellement de motifs qu'elle fait échouer la fouille. Dans la figure 5(d), les deux attributs de genre ont été enlevés du jeu de données Google+. Le jeu Twitter n'a pas été modifié. Malgré sa taille, le jeu de données Google+ a pu être fouillé en fixant un support minimum de 3%. Le jeu de données Twitter, pourtant plus modeste, est traité jusqu'à un support de 5%. La caractéristique commune de ces deux jeux de données est qu'ils sont traités relativement rapidement jusqu'à un seuil précis correspondant à la prise en compte d'un nouvel attribut fréquent qui fait exploser le nombre de motifs.
Avec le jeu de données de PubMed Central, il a été possible d'utiliser un support très faible. Cela a permis d'identifier quelques motifs intéressants. Un motif, par exemple, concerne un groupe d'articles annotés avec "p53" qui est le nom d'un oncogène. Ces articles sont cités par d'autres articles annotés "rapamycin" (un médicament immunosuppresseur élaboré en 1975) qui sont eux-même cités par des articles annotés "aging" (des effets de la rapamycine sur la sénescence cellulaire ont été découverts en 2009) et "mT OR" (un gène qui est inhibé par la rapamycine). Ce motif représente de manière très condensée le cheminement de certaines recherches portant sur le vieillissement. 
Conclusion et perspectives
Nous avons, dans cet article présenté des méthodes permettant de fouiller des graphes attribués orientés et montré leur efficacité lors de l'analyse de plusieurs jeux de données. Nous nous sommes attachés aux configurations de graphes dans lesquelles, comme dans les données réelles, certains attributs sont partagés par un nombre important de noeuds. Nous avons montré que, selon la structure du motif en cours d'analyse, il n'est pas toujours nécessaire d'étendre toutes les formes que peut prendre le motif. Nous nous sommes intéressés plus particulière-ment aux motifs automorphes, mais il existe d'autres motifs pour lesquels des optimisations peuvent être effectuées. Par exemple, lorsque le motif de la figure 1(b), qui ne possède pas d'automorphisme, est trouvé 12 fois sur le graphe de la figure 4(a), ont voit assez facilement que certaines mises en correspondance peuvent être écartées. Nous sommes convaincus que l'analyse détaillée des motifs et de leurs mises en correspondance peut révéler d'autres configurations permettant de réduire l'espace de recherche et ainsi de traiter des jeux de données plus importants ou plus denses. C'est une piste que nous avons l'objectif d'explorer.
Remerciements. Ce travail a été financé par le contrat ANR-2010-COSI-012 FOSTER.

Introduction
Avec l'émergence du web 2.0, les internautes ne sont plus de simples consommateurs, ils sont également acteurs par le biais des messages qu'ils peuvent déposer, des commentaires qu'ils peuvent laisser et de toute action qu'ils peuvent effectuer. Dans ce cadre, les messages laissés dans les réseaux sociaux représentent une source précieuse d'informations, que de nombreuses recherches cherchent à analyser dans le but d'en comprendre le contenu, d'en extraire les relations cachées, mais aussi de prédire de l'information. Le flux de messages peut être considéré comme une séquence ordonnée par la date de création des messages. On appellera "item" un élément représentant un message (mot du message, opinion ou sujet extrait, etc.). À un temps t, plusieurs items apparaissent donc dans cette séquence : l'ensemble des items du message créé au temps t. Ce type de séquence est appelée "séquence complexe".
Dans le cas où les données sont formées d'une unique et longue séquence, l'extraction d'épisodes est une tâche essentielle. Un épisode est un motif temporel composé d'items "relativement proches", qui apparaît souvent tout au long de la séquence ou sur une partie de cette séquence (Mannila et al., 1997). Mannila (Mannila et al., 1997) a proposé les premiers algorithmes d'extraction d'épisodes : W inepi et M inepi qui seront la base de nombreux autres algorithmes proposés par la suite. Ces deux algorithmes extraient dans un premier temps les épisodes les plus petits, et forment incrémentalement des épisodes plus grands en se basant sur leur fréquence. Ces méthodes ont la caractéristique d'extraire un ensemble complet d'épisodes.
L'extraction d'épisodes dans des séquences complexes est une problématique récente qui nécessite un algorithme adapté pour prendre en compte l'existence de plusieurs items à chaque temps. Huang et Chang (Huang et Chang, 2008) proposent un algorithme appelé EMMA qui extrait un ensemble complet d'épisodes à partir d'une séquence complexe. Dans les deux premières phases, EMMA extrait un ensemble de motifs fréquents représentant des 1-uplet épi-sodes, associe un identifiant id à chaque 1-uplet épisode, puis encode la séquence avec ces id. Les épisodes sont construits incrémentalement pendant une troisième phase en concaténant des id. EMMA définit deux notions : borne et borne projetée. Une borne d'un épisode P est un intervalle [t s , t e ] dans lequel P apparaît. Une borne projetée d'un épisode P représente l'intervalle de taille au maximum w dans lequel l'algorithme cherche les id pour étendre P . EMMA est un algorithme rapide et facile à adapter.
La communauté d'extraction de motifs fréquents admet que dans la plupart des applications, il est suffisant d'extraire un ensemble d'épisodes condensé et significatif. Par conséquent, les travaux récents se focalisent sur la détection d'épisodes comportant certaines caractéris-tiques et contraintes : épisodes maximaux (Gan et Dai, 2011), ou approximativement fermés et non dérivables (Gan et Dai, 2012).
Tout comme il est possible d'extraire des règles d'association à partir de motifs, des règles d'épisodes peuvent être extraites à partir d'épisodes. Ces règles d'épisodes peuvent aussi être utilisées dans un objectif de prédiction (Daurel, 2003). La majorité des règles d'épisodes construites à partir d'épisodes fermés ou maximaux ont la caractéristique d'avoir un anté-cédent long (composé de nombreux items). De notre point de vue, elles ne sont pas adaptées à la prédiction au plus tôt d'informations, car pour prédire une conséquence, il faut attendre l'apparition de la totalité des items de l'antécédent. Dans ce cas, pour détecter rapidement des événements, une règle d'épisodes avec un antécédent plus petit est plus pertinente qu'une règle avec un antécédent plus long. Par conséquent, notre premier objectif est d'extraire des règles d'épisodes composées d'un antécédent de taille minimale, que nous appellerons "règles d'épisodes minimales".
Dans la tâche d'identification au plut tôt d'informations, nous faisons l'hypothèse qu'il est inutile de chercher à former des règles d'épisodes très complexes ou très précises, mais des règles d'épisodes représentant les premiers signaux déclencheurs d'un événement. Nous trouvons donc qu'il est inutile d'extraire les règles d'épisodes contenant plusieurs fois le même item. Il est vrai qu'une apparition multiple d'un item porte plus d'informations, mais l'unicité des items permettra de diminuer la complexité de l'algorithme. Nous appelons "épisode sans répétition" un épisode composé d'items uniques. Notre second objectif est d'extraire des épisodes sans répétition.
Nous souhaitons pouvoir anticiper/prédire des informations "lointaines". Notre troisième objectif est donc d'extraire des règles d'épisodes ayant une conséquence éloignée temporellement de l'antécédent.
La notion "minimale" n'est pas nouvelle dans l'état de l'art. Elle a été proposée dans (Rahal et al., 2004) dans le cadre d'extraction de règles d'association fiables, peu fréquentes ayant l'antécédent le plus petit et la conséquence fixée par l'utilisateur à partir de données transactionnelles dans le but de prédire au plus tôt la conséquence. Cette approche est différente de notre travail. En effet, nous voulons extraire les règles d'épisodes (et non pas des règles d'association), sans répétition et avec une conséquence temporellement éloignée de l'antécédent.
Nous présentons maintenant l'approche proposé pour atteindre nos objectifs.
Notre approche : extraction de règles d'épisodes minimales
Rappelons que notre objectif est de pouvoir anticiper au plus tôt des événements de façon fiable. Pour cela, nous proposons un algorithme d'extraction de règles d'épisodes qui, en plus d'être fréquentes et fiables comme celles extraites par les algorithmes de l'état de l'art, sont sans répétition, minimales et avec une conséquence éloignée temporellement de l'antécédent.
Nous utilisons les mêmes étapes d'initialisation et d'encodage comme dans l'algorithme EMMA, mais nous proposons une autre approche pour obtenir des règles d'épisodes avec les caractéristiques souhaitées. Le principe de notre algorithme est le suivant : chaque règle d'épisodes est construite en fixant le préfixe de la future règle (1-uplet épisode), puis en fixant la conséquence, qui est la plus éloignée possible de ce préfixe, et enfin en complétant l'antécédent avec des 1-uplet épisodes les plus proches possibles du préfixe. Nous détaillons ci-dessous cet algorithme.
Déroulement de l'algorithme
Identification du préfixe Chaque id obtenu dans la deuxième étape de EMMA représente un préfixe d'un épisode potentiel.
Soit id i un préfixe, nous construisons P roj ID f in (id i ) la liste des id apparaissant loin de id i (dans les dernières positions des bornes projetées de id i ). Nous construisons également P roj ID deb (id i ), la liste des id apparaissant à proximité de id i (présents au début des bornes projetées de id i ). La taille des intervalles "début" et "fin" est une proportion de w.
Identification de la conséquence id i est étendu avec les éléments id j ? P roj ID f in (id i ) qui représentent une conséquence potentielle d'un épisode dont le premier élément est id i , formant ainsi un épisode candidat < id i , id j >. La fréquence de cet épisode candidat est calculée en utilisant le nombre de bornes dans sa liste de bornes. Si < id i , id j > n'est pas fréquent, on arrête cette itération et on considère que id j ne peut pas être une conséquence de id i . On itère de nouveau pour étendre id i avec d'autres id ? P roj ID f in (id i ). Si < id i , id j > est fréquent, alors la règle id i ? id j est construite. Si elle est fiable, elle est considérée comme minimale. Elle n'est donc plus étendue et elle est ajoutée à la liste finale des règles d'épisodes. Si la règle est fréquente mais pas fiable, alors le 2-uplet épisode < id i , id j > est étendu pour compléter l'antécédent.
Complétion de l'antécédent Rappelons que nous cherchons à compéter l'antécédent de façon à obtenir non seulement une règle fiable mais aussi une règle avec un antécédent minimal. Les itérations de complétion de l'antécédent s'arrêtent donc dès que la règle d'épisodes est f iable, ou s'il n'y a plus d'épisode candidat. À partir de < id i , id j >, id i est étendu itérativement avec les id dans sa liste P roj ID deb (id i ). Soit id s ? P roj ID deb (id i ), si l'épisode candidat < id i , id s , id j > n'est pas fréquent, alors le préfixe de l'antécédent id i est étendu avec un autre id ? P roj ID deb (id i ), sinon la confiance de la règle id i , id s ? id j est calculée. Si la règle n'est pas fiable, alors l'épisode < id i , id s > est étendu avec d'autres id ? P roj ID deb (id i , id s ), sinon elle est acceptable, et donc minimale, son antécédent n'est donc plus étendu et elle est ajoutée à la liste finale de règles d'épisodes.
Épisode sans répétition Comme nous l'avons mentionné, notre algorithme a pour but d'évi-ter l'apparition multiple d'un item dans les règles d'épisodes. À chaque fois qu'un épisode doit être étendu, une vérification est faite pour éviter de l'étendre avec des id ayant des items en commun, ce qui aura également l'avantage de réduire la complexité de l'algorithme.
Conclusion et perspectives
Dans ce travail préliminaire, nous proposons un algorithme de fouille de séquence de données dans le but de prédire au plus tôt et de façon fiable, des événements. Notre objectif à court terme est de fouiller des messages issus des réseaux sociaux et de prédire l'apparition d'informations. Nous avons mis en évidence que l'extraction de règles d'épisodes est adaptée à nos données représentées sous la forme d'une séquence complexe ordonnée selon le temps. Pour atteindre nos objectifs, nous avons déterminé plusieurs caractéristiques des règles d'épisodes à extraire : elles doivent être sans répétition, fréquentes, fiables, ayant l'antécédent le plus petit possible et la conséquence éloignée temporellement de l'antécédent.
L'algorithme, tel que nous l'avons défini, ne permet d'extraire que des conséquences de taille 1 (1-uplet épisodes). Il est évident qu'une conséquence plus longue sera plus porteuse d'informations sur les futurs événements. Une de nos perspectives est donc d'adapter cet algorithme de façon à ce qu'il puisse extraire des conséquences plus longues.

Introduction
La minimalité est un concept essentiel de l'extraction de motifs. Pour une fonction f et un langage L, un motif X est minimal si sa valeur pour f est distincte de celle de chacun de ses sous-ensembles. La collection de tous les motifs minimaux constitue une représentation condensée de L adéquate à f : il est possible de retrouver f (Y ) pour n'importe quel motif Y ? L. Typiquement, l'ensemble des itemsets libres (Boulicaut et al., 2000) est une représen-tation condensée de tous les itemsets (dans ce cas, f et L sont respectivement la fréquence et le langage des itemsets). Bien sûr, il est souvent plus efficace d'extraire les motifs minimaux plutôt que la totalité des motifs. En plus, les motifs minimaux ont de nombreuses applications utiles notamment en extraction de connaissances : la production de bases de règles d'association, la construction de classifieurs ou la génération des traverses minimales. La minimalité a été étudiée dans le cas de différentes fonctions (comme la fréquence (Calders et al., 2004) ou les fonctions condensables (Soulet et Crémilleux, 2008)) et avec des langages variés (dont les itemsets (Boulicaut et al., 2000) et les séquences ). Bien que la minimalité ait des avantages évidents (Li et al., 2006), elle reste peu explorée en comparaison de la maximalité (i.e., les motifs fermés). En particulier, à notre connaissance, il n'existe pas de cadre suffisamment général comme celui proposé par Arimura et Uno (2009) pour les maximaux.
Nous pensons qu'un des inconvénients importants des motifs minimaux réside dans leur difficile extraction. L'efficacité limitée des algorithmes existants découle principalement de leur approche par niveaux (Boulicaut et al., 2000;Soulet et Crémilleux, 2008;Casali et al., 2005) (i.e., en largeur avec la méthode générer-et-tester). Comme ils stockent tous les candidats d'un même niveau en mémoire durant la phase de génération, l'extraction peut échouer par manque de mémoire. Pour éviter cet écueil, il semble préférable d'adopter un parcours en profondeur qui souvent consomme moins de mémoire tout en restant très rapide. Cependant, vérifier si la minimalité est satisfaite ou non est vraiment difficile avec un parcours en profondeur. Dans le cas de la fréquence et des itemsets, la meilleure façon d'évaluer la minimalité d'un motif (disons abc) est de comparer sa fréquence avec celle de ses généralisations directes (ici, ab, ac et bc). Mais, quand le parcours en profondeur atteint le motif abc, seules les fré-quences de a et ab ont été précédemment calculées. Comme les fréquences de ac et bc ne sont pas connues, il est impossible de vérifier si la fréquence de abc est bien strictement inférieure à celle de ac et bc. Contributions. L'objectif principal de cet article est de présenter un cadre générique et efficace pour l'extraction des motifs minimaux, et ce en proposant un algorithme en profondeur peu consommateur en mémoire. Nous introduisons la notion de système minimisable d'ensembles qui est au coeur de la définition de notre cadre. Ce dernier couvre un très large spectre de motifs minimaux incluant tous les langages et les mesures étudiés par Soulet et Crémilleux (2008); Arimura et Uno (2009). Une vérification rapide de la minimalité lors d'un parcours en profondeur est réalisée grâce à la notion d'objets critiques dont la définition découle du système minimisable d'ensembles considéré. En s'appuyant sur cette nouvelle technique, nous proposons l'algorithme DEFME. Il extrait tous les motifs minimaux d'un système minimisable d'ensembles en utilisant un parcours en profondeur. À notre connaissance, il s'agit du premier algorithme qui énumère tous les motifs minimaux en polynomial delay et avec une consommation linéaire d'espace par rapport au jeu de données initial.
2 Système d'ensembles pour les motifs minimaux
Définitions
Un système d'ensembles (F, E) est une collection F de sous-ensembles d'un ensemble E (i.e. F est un sous-ensemble des parties de E). Un membre de F est appelé un ensemble acceptable. Un système d'ensembles fortement accessible (F, E) est un système d'ensembles où pour tous les ensembles acceptables X, Y satisfaisant X ? Y , il existe un élément e ? Y \ X tel que Xe ? F 1 . Bien entendu, les itemsets rentrent dans ce cadre avec le système 1. Nous utilisons la notation Xe pour dénoter X ? {e}. 
Cette définition indique que la couverture de l'union de deux motifs correspond exactement à l'intersection de leurs deux couvertures. Pour les itemsets, un opérateur de couverture naturel est l'extension d'un itemset X qui retourne tous les identifiants des tuples contenant X : cov I (X) = {o ? O | X ? o}. La couverture permet surtout de dériver des informations utiles : la cardinalité de cov I (X) correspond à la fréquence de X. Dans le contexte des chaînes, la liste des index d'une chaîne X définit également un opérateur de couverture :
Pour certains langages, le même motif est décrit par plusieurs ensembles distincts et alors, il est nécessaire d'avoir une forme canonique (pour éviter de l'extraire plusieurs fois, par exemple). Typiquement, considérons l'ensemble {(a, 1), (b, 2), (r, 3)} correspondant à la chaîne abr. Son suffixe {(b, 2), (r, 3)} désigne la même chaîne br que {(b, 1), (r, 2)}. Dans notre cas, {(b, 1), (r, 2)} sera la forme canonique de br car le premier élément commence par 1. Pour retrouver la forme canonique d'un motif, nous introduisons un nouvel opérateur :
Dans cette définition, la propriété (i) nous garantit que les formes canoniques de deux ensembles comparables (au sens de l'inclusion) restent comparables. La propriété (ii) signifie que le système d'ensembles (F, E) inclut toutes les formes canoniques. Continuons encore notre exemple sur les chaînes : on peut constater que ? S :
Système minimisable d'ensembles
Plutôt que de considérer un système d'ensembles dans son intégralité, il peut s'avérer judicieux d'en sélectionner une partie qui apporte la même quantité d'information (au sens de l'opérateur de couverture). Pour cela, il est nécessaire que ce système d'ensembles et l'opéra-teur de couverture visé forment un système minimisable d'ensembles : Définition 3 (Système minimisable d'ensembles) Un système minimisable d'ensembles est un tuple E), G, cov, ? où :
-(F, E) est un système d'ensembles fini et fortement accessible. Un ensemble acceptable dans F est appelé motif. -(G, E) est un système d'ensembles fini et fortement accessible satisfaisant pour chaque couple d'ensembles acceptables X, Y ? F tels que X ? Y et chaque élément e ? E, X\{e} ? G ? Y \{e} ? G. Un ensemble acceptable de G est appelé une généralisation.
O est un opérateur de couverture. -? : F ? G ? F est un opérateur canonique tel que pour chaque ensemble acceptable X ? G, on ait cov(?(X)) = cov(X).
Illustrons maintenant le rôle de G par rapport à F dans le cas des chaînes. En fait, G S regroupe tous les suffixes de n'importe quel motif de F S . Typiquement, {(b, 2), (r, 3)} ? G S est une généralisation de {(a, 1), (b, 2), (r, 3)} ? F S . Comme dit au-dessus, {(b, 2), (r, 3)} a une forme équivalente dans F S : ? S ({(b, 2), (r, 3)}) = {(b, 1), (r, 2)}. Par convention, nous étendons la définition de cov S et G S en considérant que cov S (? S (X)) = cov S (X). De plus, on voit que G S satisfait la propriété désirée par rapport à F S : pour tous les ensembles acceptables X, Y ? F S tels que X ? Y et chaque élément e ? E S , X \ {e} ? G S ? Y \ {e} ? G S . En effet, si X \ {e} est un suffixe de X, cela signifie que e est la première lettre. Si nous considérons une spécialisation de X et que nous retirons la première lettre, nous obtenons aussi un suffixe qui appartient à G S . Par conséquent, S , E S ), G S , cov S , ? S est minimisable.
Bien évidemment, un système minimisable d'ensembles peut être réduit à un système de cardinalité inférieure dont les motifs sont appelés motifs minimaux :
La définition 4 signifie qu'un motif est minimal dès que sa couverture diffère de celle de toutes ses généralisations. Par exemple, pour l'opérateur de couverture cov S , les motifs minimaux ont une couverture strictement plus petite que celles de leurs généralisa-tions. La chaîne ab n'est pas minimale à cause de son suffixe b car cov S ({(b, 2))}) = cov S ({(a, 1), (b, 2)}) = {0, 7}. Dans notre exemple, la collection des chaînes minimales est M(S S ) = {a, b, r, c, d, ca, ra, da}.
Etant donné un système minimisable d'ensembles S = E), G, cov, ? l'extraction des motifs minimaux consiste à énumérer tous les motifs minimaux de S.
Énumération des motifs minimaux
Le but de cette section est de proposer une methode d'extraction en profondeur des motifs minimaux (section 3.3). Pour cela, nous nous appuyons sur deux idées principales : l'élagage de l'espace de recherche (section 3.1) et la vérification rapide de la minimalité (section 3.2).
Auparavant il est important de rappeler que les motifs minimaux suffisent pour déduire la couverture de tout motif. Nous considérons désormais un système minimisable d'ensembles S = E), G, cov, ? Les motifs minimaux M(S) consituent une représentation sans perte de tous les motifs de F :
Théorème 1 (Représentation condensée) L'ensemble des motifs minimaux est une représen-tation condensée de F adéquate pour le calcul de cov : pour tout motif X ? F, il existe
Le théorème 1 signifie que la couverture de tout motif de S peut être déduite de M(S) (les preuves sont omises par manque de place). Par exemple, la couverture du motif non minimal
Il est préférable d'extraire M(S) plutôt que S car sa taille est plus petite que celle du système complet.
Élagage de l'espace de recherche
Le premier problème auquel nous sommes confrontés est plutôt classique. Étant donné un système minimisable d'ensembles S = E), G, cov, ? le nombre de motifs |F| est en général très grand (dans le pire des cas, il atteint 2 |E| ). Il est donc absolument nécessaire de ne pas parcourir l'espace de recherche exhaustivement, mais de se concentrer sur les motifs minimaux. Heureusement, des techniques efficaces peuvent être utilisées pour élaguer l'espace de recherche, grâce à la propriété d'anti-monotonie de la contrainte de minimalité :
Théorème 2 (Système indépendant) Si un motif X est minimal pour S, alors tout motif Y ? F satisfaisant Y ? X est aussi minimal pour S.
La preuve de ce théorème est fortement dépendante d'un lemme clé indiquant qu'un motif non minimal a une généralisation directe possédant la même couverture.
Lemme 1 Si X n'est pas mininal, ?e ? X tel que X \ {e} ? G et cov(X) = cov(X \ {e}).
Par exemple, comme la chaîne da est minimale, les sous-chaînes d et a le sont également. En outre, comme ab n'est pas minimal, la chaîne abr ne l'est pas non plus. Cela signifie que la chaîne ab est un point de rupture dans l'espace de recherche. En pratique, l'élagage par antimonotonie est reconnu comme un outil puissant, quelque soit le type de parcours de l'espace de recherche, par niveau ou en profondeur.
Vérification rapide de la minimalité
La difficulté principale de l'extraction des motifs minimaux est de tester si un motif est minimal ou non. Comme mentionné précédemment, ceci est particulièrement difficile lors d'un parcours en profondeur, car tous les sous-ensembles du motif à tester n'ont pas été énumérés. En effet, les approches en profondeur n'ont accès qu'à la branche parente, au contraire des approches par niveau. Pour résoudre ce problème, nous introduisons le concept d'objets critiques inspiré de celui d'hyper-arête critique, introduit pour le calcul des traverses minimales par Murakami et Uno (2013). Intuitivement, les objets critiques d'un élément e pour un motif X sont les objets qui ne sont pas couverts par X du fait de e.
Nous donnons maintenant une définition formelle des objets critiques, dérivant d'un opé-rateur de couverture quelconque :
Définition 5 (Objets critiques) Pour un motif X, les objets critiques d'un élément e ? X, notés cov(X, e) sont l'ensemble d'objets qui appartiennent à la couverture de X privé de e mais pas à la couverture de e : cov(X, e) = cov(X \ e) \ cov(e).
Illustrons le concept d'objets critiques sur notre exemple. Pour {(a, 1), (b, 2)}, les objets critiques cov S (ab, a) de l'élément (a, 1) correspondent à ? (= {0, 7} \ {0, 3, 5, 7, 10}). Cela signifie que l'ajout de a à b n'a pas d'impact sur la couverture de ab. En revanche, pour le même motif, les objets critiques de (b, 2) sont {3, 5, 10} (= {0, 3, 5, 7, 10} \ {0, 7}). C'est à cause de l'élément b que ab ne couvre pas les objets {3, 5, 10}.
Les objets critiques sont centraux pour notre proposition : 1) les objets critiques caracté-risent facilement les motifs minimaux ; et 2) les objets critiques peuvent être calculés efficacement au sein d'un algorithme en profondeur. Caractérisation de la minimalité La réciproque du lemme 1 indique qu'un motif est minimal si sa couverture diffère de celle d'une de ses généralisations. Nous pouvons reformuler cette définition grâce à la notion d'objets critiques :
Propriété 1 (Minimalité) X ? F est minimal si ?e ? X tel que X \ e ? G, cov(X, e) = ?.
Typiquement, comme b est une généralisation de la chaîne ab, et cov S (ab, a) est vide, ab n'est pas minimal. La propriété 1 signifie que tester si un candidat X est minimal ne néces-site que la connaissance des objets critiques relativement aux éléments de X. À la différence de la définition traditionnelle, il n'est pas nécessaire de disposer d'informations sur les sousensembles. Les objets critiques nous permettent donc de concevoir un algorithme en profondeur si (et seulement si) le calcul des objets critiques ne requiert pas non plus d'informations sur les sous-ensembles. Calcul efficace d'objets critiques Selon un parcours en profondeur, nous voulons mettre à jour les objets critiques d'un élément e pour le motif X lorsqu'un nouvel élément e est ajouté à X. Nous montrons que, dans ce cas, les objets critiques peuvent être calculés efficacement par intersection des objets critiques cov(X, e) avec la couverture du nouvel élément e
:
Propriété 2 L'égalite suivante est vraie pour tout motif X ? F et deux élément e, e ? E : cov(Xe , e) = cov(X, e) ? cov(e ).
Par exemple, la définition 5 donne cov S (a, a) = {1, 2, 4, 6, 8, 9}. Comme cov S (b) = {0, 7}, on obtient que cov S (ab, a) = cov S (a, a) ? cov S (b) = {1, 2, 4, 6, 8, 9} ? {0, 7} = ?. Il est intéressant de noter que la propriété 2 nous permet de calculer les objets critiques de tout élément d'un motif X en ne disposant que de l'information sur une seule branche du parcours. C'est une situation idéale pour la conception d'un algorithme en profondeur.
3.3 Algorithme DEFME L'algorithme DEFME prend en entrée le motif courant X et la queue tail des éléments restant à vérifier, il retourne tous les motifs minimaux contenant X, basés sur tail. Plus pré-cisément, la ligne 1 teste si X est minimal ou non. Si X est minimal, il est affiché (ligne 2). Les lignes 3 à 14 explorent le sous-arbre contenant X selon la queue. Pour chaque élément e pour lequel Xe est un motif de F (ligne 4) (propriété 1), la branche est construite à l'aide des informations nécessaires. La ligne 7 met à jour la couverture et les lignes 8 à 11 calculent les objets critiques en utilisant la propriété 2. Enfin, la fonction DEFME est appelée récursivement à la ligne 12 avec la queue mise à jour à la ligne 5.
Algorithm 1 DEFME(X, tail)
Input: X est un motif tail est l'ensemble des items restant à utiliser pour générer les candidats. Valeurs initiales : X = ?, tail = E. Output: calcule les motifs minimaux de manière incrémentale polynomiale.
1: if ?e ? X, cov(X, e) = ? then 2: print X 3: for all e ? tail do 4:
if Xe ? F then
5:
tail := tail \ {e} 6:
cov(Y, e) := cov(X) \ cov(e)
9:
for all e ? X do 10: Les théorèmes 3 et 4 montrent que l'algorithme DEFME possède un comportement efficace, tant en terme d'espace que de temps de calcul. Cette efficacité découle du calcul écono-mique des couvertures et des objets critiques, ainsi que la propriété suivante l'explique :
La propriété 3 signifie que, pour un motif, le stockage de sa couverture plus celui de ses objets critiques est majoré par le nombre total d'objets (i.e., |cov(?)|). La déduction de l'espace de mémoire nécessaire pour l'algorithme est donc directe : En pratique, l'espace mémoire utilisé par l'algorithme est très limité, car m est petit. En outre, le delai entre deux motifs est polynomial :
Théorème 4 (Complexité polynomiale en délai) M(S) est énumerable en temps O(|E| 2 · |cov(?)|) par motif minimal. DEFME requiert un nombre polynomial d'opérations entre deux motifs, en supposant que l'oracle d'appartenance agit en temps polynomial (ligne 4). En effet, le calcul de la couverture et celui des objets critiques (lignes 7 à 11) est linéaire avec le nombre d'objets, grâce à la propriété 3 ; la boucle à la ligne 3 n'excède pas |E| itérations et finalement, le nombre de retours en arrière consécutifs est au plus de |E|.
libres, pour laquelle plusieurs prototypes existent dans la littérature. Ensuite, nous utilisons DEFME pour extraire la collection des chaînes minimales et nous comparons sa taille avec celle des chaînes maximales. Tous les tests ont été effectués sur une machine Linux dotée d'un processeur Opteron à 2,2 GHz et de 200 Go de RAM.
Extraction des motifs libres
Nous avons réalisé un prototype de DEFME pour l'extraction des motifs ensemblistes, en tant que preuve de concept, et nous l'avons comparé avec deux autres prototypes : ACMINER, basé sur un algorithme par niveaux (Boulicaut et al., 2000) et NDI 2 utilisant un parcours en profondeur réordonnant les items (Calders et Goethals, 2005 Les meilleurs performances sont indiquées par l'utilisation d'une police grasse dans la table 1 pour les temps d'exécution et la consommation mémoire. ACMINER est de loin le prototype le plus lent. Son approche par niveau est particulièrement pénalisée par l'importance de la consommation mémoire. Excepté sur les données génomiques et sur chess, les temps d'exécution de NDI surpassent clairement ceux de DEFME. À titre d'information, la figure 1, à gauche, indique la vitesse de DEFME, selon différents seuils de support minimal. Cette figure représente le nombre de motifs minimaux calculés par seconde.
Concernant la consommation mémoire, DEFME est (comme prévu) le plus efficace des algorithmes. Dans certains cas, augmenter la quantité de mémoire disponible ne suffirait pas à traiter les données les plus difficiles. Ici, ACMINER et NDI ne peuvent traiter les données génomique même avec 200 Go de mémoire, à des seuils de support minimal pourtant élévés. Plus précisément, la figure 1, à droite, représente le rapport entre les utilisations de mémoire de NDI et de DEFME, selon le seuil de support. On remarque grâce à ce rapport que NDI explose en mémoire. Rappelons que DEFME travaille en mémoire bornée et n'est donc pas limité lors de faibles seuils de support.
Extraction de chaînes minimales
Dans cette section, nous adoptons le formalisme des chaînes, utilisé dans notre exemple récurrent. Nous avons comparé notre algorithme d'extraction de chaînes minimales avec le 2. Ce prototype permet d'extraire les motifs libres en fixant le paramètre de profondeur à 1. 3. fimi.ua.ac.be/data/ et lisp.vse.cz/challenge/ecmlpkdd2004/ FIG. 1 -Ratio de la vitesse d'extraction (à gauche) et de la consommation mémoire (à droite) de NDI par DEFME.
prototype MAXMOTIF fourni par Takeaki Uno, extrayant les motifs minimaux. Notre but est de comparer la taille des représentations condensées fondées sur les chaînes minimales avec celles utilisant les chaînes maximales. Nous n'avons pas reporté les temps d'exécution car MAXMOTIF, développé en Java, est bien plus lent que DEFME. Les expérimentations ont été effectuées sur chromosom 4 et msnbc issu du dépôt UCI pour l'apprentissage automatique 5 . La figure 2 reporte les quantités de chaînes, de chaînes minimales et de chaînes maximales extraites dans chromosom (à gauche) et msnbc (à droite), selon le seuil de support minimal. Le nombre de ces chaînes augmente bien-sûr lorsque ce seuil diminue. On note que les deux représentations condensées des minimaux et des clos deviennent particulièrement intéressantes pour de faibles seuils de support. Le nombre de chaînes minimales est clairement plus grand que celui de chaînes maximales, mais l'écart n'est pas aussi important que dans le cas des motifs libres ou fermés. À ces seuils de supports, l'écart des moyennes des longueurs entre minimaux et maximaux est d'un item pour chromosom et de trois pour msnbc.
Travaux relatifs
La collection des motifs minimaux est une forme de représentation condensée. Un grand nombre de représentations condensées ont été proposées dans la littérature (Calders et al., 2004;Hamrouni, 2012) : les motifs fermés (Pasquier et al., 1999), les motifs libres (Boulicaut et al., 2000), les motifs essentiels (Casali et al., 2005), les motifs non-dérivables (Calders et Goethals, 2005), etc. Les deux idées fondatrices des représentations condensées sont les classes d'équivalences souvent issues d'un opérateur de fermeture (Hamrouni, 2012)  L'extraction des motifs minimaux a de nombreuses applications et n'est pas seulement utilisée pour accélérer l'obtention des motifs fréquents. Leurs propriétés sont utiles pour certaines tâches. Par exemple, les motifs minimaux sont utilisés en conjonction de motifs fermés pour produire des règles non-redondantes ou informatives (Zaki, 2000;Pasquier et al., 1999). Les règles séquentielles bénéficient également de la minimalité (Lo et al., 2009). Il est aussi possible d'exploiter les motifs minimaux pour extraire des règles de classification qui sont les éléments clés des classifieurs associatifs (Liu et al., 1998). Notre cadre est bien adapté pour extraire de telles règles de classification qui satisfont une mesure d'intérêt impliquant des fré-quences. Supposons que l'ensemble d'objets O se répartisse en deux classes disjointes O = O 1 ?O 2 , la confiance de la règle de classification X ? class 1 est |O 1 ? cov I (X)|/|cov I (X)|. Plus généralement, il est facile de montrer que n'importe quelle mesure fondée sur des fré-quences (e.g., le lift, le bond) peut être dérivée des couvertures positive et négative. En plus, les motifs essentiels sont utiles pour dériver les traverses minimales qui correspondent exactement aux motifs maximaux de M( I , I), 2 I , cov I , Id Rappelons que la génération des traverses minimales est un problème important aux nombreuses applications en logique, en intelligence artificielle et apprentissage (Eiter et Gottlob, 2002;Murakami et Uno, 2013).
Les représentations condensées de motifs minimaux ne sont pas limitées aux seules mesures impliquant la fréquence et au seul langage des itemsets. En effet, il est aussi possible d'extraire des motifs minimaux adéquats à des fonctions d'agrégat comme min,

Introduction
La programmation visuelle de composants de haut niveau permet d'améliorer le dévelop-pement des chaînes de traitement. C'est pourquoi les plate-formes libres pour l'analyse de données (Weka, RapidMiner, Knime 1 ) proposent des environnements de développement visuel. Pour autant, il y est difficile d'ajouter ses propres réalisations, par exemple des binaires compilés, car ces plate-formes requièrent l'utilisation d'une API dédiée, imposant par exemple un format spécifique pour la manipulation des données. Les possibilités de programmation structurée y sont également limitées : ces plate-formes sont conçues pour enrober leurs propres composants de base pour l'apprentissage automatique.
Nous présentons ici la plate-forme KD-Ariane, un déploiement d'outils pour la fouille de données dans l'environnement de programmation visuelle Ariane. Ce déploiement facilite la conception de chaînes structurées de traitements pour l'extraction de connaissance dans les données (Clouard (2009)). KD-Ariane est simple à prendre en main et possède un fort potentiel pédagogique. Les composants graphiques exécutent simplement des commandes système, rendant aisé l'interfaçage avec les librairies disponibles. La puissance réside dans les possibilités de programmation structurée offertes par les boucles, les macro-composants et leur partage avec d'autres utilisateurs sous forme de routines. Enfin, les chaînes développées avec KD-Ariane sont exportables en scripts shell ou perl.
Ariane : c'est une plate-forme de programmation visuelle, initialement conçue pour valoriser les opérateurs de traitement d'image de la librairie Pandore (Pandore (2013)). Nous illustrons ici son potentiel dans la réalisation de chaînes de traitement pour l'extraction de connaissances dans les bases de données.
Le principe est celui de la programmation visuelle : des composants sont assemblés pour réaliser la chaîne de traitements. Ces traitements sont réalisés sur des fichiers de données, afin de produire d'autres données, ou des résultats sous forme de chaîne de caractère ou de valeur numérique.
Les composants d'Ariane
Un composant est représenté par une boîte colorée, constituée de plots de liaison, de paramètres et de résultats. Sur la figure 1, une chaîne élémentaire de traitement est représentée. Les fichiers d'entrée figurent sur la gauche et contiennent les données suivantes :
Fichier
élémentaire mesure le support (i.e le nombre d'occurrences) de chaque motif du fichier sample.sup dans les données de sample.bin. Elle ne conserve que les motifs fréquents, dont le support est supérieur à la constante minsup, ici fixée à 2.
FIG. 1 -Anatomie d'un opérateur.
Le composant support effectue cette tâche en réalisant un appel au système : -exécuter le binaire support -avec les paramètres du composant (symbolisés par l'utilisation de $p dans la propriété Binary) ; -indiquer au binaire les entrées (sympbolisées par $i) ; -rediriger le résultat vers le fichier de sortie (symbolisé par $o). -la conception de macro opérateurs ; -l'enregistrement de ces macros dans une hiérarchie de routines ; -l'exportation, le partage et la mutualisation de ces routines entre les utilisateurs.
Programmation visuelle structurée
Un exemple de réalisation
La figure 2 illustre ces concepts avec une chaîne mesurant la complexité de l'extraction des motifs fréquents. Pour cela, on souhaite chronométrer cette extraction dans une base de données classique de l'UCI, pour un seuil de support minimum allant de 1 à 50, et compter le nombre de motifs obtenu.
Nous utilisons donc une boucle for (cf. la partie supérieur de la figure 2), qui itère un traitement paramétré pour des valeurs de support minimum de 1 à 50. Cette boucle prend en entrée les données de benchmark (fichier zoo.bin) ainsi qu'un accumulateur destiné à recueillir les résultats de chaque itération. Cet accumulateur consiste en un fichier vide, créé à l'aide de la commande echo -n.
Le traitement à l'intérieur de la boucle for, sur la partie inférieure de la figure, consiste à recueillir la date d'exécution, extraire les motifs puis mesurer le temps écoulé depuis le recueil de date et enfin compter le nombre de lignes produites. Un simple echo assemble ces résultats et l'ajoute à l'accumulateur. Le résultat est visualisé à l'aide d'un script GnuPlot.

Introduction
La classification non-supervisée est une tâche importante dans l'exploration de données non-étiquetées, elle vise à les organiser en groupes (ou classes) contenant des données similaires. Cette technique est utilisée avec succès dans de nombreux domaines d'application tels que le marketing et la recherche d'information. Cependant, dans plusieurs de ces applications, les données s'organisent naturellement en groupes non-disjoints nécessitant donc de l'émer-gence de groupes qui se chevauchent. Le domaine de recherche correspondant à cette problé-matique est la classification recouvrante (overlapping clustering), étudiée à travers différentes approches au cours du dernier demi-siècle (Shepard et Arabie, 1979;Diday, 1987;Banerjee et al., 2005;Cleuziou, 2008;Depril et al., 2008;Fellows et al., 2011).
Le clustering recouvrant trouve ses applications dans de nombreux domaines nécessitant qu'un individu appartienne à plusieurs classes. Par exemple, en analyse des réseaux sociaux, un acteur peut appartenir à plusieurs communautés (Tang et Liu, 2009;Wang et al., 2010;Fellows et al., 2011) ; en classification de vidéos, chaque entrée peut potentiellement avoir plusieurs genres différents (Snoek et al., 2006) ; en détection d'émotions, une pièce de musique peut engendrer plusieurs émotions (Wieczorkowska et al., 2006), dans les systèmes de recherche d'information, un document peut aborder plusieurs thématiques (Gil-García et Pons-Porrata, 2010;Pérez-Suárez et al., 2013), etc.
Contrairement aux méthodes floues, la modélisation des recouvrements suppose que chaque observation peut avoir une appartenance totale à plusieurs groupes simultanément (sans recours à un degré d'appartenance). Quelle que soit l'approche utilisée, clustering hié-rarchique ou partitionnement, les algorithmes existants produisent des groupes sans possibilité de contrôle sur la taille et la qualité des recouvrements. Bien que la méthode devrait idéale-ment révéler une classification qui convient le plus aux structures et formes sous-jacentes des données, un tel objectif n'est généralement pas unique et la nature des recouvrements doit être utilisée comme un paramètre dans le processus de classification. Nous proposons dans cette étude, partant de l'algorithme bien connu k-moyennes, deux nouvelles méthodes R 1 -OKM et R 2 -OKM permettant l'ajustement des recouvrements de clusters selon deux principes de régu-lation à savoir : le nombre et la dispersion des groupes concernés.
La suite de l'article est organisée ainsi : la Section 2 donne un bref aperçu des différentes approches de classification recouvrante et en particulier les algorithmes OKM et ALS. La Section 3 présente la motivation liée au contrôle des recouvrements ainsi qu'une description des deux principes de régulation que nous proposons pour ajuster les recouvrements. Les deux dernières sections 4 et 5 exposent respectivement les résultats expérimentaux obtenus puis les conclusions et les perspectives de l'étude.
Méthodes de classification recouvrante
La tâche de classification recouvrante a été étudiée et partiellement résolue durant les trente dernières années par une série d'études et de propositions de deux types : des solutions heuristiques ou théoriques. Nous appelons heuristiques les solutions qui consistent soit à modifier les sorties d'approches usuelles de clustering (e.g. k-moyennes ou k-moyennes flou) telles que proposé dans Lingras et West (2004) ou Zhang et al. (2007), soit à proposer un nouveau processus intuitif de construction de classes recouvrantes telles que l'algorithme CBC (Clustering by Committee) proposé par Pantel et Lin (2002) ou encore POBOC (Pole-Based Overlapping Clustering) proposé par Cleuziou et al. (2004). Ces deux types de contributions peuvent conduire à des résultats pertinents sans toutefois s'appuyer sur des modèles théoriques, limitant de fait la possibilité de les améliorer ou de les généraliser.
Les études théoriques sont, en revanche, des extensions de modèles usuels de classification non-supervisée, tels que les approches hiérarchiques, à base de modèles de mélanges ou de graphes. Les variantes recouvrantes des hiérarchies sont les pyramides (Diday, 1987) et plus généralement des hiérarchies faibles (Bertrand et Janowitz, 2003) ; elles visent à améliorer la correspondance entre l'indice de distance induit par la structure et la mesure de dissimilarité initiale. Cependant, les structures pseudo-hiérarchiques recouvrantes sont soient restrictives en terme de configuration des recouvrements, soient complexes à générer et à visualiser.
Plus récemment, les modèles de mélanges recouvrants ont été introduits (Banerjee et al., 2005;Heller et Ghahramani, 2007;Fu et Banerjee, 2008;Cleuziou et Sublemontier, 2008) ; ils sont motivés par la modélisation de processus biologiques et se fondent sur l'hypothèse que chaque observation est le résultat d'un mélange de lois et de combinaisons additives (Banerjee et al., 2005) ou multiplicatives (Heller et Ghahramani, 2007;Fu et Banerjee, 2008) de ces lois. Ce formalisme probabiliste permet de considérer non seulement des distributions gaussiennes mais peut se généraliser à toute loi exponentielle ; en revanche, les modèles génératifs ne sont pas paramétrables et n'autorisent pas le contrôle des recouvrements notamment.
Nous concentrons notre étude sur un autre type de méthodes recouvrantes, celles formalisées par des critères objectifs et résolues de manière itérative. Deux types de modélisations des recouvrements ont été proposées et se réfèrent à deux hypothèses différentes :
-le modèle additif a été introduit initialement par Shepard et Arabie (1979), réutilisé par Mirkin (1987) et Depril et al. (2008) et formalisé en terme de modèle de mélange par Banerjee et al. (2005). Il se fonde sur l'hypothèse que les données situées à l'intersection de plusieurs clusters résultent d'une addition des caractéristiques de chacun des clusters ; les recouvrements sont alors modélisés par la somme des profils de ces clusters. Le modèle additif a été appliqué avec succès dans divers domaines tels que le marketing, l'expression de gènes et la psychologie pour lesquels il semble effectivement adapté de modéliser les données multi-classes par une combinaison additive des caractéristiques de chaque classe. -le modèle géométrique a été introduit par Cleuziou (2008) puis réutilisé par BenN'Cir et al. (2010). Il modélise les recouvrements comme une combinaison barycentrique des profils de clusters. Ce modèle est basé sur un raisonnement géométrique dans l'espace (Euclidien) où les intersections de clusters correspondent à des recouvrements au sens spatial. Ce modèle a attesté expérimentalement sur des données textuelles et multi-média par exemple.
Nous détaillons dans la suite de cette section les algorithmes ALS (Depril et al., 2008) et OKM (Cleuziou, 2008) et leur modèle additif et géométrique respectivement.
Étant donnée une matrice
, l'algorithme ALS (Alternating Least Square) consiste à minimiser la fonction objective suivante :
En se basant sur le modèle additif, ALS optimise une somme d'erreurs locales, propres à chaque observation x i , et définies par la distance Euclidienne entre l'observation et la somme des profils des clusters auxquels x i appartient.
De même, la fonction objective utilisée dans OKM (Overlapping K-Means) est basée sur les erreurs locales, mais diffère dans la manière de combiner les profils de clusters. En se basant sur un modèle géométrique, OKM évalue l'erreur locale à chaque donnée x i par la moyenne (barycentre) des profils :
Les deux critères objectifs peuvent également s'exprimer sous forme matricielle comme suit :
avec F la norme de Frobenius, et S définie pour chaque entrée par
minimisation des critères objectifs (3) et (4) est réalisée par itération classique de deux étapes, décrites dans l'Algorithme générique 1 :
1. L'étape d'affectation correspond à un problème d'optimisation discret, résolu dans ALS par l'évaluation de toutes les affectations possibles 2 K pour chaque observation x i ou bien par la relaxation du problème en utilisant une heuristique telle que dans OKM.
2. L'étape de mise à jour des profils peut être réalisée de manière optimale en utilisant la pseudo-inverse de A ou S tel que proposé dans ALS ou bien à travers une mise à jour successive des profils pour éviter l'inversion de la matrice qui est généralement coûteuses (utilisé dans OKM). Calculer les nouvelles affectations A sachant P 2:
Calculer les nouveaux profils P sachant A Tant que J(A, P ) diminue Retourner les affectations finales A.
Par la suite, le problème de la régulation des recouvrements est considéré pour le modèle géométrique sans perte de généralité puisque les principes de régulation proposés peuvent être appliqués aux modèles additifs.
Ajustement des recouvrements
Dans un processus d'extraction de connaissances, l'utilisateur ou l'expert préfère géné-ralement avoir les moyens d'interagir avec le système afin d'explorer plusieurs alternatives concernant le nombre de clusters ou la métrique ou encore afin de contrôler le degré de "flou" du clustering. De même, dans un processus de recherche de classes recouvrantes, l'expert pré-fèrera un système lui permettant d'ajuster l'importance des recouvrements relativement à ses attentes et à ses connaissances sur les données.
Pour rendre possible la régulation des recouvrements pour le modèle géométrique nous formalisons deux principes de régulation qui sont basés respectivement sur le nombre de clusters concernés et la dispersion des profils des groupes concernés. Pour visualiser ces principes de régulation, nous visualisons les zones de recouvrements avec des cellules de Voronoï dans un espace à deux dimensions dans un contexte de trois clusters. La Figure 1.(a) montre les cellules de Voronoï construites avec le modèle OKM en utilisant trois profils de classes de coordonnées respectives (4, 8), (2, 6) et (8, 3). Chaque zone de couleur est une cellule de Voronoï qui repré-sente une classe ou une intersection possible (recouvrement) de classes. Par exemple toutes les données situées dans la zone en jaune seront affectées uniquement à la classe 3 avec le modèle OKM, tandis que les données situées dans la zone en vert seront attribuées à l'intersection des classes 1 et 3 (bleu ? jaune ? vert) et la zone en noir illustre l'intersection des trois classes.
L'idée directrice de notre étude est donc de gérer différemment les classes et les combinaisons de classes de manière à limiter ou favoriser les affectations multiples aux situations où l'amélioration induite est réellement significative.
Régulation des recouvrements par le nombre de clusters (R 1 -OKM)
Nous proposons une première modélisation consistant à introduire une pondération, ajustable avec un paramètre ?, visant à réguler chaque erreur locale relativement au nombre d'affectations de l'individu. Le nouveau modèle s'exprime par le critère objectif suivant :
avec k a i,k le nombre de classes auxquelles l'objet x i appartient et ? ? R un paramètre contrôlant les recouvrements :
1. ? = 0 annule la pondération sur les tailles de combinaison conduisant ainsi au modèle OKM originel, 2. ? > 0 pénalise les affectations à des combinaisons larges tant que ? augmente jusqu'à aboutir à un modèle équivalent à k-moyennes (? ? +?), 3. ? < 0 favorise les recouvrements tant que ? diminue jusqu'à aboutir à un modèle totalement recouvrant avec des données affectées à toute les classes (? ? ??). 
Régulation des recouvrements par la dispersion des profils (R 2 -OKM)
Contrairement au modèle de régulation précédent qui se base sur le nombre de combinaisons de classes, le second modèle se base sur la dispersion des profils de classes afin de contrôler l'importance des recouvrements. L'hypothèses sous-jacente au modèle R 2 -OKM est que les recouvrements doivent être d'autant plus pénalisés qu'ils mettent en jeu des classes distantes les unes des autres. Inversement, les recouvrements sont autorisés (voire encouragés) pour les classes dont les profils sont plus proches.
Afin de formaliser ce principe de régulation, nous proposons de favoriser ou pénaliser l'erreur locale à chacune des données x i en utilisant le critère de dispersion suivant, quantifiant (le carré de) la distance moyenne de x i avec ses profils de classes :
Le nouveau critère objectif du modèle R 2 -OKM se présente alors comme suit : 
Processus d'optimisation
L'algorithme d'optimisation pour R 1 -OKM et R 2 -OKM suit le processus général de réaffec-tations décrit dans l'Algorithme 1. La stratégie d'affectation du modèle OKM (Cleuziou, 2008) demeure valide, mais en utilisant les nouveaux critères objectifs J R1?OKM et J R2?OKM .
La mise à jour des profils est effectuée successivement pour chaque cluster et nécessite la dérivation de nouveaux profils P * k,. garantissant la convergence des modèles proposés. Étant donnée la matrice A des affectations, les profils optimaux pour R 1 -OKM et R 2 -OKM sont obtenus par dérivation des critères (5) et (7) respectivement :
avec P i k le profil de la classe P k idéal vis-à-vis de l'individu x i , c'est-à-dire tel que l'erreur locale pour x i est égale à zéro : p i k,j = x i,j l a i,l ? l =k a i,l p l,j . Nous avons proposé et formalisé deux principes de régulation des recouvrements dans le cadre des modèles géométriques ainsi que leur mise en oeuvre algorithmique. Notons que R 1 -OKM et R 2 -OKM sont deux généralisations de l'algorithme k-moyennes : si les affectations sont restreintes à un seul groupe ou si les paramètres (? et ?) sont suffisamment pénalisant, le critère objectif est équivalent au critère des moindres carrés utilisé par k-moyennes. La convergence des méthodes proposées est assurée par un algorithme à faible coût en terme de complexité : O(T N KlogK) où T est le nombre d'itérations.
Expérimentation
L'évaluation des méthodes de classification non-supervisée est connue pour être une tâche difficile puisqu'il n'existe pas, par définition, de classification exacte sur laquelle se fonder pour mesurer la qualité d'un clustering. La plupart des méthodes de classification recouvrantes ont été évaluées en utilisant la "F-mesure" combinant précision et rappel sur les paires d'individus (Banerjee et al., 2005;Cleuziou, 2008;Fu et Banerjee, 2008). Cependant, dans le contexte recouvrant, la "F-mesure" ignore la multiplicité des affectations. Amigó et al. (2009) ont proposé une extension de leur métrique "BCubed" à la classification recouvrante qui permet d'affiner le calcul des mesures de précision et de rappel en intégrant cette multiplicité. De la même manière que Suárez et al. (2013), nous avons décidé d'utiliser la mesure ajustée "F-BCubed" pour une évaluation plus fine des classifications recouvrantes.
TAB. 1 -Statistiques des jeux de données utilisés.  La taille des recouvrements a une grande influence sur la qualité des mesures de performance et sur la structure des classes attendues. Ainsi, plutôt que de fournir des tableaux de valeurs, nous avons positionné les scores (F-BCubed) de chaque méthode relativement au taux de recouvrement. La Figure 2 présente ces positionnements : chaque point sur la figure est obtenu par une moyenne sur dix exécutions de chaque algorithme dans les mêmes conditions initiales (initialisation des profils de clusters) et pour un nombre de classes égal au nombre d'étiquettes de la référence.
Les méthodes MOC, ALS et OKM étant sans contrôle possible sur les recouvrements, un unique clustering est produit pour chaque initialisation, ce qui se traduit par un seul point moyen sur les figures. En revanche, pour les trois autres algorithmes, les scores obtenus pour des paramétrages consécutifs ont été reliés afin d'observer les tendances. Les lignes verticales en pointillés indiquent le taux de recouvrement de la classification de référence.
Les principales observations que nous pouvons relever de ces résultats sont : -que les modèles additifs (MOC et ALS) ne parviennent pas à construire de recouvrements entre les classes pour EachMovie, Music emotion et Scene. Par contre, sur des données biologiques (Yeast), ces méthodes se caractérisent par une bonne qualité de classification et parviennent même à construire de larges recouvrements. -la capacité des principes de régulation proposés à produire des recouvrements adaptés à la structure sous-jacente des données : sur EachMovie et Scene, caractérisés par de faibles recouvrements, les méthodes existantes produisent des recouvrements larges conduisant à affaiblir leur score ; l'ajustement des recouvrements offre un moyen d'amé-liorer significativement la qualité des classifications générées. -les performances de k-moyennes flou seuillé sont rapidement limitées lorsque le nombre de clusters augmente. Cette méthode ne parviens pas à égaler les résultats de R 1 -ou R 2 -OKM sur les jeux de données Music emotion, Scene et Yeast avec 6, 6 et 14 clusters respectivement.
Conclusion
Nous avons proposé dans cette étude deux modèles généralisant k-moyennes afin de produire des classifications recouvrantes avec un contrôle sur la taille des recouvrements. Ces deux modèles offrent une régulation par le nombre et la dispersion des profils des clusters concernés, conduisant à des schémas de classification plus appropriés.
Pour compléter cette étude nous envisageons, dans une version plus étendue, de proposer une analyse détaillée ainsi qu'une évaluation experte sur un jeu de données réel lié au domaine de la recherche d'information. Les nouveaux principes de régulation des recouvrements, introduits dans ce présent travail, ne sont pas limités aux modèles géométriques et pourront être transposés aux modèles additifs (type ALS).

Introduction
De nos jours, les volumes de données textuelles gérées et échangées ne cessent d'augmenter. Ceci est dû au fait qu'un texte est plus riche en sémantique que tout autre support informationnel. Toutefois, pour les systèmes de gestion de bases de données, les informations gérées se présentent principalement sous format structuré. Bien que cette structuration offre un confort au niveau de l'exploitation de données, elle présente une limite quant à la représen-tation des connaissances d'une manière explicite. Généralement, les données textuelles sont plus significatives que les schémas conceptuels des bases de données, dans la mesure où elles permettent une description sémantique des relations entre les éléments d'un domaine. Pour profiter de l'abondance des données structurées et en même temps expliciter les sémantiques qui y sont incarnées, nous proposons une approche visant la génération des textes à partir de bases de données. Cette approche s'articule autour de trois grandes phases : (i) une phase de prétraitement de données pour la génération d'ontologies à partir des bases de données, (ii) une phase de génération de règles d'association pour mieux expliciter la sémantique de l'ontologie globale et (iii) une phase de génération d'un extrait textuel. Dans le reste de cet article la section 2 détaille les trois étapes de base de notre approche. La section 3 est consacrée à la mise en oeuvre de notre approche.
Approche proposée
Notre approche pour la génération d'un extrait textuel est basée essentiellement sur les ontologies comme ressources sémantiques. En fait, ce choix est justifié par la capacité des ontologies d'offrir une modélisation sémantique des concepts et des relations associées. Étant donné que nous visons la production d'un extrait textuel, une phase de génération de règles d'association s'avère indispensable pour guider le passage de l'ontologie au texte. Concrète-ment, cette approche comporte principalement trois grandes phases (voir figure 1).
FIG. 1 -Architecture générale de l'approche proposée
Phase de prétraitement des données
Cette phase vise l'association d'une ontologie globale aux bases de données alimentées comme inputs à notre approche. Ceci étant accompli en trois étapes. La première dite de conceptualisation, assure la transformation de chaque base de données fournie en entrée en une ontologie qu'on qualifiera de locale. Une phase d'ontologisation, pour aboutir une ontologie globale est accomplie par la suite pour faire face à l'hétérogénéité des ontologies locales gé-nérées. Il s'agit de procéder à un mapping pour découvrir la correspondance sémantique entre les éléments dans différentes ontologies locales. Pour ce faire, nous avons utilisé l'algorithme FOAM (Framework for Ontology Alignment and Mapping) (Ehrig, 2007). L'idée sous-jacente à ce dernier est de procéder à un calcul de similarité de certaines paires d'entités (E 1 , E 2 ) des ontologies à intégrer. Une fois, l'ontologie globale créée, une étape d'opérationnalisation prend lieu. L'objectif étant d'évaluer la consistance de l'ontologie.
Phase de génération de règles d'association
L'ontologie créée lors de la phase précédente est riche sémantiquement par rapport aux bases sources. Néanmoins, sa sémantique n'est pas exprimée d'une manière explicite. En fait, le passage à une vue textuelle est tributaire d'une interprétation claire de l'ontologie en question. C'est l'objectif de la phase en cours. Une fois l'ontologie globale sollicitée moyennant une requête utilisateur, les données sont injectées comme inputs de la phase de génération de règles. Lors de cette phase nous avons adopté l'algorithme Apriori (Agrawal et Srikant, 1994) pour déterminer les règles. Une fois les règles produites, ils seront considérés comme inputs pour déclencher la phase de génération de l'extrait textuel.
Phase de génération d'un extrait textuel
Cette phase est assurée en suivant trois étapes : une planification de document, une planification de surface et enfin une réalisation de surface. Quant à la planification de document elle vise la détermination de la forme finale de l'extrait textuel à générer. Plus explicitement, l'ensemble de règles subit une sélection basée sur un calcul de support et de confiance pour maintenir un sous ensemble de règles jugées pertinentes. Une fois le contenu déterminé, il doit être structuré. La structuration du contenu consiste à analyser les règles d'association sé-lectionnées afin d'extraire les éléments d'information qui les constituent. Une fois les règles analysées, une étape d'organisation est nécessaire. Pour assurer cette structuration, nous proposons l'utilisation des schémas de McKeown (1985). Ces derniers présentent un ensemble de spécifications sur la méthode d'organisation des éléments d'information. Une fois le contenu de notre extrait textuel est déterminé et structuré, il convient de convertir les éléments constituant les règles d'association en termes lexicaux grâce à deux fonctions qui sont la lexicalisation et l'agrégation. La dernière étape de cette phase est consacrée pour la réalisation de surface. Elle consiste en fait à traduire la représentation conceptuelle en extrait textuel compréhensible.
Implémentation
Pour la mise en oeuvre de notre approche un ensemble de bases de données géographiques du domaine de l'agriculture a été fourni comme input à notre système. Ces bases de données ont été téléchargées à partir d'un système mondial d'information sur l'eau et l'agriculture de la FAO (Food and Agriculture Organization) AQUASTAT 1 . Chacune de ces bases doit être convertie en une ontologie locale, via l'activation du plugin DataMaster (Nyulas et al., 2007)  . L'intégration des ontologies locales est réalisée grâce au plugin Prompt (Noy et Musen, 2001) en appliquant l'algorithme FOAM. Pour l'extraction des règles d'association, l'interrogation de notre ontologie est indispensable. Le résultat de la requête est enregistré sous format CSV. Ce fichier est exploité pour la génération des règles d'association. Ces dernières sont exploitées moyennant l'utilisation du langage prolog

Introduction
Les graphes sont un puissant outil de représentation pour la découverte de connaissances dans de nombreux contextes. Ainsi, nous pouvons nous intéresser à des graphes qui décrivent des entités (noeuds) mises en relation (arêtes) : souvent, ces entités peuvent être décrites au moyen d'attributs et les relations ou descriptions des attributs peuvent évoluer au cours du temps. Nous parlerons alors de graphe attribué et dynamique (Jin et al. (2007); Boden et al. (2012)). Développer de nouvelles méthodes pour la fouille de tels graphes est important, ne serait-ce que pour le potentiel applicatif des analyses d'interactions sociales. L'explicitation puis l'exploitation de hiérarchies déclarant certaines relations entre attributs a été très étu-dié, notamment dans le contexte de la découverte de motifs et de règles d'association multidimensionnelles (Srikant et Agrawal (1996); Han et Fu (1999); Chen et al. (2009) ;Plantevit et al. (2010)). Nous pensons que de telles hiérarchies, souvent faciles à expliciter, permettraient d'ajouter de la connaissance du domaine sur des graphes attribués dynamiques pour améliorer la pertinence des fouilles réalisées. Nous décidons d'étendre la proposition présen-tée dans Desmier et al. (2013) pour la découverte de motifs de co-évolution dans des graphes attribués dynamiques : chaque motif découvert va correspondre à trois ensembles qui sont (a) un ensemble de noeuds, (b) un ensemble de pas de temps, et (c) un ensemble d'attributs tel qu'une tendance d'évolution (croissance, décroissance) est associée à chaque attribut. Prenons par exemple un graphe représentant un ensemble d'aéroports pour les noeuds, reliés deux à deux par une arête s'il existe au moins un vol direct entre les deux d'aéroports et dont les attributs représentent le nombre de vols au départ et à l'arrivée de chaque aéroport. Un motif pourrait par exemple être un groupe de 9 aéroports qui ont vu leur nombre de vols au départ et à l'arrivée diminuer en décembre car il y a eu de fortes chutes de neige. Cependant ce type de motif peut entrainer beaucoup de redondances dûes à la nécessité d'avoir un respect strict des tendances sur les attributs. Supposons par exemple que 11 aéroports ont vu leur nombre de vols au départ diminuer, 10 ont vu leur nombre de vols à l'arrivée diminuer tandis que 9 d'entre eux ont eu les deux à la fois : dans une approche comme celle de Desmier et al. (2013), trois motifs seront alors extraits. Afin d'améliorer la pertinence des motifs de co-évolution calculés, nous introduisons ici l'utilisation d'une hiérarchie sur les attributs. Nous supposons que la hiérarchie est fournie par l'analyste et nous étudions la découverte de motifs multi-niveaux, c'est à dire pouvant contenir des éléments appartenant à plusieurs niveaux de la hiérarchie. Nous proposons une mesure de pureté pour vérifier que la tendance est respectée par un certain pourcentage des attributs fils. Cette mesure permet d'accepter qu'un ensemble de noeuds et de pas de temps ne respecte pas strictement la tendance des attributs bien qu'il apporte une information suffisamment pertinente. Dans notre exemple, considérons une hiérarchie qui expliquerait que l'attribut « Nombre de vols » est le parent du nombre d'arrivée et du nombre de départs. Le motif extrait présenterait alors un groupe de 12 aéroports qui ont vu leur nombre de vols diminuer en décembre, en incluant ceux qui ont vu leur nombre de vols diminuer soit au départ soit à l'arrivée. Cette multiplicité des niveaux des attributs permet d'obtenir des motifs plus concis et donc d'une certaine façon plus robustes au bruit ou aux erreurs : des motifs plus généraux sont découverts dont les noeuds ne respectent pas une co-évolution stricte. Il devient donc possible d'intégrer au motif des entités ou des pas de temps pour lesquels la co-évolution n'est en partie pas respectée parce qu'ils ont eu temporairement un comportement différent.
Nos contributions sont les suivantes. Nous définissons un nouveau problème de fouille de données : la découverte de motifs hiérarchiques de co-évolution dans des graphes attribués dynamiques. Nous définissons ce type de motif comme une séquence de graphes connexes et dont les attributs co-évoluent, et nous introduisons des mesures qui permettent d'évaluer leur pureté. Ensuite, nous proposons un algorithme qui calcule l'ensemble des motifs qui satisfont une combinaison de contraintes spécifiée par l'analyste. Une formalisation du problème et des mesures est faite dans la Section 2 et l'algorithme est décrit dans la Section 3. Les résultats des expérimentations quantitatives et qualitatives sont présentés dans la Section 4. Un état de l'art est proposé dans la Section 5 avant une brève conclusion en Section 6.
Motifs Hiérarchiques de Co-évolution
Un graphe attribué dynamique G se définit comme une séquence de graphes attribués G t sur T pas de temps, i.e., G = {G t |t = 1..T } avec G t = (V, E t , A t ). V est un ensemble de noeuds, E t est un ensemble d'arêtes qui dépendent du temps et A t est un vecteur de valeurs pour les attributs de A au temps t. A est un ensemble d'attributs commun à tous les noeuds et à tous les temps. Une hiérarchie H sur A est un arbre dont le noeud All est la racine et les attributs de A les feuilles. Les arcs représentent une relation is_a. La relation de spécialisation (resp. généralisation) correspond à un parcours de haut en bas (resp. de bas en haut), c'est à dire de la 
racine aux feuilles (resp. des feuilles à la racine). Cette hiérarchie représente une connaissance a priori de l'analyste. La fonction parent(x) retourne les noeuds parents directs du noeud x, children(x) les noeuds fils directs, up(x) l'ensemble des ancêtres de x, x inclus, down(x) l'ensemble de ses descendants, x inclus. La fonction leaf (x) retourne les descendants de x qui sont des feuilles de la hiérarchie, i.e., leaf (x) = down(x) ? A. Le domaine de la hiérarchie dom(H) contient tous les noeuds sauf le noeud racine. Un motif hiérarchique de co-évolution extrait à partir d'un graphe attribué dynamique et d'une hiérarchie est un ensemble de noeuds, de pas de temps et d'attributs. Ses noeuds respectent la même tendance sur les attributs à chaque pas de temps et ils sont connectés dans le graphe par un chemin de taille maximale définie. Formellement, un motif hiérarchique de co-évolution
. Par abus de langage, nous utiliserons indifféremment ? ou A pour désigner les attributs du motif. Un attribut associé à une tendance est noté a s ? ? tel que s ? {+, ?} et a ? A. On dit que s est le symétrique de s (i.e., si s = + alors s = ? et inversement). Par définition, ce motif M doit respecter une contrainte de co-évolution et une contrainte de diamètre. La première vérifie que la tendance associée à chaque attribut du motif est respectée pour au moins un de ses attributs fils par tous les noeuds à tous les temps. La deuxième vérifie que la longueur du plus court chemin entre chaque paire de noeuds à chaque pas de temps est inférieure à un seuil donné. coevolution(M) Soit ? condition la fonction de Kronecker telle que ? a s (v,t) = 1 si le noeud v au pas de temps t respecte la tendance s pour l'attribut a. Un motif M = (V, T, ?) respecte la contrainte de co-évolution si ?a
Pour éviter de produire certains motifs, nous utilisons également des contraintes de taille
Notons que ces définitions sont adaptées de celles décrites dans Desmier et al. (2013) pour prendre en compte l'existence des hiérarchies et donc la possibilité qu'un motif contienne des attributs appartenant à plusieurs niveaux.
Par définition, la co-évolution requiert que la tendance soit vraie pour au moins l'un des attributs fils, mais si elle n'est vérifiée que pour une faible proportion des attributs fils, l'information apportée par le motif n'est pas satisfaisante. Il faut donc savoir si le motif apporte une information intéressante et s'il est préférable de spécialiser l'attribut ou de le conserver en l'état. Considérons la Figure 1, soit le motif
}} n'est pas un motif valide par rapport à la contrainte de volume, le motif M 1 est plus précis que son parent M .
Soit une mesure de pureté du motif M = (V, T, ?), purity(M ) est le nombre de triplets (v, t, a s ), v ? V, t ? T, a s ? ? valides par rapport au nombre de triplets possibles :
Pour savoir si l'attribut résume bien l'information sur ses fils dans le motif, introduisons la contrainte pureMin(M) qui impose que la pureté du motif soit supérieure à un seuil ? ? [0, 1] fixé par l'analyste. Il faut également savoir si la spécialisation de l'attribut apporte un gain de pureté tel qu'il compense la perte de généralité. Pour celà, nous définissons la contrainte gainMin(M) qui impose que la pureté du motif fils divisée par la pureté maximale de ses motifs parents soit supérieure à un seuil ? > 1 fixé par l'analyste. Formellement, le gain de pureté apporté par un motif M = (V, T, ?) par rapport à ses motifs parents
s avec a ? ? se définit ainsi :
Nous pouvons maintenant dire que la contrainte pureMin(M) impose que purity(M ) > ? et que la contrainte gainMin(M) impose que gain(M ) > ?.
Un motif qui respecte gainMin(M) est un motif qui apporte une information plus pure que tous ses motifs parents. S'il ne la respecte pas, ses motifs parents ont donc une pureté équivalente et apportent au moins autant d'information. Notons que si un motif fils n'apporte pas une gain de pureté suffisant, la pureté de ses descendants pourrait être bien supérieure à celle du motif parent (la pureté est toujours égale à 1 pour les attributs feuilles de la hiérar-chie). Cependant la spécialisation engendrerait beaucoup de motifs potentiels sans réel apport d'information.
Un motif hiérarchique de co-évolution M tel qu'il est « inclus » dans un motif M n'apporte aucune information supplémentaire, un motif ne doit être conservé que s'il est maximal. Un motif hiérarchique de co-évolution M = (V, T, ?) respecte maximal(M), i.e., s'il n'existe pas de motif hiérarchique de co-évolution
Définition du problème : Étant donné un graphe attribué dynamique et une hiérarchie, notre problème consiste à extraire l'ensemble des motifs hiérarchiques de co-évolution tels qu'ils respectent les contraintes de co-évolution et de diamètre et potentiellement une conjonction de contraintes supplémentaires dont les seuils sont choisis par l'analyste.
L'algorithme 1 présente notre proposition. L'énumération peut être représentée par un arbre. Chaque noeud de l'arbre contient 2 tri-sets (collections de trois ensembles), P contient les éléments présents dans le motif en construction et C contient les éléments encore à énumé-rer. Au début de l'algorithme P = ? et C = }{children(All) × {?, +}}}. À chaque étape soit un élément est énuméré (noeud, temps ou attribut) soit un attribut a de P est spécia-lisé et un attribut de C est énuméré en conservant a non spécialisé. Au début de l'algorithme et afin de pouvoir mieux exploiter les propriétés d'élagage, un noeud est énuméré puis un temps et un attribut. A chaque étape, les éléments de C sont supprimés s'ils ne peuvent pas être ajoutés à P en donnant un motif valide, i.e., s'ils ne peuvent pas respecter les différentes contraintes. Si le nouveau motif P ne respecte pas les contraintes, l'énumération est stoppée. Cependant la majorité des contraintes présentées dans la Section 2 n'exhibe pas de propriété de monotonie et nous avons défini des bornes et des contraintes relaxées pour élaguer l'espace de recherche.
Lorsqu'un élément est énuméré, il est supprimé de C et ajouté à P , si l'élément est un attribut a s , son symétrique a s est également supprimé de C. Lors de l'étape de spécialisation d'un attribut a de P , tous les motifs fils qui apportent un gain de pureté sont énumérés, i.e., ?a i ? children(a) t.q. gainMin(P i ) est respecté, le motif P i est énuméré et les a s j et a s j t.q. a j ? children(a) avec j > i sont ajoutés à C. Tous les motifs contenant a ? P .? non spécialisé et b ? C.? sont également énumérés. L'étape de spécialisation est présentée dans la Fig. 2, les deux noeuds fils de gauche représentent la spécialisation, tandis que les deux de droite représentent l'énumération avec l'attribut non spécialisé. Lors d'une étape d'énumération sans spécialisation, il ne faut considérer que les deux noeuds fils de droite comme exemple.
FIG. 2 -Exemple d'une étape de spécialisation. Les deux noeuds de gauche présentent la spécialisation de l'attribut, les noeuds de droite présente l'énumération d'un autre attribut.
. À chaque étape C est élagué en fonction de la contrainte de co-évolution, ne sont conservés que les v ? V t.q.
De plus, si coevolution(P ) n'est pas respectée, aucun motif valide ne pourra être énuméré par la suite.
Le diamètre n'est pas monotone, l'ajout d'un noeud à un ensemble de noeuds peut diminuer ou augmenter le diamètre du sous-graphe induit. Il faut donc uniquement vérifier que le graphe induit par l'ensemble de noeuds de P peut respecter la contrainte de diamètre en ajoutant tout ou partie de l'ensemble de noeuds de C. Considérant la Fig. 1 et un diamètre maximal k = 2, le motif 1 v 4 }{t 2 }{}} ne respecte pas diameter, pourtant si le noeud v 3 est ajouté, le motif résultant 1 v 3 v 4 }{t 2 }{}} respecte la contrainte. À l'inverse si le noeud v 2 est ajouté à ce dernier, le motif 1 v 2 v 3 v 4 }{t 2 }{}} ne respecte plus diameter puisque 
) n'est pas respectée, aucun motif valide ne pourra être énuméré. Ne sont conservés dans C que les v ? V t.q.
. L'espace de recherche est également élagué grâce à la notion de volume : l'énumération est arrêtée si le volume du motif ne peut pas être supérieur à ?. Contrairement à Desmier et al. (2013) 
énu-mération continue, sinon plus aucun motif valide ne peut être énuméré (preuve disponible). Finalement, la contrainte de pureté, qui n'est pas monotone puisqu'elle dépend de la fonction volume, donne également lieu à élagage. Si
> ? l'énumération continue, sinon aucun motif valide ne pourra être énuméré.
Lorsque C est vide, P = (V, T, ?) est un motif final potentiel. S'il vérifie sizeMinV(P ), sizeMinT(P ), sizeMinA(P ), volumeMin(P ), pureMin(P ) et maximal(P ) alors le motif est valide. Il est maximal s'il n'existe pas un ensemble de noeuds v ? V t.q. le motif (V ? v, T, ?) respecte les contraintes, il n'existe pas t ? T t.q. le motif (V, T ?T, ?) respecte les contraintes et il n'existe pas d'attribut a
respecte les contraintes. L'algorithme extrait donc tous les motifs hiérarchiques de co-évolution qui respectent les contraintes considérant les seuils minV ,minT , minA, V, ? et ?. Nous présentons des résultats expérimentaux obtenus sur des jeux de données réels pour illustrer l'intérêt de notre approche. Toutes les expérimentations ont été réalisées sur une ferme de calcul. Chaque machine est équipée de 2 processeurs à 2,5GHz et 16GB de RAM et utilise la distribution « Scientific Linux ». L'algorithme est implémenté en C++. L'objectif des expérimentations est de répondre aux questions suivantes : Les motifs sont ils pertinents ? Est-il possible de les extraire en un temps acceptable ? Les nouvelles mesures qui ont trait à la pureté permettent-elle d'obtenir des motifs plus faciles à interpréter ? « DBLP » est un graphe de co-auteurs créé à partir de la base DBLP Ceci est dû au fait que la majorité des triplets (v, t, a ? ) et (v, t, a + ) sont invalides dans le jeu de données car beaucoup d'auteurs ne publient jamais dans certains journaux ou conférences. Considérant le seuil de gain, le nombre de motifs est également très dépendant de ?. Par contre, le comportement en terme de temps d'exécution n'est pas monotone. Ce temps diminue fortement puis ré-augmente. Ceci s'explique par le fait que la hiérarchie possède beaucoup de niveaux. Lorsque ? est bas, les attributs sont spécialisés et il y a de nombreuses énumérations possibles. Avec une valeur moyenne pour ?, les attributs sont conservés à des niveaux moyens de la hiérarchie, ce qui permet de bien élaguer le reste des éléments puisque les auteurs ont des comportements similaires sur ces ensembles de conférences ou journaux. À l'inverse, si ? est élevé, les attributs ne sont que très peu spécialisés et le reste des éléments est peu élagué. Enfin, avec le seuil de volume ?, le nombre de motifs est logiquement impacté mais on note que le temps de calcul l'est également bien que la contrainte ne soit pas monotone. La hiérarchie a un effet important sur le résultat de l'extraction. Pour le montrer nous avons créé cinq hiérarchies pour le jeu de données DBLP en enlevant à chaque fois un niveau de profondeur à celle proposée dans la Fig.  3(a). Les résultats de l'expérimentation sont présentés dans la Dans les deux cas, les motifs contiennent les même temps, i.e., principalement les 6 premières semaines du jeu de données. Un motif en particulier est intéressant : lors de la première extraction il possède 65 aéroports avec « NbDep et « NbArr » qui diminuent ; dans la deuxième extraction il contient 83 aéroports incluant les 65 premiers avec « NbFlights » qui diminue. Les motifs extraits en augmentant ? contiennent principalement des attributs parents pour les délais et les vols, pourtant les motifs ont tous une pureté supérieure à 0, 9. Ils apportent donc une information plus concise tout en conservant une précision tout à fait acceptable. De plus, l'arrêt à un niveau supérieur de la hiérarchie permet d'augmenter le nombre d'aéroports concernés qui pouvaient ne pas apparaitre dans la première extraction à cause d'une erreur dans les données ou plus simplement parce-qu'ils ne respectaient pas une tendance à un temps du motif alors qu'ils respectaient toutes les autres évolutions.
Expérimentations
État de l'art
La fouille de graphes dynamiques est très étudiée. Lahiri et Berger-Wolf (2010) recherchent des sous-graphes similaires apparaissant périodiquement. Inokuchi et Washio (2010) proposent l'extraction de sous-séquences de sous-graphes induits fréquents tels qu'un graphe est un sousgraphe d'un autre s'il existe une fonction sur les noeuds, arcs, étiquettes et graphes de la sé-quence. Prado et al. (2013a) proposent un algorithme de fouille de sous-graphes planaires fréquents à partir d'une base de données de graphes planaires. Ces motifs peuvent être utilisés comme base pour l'extraction de motifs spatio-temporels. Les graphes attribués ont également été étudiés. Moser et al. (2009) ont proposé une méthode pour trouver des sous-graphes homogènes denses, c'est à dire qui partagent un grand nombre d'attributs. Silva et al. (2012) proposent l'extraction de paires de sous-graphes et d'ensembles d'attributs booléens tels que les attributs sont fortement corrélés avec les sous-graphes. Mougel et al. (2012) recherchent des collections de k-cliques percolées homogènes, c'est à dire des ensembles de cliques qui se chevauchent et partagent un ensemble d'attributs. Prado et al. (2013b) proposent une mé-thode pour trouver des régularités sur les descripteurs des attributs dans des graphes attribués. Pour cela, ils utilisent les attributs associés aux noeuds ainsi que des propriétés topologiques calculées pour chaque noeud. Récemment des études ont également été faites sur les graphes attribués dynamiques. Boden et al. (2012) proposent d'extraire des clusters dans des graphes attribués puis d'associer les clusters similaires à des temps consécutifs. Jin et al. (2007) étu-dient les graphes dynamiques dont les noeuds sont associés à un poids. Ils extraient des groupes de noeuds connectés dont le poids suit une évolution similaire croissant ou décroissant sur des temps consécutifs. Desmier et al. (2013) extraient des ensembles de noeuds similaires ayant une même tendance dans le temps sur leurs attributs et une évolution différente du reste du graphe. Aucun de ces travaux n'intègre de connaissance a priori via des hiérarchies. L'utilisation de connaissances utilisateur dans le processus d'extraction est très étudiée en fouille de données. Cela peut se faire en pré-traitement des données, en processus itératif lors de l'extraction ou encore pendant l'extraction. Nous présentons ici des méthodes utilisant une hiérarchie comme connaissance à priori. Srikant et Agrawal (1996) proposent l'extraction de « séquences étendues ». Ils utilisent une taxonomie sur les objets pour extraire des séquences possédant plusieurs niveaux de hiérarchie. Han et Fu (1999)  
Conclusion
Nous nous sommes intéressés à l'extraction de motifs hiérarchiques de co-évolution dans des graphes attribués dynamiques, c'est à dire un sous-graphe connexe induit par un ensemble de noeuds de temps et d'attributs appartenant à une hiérarchie et associés à une tendance respectée par les noeuds. Nous avons proposé plusieurs contraintes qui permettent d'obtenir des motifs concis et sans perte d'information et nous avons présenté un algorithme complet exploitant l'ensemble des contraintes. Les résultats des expérimentations sur trois jeux de données réels montrent que l'utilisation de la hiérarchie permet d'obtenir un nombre restreint de motifs. Deux perspectives semblent intéressantes, premièrement, la possibilité d'avoir une hiérarchie à héritage multiple ; deuxièmement créer une hiérarchie sur les temps et les noeuds du graphe.
Remerciements Les auteurs remercient l'ANR pour le financement de ce travail à travers le projet FOSTER (ANR-2010-COSI-012-02), ainsi que le Centre de Calcul du CNRS/IN2P3.

Introduction
La classification non-supervisée a pour but de regrouper les observations proches dans un même groupe, tandis que les observations éloignées doivent être affectées à des groupes différents. Cette définition pourrait être insuffisante dans de nombreuses applications de regroupement dans lesquelles un objet peut appartenir à la fois à plusieurs classes. Ce type de problématique est appelée classification recouvrante ou encore classification non-exclusive. Plusieurs applications réelles nécessitent d'utiliser ce type de schéma de classification tels que le regroupement de documents où chaque document peut aborder plusieurs thèmes, la classification de vidéos où un film peut avoir différents genres (Snoek et al., 2006), la détection d'émotions où un morceau de musique peut évoquer plusieurs émotions distinctes (Wieczorkowska et al., 2006).
Afin de produire des classes non-exclusives, divers méthodes ont été proposées utilisants des approches hiérarchique (Diday, 1984), de partitionnement (Fu et Banerjee, 2008;Banerjee et al., 2005;Heller et Ghahramani, 2007), de corrélation (Bonchi et al., 2011) et de la théorie des graphes (Fellows et al., 2009(Fellows et al., , 2011. Nos travaux se limitent à l'étude des méthodes recouvrantes basées sur le partitionnement. Les méthodes de partitionnement existantes utilisent des modèles de mélange de lois (Fu et Banerjee, 2008;Banerjee et al., 2005;Heller et Ghahramani, 2007) ou bien utilisent l'approche k-moyennes (Cleuziou, 2008). Des exemples de ces méthodes sont OKM (Cleuziou, 2008) et Parameterized R-OKM (Ben N'Cir et al., 2013. Ces dernière méthodes ne considèrent que la distance euclidienne entre chaque observation et ses représentants. En outre, la densité des observations dans un groupe pourrait être nettement différente des autres groupes dans l'ensemble de données. La métrique euclidienne utilisée évalue seulement la distance Euclidienne entre l'observation et le représentant, ou la combinaison de représentants, de clusters. Elle ne tient pas compte de la variation de la distance globale pour toutes les observations dans un groupe.
Récemment, une nouvelle mesure de distance a été proposée par Tsai et Lin (2011). Elle intègre la variation de distance au sein d'un même cluster et sert à régulariser la distance entre un objet et le représentant de cluster relativement à la densité interne du cluster en question. Nous proposons dans ce qui suit d'adapter cette nouvelle mesure pour détecter des groupes non-disjoints avec des densités différentes.
Cet article est organisé comme suit : la Section 2 décrit deux méthodes existantes de classification recouvrante à savoir OKM et Parameterized R-OKM. Ensuite, la Section 3 présente le problème d'identification des groupes avec une densité différente. La Section 4 décrit les méthodes proposées OKM-? et Parameterized R-OKM-? tandis que la Section 5 illustre les expériences réalisées sur des ensembles de données artificielles. La conclusion et les perspectives feront l'objet de la Section 6.
La classification recouvrante
Nous décrivons dans cette partie deux méthodes existantes de classification recouvrantes basées sur l'algorithme k-moyennes. Ces méthodes généralisent k-moyennes pour produire des recouvrements de classes.
Overlapping K-Means
La méthode OKM cherche des recouvrements optimaux plutôt que des partitions optimales, étant donné un ensemble d'objets à classifier
et N le nombre d'objets, OKM recherche un k recouvrement de telle sorte que la fonction objective suivante soit optimisée :
où x i désigne l'image de x i définie par la combinaison des centres des clusters auxquels x i appartient :
k?? i avec ? i l'ensemble des affectations de l'objet x i aux différents clusters, c'est-à-dire les clusters auxquels x i appartient et c k correspond au représentant du cluster k. Le critère J de la fonction objective généralise le critère des moindres carrés utilisé dans la méthode k-moyennes. Pour minimiser ce critère, deux étapes principales sont exécutées itérativement :
1. la mise à jour des représentants de classes (C) puis.
2. l'affectation des objets à ces représentants (?).
les conditions d'arrêt de la méthode reposent sur plusieurs critères à savoir le nombre d'itéra-tions maximales et le seuil minimal d'amélioration de la fonction objective entre deux itéra-tions. OKM produit des classes non-disjointes avec des zones de recouvrement trop larges. Cependant, les groupes ayants des zones de recouvrement larges ne sont pas appropriés à la plupart des applications réelles. Pour résoudre ce problème, une méthode récente, référencée Parameterized R-OKM (Ben N' Cir et al., 2013), propose un nouveau modèle qui produit des clusters non-disjoints avec possibilité de contrôle sur les zones de recouvrements.
Parameterized R-OKM
Afin de contrôler la taille des recouvrements entre les classes, Parameterized R-OKM restreint l'attribution d'une observation à plusieurs groupes en fonction du nombre d'affectations |? i |. Parameterized R-OKM est basée sur la minimisation du critère objetif suivant : une mesure, intégrant la variation de densité entre les classes, à été introduite par Tsai et Lin (2011). Cette technique à été utilisée dans k-moyenne floues et a été référencée (FCM-? ). FCM-? introduit la variation de densité pour chaque groupe de données afin de régulariser la distance entre un objet et le représentant. Cette mesure peut être mieux appliquée sur des données contenant des groupes de densités différentes. La nouvelle métrique de distance entre chaque objet x i et chaque représentant c k est définie par :
où ? k représente la moyenne pondérée des distances dans un cluster k, définie par
ik , FCM-? minimise la fonction objective suivante :
La mise à jour des centres et la mise à jour des degrés d'appartenance aux différents clusters sont déterminées par :
L'évaluation de FCM-? sur des données de densités equilibrées montre que cette méthode donne les mêmes résultats que FCM (Tsai et Lin, 2011). Par contre, si les données contiennent des groupes de différentes densités, FCM-? est plus performant que FCM.
Méthodes Proposées
Dans les domaines d'applications de la classification recouvrante, l'algorithme d'apprentissage doit être capable de détecter des groupes non disjoints avec des densités uniformes et non uniformes. Par conséquent, nous proposons d'étendre OKM et Parameterized R-OKM pour détecter ces types de groupes en introduisant une régularisation de la densité dans la fonction objective de ces méthodes. Les nouvelles méthodes proposées sont désignées OKM-? et Parameterized R-OKM-?.
Overlapping k-means-? (OKM-?)
Pour tenir compte des différences de densité entre les classes, nous introduisons un facteur de régularisation ? i pour chaque observation x i . Compte tenu de l'ensemble des N observations, OKM-? minimise le critère objectif suivant :
i=1 où ? i est la valeur minimale des densités des groupes ? i auxquels l'observation x i appartient :
avec ? k le poids local au cluster k qui mesure le degré de déviation des observations contenues dans le cluster k par rapport à leur image respective. Ce poids peut être décrit formellement par :
où P ik une variable binaire indiquant l'appartenance de l'objet x i au cluster k.
Parameterized R-OKM-?
En se basant sur le même principe de régularisation de densité, nous proposons la mé-thode Parameterized R-OKM-? permettant la régularisation des variances de densité entre les classes. Le nouveau critère objectif proposé est décrit par :
i=1 où ? i le facteur de régularisation locale de l'observation x i décrit de la même manière que dans l'Equation (10) de OKM-?. Cependant, la nouvelle densité du groupe ? k est définie pour Parameterized R-OKM-? par :
Résolution algorithmique
La minimisation de la fonction objective de chaque méthode proposée (OKM-? and Parameterized R-OKM-?) est réalisée par itération de trois étapes indépendantes : (1) calcul de représentants de groupes C, (2) affectation multiple (?) d'observations à un ou à plusieurs groupes et (3) le calcul de poids (? k ) pour chaque classe.
Sachant que OKM-? est un cas particulier de Parameterized R-OKM-? (quand ? = 0), nous présentons dans l'algorithme 1 les différentes étapes de Parameterized R-OKM-?. Cet algorithme utilise la fonction ASSIGN ?? permettant l'affectation multiple de chaque observation à un ou plusieurs groupes. Cette fonction utilise une heuristique, utilisée aussi dans OKM et Parameterized R-OKM, permettant de réduire l'espace exponentiel (en terme de nombre de classes) des affectations possibles. Cette heuristique consiste à continuer l'affectation de l'observation au cluster le plus proche tant que le critère objectif est amélioré. Les différentes étapes de ASSIGN ?? sont décrites dans l'algorithme 2.    
Expérimentations
Cette section évalue l'efficacité de OKM-? et Parameterized R-OKM-? sur des ensembles de données artificielles. Afin d'évaluer la capacité des méthodes proposées à produire des groupes non-disjoints sur des ensembles de données ayant des densités différentes, nous avons généré deux jeux de données artificielles référencés "Ensemble 1" et "Ensemble 2". Le premier jeu de données contient deux classes où chaque classe est formée de 500 observations décrites dans un espace à deux dimensions. Les deux classes ont des densités différentes : la classe "bleue" a une grande densité par rapport à la classe "rouge". Pour le deuxième ensemble de données, nous avons modifié le rayon du groupe "rouge" qui devient plus grand.
Pour donner au lecteur une interprétation visuelle de la performance des méthodes proposées par rapport à OKM et Parameterized R-OKM, nous commençons par visualiser les classes obtenues dans un espace à deux dimensions. jeux de données artificielles. Ces résultats montrent que les méthodes proposées surpassent les méthodes originales en terme de F-mesure. Par exemple, en utilisant OKM-? sur "l'ensemble 2", la F-mesure obtenue augmente de 0.817 à 0.921 par rapport à OKM. L'amélioration de la F-mesure sur les deux jeux de données est due à l'amélioration de la Précision. Ce résultat s'explique par le fait que les méthodes originales construisent des recouvrements de plus en plus larges à mesure que la densité entre les deux classes diffère. Cependant, en utilisant les méthodes proposées, le taux de recouvrements est réduit.
Conclusion
Nous avons proposé dans ce travail deux nouvelles méthodes capables de produire des classes non-disjointes lorsque les données s'organisent en groupes de densités différentes. Ces nouvelles méthodes s'appuient sur une nouvelle mesure de distance qui régularise la variation de densité entre les classes obtenues. Des expériences réalisées sur des ensembles de données artificielles montrent l'efficacité des méthodes proposées par rapport à celles existantes.
L'évaluation des méthodes proposées est effectuée sur des ensembles de données artificielles. Comme perspectives, nous envisageons de confirmer ces résultats sur des jeux de données réels.

Introduction
Les réseaux complexes sont des graphes modélisant des systèmes réels. La structure de communautés (Fortunato, 2010) d'un réseau complexe est une partition de l'ensemble des noeuds, dont les parties (communautés) sont des groupes de noeuds densément interconnectés. Cette structure permet l'étude du réseau à un niveau intermédiaire, par comparaison avec les plus classiques niveaux local (voisinage du noeud) et global (réseau entier). Le rôle communautaire décrit ainsi la position d'un noeud dans le réseau à ce niveau. Il a été initialement introduit par Guimerà et Amaral (2005). Ces auteurs caractérisent le positionnement communautaire de chaque noeud au moyen de deux mesures topologiques ad hoc. Les noeuds sont ensuite caté-gorisés au moyen de seuils prédéfinis pour ces mesures. Cette approche peut être critiquée sur trois points. Premièrement, elle est définie seulement pour des réseaux non-orientés. Deuxiè-mement, les mesures utilisées ne prennent pas en compte tous les aspects de la connectivité communautaire d'un noeud. Troisièmement, rien ne garantit que les seuils fixés empiriquement pour définir les rôles soient pertinents pour d'autres données. Nous proposons des solutions à ces trois problèmes. Pour le premier, nous adaptons les mesures de Guimerà & Amaral aux réseaux orientés. Pour le deuxième, nous définissons des mesures supplémentaires distinguant trois aspects de la connectivité communautaire : diversité des communautés, hétérogénéité de la distribution des liens, et intensité de la connexion. Pour le troisième, nous proposons une méthode non-supervisée de définition des rôles. Afin d'illustrer l'intérêt de notre méthode, nous l'appliquons à l'étude du rôle communautaire d'un type particulier d'utilisateur de Twitter, appelé capitaliste social. Ces utilisateurs mettent en oeuvre deux principes simples pour accroître leur nombre de followers et donc leur visibilité. Follow Me, I Follow You (FMIFY) : le capitaliste promet aux utilisateurs qui le suivent de les suivre en retour. I Follow You, Follow Me (IFYFM) : le capitaliste suit un maximum d'utilisateurs, en espérant être suivi en retour.
Dans la section suivante, nous décrivons l'approche originale de Guimerà & Amaral. Nous mettons ensuite en évidence ses limitations et décrivons les trois modifications que nous proposons pour les résoudre. Dans la section 3, nous présentons les rôles obtenus sur le réseau Twitter et discutons du positionnement des capitalistes sociaux. Enfin, nous concluons en indiquant les perspectives ouvertes par ce travail.
Méthode proposée 2.1 Approche originale
Pour caractériser les rôles des noeuds, Guimerà & Amaral définissent deux mesures, qui leur permettent de placer chaque noeud dans un espace bidimensionnel. Puis, ils proposent plusieurs seuils pour discrétiser cet espace, chaque zone ainsi définie correspondant à un rôle. La première mesure, le degré intra-module traite de la connectivité interne du noeud, i.e. des liens avec sa propre communauté. Elle est basée sur la notion de z-score. Comme celle-ci sera réutilisée plus loin, nous la définissons ici de façon générique.
Équation 1 (Z-score). Pour une mesure nodale quelconque f (u), permettant d'associer une valeur numérique à un noeud u, le z-score Z f (u) par rapport à la communauté de u est :
dé-notent respectivement la moyenne et l'écart-type de f sur les noeuds de la communauté C i .
Le degré intra-module z(u) correspond au z-score du degré interne, calculé pour la communauté du noeud considéré. On l'obtient donc en substituant le degré interne d int à f dans l'équation (1). La seconde mesure, appelée coefficient de participation, traite de la connectivité externe du noeud, i.e. relative à toutes les communautés auxquelles il est lié. 
Orientation des liens
Aspects de la connectivité externe
Le coefficient de participation se concentre sur un aspect de la connectivité externe d'un noeud : l'hétérogénéité de la distribution de ses liens, relativement aux communautés auxquelles il est connecté. Mais il est possible de caractériser cette connectivité de deux autres manières. On peut considérer sa diversité, c'est à dire le nombre de communautés concernées, ainsi que son intensité, i.e. le nombre de liens concernés. Par souci de simplicité, nous présen-tons les mesures dans un contexte non-orienté. L'adaptation aux réseaux orientés peut se faire en distinguant les liens entrants et sortants, comme en section 2.2.
Diversité. La diversité D(u) évalue le nombre de communautés différentes auxquelles le noeud u est connecté. Soit le nombre de communautés, autres que la sienne, auxquelles u est connecté. D(u) est définie comme le z-score d' relativement à la communauté de u.
Intensité externe. L'intensité externe I ext (u) mesure la force de la connexion de u à des communautés externes, en termes de nombre de liens. Soit d ext (u) le degré externe de u, i.e. le nombre de liens que u possède avec des noeuds n'appartenant pas à sa communauté. I ext (u) est alors définie comme le z-score du degré externe. Hétérogénéité. L'hétérogénéité H(u) quantifie la variation du nombre de connexions externes de u d'une communauté à l'autre. Nous utilisons l'écart-type du nombre de liens externes que u possède par communauté, noté ?(u), et définission H(u) comme le z-score de ?, relativement à la communauté de u.
Intensité interne. Pour représenter la connectivité interne du noeud, nous conservons la mesure z de Guimerà & Amaral. Celle-ci est construite sur la base du z-score, et est donc cohérente avec les mesures définies pour décrire la connectivité externe. Cependant, par symé-trie avec notre intensité externe, nous désignons z sous le nom d'intensité interne, et la notons I int (u). Guimerà et Amaral (2005) supposent que les seuils sur les mesures établis de façons empiriques pour définir les rôles sont indépendants des jeux de données utilisés. Pourtant, seule P est normalisée sur un intervalle fixé. En effet, z n'est pas limitée, et donc rien ne garantit que le seuil défini pour cette mesure reste cohérent pour d'autres réseaux. Cet argument est d'autant plus fort que toutes nos mesures présentées en section 2.3 sont des z-scores. De plus, leur nombre élevé (8) rend l'utilisation de la typologie originale impossible. Afin de contourner ces problèmes, nous proposons d'appliquer une méthode de classification non supervisée. Dans un premier temps, nous calculons l'ensemble des mesures sur les données considérées. Ensuite, nous appliquons une analyse de regroupement. Chaque groupe ainsi identifié correspond a un rôle communautaire.
Identification non-supervisée des rôles
Résultats
Le réseau sur lequel nous avons travaillé comporte un peu moins de 55 millions de noeuds représentant les utilisateurs de Twitter, et près de 2 milliards d'arcs orientés qui matérialisent les abonnements entre utilisateurs. La détection de communautés a été réalisée au moyen de l'algorithme de Louvain (Blondel et al., 2008). L'analyse de regroupement a ensuite été menée au moyen d'une implémentation libre et distribuée de l'algorithme des k-moyennes (Liao, 2009). Nous avons appliqué cet algorithme pour des valeurs de k allant de 2 à 15, et avons sélectionné la meilleure partition d'après l'indice de Davies et Bouldin (1979). Pour valider les résultats obtenus, nous étudions les rôles détectés pour les capitalistes sociaux, identifiés via la méthode proposée par Dugué et Perez (2013). Nous distinguons différentes catégories de capitalistes sociaux en fonction de deux de leurs caractéristiques topologiques. La première est le ratio. Il s'agit du nombre de followees divisé par le nombre de followers. Ce critère permet de distinguer ceux qui appliquent la méthode FMIFY (ratio inférieur à 1) de ceux utilisant IFYFM (ratio supérieur à 1). La seconde est le degré entrant : nous séparons ceux de faible degré (entre 500 et 10000) et ceux de degré élevé (supérieur à 10000).
Calculs effectués
Nous avons d'abord appliqué l'approche originale (non-orientée) de Guimerà & Amaral sur nos données. Les valeurs de z obtenues sont bien supérieures à celles observées dans Guimerà et Amaral (2005). Le seuil défini pour z n'est ainsi plus utilisable pour l'identification des rôles. Nous avons donc procédé à une analyse de regroupement qui identifie 2 rôles, contenant chacun trop de noeuds pour obtenir une information pertinente. Nous avons ensuite appliqué l'approche originale orientée (section 2.2). L'analyse de regroupement a identifié 6 rôles, ce qui montre l'intérêt des mesures orientées. Néanmoins, lorsque l'on regarde le positionnement des capitalistes sociaux au sein de ces groupes, certaines incohérences apparaissent. Une large majorité des capitalistes sociaux de degré élevé est ainsi classée comme non-pivots périphé-riques ou ultra-périphériques. Ces noeuds ont pourtant un degré entrant supérieur à 10000. Si ces noeuds ne sont pas pivots, donc peu connectés en interne, ils devraient néanmoins être connectés avec l'extérieur. Cela vient des limitations de la participation, comme indiqué en section 2.3. Nous avons enfin appliqué le dernier groupe de 8 mesures, aboutissant aux résul-tats discutés dans le reste de cette section.
Étude des groupes
L'analyse de regroupement nous donne k = 6 groupes (Tableau 1). Nous caractérisons ces groupes relativement à nos huit mesures, afin d'en identifier les rôles. Dans les groupes 1, 4 et 5, presque toutes les mesures sont négatives et proches de 0. Il ne s'agit donc pas de pivots (noeud largement connecté à sa communauté) ni de de noeud qualifiés de connecteurs (ayant une connexion privilégiée avec d'autres communautés que la leur). La diversité entrante (resp. sortante) du groupe 4 (resp. 5) est l'unique mesure positive, ceci indique que ces noeuds reçoivent (resp. envoient) des liens d'un nombre relativement élevé de communautés et sont moins isolés. Toutes les mesures sont positives dans le groupe 6. L'intensité interne reste proche de 0, donc on ne peut toujours pas parler de pivot. L'intensité externe faible, mais positive, et la diversité élevée permettent de considérer ces noeuds comme des connecteurs. Toutes les mesures du groupe 3 sont largement positives L'intensité interne élevée associe ce groupe au rôle de pivot. Les valeurs externes montrent que ces noeuds sont connectés à de nombreux noeuds présents dans de nombreuses autres communautés. Toutefois, les liens sortants sont plus nombreux, ces noeuds correspondent donc à des utilisateurs plus suiveurs que suivis. Toutes les mesures du groupe 2 sont particulièrement élevées. Pour une mesure donnée, la variante concernant les liens entrants est toujours largement supérieure, ce qui signifie que les utilisateurs représentés par ces noeuds sont particulièrement suivis. Nous associons ce groupe au rôle de pivot orphelin.
Groupe
Taille 
Positionnement des capitalistes sociaux
Avec la méthode définie par Dugué et Perez (2013), nous détectons près de 160000 capitalistes sociaux. Nous étudions leur positionnement dans les 6 groupes identifiés. Les capitalistes sociaux de faible degré se retrouvent dans trois groupes : 3, 5 et 6. Les noeuds du groupe 3 sont des pivots connecteurs qui suivent plus d'utilisateurs du réseau que la normale, ce qui est cohérent avec le comportement des capitalistes sociaux. Il semble également cohérent d'observer que les capitalistes sociaux dont le degré sortant est supérieur au degré entrant sont près de deux fois plus présents dans ce groupe que les autres. La majorité des capitalistes sociaux de faible degré se place au sein du groupe 6, non-pivot connecteur. Ces noeuds légèrement plus connectés au sein de leur communauté et avec l'extérieur que la moyenne, ont en revanche une diversité bien plus élevée. Les capitalistes sociaux qui s'y situent semblent ainsi avoir débuté l'application de leurs méthodes, en créant des liens avec de nombreuses autres communautés. Les capitalistes sociaux de degré élevé se placent presque exclusivement dans les groupes 2 et 3, pivots connecteurs et orphelins. Cela semble cohérent avec les degrés élevés de ces noeuds.
Les noeuds classés dans le groupe 2 sont ceux de ratio inférieur à 0,7 avec beaucoup plus de followers que de followees, ce qui correspond à la définition du rôle donné par nos mesures. Notre approche établit une séparation entre capitalistes sociaux de faible degré, majoritairement connecteurs et non-pivots et ceux de degré élevé, classés pivots. 
Ratio
Perspectives
Le travail présenté peut s'étendre de différentes façons. Tout d'abord, certains des rôles définis dans Guimerà et Amaral (2005) n'apparaissent pas dans notre analyse. Il serait intéres-sant d'étudier d'autres réseaux afin de déterminer si cette observation reste valable. Une autre piste consiste à baser nos calculs sur des communautés recouvrantes (i.e. non-mutuellement exclusives). En effet, les réseaux sociaux que nous étudions sont réputés posséder ce type de structures, dans lesquelles un noeud peut appartenir à plusieurs communautés en même temps. L'adaptation de nos mesures à ce contexte se ferait naturellement, en définissant des versions internes de l'hétérogénéité et de la diversité.

Contexte et objectifs
Le modèle Linked Data vise à favoriser l'interopérabilité de sources de données hétéro-gènes sur le Web, en proposant un ensemble de bonnes pratiques de publication de données qui s'appuient sur les technologies du Web sémantique. En particulier, il préconise de décrire les données selon le modèle RDF, de fournir les vocabulaires utilisés par les données sous la forme d'ontologies et d'interconnecter les différents jeux de données disponibles en identifiant les ressources équivalentes d'un jeu de données à l'autre.
Parmi les données publiées sous la forme de Linked Data, de nombreuses ressources font référence, de façon directe ou indirecte, à une localisation. Dans DBpedia 1 , par exemple, 1373482 objets sont décrits par les propriétés geo:long et geo:lat. Geonames 2 contient plus de 8500000 entités spatiales nommées et environ 125000000 triplets RDF. La présence d'une information de localisation dans la description d'une ressource signifie que cette ressource est liée d'une manière ou d'une autre à une entité géographique du terrain réel. Deux ressources dotées d'informations de localisation identiques ou spatialement proches seront donc très susceptibles de présenter une relation sémantique. La prise en compte d'informations de localisation s'avère donc généralement importante pour l'interconnexion, et est le plus souvent réalisée au travers de mesures de distance géographiques entre les ressources à interconnecter. Cependant, les informations de localisation associées à des ressources Linked Data peuvent s'avérer très hétérogènes d'une source de données à l'autre, qu'il s'agisse d'hétérogénéités liées au type de localisation utilisé (directe via des coordonnées ou indirecte via une adresse par exemple), à l'origine de cette information, à sa précision ou au vocabulaire adopté pour la décrire. Cette hétérogénéité des informations de localisation peut rendre l'utilisation de mesures de distances géographiques pour l'interconnexion peu fiable voire impossible.
Dans cet article, nous proposons de mettre à profit des bases de données géographiques de référence pour l'interconnexion et l'exploitation de sources de données thématiques dotées d'informations de localisation hétérogènes. En effet, nous proposons d'ancrer les différentes ressources à interconnecter sur un référentiel géographique afin de favoriser la détection de relations d'interconnexion entre ces ressources. Parmi les applications découlant de cet ancrage de données thématiques à une base de données géographique de référence, la visualisation cartographique permet leur mise en valeur en fournissant aux utilisateurs un moyen convivial de découverte de ces données. Nous proposons donc une approche de visualisation cartographique des données thématiques ainsi interconnectées au référentiel géographique faisant porter les informations qu'elles renferment par les objets géographiques eux-mêmes et permettant leur visualisation à différentes échelles.
La section suivante dresse un état de l'art des approches proposées dans le domaine de l'interconnexion de données dotées d'informations de localisation, et dans le domaine de la visualisation cartographique de données Linked Data. La section 3 présente l'approche que nous proposons, pour l'ancrage de Linked Data sur un référentiel géographique et la visualisation des données. La section 4 décrit la mise en oeuvre de cette approche sur des données décrivant des monuments historiques. La section 5 conclut cet article.
Interconnexion et visualisation de données thématiques dotées d'informations de localisation 2.1 Utilisation d'informations de localisation pour l'interconnexion
L'interconnexion désigne l'étape du processus de publication de données sur le Web des données qui vise à créer des liens d'équivalence entre les ressources des différentes sources de données qui représentent la même entité du monde réel. Cette tâche est généralement fondée sur l'évaluation du degré de similarité entre ces ressources par comparaison de leurs proprié-tés, sous l'hypothèse que plus la similarité entre les descriptions des différentes ressources est importante, plus la probabilité qu'elles représentent la même réalité l'est aussi Ferraram et al. (2013). L'automatisation de cette tâche d'interconnexion s'appuie donc sur des approches et des mesures de similarité proposées dans des domaines ayant des besoins similaires d'identification de relations de correspondance, comme la réconciliation de références ou l'alignement d'ontologies Heath et Bizer (2011).
Différentes approches ont été proposées afin de détecter automatiquement des relations de correspondance entre objets issus de bases de données géographiques hétérogènes qui représentent une même entité topographique du monde réel. Tout comme dans le domaine de l'interconnexion de Linked Data, l'appariement de données géographiques repose sur l'éva-luation de la similarité des objets des différentes bases par comparaison de leurs propriétés. Le critère d'appariement privilégié est la géométrie des données qui représente la forme et la localisation des entités du monde réel via des primitives géométriques (point, ligne ou polygone). Différentes approches ont été proposées pour traiter chaque cas de figure : comparaison de données ponctuelles Minami (2000)   Samal et al. (2004). Dans chaque cas, la quantification du niveau de similarité entre les géométries est réalisée à l'aide d'une mesure spécifique. Des approches multicritères combinant critères géométriques, attributaires ou encore topologiques ont également été proposées Olteanu (2008).
A la croisée des deux domaines, des approches qui visent à trouver des équivalences entre des données Linked Data dotées d'informations de localisation ont également été proposées. Ces approches s'appuient sur les mesures de similarité entre géométries utilisées dans le domaine de l'appariement de données géographiques Salas et Harth (2011). Elles sont le plus souvent appliquées sur des données issues de bases de données géographiques représentées selon le modèle RDF et publiées sur le Web des données. L'approche proposée par Hahmann et Burghardt (2010) prend en considération des objets décrits par des géométries de type point et se fonde sur deux critères : le nom et la géométrie. La similarité entre deux points est estimée via un critère d'inclusion bilatérale d'un point candidat à l'appariement dans un rectangle englobant centré sur un point potentiellement homologue. L'approche proposée par Salas et Harth (2011) repose sur une projection cylindrique équidistante préalable des géométries, afin de pouvoir utiliser une mesure de distance de Hausdorff entre les géométries de type polygone que les auteurs souhaitent comparer pour détecter des cas de superposition entre géométries.
Visualisation cartographique des Linked Data
La plupart des approches de visualisation cartographique de données Linked Data visent à permettre l'affichage de données géographiques publiées selon le format RDF à l'aide d'outils de cartographie sur le Web. C'est le cas par exemple pour les données géographiques publiées par l'Ordnance Survey 
Approche proposée
Les approches d'interconnexion de données Linked Data dotées d'informations de localisation proposées s'appuient sur des mesures permettant de comparer des informations de localisation du même type, qu'il s'agisse de calculer des distances entre géométries ou d'éva-luer la similarité de chaînes de caractères représentant des adresses postales ou des toponymes. Dans le cas de ressources présentant des informations de localisation de différentes natures, il est donc nécessaire de procéder au géoréférencement des informations disponibles afin de disposer de coordonnées géographiques pour chaque source de données à comparer. De plus, dans le cas d'informations de localisation du même type, des différences de précision ou d'origine des données peuvent engendrer des valeurs de distance géographique très importantes entre les ressources à comparer, alors que leurs géométries se rapportent bien à une même entité topographique du monde réel. C'est pourquoi, nous proposons de mettre à profit des bases de données géographiques de référence comme ressources de support pour l'interconnexion de sources de données thématiques dotées d'informations de localisation hétérogènes. Suivant l'approche proposée par Aleksovski et al. (2006) dans le domaine de l'alignement d'ontologies, nous proposons d'ancrer les différentes ressources à interconnecter sur un ré-férentiel géographique afin de favoriser la détection de relations d'interconnexion entre ces ressources. La figure 1 illustre cette approche sur un exemple d'interconnexion de ressources localisées dans le premier cas, par des coordonnées géographiques et dans le second, par des adresses. Le géocodage via la base de données d'adresses permet de disposer de coordonnées géographiques pour la seconde ressource. En revanche, ces coordonnées demeurent trop éloi-gnées de celles de la première ressource pour permettre une interconnexion. Le fait d'ancrer les deux ressources sur le bâtiment auquel elles se rapportent permet finalement de les mettre en correspondance.
L'étape d'ancrage des sources d'informations thématiques au référentiel géographique né-cessite de détecter des liens de correspondance entre ressources thématiques et objets géogra-phiques auxquels elles se rapportent. Dans certains cas, il ne s'agira pas nécessairement de liens d'équivalence. Cependant, dans la mesure où les indications de localisation fournies avec les données thématiques font référence à des points de l'espace proches des objets géographiques auxquels on devra les ancrer, les approches proposées pour l'interconnexion de données dotées d'informations de localisation pourront être mises à profit lors de cette étape. Nous proposons donc pour cette étape d'ancrage de nous appuyer sur les outils d'interconnexion de données existants, et de les compléter à l'aide d'approches proposées dans le domaine de l'appariement de données géographiques.
Notre approche doit permettre la transformation des données géographiques dans le modèle RDF, afin de permettre leur traitement via un outil d'interconnexion de Linked Data disposant Celle-ci permet la transformation selon le modèle RDF de données représentées selon diffé-rents modèles, y compris des données géographiques sous format GML ou SHP (format vectoriel Shapefile). Les monuments historiques décrits par la base Mérimée sont localisés à l'aide d'adresses postales multiples. Nous avons donc dû procéder, après l'étape de conversion, à un nettoyage de ces données à l'aide de requêtes de type CONSTRUCT, afin de pouvoir traiter chaque adresse de façon individuelle lors de l'interconnexion. Dans le cas de la BD TOPO R nous avons converti les données sur les bâtiments de Paris. Chaque bâtiment est décrit par une géométrie de type polygone. Dans le cas de la BD ADRESSE R il s'agit de données décrivant des adresses de façon structurée, et associant à chaque adresse une géométrie de type point.
L'ensemble des données est ensuite stocké dans un triple store local (OpenRDF Sesame) pour faciliter l'étape d'interconnexion. 
Mesure de distance entre géométries vectorielles
Les indications de localisations généralement utilisées dans les sources de données Linked Data, sont le plus souvent des adresses, des géométries de type " point " ou des coordonnées de type longitude et latitude. Les géométries représentant les objets géographiques dans les bases de données géographiques décrivent la réalité d'une manière plus détaillée : outre des points, on rencontre fréquemment des lignes ou des polygones. La mesure de distance que nous avons choisie d'utiliser pour ancrer des données Linked Data sur le référentiel géographique calcule donc la distance minimale entre un point et une géométrie de type quelconque, ou retourne une valeur nulle si le point est inclus dans cette géométrie. Les coordonnées des géométries peuvent être exprimées dans différents systèmes de coordonnées. Nous commençons donc par 11. https ://www.assembla.com/spaces/silk/wiki/Home projeter toutes les coordonnées dans le même système de projection. Ainsi, nous pouvons faire reposer le calcul de notre mesure de distance sur le mesure de la distance euclidienne minimale entre deux objets de la bibliothèque Java Geotools 12 , et l'intégrer sous forme de plugin dans l'outil d'interconnexion Silk ( (3)  figure 2).
Paramétrage de l'interconnexion
FIG. 3 -Interconnexions réalisées.
La première étape de notre démarche d'interconnexion est l'ancrage des données sur les monuments historiques issues de DBpedia sur les bâtiments de la base BD TOPO R ((1) figure 3). Dans ce cas, le seul critère d'interconnexion pouvant être utilisé est la localisation. Nous avons donc créé et exécuté un script Silk utilisant notre mesure de distance entre les positions ponctuelles de monuments de DBPedia et les polygones décrivant les bâtiments dans la BD TOPO R Nous avons choisi un seuil maximal de 40 mètres, défini en nous appuyant sur le précision planimétrique connue de la BD TOPO R (10 mètres) et l'estimation empirique de la précision des indications de localisation dans les données DBPedia.Dans le cas des monuments de la base Mérimée, localisés par des adresses, nous avons tout d'abord procédé au géocodage des données. Nous avons donc réalisé une interconnexion entre les indications de localisation des monuments dans la base Mérimée et la base BD ADRESSE R Pour ce faire, nous avons combiné les attributs qui constituent les adresses textuelles dans les deux bases et les avons comparés via une mesure de Levenshtein modifiée pour devenir insensible au désordre des mots. Ainsi, nous avons pu affecter à chaque monument interconnecté les coordonnées géographiques correspondant à son ou ses adresse(s) ( (2)  figure 3). Nous avons ensuite pu procéder à l'interconnexion des monuments Mérimée dotés de coordonnées avec les bâtiments BD TOPO R en réutilisant l'approche adoptée pour les monuments DBPedia ((3) figure 3). Enfin, nous avons procédé à une dernière interconnexion ((4) figure 3) entre les monuments issus de Mérimée et les monuments de Dbpedia qui ne disposaient pas d'informations de localisation fiables. Pour ce faire, nous avons utilisé les noms attribués aux monuments comme critère d'interconnexion, et les avons comparés en nous servant de la distance de Levenshtein modifiée disponible dans Silk.
12. http ://www.geotools.org/
Résultats et évaluation
Pour les deux sources de données, sur les 2462 resources récupérées initialement, nous avons pu en interconnecter 1653 avec des bâtiments BD TOPO R Ce résultat s'explique en partie par l'étape préalable de géocodage que nous avons dû appliquer aux données Mérimée. Lors de cette étape, nous avions paramétré Silk de sorte à obtenir le plus possible d'interconnexions justes, au risque de ne pouvoir géocoder certaines données Mérimée faute d'interconnexions. Pour cette étape, nous avons comparé les liens générés avec un mapping de référence créé manuellement sur la zone du premier arrondissement de Paris. Cette comparaison a révélé une précision de 100% et un rappel de 90,35%. Cette précision peut se justifier par le fait que les liens comparés ne concernent que le 1er arrondissement. Le rappel s'explique en partie par le paramétrage choisi et par la présence dans la base Mérimée d'indications de localisation non conformes à des adresses comme " face au 14 " ou " dans le cimetière ".
FIG. 4 -Interconnexion du Palais de la porte dorée avec les deux sources de données.
Nous n'avons pu procéder à une évaluation chiffrée des résultats d'interconnexion avec la BD TOPO R faute de mapping de référence. Nous avons donc procédé par validation visuelle effectuée par une seule personne sur une interface cartographique. Ainsi, nous avons pu constater que l'approche d'ancrage sur un référentiel géographique permettait bien de mettre en correspondance des ressources qui n'auraient pas pu l'être par comparaison directe, en raison de localisations trop éloignées ou de dénominations différentes. C'est le cas pour " le palais de porte dorée " nommé ainsi dans DBpedia, et intitulé " Ancien Musée National des Arts Africains et Océaniens, devenu Cité nationale de l'histoire de l'Immigration" dans la base Mérimée avec une distance supérieure à 60m entre les deux localisations (figure 4).
Les interconnexions ont été réalisées sur un ordinateur bureautique sous Ubuntu12.04 LTS avec 3,8 Go de RAM et un processeur intel Core i5 2.40GHz*4, en utilisant la variante «Single Machine» de Silk. Nous avons utilisé les options de blocking disponibles pour optimiser le temps de calcul : chacune des interconnexions réalisées a pris moins de 3h30. Pour améliorer les temps de calcul d'interconnexion ou traiter plus de données, nous aurions pu utiliser la variante "Slik MapReduce". Cependant la validation demeure manuelle ; il est donc impossible pour l'instant de passer à l'échelle pour cette étape. . La première permet l'affichage, sous forme de couches, de données stockées sur des serveurs géographiques et envoyées conformément aux protocoles décrits par les spécifications de l'OGC (Open Géospatial Consortium). La seconde nous fournit des fonds cartographiques et orthophotographiques produits par l'IGN.
OpenLayers permet également de créer d'autres couches vectorielles à partir de données récupérées par différents protocoles, notamment le protocole HTTP. Elle permet donc d'interroger un point d'accès SPARQL. De plus, elle offre la possibilité de mettre en forme les données ainsi récupérées avant de les visualiser. Ainsi, on peut créer des couches de données vectorielles en utilisant les géométries des données géographiques de référence que nous avons préalablement converties et stockées en RDF pour l'étape d'interconnexion. De plus, on peut mettre à profit les liens d'ancrage entre ressources externes et données géographiques pour récupérer des informations thématiques sur ces ressources externes. La figure 6 présente un exemple de co-visualisation des données géographiques de référence et des ressources théma-tiques ancrées sur ces données. Les bâtiments interconnectés avec des monuments historiques de la base Mérimée sont affichés dans des tons de bleu, la valeur du bleu variant selon le siècle de construction de monument en question fourni par la base Mérimée. Les bâtiments interconnectés avec des monuments historiques DBPedia sont dotés de contours noirs. Ceux interconnectés avec les deux sources sont à la fois en bleu et avec un contour noir. C'est le cas par exemple du bâtiment qui héberge la bourse du commerce de Paris. 
Visualisation à différentes échelles
Afin de fournir un affichage cartographique lisible à différentes échelles, on a généralement recours à des techniques dites de généralisation. La généralisation est un processus de synthèse d'information, qu'on peut comparer à un processus de résumé de texte qui réduit le nombre de mots mais garde les idées principales Ruas (2002). Elle vise à simplifier l'information en passant d'un niveau d'échelle à un autre en gardant le même sens pour la carte. Dans notre cas, nous avons eu recours à une approche de simplification fondée sur une classification sous contrainte de contigüité spatiale Carvalho et al. (2009).
FIG. 7 -Visualisation des îlots et des îlots agrégés à différentes échelles
Cette méthode consiste à unir les objets voisins qui présentent des propriétés similaires. Nous regroupons les objets en fonction du siècle de construction des monuments historiques. Cette technique ne peut pas être appliquée directement sur les bâtiments, compte tenu de la non-contigüité entre ces derniers. Nous avons donc appliqué cette méthode sur les îlots surfaciques créés à partir du réseau routier, en leur affectant le siècle de construction le plus fréquemment rencontré parmi les bâtiments correspondant à des monuments classés qu'ils contiennent (partie gauche figure 7). Ceci nous a permis de générer des représentations plus agrégées pour un affichage à petite échelle (partie droite figure7).
Conclusion et perspectives
Dans cet article, nous avons proposé une approche visant à tirer parti des informations de localisation associées à des ressources Linked Data dans un processus d'interconnexion. Cette approche consiste à interconnecter les ressources non pas directement entre elles, mais avec un référentiel géographique afin de pallier les difficultés d'interconnexion liées à l'hétérogé-néité des indications de localisation. Les interconnexions générées entre données thématiques et données géographiques ont été ensuite mise à profit dans une application de visualisation cartographique des données à différentes échelles reliant les informations thématiques directement aux objets géographiques. Les liens d'ancrages générés entre données thématiques et données géographiques sont des relations d'équivalence. Pour plus de cohérence sémantique, nous envisageons d'étudier les différents types de liens sémantiques qui peuvent exister entre une ressource thématique et un objet géographique.

Introduction
L'activité d'un réseau social en ligne, d'un réseau de télécommunication, de requêtes sur un moteur de recherche, ou encore d'un système de paiement par carte bancaire, peut être modélisé par un grand réseau d'interactions ; chaque interaction peut en effet être vue comme un lien entre deux entités, apparaissant au déclenchement de l'interaction. Le réseau est alors représenté par un flot de liens ordonnés chronologiquement.
Malgré la diversité des systèmes modélisables par des flots de liens, la dynamique de ces flots de liens reste peu explorée. La dynamique des graphes étant elle-même un sujet d'étude scientifique récent, il a fallu attendre ces dernières années et la publication de grands jeux de données à ce niveau de précision pour que l'étude des flots de liens puisse émerger. La plupart des graphes dynamiques étudiés jusqu'ici sont en effet constitués d'instantanés capturés à une certaine fréquence (ex. un graphe par jour). L'enjeu principal est de caractériser l'évolution de ces systèmes pour mieux les comprendre et en particulier différencier une dynamique normale de comportements anormaux. Pour un analyste surveillant un système et levant des alertes en cas d'anomalies, comme une fraude à la carte bancaire ou une intrusion dans un intranet, il est crucial de pouvoir identifier et valider de tels événements rapidement. En d'autres termes, il s'agit de déterminer où et quand la structure du réseau d'interactions est anormalement altérée.
Dans cet article nous proposons une méthode d'investigation à la fois statistique et visuelle. Celle-ci repose sur une méthode de fouille de données statistique pour détecter des événements significatifs, suivie de visualisations interactives pour les valider et les interpréter.
État de l'art
La plupart des méthodes actuelles de fouille de graphes dynamiques (i.e. qui évoluent dans le temps) reposent sur des séries de graphes statiques représentant soit l'état du graphe à l'instant de chaque capture, soit l'agrégation des noeuds et liens apparus entre cet instant et l'instant de la précédente capture (par exemple le graphe des interactions du mois d'avril dans un réseau social). Ces graphes sont communément représentés par des diagrammes noeudsliens, que l'on trouve dans la majorité des systèmes de visualisation scientifique tels que Visone (Brandes et Wagner (2004)), SoNIA (Moody et al. (2005)), Gephi (Bastian et al. (2009)), ViENA (Windhager et al. (2011)), et GraphDiaries (Bach et al. (2012)).
Quatre stratégies émergent pour la détection visuelle d'événements dans l'évolution d'un graphe : 1) le morphing est une correspondance de entre deux instants montrant l'animation du diagramme noeuds-liens (Ghani et al. (2012)) comme une vidéo (dont la position des noeuds peut varier ou non) via une chronologie ou un slider (Moody et al. (2005)), voir Gephi, NodeXL, SoNIA, TempoVis. Cette approche est intuitive mais l'analyse est difficile si la densité de liens est élevée et le nombre de changements important (Brandes et al. (2012);Archambault et al. (2011)). 2) la comparaison de couches montre l'évolution du graphe sur une vue unique via une série de small multiples représentant l'état du graphe à différents instants (Archambault et al. (2011)). Ces small multiples sont intégrables à une chronologie (Bach et al. (2012)), voir GraphDiaries. Cette approche permet de comparer rapidement les différences, mais se limite à de petits graphes. 3) la fusion de couches intègre deux graphes statiques en un seul. Cette représentation "deux-en-un" distingue les deux instants du graphe par des couleurs et effets de style. Elle amplifie les différences structurelles, mais se limite à des graphes de faible densité (Krempel (2005)), voir ViENA. 4) la vue en 2.5D utilise la troisième dimension pour afficher les changements du graphe, mais son efficacité n'est pas démontrée.
Visualiser l'ensemble du graphe dynamique entraîne de nombreux problèmes, notamment l'occlusion, le croisement des liens dans des graphes de forte densité, et la difficulté à pré-server la carte mentale (Archambault et al. (2011)). Des métaphores et approches orientées pixels (matrices, codes barre (Albano et al. (2011))) contournent ces problèmes mais restent marginales. Pourtant, voir l'ensemble du graphe n'est pas toujours nécessaire ni souhaitable, en particulier dans un flot de plusieurs millions de liens. La visualisation d'un sous-graphe bien choisi (grâce à une méthode statistique par exemple) permet de détecter des événements pertinents (Sun et al. (2007);Chau (2012)). Les approches existantes sont cependant basées sur des séries de graphes statiques, et ne sont pas adaptées aux flots de liens à cause des biais induits par les changements d'échelle (Clauset et Eagle (2007)).
Contribution et organisation du papier
Nous proposons une méthode pour valider, par la visualisation, les événements statistiquement significatifs détectés par une méthode de fouille de données. On identifie ainsi où et quand un événement statistiquement significatif apparait dans le réseau. Nous décrivons notre méthode en Section 2, l'appliquons sur l'étude d'un grand réseau social en ligne en Section 3, et concluons en Section 4.
Méthode
Notre méthode se compose des deux étapes suivantes.
1. Identifier automatiquement des changements statistiquement significatifs dans la structure du graphe sur une fenêtre de temps glissante, en fonction d'une métrique de graphe. Cette étape permet de traiter l'ensemble du graphe rapidement.
2. Valider la pertinence de chaque événement détecté en identifiant visuellement un motif anormal dans le sous-graphe correspondant à la fenêtre temporelle anormale, puis en vérifiant que ce motif est unique au cours du temps.
Nous posons les définitions nécessaires dans la section suivante.
Définitions
Aucune définition concernant les flots de liens n'étant à notre connaissance disponible dans la littérature, nous posons les définitions suivantes. Flot de liens : soit F = {f 0 , f 1 , ..., f m } le multi-ensemble ordonné de triplets f i =< n x , n y , t i >, avec la relation d'ordre f i ? f i+1 si et seulement si t i ? t t+1 , où i ? N, t i ? R. F est appelé flot de liens. Chaque triplet représente un lien entre deux noeuds n x , n y observé à une date t i à la position i dans F . 
Détection d'événements statistiquement significatifs
Nous appliquons la méthode Outskewer (Heymann et al. (2012)), une méthode de fouille de données non supervisée applicable à un échantillon de valeurs univarié. Cette méthode considère comme anormale une valeur extrême qui rend le coefficient d'asymétrie 1 de la distribution de valeurs éloigné de zéro (i.e. symétrie parfaite de la distribution de valeurs). Outskewer peut détecter de multiples valeurs anormales dans un échantillon, et peut être étendu à l'analyse d'une série temporelle (potentiellement en temps réel). Soit X = {x 0 , x 1 , ..., x n } une série temporelle. On considère le multi-ensemble de w o valeurs X i = {x i?wo+1 , ..., x i }, où w o est appelée la taille de la fenêtre temporelle de détection d'événements. Chaque valeur
. Nous utilisons Outskewer sur tous ces w o multi-ensembles, et calculons le nombre de fois où x i est classée comme normale, probablement anormale, ou anormale. La classe finale est celle ayant obtenu le score le plus grand. Nous utilisons X = S pour appliquer Outskewer à une statistique de graphe. Nous posons la définition suivante.
Événement : ensemble consécutif de valeurs {x i , x i+1 , ..., x j }, i ? j classées comme anormales dans X. Par commodité, nous nous référons aux événements par la valeur i par la suite.
Analyse visuelle des événements
Cette étape a pour objectif de valider les événements détectés par Outskewer, et de les interpréter en les corrélant à des motifs anormaux dans la structure des sous-graphes G i w correspondants. Par exemple, la soudaine et fréquente répétition de l'apparition d'un lien contribue à diminuer brusquement le nombre de noeuds uniques observés via une fenêtre temporelle à cet instant. Cet événement peut être détecté automatiquement mais peut avoir plusieurs origines comme nous le verrons.
Nous proposons une visualisation interactive permettant d'investiguer ces événements. Nous faisons l'hypothèse que l'utilisateur sait lire et interpréter un diagramme noeuds-liens. Notre prototype doit respecter les contraintes suivantes : a) représenter la structure du graphe par un diagramme noeuds-liens ; b) ne pas afficher tout le graphe, qui peut compter des millions de noeuds et liens, ce qui rendrait la visualisation illisible ; c) tenir compte de la possibilité que les événements détectés par Outskewer soient très espacés des uns des autres dans le temps.
Pour investiguer un événement (cf. section 2.5), l'utilisateur extrait le sous-graphe G i w et y identifier un ou plusieurs motifs suspects selon ses critères. Il désambiguïse enfin l'interpréta-tion de l'événement : il détermine si le motif anormal apparaît seulement lors de l'événement. Si c'est le cas, alors il le considère comme corrélé à l'événement.
Description du prototype
Notre prototype étend les fonctionnalités du logiciel commercial Linkurious 2 , réalisé en HTML5 et Javascript pour s'exécuter dans un navigateur Web. Il se connecte à une base de données de graphe Neo4j 3 . L'interface se compose des quatre espaces suivants : 4) Les panneaux du bas sont la nouveauté apportée à Linkurious. Ils permettent de sélec-tionner un événement i dans une liste déroulante, de préciser une taille de fenêtre temporelle w (qui peut être différente de w), et d'afficher le sous-graphe correspondant G i w . Un curseur permet de décaler la position de la fenêtre un peu avant ou un peu après la date de l'événement (jusqu'à 10 000 liens en amont ou en aval) pour observer l'évolution du graphe autour de l'évé-nement. Un second diagramme représente la totalité du flot de liens F permettant à l'utilisateur de déplacer la fenêtre temporelle à n'importe quelle date via un curseur. Un bouton permet de colorer le diagramme en fonction du nombre de liens apparaissant entre les noeuds visibles à l'écran à chaque pourcentage de la largeur du diagramme : de gris (pas d'apparition) à vert clair (le maximum d'apparitions). Si un seul noeud est visible, alors la couleur correspond au nombre de liens ayant ce noeud pour extrémité. Ce diagramme fournit des indications sur la fréquence des apparitions des liens entre noeuds d'un motif durant toute la durée de l'évolu-tion du graphe. Ce diagramme est compacte (10 pixels par 500 pixels) pour répondre à une contrainte de hauteur, mais la précision d'affichage et de positionnement du curseur dépend de sa largeur l et du nombre de liens m. Une largeur de 1 pixel représente en effet m/l liens. Une fonction de zoom serait une amélioration possible pour gagner en précision. 2) Identification d'un motif anormal : L'analyste sélectionne un événement i dans la liste dé-roulante des événements. Le sous-graphe G i w s'affiche ; il peut diminuer la taille de la fenêtre temporelle pour l'affichage (ainsi réduire la taille du sous-graphe) ou conserver la taille utilisée lors de la détection d'événements (w). Il observe le diagramme noeuds-liens à la recherche de motifs suspects (au regard de la statistique utilisée par Outskewer), comme une étoile, une composante connexe, ou un lien très redondant. S'il trouve un motif suspect, il explore l'évo-lution du graphe autour de la date de l'événement. Si ce motif reste visible avant ou après cette date, alors il est évident qu'il n'est pas corrélé à l'événement. L'analyste doit alors chercher un autre motif, ou considérer que l'événement détecté par Outskewer n'est pas valide. 3) Interprétation de l'événement : Une fois un motif suspect identifié, l'analyste cherche à déterminer s'il est exceptionnel ou s'il est récurrent. Cette étape permet de tester la corrélation de motifs pour un événement. Comme illustré dans la section suivante, l'analyste sélectionne les noeuds du motif et cache les autres. Il applique alors la coloration de la barre du flot de liens, et voit en un coup d'oeil si les liens entre ces noeuds apparaissent à d'autres moments (périodes en vert). Si ce n'est pas le cas, alors le motif entre ces noeuds est unique et corrélé à l'événement. Sinon il déplace le curseur sur chaque période d'apparition pour voir la forme des relations entre ces noeuds. Lors de cette opération la position des noeuds est stable, ce qui permet de comparer plusieurs versions du motif au cours du temps en réalisant un export de l'image du graphe (via la barre d'outils). Si le motif est redondant, alors l'analyste ne peut pas affirmer qu'il soit corrélé à l'événement, bien qu'il puisse être anormal si nous ne considérons que le graphe visible. Dans ce cas l'événement est ambigu, et une analyse approfondie est né-cessaire. Au contraire, si le motif apparaît uniquement lors de l'événement, alors il est corrélé à cet événement ; l'analyste peut valider l'événement en interprétant le contenu de ce motif. 
Validation expérimentale des événements
Application
Jeu de données Github
Détection automatique d'événements
Nous illustrons l'intérêt de notre approche sur une statistique basique : le nombre de noeuds uniques S , et nous remarquons que le compte a par la suite été renommé "mapserver-bot". Le but de ce robot a été de migrer le code source et la liste de bogues du projet "mapserver" de Trac vers Github. Nous trouvons des traces de la discussion lancée le 19 mars 2012 sur la mailing-list publique 8 . L'événement correspond donc à la date de migration du code source ou de la liste de bogues sur Github.com : le bug n°1 correspond en effet au début de l'événement le 3 avril 2012 à 17 :37 :55 (t i=413000 ). 6. https://github.com/mapserver/mapserver 7. https://github.com/mapserver-bot?tab=activity 8. https://lists.osgeo.org/pipermail/mapserver-dev/2012-March/012100.html ligne Github. Des travaux préliminaires ont montré l'intérêt de représenter ces données comme une succession d'apparition de liens dans un graphe dynamique, que nous formalisons par un flot de liens ordonnés chronologiquement. Peu de jeux de données de graphes dynamiques possèdent une telle granularité pour un volume de plusieurs millions de liens, ce qui constitue une opportunité pour développer des méthodes de détection d'événements adaptées.
Nous avons combiné une méthode statistique, Outskewer, avec un système de visualisation. Outskewer détecte des événements statistiquement significatifs selon une statistique du graphe, et donne des instants de l'évolution du graphe à investiguer. Ces événements n'étant pas tous pertinents, nous avons proposé une étape de validation supplémentaire par la visualisation. Cette dernière montre un sous-graphe correspondant au moment des événements et facilite le suivi longitudinal des motifs anormaux présents dans ce sous-graphe, ce qui permet de localiser des motifs anormaux à ces instants, et de vérifier s'ils semblent corrélés à l'évé-nement. Nous avons illustré notre méthode sur les événements à investiguer, et montrons par plusieurs exemples que nous détectons des événements pertinents, et rejetons des événements proposés par Outskewer pour lesquels nous ne trouvons pas d'anomalie dans le graphe. Nous avons ainsi illustré la complémentarité des statistiques et de la visualisation pour détecter des événements dans un grand flot de liens.
Dans des travaux futurs nous proposerons une évaluation du système avec un analyste, expert de son domaine, pour en évaluer l'approche et en critiquer l'IHM. Une comparaison avec des méthodes alternatives est aussi souhaitable. Enfin, nous tenterons de généraliser notre méthode par plusieurs cas réels comme la détection de fraudes ou la détection d'intrusions.
Une telle méthode sans a priori sur les données ni sur les événements est idéale lors d'une démarche exploratoire. La visualisation, en particulier, offre une grande flexibilité quant à la nature des motifs anormaux à observer. Une fois une typologie des événements identifiée en fonction de la tâche d'investigation, il est envisageable d'automatiser entièrement la détection et la validation des événements en replaçant la visualisation par des statistiques sur les motifs. 

Introduction
L'expansion de la requête est une approche souvent utilisée en recherche d'information pour aider le modèle de recherche à mieux identifier les documents pertinents. Le succès de cette technique dépend du bon choix des termes d'expansion et la façon d'ajouter ces nouveaux termes à la requête. D'un côté, si les nouveaux mots clés apportés par l'approche d'expansion ne sont pas pertinents pour le besoin d'information, la possibilité de trouver des documents pertinents baisse, ce qui a un effet négatif sur le rappel et la précision. Pour cette raison, le contrôle de la qualité des termes d'expansion est une étape indispensable de l'expansion de la requête. D'un autre côté, un bon choix des termes d'expansion ne suffit pas pour garantir le succès de l'expansion si ces termes n'ont pas été intégrés correctement dans la requête. Dans cet article, nous nous concentrons sur les entités nommées, c'est à dire les termes qui représentent des personnes, des lieux ou des événements. Ces termes, riches en information, ont motivé plusieurs études dans des domaines liés au Traitement du Langage Naturel. Ces études s'intéressaient à reconnaitre les entités nommées dans un texte (Guo et al., 2009), à les désambiguïser (Hoffart et al., 2011), ou à les classifier (Nadeau et Sekine, 2007). Dans le domaine de la recherche d'information, les entités nommées sont souvent utilisées pour l'annotation (Kiryakov et al., 2004), l'indexation (Buizza, 2011) ou la recherche (Petkova et Croft, 2007). Des travaux récents (Guo et al., 2009) ont remarqué l'importance des entités nommées pour les requêtes des utilisateurs, car ils ont constaté que plus de 70% des requêtes web contiennent au moins une entité nommée. En revanche, ces travaux s'intéressaient plutôt à la classification des entités nommées de la requête sans considérer l'effet de ces éléments sur la performance du modèle de recherche. Dans notre article, nous étudions l'expansion des entités nommées dans la requête, notamment par différentes méthodes de reformulation. Notre idée est construite sur trois éléments : premièrement, l'utilisation des entités nommées dans les requêtes est fréquente et utile (Guo et al., 2009;Kiryakov et al., 2004). En deuxième lieu, certaines études ont constaté que les requêtes web contiennent souvent une seule entité nommée (Guo et al., 2009). La troisième élément est l'avantage potentiel de l'expansion de la requête qui peut améliorer significativement la performance d'un modèle de recherche. Nous commençons en section 2 par une présentation des études précédentes sur l'expansion sémantique des requêtes, avec un zoom sur le cas des entités nommées. En section 3 nous présentons notre approche de l'expansion sémantique des entités nommées dont nous détaillons les différents étapes. La section 4 est consacrée à l'explication et l'évaluation de nos expériences.
État de l'art
Le but de l'expansion de la requête est de trouver des nouveaux documents pertinents (i.e. améliorer le rappel), et de tirer les documents pertinents déjà trouvés par la requête initiale vers le haut de la liste des résultats (i.e. améliorer la précision). Nous pouvons diviser les approches d'expansion de requête en deux catégories : la première catégorie regroupe les approches qui dépendent de la collection de documents. Ces approches peuvent être locales, comme les méthodes de retour de pertinence, ou globales. La deuxième catégorie contient les méthodes fondées sur l'utilisation d'une ressource externe comme une ontologie. Traditionnellement, pour ces deux catégories, les entités nommées sont souvent traitées comme les autres termes de la requête. Les approches fondées sur la collection de documents utilisent des calculs statistiques sur les termes dans les documents sans un traitement spécifique concernant les entités nommées. Par ailleurs, les méthodes fondées sur une ressource externe sont confrontées à la difficulté de traitement des entités nommées surtout quand la ressource externe décrit mal ces éléments (Navigli et Velardi, 2003). Par exemple, l'utilisation de WordNet pour l'expansion de la requête n'a pas beaucoup de succès (Mandala et al., 1998) : l'une des raisons est que les entités nommées dans cette ressource sont souvent manquantes, pas à jour, ou n'ont pas (ou très peu) de synonymes dans leurs Synsets 1 . Ainsi, WordNet n'est pas suffisant pour désambiguïser ou étendre les requêtes qui contiennent des entités nommées. Malgré ces difficultés liées à l'expansion de la requête, d'autres études en recherche d'information confirment l'importances des entités nommés dans les requêtes. Par exemple, certaines recherches sur les requêtes longues considèrent qu'une sous requête contenant une entité nommée est un bon candidat qui doit être considéré pour la reformulation (Huston et Croft, 2010;Kumaran et Carvalho, 2009). D'autres études, comme (Maxwell et Croft, 2013), proposent un algorithme pour classer des groupes nominaux identifiés dans la requête afin de les utiliser pour construire une nouvelle requête. Récemment, la disponibilité de ressources riches et ouvertes comme Wikipedia, a permis certains travaux explicitement dédiés à l'étude de l'expansion des entités nommées. Par exemple, Xu et al. (2008) ont utilisé Wikipedia pour extraire des termes qui sont sémantiquement proches des termes de la requête, alors que Brandao et al. (2011) ont étudiés des approches basées sur les pages et les Infobox de Wikipedia pour retrouver des expansions des entités nommées. Comme Xu et al. (2008) et Brandao et al. (2011, notre travail s'intéresse aussi à l'expansion des entités nommés, par contre, nous faisons le point sur les méthodes d'intégration des termes d'expansion dans la requête, ce qui n'était pas abordé dans les travaux précédents. De plus, nous étudions l'effet du rôle des entités nommées sur l'expansion.
L'expansion sémantique des entités nommées
Dans cette section nous présentons notre approche d'expansion de la requête fondée sur les entités nommées. Nous précisons dans les sous-sections suivantes les étapes de notre approche en considérant le cas des requêtes Web et l'ontologie générique YAGO (Suchanek et Weikum, 2007) que l'on utilise à la fois pour la désambiguïsation et pour le choix des termes d'expansion. Cette ontologie est une combinaison de WordNet et des catégories de Wikipedia. Elle a l'avantage d'être une ressource riche en entités nommées, liées entre elles par une grande variété de relations sémantiques (Hoffart et al., 2011).
La désambiguïsation
La désambïguisation est un problème complexe, largement abordé en Traitement du Langage Naturel (Navigli, 2009). Alors que ce problème est implicitepment résolu par les mé-thodes locales d'expansion de la requête, il reste un vrai défi pour les approches fondées sur l'utilisation d'une ressource externe. D'un autre côté, la désambiguïsation des entités nommées est un domaine relativement récent. À notre connaissance, la première méthode qui a traité ce sujet a été proposée en 2006 : elle utilisait Wikipedia en combinaison avec une approche d'apprentissage supervisé pour la désambïguisation des entités nommées (Bunescu et Pasca, 2006). Depuis, Wikipedia est devenu une ressource importante et à jour des entités nommées. Il faut noter également que dans le cas des requêtes web, la tâche de désambiguïsation est compliquée, car ces requêtes sont souvent très courtes, et contiennent donc peu d'éléments de contexte. Dans cet article, nous utilisons l'approche de désambiguïsation proposée par (Hoffart et al., 2011). Cette approche utilise l'outil de Stanford NER (Named Entity Recognition) pour identifier les entités nommées dans une requête, puis elle applique une combinaison de trois techniques de désambiguïsation : le sens le plus utilisé (popularity prior), la similarité (syntaxe et surface commune) entre une entité nommée et le concept correspondant dans l'ontologie, et enfin le graphe de cohérence entre les concepts. Quand il n'y a pas de contexte, l'algorithme va choisir la référence la plus courante pour l'entité à désambiguïser. Ce comportement est cohé-rent avec un scénario de recherche sur le Web, où on peut considérer que si l'utilisateur ne met
Topic
Entité Nommée Variations sémantiques 515
Alexander Graham Bell "Alexander Gram Bell", "Aleck Bell", "The father of the deaf" 517 Titanic "Jinx Titanic" 478 Baltimore "Baltimore City", "City of Baltimore", Baltamore, Bmore, "Baltimore riots", Baltimoreans, "Charm city" Mobtown, "Charm City"
TAB. 1 -Exemples de variations sémantiques trouvés dans YAGO pour des entités nommées.
que l'entité nommée dans sa requête, il souhaite alors orienter sa recherche vers la référence la plus courante de cette entité.
Le choix des termes
Une fois choisie la référence unique de l'entité nommée dans Yago, la sélection des termes d'expansion peut être effectuée. Ce choix dépend de la relation sémantique qu'on souhaite utiliser. Dans l'ontologie Yago, les relations sémantiques qui lient un concept à son entourage sont nombreuses 2 . Ces relations dépendent de la nature de chaque concept : par exemple une ville peut être reliée à sa surface ou à son nombre d'habitants, alors que d'autres types de relations sont utilisés dans le cas d'une personne (sa date de naissance par exemple). Dans le cas de l'expansion de la requête, le choix de la bonne relation sémantique n'est pas une tâche facile, elle sera le sujet de prochaines études. Pour cet article nous considérons la relation "Label" qui existe pour tous les concepts. L'avantage de cette relation est qu'elle lie une entité nommée à ses différentes appellations que l'on va appeler des synonymes pour simplifier. Ces synonymes peuvent être des alternatives orthographiques du terme (Baltimore-Baltamore), ou complètement différents au niveau de la syntaxe mais sémantiquement identiques au terme original (Baltimore-Mobtown). Le tableau 1 présente trois exemples d'entités nommées de type différent (personne, objet et lieu) avec leurs synonymes. Dans ce tableau, on constate également qu'un synonyme peut être une autre entité nommée ("Aleck Bell") ou un groupe nominal ("The father of the deaf").
La reformulation de la requête
La plupart des modèles de recherche considère la requête utilisateur comme un sac de mots, que l'on modifie avec l'expansion de la requête en ajoutant de nouveaux termes avec ou sans pondération. Selon le modèle de recherche d'information utilisé, des alternatives au sac de mots peuvent être considérées pour représenter la requête. Dans notre travail nous utilisons le modèle de query likelyhood (Strohman et al., 2004) qui est le modèle par défaut de l'outil de recherche d'information Indri est de calculer les croyances pour chaque noeud dans le réseau, puis de combiner ces croyances selon l'opérateur utilisé. Les équations 1 et 2 présentent comment Indri combine les croyances (b i ) pour les opérateurs #combine et #weight que nous utilisons dans notre approche.
Le troisième opérateur que nous utilisons #syn est un opérateur virtuel, dans le sens où il n'est pas associé à une équation de combinaison. Son rôle est de signaler au modèle de recherche qu'il ne faut pas distinguer les occurrences de chacun des termes donnés dans la liste de synonymes. Le langage de requête d'Indri permet également, avec les opérateurs #N et #uwN (unordered window), d'exprimer le nombre maximum de mots autorisés entre des termes dans les documents du corpus pour qu'une occurrence soit comptabilisé 4 . Nous expliquerons par la suite les trois approches que l'on utilise pour l'intégration des nouveaux termes dans la requête avec ces opérateurs. Pour simplifier la compréhension de ces approches, nous utilisons la requête démonstrative suivante (issue de TREC n?455n?455 après la suppression des mots vides) :
Jackie Robinson appear first game que l'on considère comme la requête de base sans expansion.
Sac De Mots (SDM)
Dans le cadre de l'expansion de la requête, l'utilisation du principe "sac de mots" signifie l'ajout de nouveaux termes à la requête originale, ce qui peut être codé par l'opérateur #combine. Ainsi, l'expansion de notre exemple démonstratif pourrait donner la requête suivante :
#combine(Jackie Robinson appear first game #1(Jackie Robinson) #1(Jack Roosevelt Robinson)) Néanmoins, dans cette requête étendue tous les éléments constitutifs du #combine seront interprétés indépendamment les uns par rapport aux autres, et donc avec une même probabilité. Dans notre cas, il faudrait que #1(Jackie Robinson) et #1(Jack Roosevelt Robinson) aient, l'un ou l'autre, la même probabilité que chacun des autres termes de la requête étendue. L'opérateur #syn nous permet de le faire avec cette formulation : #combine(Jackie Robinson appear first game #syn( #1(Jackie Robinson) #1(Jack Roosevelt Robinson)))
La Dépendance Séquentielle (DS)
La dépendance séquentielle a été proposée par Metzler et Croft (2005). L'idée d'origine a été fondée sur la reformulation des requêtes longues en trois parties pondérées : la première partie contient la requête originale sous la forme d'un sac de mots, cette partie a le poids le plus important. La deuxième partie combine des fenêtres de taille 1 pour chaque paire de termes successifs. La troisième partie contient les mêmes paires de termes mais avec une taille de fenêtre égale à 4. Dans (Maxwell et Croft, 2013), les auteurs ont adapté cette approche en remplaçant les paires de termes par des groupes nominaux qu'ils choisissent à partir de la requête initiale 5 en utilisant leur algorithme PhraseRank. Notre expansion de requête s'inspire de ces travaux. Nous reformulons les requêtes initiales en trois parties, en utilisant les mêmes poids que dans (Metzler et Croft, 2005) et (Maxwell et Croft, 2013). Nous suivons aussi l'approche de (Maxwell et Croft, 2013) pour calculer la taille des fenêtres des expressions dans la dernière partie :
-requête initiale (coefficient de pondération = 0.85) -extension avec les synonymes (issus de YAGO) dans des fenêtres de taille 1 (coefficient de pondération = 0.1) -extension avec les même synonymes dans des fenêtres d'une taille équivalant à quatre fois le nombre de mots du groupe nominal (coefficient de pondération = 0.05) Ce qui donne la requête suivante pour notre exemple : #weight( 0.85 #combine(Jackie Robinson appear first game) 0.10 #combine(#syn(#1(Jackie Robinson) #1(Jack Roosevelt Robinson) appear first game) 0.05 #combine(#syn(#uw8(Jackie Robinson) #uw12(Jack Roosevelt Robinson) appear first game))))
Le Concept Clé (CC)
La troisième approche, fondée sur le principe de concept clé de (Bendersky et Croft, 2008), contient deux parties. La première partie est la requête originale sous forme d'un sac de mots, elle a le poids le plus important comme c'est le cas dans l'approche de dépendance séquentielle. La deuxième partie contient des groupes nominaux choisis dans la requête. Dans (Bendersky et Croft, 2008), ces groupes nominaux sont pondérés à l'intérieur de cette deuxième partie en fonction du modèle probabiliste qui les a classés, mais comme dans (Maxwell et Croft, 2013), nous n'utilisons pas cette pondération. Nous obtenons alors : Les poids (0.8, 0.2) sont ceux de (Bendersky et Croft, 2008;Maxwell et Croft, 2013).
L'algorithme final
L'Algorithme 1 illustre les principales étapes de notre expansion sémantique des entités nommées.
Tout d'abord, nous commençons par initialiser la requête originale de l'utilisateur en enlevant les mots vides pour limiter le bruit (ligne 2). Ensuite, le processus de désambiguisation de Hoffart et al. (2011) identifie les entités nommées et choisit un unique sens pour chacune d'entre elles (ligne 3). En ligne 5, nous utilisons YAGO pour obtenir des termes d'expansions pour chaque entité nommée. Ces candidats à l'expansion passent ensuite par deux filtres : le premier (ligne 6) enlève les termes bruités (ceux qui contiennent moins de 3 caractères), et le deuxième assure qu'un terme d'expansion n'existe pas déjà dans la requête (ligne 7). Avec cette liste filtrée de termes d'expansion, la procédure de reformulation est finalement appelée (ligne 8) avec comme paramètre l'approche que l'on souhaite utiliser pour la reformulation (SDM, DS, ou CC).
Évaluation
Alors que la majorité des approches de reformulation ou d'expansion de requête s'intéresse plutôt à la précision en mesurant le MAP, R-Prec ou P@x (Maxwell et Croft, 2013;Carpineto et Romano, 2012;Bendersky et Croft, 2008;Petkova et Croft, 2007), dans notre travail nous considérons de plus le rappel pour obtenir une meilleure caractérisation de nos résultats. Pour cela, nous utilisons deux groupes de mesures : les mesures de précision (MAP, P@10, P@20, R-Prec), et les mesures de rappel (R@30, R@200, R@dernierRang). Les tests statistiques que nous utilisons sont "T-test" et "Randomization test" comme recommandé par (Smucker et al., 2007). Par notre évaluation nous souhaitons répondre aux questions suivantes : Quelle est l'effet des différentes méthodes d'intégration de nouveaux termes (SDM, DS, ou CC) ? Notre approche améliore-t-elle les résultats du modèle de référence sans expansion ? En comparant avec une approche traditionnelle d'expansion de la requête, où se situe notre approche ? Nous faisons nos expériences sur deux collections web : WT10G nommée (tableau 2). Ce choix a été fait manuellement pour garantir que les requêtes de test contiennent au moins une entité nommée. Pour l'indexation et la recherche on utilise Indri5.5 sans modifier les paramètres par défaut.
Expériences
Dans le tableau 3, nous constatons que l'intégration des termes d'expansion dans la requête a un effet très important sur les résultats. Nous observons que le fait d'intégrer les termes d'expansion en mode sac de mots (sans pondération) dans la requête dégrade toutes les mesures de performance. Alors qu'avec les approches DS et CC, la précision et le rappel ont bien été améliorés. Ces améliorations sont même statistiquement significatives pour les mesures RPrec, R@30 et R@200 pour la collection Wt10g. Pour la collection ClueWeb09, les approches DS et CC améliorent toutes les mesures par rapport à la base. Les tests de significativité de ces améliorations sont plus souvent positifs pour les métriques de précision. En plus de comparer notre approche au modèle de référence sans expansion, il est intéressant de le comparer également à une autre approche d'expansion. Dans cette étude, nous comparons nos résultats à ceux obtenus en utilisant une méthode de retour de pertinence aveugle (Pseudo Relevance Feedback PRF) souvent utilisée comme une approche de base dans les études sur l'expansion de la requête. Pour cela, le tableau 3 propose également les résultats de l'approche d'expansion PRF par défaut de Indri sur la collection Wt10G, où nous avons fixé à 10 le nombre de documents de retour de pertinence et à 10 le nombre de termes d'expansion. En observant ces résultats, nous remarquons que PRF a dépassé notre approche en MAP, alors que pour le rappel, plus on avance dans les rangs des documents trouvés plus notre approche obtient un meilleur rappel par rapport à PRF. D'ailleurs, si on regarde le rappel au dernier rang, PRF diminue même le rappel du modèle de base sans expansion. Cela signifie que PRF a perdu des documents pertinents par rapport à la base sans expansion alors que notre approche en a trouvé des nouveaux.
Finalement, nous nous intéressons à comprendre si le fait que l'entité nommée est l'objet principal de la requête ou non a un effet sur les résultats après son expansion. Pour cela, nous divisons (manuellement) les 26 requêtes de l'expérience WT10G en deux groupes : le groupe A signifie que l'entité nommée soit le sujet de base dans la requête 8 (ex."Alexander Graham Bell"), dans le groupe B nous mettons les cas où l'entité nommée n'est pas le sujet de base, c'est-à-dire que si on enlève les autres termes de la requête cette dernière n'a plus le même sens (ex. Mexican food culture TAB. 4 -Nombre de requêtes (WT10G) améliorées (+) ou dégradées (-) par l'expansion sé-mantique des entités nommées en utilisant l'option CC de la reformulation pour chaque rôle (A et B).
de l'entité nommée a peu d'effet sur le MAP quand ces entités sont étendues. Alors que pour le rappel, on constate que pour le groupe A, toutes les requêtes ont obtenu un R@dernierRang supérieur ou égal à celui de l'approche sans expansion, mais pour le groupe B, une requête a une chance sur deux d'améliorer ce rappel une fois étendue.
Discussion
Les résultats précédents montrent le potentiel de l'expansion sémantique des entités nommées. Bien que l'ajout de nouveaux termes par l'approche SDM ait un effet négatif sur la performance, les approches DS et CC réussissent à améliorer significativement plusieurs mesures d'évaluations. En revanche, il faut prendre en compte que nous n'avons utilisé aucune pondération des termes, alors que souvent la plupart des méthodes d'expansion travaillent essentiellement sur ce sujet surtout avec le modèle SDM. Notre observation concernant les meilleurs résultats de l'approche PRF par rapport à notre approche au niveau de la précision, signifie que l'utilisation des documents de retour de pertinence aide à modifier le classement des documents de façon à tirer les documents pertinents vers le haut de la liste. En revanche, il est intéressant de savoir que sans avoir accès à ces documents riches en information, et malgré le peu de texte dans les requêtes de test, l'expansion des entités nommées a réussi à trouver plus de documents pertinents que PRF, ce dernier ayant même perdu des documents pertinents trouvés par le modèle sans expansion. Finalement, concernant les deux catégories d'entités nommées (A et B), nous constatons une amélioration plus stable au niveau du rappel quand l'entité nommée est le sujet principal de la requête. Cette observation semble logique, car l'ajout de synonymes pour ce genre d'entités est probablement utile pour trouver plus de documents pertinents, surtout que dans ce cas il y a peu d'ambiguïté si on suppose que l'utilisateur pense au sens le plus courant quand il construit une requête qui ne contient que l'entité nommée.

Introduction
Le domaine médical, comme d'autres domaines de spécialité, est caractérisé par l'hétérogé-néité de ses acteurs et utilisateurs. Mentionnons par exemple les médecins, les patients, les infirmiers, les pharmaciens, les internes, les brancardiers, les administratifs, les aides soignants, les chercheurs, les biologistes qui interagissent quotidiennement dans la pratique médicale. Il est évident que tous ces acteurs jouent des rôles différents et, de la même manière, les besoins de ces acteurs, y compris les besoins en information, sont aussi différents. Par exemple, un mé-decin recherche typiquement des informations précises qui lui permettent de faire un diagnostic ou des prescriptions appropriées, un chercheur est souvent à la recherche des derniers travaux dans un domaine bien ciblé, alors qu'un patient peut rechercher des informations plus ou moins générales afin de retrouver des explications sur une maladie ou un traitement. De manière plus générale, nous pouvons différencier les cas suivants (Pearson, 1998) : les informations créées par des spécialistes pour les spécialistes (le cas des médecins ou des chercheurs), les informations créées par des spécialistes pour les non spécialistes (le cas des patients), les informations créées par des non spécialistes pour les non spécialistes. Les textes correspondant à chaque cas ont des propriétés et fonctions différentes. De même, ils véhiculent des informations dont le niveau de spécialisation varie et qui peuvent nécessiter une expertise plus ou moins importante pour une compréhension correcte. Pour un système de recherche d'information, il peut donc être important de pouvoir distinguer entre les différents types de textes et, de cette manière, de proposer une caractérisation supplémentaire de ces textes, en en distinguant par exemple le niveau de spécialisation. Cet objectif, distinction entre les textes véhiculant différents niveaux de spécialisation, correspond à la motivation principale de notre étude.
Parmi les travaux existants, nous pouvons par exemple citer ceux relatifs à la personnalisation de la recherche d'information (Pasi, 2010). Parmi les méthodes proposées, afin d'adapter les résultats de recherche à un utilisateurs, certaines exploitent le filtrage collectif (Herlocker et al., 2004), le filtrage basé sur le contenu (Kassab et Lamirel, 2006), le filtrage par la combinaison des deux (Basilico et Hofmann, 2004), le filtrage par la modélisation de l'utilisateur (Hadjouni Krir, 2012). Nous proposons d'aborder la question par l'analyse du contenu des documents. Nous proposons d'exploiter la particularité des documents, qui concerne la subjectivité des acteurs telle qu'elle transparaît à travers leurs incertitudes et émotions. Plus particulièrement, nous proposons d'exploiter les informations relatives à l'emploi de l'incertitude (i.e., possible, il semblerait, certain), de la polarité (i.e., absence, pas de, ni), et des émotions (i.e., un lexique spécifique comme peur ou colère, la ponctuation répétée et intensifiée comme !!!, les émoticônes comme :-), les mots avec des caractères répétés comme maaaaal). Nous utilisons également des modifieurs lexicaux (i.e., très, beaucoup). Notre hypothèse est que les différents types de documents médicaux présentent des spécificités liées à la subjectivité, qui peuvent être utilisées pour une distinction automatique entre ces types de documents.
Dans la suite de ce travail, nous décrivons d'abord le matériel utilisé (section 2), ensuite la méthode (section 3). Nous présentons et discutons les résultats produits (section 4), et terminons avec des perspectives (section 5).
Collecte et préparation du matériel
Nous utilisons deux types de matériel : corpus (section 2.1) et ressources (section 2.2).
Corpus
Les données étudiées portent sur le thème de la rhumatologie (maladies des os, des articulations ou des muscles). Elles sont constituées de trois corpus, tous collectés en mai 2013 :
-le corpus scientifique contient des articles scientifiques rédigés par des médecins ou des chercheurs. Ce corpus est constitué à partir du portail médical CISMeF . Les message sont rédigés essentiellement par des patients. Les données source (pdf, doc, html, texte...) sont normalisées au format texte et converties en utf-8, en faisaint attention aux ligatures (e.g. oe ? oe, ae ? ae, fl ? fl), aux accents mal convertis (e.g. oˆ?oˆ? ô, i¨?i¨? ï, e´?e´? é), à la suppression des caractères non imprimables (e.g. retour à la ligne, tabulation verticale). La taille des corpus est indiquée dans le tableau 1 : les documents du corpus scientifique sont les plus longs, viennent ensuite les fils de discussion du forum et les documents cliniques. Les corpus sont échantillonnés, pour assurer leur comparabilité. Le nivellement est effectué par rapport au corpus scientifique, le plus petit de l'ensemble. 
Scient
Ressources
Une partie importante des ressources est dédiée à la détection de la subjectivité et des émotions (section 2.2.1). D'autres ressources sont spécifiques au domaine médical (section 2.2.2). Nous décrivons également comment les ressources sont ajustées (section 2.2.3).
Détection de la subjectivité et des émotions dans les corpus
Les ressources linguistiques exploitées contiennent plusieurs types de marqueurs : -L'incertitude (n=101) peut être exprimée aussi bien avec des verbes (i.e., supposer, apparaître, suspecter), des noms (i.e., possibilité, hypothèse), des adjectifs (i.e., vraisemblable, douteux), qu'avec des adverbes (i.e., sûrement, peut-être). Deux degrés d'incertitude sont distingués : l'incertitude forte, qui influence fortement la fiabilité des informations (i.e., douteux, évocateur, hypothèse), et l'incertitude faible, qui influence faiblement la fiabilité des informations (i.e., apparemment, certain, probablement) ; -La négation (n=20) peut être exprimée de différentes façons également : avec des adverbes (i.e., ne, pas), des noms (i.e., absence, lacune), des adjectifs (i.e., négatif, impossible), des prépositions (i.e., sans) ou encore avec le préfixe non-; -Les modifieurs (n=17) du degré d'incertitude comme peu, très peu, fort peu, extrême-ment, vraiment. Leur interprétation dépend de la polarité du terme de base : par rapport à probable, très probable conduit à une diminution de l'incertitude, tandis que très douteux, par rapport à douteux seul, conduit à une augmentation de l'incertitude ; -Un lexique des émotions (Augustyn et al., 2008) contient 1 144 entrées (verbes, noms et adjectifs). Les entrées du lexique sont associées à plus d'une trentaine d'emotions (e.g. tristesse, dégoût, joie, honte). Ceci correspond à une catégorisation plus fine que celles habituellement utilisées dans les méthodes semi-automatiques (Ekman, 1992). Pour arriver à un niveau de généralisation, les émotions sont catégorisées en trois catégories : émotions positives, négatives et neutres. Par exemple, tristesse, dégoût et honte sont des émotions négatives, joie positive, anticipation, étonnement et surprise neutres.
Détection de notions médicales dans les corpus
Les ressources sémantiques se composent de termes appartenant à trois types sémantiques : maladie (maladies, problèmes médicaux ou troubles), procédure (actes médicaux effectués par les médecins) et médicament. Les maladies et les procédures proviennent de la terminologie médicale SNOMED international (Côté, 1996) ; (2) l'UMLS (Lindberg et al., 1993), ou Unified Medical Language System, une collection de terminologies biomédicales dévelop-pées par la US National Library of Medicine ; (3) la base UCD (Unité commune de dispensation) couvrant les médicaments qui disposent d'une autorisation de mise sur le marché et sont commercialisés en France. Nous avons au total 71 449 entrées dans la catégorie maladie, 25 148 dans la catégorie procédure, et 17 571 dans la catégorie médicament.
Ajustement des ressources
Les ressources exploitées doivent être ajustées aux données traitées. Comme c'est souvent le cas avec des ressources constituées dans d'autres cadres, nous cherchons par exemple, à les rendre plus précises et/ou plus exhaustives.
Rendre les ressources plus précises. Certaines entrées peuvent avoir un sens différent dans les corpus par rapport à ce qui est prévu dans les ressources. Ceci concerne surtout le lexique des émotions. Par exemple, dans ce lexique, les mots comme irriter et irritation sont assignés à la catégorie de l'émotion colère, tendu à la catégorie attirance, manque à la catégorie colère. Cependant, dans un corpus médical, dans des expressions comme :
(1) La seule différence constatée réside dans la réponse à l'intensité de l'irritation, provoquant une extension progressive de la douleur... (2) Elever la jambe tendue jusqu'à apparition d'une douleur radiculaire (3) Les guidelines ... s'accordent sur le manque de preuve pour recommander des interventions préventives pour la lombalgie aiguë.
les entrées irritation, tendu ou manque ne signifient pas les émotions prévues dans le lexique, mais reçoivent un sens propre au domaine médical. Après le nettoyage du lexique, effectué afin de réduire le bruit lors de l'annotation, nous gardons 1 032 entrées.
La situation est similaire avec la terminologie, dont l'objectif est d'assurer l'exhaustivité des notions recensées et dont certaines peuvent être ambiguës dans certains contextes. Par exemple, les entrées comme PDF, THE, CI, base, élément, solution s'avèrent trop ambiguës. Au total, 50 entrées de la terminologie SNOMED International ne sont pas considérées.
Rendre les ressources plus exhaustives. D'un autre côté, il existe aussi des cas où des unités linguistiques, potentiellement intéressantes pour l'annotation, sont absentes des ressources. Ceci peut être dû (1) à la spécificité des corpus, comme par exemple avec le corpus du forum, (2) aux limites de la chaîne d'annotation, ou (3) à l'incomplétude des ressources, malgré la recherche de l'exhaustivité. Pour y remedier, nous effectuons plusieurs traitements :
-Pour chaque entrée simple de la terminologie, ne se terminant pas par s ou x, susceptible de marquer le pluriel, nous générons la forme plurielle correspondante. Nous obtenons un total de 6 924 nouvelles entrées, dont le type sémantique (maladie, procédure ou médicament) est identique à celui de l'entrée source. Parmi les nouvelles entrées, nous avons par exemple {achalasies, achalasie} ou {acholuries, acholurie}, appartenant à la catégorie des maladies. Ceci permet de répondre en partie aux limites de la chaîne de traitement (section 3.1.1), qui parfois n'est pas adaptée aux données médicales et qui peut être fautive dans la reconnaissance des lemmes ; -Dans les corpus de forum, la difficulté principale est différente : elle est liée à la présence fréquente des mots mal orthographiés. Nous faisons l'adaptation suivante : Annotation linguistique. L'annotation linguistique est effectuée avec l'étiqueteur morphosyntaxique TreeTagger (Schmid, 1994), qui assure la segmentation des documents en mots, la catégorisation des mots selon leurs catégories syntaxiques (i.e., alimentations est un nom, saignent un verbe), et leur lemmatisation (i.e., alimentations est lemmatisé vers alimentation, saignent vers saigner).
Annotation sémantique. L'annotation sémantique consiste en repérage des termes et de divers marqueurs des ressources (incertitude, négation, modifieurs, émotions). Pour chaque entrée des ressources, détectée par la plateforme Ogmios, le type sémantique correspondant lui est associé. L'annotation sémantique est effectuée avec les formes et les lemmes. En supplé-ment des ressources, nous détectons également des marques émotives non lexicales, particulièrement fréquentes dans le corpus du forum :
-smileys ou émoticônes : comme par exemple =), ;-), :-/, XD ; -marques de rire : comme lol, mdr, haha, hihi ; -ponctuations expressives : comme !!!??, !!!!!!!!!! ; -mots avec lettres répétées : comme maaaaaaal, grrrrrr, nooooooon ; Chaque marque émotive non lexicale est typée selon qu'elle dénote une émotion positive (i.e., =), mdr, looool), négative (i.e., :-(, :-/ ) ou neutre (i.e., ???!!?, grrrrrrrrrr, ohhhhh). La détec-tion de ces marques est réalisée avant la tokenisation de façon à s'assurer que chaque marque est considérée comme un seul token par TreeTagger. Dans le cas contraire, TreeTagger fait par exemple de !!! une suite de trois tokens.
Bilan des annotations. Grâce aux différentes ressources et annotations, nous obtenons un jeu de plusieurs types sémantiques : -incertitude, négation et modifieurs ; -émotions (lexicales et non lexicales) : positives, négatives et neutres ; -notions médicales : maladies, médicaments et procédures ; -les mêmes notions médicales mais dont les unités comportent des erreurs d'orthographe. Évaluation de l'annotation. L'évaluation des annotations concerne les termes médicaux, les marqueurs de la négation et de l'incertitude, et les marques émotives non lexicales. Pour chaque corpus, 500 annotations sont contrôlées selon trois critères :
-détection : est-ce qu'une entrée donnée est détectée ? Si oui, est-ce qu'elle est contextuellement correcte ou incorrecte ? -typage : pour chaque entrée détectée, reçoit-elle un type sémantique ? Si oui, est-ce que ce type est correct ou incorrect ? -lemmatisation : est-ce que la lemmatisation de chaque entrée est correcte ou incorrecte ? La précision (proportion d'annotations correctes par rapport à toutes les annotations effectuées) est calculée de deux manières :
-précision stricte P s : on considère les vrais positifs comme étant uniquement des tokens pour lesquels tous les paramètres sont corrects (détection, typage et lemmatisation) ; -précision lâche P l : la notion de vrais positifs est élargie aux tokens dont la détection est correcte, alors que le type peut être absent (dû à un défaut de format de sortie) ou le lemme incorrect (dû à la difficulté de TreeTagger à traiter les termes médicaux).
Catégorisation automatique
La catégorisation automatique selon le degré de spécialisation des documents est effectuée avec une approche par apprentissage supervisé. En apprentissage supervisé, le système automatique a besoin d'exemples annotés en fonction des catégories visées (corpus d'apprentissage) pour pouvoir construire un modèle de classification. Ce modèle peut être appliqué à des nouvelles données, et le système peut faire des prédictions sur leur catégorisation. Nous utilisons divers algorithmes d'apprentissage supervisé implémenté dans la plate-forme Weka (Witten et Frank, 2005), et dont nous gardons le paramétrage par défaut.
Catégories à reconnaître
Nous cherchons à distinguer trois catégories : scientifiques, cliniques et forum. En rapport avec notre hypothèse, les utilisateurs de forum laissent libre cours à leurs émotions et jugements subjectifs, tandis que les documents scientifiques et cliniques doivent montrer plus de détachement et d'objectivité. Nous nous attendons donc à ce que les documents du corpus forum soient beaucoup plus faciles à discriminer que les documents des deux autres corpus. Nous effectuons une catégorisation bicatégorie en testant les trois couples possibles de caté-gories : documents cliniques et des forum, documents cliniques et scientifiques, documents de forum et scientifiques. Nous effectuons aussi un test multicatégorie, où les trois catégories sont à discriminer en même temps. Les corpus (tableau 1) sont nivellés par rapport aux corpus scientifique, avec 265 documents dans chaque catégorie. -freq correspond à la fréquence brute du descripteur ; -norm correspond à la normalisation de la fréquence par le nombre de mots du document ; -tfidf correspond à la pondération de la fréquence brute par tfidf (term frequency*inverse document frequency) (Salton, 1991) : f req * log( tot nbdoc ), où f req est la fréquence du descripteur, tot le nombre de documents dans le corpus et nbdoc le nombre de documents où ce descripteur apparaît. Cette mesure permet d'évaluer l'importance du descripteur par rapport au corpus. Le poids augmente proportionnellement à la fréquence du descripteur dans le document. Nous avons au total 46 descripteurs.
Descripteurs utilisés et leur pondération
Évaluation de la catégorisation
Nous effectuons une validation croisée (Sebastiani, 2002), qui permet aux algorithmes d'utiliser deux ensembles distincts de données pour les étapes d'entraînement et de validation. La validation croisée à n plis est effectuée n fois sur des partitions de données différentes et le résultat global correspond à la moyenne des performances. Nous effectuons une validation croisée à 10 plis. Les mesures d'évaluation correspondent aux moyennes de toutes les itérations. Nous calculons les mesures d'évaluation standard : précision (pourcentage de documents correctement catégorisés parmi tous les documents assignés à une catégorie), rappel (pourcentage de documents correctement catégorisés par rapport aux documents qui doivent être assignés à une catégorie) et f-mesure (leur moyenne harmonique).
La baseline correspond à l'assignation des documents dans la catégorie par défaut. Typiquement, pour un test avec deux catégories (e.g. forum et clinique) et un nombre de documents égal dans chaque catégorie, une telle baseline produirait une précision de 50 % : tous les documents sont donc assignés à une seule catégorie. Par rapport à une telle baseline, nous calculons aussi le gain obtenu, qui correspond à l'amélioration effective de la performance P par rapport à la baseline BL (Rittman, 2008) :
4 Présentation et discussion des résultats
Annotations et leur évaluation
La méthode et les ressources sémantiques ont été exploitées pour effectuer une annotation sémantique des documents. Dans la figure 1, nous présentons les fréquences de différents types sémantiques dans les trois corpus. Nous pouvons par exemple voir que les notions médicales sont plus fréquentes dans les corpus scientifique et clinique ( figure 1(a)), tandis que les émo-tions, incertitudes et négations sont plus fréquentes dans le corpus forum (figures 1(b) et 1(c)). Parmi les émotions les plus fréquentes, nous observons par exemple peur, joie, tristesse, attirance et colère dans le corpus forum, joie (i.e., plaisir, rassuré, satisfaisant), peur (i.e., inquiet, anxieux, crainte, souci) et tristesse (i.e., désolé, effondrement, destabilisé) dans le corpus clinique, et doute, étonnement, peur (i.e., appréhender, menacer, craindre) et joie (i.e., plaisir, heureux, rassuré) dans le corpus scientifique.
Les résultats d'évaluation des annotations sont présentés dans le tableau 2, en termes des précisions stricte et lâche. Nous pouvons voir que la précision stricte, tout type sémantique confondu, est supérieure à 80 %, tandis qu'avec la précision lâche nous gagnons 10 %. Cela veut dire que dans 90 % de cas, les entités sont correctement reconnues, bien que leurs lemmes ou leurs types sémantiques peuvent être mal détectés.
Catégorisation automatique
Dans le tableau 3, nous présentons les performances de la catégorisation automatique des documents. Les résultats indiqués sont obtenus avec l'algorithme RandomForest (Breiman,   -dans le discours scientifique, la subjectivité a été bien étudiée (Hyland, 1995;Light et al., 2004). Il apparaît qu'elle peut avoir plusieurs rôles : entre autres, elle devient incontournable car elle permet à l'auteur de se positionner par rapport aux travaux d'autres chercheurs ou par rapport à ses propres résultats expérimentaux obtenus lors des expé-riences scientifiques. Là aussi, la subjectivité sert de moyen de précaution, de distance et de protection ; -dans les messages de forums, la subjectivité est exprimée de façon beaucoup plus géné-rale (très souvent sans relation avec les notions médicales par exemple). De plus, elle montre souvent un lien fort avec les émotions des patients.
Conclusion et Perspectives
Dans le domaine médical, où il existe plusieurs types d'acteurs et d'utilisateurs avec des besoins informationnels souvent différents, nous proposons d'effectuer les expériences afin d'observer si les traces de subjectivité permettent de différencier entre les documents scientifiques, cliniques et les messages de forum. Nous utilisons pour ceci les descripteurs relatifs à l'incertitude, la négation, les modifieurs, les émotions (lexicales et non lexicales) et les notions médicales. Nos expériences montrent qu'il existe en effet une corrélation forte de la subjectivité et des notions médicales avec ces différents types de documents, avec les performances de catégorisation automatique étant souvent supérieures à 0,90. Le travail effectué est réalisé avec les documents médicaux relevant du thème rhumatologie, mais nous pensons que ces résultats sont généralisables à d'autres thèmes de la médecine. Cela voudrait dire que les descripteurs proposés peuvent être utilisés dans les moteurs de recherche pour apporter une caractérisation supplémentaire aux documents (ici, un niveau de spécialisation et les destinataires attendus). Dans cette optique, les résultats de recherche ne sont pas filtrés a priori, mais enrichis avec des annotations supplémentaires : il revient à l'utilisateur de décider s'il veut consulter un document avec un niveau de spécialisation donné. De la même manière, nous pensons que les descripteurs proposés et testés peuvent être appliqués dans la tâche de recherche d'information appliquée à d'autres domaines de spécialité.
Parmi les perspectives de ce travail, nous prévoyons d'effectuer la catégorisation automatique de phrases, afin de détecter les catégories d'émotions et de subjectivité des acteurs. Nous voulons aussi tester l'impact individuel de différents types de descripteurs. Les classes de marqueurs portant sur l'incertitude, la négation et leur interaction avec les modifieurs peuvent être affinées (Zadeh, 1972;Akdag et al., 1992;Cornelis et al., 2004;Akdag et al., 2001). Les descripteurs proposés peuvent être combinés avec d'autres descripteurs exploités dans la littéra-ture (e.g. le lexique de manière générale, les informations syntaxiques et stylistiques, l'analyse morphologique des termes). De même, une autre baseline, plus évoluée, peut être utilisée. Finalement, la méthode peut être adaptée à d'autres domaines (e.g. juridique, financier), où il existe aussi des utilisateurs avec de différents types d'expertise.

Introduction
Bien que réputé comme singulièrement difficile et considéré comme NP-complet, le problème du calcul des traverses minimales d'un hypergraphe a suscité l'intérêt de la communauté scientifique (Berge (1989); Kavvadias et Stavropoulos (2005); Hébert et al. (2007) ;Murakami et Uno (2013); Toda (2013)). Cet intérêt pour les traverses minimales est dû au fait qu'elles présentent une solution pour de nombreuses applications dans des domaines variés tel que la cryptographie, le web sémantique, l'e-commerce, etc. (Hagen (2008)).
La principale difficulté que pose l'extraction des traverses minimales réside dans le nombre exponentiel de ces dernières, même quand l'hypergraphe d'entrée est simple. À titre d'exemple, considérons l'hypergraphe H = (X , ?) avec l'ensemble des sommets X = (x 1 , x 2 , . . . , x 2n ) et l'ensemble des hyperarêtes ? = ({x 1 , x 2 }, {x 3 , x 4 }, . . . , {x 2n?1 , x 2n }). Le nombre de traverses minimales est égal à 2 n tandis que le nombre de sommets est égal à 2n. Les algorithmes d'extraction des traverses minimales les plus performants sont des amélio-rations de l'algorithme de Berge (1989). Ce dernier traite les hyperarêtes une à une en calculant à chaque itération i les traverses minimales de l'hypergraphe constitué par les i-èmes hyperarêtes considérées. Avec pour objectif d'optimiser le calcul des traverses minimales, notre approche repose sur cette idée en usant du paradigme "diviser pour régner". Le principe consiste à réduire ce nombre d'itérations en partitionnant l'hypergraphe en un nombre précis d'hypergraphes partiels, équivalent au nombre de transversalité de l'hypergraphe d'entrée H. A partir de chaque hypergraphe partiel H i , nous calculons alors ce que nous appelons les traverses minimales locales à H i . Le produit cartésien de ces traverses minimales locales correspondra alors à un ensemble de traverses de H qui seront soumises à une vérification de la minimalité pour être considérées comme des traverses minimales. En outre, pour un hypergraphe H avec un nombre de transversalité égal à k, le fait de partitionner H en k hypergraphes H i permet d'éliminer le test de la minimalité pour les ensembles de sommets de taille k qui seront considérés comme traverses minimales de H, sans aucun autre calcul supplémentaire.
Cet article est organisé comme suit : dans la section 2, nous reviendrons sur différents algorithmes, proposés dans la littérature, pour calculer les traverses minimales d'un hypergraphe. Ensuite, dans la section 3, nous rappelons des notions-clés issues de la théorie des graphes et de la fouille de données, que nous combinerons pour introduire notre approche basée sur la notion de traverse minimale locale et d'hypergraphe partiel. La section 4 présentera notre approche pour le calcul des traverses minimales à travers un algorithme original, appelé LOCAL-GENERATOR. Enfin, une étude expérimentale sur des hypergraphes aléatoires sera décrite dans la section 5.
Etat de l'art
Berge a été le premier à proposer un algorithme dédié à l'extraction des traverses minimales mais cette solution est impraticable sur des hypergraphes de grande taille car elle nécessite le stockage des traverses minimales temporaires, calculées après chaque ajout d'une nouvelle hyperarête. En effet, l'algorithme de Berge commence par calculer l'ensemble des traverses minimales de la première hyperarête avant de le mettre à jour en ajoutant incrémentalement les autres hyperarêtes, une à une. D'après Hagen (2008) et Takata (2007, l'algorithme de Berge présente une complexité super-polynomiale en la taille de l'entrée et de la sortie. Récemment, Boros et al. (2008) ont prouvé que le temps d'exécution de l'algorithme avait une borne supé-rieure sub-exponentielle.
Plusieurs travaux ont ensuite tenté d'améliorer l'algorithme de Berge, parmi lesquels on peut citer ceux de Bailey et al. (2003), Dong et Li (2005) et Kavvadias et Stavropoulos (2005). L'approche la plus performante est celle de ces derniers qui, pour remédier à la principale lacune de l'algorithme pionner, à savoir une consommation excessive de mémoire, ont proposé une technique consistant à combiner les sommets qui appartiennent aux mêmes hyperarêtes à chaque itération de l'algorithme. Alors que l'algorithme de Berge effectue un parcours en largeur de l'arbre des traverses minimales, Kavvadias et al. proposent un parcours en profondeur de l'hypergraphe d'entrée en introduisant la notion de noeud généralisé, qui représentent un ensemble de sommets appartenant aux mêmes hyperarêtes et qui est mis à jour. Enfin, notons que dans cet algorithme, la vérification de la minimalité est effectuée à travers la notion de noeud approprié.
En l'assimilant à un problème de dualisation de fonctions booléennes et monotones, Fredman et Khachiyan ont proposé un algorithme incrémental pour l'extraction des traverses minimales en vérifiant si deux formules f et g sont mutuellement duales, (i.e. si f (x) = ¯ g(¯ x)). Cette approche a été reprise, de différentes manières, par Eiter et Gootlob ou encore par Boros (Eiter et al. (2002); Boros et al. (2003)). La version de ce dernier généralise le principe de la dualisation dans la mesure où les formules ne traitent plus que des variables booléennes mais manipulent, désormais, aussi des variables entières bornées.
Adoptant un parcours en largeur d'abord et opérant par niveau, l'algorithme MTMINER de Hébert et al. (2007)   (2013)). Actuellement, ces algorithmes sont considérés dans la littérature comme les plus efficaces aussi bien en termes de temps d'exécution qu'en consommation mémoire, notamment sur des hypergraphes de grande taille.
Le dernier algorithme en date dédié au calcul des traverses minimales, est celui de Toda. Il fait appel à des structures de données compressés BDD (diagramme de décision binaire Notre approche, qui repose sur le paradigme "diviser pour régner", est une extension de l'algorithme de Berge. Alors que dans ce dernier, ainsi que dans les améliorations qui en ont été proposées, l'idée est de traiter les hyperarêtes une à une, dans cet article, nous proposons de traiter les hyperarêtes ensemble par ensemble. L'hypergraphe d'entrée H se trouve alors partitionné en un nombre d'hypergraphes partiels égal au nombre de transversalité k de H. Chaque hypergraphe partiel renferme des traverses minimales locales et le produit cartésien, combiné à un test de la minimalité, permet de retrouver l'ensemble des traverses minimales de H. Le choix du nombre d'hypergraphes partiels n'est pas arbitraire puisqu'il garantit que les traverses dont la taille est égale à k peuvent être directement considérées comme des traverses minimales de H. traverses minimales, basée sur la notion des traverse minimale locale, nous avons combiné des concepts de la théorie des hypergraphes (hypergraphe, traverse, nombre de transversalité) avec d'autres de la fouille de données (ensemble essentiel, support). Le nombre de transversalité d'un hypergraphe est la notion-clé de cette section, autour de laquelle est bâtie notre approche.
Définition 1 HYPERGRAPHE SIMPLE (Berge (1989)) Soit H = (X , ?) avec X = {x 1 , x 2 , . . . , x n } un ensemble fini d'éléments et ? = {e 1 , e 2 , . . . , e m } une famille de parties de X . H constitue un hypergraphe simple sur X si :
1.
Les éléments de X sont appelés sommets et ceux de ? hyperarêtes de l'hypergraphe et, dans la suite, nous ne considérerons que des hypergraphes simples.
Exemple 1 La figure 1 illustre un hypergraphe simple H = (X , ?) tel que X = (1, 2, 3, 4, 5, 6, 7, 8, 9) et ? = { e 1 , e 2 , e 3 , e 4 , e 5 , e 6 } avec e 1 = {1, 2}, e 2 = {2, 3, 7}, e 3 = {3, 4, 5}, e 4 {4, 6}, e 5 = {6, 7, 8} et e 6 = {7, 9}.  (Berge (1989)) Soit un hypergraphe H = (X , ?) tel que X est l'ensemble de sommets et ? = {e 1 , e 2 , . . . , e m } est l'ensemble des hyperarêtes de H. T ? X est une traverse de H si T e i = ? ?i = 1, . . . , m. ? H désigne l'ensemble des traverses définies sur H. Une traverse T de ? H est dite minimale si 0 ? T s.t. T 0 ? ? H . On notera M H , l'ensemble des traverses minimales définies sur H. Le nombre minimum de sommets d'une traverse est appelé le nombre de transversalité de l'hypergraphe H et on le désigne par : Berge (1989), M H représente les hyperarêtes de l'hypergraphe transversal correspondant à H et défini sur X .
Exemple 2 Dans l'exemple illustratif de la figure 1, l'ensemble M H de l'hypergraphe est : { {1, 4, 7}, {2, 4, 7}, {2, 5, 6, 7}, {2, 5, 6, 9}, {2, 4, 8, 9}, {2, 4, 6, 9}, {2, 3, 6, 7}, {2, 3, 6, 9}, {1, 5, 6, 7}, {1, 3, 6, 7}, {1, 3, 6, 9} et {1, 3, 4, 8, 9}. Un hypergraphe H peut être représenté par une matrice d'incidence IM H définie par un triplet (?, X , R) où R ? ? × X est une relation binaire entre les hyperarêtes et les sommets. Cette matrice d'incidence IM H , associée à l'hypergraphe H = (X , ?), est définie par : ?x i ? X et ?e j ? ?, R(e j , x i ) = 1 si x i ? e j et R(e j , x i ) = 0 sinon. La Table de la figure 1 représente la matrice d'incidence associée à l'hypergraphe H.
Définition 3 SUPPORT D'UN ENSEMBLE DE SOMMETS
Soit l'hypergraphe H = (X , ?) et X un sous-ensemble de X . Nous définissons Supp(X) comme le nombre d'hyperarêtes de H, renfermant au moins un élément de X : Supp(X) = |{e ? ?|?x ? X ? R(e, x) = 1}| Ainsi, l'ensemble X peut être vu comme une disjonction de sommets (x 1 ? x 2 ? . . . ? x n ) tel que la présence d'un seul sommet de X suffit à affirmer que X satisfait une hyperarête donnée, indépendamment des autres sommets. (Casali et al. (2005)) Soit l'hypergraphe H = (X , ?) et X ? X . X représente un ensemble essentiel de sommets si et seulement si :
Définition 4 ENSEMBLE ESSENTIEL DE SOMMETS
Il est important de souligner que les ensembles essentiels, extraits à partir d'une matrice d'incidence, vérifient la propriété d'idéal ordre, i.e, si X est un ensemble essentiel, alors ?Y ? X, Y est aussi un ensemble essentiel. De plus, la notion de traverse peut être redéfinie par le biais du support d'un ensemble de sommets et de la notion d'ensemble essentiel, selon la proposition 1.
Proposition 1 TRAVERSE MINIMALE
Un sous-ensemble de sommets X ? X est une traverse minimale de l'hypergraphe H, si X est essentiel et si son support est égal au nombre des hyperarêtes de H, autrement dit, X est un ensemble essentiel tel que Supp(X)=|?|.
Preuve 1 Soit X un ensemble essentiel de sommets tel que Supp(X)=|?|. Par conséquent, X e i = ? ?e i ? ?, i = 1, . . . , m. Donc, d'après la définition 2, X est une traverse. La minimalité de X tient à son "essentialité". En effet, puisque X est essentiel, alors son support est strictement supérieur à celui de ses sous-ensembles directs. Par conséquent, 1 ? X s.t. Supp(X 1 )=|?|. X est donc une traverse minimale. (Berge (1989)
Définition 5 UNION ET PRODUIT CARTÉSIEN
Le résultat de cette union est un hypergraphe dont l'ensemble des sommets est constitué de ceux de H et de G, et l'ensemble des hyperarêtes contient celles de H et G, qui par souci de simplification sera aussi noté H ? G :
. . , ? m ) H × G représente le produit cartésien des deux hypergraphes dont le résultat est un hypergraphe dont l'ensemble des sommets contient ceux des deux hypergraphes. Quant à l'ensemble des hyperarêtes, il est aussi noté H × G et est égal au produit cartésien de ? et de ? autrement dit à l'union de tous les couples possibles d'hyperarêtes tels que le premier élément appartient à ? et le deuxième à ? :
Proposition 2 (Kavvadias et Stavropoulos (2005)) Soient H et G deux hypergraphes simples. Les traverses minimales de l'hypergraphe H ? G sont des couples, minimaux au sens de l'inclusion, générés par le produit cartésien des ensembles de traverses minimales de H et de G :
Définition 6 HYPERGRAPHE PARTIEL(Berge (1989)) Un hypergraphe partiel H est la restriction d'un hypergraphe H à un sous-ensemble d'hyperarêtes ? incluses dans ? et aux sommets contenus dans ces hyperarêtes.
Dans le cadre de cet article, nous proposons d'étendre la proposition 2 en considérant plus de deux hypergraphes. Plus précisément, à partir d'un hypergraphe H=(X , ?), dont le nombre de transversalité ? (H) est égal à k, et d'une traverse minimale T = {x 1 , x 2 , . . . , x k } de M H de taille k dont les sommets sont numérotés par ordre de support décroissant, nous proposons de construire k hypergraphes partiels H i = (X i , ? i ), i = 1, . . . , k tels que :
-X i = {x ? X | x ? e, ?e ? ? i } On peut remarquer que les hypergraphes partiels H i vérifient de façon évidente les propriétés suivantes :
-? ? tel que e ? ? i ? ? j , i = j. Les traverses minimales de l'hypergraphe partiel H i sont appelées traverses minimales locales à H i et leur ensemble est noté par M Hi .
Exemple 3 L'hypergraphe de l'exemple illustratif de la Figure 1 a un nombre de transversalité égal à 3. Sachant que H possède 2 traverses minimales de cardinalité minimale égale à 3, {1, 4, 7} et {2, 4, 7}. Prenons, par exemple, {1, 4, 7}. Après avoir ordonné les trois sommets le composant, selon un ordre décroissant de support, nous obtenons les trois hypergraphes partiels, présentés par la Figure 2, tel que H 1 ne contient que les hyperarêtes auxquelles appartient le sommet 7 (dont le support est égal à 3), H 2 ne contient que celles auxquelles appartient 4 (dont le support est égal à 2) et H3 contient les hyperarêtes restantes, i.e., celles qui renferment le sommet 1. Il importe de noter qu'en choisissant {2, 4, 7}, au lieu de {1, 4, 7}, le résultat reste le même.
Traverses minimales locales : approche et algorithme
Optimiser le calcul de ces traverses minimales revient donc principalement à réduire le nombre de candidats traités. Ceci passe par la réduction de la taille de l'hypergraphe d'entrée. L'approche proposée dans cet article consiste à construire, à partir de l'hypergraphe d'entrée FIG. 2: Les 3 hypergraphes partiels dérivés de H : H 1 , H 2 et H3 H, k hypergraphes partiels (H 1 , H 2 , . . . , H k ) tel que k correspond au nombre de transversalité de H. Le calcul de l'ensemble des traverses minimales locales, M H i de chaque hypergraphe partiel H i s'en trouve amélioré puisque la taille de H i est relativement petite par rapport à celle de H. Ainsi, nous proposons d'effectuer l'union des hypergraphes partiels de façon à dé-terminer l'ensemble des traverses minimales M H de H à partir des k-uplets,minimaux au sens de l'inclusion, issus du produits cartésien des ensembles de traverses minimales locales déter-minées pour les hypergraphes partiels M H i . Dans ce qui suit, nous présentons l'algorithme LOCAL-GENERATOR dédié au calcul des traverses minimales et basé essentiellement sur les notions de nombre de transversalité et d'hypergraphe partiel.
L'algorithme LOCAL-GENERATOR
L'algorithme LOCAL-GENERATOR, dont le pseudo-code, est décrit par l'Algorithme 1 prend en entrée une matrice d'incidence (correspondant à l'hypergraphe d'entrée) et fournit en sortie l'ensemble des traverses minimales. On suppose que les sommets de l'hypergraphe sont triés par ordre lexicographique. LOCAL-GENERATOR démarre par un appel à la fonction GETMINTRANSVERSALITY, dont le pseudo-code est décrit par l'Algorithme 2. Cette fonction recherche une traverse minimale dont la taille est minimale, égale au nombre de transversalité de l'hypergraphe. Pour ce faire, la fonction parcourt les sommets, un par un. Pour chaque élément x i de X , GETMINTRANSVERSALITY supprime de la matrice d'incidence IM H les hyperarêtes de ? qui contiennent x i . La fonction prend ensuite le sommet, différent de x i , ayant le plus grand support dans IM H et réactualise la matrice de la même manière, i.e., en retirant les hyperarêtes contenant ce dernier sommet. Le traitement, entre la ligne 11 et la ligne 15, s'arrête dès que IM H se vide complètement de toute hyperarête. Le vecteur T tmp sert à stocker les sommets supprimés, un à un à partir de X, pour aboutir à ? = ?. Si le nombre de sommets de T tmp est le plus petit, obtenu jusque-là dans la fonction, la cardinalité de T tmp est stockée dans min et les éléments de T tmp sont copiés dans T . Le traitement est répété pour tous les sommets de X. A chaque itération de la boucle de la ligne 5, l'ensemble ? est réinitialisé avec les éléments de ? et T tmp est vidé des éléments qu'il renferme. Au final, GETMINTRANSVERSA-LITY retourne la suite de sommets qui a permis de "vider" la matrice d'incidence en un nombre minimum d'étapes. Cette suite-là, contenue dans T tmp , représente une traverse minimale dont la taille est nécessairement égale au nombre de transversalité de l'hypergraphe d'entrée H.
Une fois le nombre de transversalité calculé et une traverse minimale de taille minimale renvoyée par la fonction GETMINTRANSVERSALITY, l'algorithme LOCAL-GENERATOR Algorithme 1 : LOCAL-GENERATOR Entrées : Une matrice d'incidence IM H associée à H = (X , ?) Sorties : M H , ensemble des traverses minimales de H début 1 initialiser(T : vecteur) ;
construit, à partir de la matrice d'incidence IM H , k hypergraphes partiels et fait appel à la fonction COMPUTE_TM pour construire leurs traverses minimales locales, stockées dans M H i (ligne 9). Cette fonction 3 prend en entrée une matrice d'incidence associée à un hypergraphe partiel H i de H et calcule, par niveaux, l'ensemble des traverses minimales locales à H i selon la définition 1. A la fin de la boucle de la ligne 6, LOCAL-GENERATOR a déjà calculé les ensembles des traverses minimales locales. Le produit cartésien (ligne 11) de ces ensembles M Hi , permet de construire l'ensemble ? H . Chaque élément de ? H issu de ce produit cartésien représente une traverse. Il reste à vérifier sa minimalité. Un des intérêts de notre décomposition de l'hypergraphe initial est d'éviter de tester la minimalité des éléments de ? H dont la cardinalité est égale à k. En effet, ces derniers représentent des traverses minimales de H puisqu'il ne peut pas exister une traverse minimale de taille inférieure au nombre de transversalité de H. Pour les traverses de taille supérieure à k, LOCAL-GENERATOR teste la minimalité (lignes 15 ? 16) suivant la Proposition 1. Si le support d'un candidat X est strictement supérieur au maximum des supports de ses sous-ensembles directs alors X est une traverse minimale et est ajouté à M H .
3. Dans les expérimentations, nous avons utilisé l'algorithme MTMINER pour implémenter cette fonction. 
Etude Expérimentale
Différentes expérimentations ont été menées sur des jeux de données variés afin d'éva-luer l'algorithme LOCAL-GENERATOR. Le premier lot de jeux de données considérés dans cette étude expérimentale comporte des hypergraphes générés à partir des bases de données "Accidents" 4 et "Connect-4" 5 alors que le deuxième lot contient des hypergraphes aléatoires générés, à travers le "random hypergraph generator" implementé par Boros et al. (Boros et al. (2003)), en fonction du nombre de sommets, du nombre d'hyperarêtes et de la taille minimale des hyperarêtes. dès que la taille des plus petites traverses minimales de l'hypergraphe d'entrée est élevée ce qui lui permet de partitionner l'hypergraphe en plusieurs hypergraphes partiels. De plus, le nombre de traverses minimales est très important. Sur des hypergraphes renfermant peu de traverses minimales, LOCAL-GENERATOR peine à se montrer efficace puisque le nombre de traverses minimales devient négligeable par rapport au nombre de candidats traités et testés. Cette constatation est confirmée par le tableau 2 en observant les hypergraphes H1 et H4. Quand le nombre de traverses minimales a pratiquement doublé, l'écart en secondes est passé de 420 à 1234 entre les deux algorithmes.
Conclusion
Dans cet article, nous avons introduit une nouvelle approche pour le calcul des traverses minimales d'un hypergraphe. Cette approche repose sur le paradigme "diviser pour régner" afin de partitionner l'hypergraphe d'entrée en hypergraphes partiels, en fonction du nombre de transversalité. Le calcul des traverses minimales locales, correspondantes à ces hypergraphes partiels, permet de retrouver l'ensemble des traverses minimales à travers un produit carté-sien combiné à un test de la minimalité. Ceci nous a permis d'introduire un nouvel algorithme LOCAL-GENERATOR pour l'extraction des traverses minimales. L'étude expérimentale a confirmé l'intérêt de notre approche sur un type précis d'hypergrpahes renfermant des propriétés données. Les résultats obtenus nous incitent, par ailleurs, à réfléchir à la mise en place d'un algorithme hybride qui s'auto-adapte à la valeur du nombre de transversalité, i.e., si ce dernier est jugé petit, alors on évitera de générer les hypergraphes partiels. Par ailleurs, nous chercherons à explorer l'espace de recherche en profondeur d'abord dont l'efficacité par rapport à l'exploration par niveaux a été montré.

Introduction
Construire un modèle articulatoire de la parole, c'est être capable d'indiquer les mouvements des articulateurs (mâchoires, lèvres, etc.) à l'origine de celle-ci (voir figure 1, à gauche). Les applications pratiques d'un tel modèle sont nombreuses 1 . Nous exposons ici comment nous avons extrait un modèle articulatoire à partir de données recueillies auprès d'un locuteur. Ce travail se situe dans la lignée des travaux initiés par Maeda (1990). Il a construit son modèle articulatoire (voir figure 1, à droite) au moyen d'analyses en composantes principales sur des données de même type. Puis il l'a évalué de façon acoustique en comparant les sons réels aux sons produits par un synthétiseur de sons piloté par son modèle. La nouveauté de notre démarche consiste en l'utilisation d'une méthode d'analyse 3-way pour extraire le modèle, et de méthodes d'apprentissage supervisé pour le valider. Notre éva-luation se fait en comparant de façon phonétique les sons prédits aux sons réels. L'acoustique intervient de surcroît dans notre évaluation car nous mettons en parallèle les performances de notre modèle et celles du modèle acoustique formé des coefficients cepstraux ces dissimilarités, le stress brut est donné par la formule Stress
C'est par le choix de la fonction f que le stress est minimisé.
Le 3-way MDS pour nos données Nos données, coordonnées des points des articulateurs sur les radiographies ont été transformées en autant de tableaux de distances entre images que de points. Les méthodes de 3-way MDS permettent de traiter simultanément plusieurs tableaux par MDS, d'après Borg et Groenen (1997);Carroll (1972);Carroll et Chang (1970). Nous avons choisi la fonction smacofIndDiff du package SMACOF (de Leeuw et Mair, 2009), avec le paramètre "idioscal" correspondant à la variante "idiosyncratic" décrite dans Carroll (1972), qui s'est avérée moins gourmande en mémoire vive que la méthode INDSCAL de Carroll et Chang (1970) que nous avions utilisée pour des données plus simples dans Busset et Cadot (2013). Avec 200 itérations, nous avons pu obtenir à partir des 50 points les positions des 732 images dans des espaces allant de 2 dimensions à 18 (64bits, 4 coeurs, 8 Gio de RAM, pour dim=18, temps=48h).

Introduction
Le nombre de caméras de vidéosurveillance installées dans le monde augmente chaque jour. En France, le système de la RATP déployé sur Paris comprend 9000 caméras fixes et 19000 mobiles. Lors de faits particuliers (e.g., agressions, vols), les opérateurs de vidéo surveillance se basent sur les indications spatiales et temporelles de la victime et sur leur connaissance de la localisation des caméras pour sélectionner les contenus intéressants pour l'enquête. Deux grands problèmes peuvent alors survenir : (1) le temps de réponse est long (jusqu'à plusieurs jours de traitement) et (2) un risque important de perte de résultats à cause d'une mauvaise connaissance du terrain (appel à des opérateurs extérieurs). Le but de notre recherche est de définir des outils d'assistance aux opérateurs qui puissent, à partir d'une trajectoire donnée, sélectionner de façon automatique les caméras pertinentes par rapport à la requête.
Contexte de la vidéosurveillance et problématique
Dans un système de vidéosurveillance le contenu est acquis par des caméras fixes et mobiles installées dans des contextes différents (à l'extérieur ou à l'intérieur des bus, des stations de métro, dans les rues, etc.).
Lorsqu'une victime d'une agression porte plainte, on lui demande de décrire les éléments qui pourront aider à trouver les segments vidéos pertinents. Les éléments d'une telle description sont : la Localisation (e.g., Paris), la Date et l'Heure (e.g., 10 octobre 2013 entre 10h et 11h), l'Événement recherché (e.g., vol de sac), et la Trajectoire de la victime avant et après l'événement (e.g., Rue de Rivoli, de la sortie du musée du Louvre jusqu'à l'entrée de la station de métro Chatelet). Les opérateurs, en se basant sur les aspects spatiaux et temporels de la requête et sur leur propre connaissance de la localisation des caméras, sélectionnent les caméras qui auront référencé r des informations concernant la trajectoire de la personne pour ensuite analyser les enregistrements vidéo.
Le but et de définir un(des) nouveau(x) opérateur(s) prennant comme entrée une trajectoire cible définie comme un ensemble d'unités u k (segments cartographiques de la rue de Rivoli) et un intervalle temporaire et qui calcule la liste d'extraits vidéo pertinents pour la requête :
ce qui équivaut à dire que la caméra c i a potentiellement "filmé" le segment u k de la trajectoire de la requête entre t Dans notre approche nous proposons un modèle à quatre couches indépendantes : (1) Ré-seau routier, (2) Réseau de transport, (3) Objets, (4) Caméras.
La couche Réseau Routier est basée sur l'approche de modélisation sous forme de graphe consacrée dans la littérature (Liu et al., 2012). Le réseau routier est un graphe G= (N, A), dans lequel les noeuds sont les croisements des rues et les arcs sont les rues. Le Réseau de Transport est aussi modélisé sous forme de graphe. A ce niveau, les noeuds sont constitués par des stations de bus qui sont situées sur des segments de rues. Des séquences ordonnées de noeuds de transport forment des lignes (e.g., des lignes de bus). La couche Objets modélise les positions des objets fixes et mobiles par rapport aux couches sous-jacentes. Chaque Objet Mobile transmet périodiquement sa position en fonction de différentes stratégies (e.g., chaque fois que l'objet change de segment) que nous ne traitons pas dans ce papier. Nous supposons que nous avons au moins une mise à jour de la position par segment de rue. L'ensemble de points avec les timestamps associés forment leur trajectoire (Gutting et al., 2006). Au-dessus de toutes ces couches, nous modélisons les Caméras de Vidéosurveillance. Un schéma simplifié de cette couche est illustré dans la Figure 1. Cette couche est composée des caméras fixes et mobiles. Les caméras fixes ont une position 2D fixée au moment de l'installation. Les caméras mobiles sont associées à un objet mobile (e.g., bus) et leur trajectoire est la même que celle de l'objet. La nouvelle génération de caméras a des capteurs GPS incorporés et même des boussoles. Les technologies développées autour de ces caméras rendent possible l'extraction automatique des caractéristiques de prise de vue, par exemple : l'orientation, le zoom, la distance focale, etc. En se basant sur ces éléments, il est possible de modéliser le champ de vue et de tracer ces modifications dans le temps (Arslan Ay et al., 2010 Q1 : Sélectionner les caméras fixes dont le champ de vue a intersecté un segment donné :
SEGMENT, geometry(F, C.POSITION),'mask=anyinteract')='TRUE' AND C.IDFOV=F.IDFOV AND E.ID=idSegmentRequete AND ((F.TIMESTAMP<= t 2 AND F.TIMESTAMP >= t 1 ) OR (F.TIMESTAMP<t 1 AND NOT EXISTS (SELECT F1.TIMESTAMP FROM FOV F1 WHERE F1.TIMESTAMP>F.TIMESTAMP AND F1.TIMESTAMP<=t 1 AND F1.IDFOV=F.IDFOV))) } La requête sélectionne les caméras dont la géométrie du champ de vue (calculée à partir des caractéristiques stockées dans la table FOV et de la position de la caméra par la fonction geometry(F, C.POSITION)) intersecte la géométrie du segment de rue donné (SDO_RELATE est un opérateur spatial qui vérifie si deux géométries ont n'importe quel type d'intersection (paramètre anyinteract)) dans l'intervalle recherché.
Q2 : Sélectionner les caméras mobiles dont la trajectoire a intersecté un segment donné :
SELECT MC.IDMC FROM < EDGES E, MO, MC> WHERE { (SDO_RELATE(E.SEGMENT, MO.POSITION), 'mask=anyinteract') ='TRUE' AND t 1 <= MO.TIMESTAMP AND MO.TIMESTAMP<= t 2 AND MO.IDOBJ=MC.IDOBJ AND E.ID=idSegmentRequete } La deuxième requête sélectionne les caméras mobiles associées aux objets mobiles dont la trajectoire a intersecté le segment donné entre t 1 et t 2 .
Conclusion et perspectives
Afin d'aider les opérateurs, nous proposons une approche de modélisation spatiale et temporelle des caméras de vidéosurveillance dans le cadre d'un réseau routier, modélisation basée sur des informations spatiales et des trajectoires des objets mobiles. A partir de ce modèle on peut sélectionner la liste des caméras qui qui ont potentiellement "filmé" la trajectoire d'une personne. Nous envisageons d'étendre le modèle pour prendre en compte les réseau de camé-ras à l'intérieur de stations de métro ou de train.Un autre aspect concerne l'ordonnancement des résultats en fonction de la distance des caméras par rapport aux segments de la requête. 
Références
Summary
Our work concerns the assistance to the operators of videosurveillance in the search for partiular videos (e.g., attack of persons, lost object) by the automatic identification of a set of cameras likely to have filmed a required scene. This paper presents a multi-layer modeling (cameras fixed and mobile, road network, public transport network) whose characteristic consists in rather including calculation data of the geometry of the cameras viewpoints on the network layer than to store this geometry as such.

Introduction
L'extraction de motifs fréquents est une tâche essentielle en fouille de données. Les motifs permettent de résumer un jeu de données de manière intelligible et peuvent être utilisés pour d'autres tâches comme l'analyse d'association, la classification supervisée associative, ou la classification à base de motifs. Des algorithmes efficaces ont été proposés pour extraire des motifs dans différents types de données comme les données transactionnelles, les séquences d'évènements, et les graphes. Le principal inconvénient des techniques d'extraction de motifs est l'abondance des motifs produits, qui résulte de la nature combinatoire des algorithmes en oeuvre. Différentes solutions ont été proposées face à ce problème, comme l'intégration de contraintes dans les algorithmes (Boulicaut et Jeudy, 2005), le filtrage des motifs par des mesures d'intérêt (Blanchard, 2005;Blanchard et al., 2007), et l'extraction de représentations condensées des motifs fréquents, i.e. un sous-ensemble des motifs qui permet de générer la totalité des motifs de manière exacte ou approchée (Calders et al., 2006). Malgré ces efforts, le problème reste à peine atténué, comme le rappelle l'étude récente de Giacometti et al. (2013).
Dans cet article, nous proposons une méthode originale pour extraire un résumé compact, représentatif et intelligible des motifs fréquents dans des données transactionnelles ou séquen-tielles (par exemple une ou plusieurs séquences d'évènements, du texte, des séquences biologiques). Ce résumé peut être lu et interprété directement, mais il offre aussi la possibilité de générer de manière approchée l'ensemble des motifs fréquents et d'estimer leur support. Dans le détail, notre approche consiste à extraire un nouveau type de motifs que nous appelons motifs récursifs, i.e. des motifs de motifs, à l'aide d'un algorithme hiérarchique agglomératif nommé RepaMiner. La nature hiérarchique de ces motifs nous permet de produire non pas un simple ensemble de motifs mais une véritable structure, nommée RPgraph, fondée sur les motifs et dérivée de dendrogrammes. Les spécificités de notre approche sont les suivantes :
-L'algorithme RepaMiner est un algorithme dynamique, comme certaines méthodes récentes de fouille de motifs (Vreeken et al., 2011 2 Extraction ascendante hiérarchique de motifs récursifs L'algorithme RepaMiner (Recursive Pattern Miner) a été conçu à l'origine pour analyser des données séquentielles, l'analyse de données tabulaires étant un simple cas particulier. Cependant, pour présenter notre approche, il est plus clair de nous placer dans le cas classique d'un jeu de données transactionnel. Nous adoptons ce point de vue dans les sections qui suivent. Nous considérons donc un ensemble I de littéraux nommés items, et un ensemble T de transactions où chaque transaction t est un ensemble t ? I.
Motif récursif
Definition 1 (Motif récursif). Etant donné un ensemble I d'items, un motif récursif est soit un item de I, soit une paire non ordonnée {x, y} où x et y sont des motifs récursifs. x et y sont appelés les parents du motif récursif. Example 1. Soit l'ensemble d'items I = {a, b, c, d, e, f }. m 1 = {{a, b}, c} et m 2 = {{{a, b}, {c, d}}, {e, f }} sont deux motifs récursifs. Ils sont représentés dans la figure 1.
Un motif récursif est donc une agrégation de deux éléments, et peut être représenté par un arbre binaire (figure 1). Les motifs récursifs généralisent la notion d'itemset en y introduisant un ordre partiel (les niveaux de l'arbre). Le support dans T d'un motif récursif m est défini de manière classique comme étant le nombre de transactions de T qui contiennent tous les items de m. Les niveaux hiérarchiques des items ne sont donc pas pris en compte dans le calcul du support. Un motif récursif est qualifié de fréquent dans T si son support est supérieur à un seuil minsup défini par l'utilisateur.
L'algorithme RepaMiner
L'espace de recherche des motifs récursifs fréquents dans T est de taille exponentielle par rapport au nombre d'items. Dans RepaMiner, nous préférons adopter la stratégie gloutonne de la CAH afin de réduire l'espace de recherche à une taille quadratique. En partant des motifs récursifs singletons, on agrège à chaque itération les deux motifs récursifs les plus proches au sein d'un nouveau motif récursif, puis on met à jour les données. La similarité entre motifs est évaluée par la surface de leur intersection, définie par support(m 1 ? m 2 ) × (nombre d'items dans m 1 ?m 2 ). Cette mesure est utilisée dans les algorithmes de pavage par itemsets (tiling). La maximisation de la surface sert aussi bien de mesure de similarité que de critère d'agrégation puisqu'elle s'adapte autant aux singletons qu'aux motifs issus d'agrégations. Parmi une dizaine de mesure étudiées, nous avons constaté empiriquement que maximiser la surface permet à RepaMiner de minimiser l'erreur de restauration des itemsets (Yan et al., 2005).
Input : ensemble de transactions T , seuil de support minimal minsup. Output : Ensemble des motifs récursifs fréquents H. C ? {itemsets fréquents de taille 2} 9: end while 10: return H RepaMiner n'utilise que des méthodes classiques de calcul d'itemsets (motifs ensemblistes) de taille 2. C'est la propriété dynamique de notre approche (création de nouveaux items à chaque itération) qui permet de construire des hiérarchies d'items à l'aide de méthodes d'extraction d'itemsets de taille 2. Le pseudo-code est résumé dans l'algorithme 1. C est l'ensemble des motifs récursifs candidats, parmi lesquels le meilleur (au sens du critère d'agréga-tion) sera choisi. A la ligne 3, C est initialisé à l'ensemble des itemsets de taille 2 fréquents dans T , accompagnés de leur support. A la ligne 5, on identifie le meilleur motif candidat m au sens du critère d'agrégation. La procédure MiseAJourDonnées() de la ligne 7 modifie le jeu de données T . Un nouvel item est créé pour coder les occurrences du motif m, et toutes les occurrences des parents de m qui participent au support de m sont supprimées. L'idée est de ne pas perdre l'information représentée par les 1 dans les données, en la répartissant parmi les trois items. La procédure s'achève en supprimant les items qui sont devenus non fréquents suite à la mise à jour des données. A la ligne 8, on calcule les itemsets de taille 2 fréquents dans T en prenant en compte les mises à jour effectuées sur T à la ligne 7. Ces itemsets constituent le nouvel ensemble C de candidats. Lorsqu'il n'y a plus aucun motif fréquent candidat au titre de meilleure agrégation, l'algorithme retourne l'ensemble H des motifs récursifs qui ont été extraits, i.e. l'ensemble des agrégations qui ont été réalisées. Chaque motif de H est fréquent (par construction), et est accompagné de sa valeur de support au moment de l'agrégation.
Résultats de l'algorithme. RepaMiner produit l'ensemble H des motifs récursifs qui ont été générés à chaque itération. Chaque motif est accompagné de la valeur de support qu'il présentait au moment de sa création. Pour visualiser l'ensemble H, nous représentons chaque motif récursif maximal 1 par un dendrogramme indicé par le complément à 1 du support. Ce choix est justifié par le fait que le complément du support est une ultramétrique sur l'ensemble des items produits par RepaMiner. Il est à noter que H ne constitue une hiérarchie unique et complète que dans le cas où le seuil de support minsup est nul. Avec RepaMiner, dans le cas général, le seuil minsup impose une coupure dans le dendrogramme. On obtient alors plusieurs hiérarchies déconnectées, chacune ayant à son sommet un motif récursif maximal.
Visualisation du flux agglomératif à l'aide d'un RPgraph. Nous tirons profit des recouvrements entre les motifs récursifs maximaux de H pour construire une représentation plus synthétique, que nous nommons RPgraph (Recursive Pattern graph). Un RPgraph peut être vu comme une vue "de dessus" des dendrogrammes représentant chaque motif maximal. Cette vue nous prive de la hauteur d'agrégation, i.e. le support du motif, mais permet de bénéficier de davantage d'espace pour disperser les structures dans le plan. Nous profitons de cet espace pour réunir les dendrogrammes qui ont des intersections communes. Ce processus est illustré en figure 2.a. Au final, nous obtenons un graphe dont les noeuds représentent les motifs récur-sifs (soit un item, soit une agrégation), et les arcs relient les motifs agrégés. Par exemple, un motif récursif m = {x, y} est représenté par un noeud m qui est relié à x et y par deux arcs (x, m) et (y, m). Les arcs sont orientés vers m pour montrer le "flux agglomératif" découvert dans les données. Le support du motif récursif m est représenté par la largeur des deux arcs. Les couleurs des arcs servent uniquement à repérer les deux branches (x, m) et (y, m) d'une même agrégation, en leur donnant la même couleur.  
Conclusion
Nous avons proposé une méthode originale pour extraire un résumé compact, représentatif et intelligible des motifs fréquents dans des données transactionnelles ou séquentielles. Cette méthode repose sur la notion de motif récursif, et sur l'algorithme dynamique RepaMiner qui permet de réaliser l'extraction ascendante hiérarchique de ces motifs. Nous générons une véri-table structure nommée RPgraph qui permet de visualiser un "flux agglomératif" dans les données. Au final, RepaMiner peut être vu comme une Classification Ascendante Hiérarchique adaptée aux items (données transactionnelles) et aux évènements (données séquentielles). Ce travail se poursuit par des expérimentations qui montrent que le résumé généré est représentatif puisqu'il permet d'estimer précisément l'ensemble des itemsets ou épisodes fréquents.

Introduction
La classification recouvrante consiste à organiser de manière non-supervisée un ensemble d'individus en classes chevauchantes ou recouvrantes composées d'individus similaires. L' étude des méthodologies associées vise avant tout à répondre aux besoins réels et pratiques communs à de nombreux domaines d'application : qu'il s'agisse de classer des documents (textes, images, vidéos), de constituer des communautés de personnes sur des critères sociaux ou marketing ou encore d'exhiber des groupes de gènes ou de molécules présentant des caractéristiques structurelles ou fonctionnelles communes, il est très fréquents d'être confronté à des données qui s'organisent naturellement en classes recouvrantes. Dans ces applications, le recours à des techniques usuelles de classification stricte ou disjointe apporterait un biais préjudiciable à l'usage final de la classification.
Depuis plus d'une trentaine d'années, la classification recouvrante est identifiée comme une problématique à part entière et donne lieu à des avancées constantes au fil de l'évolution des techniques de classification traditionnelles. Partant des premiers travaux de Shepard et Arabie (1979) portant sur le clustering (recouvrant) additif, la problématique s'est ensuite orientée vers l'acquisition et la théorisation des classifications hiérarchiques recouvrantes ou empiétantes (Diday, 1987;Diatta et Fichet, 1994;Bertrand et Janowitz, 2003) avant d'être reconsidérée plus récemment et de façon plus formelle du point de vue "partitionnement" (Banerjee et al., 2005;Cleuziou, 2008;Depril et al., 2012). Parmi ces dernières avancées on notera d'une part les modèles additifs ALS (Depril et al., 2012) et son équivalent en terme de modèle de mélanges recouvrants MOC (Banerjee et al., 2005) et d'autre part le modèle OKM (Cleuziou, 2008) que l'on pourrait qualifier de modèle géométrique tant il modélise les intersections de clusters par un barycentre plutôt que par une somme des profils de clusters comme c'est le cas des modèles additifs précédents. Ces trois dernières méthodes se fondent sur un même cadre théorique qui consiste à explorer de manière efficace l'espace des solutions recouvrantes pour un nombre fixé de classes. Les algorithmes qui en découlent sont guidés par des critères objectifs quantifiant l'accumulation des imprécisions liées à l'affectation de chaque données à une classe ou à une combinaison de classes ; ce choix d'affectation posant lui même un problème combinatoire puisque le nombre de combinaisons possibles d'un ensemble fixé de classes est exponentiel.
Fort des avancées de cette dernière décennie dans le domaine de la classification recouvrante, nous nous intéressons à présent à la possibilité d'étendre ces modèles à l'utilisation de noyaux. La "kernélisation" des méthodes recouvrantes permettrait d'en étendre l'usage à des données de très grande dimensionalité (telles que les données textuelles), à des espaces non-euclidiens à l'origine, d'avoir recours aux mêmes procédés de projections que ceux qui ont fait leurs preuves en classification traditionnelle (non-recouvrante) et d'envisager de nouvelles avancées dans les domaines du clustering spectral ou semi-supervisé recouvrant (Dhillon, 2004;Filippone et al., 2008;Kulis et al., 2009) par exemple.
Dans cette contribution nous présentons tout d'abord les difficultés qui ont freiné l'évolu-tion des modèles recouvrants vers le clustering à noyaux (Section 2). Nous proposons ensuite en Section 3 une nouvelle modélisation ensembliste des recouvrements. Cette modélisation se base sur le modèle OKM et permet à la fois de corriger certains biais du modèle originel et de rendre possible le passage aux noyaux via un algorithme adaptatif également présenté dans cette section. Nous confirmons enfin les attentes liées à ce nouveau modèle par le biais d'une étude empirique préliminaire sur des jeux de données réels et artificiels (Section 4).
Problématique des noyaux en classification recouvrante
En clustering, l'astuce du noyau, consiste à réaliser le processus de classification (classiquement les réallocations successives) dans un espace induit par le noyau sans jamais calculer explicitement les projections des données de départ dans ce nouvel espace. Considérons X = {x 1 , . . . , x n } dans R p l'ensemble des données à traiter et K la matrice noyau induite par une projection implicite ?(.) telle que K i,j = i ), ?(x j ) A priori, la méthode des kmoyennes ne peut pas être "kernélisée" directement puisqu'elle considèrerait la minimisation du critère d'inertie suivant
et que ce dernier s'appuie sur k centre mobiles (modélisant les profils des clusters) {m c } k c=1 dont on ne doit pas calculer explicitement les projections ?(m c ). Cependant en observant que les centres mobiles sont redéfinis à chaque itération de façon optimale par les moyennes (centres de gravité) des individus de chaque classe, (Dhillon, 2004) ont proposé de se passer des variables associées à ces centres et d'intégrer leur définition dans le critère initial :
Cette astuce permet d'envisager un clustering totalement identique à k-moyennes dans n'importe quel espace induit par un noyau K, cependant il est important d'observer que cette transformation a un coût (raisonnement sur les paires d'individus).
Lorsque l'on tente de kernéliser les méthodes de classification recouvrante de type "réal-location dynamique" (e.g. ALS, MOC ou OKM), il semblerait naturel de procéder de façon similaire. Si l'on choisit -sans perte de généralité -le modèle OKM 1 , on serait amené à considérer le critère objectif suivant
où 1 xi??c = 1 si x i ? ? c et 0 sinon. Ce critère quantifie une somme de distances entre chaque individu x i et une combinaison (la moyenne dans OKM) des profils des clusters auxquels il appartient. Malheureusement, la définition d'un profil de cluster ne correspond plus à un simple centre de gravité : les profils dépendent les uns des autres ce qui rend impossible la réécriture à partir des seules données relationnelles du noyau (produits scalaires entre individus).
Ben N'Cir et Essoussi (2012) se sont intéressés à la kernélisation de OKM ; afin de contourner cette difficulté, ils ont proposé de conserver les variables de profils mais de limiter leur domaine de définition à l'ensemble des individus X, à la manière de médoïdes. L'individu choisi comme profil du cluster ? c sera celui qui minimise un critère d'inertie sur ? c :
On voit effectivement que les profils de clusters pourront ainsi être choisis à partir des informations issues du noyau uniquement et une fois ces profils déterminés, les auteurs montrent que l'heuristique utilisée dans OKM pour l'affectation peut être réalisée à l'identique dans l'espace de projection. Néanmoins le recours aux médoïdes est une solution coûteuse nécessitant de considérer toutes les paires d'individus de chaque cluster pour leur mise à jour. Enfin et surtout, la définition du médoïde (2) reste un choix arbitraire qui n'est pas induit par le critère initial (1) et ne garantit pas la convergence de l'algorithme qui en découle.
Dans la suite de l'article nous proposons une approche différente pour le problème du clustering recouvrant à noyau. Il s'agit d'une approche ensembliste qui diffère légèrement du modèle OKM originel en corrigeant certains de ses inconvénients mais surtout permettant l'utilisation des noyaux dans un processus recouvrant et convergeant.
3 OKSETS : modèle et algorithme
Le modèle de combinaison
Nous commencerons par l'observation de Depril et al. (2012) concernant la méthode ALS, qui est également valable pour toute méthode de classification recouvrante fondée sur une combinaison de profils de clusters : dans ces approches, la notion de "profils" de clusters ne correspond plus à l'idée intuitive de "centres" que l'on peut avoir dans les méthodes nonrecouvrantes de type k-moyennes ; en effet, l'optimisation du critère objectif conduit à des profils pouvant être éloignés des données des clusters qu'ils représentent. Plus précisément, Cleuziou (2008) montre par dérivation de (1) que dans OKM les profils peuvent être mis à jour l'un après l'autre de façon optimale avec la règle suivante : Nous proposons de profiter de la nécessité de faire abstraction des variables de profils dans le passage aux noyaux, pour corriger le phénomène de "sur-recouvrements" que nous venons d'évoquer. Pour cela nous commençons par introduire la notion de nuage dans une classification.
Définition 1 Étant donné un ensemble de clusters ? = {? 1 , . . . , ? q }, on appellera "nuage" de ? l'application N (.) qui lui associe l'union de ses extensions
On parlera également de "nuage" associé à un individu N (x i ) pour indiquer de façon analogue l'union des clusters auxquels il appartient
Nous définissons ensuite un nouveau critère objectif pour la classification recouvrante. Ce nouveau critère est défini sur la base d'une somme d'erreurs locales modélisées par les distances des individus au centre de gravité de leur nuage associé :
De cette manière on est assuré que l'affectation d'un individu à un cluster se réalise sur la base de sa distance au centre de gravité du cluster. De plus, un individu sera affecté à plusieurs clusters si le centre de gravité du nuage de cette combinaison est plus proche de cet individu. Enfin, on montre que ce critère est adapté à l'utilisation de noyaux :
et on observe que dans le cas d'un clustering non-recouvrant, chaque individu appartient à un seul cluster x i ? ? c ? N (x i ) = ? c , ce qui nous ramène exactement au critère objectif de l'algorithme des k-moyennes à noyaux (cf. Section 2). Le modèle K-OKSETS défini précédemment est donc une généralisation recouvrante du modèle des k-moyennes à noyaux.
L'algorithme adaptatif
Contrairement au contexte du k-moyennes à noyaux où l'on sait que le centre de gravité d'un cluster correspond à sa représentation optimale, dans le contexte recouvrant nous avons évoqué le fait que cela n'est plus vérifié. Ainsi un algorithme (batch) classique qui consisterait à réaffecter itérativement tous les individus avant de remettre à jour globalement tous les nuages n'assurerait pas la décroissance du critère (3) et n'aurait donc aucune raison de converger vers un recouvrement et des profils stables. Nous proposons donc un algorithme adaptatif guidé par le critère J K?OKSets (Figure 1). L'étape cruciale de l'algorithme réside dans la pro-
K-OKSETS
Entrées : X un ensemble de n individus, K une matrice noyau sur X, k le nombre de clusters attendu, {x 1 , . . . , L'heuristique d'affectation que nous utilisons consiste, pour un individu x i à : 1. Ordonner les clusters du plus proche au plus éloigné de x i selon la distance ?(
2. Affecter x i au premier cluster puis aux suivants tant que l'erreur locale (rappelée cidessous) diminue
3. Revenir à l'ancienne affectation si l'erreur locale n'est pas améliorée La structure de données sur laquelle se base l'algorithme conserve les informations utiles à l'ajout ou la suppression d'un élément de manière à simplifier les calculs d'erreurs locales et donc de l'erreur globale. Elle est organisée de manière arborescente où chaque noeud de l'arbre correspond à une combinaison de clusters et contient : l'ensemble des individus affectés à cette combinaison, le nuage associé ainsi qu'un score correspondant à la moyenne des produits scalaires sur les couples contenus dans le nuage : Figure 2, illustre une structure ainsi que la classification en 3 classes associée : par exemple l'individu x 6 apparaît à l'intersection des 3 clusters, on le retrouve donc dans le noeud d'éti-quette ? 1 ? ? 2 ? ? 3 dans lequel on trouve également l'information sur le nuage et le score associé à ce nuage. Ainsi la réaffectation d'un individu x i nécessitera les étapes suivantes :
1. Un premier parcours partiel de l'arbre pour supprimer x i du noeud auquel il est affecté, puis une mise à jour de tous les nuages et scores des noeuds prédécesseurs :
2. Un second parcours partiel pour l'heuristique d'ajout nécessitant les mises à jour inverses selon le même procédé.
Compte-tenu de l'augmentation exponentielle du nombre théorique de combinaisons de clusters avec le nombre de clusters k, la structure proposée n'est envisageable que sous l'hypothèse que seul un petit nombre de combinaisons est effectivement exploré par l'heuristique d'affectation et la condition que seule les combinaisons explorées soient stockées. Il n'est en effet pas déraisonnable de penser que des combinaisons de clusters éloignés ont peu de chances d'accueillir des individus qui sont affectés à leurs plus proches clusters d'après l'heuristique choisie. Nous observerons dans l'étude empirique qui suit les premiers gages de confirmation de cette hypothèse.
Étude empirique
Nous proposons dans cette section une première évaluation empirique du modèle de classification ensembliste OKSETS et de son utilisation dans le cadre du clustering à noyau. Nous proposons en Figure 3 différentes classifications d'un même jeu de données artificielles généré par deux gaussiennes légèrement chevauchantes de 500 individus chacune en deux dimensions. On observe en premier lieu (ligne supérieure de la Figure 3) que OKM et sa variante ensembliste génèrent des résultats comparables en terme de nature du recouvrement entre les deux classes (individus en noir) ; cependant OKSETS présente une "bande" de recouvrement plus étroite du fait de l'absence de profils de clusters mobiles. Les autres visualisations rendent compte des classifications obtenues par K-OKSETS avec différents noyaux : polynomial de degré 2 et gaussiens avec variances de 0.5 et 2 ; nous visualisons les classifications (et en particulier les recouvrements) à la fois dans l'espace initial et dans l'espace de projection approximé par un positionnement multidimensionnel (MDS) à partir des distances euclidiennes induites par le noyau. On notera de façon générale la cohérence des classes et recouvrement générés et en particulier qu'il semblerait possible de détecter par les recouvrements le contour des classes en utilisant un noyau approprié.
Dans un second temps, nous procédons à une évaluation dite "externe" des classifications obtenues. Il s'agit alors de mesurer l'adéquation entre la classification générée par l'algorithme de façon totalement non-supervisée et une classification de référence attendue par des experts du domaine. Nous utilisons trois jeux de données réelles : Iris, EachMovie et Scene.
-Iris (D.J. Newman et Merz, 1998)  visée ou non), il est constitué de 150 fleurs décrites selon 4 caractéristiques numériques, chacune des fleurs étant étiquetée par une seule des trois catégories Setosa, Versicolor ou Virginica, dont la première est réputée facilement identifiable, contrairement aux deux autres catégories plus mélangées. -EachMovies 2 est plutôt utilisé en classification "multi-étiquettes", il s'agit d'extraits de films décrits par les préférences d'utilisateurs (3 notes par film) et organisés en classes de genres où chaque film peut être associé à plusieurs genres. Nous utilisons ici un sousensemble de 3 genres correspondant à 75 extraits de films associés en moyenne à 1.14 genres chacun (taux de recouvrement). (k-moyennes à noyau) qui ne produisent pas de recouvrements avec OKM, OKSETS et KOKSETS qui génèrent des classifications recouvrantes. Toutes les mé-thodes ont été paramétrées avec un nombre de clusters attendus (k) égal au nombre d'étiquettes associées au jeu de données. Les résultats les plus remarquables pour chaque type de classification sont identifiés en gras dans le tableau. On observe sur Iris que, bien que la classification attendue soit non recouvrante, il est possible d'obtenir une meilleure adéquation en autorisant des recouvrements limités : en effet sans recours aux noyaux OKSETS (linéaire) obtient un score de 0.82 (FBCubed) légèrement supérieur au meilleur score obtenu sans recouvrements (0.81 pour KkMeans avec noyau Gaussien). Ceci s'explique en particulier par le chevauchement naturel entre les catégories Versicolor et Virginica qui conduit, lorsqu'on l'autorise, à un recouvrement ; en terme d'adéquation, mieux vaut attribuer un individu à deux classes plutôt qu'à une seule avec le risque de choisir la mauvaise. On notera que la projection opérée par un noyau polynomial n'est pas pertinente sur Iris, tandis que le noyau Gaussien permet d'amélio-rer la qualité des classifications ; en particulier le score obtenu par KOKSETS avec le noyau gaussien (? = 1) est intéressant dans la mesure où il égale le meilleur score tout en réduisant les recouvrements (1.07 contre 1.14).
Les tests réalisés sur les données EachMovie sont encore plus encourageants. La possibilité d'introduire les noyaux dans le clustering recouvrant conduit à un score inégalé de 0.69 obtenu en alliant la qualité à la fois en précision et en rappel, et avec un taux de recouvrement identique à la référence (1.14). Enfin, les expérimentations comparatives menées sur Scene valident sur un jeu de données plus important, l'intérêt de la modélisation ensembliste proposée dans OKSETS sans d'avantage de gain à attendre de l'utilisation de noyaux, que ce soit en partitionement ou en recouvrement.
Pour terminer cette première étude expérimentale, nous avons cherché à confirmer empiriquement l'hypothèse utilisée par l'algorithme KOKSETS selon laquelle le nombre de combinaisons (de clusters) explorées durant le processus de classification reste raisonnable par rapport à l'ensemble théorique des combinaisons possibles. La Figure 4 rend compte d'une première simulation qui tend à confirmer l'hypothèse : par exemple pour 15 clusters sur Iris, au maximum 52 combinaisons seront considérées et stockées dans la structure, contre plus de 32 000 combinaisons théoriques envisageables.  
Conclusion et perspectives
Nous avons abordé dans cette contribution la problématique de la classification recouvrante à travers la possibilité d'étendre de façon théoriquement bien fondée les modèles actuels vers le clustering à noyau. Nous nous sommes concentré pour le moment sur l'extension du modèle OKM en proposant une version ensembliste légèrement corrigée nommée OKSETS pour Overlapping k-Sets qui se prête aisément au passage aux noyaux via l'algorithme K-OKSETS (Kernelized-OKSETS). Les retours d'expériences ont permis de confirmer la faisabilité et l'intérêt de recourir aux noyaux en classification recouvrante.
Cependant, même si une solution théorique au problème est à présent effective et confirmée de manière pratique, une seconde phase d'étude sera indispensable afin de proposer une expé-rimentation plus étendue faisant intervenir d'avantage de jeux de tests, plus variés en terme de domaines et plus complexes en terme de recouvrements (nature et taille). Nous avons égale-ment observé que l'utilisation de noyaux offre les possibilités inattendues de réguler la taille des recouvrements et dans une certaine mesure d'utiliser les recouvrements pour détecter les contours des clusters ; il s'agira alors d'exploiter le potentiel de ces observations. Enfin, nous envisageons de la même manière d'adapter le modèle additif ALS aux noyaux puis, à moyen terme, de s'appuyer sur ces nouvelles avancées pour appréhender des problématiques nouvelles telles que le clustering spectral et semi-supervisé recouvrant.

Introduction
Les approches de bi-partitionnement sont devenues un sujet d'intérêt majeur en raison de leurs nombreuses applications dans le domaine de la fouille des données. Une méthode de bi-partitionnement, aussi appelée bi-clustering, co-clustering ou classification croisée, est une méthode d'analyse qui vise à regrouper des données en fonction de leur similarité. La straté-gie classique des méthodes de bi-partitionnement cherche à trouver des sous-matrices ou des blocs, qui représentent des sous-groupes de lignes et des sous-groupes de colonnes d'une matrice de données.
Un des objectifs d'une méthode de bi-partitionnement est la recherche d'un couple de partitions, l'une sur les observations (les lignes d'une matrice de données), l'autre sur les variables (colonnes d'une matrice de données), tel que la "perte d'information" due au regroupement soit minimale (Charrad et al., 2008) ; c'est-à-dire de sorte que la différence entre l'information apportée par la matrice de données initiale et celle apportée par le regroupement obtenu soit minimale. Depuis le premier algorithme de bi-partitionnement, appelé Block Clustering proposé par Hartigan (1972), de nombreuses techniques ont été proposées telles que l'énumération exhaustive (Tanay et al., 2002), l'analyse spectrale (Greene et Cunningham, 2010), les réseaux bayésiens (Shan et al., 2010) et d'autres (Angiulli et al., 2006). La pondération de variables est un processus couramment utilisé dans le domaine de l'apprentissage non supervisé, dont le but est de pondérer (ou sélectionner) des variables à partir d'une base de données en appliquant un algorithme d'apprentissage. La pondération de variables est difficile car, contrairement à l'apprentissage supervisé, les données ne sont pas étiquetées (Guyon et Elisseeff, 2003;Tsai et al., 2012). La taille des bases de données pose un défi sans précédent pour la fouille de données massives. Afin de remédier au problème de la grande dimension des variables, nous proposons dans un cadre de bi-partitionnement, une pondération des sous-ensembles de variables au lieu de pondérer les variables séparément.
Etat de l'art
Dans le domaine de la classification, bien que la plupart des méthodes utilisées cherchent à construire des partitions soit sur l'ensemble des observations soit sur celui des variables sé-parément. Il existe d'autres méthodes de bi-partitionnement qui considèrent simultanément les deux ensembles (Hartigan, 1972;Govaert, 1983;Nadif et Govaert, 2010;Ayadi et al., 2012). Les méthodes de bi-partitionnement utilisant les cartes auto-organisatrices (SOM) (Kohonen et al. (2001)) ont été définies par plusieurs auteurs (Busygin et al., 2002;Cottrell et al., 2004;Benabdeslem et Allab, 2012). Ce type de méthodes rentrent dans la catégorie des approches basées sur le partitionnement car souvent, elles utilisent des algorithmes de classification simple appliqués séparément sur les lignes et les colonnes d'une matrice des données. (Govaert, 1983) a défini un algorithme de bi-partitionnement nommé "Croeuc" pour les données quantitatives et qui consiste à déterminer une série de couples de partitions minimisant une fonction de coût sur la matrice des données en appliquant l'algorithme des nuées dynamiques. (Long et al., 2005) ont proposé une approche de décomposition matricielle "NBVD" (Non-negative Block Value Decomposition) pour le bi-clustering. Cette approche permet de décomposer une matrice de données en trois composantes en procédant par un algorithme itératif appliqué sur des données non négatives. Dans la même catégorie de méthodes, (Labiod et Nadif, 2011) ont proposé une approche de factorisation matricielle appelée "CUNMTF" (Co-clustering under Nonnegative Matrix Tri-Factorization). L'idée principale de cette approche est que la structure du bloc latent dans une matrice de données rectangulaire non négative est factorisée en deux facteurs plutôt que trois : la matrice des coefficients des lignes et la matrice des coefficients des colonnes qui indiquent respectivement le degré d'appartenance d'une ligne et d'une colonne à un cluster. On retrouve dans la littératures plusieurs approches de bi-partitionnement, qui utilisent des algorithmes hiérarchiques (Caldas et Kaski, 2011;Mao et al., 2005;Getz et al., 2000a). L'approche la plus utilisée dans cette famille de modèle est CTWC (Coupled two-way clustering) (Getz et al., 2000a). CTWC consiste à appliquer un algorithme de classification hiérarchique, le SPC "Super Paramagnetic Clustering (SPC)" (Getz et al., 2000b) sur les colonnes en utilisant toutes les lignes puis sur les lignes en utilisant toutes les colonnes.
Dans ce papier, une approche de pondération de blocs de variables en utilisant un modèle de bi-partitionnement, basée sur les cartes topologiques est proposée. Plusieurs méthodes de pondération de variables sont recensées dans la littérature scientifique. Nous trouvons des approches de pondération locale basées sur l'apprentissage non supervisé (Blansché et al., 2006;Frigui et Nasraoui, 2004) ainsi que sur les k-means (Huang et al., 2005). Il existe aussi des mé-thodes de sélection locale de caractéristiques basées sur l'apprentissage non supervisé (Basak et al., 1998;Liu et al., 2009;Grozavu et al., 2009;Chen et al., 2012). (Ouattara et al., 2013) proposent une extension des cartes topologiques pour le traitement des données multiblocs.
Dans ce papier, nous proposons une approche qui permet d'aborder le problème de la pondération de "blocs de variables" dans un cadre de bi-clustering topologique. Pour cela, nous nous basons sur l'algorithme BiTM (Biclustering using Topological Maps) (Chaibi et al., 2013). Notre modèle attribue à chaque bloc de variables un nouveau score de pondération constituant un vecteur des pondérations locale nommé F BR (Feature Block Relevance). La principale différence entre notre approche nommée FBR_BiTM et les méthodes existantes est que la pondération n'est pas associée à une seule variable, mais à un bloc de variables.
Bi-partitionnement et pondération des blocs de variables
Le modèle FBR_BiTM est constitué d'un ensemble de cellules discrètes C de taille K appelées "carte". Pour chaque paire de cellules (c, r) de la carte, la distance ?(c, r) est définie par le plus court chemin reliant les cellules r et c sur la grille. Soit d l'espace euclidien des données et A la matrice des données où chaque observation
. L'objectif de FBR_BiTM est de fournir des bi-clusters organisés dans une carte topologique et un vecteur des pondérations locales de chaque bloc de variables. Dans FBR_BiTM, chaque cellule c de C est associée à un prototype sous la forme d'un vecteur :
pour sauvegarder les informations associées respectivement aux observations et aux variables.
Le modèle que nous proposons dans ce papier se base sur la formulation du modèle BiTM (Chaibi et al., 2013) en introduisant le nouveau paramètre f l r qui sera estimé au cours de l'apprentissage. Ainsi nous proposons de minimiser la nouvelle fonction de coût suivante :
Cette fonction de coût peut être réécrite de la manière suivante :
Où :
. . , g k } désigne l'ensemble des vecteurs prototypes. F = {f 1 , . . . , f k } représente l'ensemble des vecteurs de pondération. ? z est la fonction d'affectation des lignes. ? w est la fonction d'affectation des colonnes. K T (?(r, k)) est la fonction de voisinage. En pratique, nous utilisons la fonction de voisinage suivante :
T où T représente le paramètre contrôlant le rayon du voisinage.
La minimisation de J (? w , ? z , G, F ) se fait d'une manière itérative avec la version nuées dynamiques par l'exécution de 4 étapes jusqu'à un nombre d'itérations prédéfini (algorithme 1). De la même manière que les cartes topologiques, on fait décroître le rayon d'apprentissage pour constituer deux phase : une phase d'auto-organisation associée aux grandes valeurs et une phase de quantification associée aux petites valeurs.
Expérimentations
Nous avons testé l'algorithme FBR_BiTM avec des jeux de données du répertoire UCI (Frank et Asuncion (2010) Phase itérative 1-Affectation des observations : chaque observation x i est affectée au prototype g k le plus proche en utilisant la fonction d'affectation :
2-Affectation des variables : chaque variable x j est affectée au prototype le plus proche en utilisant la fonction d'affectation :
3-Mise à jour des prototypes : les composantes g l r des prototypes sont mis à jour suivant la formule ci-dessous :
4-Mise à jour des pondérations : les composantes f l r des vecteurs des pondérations sont mis à jour suivant la formule ci-dessous :
RÉPÉTER les phases 1, 2, 3 et 4 jusqu'à t = t max . TAB. 2 -Description des jeux de données simulées.
Protocole de validation
Afin de comparer FBR-BiTM avec les approches de bi-partitionnement, nous avons sé-lectionné les approches suivantes : BiTM (Chaibi et al. (2013)), CTWC (Getz et al. (2000a)), NBVD (Long et al. (2005)). Les résultats expérimentaux sont présentés dans les tableaux 3 et 4. Nous avons choisi la taille des cartes FBR-BiTM et BiTM selon l'heuristique de Kohonen. Le nombre de clusters des variables (colonnes de la matrice A) est exactement le même pour l'ensemble des approches FBR-BiTM, BiTM, CTWC, NBVD. Cependant, pour le nombre de clusters des observations (lignes de la matrice A), nous avons pris la même taille des cartes 
Comparaison des performances de FBR-BiTM avec les approches de bi-partitionnement
Le tableau 3 résume les résultats expérimentaux de l'indice de pureté. Nous remarquons que FBR-BiTM fournit des résultats équivalents à ceux de l'approche BiTM. Dans la plupart des cas, nous constatons des résultats comparable et souvent meilleur avec notre approche BiTM ou FBR-BiTM. Nous observons aussi la difficulté d'obtenir de grandes valeurs de l'indice de pureté pour la base isolet5. Le tableau 4 présente l'indice de rand. Nous observons que FBR-BiTM fournit un indice de rand meilleur que celui des autres approches dans 4 bases sur 9. Les résultats de FBR-BiTM restent compétitifs et équivalents aux résultats obtenus avec les autres approches. Nous constatons après cette étude comparative, que notre approche FBRBiTM est une méthode qui ne perturbe pas le bi-partitionnement topologiques BiTM. 
Cas particulier : application aux bases de données simulées binaires
Dans les bases de données réelles, il est très difficile d'obtenir les étiquettes des classes des variables. Afin de valider le clustering des variables de notre modèle FBR-BiTM, nous avons utilisé des bases de données simulées étiquetées en lignes (observations) et en colonnes (variables) décrites dans le tableau 2. Les tableaux 5 et 6 montrent les résultats obtenus avec les indices de pureté et de rand pour le partitionnement des observations et des variables.
Nous constatons à travers les 2 indices de performance que notre approche est meilleure ou équivalente à BiTM dans le cas du clustering des observations dans la plupart des bases de données. Cependant, nous remarquons une légère baisse des performances de FBR-BiTM au niveau du clustering des variables. en fonction des groupes de lignes et de colonnes. Cette organisation est très claire dans le cas des bases binaires (voir la figure 1(a)).
Résultats visuels
Ces figures peuvent être obtenues par toutes les méthodes de bi-partitionnement. Cependant, en utilisant cette visualisation, il est difficile d'analyser les blocs ou les bi-clusters obtenus. Afin de faciliter cette tâche, nous proposons de visualiser les bi-clusters en utilisant l'organisation topologique du modèle FBR-BiTM. Ainsi, chaque cellule de la carte est associée au cluster des observations et des variables. Cette organisation est illustrée par les figures 1(b) et 2(c) en organisant les cellules selon l'ordre des blocs de variables obtenus. Dans le cas de la base Lung Cancer par exemple, la figure 2(b) représente la carte topologique associée au modèle FBR-BiTM. Cette figure représente la topologie des groupes obtenus en appliquant l'algorithme FBR-BiTM. Nous remarquons une répartition des données au niveau de chaque cellule. Plus la couleur est rouge, plus les variables ont de fortes valeurs. Nous avons organisé la carte selon les blocs de variables obtenus. Le résultat est illustré dans la figure 2(c). Dans la première cellule par exemple, nous remarquons que les variables ont changé de disposition de manière à créer une organisation au niveau de la cellule. Nous constatons clairement dans cette première cellule (en haut à gauche de la carte) que les blocs de variables se comportent différemment à l'intérieur de cette cellule. Ce comportement est illustré par une couleur. Plus la couleur est rouge, plus le bloc de variable tend vers de fortes valeurs. Dans ce cas, nous remarquons que les premiers blocs de variables (totalement à gauche de la cellule) ont une couleur plutôt rouge. Par contre, le second bloc (au milieu de la cellule) est moins "important" car il est constitué de variables d'une couleur bleu. Enfin, le troisième bloc (totalement à droite) est constitué des variables moyennement importantes (couleur verte). Nous nous sommes focalisés dans cette analyse sur la base Lung Cancer. En fait, cette analyse peut être également réalisée sur les autres bases de données.
Distribution des blocs de variables : la figure 2(  

Introduction
Dans cet article, nous proposons une solution originale à un problème classique de bases de données qui consiste à prédire/estimer les valeurs manquantes dans une base de données relationnelle incomplète. De nombreuses approches ont été proposées pour traiter cette question, à la fois dans la communauté des bases de données et dans celle de l'apprentissage automatique, fondées sur des dépendances fonctionnelles (Atzeni et Morfuni (1986)), des règles d'association (Ragel (1998)), des règles de classification (Liu et al. (1997)), des techniques de clustering (Fujikawa et Ho (2002)), etc. Nous explorons quant à nous une nouvelle idée, issue de l'intelligence artificielle, qui consiste à exploiter les proportions analogiques (Prade et Richard (2012)) pouvant exister dans les données.
La suite de l'article est organisée comme suit. Dans la section 2, nous rappelons les notions de base concernant les proportions analogiques. La section 3 présente le principe général de l'approche que nous proposons pour estimer les valeurs manquantes, inspirée par la technique de classification proposée dans (Bayoudh et al. (2007); Miclet et al. (2008)). La section 4 est consacrée à une expérimentation visant à évaluer les performances de la méthode et à comparer cette dernière avec une technique classique d'estimation (kNN). Finalement, la section 5 rappelle les contributions principales de l'article et trace quelques perspectives de recherche.
Rappels sur les proportions analogiques
La présentation qui suit est tirée principalement de Miclet et Prade (2009). Une proportion analogique est une proposition de la forme « A est à B ce que C est à D », ce qui sera noté : Comme noté dans (Prade et Richard (2013)), l'idée de proportion est fortement liée à celle d'extrapolation, autrement dit à l'objectif de deviner/calculer une nouvelle valeur à partir de valeurs existantes, ce qui est bien le but que nous nous fixons ici.
3 Principe de l'approche 3.1 Idée générale L'approche que nous proposons s'inspire d'une méthode de « classification par analogie » introduite dans (Bayoudh et al. (2007)), où les auteurs décrivent un algorithme appelé FA-DANA. Ce dernier utilise une mesure de dissimilarité analogique entre quatre objets, qui estime dans quelle mesure ces objets sont loin d'être en proportion analogique. En deux mots, la dissimilarité analogique ad entre quatre valeurs booléennes est le nombre minimal de bits qui doivent être modifiés pour obtenir une analogie valide. Par exemple ad(1, 0, 1, 0) = 0, ad(1, 0, 1, 1) = 1 et ad(1, 0, 0, 1) = 2. Ainsi, en désignant par A la relation quaternaire de proportion analogique, on a :
Lorsque, au lieu d'avoir quatre valeur booléennes, on manipule quatre vecteurs booléens dans B n , il faut ajouter les évaluations ad obtenues pour chaque composante de façon à obtenir la dissimilarité analogique entre les vecteurs, ce qui conduit à un entier dans l'intervalle [0, 2n]. L'algorithme, qui prend en entrée un ensemble d'apprentissage S d'éléments déjà classifiés, un nouvel élément d à classifier, et un entier k, procède comme suit :
2. Tri de ces n triplets par valeur croissante de leur ad. 3. Si le k e triplet a la valeur entière p pour ad, alors notons q le plus grand entier tel que le q e triplet a la valeur p. 4. Résolution des q équations analogiques sur l'étiquette de la classe. On retient le vainqueur des q votes, que l'on affecte comme étant la classe de d.
Application à la prédiction de valeurs inconnues 3.2.1 Cas des attributs booléens
Cette méthode peut être adaptée au cas de la prédiction de valeurs nulles dans une base de données transactionnelles de la façon suivante. Soit une relation r de schéma (A 1 , . . . , A m ) et t un n-uplet de r comportant une valeur manquante pour l'attribut A i : t[A i ] = NULL. Pour prédire la valeur de t[A i ] qui est 0 ou 1 dans le cas d'une base de données transactionnelle, on applique l'algorithme précédent en considérant que A i correspond à la classe cl à déterminer. L'ensemble S d'apprentissage correspond à un échantillon (d'une taille fixée à l'avance) des n-uplets de la relation r (privée de l'attribut A i qui n'intervient pas dans le calcul de ad mais représente la « classe ») ne comportant aucune valeur manquante. Par ailleurs, on ignore les attributs A h , h = i tels que t[A h ] = NULL lors du calcul visant à prédire la valeur de t[A i ].
Cas des attributs numériques
Dans le cas d'attributs numériques, une première solution consiste à se ramener au cas booléen : un attribut numérique A est dérivé en autant d'attributs booléens qu'il y a de valeurs dans le domaine actif de A. Cette solution peut cependant paraître discutable car la binarisation conduit à se limiter à des cas d'analogie assez limités (égalité ou non égalité).
Une deuxième solution consiste à rechercher des analogies de type
la première étant appelée proportion géométrique et la seconde proportion arithmétique. Ces définitions peuvent être raffinées en introduisant une certaine tolérance sur les relations d'analogie, de façon à couvrir plus de cas. Ainsi on peut considérer que (100 : 50 :: 80 : 39) est presque vrai (on a 39 au lieu de 40).
Une façon de procéder est d'adopter une vision graduelle de la dissimilarité analogique, et de considérer que la valeur de AD n'est plus un entier mais un réel. Dans le cas d'une proportion géométrique (formule (1)), pour l'attribut A i , on peut ainsi ajouter à AD la valeur
Dans cette formule, la multiplication par 2 a pour but de rendre la pénalité commensurable avec celle appliquée dans le cas booléen (où la dissimilarité analogique peut prendre l'une des valeurs 0, 1 ou 2). Notons que si max(ad, bc) = 0, cette formule est inapplicable et l'on doit alors se limiter à rechercher une proportion arithmétique (formule (2)).
Dans le cas d'une proportion arithmétique, une solution est de définir, pour chaque attribut numérique, une fonction d'appartenance associée au terme flou « proche de ». Pour un attribut donné A i , on peut définir une fonction trapézoïdale µ Ai de telle sorte que :
La pénalité à appliquer se définit alors par :
Expérimentation préliminaire
Le principal objectif de l'expérimentation qui a été menée est de comparer les résultats obtenus à l'aide de cette technique avec ceux produits par l'approche classique des plus proches voisins, donc d'estimer son efficacité relative en termes de précision (i.e., de pourcentage de valeurs correctement prédites). Nous détaillons ici uniquement des résultats expérimentaux portant sur des bases de données transactionnelles, l'extension au cas d'attributs numériques étant actuellement en cours.
Un ensemble de données de plus de 50 000 n-uplets contenant 20 attributs sur des accidents de la route est utilisé (Geurts et al. (2003)). Un échantillon E est extrait en choisissant 1000 n-uplets de la relation au hasard. Un sous-ensemble M de E est soumis à des modifications, autrement dit, un certain pourcentage de valeurs de chacun de ses n-uplets est remplacé par NULL. Ensuite, l'algorithme FADANA est exécuté pour prédire les valeurs manquantes : pour chaque n-uplet d possédant des valeurs manquantes, un échantillon aléatoire D d'une taille fixée de l'ensemble E ? M (donc complètes) est choisi. À chaque fois, la méthode des k plus proches voisins (kNN) a été aussi utilisée, en prenant le même nombre de n-uplets et la même valeur de k. Rappelons que la méthode kNN est fondée sur un calcul de distance entre le tuple à compléter et les tuples de l'ensemble d'apprentissage (on ne retient que les k plus proches, et une procédure de vote, analogue à celle de FADANA, permet de prédire la valeur manquante).
On a cherché également à évaluer la proportion  83,5 84,16 84,33 83,16 84,16 83,83 84,16 kNN 82,83 84,16 83,83 83,66 84 83,33 83,33 FADANA & kNN 74,66 76,66 77,33 76 77 75,83 76,16 5,66 6,33 5,5 6,33 5,83 5,83 5 6,66 5,33 5,66 5,33 9,33 10,33 10 8,33 9,83 9,66 Le tableau 1 montre comment varie la précision en fonction de la valeur de k utilisée dans l'algorithme. On constate une remarquable stabilité, aussi bien pour FADANA que pour kNN, et l'on peut voir que même avec une valeur de k assez petite, le vote conduit à des résultats corrects dans la grande majorité des cas.
TAB. 2 -Évolution de la précision en fonction de la taille de l'ensemble d'apprentissage (proportion de tuples modifiés : 70%, k = 40, ratio de valeurs modifiées par tuple : 40%) Le tableau 2, quant à lui, montre l'impact que la taille de l'ensemble d'apprentissage a sur la précision. On constate, ce qui n'est guère surprenant, qu'un trop petit ensemble affecte négativement les performances, mais qu'à partir de 30 ou 40 tuples, on atteint pour FADANA un niveau de précision quasiment optimal (autour de 85%). Notons qu'il est illusoire d'espérer atteindre 100% puisqu'il existe en général des tuples qui ne sont en relation de proportion analogique avec aucun triplet de la relation initiale.
Un résultat intéressant est qu'il existe un pourcentage non négligeable (autour de 6%) de valeurs qui sont prédites correctement par FADANA mais pas par kNN, et réciproquement. Ceci permet d'envisager d'utiliser une méthode hybride, qui utiliserait FADANA dans la plupart des cas, mais basculerait vers kNN pour prédire les valeurs dont on peut prévoir qu'elles seront incorrectement estimées par FADANA. L'intuition qu'on peut avoir est que ces dernières se caractérisent par des valeurs élevées de dissemblance analogique ad dans la liste construite à l'étape 2 de l'algorithme (voir sous-section 3.1) mais ceci reste à confirmer expérimentalement.
Conclusion
Dans cet article, nous avons présenté une méthode originale de prédiction de valeurs manquantes dans les bases de données relationnelles, fondée sur la notion de proportion analogique. Nous avons également montré comment un algorithme proposé dans le cadre de la classification automatique pouvait être adapté à cette fin. Les résultats obtenus, quoique préliminaires, apparaissent encourageants, puisque l'approche conduit à une précision meilleure en moyenne que celle de la technique classique des plus proches voisins.
Il conviendra notamment, dans des travaux futurs, de i) comparer l'approche de prédiction par analogie, au delà de kNN, avec d'autres approches de la littérature ; ii) traiter de façon plus raffinée les attributs catégoriels en prenant en compte des notions telles que synonymie, hyponymie/hypernymie, etc. iii) étudier la façon dont on doit traiter les valeurs prédites lors du processus d'interrogation de la base de données. Cela nécessitera certainement d'utiliser un

Introduction
Les forums de santé en ligne sont des espaces d'échanges où les patients, sous couvert d'anonymat, relatent très librement leurs expériences personnelles. Ces ressources s'avèrent très riches pour les professionnels de santé qui ont accès à des échanges entre patients, entre patients et professionnels et même entre professionnels. Même si tous les patients ne s'expriment pas dans les forums de santé, ces derniers représentent une base volumineuse et variée des connaissances et des perceptions qu'ont les patients de leur maladie et des soins qui leur sont éventuellement prodigués. Dans le cadre du projet Parlons de nous 1 , nous cherchons à associer différents marqueurs (émotions, risques, incertitudes, etc.) à des objets médicaux (mé-dicaments, traitements, etc.) pour identifier des cooccurrences fréquentes (e.g. une association de type Médiator et peur). Dans cet article, nous nous focalisons sur l'identification des émo-tions. Si de nombreuses approches ont été proposées pour l'analyse de la polarité des textes (positif et négatif ), on trouve peu d'approches pour l'analyse des sentiments (joie, colère, tristesse, etc.). Nous avons utilisé le lexique des mots d'émotions de (Mohammad et Turney, 2010) pour annoter automatiquement un corpus de messages. Une sous partie de ce corpus a été annotée manuellement. L'étude de l'accord entre ces annotateurs nous a permis de montrer qu'il était difficile, même pour des humains, d'associer une émotion précise à un message. Nous avons donc décidé de donner deux informations aux professionnels de santé : la polarité du texte (positive ou négative) et les émotions associées. Pour obtenir ces deux informations, nous avons travaillé sur la recherche des meilleurs descripteurs. Des expérimentations sur des jeux de données réelles ont montré l'efficacité de cette approche et des discussions avec les professionnels de santé ont montré l'intérêt médical d'identifier de telles informations.
État de l'art
Depuis le début des années 2000, l'analyse de sentiments, également appelée fouille d'opinions (opinion mining), a connu un intérêt croissant. Beaucoup de communautés se sont inté-ressées à ce domaine et ont donné des définitions et interprétations variées (e.g. psychologie, sciences sociales, linguistique computationnelle, traitement automatique du langage, fouille de données, etc.). L'analyse de sentiments vise l'extraction des états affectifs exprimés explicitement ou implicitement dans des textes (Liu, 2012). Elle englobe les tâches suivantes : 1) l'analyse de subjectivité porte sur la détection de la présence de sentiments via l'identification d'expressions ou des mots dit subjectifs ; 2) l'analyse de polarité porte sur la détection de la polarité positive, négative ou neutre des textes ; 3) l'analyse des émotions porte sur la catégorie émotionnelle du texte (e.g. colère, dégoût, peur, etc.) ; 4) l'analyse d'intensité porte sur les différents niveaux d'intensité de la polarité et de l'émotion (e.g. très positif, très triste, etc.). Ces approches offrent une granularité plus précise sur les opinions et les émotions exprimées. Dans ce travail, nous nous focalisons sur la troisième tâche. Comme la plupart des méthodes semiautomatiques de la littérature, nous utiliserons la typologie des émotions de (Ekman, 1992) qui décrit six émotions.
Les méthodes appliquées pour analyser les sentiments sont très nombreuses et générale-ment, spécifiques aux types des textes : aux tweets (Roberts et al., 2012), aux titres de presse (Strapparava et Mihalcea, 2008), etc. et aux domaines d'application : l'analyse de media sociaux (Balahur, 2013) ou l'identification de mails suicidaires (Pestian et al., 2012).
Quelle que soit la tâche d'analyse de sentiments étudiée (polarité et émotions), la plupart des travaux porte soit sur la création de ressources permettant de décrire les sentiments, soit sur l'utilisation de ces ressources pour classifier des textes selon les sentiments étudiés. Dans la première catégorie de travaux, la plupart des méthodes associe les mots des textes à des termes appartenant à des ressources préalablement annotées par des sentiments. La plupart des ressources ont été construites pour l'anglais et l'analyse de la polarité (e.g. General Inquirer (Stone et al., 1966)). Toutefois des ressources plus spécifiques, comme le lexique de (Mohammad et Turney, 2010) ont été créées pour les mots chargés d'émotions. Pour la classification proprement dite, la plupart des approches utilisent des techniques d'apprentissage basées sur des attributs spécifiques incluant les mots d'émotions (Strapparava et Mihalcea, 2008) pour construire un modèle statistique à partir d'un corpus de textes et l'utiliser pour la détection des sentiments dans d'autres textes. Si beaucoup de ces méthodes s'avèrent efficaces sur des corpus de textes importants, elles se retrouvent limitées dans le cas des textes courts comme les tweets ou spécifiques comme les forums de santé. sur des émotions portant sur des objets médicaux, nous avons utilisé le MESH 2 pour repé-rer des entités médicales et nous avons filtré 6% des messages n'en contenant pas. Dans un message, plusieurs émotions sont généralement exprimées du fait de sa longueur. Nous avons donc choisi de segmenter les messages en phrases et gardé 3 000 phrases pour constituer un Corpus Annoté Automatiquement (CAA). Toutes les phrases contenant plusieurs émotions ont été étiquetées par l'émotion majoritaire. Un sous-ensemble de ce corpus (600 phrases) a été annoté manuellement par 60 non professionnels de santé 3 . Nous le notons CAM (Corpus Annoté Manuellement).
Pour évaluer l'accord entre les annotateurs, nous avons utilisé la mesure Kappa. 150 phrases issues du CAM ont été annotées par deux annotateurs non professionnels (Kappa égal à 0, 26 soit accord très faible) et par un annotateur professionnel de santé et un non professionnel (Kappa de 0, 46 soit accord modéré). Cette expérimentation préliminaire souligne la difficulté de la tâche d'annotation manuelle. Par ailleurs, le désaccord entre annotateurs est essentiellement dû à la variabilité entre personnes et non à leur sensibilité au domaine de la santé. Un premier biais consiste à ne considérer que le point de vue de l'annotateur qui est parfois très différent de celui de l'auteur. En effet, les messages des forums de santé traitent de la maladie, des traitements, etc. Ces informations sont par nature négatives et l'annotateur aura, par empathie, tendance à associer une émotion telle que la tristesse à une information factuelle comme la description d'un diagnostique. Un deuxième biais réside dans le fait que le corpus est rédigé en langue anglaise, alors que les annotateurs sont des français natifs. Par ailleurs, en étudiant les phrases ayant suscité des écarts d'annotations, nous avons remarqué qu'il est plus facile d'identifier la polarité que de trouver l'émotion elle même. Il est également plus facile de prédire les émotions positives car les émotions négatives partagent un vocabulaire très proche. Nous avons noté également que la surprise est la plus difficile à identifier. La qualité de notre corpus annoté est finalement assez discutable mais cette étude donne une bonne intuition des difficultés liées à l'obtention d'un corpus de qualité et de la méthodologie à suivre pour améliorer sa qualité. Dans la suite, à partir de ces différents constats, nous avons décidé d'évaluer différentes méthodes permettant de caractériser un texte extrait d'un forum en nous basant sur : 1) une classification bi-classes pour identifier la polarité des émotions (les émotions positives correspondant à la joie, les émotions négatives à la colère, la peur, la tristesse et le dégoût). L'émotion surprise a été éliminée du fait de sa neutralité. 2) une classification multi-classes pour les 6 émotions : une phrase ne peut être associée qu'à une seule classe d'émotion ; 3) une classification multi-labels nous permettant d'associer une phrase à plusieurs classes d'émotions.
Protocole expérimental
Prétraitements : les messages dans les forums contiennent des mots qui ne se retrouvent pas forcément dans les dictionnaires classiques (argot, mise en forme particulière, abrévia-tions, émoticons, etc.). Il est donc nécessaire de les normaliser en généralisant leur contenu. Pour cela, nous avons appliqué les prétraitements correspondant à la chaîne mise en place par (Balahur, 2013)  Classification : nous avons utilisé les attributs ci-dessous afin de trouver les meilleurs descripteurs des émotions : 1) les attributs basés sur les N-Grammes (U,U+B) ; 2) les mots d'émotions (ME) : si une phrase contient deux mots correspondant à l'émotion joie, elle prendra la valeur 2 pour cet attribut ; 3) les smileys (SMI) : l'ensemble des émoticons ( :-), :-( ...) a été classé selon les six émotions. Si une phrase contient un smiley correspond à la joie, elle prendra la valeur 1 pour cet attribut ; 4) les intensifieurs (INT) : ponctuations ( ! !, ? ?, etc.), lettres répétées (looool) et mots en majuscules (HATE). Si une phrase contient un intensifieur, elle prendra la valeur 1 pour cet attribut ; 5) le contexte de l'émotion (CONT) : nous utilisons deux attributs que nous appelons émotion voisine et globale. Une phrase prend la valeur vraie pour cet attribut si une phrase qui l'entoure exprime la même émotion et pour l'émotion globale on considère tout le message ; 6) les patrons : en nous inspirant de l'approche de (Béchet et al., 2013), nous enrichissons les attributs en utilisant des patrons obtenus en appliquant un algorithme de recherche de motifs séquentiels. Pour cela, nous utilisons le thésaurus médical MeSH pour identifier des mots d'émotion, le lexique des mots d'émotions pour identifier des traces d'émotions et un lemmatiseur pour obtenir la catégorie grammaticale des mots. Chaque phrase est alors considérée comme une séquence d'itemsets correspondant à une combinaison de ces trois informations. Nous avons ensuite utilisé l'algorithme GSP (Zhang et al., 2002) pour obtenir les sous séquences fréquentes. Nous n'avons conservé que celles contenant au moins une entité médicale et un mot d'émotion. Ces motifs ont ensuite été utilisés comme attributs. Une phrase est étiquetée vraie pour un patron si sa forme syntaxique respecte le patron.
Évaluation : la qualité de la classification bi-classes est évaluée en utilisant les mesures classiques de précision P , rappel R et la F-measure F . Pour la classification multi-classes, nous calculons à la fois la moyenne au niveau micro F mi et macro F ma . Pour la classification multi-labels, on utilise alors d'autres mesures comme le Hamming loss HL, l'exactitude A et la F-mesure au niveau macro F ma .
Mise en oeuvre : nous utilisons les implémentations de Weka 4 pour la classification biclasses et multi-classes et Meka 5 pour la classification multi-labels. Nous utilisons SVM comme classifieur avec sa mise en oeuvre SMO dans Weka en utilisant les paramètres par défaut. Nous utilisons le classifier chain CC mis en oeuvre dans Meka pour la classification multi-labels. Nous avons utilisé deux jeux de données : le corpus CAM et le corpus CAA. Nous réalisons une validation croisée (à 10 folds).
Résultats et discussions
La table 1 présente les résultats obtenus sur le corpus CAM. Nous ne présentons pas les résultats obtenus sur le corpus CAA qui sont similaires. La classification bi-classes donne les meilleurs résultats, ce qui semble relativement cohérent car cette tâche s'est également avérée plus facile pour les annotateurs humains. La classification multi-labels présente de meilleurs résultats que la classification multi-classes moins efficace lorsqu'un exemple peut être associé à plusieurs classes. Par ailleurs, nous constatons le peu de différence de la F-mesure micro et macro pour la classification multi-classes, qui semble suggérer que toutes les classes sont aussi difficiles à identifier. Ces résultats sont à mettre en parallèle avec l'accord interannotateurs (voir section 3). Dans les deux cas, la tâche est difficile mais les méthodes semi-automatiques semblent détecter des régularités plus systématiquement sauf cas particuliers comme par exemple l'ironie. On peut également conclure que les meilleurs descripteurs sont les unigrammes combinés aux bigrammes et aux mots d'émotions (U+B+ME). La prise en compte des smileys et des intensifieurs n'améliore pas la classification. En effet, ils sont souvent utilisés à des fins d'ironie que nous ne captons pas en regardant uniquement leur présence dans la phrase. De même, le contexte n'est pas un attribut intéressant. En effet, un message est souvent long (7 phrases en moyenne dans notre corpus) et contient de nombreux sentiments (jusqu'à 6 émotions dans 41% des messages). Deux phrases consécutives contiennent donc très souvent des émotions différentes non corrélées. Pour finir, les patrons se sont également avérés peu efficaces car non définis pour chaque classe. Il est important de noter que, quand les différents classifieurs se trompent, ils placent souvent l'exemple dans une classe "proche" appartenant à la même polarité. Ces mauvaises prédictions sont dues au fait que les classes partagent de nombreux mots (comme colère, dégoût et tristesse). Les ressources utilisées étant des dictionnaires et un lemmatiseur, la méthode pourra être reproduite pour d'autres langues en utilisant des ressources similaires.
Bi-classes
Multi 
Conclusions et perspectives
Nous avons décrit dans cet article une méthode d'analyse d'émotions dans les messages des forums de santé. La principale difficulté a résidé dans l'acquisition de données annotées et cette étape devra encore être améliorée. Pour l'extraction des émotions, nous avons comparé différents attributs, pour différentes tâches de classification (bi-classes, multi-classes et multilabels) et montré que les plus efficaces étaient les unigrammes combinés aux bigrammes et aux mots d'émotions pour la classification bi-classes. Toutefois, suggérer une étiquette, malgré la précision obtenue, semble pertinent aux professionnels de santé impliqués dans cette étude. Les perspectives associées à ce travail sont nombreuses. Du point de vue de la tâche d'analyse des émotions, nous allons appliquer notre méthode sur des jeux de données plus importants et non spécifiques à la santé, comme ceux du challenge SEMEVAL 6 . Nous prendrons également en compte les "inverseurs de sens" (shifters). Nous validerons la généricité de notre approche sur un corpus en français. En effet, notre méthode repose uniquement sur des lexiques et un outil de lemmatisation. Nous avons également identifié des perspectives liées au domaine d'application. Le forum spine-health est spécialisé dans la thématique "douleur" et la pathologie 6. http://www.cse.unt.edu/ rada/affectivetext/

Introduction
Avec l'arrivée du Web 2.0, on assiste à un foisonnement de services de réseautage social, qui mettent l'utilisateur, en tant que créateur de contenu, au centre des préoccupations. Ces services permettent de partager des ressources comme des vidéos (YouTube), des photos (Flickr) ou des ressources annotées (Del.icio.us), d'échanger des informations et de construire des relations personnelles ou professionnelles (Facebook, LinkedIn) ou encore de diffuser des news (Twitter, blogs). Les utilisateurs disposent ainsi de plusieurs espaces d'informations sur différents réseaux sociaux, où ils partagent des informations personnelles telles que leurs noms et prénoms, leur lieu géographique, leur âge, leurs adresses électroniques, leurs numéros de téléphone, leurs relations, les institutions fréquentées, etc. (Gross et Acquisti (2005); Little et al. (2011);Stutzman (2006)). Selon la politique d'accès aux données définie par le réseau social, certaines informations sont par construction publiques ou semi-publiques (accessibles aux autres utilisateurs du réseaux social), pour d'autres c'est l'utilisateur qui choisit de restreindre l'accès à ses contacts ou d'ouvrir l'accès à tous.
Au vu de la diversité des réseaux sociaux, ces espaces permettent l'accès à une grande quantité d'informations en puisant dans des ressources très variées, complémentaires et parfois insoupçonnées, qui pourraient surprendre l'utilisateur lui-même s'il en connaissait réellement toute l'étendue. Réconcilier les différentes profils d'un utilisateur à travers ses divers réseaux sociaux lui permet de construire un espace d'informations global afin de mieux gérer et protéger ses données. Le principal problème réside dans le fait que ces informations sont souvent incomplètes, ambiguës, non mises à jour voire fausses. Il existe déjà, sur le Web, des outils agrégateurs comme FriendFeed Dans cet article, nous proposons une approche de réconciliation de profils des utilisateurs à travers ses différents réseaux sociaux en exploitant la topologie de l'ensemble des réseaux sociaux interconnectés et les informations publiques fournies par les utilisateurs dans leurs profils. Notre approche repose sur l'observation que, d'une part, les réseaux sociaux sont interconnectés grâce aux liens transversaux que certains utilisateurs déclarent sur leurs différents profils (Golbeck et Rothstein (2008)  Carmagnola et Cena (2009) ont introduit la notion de facteur d'importance afin de pondérer la contribution de chaque attribut pour déterminer la similarité de deux profils. Dans notre approche, les attributs sont considérés de la même manière dans les règles. Néanmoins, il y a un ordre d'exécution des règles, de la plus contraignante à la moins contraignante. De plus, notre approche découvre de nouveaux profils par propagation de ceux découverts et réconciliés à l'itération précédente. L'évaluation est basée sur des données réelles provenant de quatre réseaux sociaux, tandis que leur expérience est limitée à de petits systèmes fermés. La différence est importante. En effet, dans les réseaux sociaux ouverts, certains utilisateurs ne renseignent pas leur identité réelle, contrairement aux réseaux fermés, où ils pensent que leur identité n'est pas menacée. Par conséquent, les données que nous considérons sont susceptibles d'être fortement bruitées ou erronées ; ce qui constitue un défi non négligeable dans notre contexte. Cortis et al. (2012); Raad et al. (2010) ont proposé le calcul d'une similarité sémantique entre les attributs des profils ; bien que ces approches soient originales, elles ne sont évaluées que sur des petits ensembles de données (par ex., 50 profils (Raad et al. (2010))). Bartunov et al. (2012); Buccafurri et al. (2012); Jain et al. (2013); Narayanan et Shmatikov (2009) ont étudié les propriétés topologiques des réseaux. Buccafurri et al. (2012) adoptent un raisonnement récursif et considèrent que deux profils sont similaires et donc susceptibles de référencer un même utilisateur, s'ils ont des pseudonymes similaires et les utilisateurs auxquels ils sont liés sont similaires. Cette approche a deux inconvénients que notre approche élimine. Le premier est que les profils ayant des pseudonymes différents sont ignorés, même s'ils pourraient identifier un même individu. Le second est que les profils réconciliés découverts ne sont pas propagés pour découvrir des nouveaux. Bartunov et al. (2012) proposent une approche combinant les attributs d'un profil et la topologie du réseau en utilisant les champs aléatoires conditionnels. Cette approche est robuste car elle peut être aussi utilisée quand les valeurs des attributs ne sont pas disponibles, ce qui est le cas des réseaux anonymisés. Cependant, comme elle s'appuie sur un modèle probabiliste, les valeurs des paramètres nécessitent l'utilisation de techniques d'apprentissage supervisé et de disposer d'une base d'exemples d'apprentissage.
Notations et Formalisation du Problème
Dans notre approche, nous représentons un ensemble de réseaux sociaux interconnectés par un graphe orienté étiqueté, où les noeuds représentent les profils des utilisateurs, et les arcs représentent les liens existant entre eux. Chaque profil possède une uri identifiant sa page sur le Web et un ensemble d'attributs décrits sur cette page. Chaque arc possède une étiquette décrivant le type de lien.
4
Nous considérons deux types de liens, les liens d'amitié entre les utilisateurs d'un même réseau social et les liens transversaux reliant deux profils d'un même utilisateur appartenant à deux réseaux différents. Plus formellement, un ensemble de n réseaux sociaux est un graphe défini comme suit :
A représente l'ensemble des attributs définis dans un profil, P a (v) représente la(les) valeur(s) associée(s) à l'attribut a ? A dans le profil v. 
Le problème de réconciliation des profils d'un même utilisateur à travers différents réseaux sociaux peut être réduit au problème de détermination des liens transversaux me manquants noté mirror et formalisé comme suit :
Notre approche de réconciliation des profils
Une solution intuitive au problème posé consiste à comparer chaque paire de profils (v i , v j ), non reliée par le lien me, pour chaque paire de réseaux (i, j), 
Sélection des paires de profils candidats
A partir de l'analyse des différents profils d'un même utilisateur sur ses réseaux sociaux, nous avons constaté qu'un utilisateur a souvent tendance à déclarer en tant qu'ami au moins un même utilisateur dans ses différents profils. De plus, deux utilisateurs ayant différents profils déclarent qu'ils sont amis sur plusieurs d'entre eux (Golbeck et Rothstein (2008)). Notre hypothèse de base de sélection des paires de profils candidats repose sur ce constat. Par ailleurs, dans un réseau social, il existe des utilisateurs qui déclarent explicitement les liens entre leurs différents profils, ce qui permet de déterminer les amis communs déclarés dans deux profils appartenant à deux réseaux différents.
En
L'ensemble des paires de profils candidats, pour les réseaux sociaux i et j, est formellement défini comme suit :
Détermination des paires de profils mirror
Une fois l'ensemble des paires de profils candidats, S, construit, il s'agit de déterminer parmi ces candidats les profils qui référencent un même utilisateur en exploitant les valeurs des attributs définis dans ces profils. Dans ce qui suit, nous présentons les attributs considérés par notre approche. Nous détaillons, ensuite, les règles permettant de déterminer les paires de profils mirror ainsi que l'algorithme de réconciliation que nous avons définis.
Les attributs
Dans la majorité des réseaux sociaux, certains attributs sont rendus publics soit par défaut soit c'est l'utilisateur qui choisit de les publier. Krishnamurthy et Wills (2009) ont identifié un ensemble d'attributs souvent publics dans les 12 plus répandus réseaux sociaux. Notre approche de réconciliation vise à exploiter un grand nombre de réseaux sociaux, nous avons donc besoin d'isoler un ensemble d'attributs accessibles, publics et permettant de déterminer si deux profils référencent le même utilisateur. Dans cet article, nous nous focalisons sur les attributs suivants : le pseudonyme, les noms et prénoms, les adresses électroniques et les liens vers des pages Web. L'attribut pseudonyme, noté p, est un attribut public dont la valeur est toujours accessible. C'est l'identifiant du profil sur un réseau social donné. Il fait généralement partie de l'uri de la page du profil sur le Web. Des études comme dans (Buccafurri et al. (2012); Perito et al. (2011);Zafarani et Liu (2009)) ont montré que les utilisateurs des réseaux sociaux ont tendance à utiliser des pseudonymes identiques ou comportant des sous-chaînes identiques sur leurs différents profils.
Pour évaluer la similarité de deux pseudonymes, nous avons choisi d'utiliser la distance de Levenshtein. En effet, cette distance permet de capturer les variations des sous-chaînes de caractères en calculant le nombre suppressions ou d'insertions de caractères d'un mot p 1 pour qu'il soit identique à un mot p 2 . Prenons l'exemple de deux profils référençant le même utilisateur dont le pseudonyme sur la page www.flickr.com/photos/cospics est "cospics" et le pseudonyme sur la page www.livejournal.com/users/cos/profile est "cos", le nombre de suppressions est de 4. Le seuil de similarité est défini dans les expérimen-tations. L'attribut nom, noté n, est un attribut dont la valeur représente les noms et/ou prénoms renseignés par l'utilisateur. En effet, ces valeurs ne correspondent généralement pas à des champs distincts et bien identifiés dans un réseau social et ne sont pas renseignés avec le même niveau de détail d'un réseau social à un autre. Le nom est un attribut ambigu, en particulier, lorsque les valeurs correspondent à des noms et prénoms communs. Il est également sensible dans le sens où l'utilisateur préfère rester anonyme, même s'il renseigne cette information, souvent il ne révèle pas son véritable nom. Il est donc clair que l'utilisation seule de cet attribut est insuffisante pour réconcilier deux profils.
Pour mesurer la similarité de deux valeurs de l'attribut n, nous utilisons la mesure de Jaccard qui permet de calculer le nombre de mots communs sans prendre en compte leur ordre d'occurrence dans la chaîne de caractères. Par exemple, la mesure de Jaccard pour les chaînes "Barack, Obama" et "Obama Barack" est égale à 1 et à 2 3 pour les chaînes "Barack, Hussein, Obama" et "Obama Barack" . L'attribut adresse électronique, noté m, est un attribut multivalué dont les valeurs correspondent aux différentes adresses électroniques d'un utilisateur. Si renseignée, il s'agit d'un attribut qui permet d'identifier de manière unique un utilisateur. Cependant, cette information n'est pas souvent renseigné, et l'utilisation seule de cet attribut est insuffisante pour réconcilier deux profils.
Pour comparer les valeurs de l'attribut m de deux profils, il s'agit de déterminer si l'une des adresses électroniques d'un profil est identique à l'une des adresses électroniques de l'autre profil. Dans le cas positif la similarité est égale 1. Sinon, elle est égale à 0.
L'attribut liens vers d'autres pages Web est un attribut multivalué dont les différentes valeurs correspondent à différentes url référençant des liens vers des pages Web. Nous distinguons deux types de liens, ceux qui référencent des liens vers des profils de réseaux sociaux, noté s, de ceux qui référencent des liens vers d'autres pages, noté w. Le but étant de pouvoir analyser séparément la contribution de chacun des types. En effet, les liens vers les réseaux sociaux, figurant sur le profil d'un utilisateur, pourraient être des liens vers ses autres profils. Ce qui pourrait correspondre à un lien transversal. Par ailleurs, les liens vers d'autres pages Web sont des liens vers des ressources que l'utilisateur souhaite partager avec les autres et qui pourraient correspondre à ses pages personnelles.
Dans cet article, nous nous limitons dans cette première étude à rechercher si les valeurs de l'attribut w ou s, pour deux profils, ont au moins une url commune sans analyser le contenu de ces pages, en particulier les liens vers les réseaux sociaux, qui pourraient être pertinents pour la réconciliation des profils. L'attribut lieu, noté l, est un attribut dont la valeur correspond aux différents lieux renseignés par l'utilisateur. Il s'agit d'un attribut public souvent renseigné par les utilisateurs. Néanmoins, cet attribut est souvent ambigu lorsqu'on est amené à comparer les valeurs de deux profils différents, et ce pour plusieurs raisons :
-le lieu peut être incomplet, ce qui ne permet pas de l'identifier de manière unique en tant qu'entité géographique. Même si les deux valeurs sont identiques, par exemple pour la valeur "Paris" il existe plusieurs entités géographiques avec ce même label "Paris" au "Texas" ou "Paris" en "France". 
Les règles
Pour déterminer si deux profils v i et v j référencent un même utilisateur, nous avons défini un ensemble de règles exploitant les attributs présentés ci-dessus à appliquer à chaque paire de S. Chaque règle prend en considération la contribution d'un ou plusieurs attributs. Étant donné qu'aucun attribut ne constitue l'identité de l'utilisateur, nous supposons que plus le nombre d'attributs qui correspondent, selon la mesure de similarité définie, est grand pour deux profils plus la probabilité qu'ils référencent un même utilisateur est forte. Ainsi, les règles définies sont classées par ordre de pertinence noté k. La règle la plus pertinente, d'ordre maximale, est celle qui détermine que tous les attributs correspondent ; dans ce cas k = |A|. La règle la moins pertinente est celle qui détermine que un seul attribut correspond ; dans ce cas k = 1.
Soit le prédicat noté match(P a (v i ), P a (v j )) qui retourne vrai si les valeurs de l'attribut a pour les profils v i et v j correspondent selon la mesure de similarité définie pour l'attribut a. Une règle d'ordre k, noté R k est définie comme suit :
Ces liens mirror découverts sont exploités et considérés comme des liens me pour sélec-tionner de nouveaux couples de profils candidats C Ainsi, les règles sont ré-exécutées pour C et ainsi de suite, jusqu'à ce qu'il n y ait plus de nouvelles paires de profils mirror. Pour réaliser une première évaluation, nous avons appliqué notre approche de réconcilia-tion aux 2 réseaux sociaux Flickr et LiveJournal. Tout d'abord, les paires de profils candidats sont obtenues en exploitant les liens transversaux comme expliqué dans la section 4.1. L'ensemble des paires de profils mirror, noté M, est ensuite généré en appliquant les règles présentées dans la section 4.2.2.
Arcs
Les premiers résultats que nous avons obtenus, à l'issue de la 1 ère itération, pour un seuil fixé arbitrairement ? = 0.7, et pour chacune des règles vérifiée, sont décrits dans la table 3. Au total 3424 couples vérifient les règles sur environ 16000 couples candidats dont 0.03% vérifie la règle d'ordre 4, 0.91% vérifie les règles d'ordre 3, 7.65% vérifie les règles d'ordre 2 et 91.41% pour les règles d'ordre 1. Comme l'attribut adresse électronique n'est pas renseigné, dans la collection considérée, pour au moins un profil d'un couple candidat, l'attribut m n'intervient dans aucune règle. Ces résultats montrent qu'un faible pourcentage de paires de profils vérifient les règles d'ordre k ? 2, et le plus grand pourcentage vérifie la règle d'ordre 1. Ce qui confirme le fait que les utilisateurs préfèrent généralement ne pas divulguer trop d'informations personnelles. Les résultats montrent aussi que l'attribut pseudonyme est l'attribut intervenant dans les règles vérifiées par le plus grand nombre de couples de profils candidats.
Pour l'évaluation des résultats, nous avons défini trois sous-ensembles de l'ensemble de couples mirror M : (i) corrects référençant le même utilisateur, noté C ; (ii) faux référençant deux utilisateurs différents, noté F et (iii) indéterminés pour lesquels nous ne disposons pas d'informations suffisantes pour décider. La précision est donc calculée en utilisant la formule suivante : précision= |C| |M| . Il faut noter que nous avons choisi dans ce calcul de tenir compte de l'ensemble des couples indéterminés pour évaluer au mieux notre approche. L'évaluation est faite manuellement et ce sur la totalité des 3424 couples vérifiant les règles en utilisant les uri des pages des profils.
Les résultats des règles d'ordre 1 montrent que les attributs générant le plus grand nombre de paires de profils mirror faux sont le nom (54.55%) et ensuite le pseudonyme (30.19% Ces résultats soulignent qu'une similarité forte de deux noms n'est pas à elle seule un élé-ment pertinent. En effet, l'attribut nom est un attribut non seulement ambigu mais aussi sensible, souvent l'utilisateur fournit une information erronée de manière délibérée afin de ne pas dévoiler son identité. Par exemple, en analysant précisément les pages de profils appartenant au même utilisateur, souvent les noms sont partiellement ou complètement différents.
Pour mieux comprendre les résultats obtenus pour les attributs nom et pseudonyme, nous avons procédé à d'autres tests en utilisant un seuil de similarité plus grand et nous avons constaté que la précision augmente significativement pour les règles d'ordre 1 utilisant le pseudonyme, mais l'impact reste non significatif pour les règles utilisant le nom. Comme expliqué dans la section 4.2.1, dans notre approche, deux pseudonymes très similaires diffèrent très peu en termes de nombre et de séquencement de caractères ; et deux noms sont similaires s'ils possèdent un grand nombre de mots en communs.
Afin de propager la découverte des couples mirror pour les réseaux Flickr et LiveJournal sans dégrader la précision, nous avons supprimé la règle d'ordre 1 portant sur l'attribut nom 
Conclusions et perspectives
Dans cet article, nous avons présenté notre approche de réconciliation de profils dans les réseaux sociaux qui exploite la topologie du graphe et les attributs publics définis dans les profils et accessibles dans de nombreux réseaux sociaux. Les résultats obtenus sur la collection de données issues des réseaux LiveJournal et Flickr ont montré la pertinence des attributs considérés et l'efficacité des règles que nous avons définies. La précision a atteint 94% et le nombre de liens découverts est passé de 148 liens transversaux me initialement renseignés entre LiveJournal et Flickr à 2768 au bout de 4 itérations. De plus, cette précision peut être contrôlée en appliquant les règles dont l'ordre est supérieure à une valeur k fixée, contraignant ainsi la similarité d'au moins k attributs. Elle peut également être contrôlée en triant, de manière croissante, les paires de profils mirror découverts (v i , v j ), pour chaque noeud v i , selon l'ordre de la règle et le nombre de liens f riend communs entre (v i , v j ). Le nombre de liens découverts sur les paires de réseaux sociaux Twitter et YouTube comportant peu de noeuds et peu d'informations renseignées sur les profils est faible. Dans ce cas, d'autres éléments peuvent être prises en compte : (i) la topologie du graphe peut être davantage exploitée en distinguant la nature des liens f riend entrant ou sortant et en considérant leur nombre ; (ii) analyser le contenu ou les tags associés aux ressources sont des éléments qui pourraient être pertinents pour déterminer la similarité de deux profils ; (iii) exploiter l'attribut lieu en désambiguïsant les valeurs associées et en définissant une mesure de similarité entre deux valeurs de lieu définies dans deux profils différents. Ces derniers aspects font l'objet de nos travaux actuels.

Introduction
En raison de l'évolution des nouvelles technologies, le domaine de la criminalistique informatique se heurte à des problèmes qui étaient encore anecdotiques il y a quelques années. Bien que des outils existent pour aider les enquêteurs, leur portée est limitée aux premières étapes du processus d'investigation défini par (Palmer, 2001). La collecte et l'étude des caractéristiques des pièces à conviction sont d'importantes phases du processus, toutefois il est également nécessaire de déduire de nouvelles connaissances telles que les raisons de l'état actuel des pièces à conviction (Carrier et Spafford, 2004) pour produire des conclusions utiles dans un procès. La reconstruction d'évènements peut être vue comme un processus utilisant un ensemble de pièces à conviction pour produire une chronologie décrivant les évènements composant un incident. Dans ce papier, nous présentons l'approche SADFC (Semantic Analysis of Digital Forensic Cases) qui permet de reconstruire et d'analyser des chronologies à partir de sources de données hétérogènes (traces laissées sur une scène de crime). La section suivante passe en revue les approches de reconstruction existantes. La section 3 présente ensuite notre approche et expose notamment les aspects relatifs à la gestion des connaissances et aux possibilités de raisonnements offertes. La section 4 introduit un prototype ayant permis la validation expérimentale de notre approche. Enfin, les travaux futurs sont présentés dans la section 5.
Étude des approches de reconstruction d'évènements
Volume et hétérogénéité des données La taille des données à traiter et leur hétérogénéité (due à l'utilisation de nombreuses sources de traces numériques telles que les fichiers de journalisation, les historiques de navigateur Web...) sont deux challenges majeurs de la reconstruction d'évènements. Dans une large part des approches existantes, des solutions sont proposées pour l'extraction automatique des évènements à partir de sources hétérogènes et la construction de la chronologie. Pour cela, des extracteurs automatiques et dédiés à chaque source d'évène-ments sont utilisés pour peupler un élément de stockage central (base de données (Chen et al., 2003), ontologie (Schatz et al., 2004), etc.). Concernant l'analyse de scénarios, les approches existantes proposent des fonctionnalités permettant de corréler des évènements (Schatz et al., 2004) ou d'aider l'enquêteur dans la lecture de la chronologie en produisant des évènements de haut niveau conceptuel à partir d'évènements extraits depuis des sources de traces (Hargreaves et Patterson, 2012). Toutefois, aucune des approches étudiées ne proposent une solution complète pour assister les enquêteurs dans l'interprétation et l'analyse des chronologies.
Exigences légales Les approches de reconstruction d'évènements doivent également satisfaire un ensemble d'exigences telles que la crédibilité des conclusions produites, l'intégrité des données utilisées et la reproductibilité du processus d'investigation (Baryamureeba et Tushabe, 2004). De plus, (Gladyshev et Patel, 2004) avance qu'une formalisation du problème de reconstruction d'évènements est nécessaire afin de mieux structurer le processus de reconstruction, de faciliter son automatisation et d'assurer la complétude de la reconstruction. Un problème récurrent parmi les approches étudiées est le manque de fondements théoriques permettant de valider et d'expliquer les conclusions produites.
Représentation de connaissances et analyse de chronologies cybercriminelles
Pour répondre aux limites précédentes, nous présentons l'approche SADFC qui permet d'assister les enquêteurs depuis l'extraction des traces numériques jusqu'à l'interprétation de la chronologie. Cette approche s'appuie sur l'analyse avancée de chronologies cybercriminelles basée sur une représentation des connaissances décrivant les activités d'un utilisateur sur un ordinateur. Bien que les informations temporelles des évènements soient une dimension primordiale, d'autres aspects doivent également être pris en compte pour offrir des fonctions d'analyse avancées. L'approche SADFC introduit une nouvelle représentation sémantiquement riche des évènements et de leurs interactions avec l'environnement intégrant les notions de scène de crime (espace virtuel où se déroule un ensemble d'évènements illicites), d'évène-ments (action survenant à un instant donné), d'incidents (ensemble des évènements illicites et d'évènements corrélés à ces derniers), de traces (résidus laissés par un évènement et permettant sa reconstruction), d'objets (ressources utilisées, générées, modifiées ou supprimées par les évènements) et de sujets (processus ou personnes initiant ou subissant les évènements). Le modèle proposé définit également les relations entre ces concepts parmi lesquelles les relations de composition (liant un évènement avec les évènements le composant), de participation (liant un sujet aux évènements auxquels il prend part), d'utilisation (liant un évènement aux objets Y. Chabot et al.
qu'il utilise) ou encore de corrélation (liant deux évènements interdépendants). Pour aider les enquêteurs à mener à bien leurs enquêtes, des opérateurs de construction de chronologies et d'analyse sont proposés. Trois ensembles d'opérateurs sont définis dans l'approche SADFC :
-Les opérateurs d'extraction ayant pour fonction d'extraire les informations pertinentes contenues dans les traces numériques et de peupler la base de connaissances en consé-quence. -Les opérateurs d'inférence permettant d'enrichir la base avec de nouvelles connaissances déduites à partir des connaissances existantes. Par exemple, la seule information disponible pour déterminer le sujet impliqué dans un évènement d'un navigateur Web est son identifiant de session, présent dans certaines traces numériques produites par les navigateurs. Pour identifier le sujet impliqué dans d'autres actions, nous utilisons un opérateur d'inférence basé sur l'hypothèse suivante : soit e i la première visite d'une page Web d'une session s, e j la dernière visite de cette même session, t i la date de début de e i et t j la date de fin de e j , un évènement survenant sur la machine à une date comprise dans l'intervalle de temps défini par t i et t j implique la personne identifiée par la session s. -Les opérateurs d'analyse utilisés pour aider les enquêteurs dans l'interprétation des informations portées par une chronologie. Les opérateurs proposés dans cette section permettent de dispenser les enquêteurs des tâches les plus fastidieuses de la reconstruction d'évènements, leur permettant ainsi de se concentrer sur des tâches où leur expertise et leur expérience sont les plus utiles.
Implémentation
Pour valider la pertinence du modèle et la viabilité des opérateurs proposés, un prototype a été développé. Ce dernier est centré sur une ontologie (permettant notamment l'utilisation de processus automatiques pour raisonner sur les connaissances) OWL implémentant le modèle présenté dans la section précédente. L'ontologie proposée est divisée en trois couches. La couche Provenance Knowledge Layer contient des informations sur la manière dont l'investigation est menée (actions entreprises par les enquêteurs, informations utilisées pour parvenir à une conclusion, etc.). La couche Common Knowledge Layer contient des connaissances gé-nériques sur les évènements telles que des informations temporelles, les ressources ou encore les personnes et les processus participant à leur exécution. La couche Specialized Knowledge Layer contient les caractéristiques spécifiques portées par chaque évènement. La modélisation de connaissances spécialisées au sein de l'ontologie permet de bénéficier de l'expertise des acteurs du domaine durant l'analyse de la chronologie. Le prototype propose également une implémentation des trois ensembles d'opérateurs au sein d'une application Java. Les opérateurs d'extraction prennent la forme d'un ensemble d'extracteurs et de ponts permettant de peupler l'ontologie à partir de sources de traces. Les opé-rateurs d'inférence sont implémentés à l'aide de requêtes SPARQL/Update recherchant des motifs particuliers dans l'ontologie et ajoutant des connaissances en conséquence. Enfin, le prototype implémente un opérateur permettant d'identifier des couples d'évènements potentiellement corrélés en se basant sur des critères tels que la proximité temporelle, l'utilisation de ressources communes, les sujets participants ou encore des règles formulées par les experts du domaine. Par exemple, soit un évènement représentant la visite d'une page web et un évènement de création de marque-page pour cette même page Web, ces deux évènements sont corrélés car ils utilisent une même ressource (la page Web) et sont créés par le même processus (le navigateur Web Firefox par exemple).
Conclusion et travaux futurs
Dans cet article, nous avons présenté l'approche SADFC permettant d'aider les enquêteurs durant la reconstruction et l'analyse de chronologies dans le respect des contraintes juridiques. Notre principale contribution est l'introduction d'un nouveau modèle pour décrire des incidents cybercriminels et d'opérateurs permettant le peuplement de l'ontologie ainsi que l'analyse des connaissances. L'implémentation de ces éléments au sein d'un prototype a permis de valider expérimentalement la faisabilité et la pertinence de l'approche proposée. Les travaux futurs s'intéressent à l'intégration de nouvelles sources de traces, à l'enrichissement de l'ontologie et à la conception de nouveaux opérateurs d'analyse.

Introduction et motivations
L'abondance des documents, notamment sur le Web, dans de nombreuses langues a rendu nécessaire l'existence d'une Recherche d'Information Multilingue. La RIM consiste ainsi à formuler une requête dans une langue source et à rechercher des documents pertinents dans des langues cibles (RIM) Nie (2010). Dans un contexte multilingue, la requête ainsi que les documents ne sont pas représentés dans un même espace d'indexation étant exprimés dans des langues différentes. Par conséquent, la mise en correspondance de leurs descripteurs sera impossible. Ainsi, pour permettre une recherche multilingue, l'enjeu consiste à représenter les documents et la requête dans un même espace d'indexation.
Dans cet article, notre motivation est double : en premier lieu, il s'agit de déployer les techniques d'ECT qui permettent d'inférer des relations de traduction entre des unités linguistiques pour l'identification de lexiques bilingues à partir des corpus parallèles, et, en deuxième lieu, exploiter ces lexiques bilingues dans le cadre de la RIM Lavecchia et al. (2008);Latiri et al. (2010).
La revue de la littérature du domaine de la RIM montre qu'il existe plusieurs types d'approches pour modéliser la tâche de la RI multilingue. Nous pouvons citer les approches basées sur la traduction automatique qui s'appuient sur la traduction automatique d'une requête ou des documents de la collection. La traduction automatique de la requête est plus explorée Herbert et al. (2011) malgré qu'elle souffre d'un manque de précision comparée à celle basée sur la traduction d'une collection de documents dans laquelle un contexte d'information nettement plus important est utilisé, diminuant ainsi les risques de mauvaise traduction. De plus, les approches basées sur des corpus d'apprentissage parallèles ou comparables Wu et He (2010) qui s'appuient, pour la traduction des requêtes, sur un thésaurus ou des corpus comparables ou parallèles pour trouver des co-occurrences de termes Hazem et al. (2011);Bo et al. (2011). En outre, les approches basées sur des lexiques bilingues qui se basent principalement sur l'expansion de requêtes. Elle consiste à reformuler la requête à l'aide de dictionnaires grâce aux lexiques bilingues Levow et al. (2005). Enfin, les approches à base d'un language pivot (l'interlingua) qui s'appuient sur un langage unifié (pivot) permettant de représenter la sémantique des différentes langues Hahn et al. (2004). La recherche multilingue se facilite en effectuant la traduction du corpus et des requêtes dans un même langage pivot. L'ensemble de propositions que nous introduisons dans cet article s'inscrit dans la famille des méthodes qui font appel à des méthodes de traduction, basées sur un lexique bilingue. Définition 1 Une règle d'association inter-langues, notée par RAIL, est une implication de la forme : R : S S ? S C telles que S S et S C sont deux séquences de termes fermées fréquentes en langue source et cible, de tailles respectives n et m mots Latiri et al. (2010).
Une règle d'association inter-langues est également appréciée par les deux métriques de support et de confiance Agrawal et Skirant (1994). De plus, une règle d'association interlangues est dite valide si sa confiance est supérieure ou égale au seuil minimal de confiance La génération des règles associatives inter-langues est réalisée à partir d'un corpus parallèle Anglais-Allemand par un parcours de l'espace de recherche qui s'effectue au niveau de la phrase. La phase de génération est précédée par une étape d'extraction de séquences fréquentes. Nous apportons, pour effectuer cette étape, des adaptations à l'algorithme BFSM Chang (2004) liées au contexte de la fouille de données textuelles. L'algorithme qui permet de dériver les règles d'association inter-langues à partir de l'ensemble des séquences inter-langues extraites est décrit dans Latiri et al. (2010).
Nous proposons dans ce qui suit, de déployer l'ensemble des règles d'association interlangues générées pour étudier leur apport dans la RI multilingue.
Déploiement des règles d'association inter-langue pour la RIM
Nous proposons deux réflexions de recherche en RIM à base des RAIL, à savoir : (i) Traduction d'une requête pour la RI multilingue par les règles d'association inter-langues ; et, (ii) Traduction des termes de l'index par les règles d'association inter-langues.
Traduction d'une requête pour la RI multilingue par les règles d'association inter-langues
Partant d'une requête exprimée dans une langue source, il s'agit de définir un modèle de RI capable de restituer des documents formulés dans chacune des langues cibles de la collection multilingue. Dans notre contexte, la traduction d'une requête peut être conduite en deux étapes. D'une part, une première étape qui consiste à dériver des règles d'association inter-langues à partir des corpus parallèles (l S , l C ). Nous exploitons dans ce cadre le processus d'extraction des règles d'association inter-langues défini dans Latiri et al. (2010) et explicité dans la section 3. Ainsi, le résultat de cette étape est un ensemble de lexiques bilingues. D'autre part, une deuxième étape consistant à déployer le lexique bilingue extrait dans la traduction de requête. La traduction de requêtes consiste à remplacer chacun des termes de la requête dans sa langue source avec des termes dans la langue cible. Nous soutenons l'idée que les termes de la requête à traduire, exprimée en langue source l S , figurent dans les prémisses des règles inter-langues dérivées. De ce fait, leurs traductions potentielles sont représentées par les conclusions de ces règles d'association inter-langues. Le but de cette étape est donc de supprimer certaines traductions jugées inadéquates dans le contexte de la requête Req S . Nous suggérons d'utiliser la mesure de confiance pour ne garder que les règles d'association valides par rapport à un seuil minimal de confiance minconf , qui permet de retenir les meilleures traductions avec une confiance assez élevée. Cette étape diminue ainsi l'ambiguïté dans le choix des traductions sans forcément l'éliminer.
Traduction des termes de l'index des documents par les règles d'association inter-langues
Nous partons d'une requête Req S formulée dans une langue source l S et d'une collection bilingue {C l S , C l C }. Pour chaque document d de la collection C l C , son index dans la langue cible, noté Index l C (d) est généré par un processus classique d'indexation. Ensuite, le processus d'extraction de règles d'association inter-langues est lancé sur les corpus bilingue {C l S , C l C }. Le but de ce processus est de générer des corrélations entre des unités linguistiques de la langue cible l C , dans laquelle est représenté l'index d'un document de la collection, et celles de la langue source l S . Dans ce contexte, les corrélations inter-langues sont générées dans le sens contraire que celui considéré pour la traduction de requêtes, i.e., de la langue cible vers la langue source. La sélection des RAILs les plus pertinentes pour la traduction se fait sur la base du seuil minimal de confiance minconf . L'index Index l C (d) est par la suite traduit en utilisant ces règles inter-langues valides, où chaque terme t i de l'index est traduit par le terme ou la séquence de termes qui apparaît dans une corrélation inter-langues contenant le terme t i . Cette nouvelle représentation de l'index des documents d'une collection multilingue permet ainsi une interrogation monolingue, avec une requête Req S exprimée en langue source l S et un corpus de documents en langue cible l C , dont les index des documents sont traduits moyennant les lexiques bilingues.
5 Évaluation expérimentale
Cadre d'évaluation
Nous considérons une collection fournie dans le cadre du projet MUCHMORE 1 . Cette collection regroupe des résumés de documents scientifiques dans le domaine médical, obtenus à partir du site web de Springer, rédigés en anglais et en allemand. Dans de nombreux cas toutefois, la version anglaise est une reformulation complète du résumé allemand, ce qui rend difficile un alignement au niveau des phrases. Ce corpus est considéré comme étant un corpus parallèle bruité. Premièrement, les corpus sont étiquetés et lemmatisés afin d'extraire les lemmes des mots pleins (noms, verbes, adjectifs, adverbes). Seuls les noms et les adjectifs sont pris en compte par nos algorithmes de génération des RAILs.
Scénarios d'évaluation
Nous avons réalisé deux scénarios d'évaluation basés sur la collection MUCHMORE. La base d'évaluation comparative(baseline), notée dans la suite de l'article MT est le résultat donné dans Volk et al. (2003) où les auteurs ont utilisé la collection MUCHMORE pour éva-luer une approche de RIM à base de traduction automatique des requêtes de l'allemand vers l'anglais.
Scénario 1 : Traduction des requêtes par les RAILs. Le premier scénario, noté dans la suite de l'article Trad-Req, consiste à interroger le corpus exprimé en anglais avec des requêtes exprimées en allemand de la collection MUCHMORE. Dans ce cas, les termes de la requête sont traduits de l'allemand (langue source) vers l'anglais (langue cible) en utilisant les règles d'association inter-langues dérivées à partir du corpus parallèle allemand-anglais.
Scénario 2 : Traduction des index par les RAILs. Le deuxième scénario, noté dans la suite Trad-index, consiste à interroger le corpus en allemand avec des requêtes exprimées en anglais. Dans ce cas, chaque terme de l'index relatif à chaque document du corpus de la collection sont traduits de l'allemand (langue cible) vers l'anglais (langue source) moyennant les lexiques bilingues illustrés par les RAILs (cf., sous-section 4.2).
Résultats expérimentaux et discussion
Le Tableau 1 synthétise les résultats de la recherche multilingue utilisant la collection MUCHMORE. À la lecture du Tableau 1, nous constatons que le déploiement des RAILs pour la traduction de requêtes (scénario Trad-Req) réalise une amélioration significative de la MAP 
Conclusion
Cet article met en évidence le déploiement des règles d'association inter-langues dans le cadre de la RI multilingue. L'ensemble des propositions introduites se basent sur une fouille efficace d'un corpus parallèle aligné au niveau des phrases à partir d'une collection bilingue. L'objectif est d'extraire des corrélations inter-langues entre les unités linguistiques utilisées ultérieurement dans la RI multilingue. L'évaluation expérimentale menée sur la collection de documents MUCHMORE a montré une amélioration significative de la pertinence système. Par ailleurs, ces propositions restent extensibles, dans le sens où les résultats peuvent être améliorés si nous utilisons d'autres métriques statistiques lors de l'extraction des RAILs. 
18`18ème
Conférence sur le Traitement Automatique des Langues Naturelles, TALN 2011, Montpellier, France, pp. 211-222. 

Introduction
In database research, the last two decades have witnessed a growing interest in preference queries on the one hand. Motivations for introducing preferences inside database queries are manifold Hadjali et al. (2008). First, it has appeared to be desirable to offer more expressive query languages that can be more faithful to what a user intends to say. Second, the introduction of preferences in queries provides a basis for rank-ordering the retrieved items, which is especially valuable in case of large sets of items satisfying a query. Third, a classical query may also have an empty set of answers, while a relaxed (and thus less restrictive) version of the query might be matched by items in the database.
Approaches to database preference queries may be classified into two categories according to their qualitative or quantitative nature Hadjali et al. (2008).
Dans la dernière, les préférences sont exprimées quantitativement grâce à une fonction de score monotone, le score global étant positivement corrélé avec les scores partiels. Dans les approches qualitatives, les préférences sont définies au travers de relations binaires. Comme ces relations peuvent être définies en termes de fonctions de score, cette famille est plus générale que la précédente.
Dans cet article, une vision qualitative est adoptée, à savoir l'approche dite Skyline, introduite dans (Börzsönyi et al. (2001)). Étant donné un ensemble de points dans l'espace, une requête skyline retrouve les points qui ne sont dominés par aucun autre au sens de l'ordre de Pareto. Ce problème correspond à la recherche des extrema dans un ensemble de vecteurs (Kung et al. (1975)). Quand le nombre de dimensions sur lesquelles les préférences sont exprimées devient grand, de nombreux tuples peuvent être incomparables. Quelques approches ont été proposées pour définir un ordre entre deux tuples incomparables dans le contexte des requêtes skyline, fondées sur : -le nombre de tuples que chacun de ces deux tuples domine (notion de dominance kreprésentative proposée dans Lin et al. (2007), -des ordres de préférence entre les attributs : par exemple les notions de k-dominance et de k-fréquence introduites dans Chan et al. (2006a,b), ou -la représentativité : Tao et al. (2009) redéfinissent l'approche de Lin et al. (2007) pour retourner les points du skyline les plus représentatifs possibles en présentant uniquement un représentant par cluster de points présent dans le skyline. D'autres approches ont cherché à flexibiliser le concept de skyline selon différentes directions, voir par exemple Hadjali et al. (2011).
Ici, nous nous intéressons à un problème différent, celui de la possible présence de points exceptionnels dans la relation à laquelle la requête skyline est adressée. De telles exceptions peuvent correspondre à du bruit ou à la présence de points atypiques, ou non représentatifs dans la collection considérée. L'impact de tels points sur le Skyline peut évidemment être important s'ils en dominent d'autres, plus représentatifs. Deux stratégies peuvent être envisagées pour gérer les exceptions. La première consiste à éliminer les anomalies par la mise en place d'une procédure de nettoyage des données ou de contraintes de saisie. Néanmoins, il n'est pas toujours aisé de distinguer entre des points erronés et des points qui représentent simplement des cas exceptionnels. Une meilleure solution est donc de définir une approche tolérante aux exceptions, i.e., qui mette en avant des points représentatifs de la base de données non dominés par d'autres éléments représentatifs, tout en signalant les éventuelles exceptions. Dans cet article, nous décrivons une telle approche et interprétons la notion de représentativité à l'aide de celle de typicité (Zadeh, 1984). Nous proposons une nouvelle définition du skyline basée sur la typicité et nous montrons que celle-ci permet i) de retrouver les meilleurs compromis sans pour autant évincer les points potentiellement intéressants, quoiqu'exceptionnels, et ii) d'offrir un outil flexible pour visualiser les réponses.
La suite de ce papier est organisée comme suit. La Section 2 fournit quelques rappels à propos de l'ordre de Pareto, des requêtes Skyline, et de la notion de représentativité tout en motivant l'approche proposée. La Section 3 présente notre solution et définit le concept de skyline graduel tolérant aux exceptions. La Section 4 donne les principaux éléments de mise en oeuvre de notre approche tandis que la Section 5 présente les premiers résultats obtenus sur un jeu de données réelles. Enfin, la Section 7 rappelle les principales contributions et propose des améliorations possibles de ce travail.
Rappel sur les requêtes Skyline et motivations
Une requête skyline sur D appliquée à un ensemble de points S, notée SKY D (S), selon des relations d'ordre i , retourne l'ensemble de points qui ne sont dominés par aucun autre point de S :
Selon le contexte, on peut essayer, par exemple, de maximiser ou minimiser les valeurs de dom(D i ), en supposant que dom(D i ) est un domaine numérique. Pour illustrer le principe de l'approche proposée, considérons le jeu de données issu de la base Iris (Fisher (1936)), représenté sous forme graphique dans la Figure 1 
FIG. 1 -Le jeu de données Iris
En ordonnée, figurent les largeurs de sépale tandis qu'en abscisse apparaissent les longueurs de sépale. La requête skyline : select * from iris skyline of sepallength max, sepalwidth max recherche les points iris qui maximisent les dimensions largeur et longueur des sépales (points entourés de la Figure 1) .
Dans ce jeu de données, les points sont organisés en deux groupes qui correspondent respectivement aux intervalles sur les abscisses [4, 5.5] et [5.5, 7]. Par définition, les points du skyline sont à la frontière de l'espace à deux dimensions décrits par les points du jeu Iris. Mais ces points sont très distants des zones décrites par les deux groupes et sont donc peu représen-tatifs du jeu Iris. Il pourrait être intéressant pour un utilisateur de pouvoir visualiser les points "quasi dominants", plus proches des groupes de points et donc plus représentatifs de la base. La notion de typicité introduite dans la section suivante va nous permettre de modéliser cette notion de représentativité.
Calculer un ensemble flou de valeurs typiques
La typicité d'un élément dans un ensemble indique dans quelle mesure cet élément est similaire à beaucoup d'autres points de l'ensemble. La notion de valeur floue typique d'un ensemble a été largement étudiée dans le domaine des résumés de données et dans celui du raisonnement approximatif. Zadeh (1984) définit x comme étant un élément typique d'un ensemble flou A ssi i) x a un haut degré d'appartenance à A et ii) la plupart des éléments de A sont similaires à x. Dans le cas où A est un ensemble non flou, comme ce sera le cas dans la suite, la définition devient : x est dans A et la plupart des éléments de A sont similaires à x.
Dans (Dubois et Prade (1984)), les auteurs définissent un indice de typicité basé sur la fré-quence et la similarité. Dans la suite de cet article, nous adaptons leur définition comme suit. Considérons un ensemble E de points à deux dimensions, correspondant aux attributs splength (longueur du sépale) et spwidth (largeur du sépale). Nous dirons qu'un point est d'autant plus typique qu'il est proche de nombreux autres points. La relation de proximité sera basée sur la distance euclidienne. Soit par exemple deux points p 1 and p 2 de la base Iris (cf. page précé-dente). la distance d(p 1 , p 2 ) entre ces deux points est définie comme suit :
Nous considérons que ces deux points sont proches l'un de l'autre si d(p 1 , p 2 ) ? ? où ? est un seuil prédéfini. Pour le jeu Iris, ce seuil est fixé à ? = 0.5. La fréquence d'un point est définie de la façon suivante :
Ce degré est ensuite normalisé en un degré de typicité dans [0, 1] :
Nous utiliserons également les notations suivantes :
TYP(E) représente l'ensemble flou des points un tant soit peu typiques de l'ensemble E tandis que TYP ? (E) rassemble les points de l'ensemble E dont la typicité dépasse le score ?. Un extrait du calcul de la typicité des points de la base Iris est présenté en 
Skyline tolérant aux exceptions
Comme expliqué en introduction, notre objectif est de revisiter la définition du skyline afin de prendre en compte la typicité des points dans la base de données et ainsi d'éviter la perte des points qui seraient dominés par des exceptions ou des anomalies.
Vision booléenne
Une première idée consiste à restreindre le calcul du skyline à un sous-ensemble de E correspondant aux points qui sont suffisamment typiques. La définition correspondante donne :
Une telle approche réduit le coût d'évaluation du skyline puisque seuls les points typiques au degré ? sont considérés dans le calcul. Il n'est cependant pas possible avec cette définition de discriminer les points du résultat selon leur degré de typicité. En effet, le skyline obtenu est un ensemble classique (non flou). La Figure 2 illustre ce cas de figure et montre les maxima (croix entourées sur la figure) obtenus avec les points typiques à un degré ? 0.7 (croix simples). Cette première définition a également l'inconvénient d'exclure les points atypiques qui peuvent pourtant être des réponses valides. Une définition plus prudente consiste à garder les points atypiques dans le calcul du skyline et à transformer l'équation (2) en :
La Figure 3 illustre cette nouvelle définition. Elle représente les points (points entourés) de la base Iris qui ne sont pas dominés par des points typiques (croix) au degré ? = 0.7 au moins. Avec l'équation (2), les points atypiques sont éliminés, alors qu'avec l'équation (3), le skyline devient plus large et englobe les extrema atypiques. Cette approche permet de relaxer les requêtes skyline de façon à transformer la ligne en un bandeau formé des points normaux du skyline et d'éventuels substituts.
Les principaux inconvénients de cette approche sont : i) le possible grand nombre de points retournés, ii) le fait qu'on ne puisse pas différencier, parmi les points du skyline, ceux qui ne sont pas du tout dominés de ceux qui le sont fortement. 
Vision graduelle
Une troisième version permet de calculer un skyline graduel, vu comme un ensemble flou, qui préserve la nature graduelle de la relation de typicité en entrée. Ainsi, aucun seuil (?) n'est appliqué aux degrés de typicité. Un point appartient totalement au skyline (degré d'appartenance de 1) s'il n'est dominé par aucun autre point. Un point n'appartient pas du tout au skyline (degré d'appartenance de 0) s'il est dominé par au moins un point totalement typique. Un point dominé par des points peu typiques appartient fortement au skyline alors qu'un point dominé par des points très typiques appartient peu au skyline. L'appartenance d'un point p au skyline est donc dépendante du plus fort degré de typicité des points qui le dominent. Il en découle la définition suivante :
où deg(¬(q D p)) = 1 si q ne domine pas p (i.e., (q D p) est faux), 0 sinon. Ainsi, dans le cas où p est dominé par un point quelconque, son degré d'appartenance au skyline est fixé par la non typicité de ce point. L'équation (4) peut être réécrite comme suit :
Avec la base de points Iris, on obtient le résultat présenté dans la figure 4. On observe que les points du skyline classique appartiennent totalement au skyline graduel. Néanmoins, on peut trouver des substituts intéressants qui sont plus ou moins typiques. Cette approche appa- 
FIG. 4 -Skyline graduel du jeu de données Iris
raît intéressante en termes de visualisation. En effet, le score associé à chacun des points permet un affichage restreint des points selon leur degré d'appartenance (e.g., les points dont l'appartenance au skyline dépasse un degré ?). Un affichage en 3 dimensions des points (comme le montre la Figure 4), où le degré d'appartenance au skyline donne la valeur en hauteur, permet d'accentuer l'effet "ligne d'horizon" du skyline. Une pente se dessine des points optimaux vers les points les plus typiques ou complètement dominés. En 2 dimensions, il serait égale-ment possible de distinguer les courbes de niveau, symbolisant des zones d'intérêt. Dans ces zones, l'utilisateur peut choisir des points qu'il considère intéressants. Même si ces points ne sont pas nécessairement optimaux, ils peuvent susciter l'intérêt de l'utilisateur dans la mesure où ils représentent de bonnes alternatives aux points optimaux dans le cas, par exemple, où ceux-ci apparaissent trop exceptionnels pour être crédibles. Enfin, un élément du skyline graduel possède deux degrés : un degré d'appartenance au skyline, et un degré de typicité qui permet de savoir dans quelle mesure il est exceptionnel.
On peut imaginer différentes formes de navigation dans les zones pour découvrir les points : un simple parcours de la zone pour afficher les caractéristiques des points, l'application de différents filtres dans la zone d'étude comme la recherche de la diversité des réponses (sur certains attributs à spécifier), la typicité, des zooms sur des zones d'intérêt, etc.
Éléments d'implémentation
La mise en oeuvre effectuée vise à montrer l'intérêt de la notion de typicité dans le calcul d'une requête skyline. Deux phases sont nécessaires au calcul du skyline graduel : i) le calcul de la typicité et ii) le calcul du skyline. Il existe de nombreux algorithmes pour évaluer les requêtes skyline : l'algorithme Block-Nested-Loops (BNL) (Börzsönyi et al. (2001)) ; l'algorithme Divide and Conquer (Börzsönyi et al. (2001)) ; une proposition utilisant un B-tree ou un R-tree (Börzsönyi et al. (2001)) ; un algorithme basé sur des structures de type Bitmap (Tan et al. (2001)) ; une amélioration du BNL Sort-Filter-Skyline (Chomicki et al. (2003(Chomicki et al. ( , 2005), et aussi (Bartolini et al. (2008)) qui s'appuie sur un préclassement des tuples afin de limiter le nombre de tuples à lire et à comparer. Nous avons choisi de suivre l'approche proposée dans (Tan et al. (2001)) qui permet d'implémenter facilement la formule (5).
L'algorithme proposé dans Tan et al. (2001) permet de retourner progressivement les points du skyline. La structure de données centrale à cette implémentation est celle d'un tableau de booléens ou bitmap. Un index bitmap est défini pour chacune des dimensions du skyline : chaque colonne désigne une valeur possible de la dimension et chaque ligne fait référence à un tuple de la base. La valeur de 1 à la croisée d'une ligne l et d'une colonne c indique que le tuple référencé en ligne l a comme valeur celle désignée par la colonne c. Ensuite, chaque point p de la base S est testé pour savoir s'il appartient ou non au skyline. Pour cela, deux autres structures de données sont créées. La première, appelée A, désigne les tuples qui sont aussi bons que p, la seconde B désigne ceux qui sont meilleurs que p sur une dimension. A et B sont définies comme des tableaux de booléens dont les colonnes font référence aux tuples de S. Elles sont remplies à l'aide des index bitmap. L'organisation de cet index selon des valeurs de dimension décroissantes facilite la création de A et B. Algorithm 1 Algorithme principal du calcul du skyline graduel Require: d distance, n cardinalité de la base S, les points de la base p ? S, l'ensemble des dimensions {d i } Ensure: skyline des points : ?p ? S, Sky grad (p) Prétraitement : création des index bitmap sur les d i Prétraitement : Calcul de la typicité des points T : ?p ? S, T yp(p) for all p ? S do // Recherche des points qui dominent p Création de A Création de B A := A AND B A := A MULT T Sky grad (p) := 1 ? M ax(A ) end for Le calcul de la typicité utilise une distance minimale d, évidemment dépendante des attributs sur lesquels porte la requête skyline. Si n est la cardinalité de la base, le temps nécessaire au calcul de la typicité est au plus en n 2 . En effet, l'Algorithme 2 nécessite pour chaque point p de la base, de rechercher parmi tous les points de la base ceux qui sont proches de p. Il repose sur la distance euclidienne entre deux points p et p , notée dist(p, p ).
Algorithm 2 Calcul de la typicité Require: d distance, n cardinalité de la base S, Ensure: typicité des points :
On peut envisager deux façons de calculer la typicité : soit à la demande, sur les attributs spécifiés dans la clause skyline, soit au préalable, et dans ce cas un précalcul de la typicité sur différents ensembles d'attributs pertinents doit être effectué. Dans le premier cas, se pose la question du surcoût d'une telle opération. Le second cas soulève le problème de la mise à jour de la typicité en cas d'insertion/suppression d'un élément. Dans tous les cas, les index utilisés dans le cadre de la recherche des plus proches voisins (comme les kdtree) peuvent être exploités. En outre, comme précisé auparavant, le calcul du skyline graduel portant uniquement sur un fragment de la base, le surcoût du au calcul de la typicité ne devrait pas être pénalisant.
Enfin, l'algorithme proposé ici peut être adapté pour calculer en parallèle le skyline graduel en segmentant les tableaux A, B, A et T . De même, la création des structures A et B peut être parallélisée en segmentant leur remplissage (à condition de distribuer ou de partager les index et les valeurs de typicité).
Expérimentation sur un jeu de données réelles
L'approche proposée a été testée sur un extrait du site de vente de voitures d'occasion Le bon coin moteur essence, ce qui correspond à 441 annonces. La figure 5 montre les résultats obtenus. Kilométrage (x10 4 )  Table 3 montre un extrait de la 0.6-coupe du skyline graduel, qui comporte des points plus typiques. Elle offre des compromis aux annonces de la table 2, plus représentatifs de la base, et plus rassurants, tout en restant attractifs (le kilométrage est plus faible et le prix, sensiblement plus élevé). Mentionnons aussi que le temps nécessaire au pré-calcul de la typicité des éléments sélectionnés est deux fois plus important (de l'ordre de 0,54

Introduction
Les systèmes de classification automatique usuels ne permettent pas d'interagir directement avec un algorithme d'apprentissage, et par conséquent, les résultats qu'ils produisent sont, en pratique, souvent en décalage avec les points de vue des utilisateurs. Pour personnaliser ces systèmes, une solution est d'intégrer l'utilisateur, dans le processus d'apprentissage, pour qu'il génère, explicitement via un support visuel, ses propres classifieurs (Ware et al., 2001). L'utilisateur devient donc le coach qui va annoter, en positif ou négatif, un nombre limité d'exemples pour entraîner un algorithme à apprendre ses préférences. Ensuite, cet algorithme doit être capable de généraliser et de produire des prédictions personnalisées pour le reste des exemples non-classés. Interactivement, l'utilisateur peut corriger les mauvaises pré-dictions afin de renforcer le modèle. De tels systèmes d'apprentissage ont récemment suscité un intérêt croissant et ont trouvé des applications dans plusieurs domaines (e.g CueFLIK pour la classification d'images et CueT pour la classification d'alarmes (Amershi, 2011)).
Dans le domaine télévisuel, il existe des systèmes de recommandation automatique (Bambini et al., 2011) mais ils sont, à notre connaissance, mono-label et ne prennent pas en compte des labels subjectifs de perception qualitative (e.g. « ce film me fait peur ») qui sont à l'évi-dence importants pour les utilisateurs. Dans ce contexte, notre objectif final, qui dépasse le cadre de cet article, est de concevoir un système de classification interactive personnalisée pour mieux assister les utilisateurs dans leur recherche de contenus numériques. Ce travail requiert en préalable un état des lieux sur les capacités respectives des techniques de classification multi-label adaptées aux contraintes d'interactivité. Nous nous focalisons donc ici sur une comparaison expérimentale de 12 méthodes de classification multi-label avec des mesures d'évaluation adaptées à notre cas d'utilisation final, et des jeux de données classiques de difficulté croissante. En complément, nous effectuons une première évaluation des temps d'apprentissage et de prédiction, ainsi que de la vitesse de généralisation des classifieurs en variant le nombre d'exemples d'apprentissage.
La suite de cet article est organisée de la manière suivante : la section 2 définit plus pré-cisément les objectifs de notre étude. La section 3 présente les 12 méthodes de classification multi-label sélectionnées pour cette comparaison. Les jeux de données d'évaluation et le protocole expérimental sont décrits dans la section 4. Les résultats obtenus sont résumés et discutés dans la section 5.  (Madjarov et al., 2012)  Pour qu'un classifieur soit approprié à notre futur système de classification de VoD, il doit vérifier 3 propriétés essentielles : (1) pour chaque exemple sélectionné par l'utilisateur, les premiers labels qu'il suggère doivent être plus discriminants et plus pertinents que les labels suivants ; (2) les prédictions de labels qu'il fournit à l'utilisateur pour un exemple sélectionné doivent être proches des vrais labels, et (3) pour apprendre un modèle et fournir des prédictions à l'utilisateur, il doit être le plus rapide possible. L'évaluation de ces propriétés est basée sur les trois mesures suivantes.
Définition du problème
La mesure Rank-Loss (Madjarov et al., 2012) permet d'évaluer la conservation de l'ordre des labels :
La mesure Log-Loss (Read, 2010) permet d'évaluer le taux de dissimilarité entre les vrais labels et les labels prédits :
L'efficacité de résolution des algorithmes est évaluée ici par les temps d'apprentissage et de prédiction en terme de secondes.
Méthodes de classification multi-label
Les approches de classification multi-label se divisent en trois grandes familles selon (Madjarov et al., 2012) : 1) Méthodes de transformation : elles transforment le problème d'apprentissage multi-label en plusieurs problèmes de classification ou régression mono-label, 2) Méthodes adaptées : elles customisent des algorithmes d'apprentissage mono-label pour les adapter au cas multi-label et 3) Méthodes ensemble : elles utilisent des ensembles de classifieurs issus de la première ou la deuxième famille d'approches.
Pour notre comparaison expérimentale, nous évaluons 12 classifieurs multi-label, parmi lesquels nous retrouvons tous les classifieurs recommandés par (Madjarov et al., 2012) sauf RF-PCT que nous étudierons dans une prochaine étude expérimentale. Le choix des méthodes est fonction de leur fréquence d'utilisation dans la littérature, la disponibilité de leur implé-mentation et de leur appartenance aux différentes classes de méthodes. Les implémentations de ces algorithmes sont disponibles sur MEKA 1 . Dans tous les cas, les classifieurs sont exé-cutés avec leurs paramètres par défaut à l'exception de ML-kNN pour lequel, étant donné la petite taille des ensembles d'apprentissage, nous avons fixé son paramètre k à 1.
Dans la 1 ` ere famille, nous sélectionnons 5 méthodes : (1) Binary Relevance (BR), (2) Classifier Chain (CC), (3) Label Powerset (LP), (4) Calibrated Label Ranking (CLR) (Madjarov et al., 2012), et (5) une méthode qui retourne toujours la combinaison de labels la plus fré-quente dans l'ensemble d'apprentissage (Baseline). Les méthodes de transformation utilisent des classifieurs binaires et les plus utilisés sont : Support Vector Machine (SVM) et l'arbre de décision C4.5 (Read, 2010;Madjarov et al., 2012). Nous avons choisi ici C4.5 comme classifieur de base pour de meilleures performances en temps d'apprentissage car il exploite un sous-ensemble d'attributs contrairement à SVM qui requiert la totalité de l'ensemble. Ce choix est important dans notre contexte de VoD où le nombre d'attributs peut être très important.
Dans la 2 ` eme famille, nous choisissons 2 méthodes : (1) AdaBoost.MH et (2) ML-kNN (Madjarov et al., 2012). Enfin, dans la 3 ` eme famille, nous évaluons 3 méthodes : (1) RAndom k labEL sets (RAkEL), (2) Hierarchy Of multi-label classifiERs (HOMER), (3) Ensemble de Classifier Chains (ECC) (Madjarov et al., 2012) et Ensemble de Binary Relevance (EBR) (Read, 2010).
Evaluation
Pour la comparaison expérimentale des classifieurs, nous avons sélectionné 5 jeux de données souvent utilisés dans la littérature multi-label (Tab. 1). La dimensionnalité de leurs espaces d'attributs est petite comparée à la grande dimensionnalité d'un catalogue de VoD. Néanmoins, elles fournissent un premier aperçu sur le comportement de chaque classifieur. Afin d'éva-luer la performance prédictive et la rapidité de chacun des classifieurs, nous avons élaboré un protocole expérimental simple qui simule la phase initiale dans laquelle l'utilisateur crée TAB. 1 -Description des jeux données avec DL : nombre de combinaisons de labels diffé-rentes et Lcard : nombre de labels associé en moyenne aux exemples. . Ensuite, pour chaque mesure, nous évaluons la performance moyenne de chaque classifieur sur les 5 bases de test de chaque corpus : chaque classifieur est entraîné avec tous les sous-ensembles d'apprentissage de u 1 à u p . Puis, pour chaque classifieur, nous calculons, pour chaque taille de sous-ensemble d'apprentissage, la performance moyenne sur les 5 corpus.
Corpus
Dans toutes nos expérimentations, w a été fixé à 20 (i.e., 100 ensembles testés pour les 5 validations croisées) et p à 6. Le nombre d'ensembles testés est suffisant pour obtenir une performance moyenne de chaque classifieur avec un faible écart-type, et la taille maximale d'un sous-ensemble d'apprentissage (i.e., 64) est conforme au nombre d'exemples qu'un utilisateur pourrait annoter au maximum dans un cas d'usage réel.
Résultat expérimentaux
Les résultats présentés, dans les graphiques de la Fig. 1, sont des moyennes et des écarts-types de performances sur les 5 jeux de données. Les classifieurs sont ordonnés en fonction de leur performance globale qui tient compte de leurs performances moyennes obtenues pour toutes les tailles des sous-ensembles d'apprentissage.
Log-Loss pour la prédiction des labels
Comme nous le constatons sur la Fig. 1, ML-kNN  
Rank-Loss pour l'ordonnancement des labels
Globalement, les classements des classifieurs avec les mesures Log-Loss et Rank-Loss sont assez semblables car elles sont intuitivement corrélées. Cependant, CLR devient la meilleure approche pour Rank-Loss et Adaboost.MH passe de la 2 ` eme place à la 6 ` eme place. Il n'est pas surprenant que CLR obtienne les meilleurs résultats car elle a été conçue spécialement pour améliorer la qualité du classement des labels. De même, ML-kNN vise à minimiser cette mesure. En outre, lorsque le nombre d'exemples d'apprentissage augmente, CLR et ML-kNN sont les plus efficaces pour améliorer la qualité du classement des labels.
Temps d'apprentissage/prédiction
Comme toutes nos expérimentations ont été menées avec des implémentations de la librairie MEKA, que nous ne maîtrisons pas, les cumuls des temps d'apprentissage et de pré-dictions calculés ne donnent qu'une tendance de la complexité algorithmique (Fig. 1). Par exemple, EBR devrait être plus rapide que ECC mais nous observons ici le contraire. Parmi les meilleures approches (ECC, EBR et ML-kNN) pour les mesures précédentes, ML-kNN semble le plus rapide. En effet, il n'apprend pas de modèle mais nécessite seulement quelques millisecondes pour estimer les probabilités a priori/a posteriori à partir du sous-ensemble d'apprentissage, et moins d'une demi seconde pour la prédiction parce que le nombre de voisins fixé est faible ici (i.e., k = 1). Par ailleurs, EBR et ECC nécessitent plus de temps mais sont suffisamment rapides pour terminer dans les premières secondes. Dans une prochaine étape, nous allons étendre cette étude expérimentale avec d'autres approches intéressantes de classification multi-label telle que RF-PCT, et nous sommes en train d'analyser le passage à l'échelle des différentes approches afin de s'approcher des tailles de données réelles rencontrées dans le domaine de la VoD.
Conclusion et Travaux futurs

Introduction
En termes de performance de classification de textes, KNN se classe parmi les classifieurs les plus performants, un résultat obtenu d'une multitude de tests de comparaison effectués sur le corpus Reuters Yang (1999). En contraste avec ses performances de classification, il est reconnu que cet algorithme est lent puisqu'il requiert qu'une mesure de similarité soit calculée entre tous les documents d'apprentissage et le nouveau document. Il est caractérisé par un apprentissage très rapide, il est facile à apprendre, il est robuste aux ensembles d'apprentissage bruités et il est efficace si le corpus est grand Bhatia et Vandana (2010). Un inconvénient majeur du KNN reste le temps qu'il met pour classer un nouveau document. Différentes solutions ont été proposées pour réduire la complexité de calcul. Nous nous intéressons, dans ce papier, aux méthodes de sélection de prototypes. Plus précisément, nous étudions l'impact de diffé-rentes méthodes de sélection de prototypes sur la performance de la catégorisation de textes avec le classifieur KNN. Essentiellement, voici comment se structure la suite du papier, la section 2 présente une série de méthodes de sélection de prototypes, en décrivant leurs principales caractéristiques. La section 3 présente les différentes expérimentations effectuées sur les différents corpus de textes pour comparer les différentes méthodes de sélection de prototypes. La conclusion générale résume le travail effectué et les résultats obtenus.
Sélection de prototypes
Depuis la création de l'algorithme des plus proches voisins en 1967Hart et Cover (1967, une grande variété de techniques de sélection de prototypes ont fait leur apparition pour remédier aux principaux inconvénients associés à l'algorithme et ses variations José (2002); Olvera-López et al. (2010); Garcia et al. (2012). Leur objectif principal consistait à améliorer le temps de classification du KNN.
Principe des méthodes de sélection de prototypes
Étant donné un ensemble d'apprentissage DT, l'objectif d'une méthode de sélection de prototypes (notée dans la suite SP) est d'obtenir le sous-ensemble d'instances DS ? DT tel que DS ne contient pas d'instances inutiles et lorsqu'on classe une nouvelle instance Q par la règle KNN en agissant sur DS au lieu de DT nous avons P (DS) P (DT ) Olvera-López et al. (2010). Selon le type de sélection, ces algorithmes peuvent être classés en trois catégories.
Algorithme de condensation
Ces algorithmes essayent de trouver une réduction significative de l'ensemble des instances de telle façon les résultats de classification avec KNN sont aussi proches que possible de ceux obtenus en utilisant tous les cas originaux. Ils cherchent les instances qui correspondent à leurs voisins les plus proches. Étant donné que ces instances fournissent les mêmes informations de classification que leurs voisins, elles peuvent être retirées sans dégrader l'exactitude de la classification des autres instances qui les entourent. Nous distinguons dans cette catégorie la méthode la plus ancienne CNN décrite par Hart (1968). La performance de l'algorithme CNN n'est pas bonne, mais elle a inspiré la construction de nouvelles méthodes telles que RNN Gates (1972), SNN Ritter et al. (1975), TCNN Tomek (1976), POP Riquelme et al. (2003) et FCNN Angiulli (2005).
Algorithme d'édition
Les algorithmes d'édition tentent de découvrir et de supprimer les instances bruitées. Les instances bruitées peuvent provoquer des erreurs de classification. Par conséquent, leur suppression devrait aider à augmenter l'exactitude de la classification. Le procédé est décrémental et une instance est éliminée si elle est mal classifiée par un vote à la majorité sur ses K plus proches voisins. C'est l'algorithme ENN de Wilson (1972). ENN permet de résoudre le problème d'instances bruitées avec une bonne performance, mais le taux de réduction reste toujours faible en le comparant à d'autres méthodes Olvera-López et al. (2010). Une autre variante de la méthode ENN est ALLKNN Tomek (1976).
Algorithmes hybrides
Ces algorithmes permettent à la fois une élimination des instances bruitées et inutiles. Aha et al. (1991) ont proposé une série d'algorithmes dont IB3 est la version la plus aboutie. Cano et al. (2003) ont présenté une étude expérimentale de différents algorithmes évolutionnaires, en fonction de leurs résultats, l'approche génétique CHC a obtenu les meilleures performances en précision et en réduction. En outre, CHC est la méthode qui exigeait moins de temps d'exécu-tion. La méthode Drop3 Wilson et Martinez (2002), utilise le filtrage de bruit avant de trier les instances de DS. Les objets restants sont classés par mesure de distance avec l'objet de classe différente qui est le plus proche restant dans DS, et donc les objets loin de la frontière de dé-cision réelle sont supprimés en premier. la méthode SSMA Garcia et al. (2012) a été proposée pour couvrir un inconvénient majeur des méthodes évolutionnaires classiques : leur manque de convergence face à de grands problèmes.
Étude expérimentale
Notre étude se concentre sur un problème particulier, il s'agit de voir si l'utilisation de l'une des techniques de sélection de prototypes aidera à améliorer la catégorisation de textes avec les K plus proches voisins point de vue efficacité et efficience.
Corpus utilisés et mesures d'évaluation
Nous menons une étude expérimentale impliquant différentes tailles d'ensembles de documents pour mesurer la performance des méthodes de sélection de prototypes en termes de précision, de capacités de réduction et d'exécution dans le cadre de la catégorisation de textes. Les textes des différents corpus subissent un ensemble de traitements pour récupérer une représentation numérique exploitable par l'algorithme d'apprentissage. Cette représentation est appelée représentation vectorielle. Pour prédire la classe d'un nouveau document, l'algorithme cherche les k plus proches voisins de ce nouveau document en calculant la distance euclidienne et ensuite par vote majoritaire prédit la réponse la plus fréquente de ces k plus proches voisins. Nous avons calculé, dans chaque expérience, le taux de réduction (Red), l'exactitude (A), la F-mesure micro (F µ ) la F-mesure macro (F m ), le temps de réduction (TR) en secondes et le temps de classification (TC) en secondes.
Résultats et discussion
Dans le tableau 1 sont donnés les meilleurs résultats des différentes expériences réalisées avec le corpus Webk. Pour ce corpus, les quatre méthodes IB3, RNG, SSMA et ENN combinées avec KNN donnent les meilleurs résultats en termes d'Exactitude. Par contre aucune méthode de condensation n'a amélioré les résultats de KNN. En examinant le taux de ré-duction, nous constatons que les trois méthodes MCNN, CHC et SSMA donnent les taux de réduction les plus élevés. Comme on peut le voir sur le tableau 1, ALLKNN et FCNN donnent un taux de Fmesure (micro ou macro) le plus élevé par rapport à l'ensemble des méthodes, mais elles sont moins précises que KNN. Il est particulièrement remarquable que plus le taux de réduction augmente plus le temps de classification diminue. CHC et SSMA produisent un taux de réduction le plus élevé mais on voit bien qu'elles nécessitent un temps de réduction très élevé. En termes de rapidité de classification ce sont les approches de condensation MCNN, POP et FCNN qui sont les meilleures. Nous remarquons à partir de ces expériences et en tenant compte à la fois de l'exactitude, et du taux de réduction que la méthode SSMA est meilleur que KNN dans la plupart des cas. En d'autres termes, elle permet un meilleur compromis entre exactitude et taux de réduction mais elle reste toujours gourmande en temps de réduction, par exemple elle nécessite environ 259 secondes pour réduire un corpus de 4199 documents indexé avec 300 termes. Dans le cas du corpus 20NewsGroups, les expériences sont effectuées avec 80 % pour l'apprentissage et les 20 % restants pour le test. En examinant le tableau 2, les résultats montrent que les deux méthodes MCNN et ENRBF qui donnent le meilleur taux de réduction par rapport aux autres, donnent aussi un meilleur compromis entre le temps de classification et le taux de réduction. Par contre les méthodes POP, IB3 et FCNN nécessitent un temps de classification plus élevé par apport à KNN. Une dernière remarque, qui est peut-être très importante, est que CNN et DROP3 sont assez lentes en temps de réduction, également SSMA et CHC qui n'ont pas participé pour ce corpus à cause de cette contrainte de temps. Les expériences avec le corpus Reuters sont effectuées avec 80 % du corpus, l'équivalent de 9100 documents, pour l'apprentissage et 20 % pour le test. D'après les résultats présentés dans le tableau 3, POP et SSMA offrent les meilleurs résultats de classification en termes de F mesure. Aucune méthode de condensation n'a pu améliorer le 1NN. Lorsque K=10, les méthodes de condensation POP, FCNN dépassent KNN en termes de Fmesure bien que les taux de ré-duction restent faibles, en particulier celui de la méthode POP. On peut remarquer aussi que les méthodes les plus performantes sont les approches incrémentales de condensation. En termes d'exactitude, de réduction et du temps de classification RNG , SSMA et CHC offrent un bon taux, mais elles sont lentes pendant la réduction.
Conclusion
De nombreuses méthodes de SP ont été étudiées Garcia et al. (2012), mais une conclusion précise ne peut être donnée sur la meilleure méthode. Nous réalisons que le choix dépend alors du problème à résoudre, mais les résultats des différentes expériences obtenus par plusieurs chercheurs pourraient toujours nous aider à s'orienter vers certaines méthodes qu'ils considèrent comme intéressantes. En effet cette étude bibliographique et expérimentale nous a permis de découvrir plusieurs méthodes qui sont intéressantes sur le plan performance et sur le plan efficacité. En général, les meilleures méthodes de SP en termes de performance sont RNG et SSMA, mais elles ont pour principal défaut le temps de réduction qui reste élevé. Les meilleures méthodes pour la réduction sont MCNN, CHC et SSMA. Nous avons constaté que les méthodes hybrides permettent des taux de réduction élevés, tout en préservant la performance mais elles sont les plus lentes. Des méthodes plus rapides permettant d'atteindre des taux de réduction élevés sont les approches de condensation comme MCNN, mais nous constatons que cette dernière n'est pas en mesure d'améliorer le KNN en termes de précision. Certaines méthodes présentent des différences claires lorsqu'il s'agit d'un grand corpus (voir tableau 2), c'est le cas de POP, FCNN et le cas également de ENRBF qui ont amélioré le KNN en temps de classification avec un taux de réduction intéressant. Les autres méthodes comme AllKNN, IB3, ENN ont pu améliorer KNN qu'avec le petit corpus WebK. 

Introduction
La recherche dans l'ingénierie et la gestion des connaissances s'était de plus en plus concentrée sur les problèmes de l'acquisition, de la conservation et du transfert des connaissances. Cependant, considérant la masse de connaissances à préserver dans la mémoire d'une entreprise, cette dernière est amenée à engager une réflexion afin de repérer celles qui devraient faire l'objet d'une capitalisation. Dans cette perspective, Saad et al. (2005) ont proposé une méthode de repérage des connaissances cruciales composée de deux phases. L'objectif de la première phase est d'inférer un modèle de préférences des décideurs qui se traduit par des règles de décision. La deuxième phase a pour objectif est de classer les "connaissances potentiellement cruciales" en utilisant les règles précédemment inférées. La première phase est basée sur l'approche constructive de Belton et Pictet (1997) qui repose sur une concertation entre les décideurs pour déterminer un ensemble des règles collectivement accepté à partir des différentes règles individuelles. Dans un contexte organisationnel distribué, cette procédure est difficilement applicable quand des contraintes de temps et de distance géographique s'ajoutent à une masse de connaissances grandissante et à un nombre de décideurs important. Notre objectif est, donc, de proposer une approche argumentative, basée sur la théorie des ensembles approximatifs pour automatiser la procédure de résolution de conflits entre les déideurs dans une organisation. Le papier est structuré comme suit : Section 2 détaille l'approche argumentative. Section 3 illustre notre étude expérimentale et la section 4 résume notre contribution.
Approche argumentative multicritères
Dans ce papier, nous proposons une approche argumentative basée sur les notions de force, borne, noyau, qualité d'approximation (QA) et règle de décision définies par la méthode DRSA(Dominance-based Rough Set Approach) proposée par Greco et al. (2001). Elle repose sur un protocole d'interaction et des stratégies de construction et d'évaluation d'arguments et de contre-arguments (Bouzayane et al., 2013).
Protocole d'interaction
Le protocole d'intéraction est décrit par deux processus argumentatifs. Le processus argumentatif exécuté par un initiateur est déclenché par la réception du message call_for_argument de la part de l'intermédiaire, suite auquel l'initiateur construit son premier argument envoyé au décideur récepteur. Il reste en attente d'une réponse. Il peut recevoir trois types de messages : un accept, un reject ou un contre-argument (message justify). S'il s'agit d'un contre-argument, l'initiateur procède à une phase d'évaluation afin de raisonner sur l'action à entreprendre. Le processus argumentatif exécuté par un récepteur est déclenché, ainsi, dès la réception d'un message justify de la part de l'initiateur. Le récepteur s'engage dans une phase d'évaluation de l'argument reçu afin de décider s'il le rejette, l'accepte ou le contre-argumente.
Stratégie de construction d'un argument
La stratégie de construction d'un argument que nous définissons est composée de trois étapes successives, soumises à un pré ordre croissant (Bouzayane et al., 2013) :
-Etape ? 1 : le décideur initiateur, Dm init , cherche, parmi les règles supportant la classification défendue de K i , à sélectionner la ou les règles de force maximale. L'ensemble des règles sélectionnées est noté ?. Si |?|=1, la règle est rétenue pour faire l'objet d'un argument. Sinon, passe à l'étape ? 2 . -Etape ? 2 : L'initiateur choisit parmi les règles de l'ensemble ?, la ou les règles qui ont le plus de critères en commun avec son noyau. L'ensemble des règles construit est nommé ? . Si |? |=1, cette règle est rétenue. Sinon, passe à l'étape ? 3 . -Etape ? 3 : Une règle de ? est sélectionnée si et seulement si elle contient un nombre maximal de critères dans sa prémisse. L'ensemble de règles construit est noté ? . Si |? |=1, la règle est rétenue pour faire l'objet d'un argument. Sinon l'initiateur sélec-tionne aléatoirement une règle de l'ensemble le plus sélectif ? . Pour construire un contre-argument, le décideur raisonne non seulement sur ses informations locales mais aussi sur l'argument reçu est évalué (Bouzayane et al., 2013).
Evaluation d'un argument ou d'un contre-argument
L'évaluation d'un argument ou d'un contre-argument dépend de deux critères : l'impact de l'argument reçu sur la QA du récepteur et le niveau de certitude selon lequel la connaissance est classée (son appartenance à la borne). Le récepteur doit, ainsi, raisonner sur les actions à entreprendre en calculant la nouvelle QA (NAQ) résultante une fois l'argument soit accepté. On note OAQ , la qualité d'approximation initiale que le récepteur avait juste avant la réception de l'argument. Avant de répondre, le récepteur doit raisonner comme suit : S. Bouzayane et I. Saad -Si la QA initiale du décideur récepteur est maximale (OAQ=1), donc si la nouvelle QA reste aussi maximale (NAQ=1), le récepteur accepte, sinon il rejette. -Si la qualité d'approximation initiale du récepteur est inférieur à 1 (OAQ < 1), donc :
-Si (NAQ < OAQ) : Il rejette. Cela parce que la nouvelle classification, si elle est acceptée, elle augmente le nombre des connaissances dans la borne et influe négative-ment la qualité du décideur en diminuant la valeur de sa qualité d'approximation.
-Si (NAQ > OAQ) : Il accepte. En effet, la nouvelle classification aidera à améliorer sa qualité d'approximation et à corriger , ainsi, quelques classifications incertaines.
-Si (NAQ = OAQ) : Il raisonne sur sa borne. Si la connaissance objet du conflit appartient à sa borne, ie classée avec incertitude, il contre argumente. Sinon, il rejette.
Expérimentations et résultats
Notre étude expérimentale (développée à l'aide du languge JAVA) est basée sur des données réelles déjà testées et validées dans (Saad et al., 2005)  toutes les courbes extraites, nous traitons le cas où plus que la moitié des connaissances objets de conflits appartiennent à la borne et celui où la majorité de ces connaissances ont été classées avec certitude. La première courbe (cf. Fig.1) montre que notre approche a réussi à résoudre jusqu'à 81% des conflits dans le premier cas et jusqu'à 63% dans le deuxième ce qui prouve sa sensibilité par rapport à la borne. Plus que les connaissances objets de conflits appartiennent à la borne, plus que la chance de conclure le processus argumentatif avec succès augmente. Dans la deuxième courbe (cf. Fig.2) nous traitons le taux d'amélioration de la QA qui représente la différence entre la QA initiale d'un décideur et sa QA finale obtenue après avoir discuté tous les conflits. Nous remarquons que le taux d'amélioration de la QA sur des connaissances de la borne est plus élevé que celui des classifications certaines. Cela prouve que les décideurs ayant une QA moins élevée sont plus proches à changer leurs préférences. Notre méthode est, ainsi, sensible aux profils des décideurs engagés dans le système argumentatif. Ces courbes tracent aussi un taux d'amélioration jusqu'à 0.62 pour un récepteur et 0.15 pour un initiateur.

Introduction
Le but de cet article est d'introduire un cadre d'élagage basé sur les symétries pour la fouille de données. Dans d'autres domaines, tels que la programmation par contraintes et la satisfaisabilité propositionnelle, les symétries sont souvent exploitées pour élaguer l'espace de recherche et améliorer les performances des solveurs. Notre but est de montrer l'apport d'un élagage à base de symétries dans un cadre de fouille de données. Dans cet article, nous nous intéressons au cas de l'extraction de motifs ensemblistes et nous prenons comme exemple d'application l'algorithme APRIORI (Agrawal et Srikant, 1994).
Les symétries sont un concept fondamental en informatique, mathématiques, physiques et plein d'autres domaines. Elles existent dans divers problèmes réels. Les symétries sont communément exploitées dans la résolution de problèmes combinatoires, tels que les problèmes d'ordonnancement. Par exemple, dans un problème d'ordonnancement où certaines machines peuvent être interchangeables, partant d'un ordonnancement valide, on peut en obtenir un autre valide en permutant ces machines. Exploiter les symétries permet de réduire le coût de la recherche de solution, en évitant d'explorer des branches symétriques de l'espace de recherche. Beaucoup de travaux ont ainsi porté sur les symétries dans les problèmes de satisfaction de contraintes (CSP) (e.g. (Puget, 1993;Gent et Smith, 2000)), satisfaisabilité propositionnelle (e.g. (Benhamou et Saïs, 1994;Crawford et al., 1996)) et recherche opérationnelle (e.g. (Margot, 2003;Liberti, 2012)).
En fouille de données, les symétries sont principalement étudiées en fouille de graphes (e.g. (Desrosiers et al., 2007;Vanetik, 2010)). Récemment, (Jabbour et al., 2012) a proposé un nouveau cadre pour casser les symétries dans les problèmes de fouille de données. Dans (Jabbour et al., 2012), les symétries représentent des permutations entre les items laissant invariant le jeu de données. L'exploitation des symétries dans ce cadre s'effectue en pré-traitement permettant de supprimer des items du jeu de données initial. Cette approche est différente de celle utilisée dans les problèmes CSP et SAT où les symétries sont éliminées en ajoutant au réseau de contraintes, des contraintes supplémentaires appelées prédicats d'élimination de symétries. (Jabbour et al., 2012), mentionne aussi que les symétries peuvent être intégrées dans les algorithmes de type APRIORI pour l'élagage de l'espace de recherche. Dans cet article, nous explorons en profondeur cette piste. Notre motivation vient du fait que l'élagage basé sur les symétries peut être généralisé à d'autres tâches de fouille de données ce qui n'est pas le cas pour l'approche présentée dans (Jabbour et al., 2012). Notre approche nous permet aussi d'exploiter efficacement les symétries présentes au sein d'une même transaction, ce qui n'était pas le cas avec l'approche de (Jabbour et al., 2012).
Nous avons choisi l'algorithme APRIORI, algorithme pionnier de l'extraction de motifs fréquents et de règles d'association, comme exemple pour montrer la faisabilité de notre approche d'élagage à base de symétries. Les expérimentations menées montrent la faisabilité et l'efficacité de notre approche. À la limite de nos connaissances, notre approche est la première approche opérationnelle permettant l'exploitation des symétries dans les problèmes d'extraction de motifs ensemblistes.
Définitions
Extraction de motifs
Soit I un ensemble d'items et T un ensemble d'identifiants de transactions. Un ensemble I ? I est appelé itemset ou motif. Une transaction est un couple (tid, I) où tid est un identifiant de transaction (tid ? T ) et I est un itemset (I ? I). Une base de données transactionnelle D est un ensemble fini de transactions ayant chacune un identifiant unique. Nous notons T id (D) = {tid|(tid, I) ? D} l'ensemble des identifiants des transactions de D. I items (O) dénote l'ensemble de tous les items appartenant à l'objet syntaxique O (e.g. base de données transactionnelle, motif, etc). On dit que la transaction (tid, I) supporte un motif J si J ? I.
La couverture d'un motif I dans D est l'ensemble des transactions de D supportant I :
Exemple 1 Soit le jeu de données transactionnel D (cf. Commençons par définir formellement les symétries dans le cadre de l'extraction de motifs fréquents.
Définition 1 (Permutation) Une permutation p d'un ensemble fini S est une bijection de S vers S.
Chaque permutation p peut être représentée par un ensemble de cycles, c 1 . . . c n où chaque cycle c i = (a 1 , . . . , a k ) est une liste d'éléments de S tel que p(a j ) = a j+1 pour j = 1, . . . , k? 1, et p(a k ) = a 1 . Dans la suite, pour des raisons de clarté, nous omettons les cycles de la forme (a, a).
Nous notons par P(I items (D)) (resp. P(T id (D))) l'ensemble de toutes les permutations dans I items (D) (resp. dans T id (D)).
Nous notons par
Exemple 2 Considérons le jeu de données D de la Table 1 Comme l'ensemble des transactions est invariant par symétrie, toutes les propriétés d'un motif telles que la fréquence, la fermeture et la maximalité sont préservées. La proposition suivante montre que la symétrie préserve la fréquence d'un motif.
Preuve 1 ? • f étant une symétrie, le support de I est égal à celui de ?(I). Par conséquent,
Détection de symétries dans les bases de données transactionnelles
Un moyen commun pour détecter les symétries est d'encoder les données sous forme de graphe et de rechercher les automorphismes de graphe. La plupart des outils de détection de symétries se basent principalement sur les automorphismes d'un graphe coloré. Les couleurs des sommets sont utilisées pour contraindre le lien avec des sommets de la même couleur, i.e. deux sommets de la même couleur peuvent être permutés. La première implémentation pour calculer les automorphismes de graphes, appelée nauty, est proposée dans (McKay, 1981). D'autres implémentations améliorées utilisant des heuristiques d'élagage provenant du domaine de la théorie des groupes sont présentées dans (Aloul et al., 2003;Junttila et Kaski, 2007).
Ainsi, pour détecter les symétries dans un jeu de données transactionnel D, il suffit d'encoder D sous forme de graphe coloré G de telle sorte que les symétries dans D correspondent aux automorphismes de G.
Définition 3 Un graphe coloré est un triplet G = (V, E, ?) où V est l'ensemble de ses sommets, E ? V × V l'ensemble de ses arêtes et ? est une fonction de V dans N qui associe un entier positif (une couleur) à chaque sommet.
Nous montrons maintenant comment un jeu de données transactionnelles peut être encodé sous forme de graphe coloré non orienté.
La figure 1 décrit la conversion de D (cf.  
Élagage à base de symétries
Dans cette section, nous montrons comment les symétries peuvent être exploitées afin d'élaguer l'espace de recherche dans le problème d'extraction de motifs fréquents. Nous utilisons l'exemple de l'algorithme APRIORI (Agrawal et Srikant, 1994) (cf. Algorithme 1). Cet algorithme se base sur la propriété d'anti-monotonie : si un motif est fréquent alors toutes ses généralisations le sont aussi. Il procède par une recherche par niveau des motifs fréquents. En effet, il commence par extraire l'ensemble F 1 des motifs fréquents de taille 1 (ligne 1). Puis il calcule itérativement les ensembles de F 2 (fréquents de taille 2) jusqu'à (F n ) tel que F n+1 = ? (lignes 2-8).
Nous décrivons dans ce qui suit comment nous avons intégré notre approche d'élagage à base de symétries dans l'algorithme APRIORI. Notre approche contribue essentiellement à réduire l'ensemble des motifs candidats tout comme la propriété d'anti-monotonie. Notre approche est décrite dans l'algorithme 2.
Algorithme 1 : APRIORI Données : D : jeu de données, ? : seuil de support minimal Sortie : l'ensemble de tous les motifs fréquents dans D F1 ? {motifs fréquents de taille 1};
Algorithme 2 : APRIORI Sym Données : D : jeu de données, ? : seuil de support minimal, S : symétries dans D Sortie : l'ensemble de tous les motifs fréquents dans D F1 ? {motifs fréquents de taille 1}; Lignes 1-4. Cette partie est similaire à APRIORI. Néanmoins, l'ensemble des symétries S est utilisé pour améliorer le calcul des itemsets fréquents (ligne 1). Par exemple, si {a} est fréquent (resp. non fréquent) alors, pour tout ? • f ? S, le motif {?(a)} est aussi un motif fréquent (resp. non fréquent). Nous présentons dans cette section quelques résultats expérimentaux montrant la faisabilité et l'apport de notre approche.
Nous présentons dans ce qui suit des expérimentations menées sur des jeux de données du répertoire de l'UCI 1 et le jeu de données réel BMS-WebView-2 (Zheng et al., 2001). Ce je de données contient des données sur les flux de clics enregistrés sur des sites de e-commerce. La Table 2  Les symétries sont présentes dans les jeux de données indépendamment de toute tâche d'extraction de motifs, ainsi notre approche d'élagage basée sur les symétries procède en deux étapes. Nous commençons par extraire les symétries en utilisant Saucy. Ensuite, l'entrée de notre algorithme APRIORI Sym sera composée de l'entrée standard de l'algorithme APRIORI (i.e. jeu de données et un seuil de fréquence minimale) à laquelle on ajoute l'ensemble des symétries extraites.
Afin de quantifier le gain obtenu par APRIORI Sym , nous introduisons les deux mesures suivantes :
: permet de quantifier le gain obtenu en terme de temps d'exécution. Notons que cette mesure ne prend pas en considération le temps nécessaire pour extraire les symétries étant donné que cette opération n'est effectuée qu'une et une seule fois pour chaque jeu de données considéré.
#bdd scans(APRIORI)
: permet de quantifier le gain obtenu en terme de réduction du nombre d'opérations de calcul de fréquences et, par conséquent, du nombre de scans de la base de données. Un autre résultat important, est que l'algorithme APRIORI sym garde les mêmes performances qu'APRIORI lorsqu'il s'agit de jeux de données ne contenant pas de symétries (cf .  Table 4). Ainsi, notre approche d'élagage basée sur les symétries peut être généralisée à d'autres algorithmes d'extraction de motifs leur permettant de tirer profit d'une éventuelle présence de symétries dans les jeux de données. Nous montrons dans la section 5 la faisabilité de cette généralisation.
Résultats expérimentaux
Notons que le jeu de données BMS-WebView-2 contient 10 symétries. Nous n'observons pourtant aucune influence de ces symétries sur l'élagage de l'espace de recherche. En effet, l'efficacité de l'élagage par symétrie n'est pas proportionnelle au nombre de symétries détec-tées : si les symétries détectées n'apparaissent que dans très peu de transactions, comme c'est le cas pour BMS-WebView-2, tous les motifs les impliquant sent non fréquents et sont alors efficacement élagués par APRIORI.
Discussion
Les résultats expérimentaux montrent que notre approche d'élagage à base de symétries intégrée à l'algorithme APRIORI permet d'améliorer considérablement les performances de ce dernier. Pour un jeu de données ne contenant pas de symétries, APRIORI sym a les même performances que APRIORI. Ceci est expliqué par le fait que les symétries sont extraites une et une seule fois pour un jeu de données donné. Le gain en terme de temps d'exécution et ainsi toujours supérieur ou égal à zéro (M T A (APRIORI Sym ,APRIORI ) ? 0).
Les symétries sont ainsi une propriété importante dans les données qui devrait être considérée dans le but d'améliorer les performances des algorithmes d'extraction de motifs actuels sur certaines familles de jeux de données.
Généralisation de l'utilisation des symétries
Dans cette section, nous proposons une généralisation de notre approche à d'autres problèmes d'extraction de motifs. Nous nous plaçons dans le cadre unificateur de (Mannila et Toivonen, 1997) 6 Symétries en fouille de données : état de l'art En fouille de données, les symétries sont une propriété structurelle qui peut être exploitée pour élaguer l'espace de recherche ou réduire la taille de la sortie. Les symétries peuvent aussi, dans certaines applications, être vues comme étant une connaissance pertinente à extraire. Elles permettent, par exemple, aux analystes de données de mieux comprendre certaines corrélations entre les items. Beaucoup d'intérêt a été accordé à l'exploitation des symétries dans le domaine de l'intelligence artificielle, notamment pour les problèmes SAT (Benhamou et Saïs, 1994;Crawford et al., 1996) et CSP (Puget, 1993). En fouille de données les symétries sont principalement étudiées dans le cadre de la fouille de graphes (Vanetik, 2010;Desrosiers et al., 2007).
Dans (Desrosiers et al., 2007), les auteurs exploitent les symétries de sous graphes pour élaguer l'espace de recherche lors de la génération de sous graphes candidats. Les symétries (au sens automorphismes) sont utilisées dans (Vanetik, 2010) en tant que mesure d'intérêt. Dans, (Garrido, 2011), les auteurs proposent une approche pour détecter les symétries dans les réseaux sociaux.
Les symétries sont présentes aussi dans le clustering de données. Par exemple, (Murtagh et Contreras, 2010) traite le cas de clustering hiérarchique dans de grandes masses de données pour détecter des symétries et d'autres motifs intéressants. Les symétries sont considérées dans ce cas comme une structure révélant des propriétés intrinsèques et invariantes dans les données.
Nous constatons enfin que peu de travaux se sont intéressés aux symétries dans le cadre de l'extraction de motifs ensemblistes. Nous mentionnons tout de même deux travaux traitant un cas particulier de symétries (Minato, 2006;Medina et al., 2005). En effet, les symétries traitées dans ces papiers, sont dites symétries de paires. Elles permettent d'échanger à chaque fois un couple d'items en laissant les autres items inchangés. (Minato, 2006) propose un algorithme ZBDD efficace permettant la détection des symétries de paires et discute les propriétés des items symétriques. Néanmoins, ce travail reste restreint à des cas très particuliers de symétries. Les paires d'items symétriques, appelés items clones par Medina et Nourine dans (Gély et al., 2005), sont utilisées pour expliquer pourquoi, dans certains cas, le nombre de règles d'une couverture minimale d'une relation est exponentiel en nombre d'items.
Conclusion et travaux futurs
Nous avons proposé dans cet article une approche d'élagage à base de symétries pour les problèmes d'extraction de motifs ensemblistes. Les symétries sont des propriétés structurelles qu'on détecte dans un grand nombre de bases de données. Elles ont déjà montré leur inté-rêt dans divers problèmes de satisfaction, d'optimisation et d'énumération. Dans cet article, nous avons montré comment une telle propriété peut être exploitée dans la phase d'élagage de l'algorithme pionnier d'extraction de motifs fréquents APRIORI permettant de réduire considérablement le nombre d'accès à la base et de calcul de fréquences. Nous avons montré expéri-mentalement que dans certains cas, les calculs de fréquences sont réduits d'environ 24% grâce à l'exploitation des symétries. L'un des points forts de cette approche réside dans le fait que la détection des symétries ne s'effectue qu'une et une seule fois pour chaque jeu de données et est indépendante de toute tâche d'extraction de motifs. Nous avons montré aussi que notre approche peut être généralisée à d'autres tâches d'extraction de motifs tant que le prédicat de sélection en question est stable par symétrie.
Dans nos futurs travaux, nous envisageons d'étendre notre approche d'élagage à base de symétries à la fouille de séquences. D'autres problèmes de fouille de données tel que le clustering pourrait aussi tirer profit de la présence des symétries dans les données. Nous souhaitons aussi trouver un moyen d'exploiter les symétries locales dans les données (symétries dans une portion des données uniquement).
Summary
In this paper, we show how symmetries, a fundamental structural property, can be used to prune the search space in itemset mining problems. Our approach is based on a dynamic integration of symmetries in APRIORI-like algorithms to prune the set of possible candidate patterns. More precisely, for a given itemset, symmetry can be applied to deduce other itemsets while preserving their properties. We also show that our symmetry-based pruning approach can be extended to the general Mannila and Toivonen pattern mining framework. Experimental results highlight the usefulness and the efficiency of our symmetry-based pruning approach.

Introduction
Twitter offre des fonctionnalités de microblogging qui sont utilisées par des millions de personnes à travers le monde pour publier des messages courts. Ces personnes créent et partagent de l'information liée à divers types d'évènements, allant d'évènements personnels banals à des évènements importants et/ou globaux, quasiment en temps-réel. L'explosion du nombre d'utilisateurs de ce service de réseautage social a entraîné l'apparition d'un phénomène de surcharge informationnelle. Pour lutter contre cela, il est nécessaire de doter les utilisateurs de moyens leur permettant d'identifier plus facilement les éléments d'information les plus intéressants et de se tenir au courant des derniers évènements significatifs.
L'information brute produite par Twitter est délivrée sous la forme d'un flux de messages courts. Par conséquent la manière dont ceux-ci arrivent au fil du temps recèle une part importante de leur signification. La dynamique temporelle des thématiques les plus populaires sur ces réseaux est constituée d'une succession de focus et dé-focus, autrement dits, une succession de pics de popularité (Leskovec et al., 2009). C'est pourquoi de nombreuses approchesallant de méthodes basées sur la fréquence des mots (Benhardus et Kalita, 2013) jusqu'à des méthodes plus complexes reposant sur des modèles de thématiques probabilistes dynamiques (Lau et al., 2012) -ont été proposées dans le but d'identifier ce genre de thématiques. Dans cet article, nous présentons un système implémentant la méthode décrite par Guille et Favre (2014). Contrairement à la majorité des méthodes existantes, celle-ci prend en compte l'aspect social du flux traité en considérant la fréquence de création de liens dynamiques entre utilisateurs. Un utilisateur crée un lien dynamique en insérant une ou plusieurs mentions (i.e. « @pseudonyme ») dans un tweet. Ce lien entre utilisateurs est dynamique car lié au contenu du tweet et sa durée de vie. Par ailleurs, cette méthode localise plus précisément dans le temps les thématiques que les méthodes existantes et traite les grands volumes de données plus efficacement que celles à base de modèles de thématiques probabilistes.
La suite de cet article est organisée comme suit. Dans la section 2 nous décrivons le fonctionnement du système puis dans la section 3 nous détaillons le cadre de la démonstration. Enfin dans la section 4 nous concluons. 
Le système de détection des thématiques populaires
L'objectif du système est d'identifier des thématiques à la fois riches de sens et précisément localisées dans le temps. Son fonctionnement est schématisé par la figure 1.
Entrée. Le système traite un flux de messages produit par Twitter. Le vocabulaire des termes employés dans ces messages est noté V . L'axe temporel est discrétisé en partitionnant les messages dans des tranches temporelles de même durée (cf. la figure 1 pour une illustration de ce pré-traitement). Ce pré-traitement est commun à toutes les méthodes existantes.
Sortie. Le système génère une liste de thématiques, ordonnées selon leur popularité. Une thématique est définie par un terme principal, une liste pondérée de termes liés et un intervalle temporel. Par exemple, la thématique : {["google", {("chrome",0.8), ("os", 0. Traitement. La tâche d'identification des thématiques populaires est décomposée en 3 problèmes : (1) l'identification des termes principaux et des intervalles temporels, lesquels sont associés à un score de popularité ; (2.a) la sélection de termes liés pertinents ; (2.b) la construction du graphe des redondances et du graphe de thématiques, duquel est extrait la liste finale de thématiques. La méthode se déroule comme suit. Tout d'abord, le problème (1) est résolu pour chaque terme appartenant au vocabulaire V à travers l'analyse de l'anomalie dans la fréquence de création de liens dynamiques. Ensuite, pour chaque couple de terme principal et intervalle temporel, le problème (2.a) est résolu afin d'identifier l'ensemble pondéré de termes liés. Chaque thématique ainsi constituée est insérée dans le graphe de thématiques si elle n'est pas redondante avec une autre thématique déjà présente (2.b). Les redondances constatées sont modélisées par un second graphe, qui permet d'identifier les thématiques à fusionner à la fin du processus, avant d'extraire la liste des thématiques populaires qui sera retournée à l'utilisateur.
FIG. 2 -En haut, la frise chronologique générée automatiquement. L'utilisateur navigue dans le temps à l'aide du ruban. Il obtient des détails lorsqu'il sélectionne une thé-matique. En bas, les scores de popularité, où l'aire de couleur verte correspond à la thé-matique sélectionnée dans la frise. Interfaces utilisateur. En plus de la liste de thématiques triées par popularité décroissante, le système permet à l'utilisateur d'explorer les résultats de trois autres manières : (i) en navigant dans le graphe de thématiques, (ii) en parcourant la frise chronologique et (iii) en explorant le graphique interactif des scores en fonction du temps.
Cadre de la démonstration
Le système est capable de traiter à la fois des données statiques, afin par exemple de fournir une vision rétrospective des thématiques marquantes durant une période d'observation, et des données dynamiques, autrement dit le flux délivré en temps réel par Twitter. Dans ce cas, le système met à jour le modèle de façon incrémentale et il peut alors être utilisé pour suivre les thématiques les plus populaires en temps réel. Nous décrivons ci-après les données qui seront traitées lors de la démonstration.
Données statiques. Deux corpus seront analysés : le premier correspond à l'intégralité des tweets publiés par plus de 50.000 utilisateurs anglophones d'octobre à décembre 2009 (Yang et Leskovec, 2011). Le second correspond à des tweets en français publiés durant la campagne électorale ayant précédée l'élection présidentielle de 2012. Chaque corpus contient plusieurs millions de tweets.
Données dynamiques. Le système interroge le flux public de Twitter et collecte en permanence des tweets francophones mentionnant François Hollande. Les thématiques populaires sont mises à jour chaque fois qu'une nouvelle tranche temporelle de tweets est disponible.
Scénario. Le scénario de la démonstration consistera à analyser ces données à l'aide du système, en faisant varier ses paramètres et en explorant les résultats avec ses interfaces. L'efficacité de la méthode implémentée permet de traiter plusieurs millions de tweets en moins d'une minute avec un PC standard. La figure 2 montre un extrait des résultats/interfaces géné-rés à partir du corpus statique francophone. La figure 3 montre un extrait des résultats/interfaces générés à partir des données collectées en temps réel par le système.
Conclusion
Nous avons présenté un système de détection de thématiques populaires sur Twitter. Les URL données dans l'en-tête de cet article permettent le téléchargement du prototype et la consultation des thématiques détectées en continu à partir du flux public de Twitter. Afin de faciliter encore plus sa réutilisation, le système sera intégré à la plateforme d'analyse et de fouille de données sociales SONDY (Guille et al., 2013).
Remerciements. Ces travaux ont été partiellement financés par l'ANR et le projet ImagiWeb (contrat ANR-2012-CORD-002-01).

Introduction
La classification non supervisée (clustering) de données constitue une tâche fondamentale et classique de structuration, pour l'analyse exploratoire de jeux de données. Elle a été l'objet d'un nombre considérable de travaux depuis des décennies, à la fois comme objet général d'analyse de données ou dans des contextes plus appliqués (bioinformatique Hu et Yoo (2004), segmentation d'images Hong et al. (2008), etc . . .). Toutefois, la nature même du problème ne fournit généralement pas de vérité-terrain simple, tant sur la composition des classes que leur nombre. Les algorithmes proposés dans la littérature optimisent des critères très divers et font des hypothèses elle aussi diverses sur les propriétés de cohérence intra-classe.
Depuis une dizaine d'années, un axe de recherche, le clustering d'ensemble, s'est déve-loppé, élaborant des critères et des méthodes pour aggréger différentes partitions d'un même jeu de données, construites par des critères antagonistes.
Plusieurs méthodes sont alors envisageables pour pallier l'incertitude quant à la plausibilité du résultat : -l'entremise de connaissances obtenues par le biais d'un « expert », ou inhérentes au domaine dont est issu le jeu de données, permet de formuler des hypothèses sur le méca-nisme générateur sous-jacent et donc sur la nature des regroupements les plus vraisemblables ; -la recherche d'un compromis explicite entre les différents résultats, en les combinant et suivant généralement un critère à optimiser qui définit les propriétés attendues dans le clustering devant réaliser le consensus. Il existe deux grandes familles complémentaires d'approches, pour traiter le problème de la combinaison de clustering. La première approche permet d'établir une étude des mécanismes de génération des partitions (p.ex. Von Der Gablentz et al. (2000); Dudoit et Fridlyand (2003)). Celle-ci tend à améliorer la fiabilité du résultat dès lors que des variations sont appliquées dans le protocole expérimental, où différents algorithmes avec des paramétrages différents peuvent être utilisés. Différentes projections peuvent être également utilisées dans l'espace des attributs pour engendrer chaque partition. La seconde approche procède par la définition d'une fonction de consensus qui correspond à un critère à optimiser dépendant du choix d'un modèle de représentation des partitions (i.e. hypergraphe des clusters, matrice d'associations) et d'une mesure de distance adaptée à cette dernière (p.ex. Strehl et Ghosh (2003); Topchy et al. (2005)).
Problématique et enjeux Il n'existe pas à ce jour, à notre connaissance, de théorie qui valide un ensemble de propriétés axiomatiques devant être implicitement vérifiées par chaque méthode en particulier, au-delà de la taxonomie propre à la discipline et distinguant les fonctions de consensus et les méthodes génératives. Plus généralement, chaque méthode fonde son propre compromis sur une base opérationnelle qui ne permet pas de justifier l'ensemble des choix appliqués pendant la construction du résultat final.
Par ailleurs, chaque résultat de clustering est naturellement définissable comme la donnée d'une partition P , définie sur un ensemble d'objets ?, figurant un jeu de données quelconque. Construire un résultat satisfaisant vis-à-vis des partitions originales représente en soi une gageure à cause de la nature combinatoire des partitions. De plus, la nature exploratoire de la tâche rend délicate la définition de propriétés permettant de construire inductivement un résul-tat sans introduire de biais dans le raisonnement.
Une présentation algébrique peut alors contribuer à la compréhension des mécanismes intervenant lors de la combinaisons de partitions de sorte à définir effectivement la nature du raisonnement applicable sur celles-ci et en particulier comment établir qu'une partition soit la représentation légitime du consensus de plusieurs autres partitions.
En économie et plus particulièrement dans la théorie du choix social, un consensus est considéré comme la réalisation d'un arrangement ou d'une combinaison à partir d'un ensemble de profils formulés par des individus de sorte à obtenir un profil commun, avec la contrainte que ce dernier représente fidèlement les préférences initiales du plus grand nombre possible d'individus. Dans ce cas, il est alors possible de formuler l'analogie associant un résultat de clustering au profil propre à un individu. Par conséquent, il est aisé d'interpréter l'agrégation de deux objets a, b ? ? comme une relation de nature préférentielle résultant des paramètres propres à l'algorithme de clustering et du critère statistique qu'il a employé, puis de formuler le problème de satisfaction par la recherche d'une partition qui soit maximalement compatible avec l'ensemble de départ (i.e. typiquement, par l'usage d'une relation d'ordre partielle). L'intérêt de ce paradigme est qu'il permet de contraindre les procédures de décisions qui vont régir explicitement comment les profils peuvent se combiner entre eux et ainsi d'imposer des modalités propres aux usages espérés dans un contexte applicatif.
Un exemple immédiat est de considérer les interactions attendues entre des partitions dans un système de recommandations : ceux-ci ont pour but de mettre en relation un individu ? dont le profil de préférences P ? est proche de celui d'un autre individu ?. La logique sous-jacente est alors très similaire à celle d'un système multiagent où les individus ont la faculté de raisonner (i.e. réaliser des déductions) depuis la connaissance induite par leurs propres préférences et de celles exprimées par d'autres individus afin de modifier leur propre comportement. La relation entre deux individus ? et ? va donc dépendre de la permissivité de la relation définissant la proximité entre P ? et P ? et décidant quand un individu peut avoir accès aux préférences de l'autre, et réciproquement.
Cependant, le but de cette article n'est pas de proposer un système logique modélisant l'ensemble des cadres applicatifs susceptibles d'inclure des partitions mais de présenter la construction d'un calcul algébrique basé sur le treillis des partitions où l'analogie précédente va permettre de qualifier assez simplement le pendant algébrique de la définition du consensus. Cette approche sera mis en perspective avec celle présentée dans Barthelemy et Leclerc (1995).
Contributions
-Modélisation d'un nouvel opérateur algébrique sur le treillis des partitions ; -Définition d'une fonction de consensus exploitant cette construction ; Cet article est une version condensée du rapport de recherche disponible dans Dumonceaux et al. (2013).
Dans la prochaine section, on présentera succinctement l'algèbre permettant de manipuler des partitions et la nature du raisonnement applicable entre celles-ci et comment l'exploiter par le biais d'opérateurs adjoints au treillis.
Consensus de partitions
Un treillis est un ensemble partiellement ordonnée (P, ?) dont les éléments sont soit mutuellement comparables par sa relation (réflexive, antisymétrique et transitive), ou pour lesquels il existe deux bornes accessibles par cette même relation et symbolisées par les opérateurs ? et ? (idempotent, commutatif et associatif). La relation d'ordre partielle (?) permet de distinguer les éléments vérifiant des propriétés particulières et se confond avec la relation de déduction ( qui lie une proposition logique prise pour hypothèse et sa conséquence. Les éléments d'un treillis sont alors une représentation concrète des énoncés d'un système logique.
Soit (? ? , ?, ?) le treillis des partitions, où ? est l'ensemble support pour lequel chaque partition P ? ? ? est le résumé d'une séquence d'opérations d'agrégation dont les clusters c ? ? sont les représentations idoines et mutuellement disjoints. La relation de raffinement entre deux partitions P ? Q est alors vérifiée dès lors que chacun des clusters de P est inclus dans un cluster de Q. Par exemple, sachant P = 12|3|45|6 et Q = 123|456, P est alors plus fine que Q. En particulier, imaginons que P soit un résultat d'expérience ou l'énonciation d'une hypothèse dans un problème quelconque, on écrit alors que de P , on déduit Q puisque on remarque aisément que toutes les paires d'éléments dans ? × ? sont préservées dans Q 1 Dans ? ? , la partition P ? Q est telle que les clusters sont simultanément inclus dans P et dans Q, chacun des clusters de P et de Q sont simultanément inclus dans P ? Q. Par exemple, étant donné P = 12|345|67 et Q = 123|45|67, alors P ?Q = 12|3|45|67 et P ?Q = 12345|67. Le point de vue logique adopté ici considère P et Q comme des hypothèses, autrement dit P, Q sont les prémisses du raisonnement pour lequel on peut appliquer les règles d'introductions classiques des connecteurs logiques, et ainsi calculer les propositions correspondantes.
Cependant, le treillis des partitions n'étant pas distributif, il est impossible de décomposer sous une forme invariable et minimale une partition comme la composition de plus petite partitions atomiques et figurant une unique association entre deux éléments pris dans ? (cf. Birkhoff (1937)). La conséquence immédiate est qu'on ne peut déduire la suite exacte des agrégations d'une partition particulière et par induction, faire appel de multiples fois à (?) revient à inclure lors de la combinaison de partitions, un nombre croissant d'associations entre des éléments qui n'ont pas été formulées explicitement.
Une autre conséquence est l'impossibilité de définir des opérateurs adjoints à la structure comme c'est l'usage pour le treillis des parties d'un ensemble 2 ? avec les opérateurs d'implication (?) et de différence (?). Ces opérateurs étant définis comme des connections de galois, requérant la propriété de distributivité, on choisit alors d'amender cette définition. Par exemple, Q ? P est définit par {R | R ? Q ? P } dont le résultat R figure la plus grande partition dont les parties communes avec Q sont compatibles avec P et on impose que le ré-sultat soit choisi parmi 2 P , et ainsi on a toujours Q ? P ? P quelque soit P . L'usage de cet opérateur va permettre de minorer l'importance des petits clusters déja inclus dans ceux d'une autre partition. Arrow (1951) établit un ensemble de trois propriétés axiomatiques qui devrait être vérifié par toute méthode de consensus, et démontre dans la foulée que ses critères ne peuvent être satisfait simultanément, ce fait est communément appelé paradoxe de Arrow, dont le précurseur fut Condorcet. Une solution communément admise est d'envisager la relaxation de l'une des propriétés voire d'enfreindre une ou plusieurs de ces contraintes. Ces critères sont les suivants : -L'indifférence face aux alternatives non-pertinentes : deux éléments x, y ? ? seront agrégés dans le consensus indépendamment des éléments dans ? ? {x, y} et leur agré-gation dépend uniquement de la position exprimée sur ce couple par l'ensemble des partitions dans P ; -Optimum de Pareto : si toutes les partitions sont unanimement d'accord sur l'agrégation de deux éléments x, y ? ?, alors ils sont également agrégées au niveau de la partition résultant du consensus ; -Non-didactorial : il ne doit pas exister de sous-ensemble P ? P tel que le consensus soit obtenu à l'unanimité sur P , rejetant les préférences induites par les partitions dans P ? P . On définit une fonction de consensus par f : ? n ? ? ? ? . Une fonction qui satisfait de manière évidente à chacun de ses trois critères est celle requérant l'unanimité :
1. En particulier, soit une bijection ? : ? ? ?, telle que P? représente la partition résultante, alors P ? Q ? P? ? Q? et la relation d'ordre dans le treillis est indépendante de l'étiquetage employé pour l'ensemble support. mais celle-ci est largement susceptible de renvoyer des résultats peu probants car ne sauvegardant pas suffisamment d'associations entre éléments. Il faut donc assouplir la contrainte. La règle suivante agrège l'ensemble des consensus unanimement obtenus par l'ensemble de toutes les majorités formées sur P :
Dans notre méthode alg(.), P conserve pour chaque partition, chacun des clusters qui n'est strictement pas inclus dans un cluster d'une autre partition de l'ensemble. Si l'on se représente l'ensemble des clusters de P comme un hypergraphe H, alors l'hypergraphe H résultant de P forme une famille de Sperner telle que ?e, f ? H , e ? f et f ? e. Les associations préservées dans le consensus est alors le résultat de l'intersection entre chaque paire d'hyperarêtes dans H :
FIG. 1 -Comparaison des résultats obtenues pour chaque méthode et sur les deux versions du jeu.
Concernant le premier jeu, Fig. 1 montre que notre méthode est compatible (i.e. par la relation d'ordre) avec un nombre inférieur de partitions mais en revanche, elle intègre un plus grand nombre d'associations qui demeurent compatibles avec celles déjà incluses. La même chose est observable en dupliquant la première partition, cependant cela est dû en grande partie au fait qu'aucun cluster de P 1 ne sera filtré. P 1 devient librement combinable avec les clusters préservés dans les autres partitions et on obtient alg(P m ) = p 1 ? alg(P). Par ailleurs, ce comportement peut devenir préjudiciable si le nombre de partitions identiques s'accroît.
Conclusion et Perspectives
Nous avons proposé une approche constructiviste dans le cadre de la définition réalisant partiellement l'adjonction avec les opérateurs traditionnelles du treillis et pour lesquelles, il n'existe pas de cadre formel décrivant l'interprétation de la dualité les liant. Par ailleurs, nous avons proposé la modélisation du problème de consensus entre des partitions par l'usage de l'un de nos opérateurs.
Identifier explicitement les contextes hypothétiques dans lequel une partition peut-être vraie, semble une voie prometteuse. En effet, la construction de celles-ci procède invariablement d'une étape d'uniformisation et qui résulte en une perte d'information, susceptible de

Introduction
En apprentissage non-supervisé, la plupart des méthodes de partitionnement souffrent d'une part d'un problème commun de stabilité des résultats par rapport aux paramètres d'initialisations des algorithmes. En effet, les partitions fournies par les algorithmes des K-moyennes ou des cartes topologiques auto-organisées (SOM), par exemple, dépendent du choix des centres de classes initiaux, du voisinage initial et final des cellules de la carte topologique,etc. D'autre part, en fonction de la méthode de classification utilisée, les partitions peuvent être différentes. Ainsi, en classification ascendante hiérarchique la partition obtenue dépend de la stratégie d'agrégation utilisée (critère de ward, lien moyen, lien complet, etc). Récemment, Strehl et Ghosh (2002) ;Fred et Jain (2003) ont proposé alors d'agréger les différentes partitions afin d'accroître significativement les performances de la partition finale. Ce concept connu sous le terme "d'ensemble clusters" reprend les concepts plus anciens de recherche de consensus de partitions proposés par Régnier (1983) et Gordon et Vichi (1998). Dans cette communication, on intéresse aux méthodes d'ensemble clusters dédiées aux cartes topologiques. Comme les méthodes de partitionnement classiques (K moyennes, CAH), les algorithmes d'apprentissage de type auto-organisés SOM (Kohonen, 1998) sont fortement dé-pendants des paramètres d'initialisation. Nous cherchons donc à améliorer le partitionnement des observations offert par l'algorithme SOM en adaptant les techniques de "cluster ensemble" aux cartes topologiques. L'approche proposée repose sur la méthode d'analyse de données multi-tableaux STATIS (Lavit et al., 1994) pour déterminer une matrice compromis représentant au mieux la similarité entre les partitions issues des cartes topologiques. La fusion des cartes topologiques est alors obtenue à travers une classification basée sur cette matrice compromis. La section 2 suivante présente le problème de fusion de SOM. La section 3.1 présente la mé-thode STATIS. La section 3.2 présente la méthode proposée et ses applications aux données réelles issues de l'UCI et sur des données simulées.
Fusion de SOM
La démarche des méthodes d'"ensemble clusters" se résume en deux étapes : une étape de diversification par la création d'un ensemble de partitions et une étape d'agrégation des partitions. Dans le cadre de la fusion de SOM nous désignons par C l'ensemble des cartes topologiques. Cet ensemble peut être obtenu de diverses manières. Il peut s'agir : de résul-tats obtenus par application répétée d'un même algorithme avec différentes initialisations des paramètres (Jiang et Zhou, 2004;Georgakis et al., 2005). Dans le domaine des réseaux de neurones, le problème de recherche de consensus consiste d'une part à apprendre indépendamment B cartes SOM, d'autre part à synthétiser les résultats de ces SOM en regroupant les neurones similaires des différentes cartes. Soit La méthode proposée dans cette communication est basée sur la classification d'une matrice de compromis˜Cocompromis˜ compromis˜Co représentant au mieux la relation entre les partitions et qui s'obtient en utilisant la méthode STATIS que nous présentons dans la section suivante. 
où D est la matrice des poids associés aux individus, en général ils sont choisis uniformément égaux à 1/N. On définit par S la matrice des produits scalaires entre les tableaux et dont les entrées S(a, b) valent HS(X a , X b ). Il est habituel de normaliser les matrices X b . Les entrées de la matrice S deviennent alors des coefficients de corrélation vectorielle R V entre les objets X b tel que :
Comme en Analyse en Composantes Principales, la diagonalisation de la matrice S fournit une représentation euclidienne des tables X b dans un espace de dimension réduit permettant de visualiser les différences et les ressemblances entre les matrices X b . Ce qui constitue l'étude de l'inter-structure dans STATIS. Une étude plus fine au niveau des individus permettant de comprendre la relation entre les tables est réalisée à travers l'étude de intra-structure.
L'intra-structure repose sur la détermination d'une matrice compromis˜Cocompromis˜ compromis˜Co de même nature que les matrices H b telle que˜Coque˜ que˜Co soit le plus corrélé possible au sens du produit scalaire HS avec les matrices X b . La recherche du compromis est formalisée comme un problème de recherche de la meilleure combinaison linéaire des matrices X b , celle qui maximise la corrélation vectorielle avec les matrices X b :
Comme en ACP, la solution est le premier facteur principal µ défini comme le vecteur propre associé à la plus grande valeur propre de la matrice des R V :
Approche de Fusion de SOM basée sur STATIS
On s'intéresse au consensus des partitions de l'ensemble ? = {? 1 , . . . , ? B } issues de SOM. Notre approche de recherche de consensus est basée sur la matrice compromis˜Cocompromis˜ compromis˜Co fournie par STATIS. Les matrices X b sont les matrices d'adjacence associées aux partitions ? b avec :
Pour déterminer le consensus on utilise usuellement la matrice Co =  
Évaluation
L'ensemble ? des partitions est obtenu à travers 30 applications de SOM en faisant varier des paramètres d'initialisations. Nous calculons ensuite le consensus des 30 partitions à l'aide de la méthode STATIS. Cet expérience est répété 25 fois pour les données IRIS, WINE, GLASS, IONOSPHERE et "Image segmentation (IS)" issues de UCI. Sur les données simulées D1 et D2 qui sont de types multi-vues, chaque vue est une table composée de 5 variables, le consensus est défini sur 10 partitions obtenues sur les vues. Cet expérience est aussi répété 25 fois. Afin de positionner la méthode proposée, que nous appelons, CSTATIS par rapport à quelques méthodes de consensus présentées dans la littérature, nous réalisons la même expé-rience avec des algorithmes de recherche de consensus basés sur la factorisation de matrice non-négative (NMF, Weighted NMF) présentés par Ding et al. (2006), la méthode d'ensemble cluster (CSPA) présentée par Strehl et Ghosh (2002). Le tableau 1 présente les caractéristiques des différentes tables. Nous utilisons l'indice de pureté suivant pour évaluer la similarité entre deux partitions : TAB. 2 -Résultats du consensus de partition, on observe, la moyenne des puretés des algorithmes sur 25 expériences. CSTATIS est le résultat du consensus obtenu à l'aide de STATIS, NMF est le résultat du consensus obtenu à l'aide de l'algorithme de factorisation de matrice non-négative NMF, WNMF est le résultat de la version pondérée de NMF et CSPA est le résul-tat de l'algorithme d'ensemble clusters.
FIG. 1 -Réprésentation de la carte consensus ; Les figures en haut correspondent à la carte consensus. En bas, une carte de l'ensemble de diversification pour chaque table. On observe une bonne conservation de la topologie des observations sauf pour la table IS

Introduction
Introduite par (Agrawal et Srikant, 1995), la fouille de données séquentielles permet de découvrir des corrélations entre des événements selon une relation d'ordre (e.g. le temps). En intégrant des connaissances sous forme d'a priori dans le processus de fouille, l'extraction de motifs sous contraintes contribue à réduire le nombre de motifs en ciblant les motifs potentiellement intéressants (Dong et Pei, 2007). De plus, elle permet souvent de concevoir des algorithmes plus efficaces en réduisant l'espace de recherche. De nombreux algorithmes sont proposés dans la littérature pour l'extraction de motifs séquentiels (Dong et Pei, 2007). Malheureusement, ces méthodes ne traitent que quelques classes particulières de contraintes (monotonicité et anti-monotonicité) avec des techniques dédiées. Ce manque de généricité est un frein à la découverte de motifs pertinents car chaque nouveau type de contraintes entraîne la conception et le développement d'une méthode ad hoc.
Pour lever ce frein, des travaux récents visent à croiser les techniques de Programmation Par Contraintes (PPC) et de fouille pour l'extraction sous contraintes de motifs d'itemsets (Guns et al., 2011;Khiari et al., 2010). Le point commun de ces travaux est de modéliser le problème de la fouille de motifs en un problème de CSP. Une telle modélisation présente l'avantage d'être flexible en permettant de définir de nouvelles contraintes sans s'occuper de leur résolution. Mais, les méthodes proposées sont conçues pour des données ensemblistes et la dimension séquentielle reste quasiment non exploitée, à l'exception des travaux de (Coquery et al., 2012) qui portent sur un cas particulier de chaîne (et non sur une base de séquences). L'originalité de notre travail consiste à proposer une première modélisation PPC de l'extraction de motifs séquentiels sous contraintes -à partir d'une base de séquences -dans un cadre déclaratif permettant de traiter simultanément des contraintes de nature quelconque. Les contraintes traitées dans le cadre de cet article incluent les contraintes de fréquence, clôture, taille, gap et celles portant sur les items (voir (Métivier et al., 2013)  2 Extraction de motifs séquentiels
Motifs séquentiels
Etant donné un ensemble I de littéraux distincts appelés items, une séquence s = 1 , . . . , i n est une liste ordonnée non vide d'items. Une séquence S a = 1 , . . . , a n est incluse dans une autre séquence S b = 1 , . . . , b m s'il existe des entiers 1 ? i 1 < . . . < i n ? m tels que a 1 = b i1 , . . . , a n = b in . Si la séquence S a est incluse dans S b , alors S a est une sous- ). Un motif séquentiel fréquent est un motif ayant un support minimal supérieur ou égal à un certain seuil minsup.
Fouille de motifs séquentiels sous contraintes
Les contraintes permettent à l'utilisateur de définir plus précisément ce qu'il considère comme intéressant pour ne conserver que les motifs pertinents (Dong et Pei, 2007). Un exemple classique de contraintes est celle de support minimal. Nous passons en revue quelques autres contraintes classiques et traitées par la suite : Fermeture. Cette contrainte permet d'obtenir une représentation condensée des motifs en éli-minant les redondances entre motifs : Un motif fréquent s est un motif fermé fréquent, s'il n'existe pas de motif fréquent s tel que s s et sup(s) = sup(s ). Par exemple, avec minsup = 2, le motif b c de la Table 1 est fermé contrairement au motif c Contrainte d'item. Cette contrainte spécifie le sous-ensemble d'items qui doivent apparaître ou non dans les motifs extraits. Par exemple, soit la contrainte  (a)). Mais seule la séquence 1 supporte p [1,2] .
3 Modélisation de la fouille séquentielle sous contraintes
Programmation par contraintes
La Programmation par Contraintes (PPC) est un paradigme puissant pour résoudre des problèmes combinatoires, se basant sur des techniques issues de l'intelligence artificielle et de la recherche opérationnelle. La PPC se base sur le principe suivant : (1) l'utilisateur spécifie le problème d'une façon déclarative comme un problème de satisfaction de contraintes (CSP) ; (2) le solveur cherche l'ensemble complet et correct de solutions du problème. Un CSP est un triplet (X , D, C) où X = {X 1 , . . . , X n } est un ensemble fini de variables ayant pour domaines finis D = {D 1 , . . . , D n } et C = {C 1 , . . . , C m } est un ensemble de contraintes où chaque C i est une condition sur un sous-ensemble de X . L'objectif est de trouver une affectation complète de valeur d i ? D i à chaque variable X i satisfaisant toutes les contraintes de C.
Une technique de modélisation importante en PPC sont les contraintes globales qui dé-crivent un ensemble de propriétés que doit satisfaire un ensemble de variables. Nous présen-tons succinctement deux contraintes globales, Among et Regular, permettant de modéliser les contraintes décrites en Section 2. La contrainte Among. Cette contrainte restreint le nombre d'occurrences de certaines valeurs dans une séquence de n variables (voir (Beldiceanu et Contejean, 1994) pour plus de détails). La contrainte Regular. Soit M un automate fini déterministe et X un ensemble de variables, la contrainte Regular(X, M ) impose que la séquence de valeurs de X appartient au langage régulier reconnu par M (Pesant, 2004).
Modèle
Variables. Soit I = {i 1 , . . . , i n } un ensemble de n items, EOS un symbole n'appartenant pas à I désignant la fin d'une séquence, SDB un ensemble de m séquences et la taille de la plus grande séquence de SDB. Un motif séquentiel inconnu p de taille est modélisé par les variables P 1 , P 2 , . . . P , chaque P i a pour domaine D i = I ? {EOS}. On introduit les m variables booléennes T s telle que (T s = 1) ssi (p est une sous-séquence de s) : (S s = 1) ? (p s). Alors, sup(p) = ? s?SDB T s . Modélisation de "p s". Pour chaque séquence s, nous générons un automate A s permettant de capturer toutes les sous-séquences de s. Ensuite, nous imposons la contrainte Regular indiquant que le motif p doit être reconnu par l'automate A s . Pour réduire le nombre d'états de A s , pour chaque séquence s, nous considérons uniquement ses items fréquents dans SDB. La figure 1a montre un exemple d'automate généré pour la troisième séquence de la Table 1. Modélisation de l'extraction de motifs séquentiels. Soit minsup un seuil minimal de fré-quence. Ce problème est modélisé par les contraintes suivantes : -
Les motif fermés sont les motifs maximaux des classes d'équiva-lence des motifs partageant la même fréquence. Dans notre modélisation, un motif maximal est un motif ayant le plus petit nombre de variables P i instanciées à EOS. Ce problème est formulé sous forme d'une contrainte de minimisation sur la taille des motif : (1) pour chaque variable P i , nous posons une fonction de coût unaire c i tel que c i = 1 si P i = EOS ; 0 sinon ; (2) minimiser la fonction c(p) = Pi?p c i . Ainsi, le calcul des fermés se ramène à une contrainte de minimisation sur la taille des motifs et sur l'ensemble de tous les motifs fréquents (contraintes 1 et 2). Enfin, à chaque fois qu'un motif fréquent est prouvé fermé, une contrainte est ajoutée dynamiquement pour interdire de redécouvrir une nouvelle fois ce motif. Contrainte de Gap. Pour modéliser la contrainte gap [M, N ], il suffit de modifier la construction de l'automate A s de telle sorte à ne garder que les transitions respectant la contrainte de gap. Soit A ). La figure 1b montre le nouvel automate obtenu à partir de celui de la figure 1a avec un gap [1,1].
Éxperimentations
Des expérimentations ont été réalisées dans le cadre d'une application de fouille de textes visant à découvrir des relations entre des gènes et des maladies rares (MR) dans les textes biomédicaux. La fouille de séquences a pour objectif d'extraire des motifs séquentiels utilisés comme patrons linguistiques. Cette application est détaillée dans (Béchet et al., 2012). Nous avons testé différentes tailles de corpus (de 50 à 500 phrases). Nous rapportons le nombre de motifs fermés extraits et les temps CPU (en secondes) pour les extraire. Nous indiquons entre parenthèses le nombre de motifs extraits lorsque la résolution n'a pas terminé au bout de 10 heures de calcul. Toutes les expériences ont été menées sur un processeur AMD Opteron 2, 1 GHz et une mémoire vive de 256 GO, en utilisant la bibliothèque toulbar2 2 . B) Résultats. Des résultats de la table 2, nous pouvons dresser les remarques suivantes. i) Correction et complétude. Notre approche calcule l'ensemble correct et complet de motifs séquentiels. Nous avons comparé les motifs séquentiels extraits par notre approche avec ceux trouvés par (Béchet et al., 2012), et les deux approches renvoient le même ensemble de motifs. ii) Pertinence des motifs extraits. Notre approche a permis d'extraire plusieurs motifs linguistiques pertinents. De tels motifs permettent de mettre en évidence l'expression des relations linguistiques entre gène et MR, comme par exemple, ces deux motifs traduisant une notion de causalité : (be) (cause) (by) (mutation) (in) (the) (GEN E) et (be) (dominant) (f requently) (cause) (by) (GEN E) (gene) iii) Temps CPU. Le temps d'exécution de notre approche augmente en fonction de la taille du corpus. Toutefois, pour les corpus de grande taille (? 200) et pour des valeurs de minsup ? 2%, notre approche ne parvient pas à terminer l'extraction de tous les motifs fermés dans un délai de 10 heures. En effet, l'espace de recherche augmente drastiquement et le solveur passe beaucoup plus de temps pour trouver la première solution. Enfin, en raison du caractère gé-nérique de notre approche, il est très difficile de rivaliser et de se comparer avec les meilleurs algorithmes de fouille développés pour quelques contraintes. À l'opposé, nous pouvons combiner de manière très élégante et déclarative plusieurs contraintes de nature diverse, ce qui constitue un point important pour l'extraction de motifs pertinents.
Conclusion
Nous avons proposé dans cet article une nouvelle approche croisant des techniques de programmation par contraintes et de fouille pour l'extraction de motifs séquentiels. Notre modèle offre un cadre générique et déclaratif pour modéliser et résoudre des contraintes de nature hé-térogène. La faisabilité de notre approche a été mis en évidence par des expériences sur une étude de cas pour la découverte de relations gène-MR à partir d'articles PubMed.

Introduction
L'essor du m-learning, favorisé par le développement continu des nouvelles technologies mobiles pousse à l'évolution des méthodes d'apprentissage pour s'adapter à ce nouveau type d'apprentissage. Dans le cadre de l'apprentissage au sein des entreprises, nous cherchons à développer un système m-learning dont les principaux enjeux sont : (1) l'apprentissage au travail quel que soit l'heure, le lieu, le dispositif de délivrance, les contraintes technologiques des processus d'apprentissage et adapté au profil de l'apprenant ; (2) l'apprentissage sans rupture au travers des différents contextes. Nous proposons une approche pour un système mlearning contextuel et adaptatif intégrant des stratégies de recommandation de scénarios de formations sans risque de rupture. Dans l'objectif de développer un tel système, nous commençons par identifier différents niveaux d'hétérogénéité : hétérogénéité sémantique et hétérogé-néité d'usage. Hétérogénéité sémantique : Les ressources sont conçues et développées par des organisations et des formateurs différents, constituant généralement des contenus d'apprentissage autonomes mais aussi hétérogènes au niveau sémantique. Ainsi, des conflits surviennent puisque les systèmes n'utilisent pas la même interprétation de l'information. Les besoins immédiats demandent l'application de standards en vigueur pour rendre les contenus d'apprentissage réutilisables pour assurer l'interopérabilité des plateformes e-learning hétérogènes. 
Ontologie du m-learning
Un contenu d'apprentissage est une instanciation d'objets pédagogiques, ou LOs (Learning Objects). IEEE définit un LO comme "toute entité, sur un support numérique ou non, pouvant être utilisée, réutilisée et référencée au cours d'un processus de formation". L'idée fondamentale derrière la création des LOs est la possibilité de construire un parcours de formation autour de composants de petite taille qui peuvent être sélectionnés, combinés avec d'autres LOs et réutilisés selon les besoins des apprenants dans différents contextes d'apprentissage (Abel, 2007). Seulement ces LOs sont souvent conçus et développés par des organisations et des auteurs différents constituant des contenus autonomes et sémantiquement hétérogènes. Il est alors indispensable de penser à une modélisation partagée des LOs en vue de les rendre facilement accessibles, exploitables, réutilisables et interopérables. Différentes normes ont été définies pour aider à l'élaboration de systèmes d'apprentissage, des LOs associés, leur représentation et leur interrelation. L'application de ces normes, garantit l'interopérabilité et la qualité du système. Parmi ces normes, nous citons LOM (Learning Object Metadata) (lom) qui s'intéresse à la description des contenus d'apprentissage. Il définit la structure d'une instance de métadon-nées pour la description d'un LOs. Il est constitué d'un ensemble de 80 éléments divisés en 9 catégories accomplissant chacune une fonction différente. Afin d'implémenter les différents descripteurs de LOM, une modélisation dans un langage structuré est nécessaire. La représen-tation du modèle abstrait de LOM dans un format spécifique est appelé binding. Aujourd'hui il existe 2 bindings du schéma LOM : le binding XML et le binding RDF. Le binding XML est facile à implémenter, cependant il reste insuffisant pour la représentation de tous les élé-ments de LOM puisqu'il ne permet pas d'exprimer la sémantique de ces éléments. Le binding RDF définit un ensemble de constructions RDF qui facilitent l'introduction des métadonnées de LOM dans le web, et il est complété par RDFS pour la définition des classes, des propriétés, etc. L'avantage de ce deuxième type de binding c'est qu'il rajoute de la sémantique aux élé-ments de LOM, sauf qu'il n'est pas assez expressif pour définir toutes les contraintes de LOM. Ce manque d'expressivité nous mène à penser à l'utilisation d'un autre formaliste plus puissant : OWL. Utiliser une ontologie OWL du LOM pour indexer les ressources pédagogiques permet une meilleure compréhension des éléments et des valeurs proposées et en conséquence faciliter leurs descriptions. Nous appelons cette ontologie "modèle des Learning Objects". L'informatique sensible au contexte fait référence à des systèmes capables de percevoir un ensemble de conditions d'utilisation, le contexte, afin d'adapter en conséquence leur comportement en termes de délivrance d'informations et de services (Schilit et Theimer, 1994). Le contexte d'apprentissage est un aspect crucial en m-learning afin de déterminer selon le F. Soualah Alila et al.
contexte quelles ressources à envoyer, de quelle manière, à quel moment, sur quelle interface, etc. Pour bien comprendre et appliquer cette sensibilité au contexte, il est plus simple de passer par une catégorisation des variables du contexte. Selon (Schilit et Theimer, 1994) le contexte se décompose en trois sous classes où chacune des variables répond à l'une des questions "où suis-je ?", "avec qui suis-je ?", "Quelles sont les ressources de mon environnement proche ?". (Ryan et al., 1998) catégorisent le contexte en identité de l'utilisateur, ressources de l'environnement proche, localisation de l'utilisateur et période temporelle d'exécution de l'interaction. Ici, pour avoir une meilleure visibilité du contexte d'apprentissage, nous proposons d'organiser les données qui constituent ce dernier en quatre dimensions : dimension spatiale, dimension temporelle, dimension utilisateur et dimension device (Soualah Alila et al., 2013). La prochaine étape est de trouver un moyen de représenter le contexte. Cette représentation doit fournir un cadre cohérent pour mémoriser et traiter les informations du contexte pour ré-agir aux changements de l'environnement. Il en résulte alors le "modèle de contexte". Il existe plusieurs méthodes de représentation du modèle contextuel (XML, UML, Topic maps), cependant aucun de ces modèles n'assure l'interopérabilité des données au niveau sémantique. De plus une représentation du contexte doit permettre d'effectuer des raisonnements en vue d'une adaptation. Nous soutenons qu'une modélisation à base d'un squelette ontologique est plus appropriée. Le modèle de contexte vient compléter le modèle des Learning Objects pour former ainsi une ontologie de domaine du m-learning. lon le moyen de transport, deux parcours de formation peuvent comporter des LOs différents. Tout comme le temps de parcours entre deux points varie en fonction du moyen de transport utilisé, le temps nécessaire pour parcourir un ensemble de briques de formations peut varier en fonction du support de diffusion. Enfin, la disponibilité de chaque support de formation varie dans le temps, tout comme la disponibilité des moyens de transport. Le problème général qui nous est posé est de proposer à un apprenant un panel de LOs correspondant à son contexte actuel et permettant d'optimiser son expérience d'apprentissage. Cette optimisation intervient sur différents plans : la minimisation de la durée de la formation et la maximisation du gain de compétences. Nous proposons de comparer l'efficacité de certaines metaheuristiques de type recherche locale. Nous envisageons actuellement l'implémentation d'heuristiques simples (Hill-climbing, recherche locale orientée) et un peu plus complexes (Randomized Variable Neighborhood Search, Recuit Simulé).
Conclusion
Nous proposons une approche pour un système m-learning permettant aux formateurs de représenter leur savoir-faire en utilisant des règles métier et une ontologie pour assurer une hétérogénéité des connaissances. Ensuite, dans un environnement de mobilité, elle permet de prendre en compte les contraintes de l'environnement et les contraintes utilisateur. Enfin, la partie métaheuristique de notre approche permet une combinaison dynamique de morceaux de la formation en fonction de ces contraintes.

Introduction
Le clustering d'un ensemble d'objets en un nombre de groupes pré-déterminé est un problème souvent difficile suivant le critère d'optimisation ou le modèle choisi. Le choix optimal du nombre de groupes (identifié de manière univoque par la variable k dans le reste de l'article) l'est probablement davantage. Le principe généralement accepté du rasoir d'Occam, favorisant un nombre minimal de clusters, s'oppose à leur exhaustivité, sans qu'un compromis satisfaisant pour tous soit possible a priori. En pratique, ce paramètre est donc souvent laissé à la discrétion du praticien par les logiciels d'analyse de données, même récents. Dans le cas d'une approche exploratoire, où k peut être inconnu, une heuristique est souhaitable.
Dans cet article, nous nous limitons à l'algorithme de clustering spectral, et proposons une nouvelle manière extrêmement simple, peu coûteuse, et bien fondée, d'estimer k à partir du spectre de laplacien propre à cet algorithme. Le test de Bartlett pour l'égalité des variances est utilisé depuis longtemps pour déterminer le nombre de facteurs à retenir dans le contexte d'une Analyse en Composantes Principales (ACP) (James, 1969). Nous montrons qu'il est possible de l'adapter assez facilement pour estimer k dans le contexte de l'algorithme de clustering spectral.
Dans un premier temps, nous rappelons l'état de l'art du clustering spectral, ainsi que des méthodes d'estimation automatiques de k existantes. Nous décrivons ensuite notre méthode, in fine matérialisée par un algorithme simple. L'efficacité de la méthode est illustrée par des expériences sur des données synthétiques et réelles de la littérature. L'analyse critique de nos résultats nous permet de formuler quelques perspectives, données en conclusion.
Fondamentaux du clustering spectral
Les bases du clustering spectral remontent à la théorie des graphes. Il a été popularisé par (Shi et Malik, 2000) et (Ng et al., 2001). Considérant une collection de N éléments, représentée par une matrice symétrique de similarités 1 entre couples d'éléments S, l'algorithme de clustering spectral en k groupes peut être résumé comme suit :  (Ng et al., 2001) :
version random walk (Shi et Malik, 2000) :
avec I la matrice identité de taille N . Remarquons que la multiplicité de la valeur propre 0 dans la décomposition spectrale de ces laplaciens peut être interprétée comme le nombre de composantes connexes du graphe sous-jacent (von Luxburg, 2006), i.e. le nombre de clusters que forment ses noeuds. Une autre variante notable de normalisation est Zelnik-Manor et Perona, 2004). Une implémentation R récente est d'ailleurs basée sur cette dernière (Karatzoglou et al., 2013). En inspectant l'équation (1), remarquons que L sym = I ? L alt . Conséquemment, l'adaptation de l'algorithme 1 utilisant L alt considère les vecteurs propres majeurs, et relie k à la multiplicité de la valeur propre 1.
3 État de l'art sur la détermination du nombre de clusters
Le lien entre le paramètre k de l'algorithme 1 et la multiplicité de la valeur propre 0 dans le spectre du laplacien normalisé n'est strictement valable que pour des composantes connexes. Les graphes considérés peuvent cependant contenir des composantes faiblement connectées entre elles, sans être totalement disjointes : par exemple, les similarités calculées via une Radial Basis Function (RBF) n'égalent jamais exactement 0, induisant nécessairement une seule composante connexe. Le but de l'algorithme est alors précisément d'identifier cette structure.
FIG. 1 -Profil des plus petites valeurs propres pour synth2 et synth1 (voir la section 5 pour une description).
Dans le reste du document, pour gérer la variabilité des jeux de données tant en termes de domaine que de distribution, nous utilisons la variante de RBF proposée par (Karatzoglou et al., 2013). Cette dernière adapte le rayon de la fonction à chaque élément selon la médiane de ses K plus proches voisins. Comme préconisé par les auteurs, nous avons retenu K = 5 pour nos expériences, ainsi que pour le calcul des spectres présentés dans la figure 1.
La figure 1a montre que le profil des plus petites valeurs propres peut nous renseigner sur la probable valeur optimale de k. Intuitivement, une seule valeur propre égale exactement 0, k ? 1 autres sont approximativement égales à 0, et le reste est significativement supérieur à 0 : la meilleure valeur de k est ainsi marquée par la différence absolue entre la k La plupart des travaux de la littérature détermine l'eigengap vraisemblable de manière empirique, soit en comparant les candidats à un seuil arbitraire, soit en analysant le taux de croissance du profil des valeurs propres via le scree test de Cattell (Cattell, 1966). La figure 1b illustre néanmoins que même dans des cas relativement simples a priori, l'application de ce test peut être problématique. Une procédure d'optimisation itérative a également été proposée, mais demeure complexe, tant du point de vue conceptuel que computationnel (Zelnik-Manor et Perona, 2004). Nous proposons une alternative simple et efficace, en adaptant le test de Bartlett pour l'égalité des variances au clustering spectral. À l'instar du scree test, il était originellement employé à déterminer le nombre de facteurs à extraire dans le contexte d'une ACP (James, 1969).
Description de la méthode
Considérant un échantillon de N individus définis sur p variables, l'ACP calcule les q facteurs représentatifs de la matrice de covariance de l'échantillon, en faisant l'hypothèse implicite que des échantillons uni-dimensionnels générés par n'importe lequel des k = p ? q facteurs restants doivent avoir une variance identiquement faible. Dans ces conditions, la statistique de test ci-après suit une loi du ? 2 (James, 1969) :
avec ? i la i ème valeur propre dans l'ordre décroissant (conventionnel avec l'ACP), ¯ ? k la moyenne des k valeurs propres mineures, et
?j ). L'algorithme 2 permet ainsi de trouver simplement la plus petite valeur de q acceptable. Cet algorithme est quadratique selon p. Comme la décomposition spectrale est elle-même cubique, le surcoût calculatoire est modeste.
Entrée : Le vecteur des p valeurs propres, un niveau de risque ?, e.g. 5% Résultat : La plus petite valeur de q acceptable q ? 0 ; répéter q ? q + 1 ; s ? statistique de l'équation (3) ; /* on contraint q < p ? 1 car l'équation (3) n'est définie que pour k > 1 */ jusqu'à q = p ? 2 ou P ? 2 (X < s) ? 1 ? ?; /* on obtient le minimum de q ne menant pas au rejet de l'hypothèse nulle */ Algorithme 2 : Un algorithme simple pour déterminer le nombre de facteurs de l'ACP La détermination de k pour l'algorithme de clustering spectral est analogue au problème du nombre de facteurs de l'ACP : au lieu de rechercher les q plus grandes valeurs propres d'une matrice de covariance, nous nous intéressons alors aux k plus petites valeurs du spectre d'un laplacien (voir section 2). Il suffit alors d'adapter l'algorithme 2 à la recherche de la plus grande valeur acceptable pour k = N ?q (en effet, N = p pour un laplacien). Dans le contexte du clustering, k << N : il est donc plus efficace de faire démarrer la recherche à k = 2, i.e. initialiser q à p?2 dans l'algorithme 2, et le décrémenter à chaque itération, avec une condition d'arrêt adaptée.
D'autre part, nous avons constaté empiriquement qu'avec k << N , l'ensemble de valeurs propres du laplacien normalisé {? i } i?q est très proche de 1 en moyenne : cela permet d'approcher q
dans l'équation (3), menant à un critère ne dépendant que des k valeurs propres mineures. En entrelaçant les algorithmes 1 et 2, une extraction incré-mentale des valeurs propres depuis les plus petites peut alors être arrêtée précocément. Comme k << N , nous obtenons ainsi un algorithme de clustering spectral quadratique selon N , incluant la détermination automatique de k.
Résultats expérimentaux
Nous avons implémenté notre méthode sous la forme d'un package R, speccalt 2 , i.e. une alternative à la fonction specc du package R kernlab. L'interface, très simple, ne requiert qu'une matrice de similarité ; le paramètre k, optionnel, est estimé automatiquement en cas d'absence. Nous avons utilisé la normalisation de laplacien L alt , suggérée dans (Zelnik-Manor et Perona, 2004;Karatzoglou et al., 2013), plus stable en pratique pour réaliser le clustering. Toutefois, l'algorithme 2 repose toujours sur L rw Jeu de données Notre État de l'art indice de Rand (vérité terrain) méthode corrigé synth1(3) 3 4 ± 0,00 0,88 ± 0,18 synth2(3) 3 5 ± 0,00 0,97 ± 0,12 synth3(3) 3 3 ± 0,00 0,90 ± 0,21 synth4(5) 5 5 ± 0,00 0,76 ± 0,18 synth5(4) 4 4 ± 0,00 0,89 ± 0,21 synth6(3) 2 4 ± 0,00 0,58 ± 0,00 iris(3) 2 4 ± 0,00 0,54 ± 0,00 isolet ( . Comme l'algorithme 1 est sensible aux minima locaux à travers sa dépendance à k-means, l'indice de Rand corrigé est estimé par 20 exécutions indépendantes sur chaque jeu de données. Le même procédé est appliqué pour la méthode de (Zelnik-Manor et Perona, 2004), eu égard à sa nature itérative. En revanche l'estimation de k par l'algorithme 2 est déterministe. Ces résultats sont résumés dans la figure 2.
Nous constatons d'abord que notre heuristique obtient de meilleurs résultats que la mé-thode de référence. Elle est satisfaisante dans les premiers cas, mais moins pour isolet, synth6, et iris (2 clusters découverts, contre respectivement 5, 3 et 3 d'après la vérité terrain). Ceci est d'ailleurs reflété par une nette dégradation des indices de Rand respectifs. La vérité terrain d'isolet n'est pas caractérisée par des frontières de décision tranchées, ce qui induit notre mé-thode à identifier le nombre minimal de clusters. Les cas de synth6 et iris sont plus subtils : en suivant exactement l'algorithme 2, nous aurions identifié respectivement 62 et 29 clusters. En effet, notre méthode ne pénalise pas un nombre excessif de clusters, ou l'existence de très petits clusters : chaque point du cercle peu dense de synth6 est ainsi identifié comme un cluster. La nature quasiment discrète d'iris (i.e. toutes ses valeurs ont au plus une décimale) semble également problématique. Pour pallier ceci, notre implémentation de l'algorithme 2 borne explicitement k par 20. Si 1 ? ? n'est atteint pour aucune des valeurs autorisées, ce seuil est abaissé au plus grand quantile mesuré pour k ? [2, 20]. Par souci d'équité, des bornes de valeurs de k identiques ont été imposées à la méthode de (Zelnik-Manor et Perona, 2004).
Conclusion
Dans cet article, nous avons proposé une méthode simple, peu coûteuse, et performante pour estimer automatiquement k dans le contexte du clustering spectral, ainsi que l'attestent nos résultats expérimentaux. Toutefois, nous avons également identifié des limites à l'approche, par sa focalisation exclusive sur la caractérisation de variétés dans les données.
L'algorithme spectral utilise k-means en tant qu'étape intermédiaire : ce dernier, équivalent à un algorithme EM sur un mélange de gaussiennes isotropes, ouvre la voie à une possible combinaison de notre méthode avec une estimation bayésienne de k, par exemple en utilisant notre heuristique comme a priori.

Introduction
De nos jours, les réseaux sociaux ont connu une croissance importante. Sur Twitter ou sur Facebook la plus part des utilisateurs renseignent seulement 20% de leurs profils. La détection du profil peut être utilisée dans plusieurs domaines, par exemple du point de vue marketing, les entreprises peuvent être intéressées à déterminer quels types de personnes préfèrent leurs produits. Dans la littérature, beaucoup de travaux ont focalisé sur la classification d'une conversation ou d'un texte donné et plus précisément la détection de l'âge de l'auteur, de son genre, sa personnalité, sa langue native, etc. Argamon et al. (2009);Schler et al. (2006);Koppel et al. (2003);Pennebaker (2011).
Les travaux réalisés par Koppel et al. (2003) ont montré qu'au niveau du genre il y a des différences linguistiques entre les hommes et les femmes. En effet, les hommes qui préfèrent catégoriser les choses, utilisent plus de déterminants (le/la, cette/ce, un/une, etc.) et de quantificateurs (deux, plus, peu, etc.). Les femmes, s'intéressent aux relations et plus que les hommes recourent aux pronoms personnels (je, tu, moi, etc.). La suite de ce papier est organisée comme suit, dans la section 2 nous présentons notre méthode d'apprentissage en focalisant sur le choix des classes et l'algorithme employé. La dernière section présente notre étude expérimentale. 
Expérimentation et évaluation
Nous avons utilisé les corpus discernés de la conférence CLEF 2 2013. Nous avons effectué l'expérimentation avec un extrait du corpus d'entrainement. En faite, pour la dimension genre, qui a comme moyenne (baseline) de précision 0.5, nous avons obtenu de bons résultats, 58,16% des documents ont été bien classés. Pour la dimension âge qui a comme moyenne 33% les résultats sont prometteurs et témoignent de l'efficacité de la méthode. En effet, 57% des documents ont été bien classés. Comme le montre la figure 1, nous avons trouvé que la méthode d'apprentissage fondé sur les arbres de décision donne de meilleurs résultats. 
Conclusion
Nous avons effectué la catégorisation de documents en vue de fournir une classification de l'auteur d'un texte donné selon ses caractéristiques. Les résultats obtenus sont encourageants et surtout pour la dimension genre. La sélection manuelle du contenu des classes a montré ses limites face à des corpus de langues peu connues par le chercheur. L'automatisation de cette tâche s'avère d'une grande utilité, et l'utilisation de dictionnaires bilingues ou multilingues pourra faire face aux insuffisances linguistiques.
Il s'est avéré que l'utilisation des classes lexicales à elle seule n'est pas suffisante, cependant nous comptons intégrer d'autres aspects comme l'aspect syntaxique, morphologique, sémantique, etc. D'un autre coté, pour pouvoir mieux effectuer la détection du profil de l'auteur nous pensons s'ouvrir sur d'autres dimensions, à part l'âge et le genre nous allons aborder aussi la détection de la langue native, la détection des données géographiques de l'auteur et la détection du niveau linguistique, etc. 
Summary
In this paper, we present a method for profiling the author of an anonymous text. Our approach is based on learning the author profile with a focus on dimensions age and gender. First, we computed a ranked list of words that occur in the corpus and we grouped them into classes according to their similarities. Then, we calculated the CF (class frequency) score of each class for each document in order to find the stylistic differences between men and women, on the one hand, and those between different age intervals on the other hand. Our system has shown a high level of accuracy and effectiveness in treating the gender dimension.

Introduction
Un réseau complexe est la représentation d'un système complexe sous forme de graphe. Ils sont devenus très populaires en tant qu'outil de modélisation durant la dernière décennie car ils permettent de mieux comprendre le fonctionnement et la dynamique de certains systèmes . Un réseau complexe ordinaire ne contient que des noeuds et les liens existant entre eux ; cependant il est possible de l'enrichir avec différents types de données : orientation et/ou poids des liens, dimension temporelle, attributs associés aux noeuds ou aux liens, etc. Cette souplesse a permis d'utiliser les réseaux complexes pour étudier les systèmes du monde réel dans de nombreux domaines : sociologie, physique, génétique, informatique, etc. (Newman, 2003).
La nature complexe des systèmes modélisés entraine la présence de propriétés topologiques non-triviales au sein des réseaux correspondants. Parmi celle-ci, la structure de communautés est l'une des plus répandue et des plus étudiées. Informellement, on peut définir une communauté comme un groupe de noeuds densément interconnectés relativement au reste du réseau (Newman, 2003). Cependant, dans la littérature, cette notion est formalisée de très nombreuses différentes façons (Fortunato, 2010). Il existe en fait des centaines d'algorithmes destinés à dé-tecter les structures de communautés, caractérisés par l'utilisation d'une définition et/ou d'un traitement différents. Certains sont basés sur la mesure de modularité, une mesure similarité entre noeuds, le principe de compression des données, la notion de signification statistique, les mécanismes de diffusion de l'information, la percolation de cliques, etc. (cf. (Fortunato, 2010) pour une revue détaillée). La plupart des méthodes existantes traitent des réseaux ordinaires mais de nouvelles méthodes apparaissent pour analyser les réseaux les plus riches en exploitant les directions et poids des liens, puis le temps, et plus récemment les attributs des noeuds (Ruan et al., 2013;Tian et al., 2008;Zhou et al., 2009). Ces dernières se concentrent sur la recherche de groupes de noeuds denses en termes de liens, et dont les attributs sont homogènes. Même si cela n'est pas toujours indiqué explicitement, ces méthodes exploitant les attributs s'appuient sur l'hypothèse que les noeuds d'une même communauté doivent être similaires en termes d'attributs. Bien que les algorithmes diffèrent en termes de nature des communautés détectées, de complexité algorithmique, de qualité du résultat et d'autres aspects (Fortunato, 2010), leur production peut toujours être essentiellement décrite comme une liste de groupes de noeuds. Plus précisément, dans le cas de communautés mutuellement exclusives, il s'agit d'une partition de l'ensemble des noeuds. D'un point de vue applicatif, la question est alors de donner un sens à ces groupes relativement au système étudié. L'interprétation manuelle de petites communautés est possible, mais la méthode ne s'applique pas bien à de très grands réseaux.
Seuls quelques travaux ont essayé de s'attaquer explicitement à ce problème. Dans (Lancichinetti et al., 2010), les communautés sont caractérisées en comparant les distributions de plusieurs mesures purement topologiques. Dans (Tumminello et al., 2011) et (Labatut et Balasque, 2012), les auteurs se concentrent sur les attributs nodaux, et identifient les plus représentatifs pour chaque communautés au moyen de divers outils statistiques classiques. Les méthodes de détection de communautés exploitant les attributs sont généralement aussi en mesure de donner ce type d'information, car ces attributs sont identifiés pendant le processus de détection. Cependant, aucune des méthodes citées ne prend en compte toutes les données qu'un réseau riche peut contenir (attributs, topologie et dimension temporelle), qui plus est de manière systématique. Il existe donc un besoin pour un tel procédé, qui permettrait de caractériser les communautés des réseaux complexes riches. Dans ce travail, nous proposons une solution à ce problème, sous la forme d'une méthode d'analyse des réseaux attribués dynamiques. Pour cela, nous détectons les changements communs dans les mesures topologiques et les valeurs d'attribut sur une période de temps donnée. Plus précisément, nous cherchons à trouver les motifs séquentiels fréquents les plus représentatifs pour chaque communauté. Ces motifs peuvent ensuite être utilisées à la fois pour caractériser la communauté, et pour identifier ses anomalies, i.e. ses noeuds ayant un comportement non-standard. Les motifs fréquents représentent la tendance générale des noeuds dans la communauté considérée, alors que les anomalies peuvent correspondre à des noeuds ayant un rôle spécifique dans la communauté, ou situés à sa frontière. Nous illustrons notre méthode en l'appliquant à un réseau dynamique de co-auteurs extrait de la base de données bibliographiques DBLP 1 . Notre première contribution est de considérer la caractérisation de communauté comme un problème spécifique, distinct de celui de la détection de la communauté. La méthode à appliquer doit être indépendante de la technique utilisée pour détecter les communautés, se fonder sur une approche systématique facilement reproductible, et être la plus automatisée possible. Notre deuxième contribution est l'introduction d'une nouvelle représentation des réseaux attribués dynamiques. Elle prend la forme d'une base de données contenant des séquences de mesures topologiques, d'attributs nodaux et d'information communautaire, pour plusieurs tranches temporelles. Ce type de représentation avait précédemment été utilisé pour la repré-sentation de données naturelles (Mabroukeh et Ezeife, 2010), mais pas celle de graphes. Notre troisième contribution est la définition d'une méthode basée sur l'extraction sous contraintes de motifs séquentiels qui tire parti de cette représentation pour caractériser les communautés. Enfin, notre dernière contribution concerne une application à un réseau du monde réel. Dans la section suivante, nous donnons une description détaillée de notre méthode. Dans la Section 3, nous présentons nos résultats expérimentaux obtenus sur les données DBLP. La Section 4 décrit les travaux connexes, et la Section 5 présente les extensions possibles de notre travail.
Méthode
Un réseau dynamique attribué est constituée de différentes tranches temporelles, chacune représentée par un sous-réseau distinct, contenant les liens entre les noeuds pour un intervalle de temps donné. Ces tranches temporelles sont séquentielles. Habituellement, les noeuds et leurs attributs sont les mêmes pour chaque tranche, tandis que les liens entre eux et les valeurs des attributs peuvent changer. Nous proposons de caractériser les communautés d'un réseau dynamique attribué en fonction de l'évolution commune des mesures topologiques et des attributs de leurs noeuds. Le processus que nous proposons inclut 4 étapes. La première consiste à identifier une structure de communautés de référence. La seconde vise à créer la structure de données permettant une représentation séquentielle des mesures topologiques et des attributs des noeuds. Nous calculons d'abord les valeurs de toutes les mesures topologiques sélection-nées, puis nous discrétisons les valeurs des mesures et des attributs. Lors de la troisième étape, nous recherchons des motifs séquentiels fréquents et nous extrayons les noeuds qui supportent chaque motif. La quatrième étape consiste à choisir les motifs les plus représentatifs pour caractériser les communautés selon différents critères.
Détection des communautés. Pour détecter comment les noeuds évoluent en fonction de l'appartenance communautaire, nous avons d'abord besoin d'une structure de communautés de référence. Il serait possible d'appliquer une méthode dynamique, cependant cela entraîne des complications dues aux fusions, séparations, disparitions et apparitions de communautés au cours du temps. Pour cette raison, dans ce premier travail, nous avons décidé d'utiliser des communautés statiques, détectés sur une version intégrée du réseau.
Nous créons d'abord un nouveau réseau en agrégeant tous les liens dans le temps. Un poids est attribué à chaque lien en fonction de son nombre d'occurrences, afin de représenter sa stabilité dans le temps. Nous appliquons ensuite un algorithme de détection de communautés classique pour identifier nos communautés statiques. À cette fin, nous avons sélectionné Louvain (Blondel et al., 2008), qui est une méthode reconnue pour obtenir des structures de communautés de bonne qualité. La complexité temporelle de cette algorithme est O(n log n) où n est nombre de noeuds. La structure de communautés en résultant est utilisée dans le reste de notre analyse. Bien que les communautés soient statiques, les changements dans la structure du réseau seront néanmoins considérés lors du traitement des mesures topologiques des noeuds.
Constitution de la base de données. La deuxième étape consiste à représenter le réseau d'une manière appropriée pour l'extraction des motifs séquentiels fréquents. Un réseau dynamique attribué G = 1 , . . . , G ? est constitué d'une séquence de tranches temporelles, chacune prenant la forme d'un réseau attribué séparé G j (1 ? j ? ?), représentant une période de temps donnée. Chaque réseau statique G j contient n noeuds et correspond aux connexions entre ces noeuds pour la période j. Les noeuds et leurs attributs disponibles sont supposés être statiques, c'est à dire rester les mêmes pour tout G j . Au contraire, les valeurs des attributs et la structure du réseau peuvent changer au cours du temps. Un descripteur de noeud est soit un attribut nodal, soit une mesure topologique nodale.
Nous avons sélectionné les mesures topologiques nodales les plus répandues (degré, transitivité locale), ainsi que certaines mesures permettant de caractériser les noeuds en termes de position dans leur communauté (degré interne, coefficient de participation et enchâssement). Le degré d d'un noeud est son nombre total des voisins directs et sob degré interne d int est le degré calculé dans sa seule communauté. La transitivité locale, T = ? 1)), d'un noeud est la densité des triangles auxquels il appartient. Plus précisément, il s'agit du rapport entre le nombre de triangles auxquels le noeud appartient effectivement ( et le nombre maximal de triangles auxquels il pourrait appartenir, étant donné son degré (dénominateur). Le degré interne normalisé z et le coefficient de participation P sont deux mesures définies par Guimerá et Amaral (2005). La première exprime combien le noeud est connecté à sa propre communauté, relativement aux autres noeuds de sa communauté. Il s'agit du z-score de son degré interne, i.e. du nombre de voisins directs dans sa propre communauté. Un hub au sens de Guimerá et Amaral (2005) est un noeud dont le degré interne est élevé relativement au autres noeuds de sa communauté. La seconde mesure caractérise la distribution des voisins du noeud parmi toutes les communautés :
, où d c est le nombre de liens entre le noeud considéré et la communauté c. Il s'agit plus précisément de mesurer l'hétérogénéité de cette distribution. On obtient une valeur proche de 1 si tous les voisins sont répartis uniformément entre toutes les communautés, et 0 s'ils sont tous réunis dans la même communauté. L'enchâssement, e = d int /d, évalue à quel point les voisins d'un noeud appartiennent à sa propre communauté (Lancichinetti et al., 2010). À la différence du degré interne normalisé, l'enchâssement est donc normalisé par rapport au noeud, et non pas à sa communauté. Toutes ces mesures topologiques peuvent être calculées en un temps linéaire par rapport à n.
Toutes nos mesures topologiques sont à valeurs réelles, nous avons donc dû les discrétiser pour les adapter à cette définition. Un
descripteur pour la tranche temporelle j. Une base de données M est l'ensemble des tuples (?, ? ? , C ? ) où ? est l'identifiant du noeud, ? ? est sa séquence et C ? est l'étiquette de sa communauté (tels que définis à l'étape 1). La création de M revient à réaliser séquentiellement le calcul des mesures topologiques et la détection de communautés, donc sa complexité temporelle est aussi O(n log n).
Fouille de motifs séquentiels émergents. La troisième étape a pour but de trouver des motifs séquentiels émergents permettant de caractériser les communautés. Pour chaque communauté, nous recherchons les motifs séquentiels supportés par la majorité de ses membres. Avant de présenter la façon dont nous traitons la fouille, nous donnons quelques définitions nécessaires à la bonne compréhension de l'algorithme que nous avons utilisé.
Un itemset T est un sous-ensemble de I. Une séquence s = 1 , . . . , t m est une liste d'itemsets ordonnés dans le temps, où m est la longueur de la séquence. Une séquence ? = 1 , . . . , a m est une sous-séquence d'une autre séquence
. . , a m b im On dit également que ? est une super-séquence de ?. La taille d'une communauté C est le nombre total de noeuds qu'elle contient. Le support d'une séquence s pour une communauté donnée C est le rapport du nombre de noeuds supportant s à la taille de C : sup(s, C) = |{? ? C|s ? ? }|/|C|. Le taux de croissance d'une séquence s pour une communauté donnée C est le rapport du support de s dans C au support de s dans C, où C est le complémentaire de C dans le réseau (i.e. tous les noeuds du réseau sauf ceux de C) : Gr(s, C) = sup(s, C)/sup(s, C). Le taux de croissance mesure l'émergence de la séquence s dans la communauté C. Plus il est élevé, et plus la séquence s est caractéristique de la communauté C.
Compte tenu d'un seuil de support minimal min sup , un motif séquentiel fréquent est une séquence dont le support est supérieur ou égal à min sup . Un motif séquentiel fréquent fermé pour une communauté donnée est un motif séquentiel qui n'a pas de super-séquence pour le support minimal spécifié. Dans notre problème, nous caractérisons chaque communauté selon le motif séquentiel fréquent fermé. La méthode Clospan (Yan et al., 2003) a été définie pour identifier tous les motifs séquentiels fermés existants, pour un seuil donné, avec une complexité en O(n 2 ). Cependant, nous voulons que les motifs séquentiels identifiés soient représentatifs des communautés dans lesquelles ils sont identifiés. Il est donc nécessaire de prendre en compte le taux de croissance des motifs afin de tenir compte de leur émergence. Pour ce faire, nous appliquons la méthode de post-traitement définie dans (Plantevit et Cremilleux, 2009) pour calculer les taux de croissance de séquences d'article classés. Au final, notre approche est donc la suivante : d'abord nous identifions les motifs séquentiels fermés d'une communauté pour un support minimum donné. Puis, nous calculons les supports de ces motifs pour le reste du réseau entier. Enfin, nous en déduisons le taux de croissance de chacun des motifs séquentiels fermés.
Sélection des motifs séquentiels. Une fois que les motifs séquentiels ont été extraits pour chaque communauté comme expliqué à l'étape 3, nous devons sélectionner les plus représen-tatifs afin de caractériser la communauté. Pour cela, nous extrayons d'abord les noeuds supportant chaque motif. Dans un premier temps, nous avons décidé d'effectuer ce calcul de façon naive, ce qui entraine une complexité en O(rn), où r est le nombre de motifs. Mais il faut souligner que ce calcul pourrait être accéléré en l'intégrant à Clospan. On choisit ensuite les motifs selon deux approches distinctes : (1) motif dont le support est le plus élevé et (2) motif dont le taux de croissance est le plus élevé. Les résultats de l'étape 3 nous donnent directement le support et le taux de croissance. Cependant, dans certaines communautés, le motif de taux de croissance maximal ne couvre pas une partie significative des noeuds constituant la communauté. Il est alors nécessaire d'identifier d'autres motifs, dans un souci de représentativité. Nous sélectionnons alors le motif dont l'ensemble des noeuds le supportant est le plus différent possible de celui du premier motif, afin de couvrir au maximum la partie de la communauté ignorée par celui-ci. La distance entre les ensembles des noeuds supportant est calculée au moyen du coefficient de Jaccard. En cas d'égalité, nous utilisons le taux de croissance comme critère secondaire. Si la couverture totale est toujours insuffisante, on réitère en sélectionnant d'autres motifs selon les mêmes modalités. Cette itération se poursuit jusqu'à ce qu'au plus cinq noeuds ne soient pas couverts. Nous qualifions de déviant ces noeuds ne supportant aucun motif représentatif de la communauté. Comme nous devons comparer chaque paire de motifs pour chaque communauté, le temps nécessaire à la sélection des motifs représentatifs est de l'ordre de O(r 2 /p) où p est le nombre de communautés. La complexité totale de notre mé-thode est donc O(n 2 + r 2 /p). Il faut noter que r dépend grandement des nombres de tranches temporelles ? et d'items |I|.
Résultats
Nous présentons les expérimentations réalisées sur des données réelles. Nous avons choisi de traiter le réseau dynamique attribué de co-auteurs décrit dans (Desmier et al., 2012) et extrait de la base de données DBLP. Chacun des 2145 noeuds représente un auteur. Deux noeuds sont reliés si les auteurs correspondants ont publié au moins un article ensemble. Chaque tranche temporelle correspond à une période de 5 ans. Il y a au total 10 tranches temporelles allant de 1990 à 2012. Les périodes consécutives ont un chevauchement de 3 ans pour des raisons de stabilité. Pour chaque auteur, à chaque tranche temporelle, la base de données fournit le nombre de publications pour 43 conférences et journaux. Nous utilisons ces informations pour définir les 43 attributs nodaux correspondants, et nous en rajoutons deux : le nombre total de publications dans les conférences et dans les journaux. On a donc un total de 45 attributs. Nos descripteurs sont ces attributs, ainsi que les mesures topologiques décrites à la Section 2.
Les mesures topologiques sont discrétisées différemment, en fonction de leur nature. Pour le degré, nous utilisons les seuils 3, 10 et 30. Pour la transitivité, qui est définie sur [0; 1], il s'agit de 0, 35, 0, 5 et 0, 7. Pour l'enchâssement, lui aussi défini sur [0; 1], les intervalles sont 0, 3 et 0, 7. Ces intervalles ont été déterminés de manière à tenir compte des distributions de ces mesures sur l'ensemble des noeuds et des tranches temporelles : les différents seuils correspondent aux zones de faible densité. Pour les mesures de Guimerá et Amaral (2005), nous utilisons les seuils définis dans l'article original, c'est-à-dire : 2, 5 pour z et 0, 05, 0, 6, et 0, 8 pour P . Le seuil utilisé pour z permet de distinguer les hubs (z > 2, 5) des non-hubs (z ? 2, 5) communautaires. Pour les deux attributs relatifs aux nombres de publications dans des conférences et revues, nous considérons les valeurs 1, 2, 3, 4 et > 5. Pour le nombre total de publications, nous avons identifié les seuils 5, 10, 20 et 50 comme étant les plus pertinents.
L'algorithme Louvain identifie 127 communautés dans le réseau pondéré global, pour une modularité de 0, 59. Cette valeur signifie que ce réseau est hautement modulaire. 96 de ces communautés ne contiennent qu'un seul noeud. Parmi les communautés restantes, 17 contiennent plus de 10 noeuds, la plus grande en ayant 335. Nous recherchons ensuite les motifs séquentiels de ces communautés, pour un support minimum de 0, 3. Il ne nous a pas été possible de descendre en dessous de cette valeur en raison du coût spatial de l'algorithme Clospan utilisé. Pour chacune des communautés dont la taille est supérieure à 40, nous détections plus de 5000 motifs, dont la plupart ne comprennent que des mesures topologiques.
Les motifs les plus supportés sont toujours une séquence de z < 2, 5 pour toutes les communautés, avec des longueurs variables. Cela signifie que la majorité des noeuds de chaque TAB. 1 -Longueur des motifs les plus supportés pour une sélection de communautés.
communauté ont un rôle de non-hub communautaire. Pour mémoire, Amaral & Guimerà défi-nissent un hub communautaire comme un noeud dont le degré interne est largement supérieur au degré interne moyen de sa communauté. Ainsi, le motif détecté signifie que la majorité des noeuds ont un degré interne relativement faible (ce qui est attendu), et ce durablement (ce qui ne l'est pas forcément). Bien que ce type de motif soit présent dans toutes les communautés, on peut faire une distinction en considérant la longueur de la séquence, qui mesure cette durée. Dans le Tableau 1, nous listons la longueur des motifs séquentiels les plus supportés, en indiquant leur communauté, la taille de celle-ci et la valeur de support du motif. Les communautés dont les tailles sont comprises entre 39 et 45 (communautés 40, 55 et 77 ) obtiennent de longues séquences (resp. 8, 7 et 7 ). Surtout, les supports pour les communautés 55 et 77 atteignent même la valeur maximale de 1. Cela signifie que, dans ces communautés, aucun hub n'existe ; ou bien si un hub apparait, il disparait rapidement. Cette observation est particulièrement intéressante, et traduit l'absence d'un meneur communautaire qui structurerait la communauté par ses connexions multiples. Pour la communauté 115, la longueur de la sé-quence est 1 et sa valeur de support est également 1. Cela signifie que tous les noeuds de cette communauté ont tenu le rôle de non-hub simultanément au moins une fois au cours du temps, mais qu'il existe des hubs pour le reste des tranches temporelles. Pour les communautés 38, 40 et 75, le support est inférieur à 1, ce qui signifie que si une écrasante majorité de noeuds tient le rôle de non-hub pour de longue durées, en revanche un petit nombre de noeuds occupe la place de hub, éventuellement par intermittence. Nous extrayons les auteurs qui ne suivent pas les motifs les plus supportés pour ces trois dernières communautés. Pour la communauté 38, il s'agit de Philip S. Yu, Jiawei Han et Beng C. Ooi. Comme supposé, ces noeuds ont un nombre de connexions remarquablement élevé à l'intérieur de leurs communautés, et les auteurs qu'ils représentent ont effectivement des rôles de meneurs dans leur domaine. Une analyse plus approfondie des données montre également qu'ils publient un total de plus de 10 articles par tranche temporelle. En outre, ils n'occupent jamais de rôle non-hub. Les noeuds déviants pour les communautés 40 et 75 sont respectivement Hans-Peter Kriegel et Divesh Srivastava. Là aussi, il s'agit d'auteurs importants dans leur communauté. Les séquences qui les caractérisent confirment qu'ils sont productifs et n'occupent jamais le rôle non-hub au fil du temps.
Pour les communautés dont les tailles sont comprises entre 39 et 45, nous ne constatons aucune motif émergent contenant une conférence ou revue. Les motifs les plus émergents ont un taux de croissance supérieur à 1, 79, ce qui signifie qu'il n'existe pas de motif séquentiel très distinctif pour ces communautés. Pour la plupart des grandes communautés, le motif le plus émergent inclut une conférence ou une revue spécifique, ce qui peut s'interpréter en termes de thématique de la communauté. Les autres descripteurs constituant le motif sont des mesures topologiques. Comme pour les motifs les plus supportés, l'item z < 2, 5 apparait le plus souvent dans les motifs détectés. Cependant, ces motifs les plus émergents ne permettent pas de couvrir la majorité des noeuds des communautés concernées. C'est pourquoi, comme nous l'avons expliqué dans la section 2, nous recherchons des motifs supplémentaires en minimisant l'intersection de leurs supports. Ces motifs sont généralement constitués de mesures topologiques, et n'ont pas un taux de croissance très élevé. Dans la suite, nous nous concentrons sur les communautés donnant les résultats les plus intéressants. Pour chacune, nous décrivons le motif le plus émergent et nous présentons les noeuds déviants, qui ne supportent ni le motif le plus émergent ni les motifs supplémentaires. Chaque motif est représenté formellement entre crochets, comme une séquence d'itemsets, eux-mêmes représentés entre parenthèses.
Pour la communauté 61, le motif le plus émergent est <(ICML PUB. NUM = 1) (DEGRE 3-10, Z < 2,5)> avec un taux de croissance 3, 52 et support 0, 30. Ce motif fait référence aux auteurs qui sont publiés une fois dans ICML, après quoi leur degré se stabilise entre 3 et 10 et ils occupent tous le rôle de non-hubs. Nous extrayons 7 motifs supplémentaires afin de couvrir tous les noeuds de la communauté. Parmi eux, les plus intéressants sont <(Z < 2,5) (Z < 2,5) (Z < 2,5, TOTALE CONF PUB. NUM 1-5) (AAAI PUB. NUM = 1)> avec un taux de croissance de 1, 69 et un support de 0, 30 et <(PART. COEFF 0.05-0.6, CIKM PUB. NUM = 1)> avec un taux de croissance de 1, 40 et un support de 0, 30. Le premier motif se réfère à des noeuds qui restent des noeuds non-hubs pendant un certain temps, puis commencent à publier dans des conférences, avant de publier à AAAI tout en perdant leur statut de non-hub (sans pour autant devenir massivement des hubs). Le second n'a pas de dimension temporelle, mais nous montre l'existence ponctuelle de noeuds publiant à CIKM tout en occupant une position périphérique dans leur communauté, i.e., en étant significativement connecté à d'autres communautés. Les noeuds déviants pour cette communauté sont Alex A. Freitas, Claire Cardie , Edwin P. D. Pednault. Parmi ces auteurs, Alex A. Freitas ne publie pas pendant les 8 premières tranches temporelles, puis commence à publier de manière très active, à différentes conférences telles qu'ICML ou AAAI, et dans des journaux. Pour les deux autres auteurs, tandis que Claire Cardie publie régulièrement à ICML au cours des 6 premières tranches temporelles, Edwin P. D. Pednault ne publie ni à ICML, ni AAAI ou CIKM .
Le motif <(PODS PUB. NUM = 1)> est le plus émergent dans la communauté 75. Son taux de croissance est 3, 59 et son support 0, 40. Ce motif montre que %40 des auteurs de cette communauté publie au moins une fois à PODS et ce groupe est émergent par rapport au reste du réseaux pour cette communauté. Quatre motifs supplémentaires sont nécessaires pour couvrir le reste de la communauté. Ces motifs se réfèrent à des noeuds occupant des positions de nonhub périphérique, et dont la transitivité est très élevée. Les noeuds déviants sont Ninghui Li, Li Feifei et Abdullah Mueen, qui ont la particularité de ne jamais publier dans PODS. Le motif le plus émergent de la communauté 106 est <(Z < 2,5) (Z < 2,5) (Z < 2,5) (Z < 2,5) (Z < 2,5) (PART. COEFF 0.05-0.6, KDD PUB. NUM=1)> avec un taux de croissance 2, 87 et support 0, 40. Ce motif fait référence à des noeuds durablement non-hubs, qui deviennent périphériques tout en publiant à KDD. Nous identifions 4 motifs supplémentaires pour finir de couvrir la communauté. Ceux-ci concernent les noeuds à la fois ultrapériphériques et bien intégrés à leur propre communauté. Les noeuds déviants sont Stan Matwin, qui publie plus d'un article par tranche temporelle à KDD, et qui n'est pas durablement non-hub ; et Hua-Jun Zeng qui n'a jamais publié dans KDD. Ce dernier est aussi caractérisé par un fort accroissement du nombre d'articles produits, alors qu'il ne publie pas pendant les 5 premières tranches temporelles.
Le motif le plus émergent de la communauté 45 est <(VLDB PUB. NUM=3) (DEGRE 3-10 Z < 2,5)> avec un taux de croissance de 6, 40 et un support de 0, 30. Cette séquence nous dit qu'il y a un groupe remarquable d'auteurs qui ont publié 3 fois en conférence VLDB, puis dont le degré s'est stabilisé entre 3 et 10 et qui ont occupé des positions non-hubs dans leur communauté. Nous avons dû identifier 6 autres motifs pour couvrir le reste de la communauté. L'un d'eux est <(Z < 2,5, TOTAL CONF. PUB. NUM 1-5) (Z < 2,5, EMBED 0.3-0.7, ICDE PUB. NUM= 1)>, avec un taux de croissance de 2, 30 et un support de 0, 30. Ce motif couvre les noeuds non-hubs qui ont publié entre 1 et 5 fois dans des conférences, puis dont l'enchâsse-ment s'est stabilisé à une valeur relativement élevée, tout en gardant leur position de non-hub et en publiant à ICDE . Les noeuds déviants sont Ingmar Weber et Anastasia Ailamaki, qui ne publient pas pour les sept premières tranches temporelles, puis deviennent de plus en plus productifs au cours des trois dernières tranches.
Pour résumer nos observations, le motif le plus émergent dans à peu près toutes les communautés comprend habituellement le fait d'être non-hub et d'avoir un petit nombre de publications dans divers conférence ou journaux. En fonction des quelques conférences ou journaux apparaissant dans ces motifs, il est possible de déduire le thème principal des communautés. Pour certaines communautés, cependant, les motifs séquentiels émergents sont purement topologiques (pas d'attributs). On peut alors supposer que les membres de ces communautés ne publient pas de façon suffisamment homogène pour que cela transparaisse dans les motifs. Une autre raison peut être simplement que les membres de la communauté sont reliés pour des raisons autres que thématiques, auquel cas cela n'apparait pas dans les attributs sélectionnés pour notre étude. En ce qui concerne les noeuds déviants, on peut distinguer différents types de profils. Certains peuvent correspondre à des auteurs dont la thématique principale est diffé-rente de celle de la communauté dans laquelle ils ont été placés. Dans certains cas, nous avons détecté des auteurs qui avaient visiblement changé de thématique, ou bien qui débutaient dans une thématique donnée. Il peut également s'agir d'auteurs actifs dans un autre domaine, dont les conférences et journaux ne font pas partie de ceux retenus dans les données que nous avons considérées ici. Un autre profil est celui du chercheur en train de monter en charge, et dont la position communautaire et le nombre de publications sont en train d'évoluer de conjointement.
Travaux connexes
Il existe de nombreuses méthodes de détection de communautés dans les réseaux complexes ordinaires (i.e., ne contenant que des noeuds et des liens), basées uniquement sur l'information topologique. Une revue détaillée de ces méthodes est donnée dans (Fortunato, 2010). Dans le cas des réseaux plus riches, contenant des attributs nodaux, une communauté est essentiellement définie comme un groupe de noeuds à la fois densément interconnectés, et similaires en termes d'attributs. Certaines méthodes tentent d'estimer des partitions optimisant directement ces deux critères simultanément (Cruz et al., 2011). D'autres transforment encodent d'abord les attributs sous forme d'information structurelle, et détectent ensuite les communautés de façon plus classique (Ruan et al., 2013;Zhou et al., 2009). Il existe également des méthodes de recherche de motifs (ici de petits sous-graphes densément connectés tels que des cliques) possédant des attributs homogènes (Gunnemann et al., 2010;Moser et al., 2009;Mougel et al., 2010).
Il existe aussi des travaux qui se concentrent sur la détection de communautés dans les réseaux dynamiques (Robardet, 2009). Leur but principal est l'observation de l'évolution de la structure de communautés, à travers des évènements tels que la formation, la dissolution, la croissance, la diminution et la fusion. Pour des réseaux dynamique attribuées, il existe des méthodes pour trouver les groupes de noeuds structurellement similaires dont les attributs changement de la même façon au cours du temps (Desmier et al., 2012). Ces méthodes ne cherchent pas des communautés. L'objectif de ces travaux est la découverte de motifs intéressants en tenant compte de la structure des liens et des attributs pour les réseaux dynamiques attribuées. Pour regrouper les noeuds, ces méthodes utilisent des contraintes, comme par exemple une distance limite entre noeuds de la même motif, ou la nécessité de partager les mêmes voisins.
Bien qu'il existe de nombreuses techniques différentes pour identifier les structures de communautés, il y a peu d'auteurs qui travaillent à caractériser les communautés obtenues. Dans (Lancichinetti et al., 2010), les auteurs comparent les distributions de certaines mesures topologiques afin de comprendre la forme générale des communautés, et tentent de les caractériser en fonction du type de système modélisé (biologique, informatique, social, etc.). Ils n'utilisent pas d'attributs nodaux et ne traitent que des réseaux statiques. Dans (Tumminello et al., 2011), les auteurs proposent une méthode statistique inspirée d'approches utilisées en génétique, pour caractériser les communautés en termes d'attributs surexprimés. Cependant, cette étude n'a pas recours à des mesures topologiques et se limite également aux réseaux statiques. Dans (Labatut et Balasque, 2012), les auteurs interprètent les communautés d'un réseau attribué social. Ils utilisent la régression statistique et l'analyse discriminante pour identifier les valeurs des attributs les plus caractéristiques de chaque communauté. Cependant, ici encore, le réseau est statique et l'information structurelle n'est pas exploitée.
Enfin, le travail présenté dans (Prado et al., 2013) n'est pas directement concerné par la détection ou la caractérisation de communautés, mais est néanmoins lié à notre approche. Prado et al. (2013) ont introduit l'idée d'utiliser les mesures topologiques et les attributs nodaux ensemble dans une perspective d'exploration de données. Leur but était de trouver des motifs reflétant la covariation de mesures topologiques et d'attributs sur l'ensemble du réseau. Ils se sont concentrés sur des réseaux statiques et n'ont pas considéré de structures communautaires.
Conclusion
Nous traitons le problème de la caractérisation des communautés dans des réseaux complexes dynamiques et attribués. Nous proposons une nouvelle représentation de l'information encodée dans le réseau, permettant de stocker simultanément l'information topologique, les attributs nodaux et la dimension temporelle. Nous utilisons cette représentation pour effectuer une fouille de motifs séquentiels fréquents. Chaque communauté peut ensuite être caractéri-sée par ses motifs les plus distinctifs. Nous tirons également parti des motifs pour détecter et caractériser les noeuds déviants dans chaque communauté. Nous appliquons notre méthode à un réseau de collaboration scientifique construit à partir des données publiques de la base DBLP. Les résultats montrent que notre méthode est capable de caractériser les communautés en fonction, notamment, de leur thématique. Les noeuds déviants identifiés correspondent à différents types de profils, tels que des chefs de file communautaires, des chercheurs émergents, ou d'autres en train de changer de thématique de recherche.
A notre connaissance, il s'agit de la première formulation de la caractérisation de communautés comme un problème de fouille de données. Notre but était de surmonter les limitations des rares travaux existants (Lancichinetti et al., 2010;Tumminello et al., 2011;Labatut et Balasque, 2012) en proposant une approche systématique, tenant compte à la fois de la structure, des attributs nodaux et du temps. La représentation des données que nous utilisons n'avait jamais été appliquée au traitement de graphes. Le processus proposé pour extraire les motifs les plus pertinents en se basant sur une recherche de motifs séquentiels sous contraintes est original et nous avons montré la richesse des interprétations apportées sur un cas réel de réseau. Certaines étapes de l'implémentation présentée ici étaient relativement naives, nous comptons les améliorer afin de réduire le temps nécessaire au calcul. En particulier, l'identification des ensembles de support pourrait être intégrée à Clospan.
Afin de restreindre la complexité conceptuelle de cette première approche, nous avons volontairement limité notre méthode d'analyse en ne considérant pas l'évolution des communautés au cours du temps. Dans des travaux ultérieurs, nous envisageons d'appliquer un algorithme de détection de communautés approprié en insérant cette information dans la base de données utilisée pour la fouille de motifs. Nous comptons également appliquer notre méthode d'analyse à d'autres types de réseaux pour explorer ses capacités de caractérisation. Comme autre perspective, nous pouvons aussi mieux exploiter nos représentations de réseaux attribués dynamiques. Ici, nous nous sommes seulement intéressés à l'extraction de séquences fréquentes. Cependant, notre représentation des données du réseau peut également être utilisée pour interroger les noeuds selon certaines mesures ou attributs topologiques spécifiques. Ainsi, dans nos expériences, nous avons vu qu'il y avait beaucoup de noeuds qui n'appartiennent pas à une communauté. Il serait intéressant de bien regarder ces noeuds et mieux comprendre en quoi ils sont différents des autres pour, par exemple, formuler des hypothèses sur leur isolement.

Introduction
Twitter offre des fonctionnalités de microblogging qui sont utilisées par des millions de personnes à travers le monde pour publier des messages courts. Ces personnes créent et partagent de l'information liée à divers types d'évènements, allant d'évènements personnels banals à des évènements importants et/ou globaux, quasiment en temps-réel. L'explosion du nombre d'utilisateurs de ce réseau a entraîné l'apparition d'un phénomène de surcharge informationnelle. Pour lutter contre cela, il est nécessaire de doter les utilisateurs de moyens leur permettant d'identifier plus facilement les éléments d'information les plus intéressants et de se tenir au courant des derniers évènements significatifs.
L'information brute produite par Twitter est délivrée sous la forme d'un flux de messages. Par conséquent la manière dont ceux-ci arrivent au fil du temps recèle une part importante de leur signification. La dynamique temporelle des thématiques les plus populaires est constituée d'une succession de focus et dé-focus, autrement dits, une succession de pics de popularité. C'est pourquoi de nombreuses approches -allant de méthodes basées sur la fréquence des mots jusqu'à des méthodes plus complexes reposant sur des modèles de thématiques probabilistes dynamiques -ont été proposées dans le but d'identifier ce genre de thématiques. Ces méthodes reposent sur des stratégies variées de détection des pics et produisent des résultats très différents. Nos travaux s'intéressent au filtrage et à l'identification de thématiques à partir de l'information contenue dans un flux de messages produits par Twitter afin de, entre autres, fournir une vue rétrospective des thématiques les plus populaires ou bien recommander des éléments d'information intéressants en temps réel. Une bonne solution doit satisfaire deux critères : d'une part les thématiques identifiées doivent être précisément localisées dans le temps et intelligibles et d'autre part, la méthode doit pouvoir passer à l'échelle et traiter de grands volumes de données. Afin de conserver une complexité temporelle raisonnable, beaucoup de méthodes existantes assimilent une thématique à un simple mot. C'est le cas par exemple d'approches basées sur l'analyse de la fréquence des mots telles que la méthode « Peaky Topics » (Shamma et al., 2011) ou la méthode basée sur l'indice MACD (Rong et Qing, 2012). Un pic de popularité se traduit par l'augmentation soudaine de la fréquence d'un mot. Cette défini-tion n'est pas toujours appropriée à cause de l'ambiguïté possible. Pour pallier à ce problème, Benhardus et Kalita (2013) proposent d'étudier des n-grammes de mots, mais les n-grammes ne peuvent capturer les relations entre des mots trop éloignés dans le corps d'un message et sont très sensibles au bruit. Afin d'identifier des thématiques plus explicites, de nombreuses méthodes se basant sur des modèles probabilistes ont été développées, telles que OLDA (AlSumait et al., 2008) ou Online-LDA (Lau et al., 2012). Un pic de popularité se traduit alors par une variation soudaine de la distribution des thématiques. Cependant, l'introduction de la dimension temporelle dans ces modèles augmente la complexité des mécanismes d'inférence mis en oeuvre (il faut notamment faire en sorte que les thématiques qui évoluent peu dans le temps restent comparables au sein du modèle, et également faire en sorte que le modèle ait un niveau de sensibilité constant de sorte qu'il puisse détecter de nouvelles thématiques au fil du temps), ce qui limite leur capacité de passage à l'échelle. Par ailleurs, Aiello et al. (2013) ont montré que les méthodes à base de modèles probabilistes dynamiques ne sont pas efficaces sur des flux sociaux trop hétérogènes au sein desquels de nombreux éléments d'information sont relatés simultanément. Qui plus est, la très vaste majorité des méthodes existantes ignore un aspect important de ces flux, précisément leur aspect social. En effet, un message ne se limite pas à un simple contenu textuel et il est notamment possible d'y insérer une ou plusieurs « mentions » (à l'aide de la syntaxe « @pseudonyme » dans le corps des messages). Lorsque l'auteur d'un message insère une mention, il crée en réalité un lien dynamique vers un autre utilisateur. Ce lien est considéré comme dynamique puisque sa création est datée et liée à un contenu particulier, celui du message. Les mentions permettent aux utilisateurs d'exprimer leur volonté d'engager la discussion à propos du contenu du message avec la ou les personnes ciblées, et traduisent donc l'intérêt qu'ils portent à la thématique liée. À notre connaissance, les travaux menés par Takahashi et al. (2011) sont les seuls à intégrer cette caractéristique. La méthode qu'ils proposent repose sur une modélisation probabiliste du comportement de chaque utilisateur du réseau en terme de création de liens dynamiques. En détectant les points de rupture par rapport à ces comportements standards, il est possible d'identifier des thématiques émer-gentes, une thématique étant définie comme un simple mot. Outre cette définition qui limite l'intérêt des résultats obtenus, la méthode souffre de la complexité de la phase d'apprentissage du modèle puis de détection qui rend quasiment impossible sa mise en oeuvre dans des conditions réelles (i.e. un réseau comportant un grand nombre d'utilisateurs). Globalement, il apparaît nécessaire de développer des méthodes mieux adaptées. Le reste de cet article est organisé comme suit. Dans la section suivante, nous présentons une nouvelle méthode pour la détection de thématiques populaires sur Twitter, puis nous présentons les résultats obtenus dans la section 3.
2 Méthode proposée L'objectif de la méthode est d'identifier des thématiques à la fois riches de sens et précisé-ment localisées dans le temps, tout en tenant compte de l'aspect social du flux de messages. 
FIG. 1 -Grands principes de la méthode proposée.
Entrée. Nous traitons un flux produit par Twitter contenant M messages. Le vocabulaire des termes employés dans ces messages est noté V . Nous discrétisons l'axe temporel en partitionnant les messages en n tranches temporelles de même durée (cf. la figure 1 pour une illustration de ce pré-traitement). Cette étape de pré-traitement est commune à toutes les approches de détection de thématiques temporelles citées précédemment. On note ? t (i) la fonction qui donne le nombre de messages inclus dans la i ème tranche temporelle qui contiennent au moins une occurrence du terme t. ? t (i) désigne la fonction qui donne le nombre de messages inclus dans la i ème tranche temporelle qui contiennent au moins une « mention » et le terme t. Les séries temporelles correspondantes sont notées ? t et ? t .
Sortie. La méthode génère une liste de thématiques, ordonnées selon leur popularité. Une thématique est définie par un terme principal, une liste pondérée de termes liés et un intervalle temporel. Par exemple, la thématique : {["google", {("chrome",0.8), ("os", 0.8), ("desktop", 0.75) }], ['19/11/09' ;'20/11/09']}, capture l'évènement créé par la sortie de Google Chrome OS le 19 novembre 2009.
Grands principes de la méthode. Nous décomposons la tâche d'identification des thé-matiques populaires en 3 problèmes : (1) l'identification des termes principaux et des intervalles temporels, chaque couple étant associé à un score de popularité ; (2.a) la sélection de termes liés pertinents ; (2.b) la construction du graphe des redondances et du graphe de théma-tiques, duquel est extrait la liste finale de thématiques. La méthode se déroule comme suit. Tout d'abord, le problème (1) est résolu pour chaque terme appartenant au vocabulaire V . Ensuite, pour chaque couple de terme principal et intervalle temporelle, le problème (2.a) est résolu afin d'identifier l'ensemble pondéré de termes liés. Chaque thématique ainsi constituée est insérée dans le graphe de thématiques si elle n'est pas redondante avec une autre thématique déjà présente (2.b). Les redondances constatées sont modélisées par un second graphe, qui permet d'identifier les thématiques à fusionner à la fin du processus, avant d'extraire la liste des thé-matiques populaires qui sera retournée à l'utilisateur. La figure 1 décrit le déroulement de la méthode ainsi que les IHM permettant de visualiser les thématiques identifiées.
Identification des termes principaux et des intervalles temporels
L'objectif de cette première étape est de produire une liste ordonnée de thématiques, chacune étant définie par un terme principal, un intervalle temporel et un score caractérisant sa popularité. Notre hypothèse est que la fréquence de création de liens dynamiques (i.e. fréquence des « mentions ») liée à une thématique est un meilleur indicateur du niveau d'attention qu'elle reçoit de la part des utilisateurs que sa fréquence d'apparition globale. Nous utilisons donc cette mesure pour localiser dans le temps les thématiques et estimer leur niveau de popularité.
Calcul du score de popularité. Nous définissons d'abord la fréquence de mentions attendues en chaque tranche temporelle pour le terme t : f a(t) (eq. 1). Nous exprimons ensuite l'anomalie de la fréquence des mentions liées à ce terme à la i ème tranche temporelle selon la formule donnée par l'équation 2. Identification de la période de popularité. Identifier l'intervalle I durant lequel un terme t était le plus populaire revient à trouver l'intervalle qui maximise la valeur de score(t, I). Or, nous venons de montrer que ce score est obtenu en sommant la fonction d'anomalie. Par conséquent, identifier l'intervalle le plus populaire revient à résoudre un problème du type « sous-séquence contiguë de somme maximale » (SSCSM). Nous résolvons ce problème de type SSCSM à l'aide de l'algorithme en temps linéaire décrit par Bentley (1984).
Sélection de termes liés pertinents
Afin de préciser une thématique décrite par un terme principal t et un intervalle temporel I, nous sélectionnons un ensemble de termes liés S. Afin de limiter la surcharge d'information pour l'utilisateur, il faut que cet ensemble soit d'une taille raisonnable et qu'il contienne des termes pertinents durant l'intervalle temporel.
L'ensemble des termes liés potentiels est réduit aux p termes les plus co-occurrents avec t durant I. Pour sélectionner les termes les plus pertinents parmi ceux-ci, nous proposons de calculer un poids w q ? [0; 1] pour chaque terme t q ? S basé sur la corrélation entre la dynamique temporelle de t et t q durant l'intervalle I. Seuls les termes dont le poids dépasse un certain seuil noté ? sont conservés. Les paramètres p et ? sont fixés par l'utilisateur de la méthode. Nous estimons la corrélation entre les séries ? t et ? t q à l'aide du coefficient récemment proposé par Erdem et al. (2012). Ce coefficient, développé à l'origine par les auteurs pour analyser des données boursières non-stationnaires, est particulièrement adapté aux données que nous traitons. Par soucis de concision, nous donnons directement la formule d'approximation de la corrélation entre la dynamique du terme t et le terme lié t
La preuve que |? O | 1 est donnée par Erdem et al. (2012 
Construction des graphes de thématiques et des redondances
Afin de générer l'ensemble final des thématiques retourné à l'utilisateur, nous construisons deux structures de graphe : le graphe de thématiques et le graphe des redondances. Le premier est un graphe orienté composé de noeuds appartenant à deux classes : les noeuds représentant les termes principaux, lesquels sont annotés par un intervalle et un score, et les noeuds repré-sentant les termes liés. Ces derniers sont connectés à l'aide d'arcs pondérés, dirigés vers les termes principaux. Le second est un simple graphe non-orienté servant à représenter la redondance entre certaines thématiques. Chaque thématique T = (t, I, S) générée par la résolution des deux problèmes précédemment décrits est insérée dans le graphe des thématiques si elle n'est pas jugée redondante avec une thématique déjà présente. Une thématique T 1 est jugée redondante avec T 2 si le terme principal t 1 serait mutuellement connecté avec le terme t 2 et si l'intersection entre I 1 et I 2 mesurée par |I1?I2| min(|I1|,|I2|) est importante (i.e. dépasse un seuil ? < 1). Dans le cas où la thématique à insérée T 1 est jugée redondante, sa définition est mise de côté et une arrête liant t 1 et t 2 est ajoutée au graphe des redondances. Les thématiques étant ordonnées selon leur score de popularité, l'utilisateur peut paramétrer le nombre k de thématiques qu'il souhaite, et seules le k plus populaires et non-redondantes lui seront pré-sentées. Une fois les deux graphes construits, l'identification des thématiques qui peuvent être fusionnées ensemble consiste en l'identification des composantes connexes au sein du graphe des redondances. Cela se fait en temps linéaire à l'aide de l'algorithme décrit par Hopcroft et Tarjan (1973). Le terme principal de la thématique fusionnée devient l'agrégation des termes principaux, et seuls les p termes liés avec les p plus grands poids sont conservés. En parcourant les noeuds de la classe principale du graphe de thématiques on reconstruit les thématique une par une à partir des annotations du noeud principal et des termes liés qui y sont connectés. TAB. 2 -Trois des thématiques identifiées : la position en terme de popularité est donnée devant le terme principal (en gras).
novembre aux USA, #7 tiger, woods (composé donc issu d'une fusion), victime d'un accident, #32 water, i.e. l'eau trouvée sur la Lune par la NASA). L'hypothèse selon laquelle ? (fréquence des mentions) est un meilleur indicateur de popularité qu'? a été vérifiée expérimentalement, puisqu'une version de la méthode se basant exclusivement sur ? a obtenu une précision@n systématiquement inférieure et a globalement détectée les thématiques avec du retard.

Introduction
Depuis les années 1990, les progrès de l'informatique et des capacités de stockage permettent la manipulation de très gros volumes de données : il n'est pas rare d'avoir des espaces de description de plusieurs milliers, voire de dizaines de milliers de variables. On pourrait penser que les algorithmes de classification sont plus efficaces avec un grand nombre de variables, mais la situation n'est pas aussi simple que cela. Le premier problème qui se pose est l'augmentation du temps de calcul. En outre, le fait qu'un nombre important de variables soit redondant ou non pertinent pour la tâche de classification perturbe considérablement le fonctionnement des classifieurs. De plus, la plupart des algorithmes d'apprentissage exploitent des probabilités dont les distributions peuvent être difficiles à estimer en présence d'un très grand nombre de variables. L'intégration d'un processus de sélection de variables dans le cadre de la classification des données de grande dimension devient donc un enjeu central. Dans la littérature, trois types d'approches pour la sélection de variables sont principalement proposés : les approches directement intégrées aux méthodes de classification, dites «embedded», les mé-thodes basées sur des techniques d'optimisation, dites «wrapper», et finalement, les approches de filtrage. Des états de l'art exhaustifs ont été réalisés par de nombreux auteurs, comme Ladha et al. (Ladha et Deepa, 2011), (Bolón-Canedo et al., 2012) ou (Guyon et Elisseeff, 2003). Nous ne faisons donc ci-après qu'un rapide tour d'horizon des approches existantes. Les approches « embedded » intègrent la sélection des variables dans le processus d'apprentissage (Breiman et al., 1984). Les méthodes les plus populaires de cette catégorie sont les méthodes basées sur les SVM et les méthodes fondées sur les réseaux de neurones. A titre d'exemple, SVM-EFR (Recursive Feature Elimination for Support Vector Machines) (Guyon et al., 2002) est un processus intégré qui effectue la sélection des variables de façon itérative en utilisant un classificateur SVM et en supprimant les variables les plus éloignées de la frontière de décision. De leur côté, les méthodes « wrappers » utilisent un critère de performance pour la recherche d'un sous-ensemble de prédicteurs pertinents (Kohavi et John, 1997). Le plus souvent, c'est le taux d'erreur (mais cela peut être un coût de prédiction ou l'aire sous la courbe ROC). A titre d'exemple, la méthode WrapperSubsetEval commence avec un ensemble vide de variables et se poursuit jusqu'à ce que l'ajout de nouvelles variables n'améliore plus les performances, en exploitant la validation croisée pour estimer la précision de l'apprentissage pour un ensemble donné de variables (Witten et Frank, 2005). Les comparaisons entre méthodes, comme celle de Forman (Forman, 2003), mettent clairement en évidence que, sans tenir compte de leur efficacité, l'un des principaux inconvénients de ces deux catégories de méthodes est qu'elles sont très gourmandes en temps de calcul. Cela proscrit leur utilisation dans le cas de données fortement multidimensionnelles. Dans ce contexte, une alternative possible est alors d'exploiter les méthodes de filtrage. Les approches par filtrage sont des méthodes de sélection qui sont utilisées en amont et indé-pendamment de l'algorithme d'apprentissage. Basées sur des tests statistiques, elles sont plus légères en termes de temps de calcul que les autres approches. La méthode du chi-carré exploite un test statistique courant qui mesure l'écart à une distribution attendue en supposant que les variables sont indépendantes des étiquettes de classe (Ladha et Deepa, 2011). Le gain d'information est également l'une des méthodes les plus courantes de l'évaluation de variables. Ce filtre univarié fournit une classification ordonnée de toutes les variables. Dans cette approche, les variables retenues sont celles qui obtiennent une valeur positive du gain d'information (Hall et Smith, 1999). Dans la méthode MIFS (Mutual Information Feature Selection), une variable est ajoutée à un sous-ensemble de variables déjà sélectionnées si son lien avec la classe cible surpasse la connexion moyenne avec les prédicteurs déjà sélectionnés. La méthode prend en compte à la fois la pertinence et la redondance (Hall et Smith, 1999). La méthode CBF (Consistency-based Filter) évalue la pertinence d'un sous-ensemble de variables par le niveau de cohérence des classes lorsque les échantillons d'apprentissage sont projetés sur ce sous-ensemble (Dash et Liu, 2003). La méthode MODTREE est un procédé de filtrage qui repose sur le principe du calcul de la corrélation par paire. Elle fonctionne dans l'espace des paires d'individus décrits par des indicateurs de co-étiquetage attachés à chaque variable d'origine. Pour cela, un coefficient de corrélation par paire, qui représente la corrélation linéaire entre deux éléments, est utilisé. Le calcul des coefficients de corrélation partiels permet alors d'effectuer une sélection de variables pas à pas (Lallich et Rakotomalala, 2000). L'hypothèse de base de la méthode Relief, qui tire son inspiration du principe des plus proches voisins, est de considérer une variable pertinente si elle discrimine bien un objet dans la classe positive par rapport à son voisin le plus proche dans la classe négative. Le score des variables est cumulatif et calculé grâce à un tirage aléatoire de données-échantillons. ReliefF, une extension de Relief, ajoute la capacité de résoudre les problèmes multi-classes. Cette variante est aussi plus robuste et capable de traiter des données incomplètes et bruitées (Konokenko, 1995). ReliefF est considérée comme l'une des méthodes de sélection à base de filtres les plus efficaces. Comme tout test statistique, les approches par filtrage sont connues pour avoir un comportement erratique dans le cas de variables de très faibles fréquences ; ce qui représente une situation habituelle dans la classification de texte (Ladha et Deepa, 2011). Nous montrons éga-lement dans cet article que, malgré leur diversité, toutes les approches de filtrages existantes s'avérent inopérantes, voir néfastes, dans le cas de données très déséquilibrées, fortement multidimensionnelles et bruitées, avec un degré de similitude élevé entre classes. Nous proposons comme alternative une nouvelle méthode de sélection de variables et de contraste basée sur la métrique de maximisation d'étiquetage, récemment développée, et nous comparons ses performances avec des techniques classiques dans le contexte d'aide à la validation des brevets. Nous étendons ensuite la portée de notre étude à des données textuelles de référence habituellement utilisées. La suite du document est structurée comme suit. La section 2 présente notre nouvelle approche de sélection de variables. La section 3 détaille les données utilisées. La section 4 compare les résultats de la classification avec et sans l'utilisation de l'approche proposée sur les différents corpus de données. La section 5 présente nos conclusions et perspectives.
Maximisation d'étiquetage pour la sélection de variables
La maximisation d'étiquetage (F-max) est une métrique non biaisée d'estimation de la qualité d'une classification non supervisée qui exploite les propriétés des données associées à chaque cluster sans examen préalable des profils de clusters (Lamirel et al., 2004). Son principal avantage est d'être tout à fait indépendante des méthodes de classification et de leur mode opératoire. Lorsqu'elle est utilisée après l'apprentissage, elle peut être exploitée pour établir des indices globaux de qualité de clustering (Lamirel et al., 2010) ou pour l'étiquetage de clusters (Lamirel et Ta, 2008). Considérons un ensemble de clusters C résultant d'une méthode de clustering appliquée sur un ensemble de données D représentées par un ensemble de variables F . La métrique de maximisation d'étiquetage favorise les clusters avec une valeur maximale
, eux-mêmes définis comme suit :
où W f d représente le poids de la variable f pour la donnée d et F c représente l'ensemble des variables représentées dans les données associées au cluster c.
Tenant compte de la définition de base de la métrique de maximisation d'étiquetage, son exploitation pour la tâche de sélection de variables dans le contexte de l'apprentissage supervisé devient un processus simple, dès lors que cette métrique générique peut s'appliquer sur des données associées à une classe aussi bien qu'à celles qui sont associées à un cluster. Le processus de sélection peut donc être défini comme un processus non paramétré basé sur les classes dans lequel une variable de classe est caractérisée en utilisant à la fois sa capacité à discriminer une classe donnée (F P c (f ) index) et sa capacité à représenter fidèlement les données de la classe (F R c (f ) index). L'ensemble Sc des variables qui sont caractéristiques d'une classe donnée c, appartenant à un ensemble de classes C, se traduit par :
où C /f représente le sous-ensemble de C dans lequel la variable f est représentée. Enfin, l'ensemble de toutes les variables S C sélectionnées est le sous-ensemble de F défini comme : 
où k est un facteur d'amplification qui peut être optimisé en fonction de la précision obtenue.
Les variables actives d'une classe sont celles pour lesquelles le gain d'information est supérieur à 1 dans celles-ci. Etant donné que la méthode proposée est une méthode de sélection et de contraste basée sur les classes, le nombre moyen de variables actives par classe est donc comparable au nombre total de variables sélectionnées dans le cas des méthodes de sélection usuelles.
Données expérimentales
Un des buts poursuivi par le projet QUAERO est celui d'exploiter les informations bibliographiques pour aider des experts à juger de l'antériorité des brevets. Il s'agit donc, dans un premier temps, de prouver qu'il est possible d'associer ces informations de manière pertinente aux classes de brevets, autrement dit de les classifier correctement dans ces classes. Nos données source expérimentales principales contiennent 6387 brevets au format XML du domaine pharmacologique, regroupés en 15 sous-classes de la classe A61K (préparation mé-dicale). Les citations bibliographiques dans les brevets sont extraites de la base de données Medline 1 . 25887 citations ont été extraites à partir de ces 6387 brevets. L'interrogation de la base de données Medline avec les citations extraites permet de récupérer les notices bibliographiques de 7501 articles. Chaque notice est ensuite marquée par le premier code de classement du brevet citant (Hajlaoui et al., 2012). Le résumé de chaque notice est traité et transformé en un sac de mots (Salton, 1971) en utilisant l'outil TreeTagger (Schmid, 1994). Pour réduire le bruit généré par cet outil, un seuil de fréquence de 45 (soit le seuil moyen de 3/classe) est appliqué sur les descripteurs extraits. Il en résulte un espace de description seuillé de dimension 1804. Une dernière étape de pondération TF-IDF (Salton, 1971) est appliquée. La série de notices étiquetées et ainsi pré-traitées représente le corpus final sur lequel l'apprentissage est effectué. Ce dernier corpus est fortement déséquilibré, la plus petite classe contenant 22 articles (classe A61K41) et la plus grande en contenant 2500 (classe A61K31). La similarité inter-classes calculée en utilisant une corrélation cosinus indique que plus de 70% des couples de classes ont une similitude comprise entre 0,5 et 0,9. Ainsi, la capacité d'un modèle de classification à détecter précisément la bonne classe est fortement réduite. Une solution habituellement utilisée pour faire face à un déséquilibre dans des données des classes est un sous-échantillonnage des grosses classes (Good, 2006) et/ou un sur-échantillonnage des petites classes (Chawla et al., 2002). Toutefois, le ré-échantillonnage, qui introduit de la redondance dans les données, n'améliore pas les performances avec cet ensemble de données, comme cela a été montré par Hajlaoui et al. (2012). Nous proposons donc ci-après, une solution alternative qui est d'élaguer les variables jugées non pertinentes et de contraster celles jugées fiables.
A titre complémentaire, 3 ensembles de données textuelles de référence sont également utilisés dans nos expériences :
-Les corpus R8 et R52 sont des corpus obtenus par Cardoso Cachopo 2 à partir des ensembles de données R10 et R90 issus de la collection Reuters 21578 3 . Le but de ces adaptations est de ne retenir que les données ayant une seule étiquette. Considérant uniquement les documents monothématiques et les classes qui ont encore au moins un exemple d'apprentissage et un exemple de test, R8 est une réduction à 8 classes du corpus R10 (10 classes plus fréquentes) et R52 est une réduction à 52 classes du corpus R90 (90 classes). -Le corpus Amazon tm (AMZ) est un ensemble de données UCI (Bache et Lichman, 2013) dérivé des avis de clients du site web Amazon et exploitable pour l'identification des auteurs. Pour évaluer la robustesse des algorithmes de classification à un grand nombre de classes cibles, 50 des utilisateurs les plus actifs qui ont fréquemment postés des com-1. http://www.ncbi.nlm.nih.gov/pubmed/ 2. http://web.ist.utl.pt/~acardoso/datasets/ 3. http://www.research.att.com/~lewis/reuters21578.html mentaires dans ces newsgroups sont identifiés. Le nombre de messages collectés pour chacun d'entre eux est de 30. Chaque message comprend le style linguistique des auteurs tels que l'utilisation de chiffres, la ponctuation, les mots et les phrases fréquentes.
Expériences et résultats 4.1 Expériences
Pour effectuer nos expériences nous prenons d'abord en considération différents algorithmes de classification qui sont mis en oeuvre dans la boite à outils Weka 4 : arbres de dé-cision (J48) (Quinlan, 1993), forêts aléatoires (RF) (Breiman, 2001), k-plus-proches-voisins (KNN) (Aha et al., 1991), des algorithmes bayésiens usuels, à savoir, bayésien naïf multinomial (MNB) et réseau bayésien (BN), et enfin, l'algorithme SMO-SVM (SMO) (Platt, 1999). Les paramètres par défaut sont utilisés lors de l'exécution de ces algorithmes, à l'exception de KNN pour lequel le nombre de voisins est optimisé sur la base de la précision résultante. Nous mettons ensuite plus particulièrement l'accent sur les tests d'efficacité des approches de sélection de variables, y compris notre nouvelle proposition (FMC). Nous incluons dans notre test un panel d'approches de filtrage qui sont applicables avec des données de grande dimension, en exploitant une nouvelle fois la plateforme Weka. L'ensemble des méthodes testées comprend : chi-carré (CHI), gain d'information (GI), CBF, incertitude symétrique (IS) (Yu et Liu, 2003), ReliefF (RLF), Analyse en Composantes Principales (PCA) (Pearson, 1901). Les paramètres par défaut sont utilisés pour la plupart de ces méthodes, sauf pour PCA pour lequel le pourcentage de variance expliquée est accordé en fonction de la précision obtenue. Dans un premier temps nous expérimentons les méthodes séparément. Dans une deuxième phase, nous combinons la sélection des variables fournies par les différentes méthodes avec la méthode de contraste que nous avons proposée (eq. 6). Une validation croisée en 10 feuillets (10-fold cross-validation) est utilisée dans l'ensemble de nos expériences.
Résultats
Les différents résultats sont présentés dans les tableaux 1 à 8. Ils se basent sur les mesures de performance standard (taux de vrai positif (TP) ou Rappel (R), taux de faux positifs (FP), Précision (P), F-mesure (F) et ROC) pondérées par la taille des classes, puis moyennés sur toutes les classes. Pour chaque table et chaque combinaison de méthodes de sélection et de classification, un indicateur de gain/perte de performance (TP Incr) est calculé en utilisant le taux TP de SMO sur les données originales comme référence. Enfin, comme les résultats s'avèrent identiques pour chi-carré, gain d'information et incertitude symétrique, ils ne figurent qu'une seule fois dans les tableaux comme résultats de type chi-carré (et sont notés CHI+). Pour notre collection principale de brevets, le tableau 1 met en évidence que les performances de toutes les méthodes de classification sont faibles sur l'ensemble de données considéré si aucun processus de sélection de variables n'est exécuté. Il confirme également dans ce contexte la supériorité des méthodes SMO, KNN et bayésiennes sur les deux autres méthodes basées sur les arbres de décision. En outre, SMO fournit la meilleure performance globale en termes de discrimination comme le montre sa valeur de ROC la plus élevée. Toutefois, la méthode 
TAB. 2 -Résultats de classification après la sélection de variables (classifieur BN).
n'est clairement pas exploitable dans un contexte opérationnel d'évaluation de brevets, comme celui de QUAERO, en raison de la grande confusion entre les classes, mettant ainsi en évidence son incapacité intrinsèque à faire face à l'effet d'attraction des plus grandes classes. Chaque fois qu'une méthode usuelle de sélection de variables est appliquée en association avec les mé-thodes de classification les meilleures dans notre contexte, son exploitation altère légèrement la qualité des résultats, comme il est indiqué dans le tableau 2. Le tableau 2 souligne également que la réduction du nombre de variables par la méthode FMC est similaire à CHI+ (en termes de variables actives ; voir la section 2 pour plus de détails) mais que son exploitation stimule les performances des méthodes de classification, et en particulier celles des méthodes bayésiennes (tableau 3), conduisant à des résultats de classification impressionnants dans un contexte de classification très complexe : précision de 0,987%, soit 94 données mal classées parmi un total de 7252 avec la méthode BN. Les résultats présentés dans le tableau 4 illustrent plus précisé- TAB. 3 -Résultats de classification après la sélection de variables FMC. ment l'efficacité de la procédure de contraste F-max qui agit sur les descriptions des données (eq. 6). Dans les expériences relatives à ce tableau, le contraste est appliqué individuellement sur les variables extraites par chaque méthode de sélection et, dans une deuxième étape, un classifieur BN est appliqué sur les données résultantes contrastées. Les résultats montrent que, quel que soit le type de méthode de sélection de variable utilisé, les performances de classification qui en résultent sont renforcées chaque fois que le contraste F-max est appliqué en aval de la sélection. L'augmentation moyenne de performance est de 44%. Le tableau 5 illustre finalement les capacités de l'approche FMC à faire face efficacement aux problèmes de déséquilibre et de similitude des classes. L'examen des variations des taux TP (surtout dans les petites classes) dans ce dernier tableau montre que l'effet d'attraction de données des plus grandes classes, qui se produit à un niveau élevé dans le cas de l'exploitation des données originales, est pratiquement systématiquement surmonté chaque fois que l'approche FMC est exploitée. La capacité de l'approche à corriger un déséquilibre de classes est également clairement mise en évidence par la répartition homogène des variables actives dans les classes, ceci malgré des tailles très hétérogènes de classe. Le résumé des résultats sur les 3 ensembles de données 2.3Ghz et avec une mémoire de 8Go.) Sur ces ensembles de données, des remarques similaires à celles mentionnées pour l'ensemble des données-brevets peuvent être faites au sujet de la faible efficacité des méthodes usuelles de sélection de variables et des méthodes de ré-échantillonnage. Le tableau 8 montre également que la valeur du facteur d'amplification du contraste, qui est exploité pour obtenir les meilleures performances, peut varier au fil des expériences (de 1 à 4, dans ce dernier contexte). Cependant, l'on peut observer qu'en prenant une valeur fixe pour ce facteur, par exemple la plus élevée (ici 4), l'on ne dégrade pas les résultats. Ce choix représente donc une bonne alternative pour faire face au problème de paramétrage. TAB. 7 -Résultats de classifications après sélection de variables FMC (classifieur MNB/BN).
Les 5 variables (lemmes) les plus contrastées des 8 classes issues du corpus Reuter8 sont présentées dans le tableau 6. Le fait que les grandes lignes des thématiques couvertes par les classes puissent être clairement mises en évidence de cette manière illustre bien les capacités d'extraction de sujets de la méthode FMC. Enfin, l'obtention de très bonnes performances en combinant l'approche de sélection de variables FMC avec une méthode de classification comme MNB est un réel avantage pour l'exploitation à grande échelle, sachant que la méthode MNB a des capacités incrémentales et que les deux méthodes ont des temps de calcul faibles. 
Conclusion
Notre objectif principal était de développer une méthode efficace de sélection et de contraste de variables qui pourrait permettre de surmonter les problèmes habituels liés à la classification supervisée de gros volumes de données textuelles. Ces problèmes sont liés à des classes déséquilibrées avec un degré élevé de similitude entre elles, hébergeant des données fortement multidimensionnelles et bruitées. Pour ce faire, nous avons proposé d'adapter une métrique développée récemment dans le cadre non supervisé au contexte de la classification supervisée. Grâce à diverses expériences sur de grands ensembles de données textuelles, nous avons illustré de nombreux avantages de notre approche, et surtout sa grande efficacité pour améliorer les performances des classifieurs dans un tel contexte, tout en mettant l'accent sur les classifieurs les plus flexibles et les moins gourmands en temps de calcul, comme les classifieurs bayésiens. Un autre avantage de cette méthode est qu'il s'agit d'une approche sans paramètre qui s'appuie sur un schéma simple d'extraction de variables ; elle peut donc être utilisée dans de nombreux contextes, comme dans ceux de l'apprentissage incrémental ou semi-supervisé, ou encore, dans celui de l'apprentissage numérique en général. Une autre perspective intéres-sante serait d'adapter cette technique au domaine de l'exploration de textes afin d'enrichir des ontologies et des lexiques grâce à l'exploitation à grande échelle des corpus existants.
Remerciements Ce travail a été réalisé dans le cadre du programme QUAERO 5 soutenu par OSEO 6 , Agence française de développement de la recherche.
5. http://www.quaero.org 6. http://www.oseo.fr/

Introduction
Les méthodes automatiques de recherche d'images fournissent un moyen d'aide à la dé-cision dans de nombreux domaines d'application. Dans le domaine médical, elles permettent d'assister les radiologues lors de leur travail d'interprétation d'images en identifiant des images similaires au sein de bases de données. Un cas typique d'utilisation est la recherche par l'exemple où l'on souhaite retrouver des images similaires à un exemple d'image donné en requête correspondant à un examen médical. Pour ce faire les images sont généralement décrites par leurs caractéristiques bas-niveaux (e.g., couleur, texture) induites de leurs pixels et une mesure de distance est utilisée pour rechercher des images similaires dans l'espace des caractéristiques. Cependant, face à la complexité des nouvelles générations d'images médicales, les processus de recherche d'images basés sur le contenu peuvent s'avérer insuffisants. Une des limites principales est liée au problème du saut sémantique : les caractéristiques bas-niveaux ne sont plus suffisamment discriminantes pour caractériser le contenu visuel haut-niveau des images.
Pour proposer une solution à ces problèmes, des travaux récents ont montré l'intérêt de caractériser le contenu des images par des termes sémantiques (Kwitt et al., 2012). Ces termes peuvent être utilisés pour décrire un nombre important d'informations relatives au contenu visuel des images ( Fig. 1(a-b)). Ils peuvent être dérivés des observations des radiologues ou automatiquement prédits à partir de caractéristiques bas-niveaux extraits des pixels. Par consé-quent, l'intégration de termes sémantiques dans les processus de recherche d'images est une solution prometteuse pour faire face aux problèmes liés au saut sémantique. Cependant, les systèmes actuels de recherche d'images basés sur les annotations sémantiques ne considèrent pas les relations intrinsèques (e.g., sémantiques, anatomiques) qui existent entre ces termes lors de la comparaison des images.  (Tousch et al., 2012) ont montré que l'utilisation de vocabulaires contrôlés (comme les ontologies) pour l'annotation d'images pouvait fournir des solutions efficaces pour faire face à ces limites. Les ontologies peuvent être employées pour modéliser les relations sémantiques entre les termes utilisés pour la description des images. De plus, des travaux en traitement automatique des langues naturelles (Pivovarov et Elhadad, 2012) ont déjà proposés des approches pour mesurer la proximité sémantique entre des termes issus d'une ontologie ( Fig. 1(c)). Par ailleurs, dans le domaine de la comparaison de vecteurs, des distances ont été proposées pour considérer la proximité relative entre les éléments composant les vecteurs, permettant de prendre en compte leurs corrélations (Turney et Pantel, 2010). En couplant ces trois stratégies, il existe ainsi une opportunité de considérer les relations sémantiques entre les termes lors de la comparaison d'images et d'améliorer les processus de recherche d'images.
Cet article propose une nouvelle approche basée sur la proximité sémantique du contenu des images, dédiée à la recherche d'images similaires au sein de bases de données (Sec. 2). Elle combine une distance hiérarchique permettant de prendre en compte les relations entre les éléments d'un vecteur (Kurtz, 2012) et une mesure de similarité ontologique permettant d'évaluer automatiquement la proximité sémantique entre des termes extraits d'une ontologie. Cette stratégie offre un moyen de capturer les corrélations sémantiques entre les annotations décrivant les images lors de leur comparaison. Cette approche est générique et peut ainsi être utilisée pour un large champ d'applications allant de la recherche d'images par l'exemple à la classification. Les résultats obtenus dans le domaine de la recherche d'images tomodensitomé-triques du foie se sont montrés encourageants et justifient l'intérêt de cette approche (Sec. 3).
2 Approche sémantique proposée L'approche proposée est composée de deux étapes (  
Annotation des images
Soit I A une nouvelle image requête. La première étape consiste à décrire le contenu visuel de l'image (ou d'une région d'intérêt spécifique de l'image) par le biais d'un ensemble de termes sémantiques extraits d'une ontologie ?. Cette étape peut être réalisée de façon automatique (par un algorithme d'apprentissage prédisant les termes à partir de caractéristiques extraites de la région d'intérêt) ou de façon manuelle. Dans le cadre de ces travaux, chaque image a été annotée par un radiologue via des termes de ?.
Pour rendre les descriptions d'images comparables, l'approche proposée nécessite la créa-tion d'un sous-vocabulaire de termes extraits de ?. Ce sous-vocabulaire, spécifique à l'application considérée, est défini en accord avec les radiologues. Il est noté X = {x 0 , x 1 , . . . , x k?1 } où les k termes x i ? ? sont des termes potentiellement utilisables pour décrire le contenu des images. Une fois l'image I A annotée, il devient possible de la caractériser via un vecteur de termes sémantiques A = 0 , a 1 , . . . , a k?1 où chaque élément a i ? A est une valeur binaire représentant la présence ou l'absence du terme x i ? X .
Comparaison d'images
Les vecteurs de termes caractérisant les images peuvent ensuite être comparés via le calcul d'une distance hiérarchique nommée HSBD (Hierarchical Semantic-Based Distance) (Kurtz, 2012). Lors de son calcul, HSBD permet de considérer des valeurs de proximité entre les éléments des vecteurs comparés. Ces valeurs de proximité sémantique sont préalablement calculées par l'intermédiaire d'une mesure de similarité ontologique.
Similarité entre termes sémantiques
On trouve dans la littérature différents types de mesures permettant d'évaluer la similarité entre des termes appartenant à une ontologie. Parmi ces mesures, les mesures structurelles permettent de quantifier la similarité entre termes en se basant sur la structure de l'ontologie. Notre contexte applicatif nécessitant la proposition d'outils automatiques, notre étude s'est ainsi penchée sur l'utilisation de telles mesures.
Considérons deux termes sémantiques x i , x j ? X . Un ensemble de liens connectant x i et x j dans l'ontologie est défini par path(x i , x j ) = {l 0 , . . . , l n?1 }. Pour quantifier une va-leur de similarité sémantique entre deux termes x i et x j , une méthode intuitive a été proposée dans (Al-Mubaid et Nguyen, 2006). Elle est basée sur une stratégie par branches qui évalue la longueur du chemin minimum entre les termes et leur profondeur taxonomique au sein des branches considérées : plus le chemin entre les termes est long, plus les termes sont différents sémantiquement. L'idée est d'évaluer la spécificité commune (SC) des deux termes en soustrayant la profondeur de leur premier ancêtre commun (PAC) à la profondeur D c de la branche principale auquel ces termes appartiennent. La spécificité commune est utilisée pour prendre en compte le fait que les couples de termes se trouvant dans les niveaux les plus bas de la hié-rarchie sont plus similaires que les couples situés à un niveau plus élevé. Nous avons modifié la définition originale de cette mesure pour la normaliser et pour attribuer un poids égal à la longueur du chemin entre les termes et à leur spécificité commune :
norm est un facteur de normalisation évaluant la valeur de similarité maximale entre termes et SC(x i , x j ) = D c ? depth(P AC(x i , x j )) représente la spécificité commune des termes.
Pour modéliser l'ensemble des valeurs de similarité sémantique entre les k termes x i appartenant au vocabulaire X , nous définissons une matrice de dissimilarité notée M sem (de taille k × k) où la valeur de chaque case M sem i,j est obtenue via la mesure s ? (x i , x j ), ?x i , x j ? X .
Distance entre vecteurs de termes
Le calcul de HSBD, entre deux vecteurs A et B, requière une matrice de dissimilarité modélisant les corrélations entre les éléments des vecteurs composant A et B. Pour ce faire, HSBD est initialisée avec la matrice de similarité sémantique M sem définie précédemment. Avant de pouvoir calculer HSBD, la stratégie adoptée (basée sur un modèle fin-à-grossier) nécessite de définir un moyen de fusionner hiérarchiquement les différents termes représentés par les vecteurs en « clusters » de termes (i.e., des termes de niveaux sémantiques plus éle-vés). Cette étape de pré-traitement repose sur la construction d'un dendrogramme D induit par M sem et modélisant la hiérarchie de fusion des termes. La construction de D est réalisée par le biais d'un algorithme de clustering hiérarchique ascendant. Il est à noter que cette étape de pré-traitement ne doit être effectuée qu'une seule fois pour une matrice M sem donnée. Une fois que le dendrogramme D a été construit, la distance HSBD peut être calculée. Son calcul se décompose en deux étapes principales :
-Étape 1. Calcul des sous-distances hiérarchiques Durant un processus de fusion ité-ratif scannant chaque étage du dendrogramme (de ses feuilles jusqu'à sa racine), les vecteurs liés à A et B, et induits par la fusion des termes composant chaque cluster de l'étage courant, sont construits. Après chaque itération, la distance de Manhattan D L1 est calculée entre chaque couple de vecteurs créé. Ces « sous-distances » permettent d'évaluer la similarité entre A et B à différents niveaux de sémantiques. -Étape 2. Fusion des sous-distances Les sous-distances hiérarchiques calculées pour tous les étages du dendrogramme, et l'énergie sémantique nécessaire pour aller d'un étage à l'autre, sont ensuite fusionnées en une fonction qui est finalement intégrée pour fournir la valeur de la distance HSBD. Pour plus de détails sur le calcul de HSBD, les lecteurs peuvent se référer à (Kurtz et al., 2013). 
Validation expérimentale
L'approche proposée a été validée dans le contexte de la recherche d'images similaires au sein d'une base de données de N = 77 images tomodensitométriques. Chaque image repré-sente une coupe de foie affectée d'une lésion (e.g., tumeur, kyste). Chaque lésion a été annotée (12 termes en moyenne) par un radiologue par l'intermédiaire de termes appartenant à un sousvocabulaire de 72 termes extraits de l'ontologie médicale RadLex (Langlotz, 2006).
Les expériences ont consisté à considérer successivement chaque image de la base comme une requête afin de rechercher des images similaires via la distance HSBD combinée à la mesure ontologique. Pour ce faire, l'image requête a été comparée à l'ensemble des N ? 1 images qui ont ensuite été classées par ordre de similarité (Fig. 3(a)). Les valeurs de similarité ainsi calculées ont ensuite été comparées via l'indice NDCG (Normalized Discounted Cumulative Gain) à des valeurs de similarité de référence définies par un groupe de radiologues pour 30 × 30 couples d'images. L'indice NDCG permet d'estimer la pertinence d'un résultat d'expé-rience de classement par rapport à un classement de référence. Pour chaque image requête, la moyenne de cet indice a été calculée pour chaque K = 1, . . . , 30, évaluant ainsi la qualité du classement pour différents nombres d'images recherchées. Les résultats obtenus par HSBD ont également été comparés à ceux obtenus par d'autres distances de l'état de l'art : la distance Euclidienne D L2 , l'intersection de vecteurs D ? et la distance EMD (Rubner et al., 2000).
La figure 3(b) présente les scores NDCG obtenus pour les 4 distances évaluées. À partir de ces résultats, on observe que les scores NDCG sont toujours supérieurs quand la comparaison des images est effectuée avec la distance HSBD que quand elle est effectuée avec les 3 autres distances. En particulier, l'approche proposée conduit à de meilleurs résultats que ceux obtenus avec la distance EMD qui permet également de prendre en compte les relations sémantiques entre les termes (via une initialisation avec la mesure ontologique présentée). Par ailleurs, les moins bons scores ont été obtenus avec les distances D L2 et D ? qui ignorent durant leur calcul les relations sémantiques entre les éléments des vecteurs.

Introduction
L'expression en langage naturel recèle des informations riches que les analystes souhaitent souvent explorer. Dans le cadre de l'activité de la Société Succeed Together qui consiste, entre autres, à recueillir et analyser des informations produites lors de séminaires interactifs, les animateurs développent et structurent les discussions établies avec les participants. Les réponses ou remarques apportées par les participants peuvent alors être consignées puis traitées, une phase de regroupent est au préalable nécessaire. Le but est ainsi de mettre en exergue des sentiments partagés par les participants selon une thématique donnée. Dans ce cadre, les travaux menés par le LIRMM (Laboratoire d'Informatique, de Robotique et de Microélectronique de Montpellier) liés au traitement automatique des données textuelles, permettent aux experts de Succeed Together d'analyser semi-automatiquement et à plus grande échelle les données. Ainsi, nous avons focalisé notre étude sur la représentation des données textuelles par des méthodes de TAL (Traitement Automatique du Langage Naturel). Ceci permet, en particulier, d'améliorer les méthodes de classification et/ou regroupement effectuées par le deuxième collaborateur académique du projet (I3S / Université de Nice).
Dans un premier temps, en section 2, nous décrivons les méthodes de représentation des descripteurs textuels. Une application spécifiquement dédiée au projet a été développée. Cette application est décrite en section 3. Les résultats issus des données fournies par la société sont décrits et analysés en section 4. Enfin, quelques perspectives sont données en sections 5.
Descripteurs textuels pour les tâches de Clustering
La sélection de descripteurs pertinents à partir de textes est une étape indispensable pour une tâche de clustering (regroupement) qui consiste à regrouper les documents ayant des contenus sémantiques proches. Pour appliquer les algorithmes de regroupement, il est dans un premier temps nécessaire d'établir une représentation pertinente des documents (Béchet (2009)). Dans cet article, nous nous concentrons sur la représentation vectorielle de Salton et al. (1975).
Nous nous appuierons sur le principe sac de mots appliqué aux textes des séminaires de la société Succeed Together. Plusieurs types de représentations sont alors possibles :
-Représentation booléenne : Le vecteur booléen donne des informations liées à la pré-sence ou l'absence d'un descripteur dans un document. -Représentation fréquentielle : La représentation fréquentielle de base revient à considérer le nombre d'occurrences d'un terme i dans un document j, la normalisation par rapport aux nombre de mots dans un document peut être aussi appliquée. -Pondération TF-IDF : La mesure TF-IDF consiste à calculer l'importance et la discriminance d'un mot dans un document relativement à une collection (Salton et al. (1975)).
Logiciel de Vectorisation
Dans le cadre du projet, nous avons développé un logiciel dédié à l'extraction des descripteurs utiles pour l'étape de clustering afin de représenter les textes sous forme d'une matrice. Le format ARFF choisi pour cette représentation est typique des logiciels de clustering tel que Weka (Nom-Préposition-Nom, NomNom, Nom-Adjectif, Adjectif-Nom, etc). Les syntagmes extraits sur la base de patrons morpho-syntaxiques se révèlent en général plus pertinents. -Traitement statistique. Le logiciel permet de ne conserver que les n-grammes présents au moins N occ fois, ce seuil étant déterminé par l'utilisateur. Précisons qu'un élagage trop strict peut engendrer un résultat nul si la taille du corpus est insuffisante. -Génération de différentes sorties. Le logiciel développé propose différents types de sorties (cf. Figure 1).
Expérimentations
Dans cette section, nous discutons les résultats obtenus en utilisant différents paramètres du logiciel développé dans le cadre du projet.
Filtres de n-grammes. La Figure 2 indique le nombre de n-grammes moyen extrait en fonction de n et en fixant le nombre d'occurrences N occ = 3. Les résultats indiquent que 1. http ://www.cs.waikato.ac.nz/ml/weka/ 2. Sygfran : http ://www.lirmm.fr/?chauche/ExempleAnl.html pour le corpus donné, de nombreux n-grammes sont extraits pour n = 1. Avec un nombre n supérieur ou égal à 2, le nombre de n-grammes extraits est significativement plus faible. Cette même constatation a été relevée quel que soit N occ .
Filtres morpho-syntaxiques. Sans filtrage syntaxique (FS), de nombreux n-grammes retournés se révèlent non pertinents. Par exemple, les 2-grammes des collaborateurs ou notre présence. À partir d'un des fichiers caractéristiques fourni par la société, 437 n-grammes sont extraits (avec N occ = 2). En appliquant les filtres syntaxiques, seulement 5% des 2-grammes sont conservés par rapport à l'extraction sans FS. Par exemple, nous retenons les syntagmes libre service, nouvelles technologies, service client, satisfaction client, contact humain, relation client. De tels termes se révèlent sémantiquement plus pertinents.
Filtres fréquentiels. La Figure 2 indique le nombre moyen de 1-grammes extraits en fonction du seuil N occ . Par exemple, 96 n-grammes sont extraits avec un seuil égal à 2. Il est notable qu'à partir de N occ = 6, très peu de 1-grammes sont extraits.
Par ailleurs, nous avons comparé un classement de type fréquentiel avec un classement sur la base d'une pondération TF-IDF. Nous avons calculé le recouvrement moyen des termes selon ces classements. Par exemple, avec N occ = 2, environ 44% des 1-grammes sont communs aux deux mesures (en tenant compte des 10 premiers termes de plus haut score).
Conclusion et perspectives
Dans le cadre de ce projet, nous nous sommes concentrés sur l'extraction des descripteurs linguistiques à partir des données textuelles fournies par la société Succeed Together. Nous avons évalué l'utilisation de différentes méthodes de Traitement Automatique du Langage à  Salton, G., A. Wong, et C. S. Yang (1975). A vector space model for automatic indexing. Commun. ACM 18(11), 613-620.
Summary
Automatic processing of textual data enables users to analyze semi-automatically and on a large scale the data. This analysis is based on two successive processes: (i) representation of texts, (ii) gathering of textual data (clustering). The software described in this paper focuses on the first step of the process by offering expert a parameterized representation of textual data.

Introduction
Utilisée à l'origine comme un outil d'aide à la décision, les arbres de décision sont très populaires en fouille et en analyse visuelle des données. Ce succès s'explique notamment par le fait qu'ils utilisent un formalisme transparent et simple à comprendre (Murthy, 1998).
En pratique, la génération automatique d'arbres de décision depuis des données est possible grâce à l'induction d'arbre de décision, une technique bien connue (Quinlan, 1986). Malheureusement, les arbres de décision générés depuis des données issues du monde réel peuvent être très grands et difficiles à exploiter. De nombreuses approches de simplification ont donc été proposées. La plus connue, l'élagage (pruning) (Breslow et Aha, 1997), consiste à supprimer les parties de l'arbre qui ont un faible pouvoir explicatif. D'autre part, il existe des approches qui travaillent directement sur les données, en utilisant des méthodes de preprocessing (Parisot et al., 2013a). Enfin, la classification non supervisée (ou clustering) est un outil particulièrement utile pour la fouille de données, et à priori, cette technique peut donc être également exploitée pour obtenir des arbres de décision plus simples : en conséquence, une étude récente a proposé une nouvelle approche de classification non supervisée pour obtenir des arbres de décisions plus simples (Parisot et al., 2013b).
Dans cet article, nous présentons une méthode permettant d'adpater une classification non supervisée existante afin de simplifier l'arbre de décision obtenu à partir de chaque cluster.
Contribution
La méthode proposée (Figure 1) a pour but d'adapter une classification non supervisée pouvant être obtenue au moyen de toute technique existante non hiérarchique (k-means, EM, etc.) (Gan et al., 2007). Plus précisément, l'adaptation d'une classification non supervisée composée de clusters C 1 , . . . , C n revient à effectuer une succession de déplacements d'éléments entre clusters, de C i vers C j (i = j), sans création ni suppression de clusters. De manière à s'assurer durant l'adaptation que la classification obtenue n'est pas trop éloignée de la classification initiale, l'indice de Jaccard (Jaccard, 1908) est utilisé pour comparer leur similarité (classifications similaires si indice proche de 1, différentes si indice proche de 0).
La phase d'adaptation se matérialise sous la forme d'un algorithme (Algorithme 11) dont les paramètres d'entrée sont les suivants : un jeu de données comprenant un attribut classe, une classification non supervisée initiale, ainsi qu'un indice Jaccard minimum sous lequel il ne faut pas descendre. Le résultat obtenu est une classification non supervisée adaptée dans laquelle chaque cluster sert à produire un arbre de décision simplifié.
Algorithm 1 Algorithme d'adaptation de clusters
nbP asses ? 0, j ? 1 while j < jaccardIndexLimit ? nbP asses < nbP assesLimit do for all item à deplacer do cherche un cluster C' cible pour item if C' existe then déplace item dans C' j ?calcul indice de Jaccard end if end for nbP asses ? nbP asses + 1 end while Le principe de l'algorithme est le suivant : des déplacements entre clusters sont effectués tant que l'indice Jaccard minimum spécifié n'est pas atteint. De plus, il se peut qu'au bout d'un certain nombre d'itérations, l'algorithme ne trouve plus de déplacement possible à effectuer, et par conséquent l'indice Jaccard minimum spécifié ne peut jamais être atteint : un nombre de passes maximum est donc également prévu.
À priori, les éléments susceptibles d'être déplacés d'un cluster à un autre sont à chercher parmi ceux qui rendent les arbres de décisions complexes, c'est-à-dire : les éléments mal classifiés/expliqués par l'arbre de décision, et les éléments classifiés par des branches ayant un faible pouvoir de classification. À posteriori, il faut ensuite s'assurer que le fait de retirer chacun de ces éléments du cluster d'origine a un effet réellement positif sur la taille de l'arbre de décision généré depuis le cluster d'origine. Nous calculons donc l'arbre de décision DT avant retrait, puis l'arbre de décision DT' après retrait, et nous comparons les tailles de DT et de DT' : si la taille de DT' est inférieure à celle de DT, alors l'élément peut être retiré.
Une fois qu'un élément a été sélectionné pour être déplacé dans un autre cluster, nous proposons de choisir le cluster pour lequel l'arbre de décision est le mieux impacté par l'ajout de l'élément. En d'autres termes, nous calculons pour chaque cluster l'arbre de décision DT avant ajout, puis l'arbre de décision DT' après ajout, et nous comparons la taille de DT et DT'. Si pour tout cluster, la taille de DT' est toujours supérieure à la taille de DT, alors il n'y pas de cluster cible : il n'y a donc pas de déplacement. Sinon, le cluster cible est le cluster pour lequel le ratio entre taille de DT' et taille de DT est le meilleur : le déplacement est donc effectué.
Expériences
Un prototype a été réalisé en Java, en utilisant des fonctionnalités offertes par Weka (Hall et al., 2009). De plus, des tests ont été réalisés sur une sélection de données UCI (Bache et Lichman, 2013) : nous avons comparé les arbres obtenus dans les situations suivantes : -CAS1 : Génération de l'arbre de décision sur le jeu de données complet.
-CAS2 : Classification non supervisée du jeu de données avec l'algorithme k-means (pour différentes valeurs de k), et génération de l'arbre de décision pour chaque cluster. -CAS3 : Application de notre méthode d'adaptation sur les clusters obtenus précédem-ment, et génération de l'arbre de décision pour chacun des clusters. La génération des arbres de décision a été effectuée en utilisant l'algorithme C4.5, via l'implémentation J48 : l'algorithme a été configuré en désactivant la fonction d'élagage, de manière à obtenir des arbres de décision initiaux très complexes.  Les résultats obtenus, notamment avec des classifications composées de 3 clusters, (Table  1), montrent que notre méthode permet de simplifier les arbres de décision de chaque cluster. Sur l'ensemble des jeux de données testés, les gains les plus spectaculaires concernent les jeux de données cmc, vehicle, credit-a, adult : en effet la taille moyenne des arbres de décision peut être réduite jusqu'à 50%. Un bémol toutefois : dans certains cas comme mushroom, k-means produit des clusters ayant des arbres de décision déjà bien simplifiés : notre méthode ne permet pas de les simplifier davantage.
Conclusion et perspectives
Dans cet article, nous avons présenté une méthode permettant d'utiliser le résultat d'une classification non supervisée pour simplifier efficacement les arbres de décision de chacun des clusters. La méthode procède par déplacements d'éléments entre clusters en réduisant au fur et à mesure la taille des arbres de décision de chacun des clusters. Les futurs travaux concerneront la recherche d'une solution similaire pour les flux de données.

Introduction
Dans le cas de la classification automatique comme pour la détection de communautés, après avoir construit une partition à l'aide d'une méthode, il convient d'évaluer sa qualité. Pour ce faire, on peut faire appel à des critères externes ou internes. Les premiers permettent de comparer le résultat obtenu avec un résultat attendu, par exemple une partition faisant office de "vérité terrain" alors que les seconds quantifient la qualité de la partition proposée. Parmi les critères externes, utilisables aussi bien en classification automatique que pour la détection de communautés, on peut citer le taux de bien classés, la pureté, l'indice de Rand ou sa version ajustée, l'entropie ou encore l'information mutuelle, éventuellement normalisée ou ajustée, mais aussi la V-mesure, l'homogénéité ou la complétude (Hubert et Arabie (1985); Vinh et al. (2010); Rosenberg et Hirschberg (2007)).
En classification automatique, les critères internes peuvent eux-mêmes être subdivisés en critères dont l'usage est spécifique à une distance ou à une méthode, comme par exemple l'iner-tie intra ou interclasses, et en critères non spécifiques, comme les indices de Dunn, de Davies et Bouldin ou de Silhouette (Rousseeuw (1987); Dunn (1973); Davies et Bouldin (1979)) Dans le domaine de la détection de communautés, on pourra citer la couverture, la conductance, la performance ou encore le coefficient de clustering mais le critère le plus couramment employé est la modularité (Yang et Leskovec (2012)).
Introduite par Newman pour juger de la qualité d'un partitionnement des sommets d'un graphe, la modularité a ensuite été utilisée directement pour identifier des classes telles que les sommets à l'intérieur d'une classe soient fortement reliés et qu'ils aient peu de relations avec ceux des autres classes (Newman (2006)). Bien que des travaux récents aient souligné des problèmes liés à l'optimisation de ce critère pour déterminer un partitionnement optimal notamment, la limite de résolution rendant difficile la détection de classes de faible taille ou dans des graphes creux ou encore l'existence de partitions à forte modularité dans des graphes sans structure communautaire, c'est un critère qui a néanmoins produit de bons résultats dans la pratique, notamment pour la détection de communautés dans un réseau social (Guimera et al. (2004); Good et al. (2010); Lancichinetti et Fortunato (2011)). De plus, elle présente des propriétés intéressantes. Premièrement, elle est calculable sur des graphes valués ou non valués, et ne nécessite pas de normalisation préalable. Ensuite, elle repose sur des concepts intelligibles, où on cherche à former des classes entre sommets mieux reliés entre eux que dans une formation aléatoire. Enfin, contrairement à d'autres critères, la modularité permet de comparer des partitions n'ayant pas nécessairement le même nombre de classes. Cependant, la modularité ne peut pas être utilisée pour évaluer la qualité d'un partitionnement dans un contexte de classification automatique et, à notre connaissance, il n'existe pas de critère ayant les propriétés de la modularité de Newman qui soit adapté à des éléments décrits par des attributs vectoriels. C'est la raison pour laquelle, dans cet article, nous définissons un critère de mesure de la qualité d'une partition d'éléments représentés par des vecteurs, inspirée de la modularité et qui pourra être utilisée pour comparer deux partitions. Ce critère sera décrit dans la section suivante. La section 3 est consacrée à l'adaptation de ce nouveau critère à l'heuristique de la méthode de Louvain. Nous proposons une nouvelle méthode de détection de communautés dans un réseau d'information appelée 2Mod-Louvain, basée sur l'optimisation en parallèle de la modularité de Newman et de la modularité que nous introduisons. Enfin, dans la section 4, à travers des expérimentations, nous évaluons les performances de cette méthode et sa robustesse à des dégradations des données. . Chaque élément v ? V est décrit par un vecteur d'attributs qui, par souci de simplification des notations, est aussi noté v :
On suppose de plus qu'une masse m v égale à 1 est associée à chaque élément v de V . La somme de ces masses est égale à N , le nombre d'éléments de V .
I(V ) désigne l'inertie de V par rapport à son centre de gravité g ou simplement comme inertie interne de V ou moment centré d'ordre 2 et défini de la façon suivante :
de V par rapport à un élément v est la somme des carrés des distances entre les éléments de V et v.
Étant donnée une partition P = {C 1 , . . . , C r } en r classes disjointes de V , Q inertie (P), le critère de mesure de la qualité de la partition P, que nous introduisons, est défini par :
où c(v) est la classe de l'élément v et ? est la fonction de Kronecker qui vaut 1 si ses arguments sont égaux et 0 sinon. Ainsi, alors que la modularité considère la force du lien et vise à regrouper les éléments les plus fortement liés, notre critère exploite la norme entre les éléments et vise à regrouper ceux qui sont les moins dissemblables ; ce qui apparait, après normalisation, dans le second terme de l'équation :
Le critère Q inertie compare la norme de chaque paire d'éléments (v, v ) issus d'une même communauté (contrôlé par la fonction de Kronecker), avec une valeur attendue d 2 (v, v ) définie par :
Il s'agit donc de comparer une fonction du carré de la distance entre v et v , à une fonction du carré des distances de chacun de ces éléments v et v aux autres éléments de V . Si la valeur attendue est plus grande que la valeur réelle, alors les deux éléments sont de bons candidats à appartenir à une même classe. C'est la raison pour laquelle, dans le critère global Q inertie (P), la valeur observée est soustraite de la valeur attendue alors que dans le cas de la modularité c'est la valeur attendue qui est retranchée de la force du lien observé.
Le critère Q inertie (P) que nous proposons varie entre -1 et 1. En effet, la partie gauche de la soustraction (6), comprenant les produits d'inerties pour toutes les paires de sommets, vaudra au plus 1. De même, la partie droite du critère Q inertie (P) (équation 5) ne pourra pas dépasser 1. Les deux parties étant strictement positives, le critère, contraint par les valeurs de la fonction de Kronecker, varie entre -1 et 1.
Ce critère présente plusieurs propriétés intéressantes. Premièrement, il conserve la même valeur, quelle que soit la transformation affine que l'on applique aux attributs, autrement dit l'ajout d'une constante et/ou la multiplication par un scalaire des vecteurs associés aux élé-ments à classer n'a pas d'incidence sur la valeur du critère. Enfin l'ordre des attributs n'a aucune incidence sur le résultat.
En revanche, ce critère présente aussi certaines limites. Il est indéfini si les vecteurs numé-riques sont identiques, car l'inertie totale est alors nulle. Ceci n'est pas réellement un inconvénient lors de la détection de communautés dans un réseau d'information car dans ce cas, les attributs n'apportant aucune information, la détection des communautés sera basée uniquement sur les données relationnelles. De plus, comme la modularité de  
Méthode 2Mod-Louvain
Comme nous l'avons indiqué en introduction, une des applications immédiates du critère Q inertie est la détection de communautés dans un réseau d'information représenté par un graphe avec attributs G = (V, E) où V est l'ensemble des sommets, E ? V ×V est l'ensemble des arêtes et où chaque sommet v ? V est associé à un vecteur v = (v 1 , . . . , v j , . . . , v T ) à valeurs réelles (Zhou et al. (2009)).
Dans cette section, en tirant parti du critère de modularité basée sur l'inertie Q inertie , nous proposons une méthode, appelée 2Mod-Louvain, dédiée à la détection de communautés dans ce type de réseaux. Cette méthode basée sur le principe d'exploration de la méthode de Louvain, exploite conjointement le critère Q inertie et la modularité de Newman Q N G (P) puisqu'elle consiste à optimiser le critère global QQ + (P) défini par :
avec :
où (v, v ) prend toutes les valeurs de V × V , k v est le degré du sommet v, A désigne la matrice d'adjacence associée à G, m est la somme des poids de toutes les arêtes du graphe et ? est la fonction de Kronecker. Il n'est pas utile de normaliser les deux critères Q N G (P) et Q inertie (P) car leurs bornes sont identiques comme indiqué dans la section précédente.
La méthode 2Mod-Louvain est détaillée dans l'algorithme 1, qui comporte deux étapes. La première est une phase itérative qui vise à déplacer un sommet de sa classe vers celle d'un de ses voisins dans le graphe si ce changement induit un gain de la modularité globale QQ + (P). La seconde est une phase de fusion qui consiste à construire un nouveau graphe dont les sommets correspondent aux communautés obtenues à l'issue de la phase précédente. Cette seconde phase fait intervenir deux procédures Fusion_Matrice_Adjacence et Fusion_Matrice_Inertie. La procédure Fusion_Matrice_Adjacence est identique à celle mise en oeuvre dans la méthode de Louvain (Aynaud et al. (2010)). La procédure Fusion_Matrice_Inertie est décrite dans la section suivante. Placer u dans la communauté B;
12
Mettre à jour la partition P suite au transfert de u dans B;  
Par la suite, A \ {u} désigne la classe A privée du sommet u et B ? {u} la classe B augmentée du sommet u. Dans un souci de simplification des notations dans la suite nous notons le terme D [v, v ] de la matrice abusivement D vv . La modularité basée sur l'inertie de la partition P vaut :
La modularité de la partition P vaut quant à elle :
Le gain de modularité lors du passage de P à P a donc pour valeur :
De plus, on peut remarquer que la variation de modularité induite par la suppression de u de sa classe d'origine sera la même quelle que soit sa classe d'affectation. Par conséquent le calcul de variation de modularité peut être effectué en considérant uniquement la différence induite par l'insertion de u dans sa nouvelle communauté d'affectation, décrite par le premier terme de l'équation 20.
Ces calculs nous permettent de montrer que notre critère bénéficie lui aussi de la possibilité d'être calculé de façon incrémentale. Le gain de modularité basée sur l'inertie repose uniquement sur des informations locales relatives au sommet déplacé et à sa distance avec les autres sommets.
Évaluation de la méthode 2Mod-Louvain sur des réseaux artificiels
Dans cette section, nous nous proposons d'évaluer la méthode 2Mod-Louvain qui optimise le critère global QQ + basé à la fois sur la modularité de Newman et la modularité par rapport à l'inertie. Pour ce faire, nous étudions sa robustesse sur des réseaux artificiels vis-à-vis d'une dégradation de la structure de communautés définie par rapport aux relations, ou des classes définies par rapport aux attributs, ou encore d'une augmentation de la taille du réseau d'information ou d'une variation de la densité des liens.
On construit un réseau de référence R comportant 168 liens et 99 sommets uniformément répartis entre 3 catégories à l'aide du modèle proposé par Dang en fixant à 2 le nombre d'arêtes introduites avec chaque nouveau sommet (Dang (2012)). De plus, chaque sommet est décrit par une valeur d'attribut réelle qui suit une loi normale d'écart-type 7, centrée autour d'une valeur propre à sa classe d'origine. Ainsi la première classe a un centre µ 1 = 10, la deuxième un centre µ 2 = 40 et la troisième un centre µ 3 = 70. La classe d'origine du sommet sert de vérité terrain pour l'évaluation. Le tableau 1 montre la répartition des arêtes entre les classes dans le graphe R.  (Combe et al. (2013) 
En exploitant l'information vectorielle en plus de l'information relationnelle, la méthode 2Mod-Louvain gagne en robustesse par rapport à la méthode de Louvain face à une dégradation de l'information relationnelle et par rapport à la méthode des K-Means en cas de dégradation de l'information vectorielle. De plus, lorsque la taille du réseau augmente, la méthode proposée permet de parer à la multiplication des classes qui survient alors avec la méthode de Louvain (4 classes contre 10). Les K-means conservent de bons résultats dans le cas où la taille du ré-seau évolue, car l'information des attributs demeure de bonne qualité mais ils sont globalement avantagés par rapport aux autres méthodes du fait que le nombre de classe est fourni en paramètre. Ainsi, l'utilisation simultanée des deux types d'information à travers des mesures de modularité adaptées permet de détecter plus efficacement des communautés dans des réseaux d'information. Enfin, la méthode 2Mod-Louvain produit aussi des résultats meilleurs ou du même ordre que Totem, une autre méthode utilisant les deux types d'information.
Conclusion
Dans cet article, nous avons étudié le problème du partitionnement de graphes avec noeuds et arêtes valués. En nous inspirant de la modularité de Newman et Girvan conçue pour la détection de communautés dans un réseau social, nous avons introduit une mesure de modularité basée sur l'inertie. Cette mesure est adaptée pour évaluer la qualité d'une partition d'éléments représentés dans un espace vectoriel réel. Nous avons également présenté 2Mod-Louvain, un algorithme qui combine notre critère de modularité basée sur l'inertie avec la modularité de Newman et Girvan pour détecter des communautés dans des réseaux d'information. Nous avons démontré formellement que ce nouvel algorithme peut être optimisé dans sa phase itérative, lui permettant d'être aussi efficient que l'algorithme de Louvain lors d'un passage à l'échelle. Comme le montrent nos expérimentations, en exploitant de manière conjointe les données relationnelles et vectorielles, la méthode 2Mod-Louvain détecte plus efficacement 
Summary
The modularity was introduced by Newman to estimate the quality of a graph vertex partition but this measure doesn't consider the values describing the nodes in the graph. In this article, we introduce a new complementary modularity measure, based on the inertia and specially conceived to evaluate the quality of a partition over vector space elements and consequently. We propose 2Mod-Louvain, a method using our inertia based quality criteria combined with Newman's modularity in order to detect communities in information networks. Our experiments show that combining the relational information and the vector information when partitioning a network detects communities more efficiently than methods using only one type of information. Our method is in addition, more robust to data degradation.

Introduction
La prosopographie est une science auxiliaire de l'histoire dont l'objectif est d'étudier les biographies des membres d'une catégorie spécifique de la société, en particulier leur origine, leur carrière, leurs déplacements et leur environnement familial. Il existe à nos jours plusieurs bases de données prosopographiques souvent peuplées à partir de fiches de description de personnes et à capacité d'interrogation réduite à l'extraction d'un ensemble de fiches. Le projet Personae propose d'intégrer un ensemble de bases de données prosopographiques de la Renaissance au sein d'un portail sémantique. Des mappings entre les ontologies locales de chaque base de données et l'ontologie du portail assurent l'accès à l'ensemble des concepts présents dans les bases reliées. L'interface d'interrogation principale de ce portail repose sur des formulaires d'interrogation classiques et les résultats retournés se présentent sous la forme de tableaux listant les réponses pertinentes dans les différentes bases. Malgré l'apport de cette solution sur le plan d'intégration sémantique et d'agrégation des données de différentes bases, elle demeure inefficace quand à la présentation de vue synthétique sur les données prosopographiques mettant en évidence les relations entre plusieurs personnages, les étapes "clés" dans la carrière d'un personnage, etc. Or, depuis plusieurs années le domaine du Visual Analytics n'a cessé d'apporter des réponses à ce genre de problème à travers différentes approches de visualisation de données permettant une exploration interactive et progressive de données Keim et al. (2008). Dans le cadre de ce projet, nous nous sommes appuyés sur des approches de visualisation de graphes et de cartes géographiques pour proposer deux interfaces de visualisation adaptées à la nature des données prosopographiques étudiées et répondant aux besoins d'interrogation du portail mis en place. Les interfaces de visualisation proposées s'appuient d'une part sur des mesures de similarité et de proximité sémantique pour l'identification et la construction de réseaux personnels et professionnels à la Renaissance. D'autre part, elles exploitent les dimensions spatiales et temporelles très présentes en termes de prosopographie pour proposer des visualisations cartographiques retraçant la carrière d'une personne ou les flux de déplacements entre les différents lieux de l'époque. Le reste de l'article est organisé comme suit. Dans la section 2 nous présentons ProsoGraph, l'interface de visualisation de ré-seaux socio-professionnels. Ensuite, dans la section 3, nous présentons ProsoMap, l'interface de visualisation cartographique spatio-temporelle. Enfin, dans la section 4, nous développons les conclusions de ce travail et exposons les travaux en cours et futurs en lien avec notre projet.
2 ProsoGraph : Visualisation de graphes de réseaux socioprofessionnels de la Renaissance
Le portail Personae intègre plusieurs bases de données de prosopographies de la Renaissance et offre ainsi une vue agrégée sur une grande quantités de données bibliographiques de personnages permettant d'aboutir à des descriptions potentiellement complètes de la carrière d'un personnage ainsi que du réseau socio-professionnel qu'il ait pu développer. Ces données s'apparentent naturellement à la définition de réseaux sociaux du fait de l'existence de personnes avec des relations et interactions entre elles. Partant de ce fait et dans le but d'assurer une meilleur exploration des données du portail, nous avons développé l'interface ProsoGraph qui permet de visualiser les données prosopographiques sous la forme de graphes où les noeuds correspondent à des personnages, des lieux ou des professions de l'époque et les arcs correspondent aux liens entre ces différents objets. ProsoGraph s'appuie sur l'API de visualisation de graphes Sigma JS (http ://sigmajs.org/). L'interface communique avec le portail à travers des requêtes SPARQL utilisant l'ontologie du portail et redistribuées ensuite sur l'ensemble des bases pour constituer une réponse globale. Les réponses, au format XML, sont par la suite parsées pour construire et visualiser le graphe correspondant. Selon la nature de la requête formulée et du graphe obtenu, ProsoGraph permet différents types d'exploration des données et d'interprétation des réseaux obtenus. La figure 1 montre une illustration dans le cas où la requête permet d'extraire le réseau sociau-professionnel d'une personne (gauche) et dans le cas où la reqûete porte sur les intéractions autours d'un lieu d'intérêt (droite).
N. Messai et T. Devogele
FIG. 1 -Un exemple de réseau socio-professionnel (gauche) et d'intéractions autour d'un lieu d'intérêt (droite)
layers.org) et des cartes issues de différentes sources (GoogleMap, OpenStreetMap, GeoPortail, etc.). La première représentation se focalise sur la carrière d'un individu. La deuxième traite des flux de déplacements entre les différents lieux. Pour relier les lieux à des coordonnées géographiques, une URI Geonames(http ://www.geonames.org/) est utilisée. Il faut noter que des lieux historiques disparus peuvent être ajoutés à la base Geonames. L'interface ProsoMap career retrace la trajectoire de vie d'un individu. Par rapport à la représentation classique de la time geography, une représentation en 2D a été retenue, la durée de résidence de l'individu dans un lieu est définie par la taille du cercle représentant ce lieu. Cette trajectoire est orientée par la nuance de gris (blanc pour le début de vie et noir pour la fin). L'interface ProsoMap flow, représente les flux de déplacements d'une population extraite du portail à l'aide de plusieurs critères obligatoires tels que le lieu et optionnels tels que la période, la profession, etc. Pour chaque lieu, deux cercles concentriques sont dessinés. Le plus petit représente le nombre d'individus de cette ville n'ayant pas changé de ville durant la période. Le plus grand décrit le nombre d'individus de cette ville ayant été dans la ville durant cette période. La ville étudiée est reliée aux villes d'où provient une partie de la population ou d'où part une autre partie durant cette période. La figure 2 donne une illustration des deux representations implémentées dans ProsoMaps.
Conclusion
Dans cet article, nous avons proposé deux interfaces complémentaires ProsoGraph et ProsoMap pour la visualisation de données prosopographiques. Ces interfaces offrent des approches génériques facilitant la navigation entre les concepts et l'analyse visuelle impossible avec un grand nombre de fiches prosopographiques. Les deux interfaces communiquent avec le portail à travers le requêtage des concepts ontologiques de haut niveau : personnes, lieux, fonctions et des relations temporelles entre ces concepts. 
Summary
This paper presents two visualization tools ProsoGraph and ProsoMap developed within a collaborative project which focuses on the access and exploration of prosopographic datasets of the Renaissance period in France. The project aims at modeling and implementing a semantic portal ensuring access to existent prosopographic datasets. ProsoGraph and ProsoMap are developed based on social networks visualization techniques and spatio-temporal data visualization techniques, respectively. They interact with the portal through a semantic layer and allow additional advanced querying capabilities.

Introduction
En 1993, Rakesh Agrawal, Tomasz Imielinski et Arun N. Swami publiaient l'un des articles fondateurs de la découverte de motifs "Mining Association Rules between Sets of Items in Large Databases" dans les actes de "the ACM SIGMOD International Conference on Management of Data" en introduisant le problème de l'extraction des règles d'association inté-ressantes. Formellement, ce problème consiste à énumérer toutes les règles du type X ? I où X est un ensemble d'items et I un item non présent dans X telles que les probabilités P (X ? I) et P (X ? I|X) soient suffisamment élevées. Moins qu'une finalité nouvelle, Agrawal et al. (1993) ont surtout substitué à la traditionnelle recherche heuristique, la recherche complète et consistante. En effet, le problème de la découverte de règles de classification (où I est seulement une valeur de classe) était déjà une thématique de recherche active dans le domaine de l'intelligence artificielle mais les algorithmes n'étaient pas exhaustifs (Breiman et al., 1984), (Quinlan, 1986), (Piatetsky-Shapiro, 1991). Ces notions de complétude et consistance très importantes en bases de données justifient la publication de Agrawal et al. (1993) dans ACM SIGMOD. De même, la généralisation proposée l'année suivante par Agrawal et Srikant (1994) (où la conclusion de la règle est désormais un ensemble d'items) est publiée dans VLDB 1 .
Depuis 20 ans, la communauté de la découverte de motifs n'a cessé de puiser son inspiration dans l'article fondateur (Agrawal et al., 1993) comme en témoignent ses nombreuses citations : 26 ème article le plus cité de l'informatique selon CiteSeer, le 5 ème le plus cité dans le domaine de l'exploration de données selon Microsoft Acamedic Research, plus de 11 900 citations selon GoogleScholar 2 . Pour mieux définir ce qu'est la découverte de motifs, revenons au problème initial qui se divise en deux sous-problèmes : (1) trouver tous les motifs ensemblistes (itemsets) présents dans au moins x% des transactions et (2) générer à partir de ces motifs toutes les règles d'association intéressantes. Cette subdivision illustre les deux grandes problématiques qui ont animées la découverte de motifs pendant 20 ans : l'extraction de motifs et l'utilisation des motifs. Premièrement, l'extraction de motifs consiste à énumérer tous les motifs d'un langage (e.g., les itemsets ou les séquences) qui satisfont une contrainte (e.g., une fréquence minimale). Il est toutefois possible de se contenter d'une représentation condensée de ces motifs i.e., une fraction des motifs qui garantit encore la regénération totale des règles. Ces trois dimensions (i.e., langage, contrainte et représentation) illustrées dans (Mannila et Toivonen, 1997) donnent lieu à un très grand nombre de problèmes. Deuxièmement, l'utilisation des motifs consiste à combiner plusieurs motifs pour constuire des modèles plus complexes et/ou plus généraux. Typiquement, des classifieurs sont construits en regroupant des règles de classification, ellesmêmes dérivées de motifs ensemblistes.
Cet article ambitionne de faire une étude des travaux de la découverte de motifs réalisés de 1995 à 2011. Plutôt que de proposer une étude bibliographique basée sur quelques dizaines d'articles et forcément parcellaire, nous avons opté pour une vision plus globale en s'appuyant sur plusieurs centaines d'articles. Il s'agit de 1030 articles consacrés à la découverte de motifs issus des 6223 articles publiés dans les 5 conférences majeures de l'exploration de données : KDD, PKDD, PAKDD, ICDM et SDM (le protocole est détaillé en section 2). Outre la vision globale, notre corpus d'étude est suffisamment conséquent pour quantifier des phénomènes et ce dans le temps. Les traitements automatisés porteront sur les titres des publications tandis que nous nous appuierons parfois manuellement sur les résumés afin de lever certaines ambiguïtés. Dans un premier temps, nous proposons de positionner la découverte de motifs au sein de l'exploration de données (section 3). Nous désirons également mieux cerner le domaine à la lumière des 3 dimensions mentionnées ci-avant : le langage, la contrainte et la représentation (section 4). Nous souhaitons pouvoir répondre à des questions simples : quels sont les langages les plus étudiés ? les motifs maximaux ont-ils été davantage étudiés que les générateurs ? etc. Au-delà des réponses, nous nous attachons à une quantification précise de ces phénomènes.  5 . Ce choix a peut-être tendance à exclure des travaux plus matures publiés en revue et à l'inverse des travaux plus prospectifs publiés en atelier. De même, ce travail manque des travaux publiés dans des conférences connexes notamment en base de données (e.g., VLDB) ou en Recherche d'Information (e.g., ICKM). Cependant, intégrer ces conférences à l'étude aurait dilué l'essence de l'exploration de données (et donc, celle de la découverte de motifs). Au final, nous estimons que les 350 publications annuelles issues des 5 conférences retenues constitue un échantillon significatif pour une étude statistique de l'ensemble de la production mondiale.
Cette étude porte sur les titres des publications indexées sous The DBLP Computer Science Bibliography 6 pour les 5 conférences. Même si le volume et les types de publication (e.g., articles longs, posters, tutoriaux, panels) varient selon la conférence et les années, la très grande majorité concerne des articles longs et courts. Par ailleurs, la première édition des conférences KDD, PAKDD et SDM n'ont pas été indexées sous DBLP. Au final, le tableau 1 récapitule pour les 5 conférences l'année de la première édition, l'année de la première indexation des publications sous DBLP et le nombre total de publications indexées. Si les traitements automatisés portent exclusivement sur les titres, la validation manuelle de ces traitements s'est appuyée sur les résumés lorsque le titre se révélait insuffisant. Le traitement de l'intégralité des articles permettrait probablement d'améliorer le filtrage automatique (à condition d'utiliser des méthodes du TAL pour circonscrire avec justesse les contributions de l'article analysé du reste).
La figure 1 (a) reporte le nombre de publications en exploration de données entre 1995 et 2011 pour chacune des conférences. Ce nombre n'a cessé de croître sur ces 17 années (excepté en 2007 et en 2010) traduisant l'essor du domaine. Cette augmentation globale s'explique à la fois par la création de nouvelles conférences (jusqu'en 2002) et l'augmentation du nombre de publications pour chacune des conférences (e.g., plus 88% de publications depuis 2002).
Frontière de la découverte de motifs
La difficulté majeure consiste à déterminer quelles sont les publications concernant la dé-couverte de motifs. Nous ne nous limitons pas aux travaux dédiés à l'extraction de motifs mais considérons également les travaux où ces derniers sont utilisés (par exemple pour construire un modèle). Par motif, nous entendons motif local au sens de Hand (2002) i.e., qui ne décrit qu'une portion de la base de données. Pour cette raison, nous considérons que les arbres de dé-cision, les réseaux bayésiens, les réseaux de neuronnes ou les machines à vecteurs de support (SVM) ne sont pas des travaux concernant l'extraction ou l'utilisation de motifs locaux. Nous décrivons maintenant le processus semi-automatique qui par raffinement successifs, a permis de déterminer les publications concernant la découverte de motifs.
1. Filtrage automatique : Cette première étape a consisté à conserver tous les articles susceptibles de concerner la découverte de motifs. Nous avons retenu toutes les publications dont le titre mentionne un terme relatif à la découverte de motifs. Cette liste a été établie en nous appuyant sur les trois dimensions mentionnées en introduction :
(a) Langage : pattern, item, sequence, rule, tree, graph, string
Bien sûr, cette liste de termes est élargie à leurs variations (e.g., pour le terme string, on considère substring, strings, etc). Ainsi, 1590 publications ont été retenues soit environ un quart de la base de données.
2. Filtrage manuel : Cette seconde étape a consisté à éliminer tous les faux positifs i.e., les publications ne concernant pas la découverte de motifs mais contenues dans les 1590 publications obtenues à l'étape 1. Pour ce faire, nous avons lu un à un chaque titre et si nécessaire, nous avons consulté le résumé de l'article voire son intégralité pour lever toute ambiguïté. 1030 publications ont été retenues par le filtrage manuel sachant que le résumé a été consulté dans 126 cas.
Il est clair que la solution de filtrage automatique a été priviligiée pour éviter de parcourir les 6223 publications. Même si le filtrage repose sur une liste de termes assez ouverte, des articles pertinents ont été manqués (i.e., des faux négatifs). Afin d'estimer ce nombre de faux négatifs, nous avons tiré au hasard 100 publications parmi les 4633 éliminées à l'étape 1. 5 de ces publications concernaient bien la découverte de motifs. Par conséquent, nous estimons qu'environ 232 (= 0.05 × (6223 ? 1590)) publications relatives à la découverte de motifs ont été manquées par notre approche. Dans la suite, nous faisons l'hypothèse que les 1030 publications retenues sont un échantillon représentatif de la découverte de motifs.
Clairement notre approche repose sur une part de subjectivité à l'étape 1 dans le choix des termes retenus et à l'étape 2 dans l'élimination des faux positifs. Nous pensons néanmoins que ce protocole n'introduit pas plus de subjectivité qu'une étude bibliographique traditionnelle.
Décroissance de la découverte de motifs
Le tableau 1 reporte le nombre de publications relatives à la découverte de motifs par confé-rence et calcule quelles proportions ces publications représentent. La découverte de motifs, en correspondant à environ 1 article sur 6 de l'exploration de données, en constitue un réel sousdomaine. Parmi les 8489 auteurs qui ont contribués aux 5 conférences, 1685 d'entre eux (soit 19, 85%) ont participé à au moins une publication en découverte de motifs. 851 auteurs (soit 10, 02%) se sont cantonnés à des publications en découverte de motifs.
La figure 1 (b) présente l'évolution de la place de la découverte de motifs au sein de l'exploration de données. Nous remarquons une décennie florissante entre 1997 et 2006 où la part de la découverte de motifs est supérieure à sa part moyenne de 17%. Pendant 5 années, entre 1998 et 2003, 1 article sur 5 est même consacré à la découverte de motifs, l'âge d'or de la dé-couverte de motifs en quelque sorte. Jusqu'en 1999, l'essor de la découverte de motifs est plus important que celui de l'exploration de données pourtant conséquent (cf. figure 1). Il est certain que KDD, PKDD et PAKDD étaient les conférences privilégiées pour diffuser les travaux naissants relatifs à la découverte de motifs (Piatetsky-Shapiro, 2000). A l'inverse, d'autres travaux (e.g., les arbres de décision) disposaient déjà de tribunes reconnues avec les conférences en apprentissage et intelligence artificielle. Peut-être que la reconnaissance des conférences "Data Mining" et leur attractivité accrue a pu attirer davantage de ces travaux à partir de 2002. Quoiqu'il en soit, depuis cette date, la part de la découverte de motifs a progressivement diminué jusqu'à atteindre moins de 10% en 2011 (proche des 7% de 1995). Cette baisse relative se traduit aussi par une baisse absolue avec une chute de 99 articles en 2005 à 59 articles en 2011.
Pour finir, nous introduisons un nouvel indicateur afin d'estimer le dynamisme d'un domaine via son ensemble de publications. Pour commencer, la fraîcheur mesure le degré de nouveauté d'une publication dans le domaine (par rapport à la période de 1995 à 2011) : F reshness(p) = (year(p) ? 1995)/16 où year(p) est l'année de la publication de p. Plus la fraîcheur d'une publication est proche de 1, plus elle est récente i.e., proche de 2011. A l'inverse, une fraîcheur égale à 0 signifie que la publication date de 1995. Nous étendons alors cet indicateur à un ensemble de publications P en en calculant la valeur moyenne : F reshness(P ) = 1/|P | × p?P F reshness(p). Cette métrique donne une tendance grossière sur le dynamisme d'un domaine via ses publications. Lorsque la fraîcheur d'un ensemble de publications atteint 1, cela signifie que les publications se concentrent sur les dernières années de la période 1995-2011. Par exemple, la fraîcheur des 6223 publications retenues pour l'étude est de 0,659 en raison de la hausse du nombre de publications annuelles. En comparaison, la fraîcheur des 1030 publications de la découverte de motifs est seulement de 0,590. Ce retrait traduit donc un affaiblissement de la découverte de motifs par rapport au reste de l'exploration de données. Dans la suite, nous utiliserons également la fraîcheur pour caractériser le 4 Typologie des publications
Les langages
Préparation des données Nous utilisons à nouveau une démarche par filtrage automatique puis correction manuelle. Tout d'abord, une liste de termes est associée à chaque type de langage comme indiqué par les deux premières colonnes du tableau 2 (sans les variantes). Aucun filtrage automatique n'est procédé pour les langages portant sur des motifs relationnels et gé-nériques. Le langage "generic" correspond aux approches destinées à plusieurs langages (Mannila et Toivonen, 1997). Ces listes sont alors utilisées pour réaliser une première classification automatique de chaque publication sachant qu'une partie n'est pas classée (27.57% soit 284 articles). Ensuite, toutes les publications (y compris celles non-classées) sont parcourues manuellement pour une classification définitive (en utilisant le titre et le résumé). Lors de cette phase, il a été observé que par défaut le mot "pattern" réfère implicitement à "itemset" puisque 135 articles comportant ce mot correspondent aux motifs ensemblistes. La dernière colonne du tableau 2 reporte la fraîcheur des publications associées à chaque langage en mettant en gras celles avec une dynamique favorable par rapport à l'exploration de données (> 0, 659). Sans surprise, les règles d'association et les motifs ensemblistes à l'origine de la découverte de motifs sont les plus étudiés à hauteur d'environ 2/3 de l'ensemble. Ensuite, environ un quart concerne les séquences et les graphes. La découverte de motifs dans les données spatiotemporelles et les données relationnelles est assez marginale. Plus étonnant, nous constatons que très peu de travaux se sont attaqués à des approches génériques en terme de langage. Une explication probable est la difficulté de proposer des approches translangagières tant sur le plan théorique que sur le plan implémentation 7 . La fraîcheur élevée de ces articles (0,705) dénote cependant un intérêt plutôt récent pour ce type de travaux.
Complexification des langages Le tableau 2 montre que globalement plus un langage est complexe, moins il y a d'articles qui lui sont dédiés. Premièrement, la complexification des 7. Cependant, les articles de revue (absents de nos données) sont peut-être plus appropriés pour les approches translangagières faisant souvent la synthèse de travaux publiés précédemment pour des langages distincts. langages entraine une marginalisation de par la difficulté intrinsèque liée à la combinatoire du problème. En effet, plus un langage est complexe, plus il est difficile d'extraire exhaustivement tous les motifs. Par exemple, avec 3 items, il est possible de former 80 séquences distinctes contre seulement 8 itemsets. Deuxièmement, cette complexification s'inscrit dans le temps comme le décrit la figure 2 : aux itemsets succèdent les séquences, puis les graphes. En fait, la capitalisation de savoir-faire diminue de langage en langage le nombre de verrous à lever. Typiquement, les méthodes d'élagage de l'espace de recherche pour les motifs ensemblistes (fondées sur l'anti-monotonie par exemple) sont transposables aux autres langages.
FIG. 2 -Evolution du nombre de publications pour les principaux langages.
Néanmoins, nous observons deux exceptions avec les arbres et les motifs ensemblistes qui sont respectivement moins étudiés que les graphes et les règles. Les arbres sont parfois simplifiés pour être traités comme une variante des séquences ou être traités comme un cas particulier des graphes. L'exception la plus notable concerne les motifs ensemblistes plus simples que les règles d'association et pourtant moins étudiés. Le fait que les règles soient particulièrement étudiées avant 2000 s'explique d'abord historiquement puisque l'extraction de règles de classification était déjà une thématique de recherche importante en intelligence artificielle, bien avant 1993. De plus, l'article fondateur de Agrawal et al. (1993) a désigné les règles d'association comme objectif ultime au détriment des itemsets considérés comme un outils intermédiaire (même si techniquement, l'obtention des itemsets est la phase la plus difficile). La faible fraî-cheur des articles concernant les règles (0,455) renforce l'hypothèse de cet ancrage historique.
Limite de la complexification Tandis que le nombre de publications concernant les règles et les itemsets diminue, les deux langages les plus complexes (les séquences et les graphes) prennent une part toujours plus grande au sein de la découverte de motifs (cf. la figure 2). Par exemple, sur les 4 dernières années, la proportion d'articles consacrée aux graphes dé-passe celle consacrée aux règles. Néanmoins, cette complexification semble atteindre ses limites puisqu'aucun langage successeur aux graphes (motifs relationnels ou spatio-temporels) ne semble pour l'instant prendre la relève de manière significative. Ces données ne sont peutêtre pas à disposition en suffisamment grande quantité et il est possible que celles disponibles soient réduites à des langages plus simples comme les graphes. Etant donné la fraîcheur non négligeable pour le langage "spatial", cette conclusion provisoire pourrait être révisée prochainement. Enfin, à la complexification de la nature du langage pourrait succéder la complexifica- TAB. 3 -Répartition des publications suivant la contrainte.
tion des caractéristiques des données, nous avons observé que certains mots clés sont de plus en plus présents dans les titres : données incertaines ("uncertain" avec une fraîcheur de 0,892), données massives ("massive" avec 0,729) ou données dynamiques ("dynamic" avec 0,687).
Les contraintes
Préparation des données Nous utilisons à nouveau une démarche par filtrage automatique puis correction manuelle. Tout d'abord, une liste de termes est associée à chaque type de contraintes (cf. les deux premières colonnes du tableau 3). Ces listes sont utilisées pour réali-ser une première classification automatique de chaque publication mettant en avant la notion de contrainte. Ensuite, les 510 publications classées sont parcourues manuellement pour une classification définitive en utilisant le titre et le résumé. Le terme "significant" regroupe les articles distinguant les motifs statistiquement valides des autres tandis que "interestingness" se rapporte à des approches plus variées souvent fondées sur des connaissances subjectives. A noter que le terme "generic" correspond aux approches dédiées à une classe de contraintes. Comme pour les langages (et pour les mêmes raisons), peu de publications sont consacrées aux contraintes génériques même si ces dernières sont plutôt récentes (cf. leur fraîcheur de 0,637).
L'obsession de la fréquence Globalement, la contrainte de fréquence minimale en représen-tant 50% des publications est de très loin la plus utilisée. En effet, de nombreuses publications s'attaque au sous-problème 1 (présenté en introduction) pour proposer une méthode plus efficace, pour l'adapter à un langage différent, pour se limiter à une représentation condensée, etc. Plus qu'une nécessité applicative, nous pensons que cette utilisation récurrente de la contrainte de fréquence découle du paradigme imposé par l'article originel de Agrawal et al. (1993). Premièrement, remplacer la contrainte de fréquence par une autre plus sélective, c'est remettre en cause la regéneration exhaustive de toutes les règles intéressantes (le sous-problème 2). Hors cette exhaustivité est un fondement du paradigme qui entraine une surabondance assumée : "When we started doing data mining, we were concerned that we were generating too many rules, but the companies we worked with said, 'this is great, this is what exactly what we want !' " (Winslett, 2003). Dès lors, la contrainte inspire toujours la peur de trop ou mal filtrer ce qui conduirait à perdre des motifs pertinents. Cette crainte s'illustre aussi avec l'obsession de diminuer le seuil de fréquence minimal quitte à extraire des motifs non-significatifs.
Deuxièmement, l'évaluation de l'approche dans (Agrawal et al., 1993) ne repose pas sur l'évaluation de la qualité des motifs extraits comme on valide par exemple la qualité d'un classifieur avec une validation croisée. Plus généralement, la plupart des articles d'extraction de motifs n'évaluent pas la qualité des motifs extraits mais la rapidité de leur extraction ou la quantité de mémoire requise. De ce point de vue, améliorer le processus d'extraction de motifs revient à diminuer le coût temporel et/ou spatial, mais surtout pas à en modifier le résultat, i.e., les motifs fréquents. De plus, la contrainte de fréquence minimale, quel que soit le langage dispose de propriétés intéressantes (souvent issues de l'anti-monotonie) qui facilitent l'extraction. L'évaluation d'une méthode fondée sur une autre contrainte est alors doublement désavantageuse. De manière grossière, comme une contrainte pertinente ne satisfait en général pas la propriété d'anti-monotonie, l'algorithme d'extraction associé sera donc moins efficace que l'extraction des motifs fréquents. Par ailleurs, il est difficile de montrer que les motifs extraits suivant une nouvelle contrainte sont de meilleure qualité que ceux extraits avec la contrainte de fréquence minimale car il n'existe pas de protocole objectif de validation.
FIG. 3 -Evolution du nombre de publications pour les principales contraintes.
Vers plus de qualité Désormais, quelque soit le langage, l'extraction des motifs fréquents est une tâche maîtrisée. Pour cette raison, le nombre de publications mettant en avant les motifs fréquents diminue spectaculairement depuis 2005 (cf. la figure 3) expliquant en partie la dé-croissance de la découverte de motifs. Le défi combinatoire de la découverte de motifs cède sa place à celui de la qualité des motifs extraits. Ainsi, le recours à une contrainte pour affiner le filtrage gagne en légitimité suivant une perspective envisagée par Agrawal : "we need work to bring in some notion of 'here is my idea of what is interesting,' and pruning the generated rules based on that input." (Winslett, 2003). Cependant, la définition de ces contraintes demeure une problématique complexe. Déjà, Fayyad et al. (2003)  Malgré cette difficulté, la fraîcheur de certaines catégories et termes semble dessiner un renouveau de l'extraction de motifs sous contrainte. Même si la fraîcheur des contraintes dé-diées aux contrastes est seulement de 0,596, on observe des fraîcheurs élevées de 0,893 pour "discriminative", de 0,812 pour "contrast" et de 0,736 pour "subgroup". Ce regain d'intérêt est aussi marqué pour les motifs significatifs (avec une fraîcheur de 0,651) et surtout pour les contraintes d'utilité (avec une fraîcheur de 0,724). Cette dynamique est également visible sur le graphique de droite de la figure 3. Enfin, une autre voie connexe au filtrage par seuillage TAB. 4 -Répartition des publications suivant le type de représentation condensée.
est le classement des motifs en utilisant une mesure pour les ordonner comme l'illustrent les termes "ranking" et "top-k" avec une fraîcheur respective de 0,823 et 0,762.
Les représentations condensées
Préparation des données Nous utilisons à nouveau une démarche par filtrage automatique puis correction manuelle. Les deux premières colonnes du tableau 4 donne les représentations condensées et leurs termes associés. Ces listes permettent de classer automatiquement chaque publication en fonction de son titre. Ensuite, les 117 publications classées sont vérifiées manuellement pour une classification définitive en utilisant le titre et le résumé. La fraîcheur de chaque type de représentation condensée est indiquée dans la dernière colonne. Pour rappel, l'objectif des représentations condensées est de réduire les redondances entre motifs (Calders et al., 2004). Les notions de bordures se contentent des motifs les plus spéci-fiques ou les plus généraux au sens de l'inclusion. Les motifs fermés et générateurs (libres ou clés) exploitent le même principe mais à fréquence égale. A noter que le terme "other" regroupe des articles se concentrant principalement sur les bases génériques de règles d'association.
Succès des représentations condensées : bordures puis motifs fermés 11,35% des publications relative à la découverte de motifs exploitent la notion de représentation condensée. Ce succès découle de leur indéniable bénéfice et de leur validation aisée (i.e., un contexte mé-thodologique opposé à celui des contraintes). Le concept de représentation condensée s'est rapidement imposée car il concilie la diminution du nombre de motifs et la conservation de l'exhaustivité via la regénération. De ce point de vue, elle s'inscrit parfaitement dans la perspective du sous-problème 2. En outre, les travaux sur les représentations condensées sont faciles à évaluer. D'une part, la validité de la regénération peut être formellement démontrée. D'autre part, la qualité de la condensation peut être estimée empiriquement en calculant le ratio de compression. Le plus souvent, ce gain en compression s'accompagne d'un gain de vitesse et d'une diminution des ressources mémoires consommées.
Parmi les différentes représentations, les bordures constituent la première représentation à succès à avoir été proposée (cf. figure 4) même si le nombre de travaux concernant les bordures décroît régulièrement depuis 1997-2000. Par nature, ces motifs maximaux/minimaux ont des propriétés extrêmes (e.g., fréquence très basse) et ne permettent pas d'inférer les propriétés des autres motifs (e.g., déduire la fréquence d'un motif plus général). Les motifs libres et fermés en dépassant ces limites se sont largement imposés. Finalement, le succès écrasant des fermés face aux générateurs s'explique par une conjonction de facteurs : moins nombreux, plus faciles à extraire et validité statistique plus forte (car ils véhiculent plus de régularités que les libres).
FIG. 4 -Evolution du nombre de publications pour les représentations condensées.
Bonne représentation mais mauvais modèle Désormais, les techniques concernant les représentations condensées sont bien maîtrisées et ce pour la plupart des langages. Le nombre de publications sur cette thématique a atteint son paroxysme entre 2005 et 2008. Seules les publications dédiées aux motifs générateurs et fermés se maintiennent dans le paysage de la découverte de motifs. Cependant, la taille des représentations condensées (exactes ou approximatives) reste trop importante pour autoriser une analyse globale des motifs. Il est alors né-cessaire de mettre en oeuvre d'autres mécanismes pour réduire leur taille soit en opérant un filtrage individuel des motifs (usage de contrainte) ou en opérant un filtrage collectifs des motifs (construction de modèle). Cette dernière option ressemble à l'objectif initial des représenta-tions condensées mais en l'étendant à des motifs non-comparables (par rapport à l'inclusion). Cette direction peut être vue comme une valorisation des motifs : "to make frequent pattern mining an essential task in data mining, much research is needed to further develop patternbased mining methods." (Han et al., 2007). Les termes "collection" et "pattern-based" avec une fraîcheur respective de 0,739 et 0,723 montre un intérêt récent pour cette piste.
Conclusion
Le titre "Extraction efficace des motifs fréquents fermés" serait probablement le meilleur résumé de la découverte de motifs, reprenant tous les codes de l'article originel : être consistant et exhaustif pour régénérer ; et ce le plus efficacement possible. De facto, tout ce qui concerne l'efficacité de l'extraction est désormais maîtrisé nous laissant face au principal problème : parmi l'abondance de motifs, trouver ceux qui sont réellement pertinents pour l'utilisateur. Traiter de nouveaux langages et de nouvelles sortes de données a juste déplacé ce problème ; utiliser des représentations condensées l'a à peine atténué. De nombreux travaux récents s'attaquent à ce challenge en s'appuyant sur un post-traitement pour sélectionner un sous-ensemble de motifs, sur des filtrages plus subtils ou sur des classements élaborés.
Enfin, si la flexibilité offerte par le choix du langage et de la contrainte offre un spectre d'applications a priori large, l'ancrage applicatif de la découverte de motifs reste moins fort que celui de l'exploration de données. En effet, de nombreux termes orientés application tels que "mobility", "multi-document", "patent", "multi-task", "advertising", "malware", "media", etc ; caractérisent les titres de l'exploration de données avec une fraîcheur supérieure à 0,9. A

Introduction
La Catégorisation de textes joue un rôle très important dans la recherche d'information et la fouille de textes. Cette tâche a été couronnée de succès en faisant face à une grande variété d'applications. Ce succès est du principalement à la participation croissante de la communauté d'apprentissage machine. Dans ce travail, nous nous intéressons à l'algorithme des K-plus proches voisins (Cover et Hart, 1967). Ce dernier développé tout d'abord par (Fix et Hodges, 1989) est devenu l'un des algorithmes les plus populaires dans la catégorisation de textes. Il est robuste et placé parmi les meilleurs algorithmes (Sebastiani, 2002). Toutefois, il présente certaines limites, (i) stockage mémoire énorme car il faut stocker l'ensemble complet d'apprentissage et (ii) coût élevé de calcul car il doit explorer l'ensemble d'apprentissage en entier pour pouvoir classer un nouveau document. Une solution intéressante à base d'automate cellulaire appelée CAkNN (Cellular Automaton combined with k-NN) a été proposée dans (Barigou et al., 2012) pour réduire le temps de classification, dans le cadre du filtrage de spam. Les expériences réalisées sur le corpus LingSpam ont montré que la méthode CAkNN permet d'atteindre de meilleures performances de classification comparée à d'autres travaux publiés dans le domaine de filtrage de spam. Dans ce papier, nous allons reprendre cette solution pour la catégorisation de textes et nous allons montrer à travers un ensemble d'expériences que CAkNN permet de réduire le temps de classification par une sélection d'un minimum d'instances d'apprentissage pour la classification d'un nouveau document et ceci sans que la performance prédictive n'en soit affectée. Ce papier est organisé comme suit :la section 2 est dédiée aux travaux connexes. Dans la section 3 nous décrivons le principe de la méthode KNN. La section 4 décrit notre contribution pour améliorer cette méthode. Les expériences et les résultats sont présentés dans la section 5, et la conclusion est donnée dans la section 6.
Travaux connexes
Différentes solutions ont été proposées pour réduire la complexité de calcul. Comme le souligne (Bhatia et SSCS, 2010), nous distinguons les méthodes de sélection d'instances et les méthodes de réduction du temps de calcul. Les premières visent la réduction du nombre d'exemples dans la base d'apprentissage par certaines techniques d'édition en éliminant certains exemples qui sont redondant dans un certain sens (Gates, 1972). Les deuxièmes méthodes accélèrent la procédure de recherche lors de la classification par la mise en structures bien organisées de l'ensemble d'apprentissage (Liu et al., 2006). Cependant, pour des dimensions très importantes, l'espace requis croit d'une manière exponentielle.
L'algorithme des K-plus proches voisins
L'algorithme des k-plus proches voisins (KNN) est une méthode d'apprentissage à base d'instances. Il ne comporte pas de phase d'apprentissage en tant que telle. Les documents faisant partie de l'ensemble d'apprentissage sont seulement enregistrés. Lorsqu'un nouveau document à classer arrive, il est comparé aux documents d'apprentissage à l'aide d'une mesure de similarité. Ses k plus proches voisins sont alors considérés : on observe leur catégorie et celle qui revient le plus parmi les voisins est affectée au document à classer. La méthode utilise donc deux paramètres : le nombre k et la fonction de similarité. Une mesure de similarité très utilisée et que nous avons adoptée dans ce papier est la similarité cosinus (équation 1), qui consiste à quantifier la similarité entre deux documents en calculant le cosinus de l'angle entre leurs vecteurs.
Nous définissons f comme étant la fonction KNN qui attribue une classe à une nouvelle instance Q. Dans notre cas, Cette fonction, utilise le vote majoritaire pondéré donné en équation 2.
Méthode proposée
Dans cette section nous reprenons la méthode étudiée dans (Barigou et al., 2012) pour cette fois-ci la catégorisation de textes. Nous proposons une solution originale permettant de surmonter l'un des inconvénients majeurs de la méthode KNN qui est le coût de classification dans une tâche comme la catégorisation de textes où nous manipulons des milliers de documents voire des milliers de milliers. Le principe de cette méthode est comme suit : au lieu de faire participer toutes les instances d'apprentissage pour la recherche des k-voisins ce qui va augmenter le temps de calcul, une sélection d'un sous ensemble réduit d'instances est tout d'abord réalisée. Cette opération de sélection a comme conséquence une réduction significative du temps de classification. L'approche proposée utilise la machine cellulaire CASI (Atmani et Beldjilali, 2007) 
Représentation des instances d'apprentissage
Nous proposons une nouvelle stratégie de représentation des documents d'apprentissage ; ces derniers vont être encodés dans une structure cellulaire. L'ensemble d'apprentissage est tout d'abord pré traité pour construire l'index. Nous distinguons trois étapes :
1. établir une liste initiale de termes en effectuant une segmentation de texte en mots ; 2. éliminer les mots inutiles en utilisant une liste prédéfini de mots vides et enfin ; 3. utiliser une variante de l'algorithme de Porter pour effectuer la racinisation des différents mots retenus. Les termes sont liés à leurs documents par deux matrices de voisinage IM et OM :
est une prémisse de r) Alors IM(t,r)=1 Sinon IM(t,r)=0 ; -?d ? {d
Sélection des instances
Le processus de sélection permet de déterminer la contribution de chaque document d'apprentissage. Les documents ayant un plus grand nombre de termes communs avec l'instance à classer seront sélectionnés pour participer dans sa classification. Nous allons tout d'abord, définir les concepts suivants :
1. NTT :le Nombre Total des Termes du vocabulaire V trouvés dans Q. ; 2. T(? ) : le seuil défini par :
Le processus de sélection des instances passe par trois étapes : -Initialisation de la couche CelTerm ; -Recherche de l'ensemble des documents A = {d i } tel que d i ? Q = ? ; -Recherche du sous ensemble E ? A vérifiant la condition donnée dans (l'inéquation 5). La recherche des instances est réalisée par l'application de la fonction booléenne globale ?f act • ?rule qui va récupérer les documents partageant au moins un terme avec Q (sous ensemble A). Les deux fonctions booléennes sont définies comme suit :
?rule : (ET, IT, ST, ER, IR, SR) ? ?? ? ?rule(ED + (OM * ER), ID, SD, ER, IR, ¬(ER)). chaque document d i de l'ensemble A se voit attribuer une valeur N T C(d i ).
Cette valeur correspond au nombre total de cellules actives obtenues par le produit booléen du vecteur ET de la couche CelTerm avec le vecteur OM T (d i ) Le seuil T(?) est ensuite appliqué afin de réduire davantage les données d'apprentissage et obtenir l'ensemble E qui sera utilisé par la méthode KNN.
Étude expérimentale et Résultats
Dans ce qui suit nous évaluons la solution proposée et la comparons avec la méthode KNN.
Corpus et mesures de performances
Nous utilisons, dans ce papier, le corpus 20 NewsGroups. Ce corpus traite 20 catégories, chaque catégorie représente 5% du corpus, il contient au total 18828 documents. Nous avons utilisé 80% du corpus pour l'apprentissage et 20% de ce corpus pour le test. Pour évaluer les performances des deux méthodes KNN et CAkNN, nous calculons pour chaque catégo-rie la mesure F 1 (ci) (Sebastiani, 2002), donnée par la formule suivante : F 1(ci) = 2 * ? * ? ?+? avec ? la précision et ? le rappel. La mesure F1 globale, sur toutes les classes, est calculée à travers une moyenne des résultats obtenus pour chaque catégorie. Le temps nécessaire pour classifier une nouvelle instance est calculé comme suit : dans le cas de la méthode KNN, ce temps considère le parcours de l'ensemble d'apprentissage D en entier. Par contre, dans le cas de CAkNN, le temps de classification est calculé en sommant le temps de sélection du sous ensemble E avec le temps de classification en considérant seulement le sous ensemble E.
Résultats expérimentaux
Les figures 1 et 2 regroupent les résultats obtenus pour le corpus 20Newsgroups dans le cas de ? = 5. A partir de ces figures nous observons la contribution de la méthode CAkNN, qui consomme moins de temps, tout en restant plus performante que la méthode KNN.
FIG. 1 -Performance de classification du corpus 20NG en fonction du seuil K.
FIG. 2 -Temps de classification de 20 % du corpus 20NG en fonction du seuil K.
Nous en dégageons deux résultats intéressants, le premier concerne l'efficacité de l'approche. La qualité de prédiction de CAkNN est meilleure que celle du classifieur KNN. Le deuxième concerne la réduction du temps de classification obtenue grâce à la réduction drastique des instances d'apprentissage. Les résultats indiquent que l'écart entre les résultats avant et après application de la méthode CAkNN sont suffisamment significatifs.
Conclusion
Dans ce papier, nous avons proposé une nouvelle solution pour améliorer le temps de classification de la méthode KNN. Nous n'avons pas besoin de tout l'ensemble d'apprentissage pour classifier une nouvelle instance. Dans ce travail, nous proposons de sélectionner un sous ensemble réduit de documents pour cette tâche de catégorisation. Ce problème de sélection est traduit en un problème de manipulation d'opérations booléennes par l'utilisation de l'automate cellulaire de la machine CASI. Cet automate est premièrement censé filtrer les instances pouvant produire du bruit et deuxièmement, assurer la convergence de l'algorithme KNN en un temps de calcul intéressant. Comme perspective, nous prévoyons une étude comparative entre la méthode CAkNN et les autres solutions proposées dans (Bhatia et SSCS, 2010)  (Gates, 1972), (Hart, 1968) et (Wilson et Martinez, 1989).
Références Atmani, B. et B. Beldjilali (2007). Knowledge discovery in database : Induction graph and cellular automaton. Computing and Informatics Journal 26,2, 

Résumé. Dans cet article nous présentons une approche conceptuelle d'aide à la décision dans la conception de systèmes complexes. Cette approche s'appuie sur le formalisme de l'analyse de concepts formels par similarité (ACFS) pour la classification, la visualisation et l'exploration de données de simulation afin d'aider les concepteurs de systèmes complexes à identifier les choix de conception les plus pertinents. L'approche est illustrée sur un cas test de conception de cabine d'un avion de ligne fourni par les partenaires industriels et qui consiste à étudier les données de simulation de différentes configurations du système de ventilation de la cabine afin d'identifier celles qui assurent un confort convenable pour les passagers la cabine. La classification des données de simulation avec leurs scores de confort en utilisant l'ACFS permet d'identifier pour chaque paramètre de conception simulé la plage de valeurs possibles qui assure un confort convenable pour les passagers. Les résultats obtenus ont été confirmés et validés par de nouvelles simulations.
Introduction
La phase de conception de systèmes est une étape cruciale dans le processus de production des systèmes complexes au cours de laquelle plusieurs aspects sont étudiés pour garantir la performance du système ainsi que sa conformité aux besoins de l'utilisateur final. Ces aspects sont souvent traduits sous la forme d'un ensemble de paramètres de conception et de contraintes associées qui font l'objet de nombreuses simulations. Lorsqu'il s'agit de la conception de systèmes complexes, les simulations produisent des données volumineuses qui doivent par la suite être analysées afin d'identifier les configurations optimales. Dans ce contexte, le recours aux outils d'aide à la décision est essentiel pour aider les concepteurs à faire des choix rationnels. Il existe plusieurs méthodes et outils d'aide à la décision qui ont été utilisés dans divers domaines d'application tels que l'économie, l'industrie, etc. Ehrgott et al. (2010). Le choix d'une méthode en particulier dépend à la fois des données à analyser (format, volume, etc.) et de la méthode elle-même (performance, visualisation, etc.). Un état de l'art sur les principales approches d'aide à la décision dans le contexte industriel est donné dans Aviso et al. (2008). Dans ce travail, nous sommes particulièrement intéressés par les approches qui s'appuient sur des structures conceptuelles telles que les treillis de concepts et le formalisme d'Analyse de Concepts Formels (ACF) associé Ganter et Wille (1999). L'utilité de l'utilisation des treillis est prouvée par les nombreuses approches qui s'appuient sur cette structure comme support pour la navigation en recherche d'information, comme ensemble réduit de motifs et de règles d'association en fouille de données, comme un ensemble d'arbres de décision en apprentissage automatique, comme outil de prédiction et d'aide à la décision, etc. Ganter et al. (2005). Cependant, malgré ces nombreuses applications réussies, les approches basées sur l'ACF sont souvent confrontées à la rigidité de son format d'entrée qui nécessite que les données soient représentées par une relation binaire ce qui est loin d'être le cas des données réelles. Afin de contourner cette limite, l'ACF propose de procéder à des échelonnages des données non binaires pour les transformer en binaires. Cette binarisation est subjective et souvent accompagnée d'une perte d'information. Pour palier à cette limite, plusieurs travaux ont proposé des extensions du formalisme de l'ACF à des données non binaires Ferré et Ridoux (2000); Ganter et Kuznetsov (2001); Messai et al. (2008). Parmis ces méthodes, l'ACF par Similarité (ACFS) considère la similarité entre les données pour les grouper en treillis appelés treillis de concepts multi-valués (MV) Messai et al. (2008). En s'appuyant sur la similarité entre les données, l'ACFS permet de générer différents treillis MV avec différents niveaux de granularité ce qui permet l'exploration progressive de données Messai et al. (2010).
Dans ce travail nous nous appuyons sur l'ACFS pour définir une approche de classification et d'analyse de données de simulation. Cette approche prend en compte les deux aspects quantitatif (les valeurs numériques des paramètres) et qualitatif (le confort dans la cabine) des données de simulation afin d'aider les concepteurs dans leurs choix de conception de systèmes complexes. L'approche proposée est appliquée à un cas test fourni par les partenaires industriels du projet et qui correspond à un système de ventilation d'une cabine d'avion. Les données de simulation relatives à ce cas test ont été classées et analysées à l'aide des treillis MV pour identifier les configurations de paramètres de conception qui assurent le confort des passagers dans la cabine. L'analyse est facilitée par deux techniques de visualisation de treillis MV proposées dans ce travail.
Le reste de l'article est organisé comme suit : La section 2 présente le contexte d'étude et le cas test cabine l'avion. La section 3 rappelle les définitions de base de l'ACFS. La Section 4 décrit brièvement deux techniques de visualisation de treillis MV pour l'aide à la décision. La Section 5 discute les détails de l'application de l'ACFS au cas test, les résultats obtenus et l'évaluation des ces résultats. La section 6 conclut ce travail.
2 Contexte de l'étude : la conception collaborative de systèmes complexes
Le présent travail de recherche s'inscrit dans le cadre du projet CSDL 1 qui implique 27 partenaires industriels et académiques et vise à fournir un environnement collaboratif pour la conception de systèmes complexes. Les différentes solutions proposées sont étudiées à travers une phase de simulation qui produit des données volumineuses. La plate-forme visée par le projet devrait permettre une analyse efficace de telles données afin d'identifier les configurations pertinentes des paramètres qui permettent de valider les choix de conception du système.
FIG. 1 -Paramètres de conception du cas test cabine d'avion (gauche) et leurs intervalles de valeurs pour les simulations (droite).
Les partenaires industriels du projet ont fourni un cas test qui correspond à un système de climatisation d'une cabine d'un avion de ligne. Pour ce cas test, l'objectif est d'identifier les configurations de conception pertinentes qui assurent des conditions de confort en terme de température et de vitesse de l'air à l'intérieur de la cabine. Les intervalles de variations des paramètres de conception dans ce cas test ainsi que l'ensemble des simulations sont obtenus en utilisant le modèle de calcul à éléments finis proposé dans Bui et al. (2011). Les 13 paramètres de conception suivants sont considérés dans ce cas test : les angles d'injection de l'air au niveau des 4 sièges des passagers (Alpha_1..4), la vitesse de l'air (Uair_1..4), la température de l'air soufflé à l'entrée principale (Tair_In), la température de l'air soufflé par le ventilateur principal (Tair_P), la vitesse de l'air soufflé à l'entrée principale (Uair_In), la température extérieure (T_Ext), et la conductivité du fuselage thermique (Kappa_F). Les valeurs moyennes de température et de vitesse pour chacun des sièges des quatre passagers ( Figure 1) ont été calculées pour évaluer le confort des passagers. Cela a abouti à huit critères de sortie (deux par passager) liées au confort auxquels s'ajoute une mesure de l'énergie consommée par le système de climatisation utilisée pour estimer le prix associé au confort.
Dans le reste de cet article, nous proposons une approche basée sur l'ACFS pour la classification et la visualisation des données de simulation relatives au cas test cabine d'avion.
3 Introduction à l'Analyse de Concepts Formels par Similarité Messai et al. (2008Messai et al. ( , 2010) est une méthode de classification et d'analyse des données qui étend l'ACF à des données complexes représentées par des contextes multi-valués  
Pour un seuil de similarité ?, l'ensemble de tous les intervalles de valeurs similaires possibles qui peuvent être définis sur W , noté par I ? , est l'ensemble des intervalles de la forme
Le choix de ? reflète les exigences en précision à prendre en compte lors de l'analyse des données. Un faible seuil ? signifie que seules les valeurs les plus proches seront considérés comme similaires tandis qu'un ? plus élevé signifie que des valeurs dont la différence est élevée peuvent également être considérées comme similaires. Le choix du seuil dépend ainsi fortement des données à analyser et de l'objectif de l'analyse. Il est possible de choisir soit le même seuil de similarité pour tous les attributs d'un contexte soit un seuil distinct pour chaque attribut. Dans ce dernier cas, ? est un vecteur (? i ) 0?i<|M | de seuils élémentaires correspondant aux attributs du contexte.
En s'appuyant sur la similarité entre les valeurs des attributs, l'ACFS étend la définition de partage d'attributs entre objets comme suit. Etant donné deux objets 
Plus généralement, l'ensemble maximal valide contenant A par rapport à B ? M est : En s'appuyant sur ces ensembles maximaux, l'ACFS définit les opérateurs de dérivation suivants pour A ? G et B ? M × I ? :
A ? est l'ensemble maximal d'attributs MV partagé par tous les objets dans A et B ? est l'ensemble maximal d'objets qui partage tous les attributs MV dans B. Il a été démontré dans Messai et al. (2008Messai et al. ( , 2010 
Dans l'exemple de contexte MV dans le Tableau 1, et pour ? = (10, 10, 10, 10, 1, 1) :  (A, B)  
Visualisation de Concepts Multi-Valués
Afin de faciliter l'identification des concepts MV d'intérêt à partir du diagramme de Hasse, nous proposons deux techniques suivant la stratégie consistant à montrer une vue d'ensemble d'abord et de zoomer et filtrer par la suite sur demande Keim et al. (2008).
La première technique de visualisation consiste à filtrer et attribuer des couleurs en fonction des mesures des scores définis par l'utilisateur. Dans la Section 2, nous montrons comment certains intervalles de la température et la vitesse de l'air sont utilisés pour déterminer la classe de confort dans la cabine conformément aux normes internationales. Un dégradé de couleurs est attribué en fonction du "score global" d'un concept défini par l'utilisateur. Par exemple, le score pour le "maximum de confort" est attribué quand les scores de vitesse et la température et sont égaux à 2 (voir Section 5.2). Dans ce cas la couleur correspondante est à la borne supé-rieure du dégradé (par exemple rouge, comme dans la Figure 4). Les concepts avec des scores inférieurs sont de couleur jaune. Cette simplification est importante dans le cas de conception des systèmes complexes, où le nombre de paramètres est souvent élevé, parce qu'elle donne une vue d'ensemble qui permet de comparer les concepts et d'identifier rapidement les plus pertinents. Alternativement, l'utilisateur peut filtrer les concepts qui sont en dessous d'un seuil de score. Ceci est particulièrement important lors de l'extraction des classes de confort.
Lorsque les concepts pertinents sont identifiés, la deuxième technique de visualisation intervient pour aider à identifier les intervalles dans ces concepts. Cette technique que nous proposons s'appuie sur un "heat map" conceptuel où chaque concept est représenté sous la forme d'une ligne de rectangles (Figure 3). Chaque rectangle représente un attribut, sa couleur indique la position dans l'intervalle où la valeur de l'attribut se situe dans une échelle chromatique continue du bleu au rouge. La largeur de chaque rectangle est proportionnelle à la taille de l'intervalle. Si un attribut n'est pas présent dans le concept, le rectangle correspondant est montré vide afin de maintenir l'ordre cohérent des attributs. Figure 2. Les couleurs indiquent le score de l'attribut sur une échelle de couleurs (du bleu au rouge) et la largeur indique la taille de l'intervalle de valeurs de l'attribut.
FIG. 3 -Visualisation du treillis MV de la
Application de l'ACFS au cas test cabine d'avion
Contraintes pour guider l'exploration des données de simulation
Dans la section précédente nous avons présenté une formalisation de l'ACFS sur des données numériques. Cependant, cette approche est générique et peut en conséquence être appliquée à d'autre types de données Messai et al. (2010) auquel cas des mesures de similarité relatives à ces données doivent être utilisées. En s'appuyant sur la définition de ces mesures de similarité, l'ACFS peut être appliquée en suivant l'intuition de "grouper ensemble les données similaires". Comme montré précédemment, cette opération nécessite également le choix de seuils se similarité. La variation du seuil de similarité entraine la variation des treillis MV obtenus en terme de nombre de granularité des concepts MV.
Dans ce travail nous sommes intéressés par l'étude des résultats de simulation des paramètres de conception d'un système afin d'identifier les configurations des paramètres d'entrée qui produisent des paramètres de sortie répondants à des contraintes prédéfinies. Pour cela nous avons exprimé ces contraintes à travers les seuils de similarité afin de guider le processus de classification de l'ACFS. Dans l'exemple détaillé précedamment (Table 1 et  Figure 2) le choix de ? = (10, 10, 10, 10, 1, 1) suit l'idée de définir des contraintes sur les paramètres de sortie T 1 et V 1 afin d'extraire les intervalles de variation des paramètres d'entrée T _Ext, T air_In, T air_P et U air_In. En effet, les seuils ? i = 10 définis pour les paramètres d'entrée dé-passent la différence maximale entre les valeurs de chacun de ces paramètres. Cela signifie que toutes les valeurs d'un même attribut peuvent être considérées comme similaires et qu'en conséquence il n'y a pas de contrainte de classification effective définie sur ces valeurs. Cependant la valeur ? i = 1 pour T 1 et V 1 signifie que les valeurs de T 1 (respectivement V 1) ne peuvent être regroupées ensemble dans un même concept MV que si leur différence ne dé-passe pas 1. Cette contrainte est exprimée afin de n'obtenir dans le treillis MV à construire que des concepts où la variation maximale des valeurs de T 1 et de V 1 ne dépasse pas 1 et où il n'y a pas de contrainte sur la variation des autres paramètres. Le treillis obtenu permet ainsi d'obtenir directement les intervalles de variation des paramètres d'entrée correspondant à certaines valeurs de T 1 et de V 1. Par exemple, le concept MV dont l'extension est {2, 4} dans la 
Données de simulations relatives au cas test cabine d'avion
Dans la suite de cet article nous étudions les données de simulations relatives au cas test cabine d'avion introduit précédemment. Le jeu de données étudié correspond aux résultats de simulation de 100 configurations. Chaque configuration correspond à une valeur par paramètre d'entrée (13 en tout) prise dans l'intervalle de variation correspondant (voir Figure 1). Pour chaque configuration de paramètre d'entrée les valeurs de simulation de 9 paramètres de sorties sont calculées pour étudier le confort dans la cabine ainsi que l'énergie dissipée pour assurer ce confort. Ces paramètres de sortie sont la température moyenne et la vitesse de l'air dans chacun des 4 sièges de passager dans la cabine (T 1 à T 4 et V 1 à V 4) et l'énergie dissipée.
Afin de faciliter l'identification des configurations correspondant au confort au niveau des sièges de la cabine, nous avons calculé des scores de confort en fonction des valeurs de la température et de la vitesse de l'air au niveau de chaque siège. Nous nous sommes appuyé dans ce calcul sur les standards ANSI/ASHRAE ASHRAE (2004) pour définir trois niveaux de confort (0 : inconfortable, 1 : acceptable et 2 : confortable) comme suit :
Nous avons remplacé dans le jeu de données de simulation étudié les valeurs de T et de V par les scores correspondants avant de procéder à la classification en utilisant l'ACFS.
Extraction des classes de confort et des configurations de paramètres d'entrée correspondantes
L'objectif de notre étude est de déterminer les paramètres d'entrée qui permettent d'obtenir des situations confortables pour les passagers dans la cabine. Partant du fait que la température dans la cabine est plus importante que la vitesse de l'air dans la définition du confort des passagers, nous centrons notre analyse sur les simulations qui permettent d'avoir les températures qui correspondent au confort maximal (score(T) = 2). Nous notons cet ensemble de simulations par S silver et nous appliquons l'ACFS à cet ensemble en suivant la stratégie détaillées précédemment qui consiste à définir des contraintes en terme de seuil de similarité uniquement pour les paramètres de sorties T et V . Etant donné que nous souhaitons identifier des classes de conforts correspondantes aux trois scores de température et de vitesse définis précédemment, nous avons choisi ? = 0 pour ne regrouper que les expériences donnant le même score pour ces paramètres de sortie. Le treillis MV obtenu est donné dans la 
Evaluation des résultats obtenus à travers de nouvelles simulations
Afin d'évaluer les résultats obtenus nous avons procédé à de nouvelles simulations en considérant comme intervalles de variation des paramètres d'entrée ceux extraits pour les différentes classes de confort et données dans le Tableau 2. Nous avons d'abord effectué une série de 12 simulations à partir des intervalles extraits pour la classe de confort maximal. Les 12 simulations ont produit des températures et des vitesses de score 2 (T entre 22.5
• C et 23.5
• C V ? 0.2) pour chacun des 4 sièges et correspondent ainsi à la classe de confort maximal. Le treillis MV construit pour ces simulations est constitué d'un seul concept correspondant à la classe de confort maximal et les intervalles de variation des paramètres d'entrée extraits à partir de ce treillis sont donnés dans le Tableau 3.
Les colonnes 2 et 3 du Tableau 3 correspondent respectivement aux deux classes de confort maximal extraites à partir des S Silver puis des 12 nouvelles simulations. La comparaison entre ces deux colonnes montre que les intervalles de valeurs des paramètres d'entrée sont approximativement les même dans les deux cas. En sachant que dans les deux cas ces configurations TAB. 3 -Les intervalles de valeurs extraits pour les 13 paramètres d'entrée à partir des résultats des nouvelles simulations.
ont aboutis à des situations de confort dans la cabine, nous pouvons déduire qu'il y a une convergence des résultats. Les intervalles extraits pour la classe de confort peuvent donc être retenus comme solution optimale possible pour le problème de choix de configuration assurant le confort dans la cabine.
Nous avons procédé de la même manière pour évaluer les résultats obtenus pour les autres classes de confort identifiées. 12 nouvelles simulations ont été effectuées pour chaque classe de confort. De la même manière, ces simulations ont également permis de vérifier la convergence des résultats obtenus. Ces résultats montrent l'utilité de l'approche présentée pour assister le concepteur de système dans le choix des meilleurs configurations possibles des paramètres de conception parmi les données de simulation de ces paramètres. La généricité du formalisme de l'ACFS rend l'approche proposée adaptable et réutilisable pour répondre à des besoins similaires dans d'autres contextes.
Conclusion
Dans cet article nous avons présenté une approche qui s'appuie sur les structures conceptuelles pour assister les concepteurs de systèmes complexes dans le choix des configurations pertinentes des paramètres de conception de leurs systèmes. L'approche proposée s'appuie sur l'ACFS pour étudier le confort des passagers dans une cabine d'avion. Le confort est étudiée à travers la simulation de la température et de la vitesse de l'air au niveau de 4 sièges de passager dans la cabine. L'utilisation des l'ACFS et le choix approprié des seuils de similarité des valeurs de paramètres ont permis d'identifier les principales classes de confort à partir des données de simulation et d'extraire pour chaque classe de confort les configurations des paramètres d'entrée qui permettent d'assurer le niveau de confort correspondant. L'identification des classes de confort a été facilitée par deux techniques de visualisation de treillis MV adap-

Résumé
L'analyse formelle de concepts (AFC) est un formalisme de représentation et d'extraction de connaissance fondé sur les notions de concepts et de treillis de concepts (Galois).
L'AFC a été exploitée avec succès dans plusieurs domaines en informatique tels le génie logiciel, les bases et entrepôts de données, l'extraction et la gestion de la connaissance et dans plusieurs applications du monde réel comme la médecine, la psychologie, la linguistique et la sociologie.
Dans cette présentation, nous allons explorer le potentiel de l'AFC et de quelques extensions de cette théorie (ex. analyse triadique de concepts) dans l'analyse de réseaux sociaux en vue de découvrir des connaissances à partir de réseaux homogènes simples (ex.  Elle collabore avec quelques équipes de recherche en France, dont le LIMOS de l'université Blaise Pascal, le laboratoire ERIC de l'université Lumière Lyon 2, et le laboratoire d'informatique de l'université François Rabelais.

Introduction
La Caisse Nationale des Allocations Familiales (CNAF), branche "famille" de la sécu-rité sociale française, gère un réseau régional de Caisses d'Allocations Familiales (CAF) dont l'objectif est de venir en aide aux familles et aux personnes en difficulté financière, pour des raisons de santé, familiales ou professionnelles. A ce titre, elles versent différentes prestations à leurs allocataires dans quatre grands domaines : le logement, la naissance du jeune enfant, l'entretien de la famille et la garantie de revenus. Dans un souci d'amélioration de la qualité de service, la CNAF veut mettre en oeuvre une politique nationale de gestion des réclamations.
Selon un travail préliminaire réalisé par la CNAF, une réclamation est définie comme "tout mécontentement exprimé à l'égard d'une décision, d'une procédure ou d'un service de la Caisse d'Allocations Familiales, quelle qu'en soit la forme, et pour lequel une réponse est explicitement ou implicitement attendue". Dans une logique marketing, la gestion des réclama-tions est un élément fondamental dans la gestion de la relation client (Customer Relationship Management ou CRM), comme le soulignent (Stauss et Seidel, 2004 
FIG. 1 -Processus général d'analyse automatique des réclamations.
Dans ce cadre, le travail confié au laboratoire de recherche ERIC consiste à étudier de façon exploratoire les opportunités offertes aujourd'hui par les techniques de fouille de données, et en particulier de fouille de textes, pour réaliser une analyse automatique des réclamations envoyées par les allocataires. Des travaux préalables concernant la gestion des réclamations (complaint management en anglais) ont été listés dans la littérature -voir à ce sujet le large panorama proposé par (Ngai et al., 2009) -, mais très peu d'efforts semblent avoir été entrepris pour traiter ce problème spécifique. Il est intéressant de noter que (Bae et al., 2005) ont déjà essayé d'utiliser des cartes auto-organisatrices (Self-Organized Maps ou SOM, c.f. (Kohonen, 2001)), c'est-à-dire une technique issue de l'apprentissage non supervisé.
La démarche adoptée dans cette étude exploratoire est illustrée dans la figure 1. Elle se décompose en quatre étapes :
Etapes 1 et 2 pour -identifier les documents contenant les réclamations et les rendre exploitables en vue d'analyses automatiques, -récupérer les données disponibles sur les allocataires ayant rédigé ces réclamations. Etapes 3 et 4 pour -à partir des informations sur les allocataires réclamants, établir une typologie des récla-mants en utilisant des techniques classiques d'analyse des données, -à partir du contenu textuel des documents, établir une typologie des réclamations sur la base d'analyses statistiques et sémantiques, -croiser les types de réclamations et les caractéristiques des allocataires afin de définir des groupes d'individus au comportement homogène en matière de réclamation.
Cet article s'organise comme suit. Tout d'abord, nous présentons en détail dans la section 2 les données issues d'un échantillon des réclamations envoyées par des allocataires à leur CAF. Nous donnons à cette occasion les différents prétraitements qui ont rendu les analyses ultérieures possibles. Dans la section 3, nous présentons les techniques de fouille de données qui ont été choisies pour construire les deux typologies. La section 4 donne les résultats des deux typologies obtenues, ainsi que la mise en correspondance réalisées entre la typologie des allocataires réclamants et celle des réclamations. Enfin, la section 5 propose une conclusion à ce travail, ainsi que des pistes d'études futures.
2 Préparation du jeu de données 2.1 Données brutes de la CAF Les allocataires peuvent adresser des réclamations à leur CAF par différents canaux, allant de l'appel téléphonique à la lettre manuscrite en passant par le mail, le(s) site(s) internet ou la lettre dactylographiée. L'un des objectifs de ce travail étant l'automatisation des traitements, il a fallu déterminer quels formats de données rendaient possible une analyse automatique. Concernant les courriers reçus par voie postale, il a été décidé d'identifier automatiquement les courriers dactylographiés pour ne retenir que ceux-ci et d'écarter les courriers manuscrits. Tous les courriers électroniques via différents sites Web ont été retenus. Une partie des documents fournis n'ayant pas été identifiée comme des réclamations, il a fallu mettre en place une procédure pour discriminer automatiquement les réclamations à partir de leur contenu textuel. Cette procédure est brièvement présentée dans la section qui suit.
Les données transmises par la CAF du Rhône pour ce travail sont des réclamations récep-tionnées entre janvier 2010 et mars 2012. Parmi les 174000 documents parvenus à la CAF pendant cette période, seul un échantillon de 12534 documents a pu être traité dans la durée de l'étude. Parmi ces 12534 documents, 2385 documents ont été identifiés automatiquement comme étant des réclamations ; ils ont donc été retenus pour les analyses ultérieures. Chaque document contient un texte, en général plutôt court et aisément identifiable à l'aide de techniques automatiques. De plus, il peut être associé aux informations ou données qui décrivent l'allocataire au moment de la réclamation, comme l'état civil (civilité, sexe, âge) et des variables construites par les agents de la CAF. Au final, 39 variables descriptives ont été sé-lectionnées initialement pour l'étude. Un allocataire qui réclame plusieurs fois voit son profil dédoublé comme s'il s'agissait de plusieurs personnes différentes, le profil pouvant en effet varier au fil du temps.
Prétraitements des données
Dans cette partie, nous détaillons les traitements nécessaires pour transformer les données brutes brièvement décrites dans la section précédente dans un format propice à l'emploi d'algorithmes de fouille de données. Ces traitements représentent au moins 70% de l'effort investi dans ce travail, ce qui justifie la place qui leur est dédiée dans cet article.
Concernant les informations liées à la description des allocataires réclamants, nous avons sélectionné 19 variables parmi les 39 initiales mises à notre disposition. Cette sélection permet de limiter la redondance de l'information (par exemple, plusieurs variables traitent du nombre d'enfants dans le foyer) et d'éviter à ce que le nombre de valeurs manquantes soient trop éle-vées (des nouvelles variable descriptives ont été créées à partir de 2011). Afin de permettre l'emploi des méthodes d'analyse des données présentées dans la section suivante, certaines variables qualitatives ont été recodées et les variables continues ont été discrétisées. La plupart du temps, cette discrétisation se base sur des intervalles définis antérieurement par la CAF.
Concernant les réclamations proprement dites, le travail de prétraitement des documents a été plus important. Il se décompose en six grandes étapes que nous listons ci-dessous.
Reconnaissance des caractères. La diversité dans la forme des documents fournis (dactylographiés, électroniques. . .) implique d'utiliser un OCR (Optical Character Recognition) afin de transformer les images de certains documents fournis au format .pdf ou .tiff dans un format texte. Après une revue de plusieurs solutions logicielles existantes sur le marché, nous avons opté pour le logiciel ABBYY FineReader 1 . Celui-ci nous a permis d'obtenir des taux de bonne reconnaissance très corrects lorsque les documents sont dactylographiés : le logiciel commet entre 2,5 et 10% d'erreur dans la reconnaissance des caractères ; erreur estimée en comparant (sur la base d'un échantillon de 50 documents) le texte extrait manuellement par nos soins au texte automatiquement reconnu.
Distinction entre écriture manuscrite et dactylographiée. La deuxième étape consiste à ne conserver que les documents dactylographiés afin d'assurer une qualité minimale aux textes reconnus à partir des images. Cette distinction a été simple à réaliser en fixant un seuil de 70% au nombre minimum de chiffres ou de lettres contenus dans un document. Lorsque ce seuil est dépassé, c'est-à-dire lorsque d'autres symboles sont trop fréquents statistiquement (comme ' ?', ' !' ou #'), il est clair que la reconnaissance a échoué. Le taux de reconnaissance des documents dactylographiés est alors de 100% sur notre échantillon.
Extraction du texte des réclamations. Les fichiers textes issus de l'étape d'OCR contiennent tous les éléments présents dans le document d'origine. Le texte de la réclamation est donc situé au milieu des informations personnelles de l'allocataire. Le but de cette troisième étape est de réussir à ne conserver que le texte ou le corps de la réclamation. Pour ce faire, nous utilisons des expressions régulières car elles permettent de rechercher des chaînes de caractères caractéristiques. Pour les réclamations issues du site web national de la CAF, par exemple, cet exercice s'est avéré aisé grâce à la présence dans le document d'une balise "Message :".
Correction du texte. Des erreurs peuvent être introduites dans le texte, imputables à l'allocataire lui-même ou à l'algorithme d'OCR. Afin de corriger certaines de ces erreurs, nous avons mis en place une étape de correction automatique. Celle-ci se base sur deux mécanismes. Tout d'abord, nous avons recours à un lexique de mots de la langue française (le dictionnaire Gutemberg 2 ), à une liste de mots utilisés à la CAF (par exemple "APL" et "AAH" qui correspondent à des prestations spécifiques), et à une liste de néologismes détectés manuellement (par exemple "sms" ou "tweet"). Les mots de la réclamation sont comparés à ce vocabulaire et directement indexés s'ils s'y trouvent : nous les appelons les mots reconnus. Le deuxième mécanisme consiste à comparer les mots non reconnus avec le vocabulaire en calculant une distance entre ces mots. Dans notre cas, nous avons employé la distance de Levenshtein (Lcvenshtcin, 1966), normalisée par la taille du mot à corriger afin de ne pas pénaliser les mots trop longs (Cohen et al., 2003), puis la distance de Jaro-Winkler (Corston-Oliver et Gamon, 2004) en cas d'ex-aequo. Après plusieurs tentatives, nous avons fixé un seuil de 0,2 à cette distance, ce qui permet par exemple de corriger "prochainemant" en "prochainement" (score de 0,077), mais pas "prme" en "prime" (score de 0,25) ou "priscilla" en "principal" (score de 0,444). Même s'il arrive que certaines corrections amènent à introduire des erreurs, les corrections effectuées avec le seuil de 0,2 sont la plupart du temps pertinentes.
Identification des réclamations. Les documents transmis par la CAF du Rhône ne sont pas tous des réclamations et une étape d'identification est nécessaire. Habituellement, ce travail est réalisé manuellement par un technicien de la CAF à l'aide d'une note de service qui définit la notion de réclamation ainsi que des critères ou des expressions pour les repérer. Ainsi sont considérés comme réclamations les textes contenant des tournures de phrases exprimant une incompréhension, une protestation ou une contestation. Il a été nécessaire d'automatiser cette étape en nous basant sur la note de service. Brièvement, des expressions comme "je ne comprends pas votre décision" ont été remplacées par l'association des mots "comprend", "pas" et "décision". C'est la présence d'une majorité de mots clefs associés à l'une des expressions typiques qui permet d'indiquer automatiquement qu'un texte relève ou non d'une réclamation. Cette méthode a permis d'automatiser le processus de discrimination afin de ne retenir que des réclamations. Bien sûr, une partie des textes a été retenue à tort. Pour estimer cette proportion, 100 documents ont été tirés au hasard. Un expert humain a lu les documents, a vérifié pour chaque document s'il s'agissait ou non d'une réclamation et a comparé avec l'identification faite automatiquement par la machine. 76% des réclamations sont retrouvées par la machine (score de rappel) et 73% des réclamations retenues par la machine en sont bien (score de pré-cision). Sachant qu'un agent de la CAF est également susceptible de retenir ou d'écarter à tort des documents, le taux de reconnaissance automatique des réclamations a été jugé acceptable mais des travaux plus poussés devraient permettre d'améliorer les performances (voir la section 5).
Suppression des mots-outils et stématisation. Afin d'optimiser les résultats des méthodes statistiques et sémantiques que nous abordons dans la section prochaine, il est souvent préfé-rable de retirer préalablement des textes les "mots outils" (stopwords), c'est-à-dire des mots fréquemment utilisés dans la langue française pour construire les phrases, comme "et", "avec", etc. Des listes préconstruites de mots outils sont disponibles pour la plupart des langues, et en particulier pour le français 3 . Une deuxième technique permet de restreindre le nombre de mots en supprimant les préfixes et les suffixes, et ce afin de se rapprocher du radical des termes. Cette technique est appelée "stématisation" (stemming en anglais), elle ne doit pas être confondue avec la lemmatisation. Dans notre cas, elle nous permet de diminuer de manière significative le nombre de termes présents dans notre vocabulaire : les 10 248 termes présents initialement dans les textes du projet ont été ramenés à 7256 formes stématisées, soit une diminution de 29% de la taille du vocabulaire.
Analyse des réclamations
Dans cette partie, nous motivons le choix des techniques employées pour analyser les ré-clamations, puis nous donnons le protocole expérimental que nous avons suivi pour obtenir les résultats présentés dans la section suivante.
Choix des techniques de fouille de données
Combiner ACM et CAH Afin de construire une typologie des allocataires qui font une réclamation, nous proposons d'utiliser l'Analyse des Correspondances Multiples (ACM) en combinaison avec une Classification Ascendante Hiérarchique (CAH), techniques éprouvées issues de l'analyse des données (Lebart et al., 1995).
Brièvement, l'ACM est une méthode de décomposition factorielle qui fournit une repré-sentation graphique synthétique d'une grande quantité de données décrites par des variables qualitatives. Elle synthétise l'information, met en évidence les informations intéressantes ainsi que les liens qui les caractérisent et aboutit à la création d'axes factoriels pour représenter les individus et/ou les modalités. En revanche l'ACM ne fournit pas de typologie proprement dite. Pour construire la typologie, nous appliquons la CAH avec le critère de Ward. La classification permet de faire émerger des groupes ou classes d'allocataires au profil semblable. Le nombre de classes est sélectionné à partir du plus grand "saut" constaté dans le critère de Ward et en faisant appel à un expert métier pour attester du meilleur niveau de granularité.
L'intérêt d'effectueur une classification après une méthode factorielle est double : (1) l'ACM procède à une réduction du nombre de variables. La classification est faite avec les axes factoriels issus de la méthode factorielle et non pas avec les variables d'origine ; (2) sur le graphique de l'ACM représentant les individus (ici les allocataires réclamants), il est possible de visualiser l'appartenance des individus aux classes, c'est-à-dire la typologie des allocataires.
Extraction de thématiques L'un des objectifs de ce projet est de construire une typologie des réclamations de manière automatique. La démarche générale consiste à travailler directement à partir du contenu textuel des documents en utilisant le moins d'information a priori, ce afin de faire émerger des groupes ou catégories de réclamations homogènes, que nous appellerons des thématiques (topics en anglais). Les textes sont placés dans la même thématique à partir du moment où leur auteur emploie un vocabulaire similaire, et dans des thématiques différentes lorsque le vocabulaire employé est différent.
Il existe plusieurs méthodes d'extraction des thématiques : par exemple les modèles à base de distance (Velcin et Ganascia, 2007), inspirés de l'algorithme classique des c-moyennes, les modèles reposant sur la factorisation de matrices de type LSA (Deerwester et al., 1990) ou NMF (Paatero et Tapper, 2006) , ou les modèles graphiques basés sur la statistique bayésienne (Steyvers et Griffiths, 2007). Relevant de ce dernier type, le modèle qui a été choisi pour cette étude est LDA (Latent Dirichlet Allocation), proposé par (Blei et al., 2003). Il a été choisi car il s'agit d'un modèle aux performances reconnues qui a déjà été appliqué à de nombreux corpus de natures variées (voir par exemple (Bíró et al., 2008)).
La méthode LDA utilise les principes des modèles graphiques et de la statistique bayé-sienne, appliqués aux données textuelles. Elle se base sur une représentation en "sac de mots" (bag of words) où un document est traité comme un ensemble de mots dont la position dans le texte n'est pas prise en compte. Cette hypothèse simplificatrice entraîne une perte dans la précision des résultats de l'analyse, mais elle rend possible le traitement de corpus volumineux.
Le modèle calculé par LDA contient un ensemble de catégories (les thématiques) et précise comment les documents sont répartis sur ces catégories. Chaque catégorie correspond à une thématique décrite comme une distribution sur l'ensemble du vocabulaire de mots choisi. La thématique est souvent caractérisée par les mots clefs qui contribuent le plus à la catégorie (top keywords). L'étiquetage (nommage) des catégories peut s'effectuer en observant les expressions que l'on peut reconstituer à partir de ces mots clefs et en s'aidant des textes du corpus qui contiennent ces expressions. Par exemple, voici une liste de mots clefs les plus pertinents pour illustrer une thématique qui pourrait être obtenue : "réponse", "allocation", "enfants", "montant", "logement", "situation", "été", "part", "aide", "allocataire". En se basant sur ces mots et sur les textes des réclamations, on retrouve des expressions telles que : "montant [de l'] allocation logement", "montant [de l'] aide [au] logement", ou encore "réponse [de votre] part". Cette thématique couvre des textes tels que le suivant : "Me mr comme je vais déménager le 1er mai 2010 je souhaite que l'aide au logement d'avril 2010 soit versée sur mon compte. Le loyer d'avril a été intégralement payé au propriétaire. L'aide au logement d'avril me permettra de compléter le paiement du loyer du nouveau logement. Je perçois toujours l'ASS, la dédite a été déjà envoyée au propriétaire je passerai pour remplir un nouveau dossier."
Protocole expérimental
Concernant la typologie des réclamations, les expérimentations ont été réalisées en deux temps. Il s'agit tout d'abord de réduire le vocabulaire en introduisant deux méthodes de filtrage des mots. Ensuite viennent les expérimentations proprement dite pour lesquelles se pose la question de la sélection du nombre de catégories thématiques.
Première étape : réduction du vocabulaire. Il est nécessaire de réduire encore la taille du vocabulaire qui contient encore 7256 mots après l'étape de stématisation. Pour cela, deux filtres sont employés et permettent de réduire le vocabulaire à 364 mots.
Le premier filtre consiste à éliminer du vocabulaire les mots qui apparaissent trop peu dans le corpus. En effet, ces mots apportent du "bruit" dans l'analyse alors qu'ils n'apportent rien dans la constitution des catégories thématiques. La "rareté" du mot (sparsity en anglais) est simplement calculée en prenant le ratio du nombre de documents contenant le mot par le nombre total de documents dans le corpus. Chaque mot est donc associé à une valeur numé-rique située entre 0 et 1 : plus la valeur est proche de 0, plus le mot est rare et doit être ignoré. Il faut donc définir un seuil ? s en dessous duquel les mots sont retirés du vocabulaire et donc de l'analyse.
Le second filtre consiste à éliminer du vocabulaire les mots trop fréquents dans les textes, mais dans des proportions similaires, et qui peuvent être assimilés à des mots outils. Pour cela, nous nous sommes basés sur la mesure classique de TF.IDF qui est le produit entre la fréquence du mot dans un texte en particulier (Term Frequency ou TF) et le logarithme de l'inverse de la fréquence du mot dans tous les documents du corpus (Inverse Document Frequency ou IDF). Cette mesure est d'autant plus proche de zéro que le mot est peu fréquent dans un texte et qu'il est au contraire présent tout au long du corpus. Là encore, il s'agit de définir un seuil ? T en dessous duquel le mot est retiré de la description du texte. Attention, contrairement à la mesure précédente, le mot est éliminé d'un texte, et non pas nécessairement du vocabulaire dans son ensemble. En effet, il se peut qu'un mot soit exceptionnellement fréquent dans un texte (score de TF élevé), ce qui compense le fait qu'il se trouve dans tout le corpus (score de IDF faible).
Etant donné un nombre de catégories thématiques choisi à l'avance, il faut être en mesure de déterminer les meilleures valeurs pour les deux seuils ? s et ? T . Pour cela, nous avons fait varier ces valeurs, ainsi que le nombre de catégories, et nous avons calculé à chaque fois le modèle LDA associé sur un échantillon d'apprentissage en relançant 20 fois l'algorithme afin de faire varier son initialisation (et donc réduire la chance de tomber dans un optimum local). L'échantillon d'apprentissage est constitué des deux tiers des 2385 réclamations de l'analyse. La mesure de perplexité, que l'on trouve dans l'article de (Blei et al., 2003), permet de confronter les modèles obtenus à un échantillon de test en se basant sur un calcul de vraisemblance. Pour ne pas être biaisée par la taille du vocabulaire qui varie en fonction des seuils utilisés, cette mesure doit être normalisée par la taille du vocabulaire (Corston-Oliver et Gamon, 2004). Il est ainsi possible de sélectionner automatiquement les seuils qui mènent aux résultats qui optimisent cette mesure, sans avoir recours à une expertise humaine.
Deuxième étape : calcul du modèle. Une fois déterminées les valeurs optimales pour les seuils permettant de filtrer le vocabulaire, il reste le problème de trouver le nombre "optimal" de catégories thématiques. Ce problème est lié à celui, très classique en apprentissage non supervisé, de déterminer le nombre optimal de classes (clusters). Pour ce faire, nous avons tout d'abord calculé la mesure de perplexité en faisant varier le nombre de catégories de 2 à 20, avec une relance de 20 fois pour chaque valeur. La courbe obtenue présentant une forme en "U" caractéristique en apprentissage automatique, nous en avons conclu que les valeurs trop faibles ou trop élevées étaient à écarter. Les valeurs trop élevées dégradent les résultats de la mesure de la même manière que pour le phénomène de sur-apprentissage (overfitting) constaté en apprentissage automatique supervisé. Parmi les valeurs intermédiaires restantes, et comme nous sommes dans le cadre d'un travail exploratoire mené sur un cas d'étude réel, nous avons finalement choisi de déterminer le nombre exact de thématiques en ayant recours à des experts métier. 
Typologie des allocataires réclamants
Pour construire la typologie des allocataires réclamants, les dix premiers axes factoriels (qui expliquent 46 % de l'intertie) sont retenus comme variables pour la CAH. Pour la CAH, le critère du "saut" dans la mesure est maximum pour des valeurs de 3, 8 et 5 classes. L'évaluation des experts nous a permis de conclure que la typologie la plus informative est obtenue pour 8 classes d'allocataires. Il est possible de visualiser la typologie des allocataires sur le graphique synthétique de l'ACM en colorant chaque individu selon la catégorie à laquelle il appartient (cf. figure 2). A partir des caractéritiques communes des allocataires constituant chacune des classes, une étiquette est donnée à chaque classe.
A titre d'exemple, Les individus en gris se démarquent des autres par une forte proportion de personnes qui ont entre 20 et 25 ans, qui sont célibataires et qui perçoivent l'allocation au logement sociale. 73 % d'entre eux ont un quotient familial inférieur à 300 et tous ont un revenu brut annuel inférieur à 20 000e. Enfin, aucun d'entre eux n'a d'enfant et, en toute logique, aucun ne touche les prestations familiales. Nous interprétons cette classe comme correspondant à des "étudiants".
Les individus colorés en orange se caractérisent quant à eux par une absence de personnes sans enfants. Plus de 90% d'entre eux ont au moins trois enfants (67% avec trois enfants et 25% avec quatre enfants ou plus). Parallèlement 90% d'entre eux touchent le complément familial et 99% les prestations familiales. Ces allocataires sont âgés de 40 à 50 ans. On trouve très peu de célibataires et de personnes avec un fort quotient familial dans cette classe qui correspond aux "familles nombreuses".
Typologie des réclamations
Le protocole expérimental permet de sélectionner le meilleur modèle en 14 catégories et avec des valeurs de seuil respectivement de 0,98 et 0,03 pour ? s (rareté) et ? T (TF.IDF). L'un des avantages de la méthode LDA est de pouvoir identifier les mots qui caractérisent ces différentes thématiques (mots clefs principaux ou top keywords en anglais) et d'analyser plus qualitativement les thématiques : pour chaque thématique, nous précisons les dix premiers mots clefs, nous proposons une étiquette, et nous donnons une réclamation type dans laquelle nous soulignons la présence d'un ou plusieurs mots clefs.
A titre d'exemple, dans la première catégorie se retrouvent les mots-clefs suivants : dossier, demande, droit, familiales, réponse, allocation, compte, été, prestations, pourriez. Cette catégorie regroupe donc des documents qui abordent le thème des allocations familiales et on retrouve dans les textes des expressions comme "Allocation familiales" ou "prestations familiales". La deuxième catégorie est aussi relativement marquée ; elle contient des réclamations qui font suite à un changement de situation de l'allocataire avec les mots-clefs suivants : mois, situation, été, caf, informations, allocataire, changement, dossier, reçu, compte. Les récla-mations de cette thématique abordent un "changement de situation" et/ou la prise en "compte [d']informations". Voici un exemple de texte de cette catégorie : "madame monsieur suite au mail que j'ai recu le 20 mars 2010 de la part de la caf de lyon dont la copie se trouve en piece jointe je souhaiterai contester la decision qui a ete prise en effet le fait que je sois en collocation n'a pas ete enregistre par les services de la caf de lyon ce qui a entraine une retenue de mon aide au logement pour les mois de janvier 2010 hauteur de 239 62 euros et de fevrier 2010 hauteur de 45 euros de plus j'ai recu une notification de dette de 807 84 euros suite cette erreur je souhaiterai que ma situation ainsi que mon dossier soient regularise".
Mise en correspondance des typologies
La typologie des allocataires a montré que ces derniers pouvaient être classés en huit groupes avec des caractéristiques différentes. La typologie des réclamations, elle, a fait émer-ger quatorze thématiques. Se pose alors la question suivante : certains de ces thèmes sont-ils le fait d'une catégorie particulière d'allocataires ? Les réclamations sur les prestations familiales ne sont-elles écrites que par les familles nombreuses, par exemple ? Pour répondre à ces questions il est possible de croiser les deux typologies.
Des analyses statistiques basées sur un test d'indépendance (p-value=0,0005) et une analyse factorielle des correspondances simples indiquent qu'il n'y a pas d'indépendance entre le type de réclamations et le type d'allocataires. Il existe en effet des thèmes de réclamations privilégiés selon les catégories d'allocataires. Par exemple, les familles nombreuses semblent réclamer davantage sur les thèmes du changement de situation, de l'allocation logement et du RSA. Les retraités réclament quant à eux principalement sur l'allocation logement (mais dans des proportions semblables aux autres allocataires) ainsi que sur le RSA et le montant des droits. Comme on peut s'y attendre, ils sont sous-représentés dans le thème des allocations familiales, leurs enfants étant des adultes.
En revanche, les analyses statistiques ne permettent pas de conclure qu'une catégorie de réclamation est imputable à une classe particulière d'allocataires. En effet, les personnes en situation de handicap et les célibataires apparaissent presque toujours en première position, mais ceci est dû au fait qu'ils sont plus nombreux que tous les autres allocataires réclamants. L'objet de l'étude présentée dans cet article est d'étudier l'opportunité d'utiliser des techniques de fouille de données pour réaliser une analyse sémantique des réclamations faites par les allocataires. L'étude a été menée dans une optique exploratoire.
Après avoir identifié les différentes sources de données, l'étude a mis en évidence l'importance et la difficulté de la phase de préparation des documents et des données. Nous avons proposé et réalisé une succession d'étapes pour traiter des problèmes de la reconnaissance de caractères, de la sélection des documents dactylographiés, de l'extraction du texte contenant le corps du document, de la correction des fautes d'orthographe, de l'identification des textes qui sont proprement dits des réclamations, et enfin de la construction automatique du vocabulaire de mots pour décrire ces réclamations. En se plaçant dans une optique nationale de traitement automatique des réclamations, nous avons cherché à automatiser ces différentes étapes afin que les agents de la CAF interviennent le moins possible dans l'opération. Nous avons aussi montré que grâce à des techniques de fouille de données, et en particulier de fouille de textes, il est possible de savoir à la fois qui sont les allocataires qui réclament et de savoir sur quelles thématiques portent les réclamations.
S'agissant d'une étude exploratoire réalisée dans un temps assez court, nous pouvons dès à présent lister des améliorations possibles de ce travail avec deux horizons.
A court terme, une amélioration consisterait à compléter les résultats obtenus dans la typologie des réclamations en lançant les algorithmes sur l'intégralité du volume de documents fournis, contenant ou non une réclamation. Une typologie de l'ensemble des documents permettrait de voir si les réclamations se distinguent significativement des autres documents et se retrouvent dans les mêmes catégories ou non. De plus, à l'heure actuelle, la seule étape manuelle du processus proposé est l'étiquetage des thématiques extraites par la méthode LDA. En utilisant des travaux développés dans notre laboratoire, une automatisation est possible et pourrait être intégrée sans rencontrer de grandes difficultés.
A plus long terme, plusieurs extensions peuvent être étudiées. Concernant la discrimination automatique des documents qui relèvent d'une réclamation, la solution proposée s'inspire de la démarche manuelle suivie par les agents de la CAF. Mais un nombre encore trop important de textes qui ne sont pas des réclamations sont retenus à tort. L'identification automatique des réclamations pourrait certainement être améliorée en utilisant d'autres techniques de fouille de données telles que l'apprentissage automatique supervisé. Dans cette étude, les typologies des réclamants et des réclamations ont été extraites de manière synchronique, c'est-à-dire que la dimension temporelle n'a pas du tout été prise en compte. Pourtant, il semble qu'il s'agisse d'un aspect très important dans la gestion d'une réclamation. Une autre extension de cette étude serait alors d'étudier s'il est possible de replacer l'allocataire réclamant dans une chronologie. Des techniques de fouille de données pourraient être utilisées, par exemple, pour extraire des "trajectoires" d'allocataires dans lesquelles s'inscrivent les réclamations.
Références
Bae, S., S. Ha, et S. Park (2005). A web-based system for analyzing the voices of call center customers in the service industry. Expert Systems with Applications 28(1), 29-41.

Introduction
L'Analyse Formelle de Concepts (Ganter et Wille, 1999), notée de manière abrégée AFC, est une méthode de classification automatique d'objets décrits par des attributs au travers d'une relation binaire. Le résultat d'une telle classification est un treillis de concepts (appelé aussi treillis de Galois (Barbut et Monjardet, 1970)) où chaque concept regroupe tous les objets ayant en commun un ensemble d'attributs. On peut naviguer dans le treillis de manière simple et intuitive, des concepts les plus spécifiques (les concepts regroupant beaucoup de caractéristiques partagées par peu d'objets) aux moins spécifiques (les concepts regroupant beaucoup d'objets partageant peu de caractéristiques).
L'AFC est exploitée dans différents domaines en tant que méthode d'extraction de connaissances et les différentes publications sur le sujet, notamment Carpineto et Romano (2004); Valtchev et al. (2004), ont permis d'en identifier les forces et les limites. Certaines de ces limites ont pu être contournées en utilisant différentes approches.
L'Analyse Relationnelle de Concepts (ARC) (Huchard et al., 2007) est une extension de l'AFC qui permet de prendre en compte non seulement les caractéristiques des objets, mais aussi les relations que les objets entretiennent entre eux. L'ARC consiste à appliquer itérati-vement un algorithme de l'AFC pour gérer les données relationnelles : les objets sont décrits par des attributs et par leurs relations vers d'autres objets. Les concepts découverts à une ité-ration donnée sont propagés le long des relations, pour permettre la découverte de nouveaux concepts à l'itération suivante. L'ARC apparaît plus intuitive à utiliser sur des données relationnelles telles que des bases de données ou des langages de modélisation orientés objet comme UML. Nous proposons dans cet article une adaptation de l'ARC en vue de l'utiliser en tant que méthode d'extraction de connaissances sur des données de mesure de qualité de l'eau des cours d'eau d'Alsace.
Ce travail s'inscrit dans le projet ANR FRESQUEAU 1 dont le but est le développement de nouvelles méthodes d'étude, de comparaison et d'exploitation de tous les paramètres disponibles sur les cours d'eau et les plans d'eau. Il approfondit une étude précédente menée avec l'AFC (Bertaux et al., 2009a) et des approches statistiques (Bertaux et al., 2009b).
Propager le long des relations les concepts découverts d'une itération à une autre permet certes la découverte de concepts intéressants, mais engendre souvent une explosion combinatoire, et les motifs intéressants sont difficiles à extraire à partir du grand ensemble de concepts construits. Plusieurs stratégies peuvent être utilisées pour pallier cette complexité, incluant la séparation des objets initiaux en plusieurs sous-ensembles après une analyse préliminaire ou l'introduction de requêtes (Azmeh et al., 2011). Nous nous intéressons dans cet article à l'utilisation de l'ARC pour explorer interactivement les données en laissant l'utilisateur/trice choisir avant chaque itération de l'AFC quels contextes (formels et relationnels) il ou elle veut utiliser.
Les données avec lesquelles nous travaillons ne sont pas initialement sous forme de relation binaire mais de nombreux travaux traitent de l'échelonnement des données en vue d'obtenir une relation binaire (Ganter et Wille, 1999) ou une représentation sous forme de structures de patron (Ganter et Kuznetsov, 2001). Ces approches ont été appliquées précédemment sur des données similaires à celles de notre projet (Bertaux et al., 2009a) et nous considérons par la suite uniquement des données sous forme de relations binaires.
Dans cet article nous allons tout d'abord présenter l'AFC puis le principe général du processus d'ARC pour mettre en évidence les différents points de variations qui permettraient d'en améliorer l'utilisation dans un contexte de fouilles de données. Nous présenterons un exemple du type de données que nous avons dans le projet FRESQUEAU et les conséquences des variations sur ces données. Nous conclurons ensuite par une courte discussion.
Analyse Formelle de Concepts
L'AFC telle que présentée par Ganter et Wille (1999)   L'AFC permet la création de tous les concepts d'un contexte donné, concepts formant un treillis de concepts aussi appelé treillis de Galois. Un concept c 1 est plus général (resp. plus spécifique) qu'un concept c 2 si l'extension de c 1 contient (resp. est contenue par) l'extension de c 2 . De manière duale l'intension d'un concept est contenue par l'intension d'un concept plus spécifique. Deux concepts donnés ont une unique borne supérieure et une unique borne inférieure.
Les treillis sont généralement représentés par leur diagramme de Hasse. Le treillis du tableau 1 est représenté par la figure 1. Les flèches représentent la relation de généralisation, c'est-à-dire que le concept pointé est plus général que le concept d'origine. Compte-tenu de l'inclusion de l'intension d'un concept dans les concepts plus spécifiques et de l'inclusion de l'extension d'un concept dans les concepts plus généraux, chaque objet (resp. attribut) n'est indiqué qu'une fois dans le concept le plus spécifique (resp. le plus général) le contenant. Par exemple le concept 6 regroupe les stations BREI0001 et DOLL001 qui possèdent les attributs petit cours et eau fraîche et vive que l'on retrouve par la relation de générali-sation vers les concepts 2 et 7.
3 Extension de l'ARC pour l'analyse exploratoire L'Analyse Relationnelle de Concepts (ARC) (Huchard et al., 2007) est une extension de l'AFC permettant de prendre en compte, en plus des caractéristiques des objets, les relations existant entre ces objets.
L'algorithme 1 présente les principales étapes de l'ARC. Le paramètre d'entrée de l'ARC est une famille relationnelle de contextes RCF = (K, R) composée de n contextes objet- 
On peut voir dans le tableau 2 un exemple de famille relationnelle de contextes. On peut voir sur la partie gauche deux contextes objetattribut taxons et stations et sur la droite le contexte objet-objet presenceDeTaxon qui relie les objets du contexte stations aux objets de taxons 2 . 
Algorithme 1: Processus de l'Analyse Relationnelle de Concepts. figure 2 présente les deux treillis obtenus après cette étape d'initialisation sur notre exemple. On remarque que la relation presenceDeTaxons n'est pas encore prise en compte à cette étape du processus et les deux treillis sont indépendants.
À l'étape p :
Contextes objet-ottribut Contextes objet-objet 
TAB. 3 -Échelonnement de la relation presenceDeTaxons et extension du contexte stations à l'étape 1. -ETENDRE-REL rajoute à K i les relations obtenues par l'échelonnement des relations dont K i est le domaine. L'échelonnement consiste à inclure les relations objet-objet en tant qu'attributs relationnels. Ils sont obtenus en utilisant les concepts des treillis de l'étape p?1 et un opérateur d'échelonnement (c'est-à-dire ?, ?). Par exemple, si l'opéra-teur d'échelonnement ? est choisi pour échelonner une relation R j donnée, les colonnes de R j sont remplacées par des attributs de la forme ?R j : C, où C est un concept dans le treillis construit à partir des objets de l'image de
Ainsi dans notre exemple nous avons étendu le contexte objet-attribut stations avec le contexte objet-objet presenceDeTaxon échelonné par l'opérateur ?. Le contexte étendu est présenté par la table 3. La station FECH001 est reliée aux concepts 0, 1 et 3 par la relation ?presenceDeTaxon car on y trouve la présence de Athericidae que l'on retrouve dans les concepts 0 et 1 et la présence de Boreobdella que l'on retrouve dans les concepts 0 et 3. Si on utilisait l'opérateur d'échelonnement ?, la station FECH001 serait reliée uniquement au concept 0 par la relation ?presenceDeTaxon car il s'agit du seul concept où l'on retrouve les deux taxons présents dans cette station. -MAJ-TREILLIS met à jour les treillis de l'étape p ? 1 dans le but de produire,
, associé à K i concaténé à tous les contextes objet-objet échelonnés dont K i est le domaine. L'algorithme s'arrête lorsqu'un point fixe est atteint c'est-à-dire lorsque l'on obtient une famille de treillis isomorphe à la famille de treillis obtenue à l'étape précédente et que les extensions des contextes restent inchangées. Dans le cas de notre exemple, les treillis de la figure 3 sont les treillis finaux. Le nombre d'itérations est prévisible lorsque les relations entre contextes ne forment pas un circuit. Mais dans certains cas, par exemple lorsqu'un contexte objet-objet a le même ensemble domaine et image, le nombre d'itérations n'est pas prévisible (seule une borne est connue) et peut être très grand selon les données.
Les treillis relationnels s'interprètent différemment des treillis de concepts classiques car ils doivent être considérés ensemble. Le treillis stations de la figure 3 doit être considéré avec le treillis taxons pour pouvoir être correctement interprété. On trouve dans les intensions de concepts des attributs faisant référence à d'autres concepts. Par exemple Concept_8 possède l'attribut relationnel presenceDeTaxons : Concept_2 ce qui signifie que tous les objets de Concept_8 sont liés par la relation presenceDeTaxons à au moins (car l'opérateur d'échelonnement utilisé est l'opérateur ?) un objet de l'extension de Concept_2 appartenant au treillis taxons.
L'avantage d'un tel processus est que les concepts obtenus ont dans leur intension des relations à d'autres concepts en plus des attributs classiques. Ces relations permettent l'extraction de motifs construits à partir de plusieurs contextes interconnectés, comme cela a été fait dans Dolques et al. (2009) et Dolques et al. (2010, qui ne pourraient pas être facilement obtenus à partir du processus classique de l'AFC.
Cependant un problème majeur de ce type de processus est la difficulté potentielle à appré-hender le résultat. Dans des travaux précédents en ingénierie dirigée par les modèles, les données extraites de modèles de taille moyenne peuvent être facilement appréhendées par l'ARC. Mais dans un contexte de fouille de données, la taille des données est beaucoup plus importante. Le temps de calcul est dépendant du nombre de concepts à générer et celui-ci est exponentiel par rapport au nombre minimum des attributs ou des objets dans le pire des cas. Ainsi, si les relations entre objets sont nombreuses et ont peu de similarité d'un objet à l'autre, le temps de calcul peut augmenter de manière exponentielle et le résultat peut apparaître difficile à comprendre par un utilisateur à cause du nombre de concepts à considérer simultanément. Ceci est particulièrement avéré quand seuls de petits motifs sont nécessaires alors que de nombreuses relations connectent les objets entre eux et que ces relations forment un circuit. Dans de tels cas, nous pensons qu'il peut être plus pertinent d'avoir une approche exploratoire.
Nous listons ci-dessous les différentes variations possibles sur l'algorithme pour mettre en pratique une approche exploratoire. Nous énumérons les points de variation possibles dans l'algorithme qui peuvent affecter le résultat en changeant les contextes pris en compte à chaque étape. Nous proposons pour chaque point de variation un scénario alternatif à partir du processus précédemment décrit qui implique l'utilisateur en lui demandant d'effectuer des choix. Toutes ces variations ou seulement un sous-ensemble peuvent être appliquées selon la granularité voulue.
-étape d'initialisation, lignes 4 à 5 Construire les treillis pour des contextes objet-attribut choisis concaténés à des contextes objet-objet choisis. -ETENDRE-REL, ligne 9 Plutôt que d'utiliser toutes les relations et d'échelonner toutes les relations objet-objet à chaque étape, sélectionner un sous-ensemble de la famille relationnelle de contextes et différents opérateurs d'échelonnement pour chaque contexte objet-objet sélectionné. Note : les treillis pour les images des relations objet-objet sélec-tionnées auront dû être calculés lors d'une étape précédente (mais pas nécessairement p ? 1). À cette étape, des contextes objet-attribut peuvent aussi être sélectionnés et le treillis correspondant peut être construit. -MAJ-TREILLIS, ligne 10 Mettre à jour seulement les treillis pour les relations sélec-tionnées. -arrêt, ligne 11 Si un point fixe n'est pas atteint, laisser la décision d'arrêter à l'expert.  La figure 4 représente les données par un schéma. Chaque noeud représente un ensemble d'objets qui est traduit en un contexte objet-attribut. Chaque arête étiquetée représente une relation entre les ensembles d'objets qui est traduite en un contexte objet-objet. Pour chaque arête nous considérons la relation dans les deux directions.  
Contextes objet-attribut
station-taxon-20-99
Asellus Athericidae Baetis Bithynia Boreobdella
station-taxon-1-19
Asellus Athericidae Baetis Bithynia Boreobdella
station-taxon-100+
Asellus Athericidae
TAB. 4 -Famille relationnelle de contextes obtenue à partir de nos données d'exemples en considérant les directions de relation de la figure 5.
Nous souhaiterions voir émerger de ces données des relations entre les différents types d'information qui permettent de décrire une station. Il serait par exemple intéressant de voir apparaître des règles suivant le format : « la modalité M du trait de vie T dans un cours d'eau de type E implique la présence du caractère physico-chimique C », ce que l'on peut obtenir en orientant les relations de la figure 4 de telle sorte que la relation traits des taxons aille du contexte taxons vers traits de vie et que toutes les autres relations aient stations pour domaine comme cela est présenté par la figure 5.
La famille relationnelle de contextes pour cette configuration particulière est présentée par la table 4. Nous rajoutons dans les contextes objet-attribut un attribut identifiant pour chaque objet afin de permettre la création d'un concept pour chaque objet après l'étape d'initialisation. La figure 6 présente les extraits de la famille de treillis obtenue nécessaires pour extraire la règle « la présence à un niveau moyen d'individus d'un taxon dont la durée de vie est supérieure à un an implique une demande chimique en oxygène (DCO) élevée » 3 . On peut en effet remarquer que Concept_3 est plus spécifique que Concept_41. Un attribut introduit dans Concept_3 impliquera ainsi un attribut introduit dans Concept_41 puisque tous les attributs de Concept_41 sont hérités par Concept_3. Concept_3 regroupe les stations qui abritent une quantité moyenne d'individus appartenant à des taxons regroupés par 3. compte tenu de la taille de l'exemple, cette règle n'est pas à considérer comme générale.
Concept_63. Concept_63 regroupe de son côté les taxons dont les individus ont une durée de vie supérieure à un an. Concept_41 regroupe les stations dont l'un des caractères physico-chimiques (de niveau 2, donc élevé) est DCO. On en tire l'implication déduite. À partir du treillis complet on peut obtenir l'ensemble des règles d'implication entre traits de vie et caractères physico-chimiques en considérant tous les cas où des caractères physico-chimiques sont introduits par un concept et des traits de vie sont introduits par un concept plus spécifique.
Mais des règles suivant le format « la modalité M du trait de vie T peut apparaître quand le caractère physico-chimique C est présent » peuvent aussi être pertinentes. Afin qu'elles puissent émerger, il serait nécessaire de changer (par rapport à la configuration précédente) la direction des relations entre traits de vie et taxons et entre taxons et stations. Il existe encore de nombreuses autres configurations dont les résultats sont potentiellement pertinents et faire varier les opérateurs d'échelonnement permet d'augmenter encore l'expressivité des règles que l'on peut obtenir grâce à l'ARC.
Concept_19
Extrait du treillis du contexte stations Extrait du treillis du contexte caractères physico-chimiques
Concept_8
Concept_21
Concept_27
Concept_29
Extrait du treillis du contexte taxons Si nous considérons le schéma de la figure 4 comme un graphe, l'exploration consiste à analyser les différentes arêtes jusqu'à obtenir un chemin entre les caractères physico-chimiques et les traits de vie. En comparant les résultats obtenus lors d'une exploration avec ceux obtenus en utilisant l'ARC classique sur les mêmes relations, nous trouvons que les treillis résultants sont plus petits et plus faciles à lire dans le premier cas.
Combiner l'ensemble des configurations possibles n'est pas envisageable car la multiplication des relations risque d'entraîner une explosion combinatoire du nombre de concepts obtenus, augmentant ainsi le temps de calcul et la complexité des concepts obtenus. Dans le cas de notre exemple, considérer toutes les relations dans les deux directions échelonnées seulement par ? entraîne la création de 120 concepts contre 66 et 63 pour les deux configurations évoquées précédemment. Nous envisageons donc une approche où l'utilisateur/trice explore différentes configurations en effectuant différents choix à chaque étape du processus comme présenté dans la section 1.
Conclusion et discussion
Dans cet article, nous avons présenté une approche exploratoire pour assister l'utilisation de l'Analyse Relationnelle de Concepts de manière plus appropriée pour un processus d'extraction de connaissances. Nous avons plusieurs raisons de vouloir modifier le processus original de l'ARC : obtenir des résultats pertinents plus rapidement en calculant moins de treillis (de préférence seulement les treillis qui nous intéressent), diminuer la complexité de la fouille de données relationnelles, ou laisser l'expert guider le processus de découverte en se basant sur son intuition et les motifs d'apprentissage qui apparaissent au cours du processus.
Plusieurs questions se posent sur cette approche d'extraction de concepts à partir de données relationnelles. L'étape d'initialisation a un fort impact sur les structures qui peuvent être découvertes par la suite. Elle peut accélérer le processus, si les relations objet-objet contiennent les informations nécessaires à l'expert, ou à l'inverse, elle peut cacher à l'expert les informations pertinentes. Néanmoins, le problème le plus important vient du fait que les modifications à chaque étape rendent la construction de concepts non monotone et qu'il est possible de construire des exemples où le processus diverge (itérant sur plusieurs configurations récur-rentes).
Dans le processus original de l'ARC, quand le point fixe est atteint, les treillis des deux dernières étapes sont isomorphes, ainsi quand un concept en référence un autre via un attribut relationnel, le concept référencé peut être trouvé dans un treillis de la même étape. Mais dans le processus exploratoire que nous proposons, quand un concept en référence un autre, le concept référencé est dans un treillis de l'étape précédente et peut référencer un concept lui-même dans une étape antérieure. Il est alors nécessaire de trouver des solutions pour présenter aux experts une information suffisamment simple à interpréter. Nous pensons malgré tout qu'une telle approche exploratoire est plus applicable qu'une approche systématique qui itère jusqu'à atteindre un point fixe et qui donne des résultats difficilement interprétables par un expert.

Introduction
La classification des images satellitaires haute résolution est de plus en plus complexe. La complexité et l'hétérogénéité des données satellitaires haute résolution ne permettent plus l'utilisation des méthodes de classification pixélique. En effet, quoi qu'elle permette d'offrir dans certains cas un résultat qui reflète de manière fidèle la réalité du terrain étudié, elle reste tributaire d'une conformité, d'une répartition homogène des classes recensées et d'une bonne stabilité radiométrique des zones d'apprentissage. Lesquelles sont aussi limitées du point de vue caractéristique. Ces difficultés ont catalysé la recherche de nouvelles approches d'analyse d'image exploitant mieux les informations présentes dans l'image ainsi que les connaissances expertes qui peuvent être une source de connaissances très fructueuse Sellaouti et al. (2012b). Dans ce sens, l'analyse d'image basée objets semble être prometteuse Blaschke (2010). En effet, elle permet de converger d'un espace image composé de pixel vers un espace objet ou chaque objet représente une entité homogène selon des critères bien définis ce qui permet d'avoir une vue plus globale de l'image. Le deuxième avantage de cette approche objet est qu'elle reste propice à l'intégration des connaissances expertes grâce à sa capacité d'intégrer des connaissances de haut niveau telles que la taille, la forme et les connaissances spatiales ce qui permet de diminuer le gap sémantique.
La classification orientée objet est une approche composée de deux étapes dont la première consiste à la construction des objets et qui est généralement une segmentation. La deuxième étape est l'identification des objets extraits. La classification orientée objet se base sur les ré-gions extraites lors de la phase de segmentation mais aucune interactivité n'existe entre ces deux processus. Cette classification utilise directement les régions extraites lors de la première étape sans mettre en cause la segmentation. Cependant, une région mal segmentée est générale-ment mal classée. Ceci s'explique par le fait qu'elle se base sur des caractéristiques qui peuvent être erronées. Une remise en cause de la segmentation par la classification et une collaboration entre eux s'impose.
Dans cet article, nous proposons une approche orientée objet sémantique collaborative permettant une collaboration entre les deux étapes d'extraction et d'identification des objets en intégrant des connaissances expertes. En effet, la première étape de l'algorithme permet d'extraire des régions homogènes et de leur assigner un score de confiance. Ce score sera ensuite utilisé pour une croissance de région sémantique dont résultera la classification finale.
Ce papier est organisé comme suit : dans la section 2, nous reviendrons sur différents algorithmes de classification orientée objet proposées dans la littérature. Ensuite, dans la section 3, nous introduisons notre approche. La section 4 sera consacrée à l'étude expérimentale.
2 Etat de l'art des approches de classification orientée-objet L'approche orientée objet est une approche qui se base sur la création d'un ensemble de régions représentant les objets de l'image afin de les classer. Plusieurs travaux ont été présentés dans la littérature. Nous proposons de décomposer ces approches en trois classes : approche naturelle, approche hiérarchique et approche collaborative. Nous présentons dans ce qui suit, un aperçu sur ces travaux.
Approche naturelle
L'approche naturelle repose sur la structure classique de la classification orientée-objet qui est composée d'une phase de segmentation de l'image en un ensemble d'objets suivi d'une classification de ces objets. Walter (2004) présente une approche pour la détection des changements apparus dans les zones urbaines. Il propose une approche divisée en deux parties, la première consiste en une classification supervisée avec l'algorithme de maximum de vraisemblance. La base d'apprentissage est extraite à partir d'une base GIS existante. Dans la deuxième partie, une mise en correspondance entre les objets classés et les objets existants dans la base GIS est appliquée pour détecter les changements. L'auteur utilise toutes les bandes spectrales. Les caractéristiques utilisées sont des caractéristiques spectrales et texturales. Ericksson (2004) utilise une approche orientée objet pour détecter les couronnes d'arbres. En effet, il commence par une segmentation qui va lui permettre de détecter les couronnes d'arbres. Cette segmentation est la croissance de région par mouvement Brownien. Une fois les couronnes des arbres détectées, Ericksson procède à une classification de ces couronnes en se basant sur un ensemble de règles spécifiques à chaque classe. La couronne qui vérifie les règles d'une classe est affectée à cette dernière. Bendhiaf et Sellaouti (2009), utilisent les couronnes extraites par l'algorithme de croissance de région pour procéder à une classification basée sur les indices de formes et la texture. Lefebvre et al. (2011) présentent une classification orientée-objet basée sur les ondelettes et la théorie des évidences. Une première étape de segmentation utilisant la ligne de Partage des Eaux (LPE) est utilisée sur une image de contours créée à partir des composantes horizontales et verticales de la décomposition en ondelettes afin d'éviter un résultat sur-segmenté. Ensuite, une agrégation des petits objets non exploitables dans l'étape suivante est effectuée en fusionnant les régions voisines de petite taille et de valeur moyenne de luminance proche. La deuxième étape est la caractérisation des objets extraits avec des attributs de luminance et de texture. La dernière étape est la classification basée sur la théorie de l'évidence en fusionnant les critères de similarité de luminance et de texture.
Approche hiérarchique
Cette approche utilise une représentation multi-échelle de l'image. En effet, elle repose sur une segmentation multi-échelle, l'image est donc représentée par une famille d'images allant de la percepftion de l'image la plus fine vers la plus grossière.
Hofmann (2001) présente une approche orientée objet pour la classification des zones urbaines, ils présentent deux approches utilisées par le logiciel eCognition pour extraire les objets d'intérêt. Le départ de l'approche est une segmentation multi-échelle qui permet de créer un réseau hiérarchique représentant l'image. La segmentation multi-échelle peut être réalisée de deux façons différentes qui sont Bottom-up et Top-down. L'approche top-down commence par générer les objets du niveau le plus haut. Tous les objets des niveaux le plus bas sont des sous objets du niveau supérieur. L'approche Bottom-up, quant à elle, opère inversement. La segmentation commence par générer les petits objets. Tous les objets générés dans les niveaux plus hauts sont alors considérés comme des super objets du niveau initial. Le logiciel offre deux classifieurs de base : le classifieur des k plus proches voisins et un classifieur flou. Marangoz et al. (2004) utilisent cette approche pour détecter les routes et les bâtiments dans une image IKONOS. Karsenty et al. (2007) utilisent la même approche pour évaluer la perméabilité des sols en zone urbaine à l'aide d'imagerie très haute résolution et de données laser scanner à Curitiba au Brésil. De même, pour Jacquin et al. (2008) qui utilisent cette approche pour voir l'effet de l'expansion urbaine sur les inondations et les anticiper. Giraudon et al. (1992) ont développé le système MESSIE. Ce système est une architecture multi-spécialiste, bâti autour d'une architecture de type tableau noir, dont l'objectif est de réaliser une interprétation basée uniquement sur une connaissance générique des objets sans utiliser de connaissances exogènes à la donnée image. Les auteurs montrent comment une modélisation des objets physiques de la scène exprimée sous les quatre points de vue forme, contexte, aspect et fonction peut améliorer la classification. Ce système est capable de manipuler des connaissances ponctuelles (informations radiométriques) et structurelles (propriétés géométriques, relations spatiales). Chaque spécialiste a une tâche spécifique et indépendante des autres spécialistes. Pour communiquer entre eux, les spécialistes utilisent une zone mé-moire commune qui est le tableau noir. Plusieurs rôles sont assignés aux spécialistes tels que l'extraction des objets, le calcul des caractéristiques structurelles, l'évaluation des hypothèses, l'étiquetage des objets, la détection des conflits, etc. Le système peut facilement accepter de nouvelles connaissances expertes et de nouveaux spécialistes. Forestier et al. (2008), quant à eux, proposent une approche orientée objet collaborative et multi-stratégie. Elle intègre un ensemble de classifieurs non supervisés et présente une nouvelle approche qui permet de faire collaborer les différents classifieurs. L'originalité de cette approche est qu'elle intègre le processus de collaboration durant l'étape de classification. En effet, elle est divisée en trois étapes. La première est une classification initiale où chaque classifieur est lancé avec ses propres paramètres, permettant de créer des objets. La deuxième étape consiste en un raffinement des résultats divisés en deux parties à savoir d'une part l'évaluation de la similarité des classes et d'autre part, leur raffinement. La dernière étape est l'unification où les résultats raffinés sont unifiés avec un algorithme de vote.
Approche hybride
Pour résumer, toutes les approches décrites ont une limite commune, c'est leur dépendance à la phase de segmentation. En effet, cette phase initiale est la plus importante vu qu'une mauvaise segmentation entraine nécessairement une mauvaise classification. Dans toutes les approches précitées, l'étape de segmentation n'est jamais mise en cause par la classification.
Approche de classification orientée-objet collaborative
Une approche de segmentation qui favorise nettement la notion de collaboration est l'approche région. En effet, son architecture itérative permet d'introduire de nouvelles connaissances le long du processus de segmentation. Dans ce cadre, nous introduisons une approche collaborative entre les algorithmes de croissances de régions et une COO supervisée permettant une collaboration entre les deux étapes d'extraction et d'identification des objets en intégrant des connaissances expertes. Notre approche est composée de deux étapes :
-Une première étape de prétraitement qui est une étape de préparation des données pour l'étape suivante. Elle permet de décomposer l'image en un ensemble de régions homogènes en se basant sur des propriétés bas niveau. Ensuite, une classification initiale va permettre d'assigner un score de confiance pour chacun des objets extraits par la segmentation. -La deuxième étape est la classification sémantique basée sur une croissance guidée par les scores des régions calculés dans la première étape.
Prétraitement
Segmentation
Le choix de l'algorithme de segmentation n'est pas très important dans cette approche tant qu'il vérifie le critère de sur-segmentation. En effet, et vues les propriétés des algorithmes de croissances de région qui se basent sur la fusion des fragments d'un objet afin de détecter l'objet tout entier, il est évident qu'une sous-segmentation de l'image implique une perte de certains objets. Le choix de la sur-segmentation s'impose donc. Nous avons ainsi choisi l'al-gorithme "watershed" qui permet une sur-segmentation de l'image, permettant de déterminer un ensemble de régions de départ, qu'on notera R.
Classification
Nous proposons ici une classification permettant d'affecter à chacune des régions de R une classe C en calculant un score de confiance d'appartenance de R à C, par rapport à l'ensemble des classes présentes dans l'image. Ce score permettra d'évaluer la légitimité des régions en se basant sur les connaissances fournies par l'expert. Nous utilisons le score de similarité proposé par Derivaux et al. (2007). Il est basé sur une approche orientée attribut vu qu'il utilise les connaissances bas niveaux sur l'image qui sont formalisées sous forme de descripteurs bas niveaux. Ce score permet de vérifier la validité des valeurs des attributs d'une région selon les intervalles définis par l'expert dans la base de connaissances. La mesure de similarité locale compare les valeurs des attributs d'une région avec les attributs de l'objet à classer. Nous pré-sentons dans ce qui suit les formules permettant de calculer le score de similarité, précédées par un ensemble de notations nécessaires pour une représentation formelle de l'approche proposée Sellaouti et al. (2012a).
Note 1 (région) : Soit R l'ensemble des régions r i obtenu à partir de la segmentation. R = {r i } i? [1,NR] où N R représente la cardinalité de R. 
Note 2 (classe) : Soit C l'ensemble des classes présentes dans l'image. C
où v(r i ,a k ) est la valeur de l'attribut a k pour la région r i . 
Définition 3 (Ensemble de similarité) : Nous définissons l'ensemble SIM comme étant l'ensemble des scores de similarité de toute région r i ? R par rapport à toute classe c j ? C.
Classification sémantique
Après la phase de prétraitement explicitée ci-dessus, nous procédons à l'étape de classification sémantique qui consiste en un traitement itératif permettant à partir des ensembles de régions R, de classes C et de scores de similarité SIM, de créer une hiérarchie de croissances basée sur la confiance en chaque région. La création de la hiérarchie est précédée par un calcul basé sur les scores de similarité que nous explicitons dans ce qui suit.
Pour une région r i ? R, nous définissions l'ensemble des classes qui maximisent le score de similarité Sim(r i , c) parmi toutes les classes c ? C. Nous notons ?(r i ) cet ensemble :
Définition 4 Pour chaque région r i ? R, nous définissions S max (r i ) et C max (r i ) comme suit :
S max (r i ) représente le score de similarité maximal de la région r i pour l'ensemble des classes de C. Dans le cas où ?(r i ) comporte plus qu'une classe, nous déduisons qu'il y a une confusion et que cette région n'est plus une région de confiance mais une région conflictuelle. Dans ce cas, C max (r i ) prendra arbitrairement l'une des classes de ?(r i ) et S max (r i ) aura la valeur 0. Dans le cas où l'ensemble contient une valeur unique, alors cette dernière sera affectée à C max (r i ) et S max (r i ) sera le score de similarité Sim(r i , C max (r i )) de la classe Cmax pour la région r i et cette région aura comme classe C max (r i ).
Le calcul des C max (r i ) et S max (r i ) servira comme départ pour l'algorithme itératif de la hiérarchie de la classification sémantique que nous proposons. Le diagramme de la figure 1 illustre les étapes de cet algorithme. En effet, chaque itération de cet algorithme concerne un niveau de croissance de la hiérarchie. Chacune de ces itérations est composée de deux phases principales (les phases 1 et 2 du diagramme), à savoir l'extraction des germes et la croissance sémantique. Dans la première phase, nous commençons par extraire les régions de départ que nous appelons germes. Ces derniers représentent les régions de confiances parmi l'ensemble des régions candidates. Ensuite dans la deuxième phase, en se fondant sur des connaissances expertes, et selon la classe d'appartenance du germe, nous procédons à une croissance sémantique à partir des germes déterminés dans la première phase.
Pour une itération k (k ? 1), nous désignerons par RegionsCandidates k?1 l'ensemble des régions candidates à l'extraction des germes, par RegionsContraintes k?1 l'ensemble des germes déja traités, par RegionsGermes k l'ensemble des germes extraits et par RegionF u? sionne k l'ensemble des régions fusionnées. Les phases 3 et 4 permettent de mettre à jour les ensembles RegionsCandidates k et RegionsContraintes k . Nous détaillons dans ce qui suit les deux phases constituant une itération.
Extraction des germes
Le choix des germes de départ est très important pour le processus de la croissance séman-tique. Visant à exploiter toutes les informations disponibles et souhaitant converger vers une
FIG. 1 -Diagramme de croissance de region sémantique.
approche sémantique, le choix des germes reposera sur la confiance que nous avons en les ré-gions non encore traitées dans l'image. L'ensemble des germes de niveau k (RegionsGerme k ) sera alors celui qui maximise la confiance et plus précisément le score de similarité. Ces ré-gions seront extraites à partir de l'ensemble RegionCandidates k?1 des régions non encore traitées dans les niveaux précédents de la hiérarchie. Les germes extraits à ce niveau sont les régions ri qui maximisent Smax(ri). Nous notons formellement :
Croissance sémantique
Nous présentons dans ce qui suit le principe de l'algorithme de croissance sémantique (la fonction SemCroiss appelée à la phase 2 du diagramme 1) qui prend comme entrée l'ensemble des germes, des régions candidates à la croissance et des régions contraintes. Il permet la fusion de chaque germe avec ses régions voisines en se fondant sur les connaissances expertes spécifiques à la classe du germe et en prenant en compte les régions contraintes. La croissance sémantique est illustrée par l'algorithme 1.
Cet algorithme itère sur l'ensemble de germes appliquant pour chacun deux fonctions (ExtraireZoneCroissance et Croissance) mettant en oeuvre les deux principaux procédés composant la croissance sémantique, à savoir l'extraction de la zone de croissance dans dans un premier lieu et la croissance au sein de cette zone dans un second lieu. Notons que nous nous limiterons dans cet article à la présentation générale de notre approche en faisant abstraction aux types de classes traitées. Or, comme la technique de croissance sémantique que nous proposons est intrinsèquement dépendante des spécificités des classes, nous nous contentons ici de présenter les principes des fonctions ainsi que leurs entrées et sorties : L'extraction de la zone de croissance : Connaissant la classe du germe, nous proposons de limiter l'espace de croissance en se basant sur les connaissances expertes. En effet, celles-ci permettent dans plusieurs cas de réduire l'ensemble des régions candidates à la fusion. Les connaissances expertes peuvent être représentées de plusieurs manières, telles que les ontologies, les règles logiques, etc. Cette phase est tributaire de la classe traitée. En effet, si nous prenons l'exemple de la classe végétation, rares sont les informations qui peuvent être utiles pour limiter l'espace de recherche. N'ayant ni forme géométrique bien définie, ni superficie limitée, la zone de croissance de cette classe ne peut pas vraiment être limitée. Par contre, si nous prenons l'exemple des classes bâtiment et route, elles possèdent toutes les deux des caractéristiques géométriques et des formes bien définies ce qui permet de limiter l'espace de croissance. L'appel de la fonction ZoneCroissance dans une itération de l'algorithme 1 permet de géné-rer, à partir, d'une région germe r donnée et des régions candidates à la croissance, l'ensemble des régions pertinentes qui peuvent fusionner avec le germe r, que l'on notera ZC_r. Cette fonction prend en considération les régions déjà fusionnées que ce soit pour d'autres germes de l'itération courante (i.e. F usion) ou dans les itérations précédentes (i.e. RegionsContraintes). L'ensemble ZC_r serait égal à l'ensemble des candidats privé des régions élaguées dans le cas où la classe C max (r) supporte l'intégration de connaissances expertes comme expliqué pré-cédemment. Dans le cas contraire, ZC_r comporterait toutes les régions candidates sauf celle déjà fusionnée dans l'itération courante (i.e. ZC_r = RegionCandidates\F usion).
Croissance : Contrairement à la croissance de région de base qui utilise les propriétés bas niveau (radiométrie, texture) sans prendre en compte la nature des objets traités, la croissance de région sémantique est, quant à elle, une croissance spécifique pour chaque germe selon sa classe d'appartenance. Prenons l'exemple de la classe route qui possède des pro-priétés qui la différentient des autres classes. En effet, elle présente une surface homogène, et elle est majoritairement composée d'asphalte et ayant des contours linéaires et parallèles et une largeur variant dans un intervalle bien déterminé. A partir de ces connaissances nous pouvons détecter la zone de croissance de chaque germe en cherchant sa direction et en utilisant les informations sur sa largeur. L'appel de la fonction Croissance dans une itération de l'algorithme 1 permet de croître à partir d'un germe r dans la zone de croissance ZC_r. La détermination des régions pertinentes qui peuvent fusionner avec ce germe r dépendra de la classe du germe et des connaissances expertes la concernant. Cette fonction retournerait l'ensemble F usionnes_avec_r des germes fusionnés avec r. L'union de ces ensembles pour chaque itération formerait l'ensemble de retour de l'algorithme SemCroiss.  Notre approche est une classification sémantique basée sur les connaissances expertes. Les connaissances que nous avons utilisés sont extraites à partir du dictionnaire de données FODOMUST réalisé par les experts géographes 1 . Il a été modélisé sous forme d'une ontologie formée d'une hiérarchie de concepts reliés entre eux par des relations par Derivaux et al. (2007). Chaque concept est défini par une étiquette (e.g. maison, route) et un ensemble d'attributs (e.g. air, forme) ou chaque attribut est associé à un intervalle représentant les valeurs que  L'évaluation numérique confirme l'intérêt de notre approche. Le tableau 1 détaille le ré-sultat des mesures de rappel et de précision pour les deux classes route et bâtiment. Le rappel varie entre 87.35% et 94.24% alors que la précision varie entre 85.28% et 87.40%. Les valeurs de la précision sont inférieures à celles du rappel à cause des erreurs de classification de la phase de prétraitement.
Etude Expérimentale

Introduction
De nombreux programmes de construction d'alignements multiples à partir d'un ensemble de séquences protéiques (appelés aligneurs) ont été développés. Cependant, il n'existe pas un aligneur capable de bien aligner tous les types de séquences. C'est la raison pour laquelle le laboratoire de bioinformatique de l'IGBMC de Strasbourg a développé un système expert pour l'alignement multiple de séquences protéiques appelé Alexsys (Aniba et al., 2009). Le système Alexsys propose d'identifier, pour un ensemble de séquences protéiques données, les aligneurs permettant d'obtenir de bons alignements en se basant sur des techniques de fouilles de données (classification supervisée) exploitant des caractéristiques des séquences. spécifiques ont été définies (Tsoumakas et al., 2010;Fürnkranz et al., 2008). Nous les utiliserons dans le cadre de l'alignement multiple de séquences protéiques.
La séquence protéique est une suite d'acides aminés (aussi appelés résidus). Chaque acide aminé est représenté par une lettre. Pendant l'évolution des espèces, des changements appelés mutations peuvent se produire. En comparant les séquences entre elles et en cherchant les résidus ou les suites de résidus qui sont conservés dans une même famille de protéines, nous pouvons beaucoup apprendre sur les résidus essentiels pour certaines fonctions Les alignements multiples de séquences sont un outil important de la biologie moderne.
Le nombre de programmes d'alignement multiple disponible est sans-cesse croissant. Cependant, aucun programme actuel n'est capable de construire un alignement multiple de haute qualité pour tous les cas possibles. Les principaux critères utilisés pour décider du programme d'alignement à utiliser sont : la qualité de l'alignement, le temps d'exécution et l'utilisation de mémoire (Thompson et al., 2011). La qualité de l'alignement est généralement le critère le plus important. Dans notre étude, nous avons utilisé les aligneurs suivants, connus pour leurs performances et leurs fiabilités : ClustalW , Dialign, Mafft, Muscle, Probcons, T-coffee et Kalign. Une description des programmes d'alignement ainsi qu'un résumé de leurs avantages et de leurs inconvénients est présenté dans (Edgar et Batzoglou, 2006). L'estimation de la qualité de l'alignement est un point critique. La fonction de score la plus utilisée est le " Sum of Pairs ". Actuellement, la qualité d'un algorithme est générale-ment estimée en comparant les résultats obtenus avec des alignements de référence prédéfinis (Benchmarks). Par conséquent, les données de référence doivent être de haute qualité. Un des premiers benchmarks à grande échelle construit pour l'alignement multiple des séquences se nomme BaliBase (Bahr et al., 2001). Les séquences utilisées dans la base de données de BaliBase ont toute une structure 3D connue. Les alignements sont construits à partir de ces structures 3D et raffinées à la main par des experts pour garantir l'alignement correct des résidus conservés. Les alignements sont organisés sous la forme de plusieurs ensembles de références, représentant des problématiques réelles de l'alignement multiple. L'autre benchmark utilisé lors de cette étude, OXBench (Raghava et al., 2003), contient des alignements multiples de séquences protéiques construits automatiquement par des aligneurs.
Amélioration de l'existant : Alexsys
Les divers algorithmes d'alignement multiple peuvent produire des alignements différents pour un même ensemble de séquences à aligner. Les qualités de ces différents alignements dépendent des caractéristiques des séquences à aligner. Les biologistes aimeraient expliciter les relations entre les caractériques des séquences protéiques à aligner et la force/faiblesse de différents algorithmes d'alignement, afin de pouvoir choisir le meilleur aligneur dans chaque cas. C'est la raison pour laquelle le système Alexsys "Alignment Expert System" a été déve-loppé (Aniba et al., 2009). Dans cette section, nous décrivons l'existant puis les améliorations apportées.
Alexsys : un système expert pour l'alignement multiple
Dans Alexsys, les connaissances ou règles du système expert ne sont pas recueillies auprès des experts, mais obtenues par apprentissage artificiel à partir de jeux de données. Les données d'apprentissage proviennent de deux jeux de données de référence (BaliBase (Bahr et al., 2001) et OxBench (Raghava et al., 2003)) qui représentent des problèmes réels et des cas difficiles d'alignement multiples. Les données consistent en un ensemble de 890 alignements choisis par les biologistes. Les attributs utilisés par Alexsys pour représenter un ensemble de séquences à aligner appartiennent à quatre catégories : physiques, structurels, fonctionnels et physicochimiques.
La classe de chaque aligneur est définie comme "fort" ou "faible" sur la base du score défini comme le nombre de paires de résidus alignés de la même façon dans l'alignement produit par l'aligneur et dans l'alignement de référence. Au-dessus d'un seuil de 0,5, un aligneur est considéré comme "fort", et en-dessous de cette valeur, un aligneur est considéré comme "faible". Cette valeur de seuil, bien que choisie par les biologistes, reste arbitraire. Alexsys construit alors un modèle par aligneur sous la forme d'une forêt aléatoire.
Améliorations
Nous avons poursuivi les travaux d'Alexys. Tout d'abord, nous avons considéré une version enrichie de la base de données contenant 1058 instances et gardant les même attributs. L'attribut de pourcentage d'identité (PCID) est un des attributs les plus importants utilisés par Alexsys. Nous avons défini une autre méthode pour le calculer. Cette méthode se base sur l'algorithme d'alignement global Needleman-Wunsch (Needleman et Wunsch, 1970) dans lequel l'identité entre deux séquences est calculée pour chaque couple de séquences de l'ensemble de séquences. Puis le maximum, le minimum, la moyenne et l'écart type d'identités obtenues sont calculés. Ainsi les valeurs de quatre attributs liés au PCID sont générées pour chaque ensemble de séquences utilisé dans la base de données.
La génération de l'attribut de PCID consomme beaucoup de temps et de mémoire Nous avons défini et évalué une autre méthode moins exigeante en ressources nous permettant d'estimer la similarité entre les séquences. C'est la méthode de comptage des n-gram. Nous avons créé plusieurs attributs de n-gram en changeant la longueur des mots d'acides aminés. Les attributs 1-gram, 2-gram, 3-gram, et 4-gram ont été testés.
Le taux de bonne prédiction macro est évalué en utilisant le PCID ou le 1-gram séparé-ment, ou les deux conjointement, en plus des attributs usuels d'Alexsys. Nous ne considérons pas seulement le seuil de 0,5 pour étiqueter les bons et mauvais aligneurs, mais nous avons essayé de faire varier le seuil de score. Plusieurs seuils ont été testés de 0 à 1, avec un pas de 0,1. L'évaluation est faite en moyennant 5 répétitions d'une validation croisée en 5 plis. Nous utilisons les forêts aléatoires implémentées dans Weka (Witten et al., 2011), comme dans la version originale d'Alexsys. En comparant le comportement des nouveaux attributs (Figure 1), nous remarquons qu'en utilisant les attributs de 1-gram avec l'attribut de PCID en même temps, le taux de bonne prédiction macro est meilleur qu'en cas d'utilisation de l'attribut de PCID uniquement, et bien sûr en utilisant toujours tous les autres attributs préalablement définis. Par ailleurs, les résultats ne sont pas présentés ici, mais nous avons observé que les attributs de 1-gram obtiennent de meilleurs performances que ceux de 2-gram, 3-gram et 4-gram. Dans le cas du seuil à 0,5, 1-gram et PCID ensemble donnent un taux de bonne prédiction macro de 90% pour les modèles créés pour les sept aligneurs.
Choix du meilleur aligneur
Pour chaque aligneur, nous lui attribuons l'étiquette "meilleur" si son score d'alignement est le score maximum à epsilon près. Nous autorisons une marge de epsilon car il peut y avoir plusieurs alignements intéressants pour les biologistes Un objectif de ce travail est d'identifier un seuil raisonnable pour epsilon. Puisque les scores varient de 0 à 1, il est clair qu'une valeur de 1 pour epsilon conduirait à considérer tous les aligneurs comme "meilleurs". Par la suite, lors des différents test, epsilon variera entre 0 à 0,2.
Notre problème consiste à trouver les aligneurs étiquetés "meilleur", autrement dit l'ensemble des aligneurs capables de produire des alignements multiples de score maximum ou presque. C'est de la classification multi-étiquettes. En calculant la cardinalité, qui est le nombre moyen des étiquettes "meilleur" sur tous les exemples (Figure 2), nous remarquons que deux aligneurs (en moyenne) sont "meilleurs" pour epsilon égal à zéro (où seulement le score maximum est choisi pour être "meilleur"). Ce résultat est surprenant. Nous nous attendions à avoir seulement un meilleur aligneur pour epsilon égal à 0. En vérifiant les données en cas de epsilon égal à 0, nous remarquons que 35% des exemples ont plus que deux aligneurs acceptables comme "meilleurs". La figure 2 montre aussi la vitesse à laquelle le nombre d'aligneur acceptables augmente en fonction de epsilon. Le nombre d'aligneurs acceptables tend vers 7 pour les grandes valeurs de epsilon. L'implémentation de Weka des forêts aléatoires est encore utilisée, en faisant 5 validations croisées en 5-plis. Nous calculons la moyenne et l'écart-type, sur ces 5 itérations de la validation croisée, des différentes mesures de performance de la classification multi-étiquettes afin de déterminer la valeur de epsilon la plus pertinente.
En observant les mesures de performance basées sur l'exemple en fonction de la cardinalité (Figure 3), nous remarquons que beaucoup de mesures n'apportent pas d'information autre que : en augmentant epsilon donc la cardinalité, c'est-à-dire quand plus d'aligneurs deviennent acceptables, ces mesures augmentent. En particulier, la précision et le rappel, donc la mesure-F1, et le taux de bonnes prédictions et le rappel pour la classe négative croissent quasilinéairement avec la cardinalité. Le taux de bonne prédiction de l'ensemble et 1-hammingLoss croissent également mais avec une pente moins régulière. Seule l'AUC a un comportement différent et atteint rapidement un maximum, reste constante puis diminue quand epsilon et la cardinalité augmentent.
Nous observons un comportement similaire des mesures de performance basées sur l'éti-quette, micro et macro, en fonction de epsilon à la Figure 4. De plus, epsilon=0,01 apparaît comme un point singulier. Tout d'abord, c'est le maximum pour la macro AUC. Rappelons que nous cherchons le plus petit epsilon (et cardinalité) ayant de "bonnes" performances pour prédire le (ou les) meilleur aligneur. À cette valeur 0,01 de epsilon, le taux de prédiction de l'ensemble est à 25%, c'est-à-dire qu'une fois sur quatre on obtient le sous-ensemble des bons aligneurs et seulement eux, parmi 2 7 = 128 sous-ensembles possibles. Enfin, la plupart des mesures macro et micro sont à 0,75, en particulier 3 fois sur 4 pour la précision et pour le rappel, c'est-à-dire que 3/4 des aligneurs prédits acceptables font réellement parti des meilleurs et que 3/4 des meilleurs aligneurs sont proposés. 
Conclusion
Le système Alexsys (Aniba et al., 2009) sert à prédire le programme à utiliser pour construire un bon alignement multiple d'un ensemble de séquences protéiques. Nous avons amélioré l'approche d'Alexsys en reprenant le calcul du PCID et en lui ajoutant des attributs à partir du 1-gram plus rapides à calculer et complémentaires, c'est-à-dire que la combinaison de ces attributs et du PCID améliore le taux de bonne prédiction par rapport au PCID utilisé seul dans Alexsys.
La seconde contribution de ce travail a consisté à reformuler le problème en considérant qu'un aligneur est acceptable si son score est maximum à epsilon près. L'analyse des diffé-rentes mesures de performance spécifiques à la classification multi-étiquette a été nécessaire pour déterminer la plus petite valeur de epsilon adéquate. En effet la plupart des mesures augmentent quand epsilon augmente et que tous les aligneurs deviennent acceptables. Cependant l'AUC nous a permis d'identifier un premier pic pour epsilon égal à 0,01. À cette valeur, le taux de bonne prédiction de l'ensemble, c'est-à-dire la prédiction d'exactement tous les aligneurs acceptables et seulement eux, est de 25%, c'est-à-dire nettement au dessus de l'aléatoire à 1/128 = 0, 8%. Cette mesure est particulièrement exigeante. La plupart des autres mesures de performance sont à 75%, en particulier la précision et le rappel micro et macro.
Les perspectives de ce travail sont nombreuses. Une première perspective concerne l'apprentissage de tri (ranking) multi-étiquettes. Une autre perspective consiste à tenir compte de

Introduction
Le monitoring du trafic routier est effectué, dans la majorité des cas, grâce à des capteurs dédiés qui permettent d'estimer le nombre de véhicules traversant la portion routière sur laquelle ils sont installés. Les coûts prohibitifs d'installation et de maintenance pour ce genre de capteurs limitent leur déploiement au réseau routier primaire (c.à-d. les autoroutes et les grandes artères seulement). Par conséquent, ce genre de solutions produit une information incomplète sur l'état du réseau routier, ce qui complique l'extraction de connaissances sur la dynamique des mouvements dans ce réseau et sur l'adéquation entre le réseau et son usage.
Une solution alternative (ou complémentaire) consiste à exploiter des traces GPS d'objets mobiles recueillies par des dispositifs ad hoc (par exemple des smartphones). Ces traces peuvent être obtenues lors de campagnes d'acquisition spécifiques (bus, taxis, flotte d'entreprise, etc.) ou par des mécanismes de crowdsourcing en proposant à des utilisateurs de soumettre leurs propres trajets. On peut ainsi obtenir un volume important d'information couvrant le réseau de façon beaucoup plus complète que des capteurs.
Le clustering (ou classification non supervisée) figure parmi les techniques d'analyse les plus utiles à de telles fins exploratoires. La majorité des travaux traitant du clustering de trajectoires s'est focalisée sur le cas du mouvement libre (Nanni et Pedreschi, 2006), (Benkert et al., 2006), (Lee et al., 2007), (Jeung et al., 2008) en faisant abstraction des contraintes liées à la topologie du réseau routier, qui jouent pourtant un grand rôle dans la caractérisation de la similarité entre les trajectoires analysées. Parmi les travaux ayant traité le cas contraint (Kharrat et al., 2008) ; (Roh et Hwang, 2010). El Mahrsi et Rossi (2012b) proposent de représenter les relations entre différentes trajectoires sous forme d'un graphe et de s'intéresser au clustering de ce dernier pour découvrir des groupes de trajectoires de profils similaires. Les auteurs étendent ce travail dans (El Mahrsi et Rossi, 2012a) en s'intéressant aux regroupements de segments routierstoujours en se basant sur une représentation par graphes -afin d'enrichir la connaissances des groupes de trajectoires et d'apporter un moyen supplémentaire de les interpréter. Nous proposons, dans cet article, de conserver cette représentation des données sous la forme d'un graphe. Plus précisément, nous modéliserons les relations qu'entretiennent les trajectoires et les segments routiers sous forme d'un graphe biparti et nous étudierons deux approches différentes de classification de ses sommets.
Le reste de l'article est organisé comme suit. La section 2 présente notre modèle de données ainsi que les approches que nous proposons. Section 3 illustre notre étude expérimentale et démontre l'intérêt de ces approches et leur capacité à mettre en valeur des structures de clusters intéressantes tant au niveau des trajectoires qu'au niveau des segments routiers. Enfin, une conclusion sera dressée dans la section 4.
Approches de classification
Dans le cas contraint, une trajectoire T est modélisée sous forme d'une succession de segments routiers appartenant à l'ensemble de tous les segments constituant le réseau. Nous modélisons les données sous forme d'un graphe biparti G = (T , S, E). T est l'ensemble des trajectoires, S est l'ensemble de tous les segments du réseau routier et E est l'ensemble des arêtes modélisant les passages des trajectoires de T sur les segments de S.
Dans un premier temps, nous proposons de projeter le graphe G afin d'étudier séparément les graphes correspondant respectivement aux trajectoires d'une part et aux segments d'autre part (Section 2.1). Dans un second temps, le graphe biparti est traité directement grâce à une approche de biclustering (Section 2.2).
Approche par projections de graphes
La projection du graphe G sur l'ensemble de ses sommets représentant les trajectoires T produit un graphe G T = (T , E T , W T ), décrivant les relations de similarité entre les trajectoires. Une arête e relie deux trajectoires T i et T j si celles-ci partagent au moins un segment routier.
La pondération ? la plus basique de cette arête peut consister en un comptage des segments communs entre les deux trajectoires. Si nous pondérons avec la mesure de similarité proposée par El Mahrsi et Rossi (2012b) la projection coïncide avec la définition du graphe de similarité entre trajectoires, introduite par les mêmes auteurs. C'est cette stratégie de pondération que nous adoptons par la suite. Le poids ? est donc la similarité cosinus entre les deux trajectoires T i et T j exprimée comme suit :
Où w s,T = n s,T ·length(s) s ?T n s ,T ·length(s ) · log |T | |{Ti:s?Ti}| est un tf-idf modifié attribué à chaque segment routier s en fonction de sa longueur, son importance dans la trajectoire T et sa fréquence dans le jeu de données.
De façon analogue, la projection du graphe G sur l'ensemble des segments S produit le graphe G S = (S, E S , W S ) décrivant les relations entre segments routiers. Ici, une arête e relie deux segments s'il y a au moins une trajectoire qui les visite tous les deux. Là également, nous opterons pour la pondération proposée par El Mahrsi et Rossi (2012a) pour affecter les poids W S au lieu d'un comptage simple des trajectoires communes.
Nous nous proposons d'effectuer le clustering de chacun de ces deux graphes de façon isolée (c.à-d. chacun est traité à part) pour obtenir une classification des trajectoires et une des segments. Pour ce faire, nous utiliserons l'algorithme de détection de communautés dans les graphes par optimisation de la modularité préconisé par Noack et Rotta (2009). Ce choix -qui est motivé par la tendance de ces graphes à avoir des sommets à fort degré et par l'efficacité des approches basées sur la modularité dans ce cas précis -n'écarte pas la possibilité d'utiliser d'autres algorithmes de clustering de graphes tels que le clustering spectral (Meila et Shi, 2000) ou le clustering par propagation de labels (Raghavan et al., 2007).
Pour un jeu de données composé de n trajectoires qui parcourent un réseau routier composé de m segments, la complexité algorithmique théorique pour effectuer le clustering de trajectoires est de O(n 3 ) tandis que celle du clustering de segments est de O(m 3 ) (Noack et Rotta, 2009). Cependant, les complexités observées en pratiques sont plutôt quadratiques.
Nous croiserons ensuite les deux classifications et essayerons d'interpréter chacune d'entre elles en fonction de l'autre.
Approche par biclustering
Nous proposons ici d'étudier directement le graphe sous sa forme bipartie G = (T , S, E). Pour cela, nous appliquons une approche de biclustering sur la matrice d'adjacence du graphe : les segments sont représentés en colonnes, les trajectoires en lignes et l'intersection d'une ligne et d'une colonne indique le nombre de passages d'une trajectoire sur un segment. Le but d'un biclustering est de réordonner les lignes et les colonnes de manière à faire apparaitre et à extraire des blocs de densités homogènes dans la matrice d'adjacence du graphe biparti G. Une fois ces blocs extraits, on en déduit deux partitions obtenues simultanément, une de segments et une de trajectoires.
Une structure de biclustering, que nous notons M, est définie par un ensemble de paramètres de modélisations décrits dans le Tableau 1. Le but d'un algorithme de biclustering va être d'inférer la meilleure partition du graphe.
Graphe G
Modèle de biclustering M T : ensemble des trajectoires C T : ensemble des clusters de trajectoires S : ensemble des segments C S : ensemble des clusters de segments E = T ? S : ensemble des passages des trajectoires sur les segments C E = C T ? C S : biclusters de trajectoires et de segments TAB. 1 -Notations.
En appliquant ce type d'approches, les trajectoires sont regroupées si elles parcourent des segments communs et les segments sont regroupés s'ils sont parcourus par des trajectoires communes. L'avantage de cette technique est qu'elle ne requière pas de pré-traitement sur les données, ni de définition de mesure de similarité entre trajectoires ou entre segments. L'inconvénient principal réside dans la complexité algorithmique de ce type d'approches qui peut s'avérer très élevée.
Nous choisirons ici d'utiliser l'approche MODL (Boullé, 2011) afin d'inférer notre structure de biclustering. Cette approche non-paramétrique a des capacités de passage à l'échelle nous permettant de l'utiliser pour le problème que nous traitons dans cet article. Un critère est construit suivant une approche MAP (Maximum A Posteriori) :
abord, une probabilité a priori P (M) dépendant des données est définie. Elle spécifie les paramètres de modélisation en attribuant à chacun d'eux une pénalisation correspondant à leur longueur de codage minimale, obtenue grâce aux statistiques descriptives des données. Ainsi, plus une structure de biclustering sera parcimonieuse, moins elle sera coûteuse. Ensuite, la vraisemblance des données connaissant le modèle P (D|M) est définie. Elle mesure le coût de recodage des données D avec les paramètres du modèle M. Donc, le modèle de biclustering le plus probable est le modèle le plus fidèle aux données initiales. En d'autres termes, la vraisemblance favorise les structures informatives. La définition du critère global est donc un compromis entre une structure de biclustering simple et synthétique, et une structure fine et informative.
D'un point de vue algorithmique, l'optimisation est réalisée à l'aide d'une heuristique gloutonne ascendante, initialisée avec le modèle le plus fin, c'est-à-dire avec un segment et une trajectoire par cluster. Elle considère toutes les fusions entre les clusters et réalise la meilleure d'entre elles si cette dernière permet de faire décroitre le critère optimisé. Cette heuristique est améliorée avec une étape de post-optimisation, pendant laquelle on effectue des permutations au sein des clusters. Le tout est englobé dans une métaheuristique de type VNS (Variable Neighborhood Search, Hansen et Mladenovic (2001)) qui tire profit de plusieurs lancements de l'algorithme avec des initialisations aléatoires différentes. L'algorithme est détaillé et évalué dans Boullé (2011).
La complexité algorithmique est en O(|E| |E| log(|E|)) avec |E| le nombre d'arcs du graphe biparti G, qui correspondent, dans le cas présent, au nombre de passages de trajectoires sur les segments. Cette complexité est calculée au pire des cas, c'est-à-dire lorsque chaque trajectoire couvre chaque segment (matrice d'adjacence du graphe biparti pleine). En pratique, l'algorithme est capable d'exploiter l'aspect creux habituellement observé dans ce type de données.
Étude expérimentale
Nous décrivons les données utilisées dans cette étude dans la section 3.1. Les résultats obtenus et leur interprétation sont donnés dans la section 3.2 et la section 3.3.
Données utilisées
Afin de tester notre proposition, nous utilisons des jeux de données synthétiques étiquetées (c.à-d. générées de façon à contenir des clusters de trajectoires qui sont supposés être les clusters naturels par la suite). La stratégie de génération de ces données est la suivante. L'espace couvert par le réseau routier (le rectangle minimal englobant tous ses sommets) est quadrillé en grille contenant des zones rectangulaires de tailles égales. Un cluster de trajectoires est alors généré comme suit. Une zone dans la grille du réseau routier est sélectionnée au hasard. Tous les sommets inclus dans cette zone sont sélectionnés pour jouer le rôle de points de départ éventuels pour les trajectoires appartenant au cluster. De façon similaire, une deuxième zone est sélectionnée au hasard et ses sommets sont retenus pour jouer le rôle de points d'arrivée. Pour chaque trajectoire à inclure dans le cluster, un sommet de départ (resp. d'arrivée) est tiré au hasard parmi les sommets de départ (resp. d'arrivée). La trajectoire est générée comme étant le plus court chemin reliant les deux sommets sélectionnés. Le nombre de trajectoires dans chaque cluster est fixé au hasard entre deux seuils paramétrables.
Pour illustrer les différentes informations qu'on peut tirer avec l'approche proposée et pour des soucis de clarté et de visibilité nous nous contentons de montrer les résultats obtenus sur un jeu de données composé de 85 trajectoires seulement. Ces trajectoires sont répandues sur cinq clusters distincts (cf . FIG. 1 
Analyse des clusters de trajectoires
Le clustering par optimisation de la modularité du graphe des trajectoires produit, au départ, un partitionnement contenant trois clusters seulement et ne détecte donc pas les clusters naturels présents dans les données. Ce problème de résolution est d'ailleurs l'une des limitations des approches basées sur la modularité où certaines communautés restent fusionnées et ne sont donc pas détectées. Cependant, l'implémentation que nous utilisons (celle décrite dans Rossi et Villa-Vialaneix (2011)) résout ce phénomène en effectuant une descente récursive sur les communautés découvertes et produit donc une hiérarchie de clusters emboités. Le deuxième niveau de cette hiérarchie révèle l'existence de huit clusters. La matrice croisée de ceux-ci avec les clusters naturels est illustrée dans TAB. 2 qui montre que les clusters trouvés sont purs. Trois des clusters originaux ont été retrouvés de façon exacte tandis que les deux autres ont été éclatées sur plusieurs clusters plus fins (le cluster 1 est éclaté en trois classes et le cluster 3 sur deux). Ce choix de "sur-partitionnement" reste, cependant, tout à fait légitime et justifiable au vu des différences assez notables entre trajectoires constituant chacun de ces deux clusters. Le biclustering génère une partition des trajectoires fidèle aux motifs générés aléatoirement. La matrice de confusion (Tableau 3) montre que les classes de trajectoires retrouvées par le biclustering sont pures, seules deux classes artificielles ont été scindées en deux par la méthode MODL. Cette technique sera donc préférée puisqu'elle découvre des motifs similaires aux motifs obtenus par maximisation de modularité, en étudiant directement le graphe biparti, se passant ainsi de toute projection et pré-traitements.
Analyse croisée des clusters
Nous proposons maintenant d'étudier la matrice d'adjacence du graphe biparti d'origine. On a réordonné les lignes et les colonnes de cette matrice de manière à rapprocher les trajectoires et les segments regroupés dans les mêmes clusters (voir Figure 2).
On observe dans le cas de l'étude de graphes projetés (Figure 2(a)) que les clusters regroupent des segments parcourus par les mêmes trajectoires, peu importe la quantité de trafic supportée. Les segments peu empruntés seront donc rattachés aux segments très empruntés par les rares trajectoires communes. Cela se caractérise dans la matrice par la présence de cellules (intersection des clusters de trajectoires et de segments) avec des distributions hétérogènes : certains segments sont couverts par toutes les trajectoires, d'autres ne sont parcourus que par quelques trajectoires.  A contrario, les clusters de segments obtenus par biclustering sont corrélés avec leur usage. On va donc pouvoir caractériser ces usages dans le réseau et ainsi détecter les hubs (Figure 3(a)), les axes secondaires (Figure 3(b)) ou encore les ruelles peu empruntées. Le résultat obtenu ici est donc une caractérisation de la structure topologique sous-jacente du réseau, dont l'information sur les usages est apportée par les trajectoires. Cela se matérialise sur la Figure 2 Définition (Contribution à l'information mutuelle). La contribution à l'information mutuelle, notée mi(c S , c T ), est définie de la manière suivante :
où P (c S , c T ) est le probabilité pour un passage d'appartenir à une trajectoire de c T et de couvrir un segment de c S , P (c S ) est la probabilité de parcourir un segment du cluster c S et P (c T ), la probabilité d'être sur une trajectoire de c T .
Une contribution positive à l'information mutuelle signifie que le nombre de passages des trajectoires du cluster c T sur les segments du cluster c S est supérieur à la quantité de trafic attendu en cas d'indépendance des clusters de trajectoire et de de segments. Dans le cas d'une contribution négative, on observe une quantité de trafic inférieure à la quantité attendue. Enfin, une contribution à l'information mutuelle nulle montre une quantité attendue de trafic ou alors un trafic très faible ou nul.
La Figure 4(b) présente les contributions à l'information mutuelle de chaque couple de biclusters. Le bicluster en haut à gauche est très caractéristique dans le sens où le cluster de segments n'est traversé que par un cluster de trajectoires et le cluster de trajectoire passe principalement par ce cluster de segments. Dans le cas présent, le cluster de trajectoires contient 21,6% des trajectoires étudiées et le cluster de segments 17,3% des segments du jeu de données. On s'attend donc, en cas d'indépendance, à observer 21,6% × 17,3% = 3,7% des parcours totaux. Or ici, on observe 17,3% du parcours totaux, ce qui représente un important excès de trafic sur le groupe de segments par le groupe de trajectoires, par rapport au résultat attendu en cas d'indépendance.
L'information mutuelle présente une information différente de celle apportée par la matrice de fréquence. On observe sur certains clusters de segments, un nombre de parcours significatifs par plusieurs clusters de trajectoires. Ce type de clusters est caractéristique des hubs routiers. Certains de ces clusters présentent peu de contrastes en terme d'information mutuelle, ce qui signifie que, malgré la nature de hub du cluster, le trafic y est plutôt bien réparti.
Conclusion
Dans cet article nous avons étudié la classification des données de trajectoires sous un angle de clustering de graphes bipartis. L'apport principal de cette étude se situe sur le plan méthodo-logique où nous avons montré l'intérêt de ce genre d'approches pour extraire des connaissances utiles sur le comportement des usagers du réseau routier. Nous avons notamment étudié le problème, dans un premier ordre, comme étant un problème de détection de communautés dans deux graphes séparés décrivant les trajectoires d'une part et les segments routiers d'une autre part. Nous avons, ensuite, étudié le biclustering direct du graphe biparti décrivant les trajectoires et les segments en même temps. Les algorithmes de clustering utilisés ici (par optimisation de la modularité dans les cas des projections du graphe biparti et MODL pour le biclustering) servent à illustrer l'intérêt de notre formulation du problème. Il est donc tout à fait possible de les remplacer par d'autres algorithmes de clustering de graphes tels que le clustering spectral.
Il serait intéressant de tester nos approches sur des données réelles et d'en comparer les résultats avec des faits réels. Il est également intéressant d'étudier leur comportement en présence de données bruitées où les clusters à découvrir sont moins évidents.

Résumé
L'accès croissant à une information pléthorique et le développement de gisements de données ambitieux posent aujourd'hui deux grands types de difficultés aux historiens.
Le premier consiste à mettre en relation des gisements qui ont été développés de manière indépendante. C'est par exemple le cas pour l'intégration d'un ensemble de bases de données prosopographiques développées entre 1980 et 2010 au Lamop, ou même dans le cadre d'un projet dont le seul lien est une problématique spatiale et temporelle (projet ANR-DFG, Euroscientia).
Le deuxième tient en la nature des données introduites dans ces différents systèmes : elles sont souvent hétérogènes, ambiguës, floues. Pour que le chercheur puisse se les approprier, les données doivent faire l'objet d'un véritable travail, afin de comprendre comment elles ont été obtenues, structurées. L'historien doit donc les évaluer et les valider s'il souhaite les mettre en relation. Cette évaluation nécessitant, elle-même de pouvoir être commentée, partagée et critiquée par d'autres chercheurs.
Dans les deux cas, il est nécessaire de développer des outils d'appropriation, qui permettent d'entrer dans le réel historique contenu dans les stocks de données. C'est là la fonction du projet Histobase, un système permettant d'entrer dans la structuration des gisements, d'en évaluer l'information, d'ajouter des couches d'interprétation (qualification de l'information historique) de les évaluer et de partager les données « obtenues ». Chacune des analyses individuelles et collectives fait l'objet d'une mémorisation. Il faut pour cela laisser une place importante aux historiens en tant qu'expert en prêtant une attention particulière aux processus métiers qu'ils mettent en oeuvre.
Biographie
Stéphane Lamassé et Julien Alerini sont docteurs en histoire médiévale et moderne et enseignent à l'Université de Paris 1.

Introduction
La classification de séries temporelles (TSC) est un sujet qui a été intensivement étudié durant les dernières années. Le but est de prédire la classe d'un objet (une série temporelle ou courbe) ? i = 1 , x 1 ), (t 2 , x 2 ), . . . , (t mi , x mi ) (où x k , (k = 1..m i ) est la valeur de la courbe au temps t k ), étant donné un ensemble de séries temporelles labellisées d'apprentissage. Les problèmes de TSC sont différents des problèmes de classification supervisée dans les bases transactionnelles puisqu'il y a une dépendance temporelle entre les attributs ; ainsi l'ordre des attributs importe. La TSC est applicable dans de nombreux domaines dont les données sont des séries temporelles : e.g., pour le diagnostic médical (par exemple la classification d'élec-trocardiogramme de patients) mais aussi dans d'autres domaines comme la maintenance de machines industrielles, la finance, la météo, . . . Le grand nombre d'applications a succité de nombreuses approches ; toutefois la majorité de la communauté s'est attachée à suivre le processus suivant (Liao, 2005) : (i) choisir une nouvelle représentation des données, (ii) choisir une mesure de similarité (ou une distance) pour comparer deux séries temporelles et enfin (iii) utiliser l'algorithme (NN) du plus proche voisin (avec la mesure choisie sur la représentation choisie) comme classifieur. Ding et al. (2008) propose un état de l'art des différentes représen-tations et mesures ainsi qu'une étude expérimentale comparative basée sur le classifieur NN. Il en ressort que le classifieur NN couplé avec une distance Euclidienne ou Dynamic Time Warping (DTW) présente les meilleures performances prédictives pour les problèmes de TSC.
Plus récemment, Bagnall et al. (2012) démontre expérimentalement que les performances de certains classifieurs augmentent fortement en utilisant certaines représentations (par rapport au domaine temporel original) ; ainsi, pour un classifieur donné, il existe une forte variance de performance selon la transformation de données utilisée. Pour pallier ce problème, Bagnall et al. (2012) proposent une méthode ensembliste basée sur trois représentations (ainsi que sur les données originales) : les résultats expérimentaux démontrent (i) l'importance de la repré-sentation dans les problèmes de TSC et (ii) qu'une simple combinaison ensembliste de plusieurs représentations permet d'atteindre des performances prédictives très compétitives. Nous adhérons à cette conclusion sur l'importance des représentations ; toutefois une des faiblesses de certains classifieurs ensemblistes est la perte en interprétabilité due à la combinaison (par pondération) des classifeurs.
Un exemple illustratif : les graphiques de la figure 1 confirment l'intérêt du changement de représentation : si à partir des données originales (a), il n'est pas évident de différencier les deux classes (bleu/rouge), de simples transformations (ici l'intégrale cumulative (b) et la double intégrale cumulative (c)) facilitent la discrimination des classes. En effet, après transformation par double intégrale cumulative, les courbes (séries) ayant des valeurs supérieures à 100 sont bleues et les courbes avec des valeurs inférieures à -100 sont rouges. Sur cet exemple jouet (extrait de la base TwoPatterns de la base de l'UCR (Keogh et al., 2011)), une transformation et deux descripteurs nous permettent de caractériser les deux classes de courbes. Dans cet article, nous proposons un processus de construction de descripteurs interprétables pour le problème de TSC. Notre contribution est donc essentiellement méthodologique. La section suivante décrit les différentes étapes de notre processus : (i) une étape de transformation des données originales en de nouvelles représentations ; (ii) une étape de coclustering ; (iii) l'exploitation des résultats du coclustering pour construire de nouveaux descripteurs et ainsi une nouvelle base de données ; et enfin le classifieur utilisé. La section 3 rapporte la validation expérimentale de notre processus.
Processus de construction de descripteurs
Notations. Pour le problème de classification supervisée de séries temporelles (TSC), une série temporelle est définie par une paire (? i , y i ) où ? i est un ensemble d'observations ordonnées ? i = 1 , x 1 ), (t 2 , x 2 ), . . . , (t mi , x mi ) de longueur m i et y i une valeur de classe. Une base de données de séries temporelles D est définie comme un ensemble de paires D = {(? 1 , y 1 ), . . . , (? n , y n )}, où chaque série temporelle peut avoir un nombre d'observations différent donc une longueur différente 1 . Le but est de construire un classifieur à partir de D pour prédire la classe de nouvelles séries temporelles ? n+1 , ? n+2 , . . .
Pour ce faire, nous appliquons le processus de construction de descripteurs décrit par la figure 2 dont chaque étape est détaillée dans la suite.
Data transformation
Recoding data 
Transformations/Représentations
De nombreuses méthodes de transformation ont été proposées dans la littérature pour représenter les séries temporelles : par exemple les transformations polynomiales, symboliques, spectrales, en ondelettes, . . . (voir (Ding et al., 2008) pour une vue synthétique sur les repré-sentations). Pour notre processus, nous utilisons les données originales ainsi que six représen-tations :
Les dérivées : DV et DDV Nous utilisons les dérivées et dérivées doubles des séries temporelles (i.e. les différences et différences doubles locales entre les valeurs au temps t et t ? 1).
Ces transformations nous permettent de représenter l'évolution locale (croissante/décroissante, accélération/décélération) des séries.
Les intégrales cumulatives : IV et IIV Nous utilisons aussi les intégrales cumulatives (simples et doubles) des séries temporelles (calculées via l'approximation par la méthode des trapèzes).
Ces transformations nous permettent de représenter l'évolution globale (accumulée) des séries.
Le spectre de puissance : PS. Une série temporelle peut-être décomposée en une combinaison linéaire de sinusoïdes d'amplitudes p, q et de phase w. Ainsi :
On appelle transformée de Fourier la série de paires ? i,F T = 1 , q 1 ), . . . (p mi , q mi ) Et, ? if le spectre de puissance (PS) est obtenu par la somme des carrés des coefficients de Fourier :
Les f k représentent la fréquence et les a k la puissance du signal. Cette transformation nous permet de représenter la série dans le domaine de fréquence.
et où ¯ x et s 2 sont la moyenne et la variance de la série originale. L'ACF décrit comment les valeurs originales séparées par une certaine durée évoluent ensemble. L'ACF permet de détecter des structures d'autocorrélation dans les séries temporelles.
Ainsi pour une base de données de séries temporelles D orig , nous construisons six nouvelles bases de données :
Dans la suite, par souci de généralisation, un objet d'une de ces représentations sera appelé "courbe" au lieu de série temporelle puisque D P S n'est plus dans le domaine temporel.
Coclustering
Une courbe peut être vue comme un ensemble de points (X, Y ) décrits par ses valeurs en abscisses et en ordonnées. Un ensemble de courbes peut être vu comme un ensemble de  (Ramsay et Silverman, 2005), elle s'adapte bien pour le cas particulier des courbes comme définies ci-dessus. KHC est libre de tout paramétrage utilisateur, robuste (évite le surapprentissage), supporte des bases de données de courbes de plusieurs millions de points et sa complexité en temps est de ?(N ? N log N ) où N est le nombre de points de la base : c'est donc une méthode adaptée à notre problématique.
KHC est une méthode basée sur l'estimation de densité constante par morceaux et suit l'approche MODL (Boullé, 2006) (similaire à une approche Bayésienne (MAP) Maximum A Posteriori). Le modèle optimal M , i.e., la grille optimale est obtenue par optimisation (gloutonne bottom-up) d'un critère Bayésien qui mise sur un compromis entre précision et robustesse du modèle :
La grille obtenue constitue un estimateur non-paramétrique de la densité jointe des courbes et dimensions des points. Du point de vue de la théorie de l'information, selon Shannon (1948), les logarithmes négatifs de probabilités s'interprètent comme des longueurs de codage. Ainsi, le critère cost peut être interprété comme la longueur de codage du modèle (la grille) plus la longueur des données D connaissant le modèle M , selon le principe de Minimum Description Length (MDL (Rissanen, 1978)).
Un exemple de visualisation de résultat de coclustering. La figure 3 présente un exemple de visualisation de deux clusters de courbes de la grille optimale obtenue pour la base de données TwoPatterns (transformée par IIV). Le graphique (a) (resp. (b)) présente un cluster dont les courbes sont majoritairement de classe c 1 (bleu dans l'exemple introductif de la figure 1),
FIG. 3 -Représentation de l'information mutuelle des cellules pour deux clusters de courbes obtenus par la méthode KHC sur la base TwoPatterns entière (transformée par IIV) : (a) cluster dont les courbes sont majoritairement de classe c 1 ; (b) cluster dont les courbes sont majoritairement de classe c 2 . Plus les couleurs sont vives, plus la différence de distribution de points entre la cellule courante (donc du cluster) et le reste des données est significative.
(resp. c 2 , rouge dans l'exemple introductif). La grille optimale obtenue par KHC est composée de 133 clusters de courbes, 7 intervalles pour X et 22 intervalles pour IIV . L'estimateur de densité jointe obtenue (i.e. la grille optimale) est plus fin que nécessite le problème de départ : en effet, la base TwoPatterns est un problème de classification à 4 classes or nous obtenons 133 clusters de courbes ; ce qui nous donne un potentiel de caractérisation fine des classes du problème, lorsque la représentation s'y prête.
Construction de descripteurs
A partir de chaque résultat de coclustering obtenus sur chacune des représentations utilisées -k C attributs numériques (un pour chaque cluster C de courbes issu de M rep ) dont la valeur pour une courbe c id est la distance définie par d(c id , C) = cost(M rep,c id ?C ) ? cost(M rep ), i.e., la différence de coût entre le modèle optimal M rep et M rep,c id ?C , la grille optimale dans laquelle on a intégré la courbe c id au cluster de courbes C. Intuitivement, la distance d mesure la perturbation qu'apporte l'intégration d'une courbe à un cluster de la grille optimale.
-Un attribut catégoriel indiquant l'index du cluster de courbes i C le plus proche d'un objet courbe c id selon la distance définie ci-dessus (i.e. au sens du critère cost utilisé pour l'optimisation de la grille). -k Y attributs numériques (un pour chaque intervalle i Y de Y issu de M rep ) dont la valeur pour une courbe c id est le nombre de points de c id dans l'intervalle i Y . Ainsi pour une courbe donnée c id , nous avons les informations suivantes fournies par les descripteurs (pour chaque représentation) : (i) la distance de c id à tous les clusters de courbes, (ii) l'index du cluster de courbes le plus proche et (iii) le nombre de points de c id dans chaque intervalle de Y .
Classification supervisée
Nous avons vu que notre processus de construction de descripteurs peut générer des centaines de descripteurs par représentation. Ainsi, l'ensemble total d'attributs générés F tot peut contenir plusieurs milliers d'attributs. Le classifieur en fin de processus doit pouvoir supporter un grand nombre d'attributs et doit être capable de sélectionner les attributs pertinents pour la tâche de classification supervisée. Nous choisissons le classifieur sélectif Naive Bayes (SNB (Boullé, 2007)) qui répond à ces attentes. Notons aussi que le prédicteur SNB exploite des prétraitements (de type MODL) de variables numériques par discrétisation et de variables catégorielles par groupement de valeurs en utilisant des estimateurs de densité conditionnelle robustes. Ainsi les varibles construites profitent de ces prétraitements et offrent un potentiel d'interprétabilité (voir section 3). De plus, le SNB est libre de tout paramétrage utilisateur, ce qui facilite l'utilisation de l'ensemble du processus.
Validation expérimentale
L'implémentation de notre processus utilise des outils déjà existants (KHC pour le coclustering et SNB pour la classification supervisée, disponibles sur http://www.khiops.com). Le branchement entre ces outils est encore au stade de prototype et a été réalisé avec MATLAB.
Protocole. Pour valider notre processus, nous utilisons 26 bases de données de classification de séries temporelles : 17 bases de l'UCR (Keogh et al., 2011) et 9 nouvelles bases introduites dans . Une description succinte des caractéristiques de ces données est présentée dans la table 1. Cet ensemble de données présente une grande variété de bases tant en terme d'applications, qu'en terme de nombre d'instances, de classes et en longueur de série. Les expériences sont menées en suivant un protocole train-test prédéfini pour chaque base. Nous comparons notre processus de classification, qu'on appellera ici MODL-TSC, avec : (i) DTW-NN un classifieur basé sur le plus proche voisin et la distance Dynamic Time Warping, considéré par la littérature comme difficile à battre ; (ii) TSC-ENSEMBLE  qui exploite de multiples représentations via une méthode ensembliste et l'agorithme du plus proche voisin (NN).
Notons que depuis 2012, il existe d'autres bases de données répertoriées par l'UCR pour la TSC. Toutefois, nous nous limitons à ces 26 bases pour nos expérimentations comparatives car les performances prédictives de nos concurrents (rapportées de  TAB. 1 -Description des bases de données de séries temporelles.
sont accessibles que pour ces bases. D'autre part, les bases de séries de l'UCR sont un cas particulier du cadre général dans lequel nous nous plaçons, puisque pour une base donnée, toutes les séries sont de la même longueur et utilisent le même domaine temporel (i.e., les t k sont identiques).
Résultats. Les résultats en terme de taux d'erreur sont reportés dans la table 2. Le meilleur résultat pour chaque base est mis en gras. Premièrement, les résultats globaux (Taux d'erreur moyen, nombre de victoires et rang moyen) indiquent que MODL-TSC est très compétitif par rapport aux deux méthodes de l'état de l'art. Même si nous avons l'avantage numérique, cet ensemble de données ne nous permet pas de montrer qu'il y a une différence significative de performance entre les trois méthodes. En effet, nous avons procédé au test de Friedman (Demsar, 2006) et ne pouvons rejeter l'hypothèse nulle.
Notons les performances remarquables de MODL-TSC sur les bases OSULeaf, FordA, ElectricDevices et ARSim. Sur ces bases, la différence de performance est d'au moins 0,1 (i.e. 10%) par rapport à ses concurrents. Ici, l'apport des représentations (via les nouveaux descripteurs) est certainement à l'oeuvre dans notre processus, alors que TSC-ENSEMBLE n'exploite que 3 représentations et que DTW-NN se base sur les données originales. A l'inverse, les performances de MODL-TSC sont dramatiques pour les bases ECG200, Coffee et OliveOil. La différence de performance tourne en notre défaveur (au moins 0,1 par rapport aux deux concurrents). Nous pensons que cette différence peut être dûe à deux raisons : (i) les bases d'apprentissage de ECG200, OliveOil et Coffee sont très petites (quelques dizaines de courbes), ce qui rend l'apprentissage difficile ; (ii) nous n'avons pas encore trouvé la bonne représentation qui Ces expériences rappellent l'importance des représentations pour la TSC d'une manière gé-nérale, et en particulier dans notre processus. Nous pouvons donc espérer une amélioration des performances prédictives en rajoutant des représentations de la littérature dans notre processus.
Interprétation : un exemple. Pour la représentation IV de la base TwoPatterns, la grille optimale obtenue par KHC est composée de 224 clusters de courbes, 11 intervalles pour X et 9 intervalles pour Y IV . Les deux variables les plus pertinentes (parmi toutes les variables générées à partir de toutes les représentations) selon les prétraitements MODL du SNB sont issues de la représentation IV et sont :
1. v 1 , le nombre de points dans l'intervalle I Y IV =] ? ?; ?3, 9082] 2. v 2 , l'index du cluster le plus proche Le groupement de valeurs pour v 2 et la discrétisation pour v 1 fournissent les tables de contingence suivantes en apprentissage (cf tables 3 et 4) : Nous observons (table 3) que le nombre de points p d'une courbe dans I Y IV (i.e. le nombre de points dont la valeur est inférieure à -3,9082) est pertinent pour caractériser sa classe. En effet, en apprentissage, les courbes telles que p ? 7 sont de classe c 1 ; lorsque p > 29 elles sont très majoritairement de classe c 4 et lorsque 7 < p ? 12 elles sont majoritairement de classe c 3 .
Dans la table 4, nous observons tout d'abord que le prétraitement supervisé par groupement de valeurs MODL sur la variable v 2 ("index du cluster de courbes le plus proche") produit 4 groupes : G 1 , (resp. G 2 , G 3 et G 4 ) constitués de 56, (resp. 53, 53 et 62) index de clusters qui sont majoritairement de classe c 4 (resp. c 3 , c 2 , c 1 ). La forme diagonale de la table de contingence (table 4) indique la pertinence de l'attribut v 2 pour caractériser la classe d'une courbe. En effet, par exemple, si i C l'index du cluster de courbes le plus proche d'une courbe c id appartient au groupe G 2 (i.e. i C ? G 2 ), alors c id est considéré comme très similaire aux courbes de classe c 3 . De plus, la variable "index du cluster de courbes le plus proche" est un indicateur de la pertinence de la représentation pour notre processus dans le problème courant de TSC. Dans cet exemple, la variable v 2 à elle seule permet de caractériser environ 95% de la base, donc la représentation IV est très pertinente pour caractériser les classes de la base TwoPatterns. A l'inverse, pour la représentation originale (D V ), la grille optimale générée par KHC est composée de 255 clusters de courbes mais le prétraitement indique que la variable "index du cluster de courbes le plus proche" n'est pas pertinente pour caractériser les classes de la base TwoPatterns.
Conclusion & Perspectives
Nous avons proposé MODL-TSC, un processus générique de construction de descripteurs pour le problème de la classification supervisée de séries temporelles (TSC). Ce processus est libre de tout paramétrage utilisateur et donc simple d'utilisation. Il se décompose en trois étapes : (i) génération de multiples représentations des données par le biais de transformations ; (ii) application d'une technique de coclustering sur chacune des représentations ; iii construction de descripteurs à partir des résultats du coclustering. La nouvelle base de données objets-attributs -dont les objets (identifiant les séries temporelles) sont décrits par des attributs issus des diverses représentations générées-est notre base d'apprentissage. Pour classer de nouvelles séries nous utilisons un classifieur naïf Bayésien sélectif. Les résultats expéri-mentaux ont montré que les performances prédictives de MODL-TSC sont très compétitives et comparables aux meilleures approches de la littérature.
Les premiers résultats expérimentaux sont prometteurs et confirment l'importance des transformations dans la TSC. En effet, selon les applications, certaines transformations faciliteront la découverte de motifs caractérisant les classes de séries temporelles. De plus, la combinaison de plusieurs représentations par le biais de notre processus MODL-TSC permet d'atteindre des performances prédictives très compétitives. Nous avons utilisé quelques représentations simples dans ces travaux préliminaires pour démontrer le bien fondé de ce processus de construction de descripteurs -ce qui nous laisse un potentiel d'amélioration des performances pour les données où MODL-TSC est moins performant que ses concurrents, pour peu qu'on trouve la bonne représentation. "Chercher la ou les bonnes représentations via une transformation" est certainement la principale perspective à ce travail et ce que nous pouvons recommander à ceux qui s'intéressent à la TSC. La littérature sur la TSC regorge de représen-tations (voir (Wang et al., 2012) pour une vue d'ensemble) et trouver une bonne représentation pour la TSC est toujours un sujet d'actualité (e.g. ). D'autre part, une perspective pratique sera d'identifier la ou les bonnes représentations pour un domaine d'application spécifique (e.g., ECG, consommation électrique, . . . ).

Introduction
La découverte de motifs est une tâche centrale en fouille de données et est utilisée avec succès dans un grand nombre d'applications. Une limite bien connue des processus de fouille de données est la production d'un grand nombre de motifs qu'il n'est pas possible d'examiner manuellement et parmi lesquels l'information utile est diluée. L'extraction de motifs sous contraintes permet de cibler l'information recherchée selon les centres d'intérêt de l'utilisateur. Un prolongement récent de cette voie de recherche est la prise en compte de l'intérêt d'un motif en fonction des autres motifs extraits, afin de produire des ensembles de motifs qui satisfont des propriétés sur l'ensemble des motifs considérés conjointement (Raedt et Zimmermann, 2007;Khiari et al., 2010). Notre travail se situe dans cette lignée et porte sur la notion de requêtes skylines (Börzsönyi et al., 2001). Notre originalité est d'introduire la souplesse dans la relation de dominance caractérisant les skylines dans le contexte de la fouille de données et de montrer l'apport de la Programmation Par Contraintes (PPC) pour cela.
La notion de skylines a été récemment étendue à la fouille de données pour extraire des motifs skylines (appelés skypatterns) (Soulet et al., 2011). Les skypatterns traduisent les préférences d'un utilisateur selon une relation de dominance. Dans un espace multidimensionnel où chaque dimension définit une préférence, un point p 1 domine un autre point p 2 ssi p 1 est meilleur ou égal à p 2 sur toutes les dimensions, et est strictement meilleur sur au moins une dimension. Par exemple, un utilisateur peut préférer les motifs ayant une fréquence peu élevée, une petite taille et une confiance élevée. Dans ce cas, un motif p 1 domine un autre motif p 2 ssi : f req(p 1 ) ? f req(p 2 ) ? taille(p 1 ) ? taille(p 2 ) ? conf iance(p 1 ) ? conf iance(p 2 ), où au moins une
FIG. 1 -Exemple de points skylines dans un espace à deux dimensions.
de ces inégalités est stricte. Les skypatterns sont intéressants à double titre : ils permettent de s'affranchir de la notion de seuil sur les mesures et la relation de dominance exprime une forme d'intérêt global sur toutes les dimensions qui est facilement compréhensible par l'utilisateur. Néanmoins, le cadre actuel est rigide et des motifs pouvant en réalité s'avérer intéressants (les motifs dominés proches des skypatterns) ne sont pas proposés. L'exemple suivant (portant sur des points skylines) illustre cette situation. Exemple. L'entraîneur d'une équipe de football souhaite recruter un ou plusieurs joueurs pour la prochaine saison. Chaque joueur sur le marché est caractérisé par le nombre de buts qu'il a marqués et le nombre de passes décisives qu'il a effectuées durant la saison actuelle (cf. figure 1). Seuls les joueurs p 1 , p 2 , p 3 , p 4 et p 5 sont des skylines, les autres joueurs (i.e. p 6 , p 7 , p 8 , p 9 et p 10 ) étant dominés par au moins un joueur skyline. Néanmoins, ces joueurs non-skylines peuvent s'avérer intéressants pour un recrutement : -pour le recrutement d'un attaquant, l'entraîneur privilégierait le nombre de buts marqués. Outre les joueurs p 1 et p 2 , les joueurs non-skylines p 6 et p 9 sont des candidats intéressants. -pour le recrutement d'un milieu de terrain offensif, l'entraîneur privilégierait le nombre de passes décisives. Outre les joueurs p 4 et p 5 , les joueurs non-skylines p 7 et p 8 deviennent intéressants. -pour le recrutement d'un joueur polyvalent, l'entraîneur privilégierait plutôt le compromis entre le nombre de buts marqués et le nombre de passes décisives réalisées. Outre les joueurs p 3 et p 4 , le joueur non-skyline p 10 est alors un candidat intéressant. Par ailleurs, les joueurs skylines sont très recherchés et ils sont coûteux parce qu'ils sont fortement sollicités : leurs salaires pourraient être hors de portée du budget du club. Les joueurs non-skylines proches des joueurs skylines peuvent ainsi devenir d'un grand intérêt pour l'entraî-neur. Nous verrons comment de tels joueurs sont découverts grâce à l'introduction de la souplesse dans la relation de dominance. Contributions. Nous proposons une approche nouvelle et efficace pour l'extraction de skypat-terns (durs et souples) en utilisant le cadre PPC. Nous montrons comment l'extraction des (soft-)skypatterns peut être modélisée et résolue avec des techniques PPC. Un tel choix présente deux avantages majeurs. Tout d'abord, nous sommes en mesure d'améliorer l'étape d'extraction grâce à l'ajout de contraintes postées dynamiquement, ces contraintes étant déduites de l'ensemble des motifs candidats déjà extraits. De plus, l'aspect déclaratif du cadre PPC permet de gérer de manière unifiée plusieurs types de relaxation des skypatterns. Finalement, la pertinence de l'approche est mise en évidence par une étude de cas en chémoinformatique pour la découverte de toxicophores.
La section 2 de cet article introduit les concepts de base. La section 3 présente le problème de l'extraction de soft-skypatterns. Notre méthode d'extraction des skypatterns (durs et souples) fondée sur la PPC est détaillée en section 4. La section 5 présente un état de l'art synthétique. Les résultats de nos expérimentations sur la découverte de toxicophores sont donnés en section 6.
Extraction des skypatterns
Extraction de motifs sous contraintes
Soit I un ensemble de littéraux distincts appelés items. Un itemset (ou motif) est un sousensemble non nul de I. La langage d'itemsets correspond à L = 2 I \?. Une base de données transactionnelle est un multi-ensemble de motifs de L. Une entrée de la base de données est appelée transaction et est un élément de L. La table 1 (gauche) présente un ensemble de données transactionnelles D où chaque transaction t i rassemble des articles décrits par des items notés A,. . .,F . L'exemple classique est une base de données de supermarchés dans laquelle chaque transaction correspond à un client et chaque item de la transaction à un produit acheté par ce client. Un prix est associé à chaque produit (cf. table 1, à droite).
La fouille de motifs sous contraintes a pour objectif d'extraire l'ensemble des motifs de L qui satisfont une requête (i.e., une conjonction et/ou disjonction de contraintes). La contrainte de fréquence est l'exemple le plus classique, elle conduit à extraire les motifs X i dont le nombre d'occurrences dans D dépasse un seuil minimal min f r fixé par l'utilisateur : freq(X i ) ? min f r . D'autres mesures quantifient l'intérêt des motifs recherchés comme par exemple la taille (nombre d'items composant un motif), le prix moyen (prixMoy : moyenne des prix associés aux items d'un motif), ou encore l'aire (aire(X i ) = f req(X i ) × taille(X i )). Dans de nombreuses applications, on souhaite caractériser des contrastes entre sous-ensembles de transactions, tels que par exemple en chémoinformatique des molécules toxiques versus non toxiques. A cet effet, le taux de croissance est une mesure de contraste très courante (Novak et al., 2009) que nous utiliserons en section 6. Soit D une base de données partitionnée en deux sous-ensembles 
|D1|×f req(Xi,D2) . Par ailleurs, la communauté porte maintenant une grande attention aux ensembles de motifs (Raedt et Zimmermann, 2007;Khiari et al., 2010;Guns et al., 2011) où l'intérêt d'un motif dépend des autres motifs extraits. La conception de classifieurs, les top-k motifs, ou encore les skypatterns se situent dans cette lignée.
Notion de skypattern
Cette section présente la problématique de l'extraction des skypatterns. Nous commençons par définir la notion de dominance.
Considérons l'exemple de la table 1 et supposons que M ={f req, aire}. Le motif BCD domine le motif BC car f req(BCD)=f req(BC)=5 et aire(BCD)>aire(BC). Pour M ={f req, taille, prixM oy}, le motif BDE domine BCE car f req(BDE)=f req(BCE) = 4, taille(BDE)=taille(BCE) = 3 et prixM oy(BDE)>prixM oy(BCE). Étant donné un ensemble de mesures M , si un motif est dominé par un autre par rapport aux mesures de M , il est considéré comme non-intéressant et ne doit pas être dans le résultat final. Cette idée est au coeur de la notion de skypatterns. DÉFINITION 2 (Opérateur skypattern) : Étant donnés un ensemble de motifs P ? L et un ensemble de mesures M ? M, un skypattern de P par rapport à M est un motif non-dominé dans P par rapport à M . L'opérateur skypattern Sky(P, M ) renvoie tous les skypatterns de P par rapport à M : Sky(P, M ) = {X i ? P | ?X j ? P, X j M X i } Le problème de l'extraction des skypatterns consiste à évaluer la requête Sky(L, M ). Ainsi, pour le jeu de données de la table 1, Sky(L,{f req,size})={ABCDEF, BCDEF, ABCDE, BCDE, BCD, B, E}. Les skypatterns sont représentés à la figure 2(a). L'aire hachurée à la figure 2(a), nommée zone interdite, est la zone où il ne peut pas y avoir de skypatterns. La zone de dominance est au-dessus de la ligne bleu, cette dernière est appelée bordure de l'aire de dominance. Elle marque la frontière entre ces deux zones.
L'extraction des skypatterns est un problème difficile en raison du nombre très élevé de motifs candidats (i.e. |L|) et une énumération naïve de L n'est pas réaliste. Par exemple, avec 1000 items une approche naïve aurait besoin de calculer (2 1000 ? 1)×|M | valeurs de mesures, et ensuite de comparer les 2 1000 motifs entre eux. Dans (Soulet et al., 2011), les auteurs proposent une méthode efficace qui tire partie des relations théoriques entre les représentations condensées de motifs et les skypatterns, rendant ainsi le processus d'extraction faisable lorsque la représentation condensée des motifs peut être extraite. Toutefois, cette méthode est limitée à la version dure de la relation de dominance.
Extraction des soft-skypatterns
Cette section montre comment introduire de la souplesse dans la problématique des skypatterns. L'idée consiste à relâcher la relation de dominance afin de capturer les motifs prometteurs de la zone interdite. À cette fin, nous définissons deux types de soft-skypatterns : les edge-skypatterns qui appartiennent à la bordure de l'aire de dominance (voir la section 3.1) et les ?-skypatterns qui sont proches de cette bordure (voir la section 3.2).
Edge-skypattern
Comme pour les skypatterns, les edge-skypatterns sont définis selon une relation de dominance et un opérateur Sky. Ces deux notions sont reformulées comme suit : DÉFINITION 3 (Dominance stricte) : Étant donné un ensemble de mesures M ? M, un motif X i domine strictement un motif X j par rapport à M , noté X i M X j , ssi ?m?M , m(Xi)>m(Xj). DÉFINITION 4 (Opérateur edge-skypattern) : Étant donnés un ensemble de motifs P ? L et un ensemble de mesures M ? M, un edge-skypattern de P par rapport à M , est un motif non-strictement dominé dans P par rapport à M . L'opérateur Edge-Sky(P, M ) retourne tous les edge-skypatterns de P par rapport à M : Edge-Sky(P, M )={X i ? P || ?X j ? P :X j M X i } Le problème d'extraire les edge-skypatterns consiste à évaluer la requête Edge-Sky(L, M ). Notons que Sky(P, M ) ? Edge-Sky(P, M ). La figure 2(a) illustre les 28 = 7+(4+8+3+4+2) edge-skypatterns extraits de l'exemple de la table 1. On remarque que tous les edge-skypatterns appartiennent à la bordure de l'aire de dominance, 7 d'entre eux sont des skypatterns durs.
?-skypattern
Comme illustré par l'exemple donné en introduction, l'utilisateur peut être intéressé par des skypatterns exprimant un compromis entre les mesures. Les ?-skypatterns expriment un tel compromis.
DÉFINITION 6 (Opérateur ?-skypattern) : Étant donné un ensemble de motifs P ? L et un ensemble de mesures M ? M, un ?-skypattern de P par rapport à M est un motif non-dominé dans P par rapport à M . L'opérateur ?-Sky(P, M ) retourne tous les ?-skypatterns de P par rapport à M : ?-Sky(P, M ) = {X i ? P | ?X j ? P : X j ? M X i } Le problème d'extraction des ?-skypatterns consiste à évaluer la requête ?-Sky(P, M ). Notons que Sky(P, M ) ? ?-Sky(P, M ) (l'égalité a lieu pour ?=0) et que Edge-Sky(P, M ) ? ?-Sky(P, M ). En appliquant l'opérateur ?-Sky(P, M ) sur notre exemple (cf. table 1), avec ?=0.25, nous obtenons 10 = 6+1+3 nouveaux motifs (cf. figure 2(b)), auxquels il faut ajouter les 28 edge-skypatterns (cf. section 3.1).
4 Mise en oeuvre de l'extraction à l'aide de la PPC L'extraction des skypatterns (durs ou souples) s'effectue en deux étapes. La première consiste à accroître la zone interdite par extensions successives. Pour cela, on construit une suite de motifs X 1 , X 2 , ..., X n où chaque X i+1 améliore X i selon au moins une des mesures, étendant ainsi la zone interdite courante. Le processus s'arrête lorsque la zone interdite ne peut plus être étendue. Soit Cand ={X 1 , X 2 , ..., X n } l'ensemble des points ainsi obtenus. Cand constitue un sur-ensemble des skypatterns recherchés, qu'il suffit de filtrer dans une seconde étape. La mise en oeuvre s'effectue de manière simple et déclarative grâce aux CSP dynamiques. L'implantation a été réalisée en Gecode en étendant l'extracteur de motifs (basé CSP) développé par (Khiari et al., 2010).
Exemple introductif
L'exemple de la figure 3 illustre la construction de la zone interdite pour deux mesures m 1 et m 2 . Soit X 1 le premier motif extrait avec <m 1 (X 1 )=2, m 2 (X 1 )=2> ; il ne peut y avoir de skypattern dans l'aire délimitée par X 1 (zone interdite en vert à la figure 3). On recherche alors un motif X qui améliore l'ensemble des éléments de Cand , ici X 1 , selon au moins une des mesures (i.e. tel que (m 1 (X)?2)?(m 2 (X)?2)). Soit X 2 <3, 4> un tel motif (X 2 améliore X 1 pour m 1 et m 2 ), alors la zone bleue devient interdite. On recherche alors un motif X qui améliore X 2 selon au moins une des mesures. Soit X 3 <5, 4> un tel motif (X 3 améliore X 2 pour m 1 ), alors la zone rouge devient interdite. On recherche alors un motif X qui améliore X 3 selon au moins une des mesures. Soit X 4 <6, 3> un tel motif, alors la zone jaune devient interdite. À nouveau, on cherche un motif X qui améliore l'ensemble des éléments de Cand . Soit X 5 <4, 5> un tel motif. Supposons désormais qu'il n'existe aucun motif améliorant X 5 pour au moins une des
deux mesures. L'ensemble Cand des motifs ainsi obtenus contient les motifs X 1 , X 2 , X 3 , X 4 et X 5 et constitue un sur-ensemble des skypatterns. Cand est alors filtré afin de ne retenir que les motifs qui sont des skypatterns.
Extraction des skypatterns à l'aide des CSP dynamiques
Un problème de satisfaction de contraintes (CSP) est défini par un triplet (X , D, C) où X est un ensemble de variables, D est l'ensemble de leurs domaines finis, et C est un ensemble de contraintes portant sur X . Pour l'extraction de motifs en fouille de données (Khiari et al., 2010), X est l'ensemble des motifs inconnus, chacun d'eux ayant L=2 I \? comme domaine. C peut contenir des contraintes arithmétiques sur les mesures, des contraintes ensemblistes sur les motifs, ainsi que des propriétés spécifiques que doivent vérifier certains motifs (par exemple, être un motif fermé pour une ou plusieurs mesures).
Un CSP dynamique (DCSP) est une séquence de CSP, où chaque CSP résulte de changements apportés au précédent (Verfaillie et Jussien, 2005). Ces changements peuvent affecter les variables (ajout/suppression), les domaines (ajout/suppression de valeurs), les contraintes (ajout/suppression). Dans notre approche, variables et domaines restent identiques. Seules sont ajoutées de nouvelles contraintes permettant d'agrandir (dynamiquement) la zone interdite comme l'illustre l'exemple de la figure 3. Considérons le DCSP P 1 , P 2 , . . ., P n où chaque
) où X i est solution de la requête q i (X). La contrainte closed M (X), qui impose que X soit un motif fermé pour les mesures de M , permet de réduire le nombre de motifs redondants. En effet, les skypatterns fermés par rapport à M constituent une représentation condensée exacte de l'ensemble des skypatterns (Soulet et al., 2011).
Cand est l'ensemble des solutions successives obtenues en résolvant le DCSP P 1 , P 2 , ..., P n . Chaque fois qu'un nouveau motif X i (solution de la requête q i (X)) est extrait, on recherche alors un nouveau motif X permettant d'agrandir la zone interdite dans au moins une de ses mesures (requête q i+1 (X)). Le processus s'arrête pour la valeur n 0 telle que la requête q n0+1 (X) ne possède aucune solution (on ne peut plus agrandir la zone interdite). Cand est alors filtré afin de ne conserver que les motifs qui sont des skypatterns.
Extraction des soft skypatterns à l'aide des CSP dynamiques
L'extraction des ?-skypatterns s'effectue de manière analogue aux skypatterns. Le seul changement concerne les contraintes disjonctives ajoutées dynamiquement. Considérons le DCSP P
Comme précédemment, Cand est l'ensemble des solutions successives obtenues en résolvant le DCSP P 1 , P 2 , ..., P n . Chaque fois qu'un nouveau motif X i est extrait, on recherche alors un nouveau motif X permettant d'agrandir la zone interdite. Le processus s'arrête lorsqu'on ne peut plus agrandir cette zone et Cand est alors filtré pour ne contenir que les motifs qui sont des ?-skypatterns.
État de l'art
Le calcul des skylines dans les bases de données s'apparente à la recherche d'un vecteur maximal en géométrie algorithmique (Matousek, 1991), au calcul de la frontière Pareto (Kung et al., 1975) et à la recherche d'une solution optimale en optimisation multi-critères (Steuer, 1992). (Jin et al., 2004) ont introduit la notion de thick skylines dans les bases de données. Un thick skyline est soit un skyline P , soit un point P dominé par un skyline P et tel que P soit proche de P (leur distance est inférieure à un seuil La notion de ?-skypattern que nous proposons est une extension des thick skylines dans le cadre de l'extraction de motifs. La notion de skypattern, ainsi que leur calcul à partir de représentations condensées selon différentes mesures, a été introduite par (Soulet et al., 2011). (Gavanelli, 2002) a proposé une première méthode fondée sur la PPC pour déterminer la frontière Pareto. Cette technique est proche de notre méthode de résolution mais elle est développée uniquement pour les skylines durs portant sur les n-uplets de la base.
Expérimentations
La toxicologie est la science qui étudie les effets toxiques des substances chimiques sur les organismes vivants. Un défi majeur en chémoinformatique est d'identifier les caractéristiques des structures chimiques liées à une activité toxique donnée (e.g. CL50
1 .). Les fragments 2 molécu-laires responsables des propriétés toxiques d'une substance chimique sont appelés toxicophores et leur découverte est bien souvent à la base des modèles de prédiction en (éco)toxicité (Verhaar et al., 1992;Benigni et Bossa, 2011). L'objectif de la présente étude menée en collaboration avec le laboratoire CERMN 3 , est d'étudier l'apport de soft-skypatterns pour la découverte semiautomatique de toxicophores.
Protocole expérimental
Nous avons utilisé un jeu de données issu du site de l'ECHA 4 . Pour évaluer le potentiel toxique d'une substance, des indicateurs de toxicité également appelés endpoints permettent de quantifier leur impact sur la santé humaine et sur l'environnement. Nous avons utilisé l'indicateur quantitatif de toxicité vis à vis des organismes aquatiques CL50. En fonction de la valeur de cet indicateur, les molécules sont réparties dans les trois catégories suivantes : H400 très toxique (CL50 ? 1 mg/L), H401 toxique (1 mg/L < CL50 ? 10 mg/L), et H402 nocif (10 mg/L < CL50 ? 100 mg/L). Dans cette étude, nous nous concentrons uniquement sur les classes H400 et H402 afin de maximiser le contraste entre classes. Le jeu de données D utilisé contient 567 molécules, 372 de la classe H400 et 195 de la classe H402. Les molécules sont représentées en utilisant 1450 sous-graphes fréquents fermés, initialement extraits de D 5 avec un seuil de fréquence de 1%.
Skypattern
Edge-skypattern TAB. 2 -Analyse de la performance de l'extraction de soft-skypattern.
Afin de découvrir des toxicophores candidats, nous combinons des mesures de contraste très utilisées en fouille (Novak et al., 2009) et caractérisant les motifs du point de vue de leur présence dans les données telles que le taux de croissance et la fréquence avec des mesures exprimant des connaissances chimiques telles que l'aromaticité d'une molécule qui est un indicateur connu de la toxicité. Notre méthode offre un moyen naturel de combiner simultanément dans un même cadre ces mesures provenant de différentes origines. Nous présentons maintenant ces mesures. Taux de croissance. Cette mesure permet de caractériser une molécule d'une classe (toxique) par rapport à une autre classe (non-toxique). Une substance chimique qui possède dans sa structure des fragments moléculaires d'un motif avec une valeur élevée du taux de croissance est particulièrement susceptible d'être toxique. Fréquence. Les motifs de faible fréquence sont souvent dus à des artefacts dans les données et constituent du bruit. Afin d'assurer une représentativité de l'information extraite, nous imposons une contrainte de fréquence minimale. Aromaticité. L'intérêt de cette mesure est qu'elle véhicule une hypothèse toxicophore : plus la valeur d'aromaticité est forte, plus la molécule possédant ces fragments moléculaires tend à être toxique puisque ses métabolites peuvent mener à des espèces réactives pouvant interagir de manière néfaste avec des biomacromolécules. L'aromaticité d'un motif est calculée en utilisant la fonction mean = min+max 2 des valeurs d'aromaticité de ses fragments moléculaires. Enfin, la redondance est réduite à l'aide des skypatterns fermés qui sont une représentation exacte condensée de l'ensemble des skypatterns pour ces trois mesures (cf. section 4.2).
Nous avons réalisé plusieurs expérimentations avec différentes combinaisons de ces trois mesures. Pour le paramètre ?, nous avons considéré deux valeurs : 10% et 20%. Une analyse qualitative des soft-skypatterns extraits effectuée par les chimistes permet l'identification de toxicophores connus. Toutes nos expérimentations ont été réalisées sur un processeur Intel core i3 à 2,13 GHz ayant 4 Go de RAM. Nous analysons d'abord les performances de notre approche sur un plan quantitatif. Ensuite, nous procédons à une analyse qualitative de résultats.
Analyse des performances et de la qualité des skypatterns extraits
La table 2 compare les performances des trois opérateurs aussi bien en termes de nombre de skypatterns extraits qu'en temps de calcul, pour différentes combinaisons de mesures.
L'augmentation du nombre de mesures conduit à un plus grand nombre de soft-skypatterns extraits. En effet, un motif domine rarement tous les autres motifs sur l'ensemble des mesures. Néanmoins, dans nos expérimentations, ce nombre reste raisonnablement réduit, au plus, il y a un maximum de 1052 ?-skypatterns. Par ailleurs, les temps de calcul indiquent que notre approche est efficace (moins de 1 heure), même avec l'augmentation du nombre de mesures (sauf pour ? = 0, 2, où le nombre de ?-skypatterns et le temps de calcul augmentent sensiblement). De ces résultats, nous dressons le bilan suivant.
Tout d'abord, en utilisant le taux de croissance et la fréquence, seulement huit skypatterns ont été détectés mais trois toxicophores bien connus ont été retrouvés. Deux d'entre eux sont des composés aromatiques : le chlorobenzène (code smiles 6 : {Clc}) et le phénol {c1(ccccc1)O}). La contamination de l'eau et des sols par les produits chimiques aromatiques est en effet très répandue puisqu'on les retrouve dans de nombreux produits industriels tels que des pesticides, des solvants, des colorants et même des explosifs. Beaucoup de ces produits peuvent s'accumuler dans la chaîne alimentaire et ont un effet néfaste aussi bien sur les plantes que sur les animaux et l'homme. Le troisième toxicophore mis en évidence correspond au motif organophosphate ({OP, OP=S}) qui figure également dans de nombreux pesticides. Concernant les soft skypatterns, aucune information supplémentaire n'est venue enrichir notre liste de toxicophores dans ce premier cas. On peut cependant souligner une énumération plus précise des cycles aromatiques substitués par un atome de chlore (ex. {Clc(ccc)c, Clcccc}) et des organophosphates (ex. {OP(=S)O), COP(=S)O}).
Ensuite, en considérant le taux de croissance et l'aromaticité, ou la fréquence et l'aromaticité, les résultats sont relativement similaires. Bien que la liste des toxicophores détectés grâce aux skypatterns soit limitée en comparaison de celle obtenue précédemment (taux de croissance et fréquence), l'extraction des soft-skypatterns permet d'identifier de façon beaucoup plus exhaustive des cycles aromatiques de différentes natures. En effet, cette nature peut varier en fonction i) de la présence/absence d'hétéroatomes (N, S, O), ii) du nombre de cycles, iii) de la pré-sence/absence de substituants. Les edge-skypatterns permettent ainsi d'extraire des composés aromatiques azotés (ex. indole {ncc, c1cccccc1}, benzoimidazole {ncnc, c1ccccc1}), des composés aromatiques soufrés (ex. benzothiophene {scc, c1ccccc1}), des composés aromatiques oxygénés (ex. benzofurane {coc, c1ccccc1}) et des composés aromatiquess hydrocarbonés (ex. naphthalène {c1ccc2ccccc2c1}). Les ?-skypatterns viennent compléter cette liste avec de nouveaux systèmes polycycliques (ex. biphényle {c1ccccc1c2ccccc2}). Il est également important de noter que dans ce cas, les ?-skypatterns permettent de détecter un autre type de toxicophore très néfaste pour les organismes aquatiques, à savoir les amines aromatiques (ex. aniline {c1(ccccc1)N}).
Enfin, les meilleurs résultats ont été obtenus en considérant simultanément trois mesures (taux de croissance, fréquence et aromaticité). En effet le phénol, le chlorobenzène et le motif organophosphate sont détectés dès l'énumération des skypatterns. De plus, une information concernant les cycles aromatiques azotés est également extraite. Les edge et ?-skypatterns permettent ensuite de retrouver l'ensemble des cycles aromatiques plus "exotiques" discutés précé-demment. De plus, les edge-skypatterns permettent d'identifier de façon beaucoup plus efficace les organophosphates (ex. {COP(=S)O, O(P(OC)=S)C, O(CC)P=S}).
La figure 4 montre la répartition des skypatterns (durs et souples) pour les trois mesures considérées. Les skypatterns durs sont situés dans différentes régions (cf. les patterns p 1 et p 2 et ceux inclus dans les quatre ellipses e 1 , e 2 , e 3 et e 4 ). Le motif p 1 correspond à un cycle aromatique substitué par un atome de chlore, alors que p 2 désigne un motif organophosphate. Les autres skypatterns inclus dans e 1 , e 2 , e 3 et e 4 correspondent respectivement aux cycles aromatiques azotés (ex. {nc}), aux cycles aromatiques alkylés (ex. {cC}), au chlorobenzène et au phénol.
6. Code smiles est une notation en ligne pour décrire la structure des molécules chimiques : http://www. daylight.com/dayhtml/doc/theory/theory.smiles.html Concernant les soft-skypatterns, la plupart des edge-skypatterns sont situés sur la bordure du volume de dominance correspondants aux motifs inclus dans e 1 . Ces motifs complètent la liste des cycles aromatiques qui n'ont pas été trouvés lors de l'extraction des skypatterns durs, tels que les cycles aromatiques soufrés (ex. {cs}) et le biphényle. Enfin, pour les ?-skypatterns, les plus informatifs sont ceux qui se trouvent autour des motifs appartenant à e 1 (ex. naphtalène et aniline).
Conclusion
Dans cet article, nous avons proposé d'introduire de la souplesse dans la problématique des skypatterns. Nous avons montré comment celle-ci permet de découvrir des motifs intéressants qui seraient manqués autrement. En s'appuyant sur la PPC, nous avons proposé une méthode efficace pour l'extraction des skypatterns et des soft-skypatterns. L'apport et l'efficacité de notre approche sont mises en évidence par une étude de cas en chémoinformatique portant sur la découverte de toxicophores. Les résultats expérimentaux montrent l'intérêt d'utiliser les soft-skypatterns afin d'obtenir de nouvelles connaissances en chimie. Dans le futur, nous voulons étudier l'introduction de la souplesse sur de nouvelles tâches comme le clustering, l'apport des soft-skypatterns pour la recommandation et d'étendre notre approche aux skycubes.
Remerciements. Ce travail a été soutenu par l'Agence Nationale de la Recherche, référence ANR-10-BLA-0214. Nous voulons aussi remercier Bertrand Cuissart, Guillaume Poezevara et Ronan Bureau pour les discussions stimulantes et fructueuses à ce sujet.

Introduction
La théorie des hypergraphes se propose de généraliser la théorie des graphes en introduisant le concept d'hyperarête où les arêtes ne relient plus un ou deux sommets, mais un nombre quelconque de sommets. De ce fait, un hypergraphe est défini comme une extension du traditionnel graphe, dans lequel les liens entre des ensembles de sommets sont appelés hyperarêtes. Une traverse minimale correspond à un ensemble de sommets qui intersecte toutes les hyperarêtes d'un hypergraphe, en étant minimal au sens de l'inclusion. L'extraction de ces traverses minimales a fait l'objet de plusieurs travaux dans littérature du fait de la diversité de ses applications dans des domaines variés tels que l'intelligence artificielle, la fouille de données, la cryptographie, le web sémantique, etc. Hagen (2008).
Pour résoudre ce problème, plusieurs approches ont été envisagées. Mannila et Toivonen (1997) ont prouvé que les traverses minimales d'un hypergraphe H peuvent être calculées à partir de la bordure négative d'un ensemble d'itemsets vérifiant la contrainte d'anti-monotonie Mannila et Toivonen (1997), Hébert et al. (2007). C'est en se basant sur ces liens entre la fouille de données et la théorie des hypergraphes que nous proposons une nouvelle définition des traverses minimales, ainsi que leur représentation de manière succincte.
Notre idée, inspirée des travaux de Hamrouni et al. est donc de chercher un sous-ensemble représentant de manière concise et exacte l'ensemble des traverses minimales Hamrouni et al. (2008). Ce sous-ensemble, qu'on appellera ensemble irrédondant de traverses minimales, sera construit en considérant l'ensemble des hyperarêtes auxquelles appartient chaque sommet, appelé extension et en construisant un hypergraphe irrédondant limité à un sous-ensemble des sommets initiaux ayant des extensions différentes. L'espace de recherche s'en trouve alors ré-duit puisque plusieurs candidats ne seront pas générés par l'algorithme. Cette intuition se base sur le fait que si deux sommets X et Y appartiennent aux mêmes hyperarêtes, (i.e, ils ont la même extension) et si X appartient à une traverse minimale T alors en substituant X par Y dans T on obtient une nouvelle traverse minimale. De plus, le risque de redondance est éliminé puisque Y n'est pas pris en compte dans l'exploration de l'espace de recherche.
Cet article est organisé comme suit : dans la section 2, nous proposons une nouvelle dé-finition de la notion de traverse minimale et nous introduisons la notion de traverse minimale irrédondante qui nous permettra de proposer, dans la section 3, un algorithme original, appelé IMT-EXTRACTOR. Enfin, une étude expérimentale sur des hypergraphes aléatoires générés à partir des données issues du site de marque-page social DEL.ICIO.US sera décrite dans la section 4.
Traverses minimales irrédondantes
Dans cette section, nous proposons de présenter des définitions et notations que nous utiliserons tout au long des sections suivantes. Pour aboutir à notre approche d'extraction des traverses minimales, nous avons défini la notion de traverse minimale irrédondante à partir des notions de la théorie des hypergraphes suivantes. Berge (1989) Soit H = (X , ?) avec X = {x 1 , x 2 , . . . , x n } un ensemble fini d'éléments et ? = {e 1 , e 2 , . . . , e m } une famille de parties de X . H constitue un hypergraphe sur X si :
Définition 1 HYPERGRAPHE
2.
Définition 2 TRAVERSE MINIMALE ET NOMBRE DE TRANSVERSALITÉ Berge (1989) Soit un hypergraphe H = (X , ?). T ? X est une traverse de H si T e i = ? ?i = 1, . . . , m. ? H désigne l'ensemble des traverses définies sur H. Une traverse T de ? H est dite minimale si 1 ? T t.q. T 1 ? ? H . On notera M H , l'ensemble des traverses minimales définies sur H.
Le nombre minimum de sommets formant une traverse est appelé le nombre de transversalité de l'hypergraphe H et on le désigne par : ? (H) = min |T|.
Définition 3 EXTENSION D'UN SOMMET
Soit un hypergraphe H = (X , ?) et x ? X . E = (e 1 , e 2 , . . . , e l ) ? ? est une extension de x si x e i , ? e i E. Nous noterons EXTENT(x), l'extension de x. Le lien entre l'extension d'un sommet x et son poids est donné par la formule : P oids (H) (x) = |Extent(x)|. De plus, pour X ? X , on définit le poids de X de la façon suivante : P oids (H) (X ) = | {e ? ? | (? x ? X, tel que x ? e)} | Définition 4 CLASSE DE SUBSTITUTION Une classe de substitution est un ensemble formé de tous les sommets de X qui ont la même extension. Ainsi, si deux sommets x i et x j appartiennent à des classes de substitution distinctes, alors EXTENT(x i ) = EXTENT(x j ) et réciproquement, si deux sommets ont des extensions différentes, ils relèveront de classes de substitution différentes.
Définition 5 REPRÉSENTANT D'UNE CLASSE DE SUBSTITUTION
Etant donnée une classe de substitution S, on dit que x X est le représentant de cette classe S si x est le premier élément de la liste triée par ordre lexicographique des sommets qui forment la classe S.
La méthode originale et efficace pour calculer les traverses minimales d'un hypergraphe proposée dans cet article, consiste à réduire l'hypergraphe en le représentant de manière plus concise mais sans perte d'information. Elle exploite le fait que parmi l'ensemble des traverses minimales, il y a une redondance d'information. Cette redondance est liée au fait que deux ou plusieurs sommets qui ont la même extension, (i.e, appartiennent exactement aux mêmes hyperarêtes) tiennent, à tour de rôle, la même position dans une traverse minimale mais ne peuvent y appartenir en même temps.
Notre approche pour une représentation concise des traverses minimales repose sur deux notions importantes : les classes de substitution et leurs Représentants. Les classes de substitution, calculées sur l'hypergraphe H=(X , ?), permettent tout d'abord de construire un hypergraphe irrédondant H associé à H.
Définition 6 HYPERGRAPHE IRRÉDONDANT ET TRAVERSE MINIMALE IRRÉDONDANTE
Soit l'hypergraphe H=(X , ?), X ? X l'ensemble des représentants des différentes classes de substitution associées aux sommets de H et ? l'ensemble des hyperarêtes de H privées de éléments de X -X et défini par ? = {e i ? X , e i ? X = ?, ?e i ? ?}, alors l'hypergraphe H = (X , ? ) est appelé hypergraphe irrédondant associé à H.
En remarquant que toute traverse minimale de H constitue une traverse minimale irré-dondante de H, on en déduit que M H , l'ensemble des traverses minimales de H (i.e des traverses irrédondantes de H) permet de construire l'ensemble des traverses minimales de H. En effet, toute traverse minimale irrédondante T = {x 1 , ...x l } de H, composée de l repré-sentants x i , i = 1, .., l des classes de substitution S i , i = 1, .., l permet de générer i=1,l |S i | traverses minimales en remplaçant les représentants de chaque classe de substitution par les autres éléments de la classe. Dans la section suivante, nous introduisons, à partir du cadre méthodologique présenté, l'algorithme IMT-EXTRACTOR pour l'extraction des traverses minimales irrédondantes.
L'algorithme IMT-EXTRACTOR
L'algorithme IMT-EXTRACTOR, dont le pseudo-code, est décrit par l'Algorithme 1 prend en entrée un hypergraphe et fournit en sortie l'ensemble des traverses minimales. On suppose que les sommets de l'hypergraphe sont triés par ordre lexicographique. L'algorithme effectue un parcours en largeur, i.e, il opère par niveau pour générer les ensembles essentiels de candidats en exploitant la propriété d'idéal ordre vérifiée par les ensembles essentiels de sommets.
L'algorithme commence par calculer les extensions de chaque sommet de X à partir desquelles seront construites les différentes classes de substitution (ligne 3). Cette tâche est effectuée par la procédure SEARCH-SUBSTITUTION. Cette procédure fournit en sortie les diffé-rentes classes de substitution, conformément à la proposition 4 avec pour chaque classe la liste des sommets qui la compose et son représentant. A partir de ces données, un nouvel hypergraph H est généré à l'aide de la procédure CHANGE-HYP suivant la définition 6. Opérant retourner MT par niveau, l'algorithme IMT-EXTRACTOR invoque la procédure GETMINTRANSVERSALITY (ligne 5) pour calculer le nombre de transversalité, (cf. définition 2) noté level dans l'algorithme, et qui correspond au niveau renfermant les plus petites traverses minimales. Le nombre de transversalité correspond ainsi à la taille des premières traverses minimales candidates. La procédure GENERATE-CANDIDATES détermine ensuite tous les sous-ensembles de sommets de X de cardinalité égale à level (ligne 6). Ceci permet à l'algorithme d'éviter le balayage de l'espace de recherche compris entre 1 et level ? 1. Une fois les ensembles de sommets de taille level générés, IMT-EXTRACTOR calcule le poids de chaque candidat avant de vérifier s'il est strictement supérieur au poids maximum de ses sous-ensembles directs ou pas. Si cette dernière propriété est vérifiée et que le poids du candidat est égal à la cardinalité de ? (ligne 10), alors le candidat est stocké dans M T irr , l'ensemble des traverses minimales irrédondantes et il est supprimé de l'ensemble des candidats (ligne 12). Si, de plus, le poids du candidat est strictement inférieur à la cardinalité de ? alors il servira pour la génération des candidats de taille level + 1. Sinon, il est élagué de C i (ligne 14). Le processus est itéré jusqu'à épuisement des candidats. A chaque itération, les candidats de taille i+1 (C i+1 ) sont générés à partir de C i par la procédure APRIORI-GEN(C i ) (Agrawal et Ramakrishnan (1994) 
Etude expérimentale
Dans cette expérimentation, quatre hypergraphes (H1, H2, H3 et H4) ont été générés à partir des données DEL.ICIO.US en considérant que les sommets correspondent aux utilisateurs et que chaque hyperarête représente une communauté constituée par les utilisateurs ayant partagé, au moins une ou plusieurs pages web. Le tableau 1 détaille les caractéristiques de chacun de ces hypergraphes. Les deux premières colonnes fournissent les probabilités minimale (p l ) et maximale (p u ) qu'un sommet appartienne aux hyperarêtes dans l'hypergraphe. La troisième colonne et la quatrième colonne indiquent respectivement le nombre de sommes |X | et le nombre d'hyperarêtes |?| de l'hypergraphe alors que la cinquième indique le nombre de Représentants, i.e., le nombre de classes de substitution. Ainsi, le nombre de sommets qui ne seront pas pris en compte lors de la génération des candidats est égal à |X | -Repr. Le nombre de transversalité ? (H) est donné dans la sixième colonne. Enfin, la septième colonne représente le nombre de traverses minimales alors que l'avant-dernière représente celui des traverses minimales irrédondantes. Pour ce qui est de la dernière colonne, nous y trouvons le taux de compacité, noté ?, de chacun des hypergraphes, et dont la valeur est donnée par la formule : TAB. 1 -Caractéristiques de nos jeux de données.
Le fait que O-M2D cible directement le niveau correspondant au nombre de transversalité, grâce à la procédure GETMINTRANSVERSALITY, permet à l'algorithme d'éviter la génération de candidats et les tests associés dans des niveaux où il ne peut pas y avoir de traverses minimales. Ceci est surtout avantageux quand le nombre de transversalité est élevé comme c'est le cas pour H3 (15) et, encore plus pour H4 (23). Venons-en maintenant à IMT-EXTRACTOR. Le nombre des Représentants étant toujours inférieur ou égal au nombre de sommets, les temps d'exécution de IMT-EXTRACTOR s'en trouvent améliorés par rapport à ses concurrents. En analysant les cardinalités de |M H | et de IMT(H ), et le taux de compacité ?, nous constatons,

Introduction
Comprendre les changements de comportement des clients est un problème essentiel qu'affronte les acteurs de la grande distribution alimentaire. Avec la concurrence féroce que se livre les acteurs de ce milieu, ne pas détecter assez tôt ces phénomènes, peut s'avérer risqué. La trace des articles achetés par un client est l'information la plus importante que possède les enseignes de la grande distribution pour détecter ces changements. En effet, les produits proposés constituent l'élément essentiel sur lequel repose la fréquentation ou non de la clientèle. L'étude de ces traces a été premièrement mise en lumière par Agrawal et Srikant (1994). Ils ont été les précurseurs de ce pan de recherche qu'est l'analyse du panier de la ménagère. Contrôler l'évo-lution de ce panier permettrait de déceler au plus tôt les événements qui font que les clients entament un processus d'attrition ou au contraire manifestent un intérêt grandissant pour les produits de l'enseigne. A titre d'exemple, la baisse de la qualité d'un ou plusieurs produits au cours du temps ou encore l'absence trop longue d'un produit recherché par la clientèle sont des facteurs qui peuvent engendrer une baisse de fréquentation voire une perte de clientèle. A l'inverse, une campagne promotionnelle sur de nouveaux produits "attractifs" peut élargir la base d'achats des clients et du coup, améliorer la fidélisation vis à vis de l'enseigne. Par ailleurs, quand on sait que des associations fortes entre produits peuvent exister, par exemple en recherchant les règles d'association ou des liens de similarité, la baisse de qualité ou encore l'absence d'un produit peut provoquer un impact de baisse d'achat non seulement sur les produits directement concernés, mais aussi indirectement sur les produits associés. Cet incidence sur les produits associés est bien souvent peu ou mal voire pas du tout analysée. Song et al. (2001) évaluent les modèles issus des données au fil du temps. L'apparition ou la disparition d'un modèle, l'augmentation ou la diminution soudaine de leur support ou encore le changement d'une partie de ces modèles sont évalués au moyen de critères présentés dans leur article. Chen et al. (2005) ou encore Bottcher et al. (2009) ont présenté de même des études analogues. Sachant que les modèles qu'ils extraient tous, sont des règles d'associations, on peut se poser la question de savoir comment est-ce qu'ils les analysent. Il est connu que l'extraction de règles d'associations génèrent un problème au niveau du nombre de motifs extraits. Choi et al. (2005) et Chen (2007) utilisent diverses techniques afin de classer ce flot de règles extraites par ordre de pertinence. C'est ici que se situe le principal apport de notre article. Nous choisissons de représenter les motifs de façon globale et ainsi de constater plus aisément les changements de tendances.
Dans cet article, nous proposons une approche qui permet de détecter de façon précoce de nouvelles tendances produits afin, d'une part de limiter l'attrition des clients et d'autre part, d'améliorer leurs fidélisation. Dans la section 2, nous explicitons précisément notre méthodo-logie que nous illustrons en section 3 avec des cas réels extraits de la base de données achats et clients d'une enseigne de la grande distribution. Enfin, en section 4, nous présentons notre conclusion.
Notre approche
A notre sens, dans le paradigme de la grande distribution, quand on a l'objectif de contribuer à la mise en oeuvre de stratégies proactives pertinentes, il faut pouvoir apporter des ré-ponses à trois phases. La première concerne la modélisation des liens entre articles fortement liés qui sera largement inspirée des réseaux sociaux. La deuxième phase concerne la recherche d'articles clés dans chaque cluster. Enfin, la dernière phase se focalise sur l'analyse de l'évo-lution des clusters, en prenant en compte particulièrement la question des cycles et de la saisonnalité, afin de détecter de véritables tendances fortes. En premier lieu, nous présenterons la façon dont nous construisons notre réseau d'articles. Puis, nous expliquerons comment déceler les articles pivots et enfin comment détecter les changements de tendances.
Construction du réseau
Les réseaux sociaux sont un ensemble d'entités sociales reliées entre elles par des liens pondérés ou non qui représentent des interactions. Raeder et Chawla (2009) proposent de modéliser le panier de la ménagère sous forme de réseau social qu'il appelle réseau d'articles. Ils construisent ce réseau de manière intuitive en considérant qu'un article (noeud) est relié à un autre article si ils ont été vendus simultanément. La transition aura un poids correspondant au nombre de ventes simultanées. Un algorithme de détection de communautés est proposé afin de détecter les meilleurs regroupements d'articles possibles au sein de ce réseau potentiellement très dense. Nous proposons à l'inverse de créer ces clusters directement en calculant les indices de similarité de Jaccard entre les articles. Une transition existera entre deux articles si l'indice de similarité respecte le seuil fixé par l'utilisateur. En ne se fixant que le seuil de la similarité, on ne passe pas à coté des niches comportementales. En effet, le support, c'est-à-dire le nombre d'occurrences d'un article ou d'un ensemble d'articles, n'est pas un critère discriminant. De cette façon, si une minorité de clients commence à élaborer un schéma jamais vu, nous serons à même de le voir. Afin de garder, des informations sur une fenêtre variable, nous construisons notre réseau d'articles avec des flags attachés aux transitions. Les flags marqueront les périodes sur lesquelles la transition existe.
Diversité
Dans une enseigne, il est indubitable que des articles ont une plus grande importance que d'autres. Dans cette optique, (Liu et al., 2010) a introduit la notion de diversité. Cette mesure caractérise à quel point le voisinage d'un noeud est divers, même quand on ne dispose que de peu d'information sur les noeuds. Dans notre contexte, nous disposons de beaucoup d'informations sur les noeuds articles, comme leur nomenclature ou encore leur marque. Dans un magasin, les articles sont rangés selon leur rayon, leur famille puis sous famille d'appartenance. Si un article est souvent lié dans notre réseau à plusieurs articles qui n'appartiennent pas au même rayon, il se révèle donc d'une forte diversité. Prenons l'exemple du client qui achète un article à forte diversité. Il est fort probable qu'il cherche à acquérir des articles qui lui sont fortement liés. Il devra par conséquent, effectuer des déplacements dans le magasin. Ces déplacements sont la clé pour atteindre le potentiel de vente maximum du client. En effet, le client sera susceptible d'être tenté par des articles qui sont sur son parcours mais qu'il n'était pas venu acheter à l'origine. Il en découle donc un potentiel d'amélioration non négligeable de chiffres d'affaires. Déceler les variations de ventes inhabituelles de ces articles, entre autres, relève d'une importance capitale dans la fidélisation de la clientèle et de la pérennisation d'un magasin. Les articles à forte diversité, de par leur importance, seront les véritables garants de la stabilité des ventes de l'enseigne.
Saisonnalité et rythme d'achats
Pour mieux prendre en compte les aspects cycliques et saisonniers précisés en amont, nous nous fixons à la période mensuelle pour pouvoir analyser au mieux les tendances et les installations de celles-ci. Pour de multiples raisons, le mois est le compromis parfait :
-il pondère au mieux les différences entre les rythmes d'achat des clients en prenant simplement en compte le versement du salaire mensuel qui est le véritable garant des achats d'un client -Pour deux articles fortement liés comme la confiture et le pain, clairement leur fréquence d'achat pour une même personne n'est la même. La cinétique de ces articles ne permet pas aux clients de les acheter tout le temps ensemble, -ce choix nous évite les épiphénomènes, essentiellement , lors de la disparition ou l'apparition de connexions entre articles. Le choix de regrouper les articles au fil des périodes grâce à la similarité en occultant les questions de supports pondèrent les variations saisonnières des ventes. En effet, les quantités vendues seules, pourraient amener des résultats biaisés et potentiellement erronés.
Détection de tendances
Pour un noeud, l'apparition ou la disparition, la diversité ou la banalité et l'évolution du nombre de voisins sont autant de paramètres qui indiqueront la stabilisation d'une tendance dans notre environnement. Nous détectons l'installation d'une tendance si on observe un changement du rythme d'apparitions de l'article sur un nombre de mois prédéfini. Exemple : Si un article est considéré comme un pivot et à un rythme d'apparition constant sur une période et qu'il cesse d'être pivot pendant un laps de temps défini. Nous déclenchons donc une alerte aux décideurs. 
Application
FIG. 1 -Réseau d'articles le plus dense du mois d'avril 2012
Un second cas intéressant est celui de la disparition d'un article 009119 et de la compensation de celui-ci par un autre de même type 149290. La figure 3 nous présente le nombre de clients ayant acheté le produit 009119 et le produit de compensation 149190. En avril (point d'abscisse 15), nous constatons selon nos paramètres de départ pour notre algorithme, une tendance à la disparition du 009119. D'après la figure 3, on suppose que cela correspond à une rupture de stock. De plus, à la même période, nous constatons l'apparition du 149290. La figure 4 nous montre que l'article 09119 est un article pivot (en rouge). En regardant, l'article sur la figure 1, on constate le poids rédhibitoire de l'article 149290. Au total, l'article prendra 3 mois pour acquérir un statut d'article pivot au même titre que son homologue. 
Conclusion
Dans ce papier, nous présentons une façon originale de détecter les tendances produits en analysant les connexions entre ceux-ci. En modélisant les relations entre articles sous forme de réseau et en marquant l'importance des articles, nous obtenons une vision globale des achats des clients à un moment donné. L'évolution de ces réseaux peut être interprétée de différentes façons et le but avant tout est de prendre des décisions assez tôt pour gérer au mieux les événe-ments. Les décideurs commerciaux pour conforter la tendance devront gérer correctement le stock de l'article et de ces satellites dans le but de répondre à la demande. De plus, dans le cas d'une rupture inévitable, communiquer correctement sur un autre article de substitution afin

Introduction
L'annotation sémantique de documents du web est une des étapes pour assurer le succès des applications du Web Sémantique en tant que support à un accès partagé et unifié aux connaissances et documents, y compris de domaines spécifiques. En faisant appel à une ontologie, on assure un meilleur partage et une meilleure interprétation de ces annotations. La qualité des annotations sémantiques nécessite donc des ontologies de domaine de qualité. Même si des ressources comme WordNet 1 ou Yago 2 offrent des vocabulaires riches et répondent aux besoins de l'annotation de connaissances générales en langue anglaise, les connaissances précises des ontologies de domaine présentent l'avantage de mieux rendre compte de l'information vé-hiculée par des documents et du sens des formulations linguistiques.
Dans un processus dual, quand les ontologies sont utilisées pour l'annotation sémantique de collections de textes de domaines particuliers, leur construction à partir de ces textes contribue à ce que l'ontologie couvre mieux les concepts et relations nécessaires à la caractérisation du contenu des documents. L'ingénierie d'ontologies à partir de textes a ainsi obtenu des résul-tats significatifs ces 10 dernières années   (Maedche, 2002), mais cette activité reste longue et complexe.
Lorsque l'on veut enrichir ou peupler une ontologie existante, les annotations textuelles peuvent fournir des informations pour trouver des indices linguistiques de relations séman-tiques. Une première catégorie de systèmes utilise les annotations linguistiques : par exemple, RelExt de Schutz et Buitelaar (2005) exploite catégories et dépendances syntaxiques. Un deuxième ensemble d'approches traite des annotations sémantiques en utilisant l'ontologie à enrichir, comme la méthode proposée dans le projet CrossMarc (Valarakos et al., 2004) , ou celle de Navigli et Velardi (2006).
Notre objectif est similaire à ces travaux : partant d'un noyau d'ontologie en adéquation avec une collection de textes, nous cherchons à l'enrichir par analyse automatique de ces textes. La chaîne de traitements que nous proposons enrichit un noyau d'ontologie avec des relations sémantiques définissant des restrictions de relations existant dans l'ontologie afin de décrire des concepts précis. Notre approche est originale au sens où nous introduisons dans ce processus deux types de connaissances sur le texte, jusque là peu utilisés conjointement : des connaissances de niveau sémantique, à savoir les concepts déjà reconnus dans le texte, et des connaissances de niveau discursif, liées à l'architecture du texte au sens de Virbel et Luc (2001). Nous avons évalué ce processus en réalisant une ontologie de plantes à partir de fiches de jardinage dont la structure est explicitée par des étiquettes XML. Le noyau d'ontologie est construit à partir d'une analyse manuelle de la structure de ce type de document. Il est composé du concept principal relié aux concepts racines de hiérarchies de concepts issues de l'exploitation des valeurs des champs, la sémantique de ces relations étant fournie par le nom du champ. Les concepts pivots deviennent alors des sous-concepts du concept principal. Par exemple, dans un site web de jardinage, le concept pivot est celui de plante, chaque page décrit une plante spécifique (concept pivot de la page) à l'aide de concepts relatifs à la nature des sols, à l'entretien ou aux parties de la plante.
Le processus d'enrichissement vise à décrire précisément les concepts pivots à l'aide de propriétés spécifiques. Elles sont exprimées par des relations qui restreignent les relations du noyau d'ontologie ayant pour domaine le concept principal en spécialisant leur co-domaine. Pour cela, le texte et les balises des champs du document sont analysés. Identifier la correspondance entre les balises des champs et les propriétés du concept pivot est ce que nous appelons par la suite la sémantisation de la structure des documents, un processus à deux étapes :
1. affecter un rôle aux champs de cette structure. Après lecture des fiches, l'ontologue définit F C , l'ensemble des champs porteurs du concept pivot et F R , l'ensemble des champs porteurs de relations. Ces derniers contiennent a priori au moins une relation entre le concept pivot et un concept existant dans le noyau d'ontologie et dont on s'attend à trouver une trace linguistique dans le texte présent dans ce champ. 2. formuler les concepts et relations associées à chaque champ. C'est là une originalité de la démarche. La recherche des relations ne s'appuie pas sur la notion de patron lexico-syntaxique tels qu'ils sont définis dans (Auger et Barriere, 2008). Elle repose sur l'expression déclarative des concepts et relations pouvant être reconnus grâce à chaque balise XML et au texte qu'elles encadrent. Cette expression est le fruit d'une interprétation par l'ontologue de la sémantique des documents. Cette approche généralise celle établie pour d'autres types de documents structurés dans (Laignelet et al., 2011).
Enrichissement de l'ontologie par les relations
Pour identifier les concepts à relier au concept pivot, nous utilisons l'annotation sémantique de ces textes à l'aide des concepts déjà présents dans l'ontologie. Dans une ontologie O = (C, R), C est l'ensemble des concepts du domaine, et R, l'ensemble des relations liant les concepts. Nous définissons alors :
-
, avec c i et c j ? C, et r ? R définit une relation autre que la subsomption, respectivement de domaine c i et de co-domaine c j . -porte_c(f, c), avec c ? C et f ? F C , est un prédicat qui indique que le champ f est porteur du concept c. -porte_r(f, r), avec r ? R et f ? F R , est un prédicat qui indique que le champ f est porteur de la relation r(c i , c j ) où un des concepts c i ou c j est le concept principal et l'autre est une super-classe d'un des concepts exprimés dans ce champ. -a_label(c, t), avec c ? C et t un terme, indique que c a pour étiquette (rdfs :label) t. L'algorithme d'extraction de relations produit un ensemble de restrictions sur des relations r(ci, cj) ? R, où c i est le concept pivot de d, et c j spécialise le co-domaine de r.
Annotation des documents avec les concepts
L'algorithme d'annotation de TextViz 3 (Reymonet et al., 2009) est utilisé pour annoter chacun des documents d de la collection avec le noyau d'ontologie. Il s'appuie sur les labels de concepts, sur les termes présents dans le texte et sur une distance sémantique. Soit le concept c ? C, d un document, et f ? F R ? F C . Le prédicat annote(c, f, d) indique que c annote le champ f de d. TextViz génère cette annotation si un des labels de c apparaît dans le contenu textuel du champ f du document d. Étant donné T f,d , les annotations de TextViz sont équivalentes au résultat de la règle : ?t ? T f,d , ?c ? C, si a_label(c, t) alors annote(c, f, d).
Identification des relations
Le processus d'identification de relations explore, pour chaque champ f de d tel que f ? F R , les annotations de f et le fichier déclarant les relations associées à f . Étant donnés c p le concept pivot du document d, un concept c i qui annote le texte du champ f , le processus vise à identifier la relation r ? R qui peut exister entre 
Le cas d'étude du jardinage
Dans le projet MOANO 4 , une ontologie doit permettre la recherche sémantique d'informations dans un livre de jardinage numérisé 5 . Ce livre, écrit en français, présente une grande collection de plantes et fournit, pour chacune, des informations sur son aspect, des conseils sur son entretien et des soins contre les maladies ou parasites. L'annotation de ce livre doit donc s'appuyer sur une ontologie dédiée aux plantes d'un point de vue non pas de la botanique scientifique, mais du jardinage, qui doit contenir des concepts liés aux plantes, à leur aspect physique, aux conditions de leur développement, ainsi que des conseils pour les entretenir ou prévenir les maladies ou parasites. Pour décrire dans l'ontologie chacune des variétés de plante 
Evaluation
Deux ontologues ont enrichi le noyau d'ontologie à partir de 10 documents du site choisis au hasard. A partir de chacun des documents, ils ont défini des concepts spécifiques de plante, et leurs propriétés, représentations que l'on peut considérer comme les résultats attendus du processus automatique. Nous avons ensuite appliqué l'algorithme d'enrichissement à partir de ces 10 documents, puis calculé le rappel et la précision par rapport aux relations retenues par les ontologues. Le Rappel est évalué à 0.73 et la Précision à 0.8. Ces chiffres sont donc prometteurs mais difficiles à interpréter. Si l'on étudie précisément pourquoi des relations ont été mal interprétées ou n'ont pas été trouvées, on identifie 4 causes de problèmes : l'incomplétude d'une ressource (labels de l'ontologie, liste de champs à analyser), la qualité de l'annotation par les concepts (erreurs de l'algorithme d'annotation de TextViz, annotation des noms de champs), la qualité de l'ontologie (signature ambigüe des relations, absence de labels de concepts), une définition incorrecte de la sémantique des champs, et les défauts de l'analyse linguistique (négations ou intervalles numériques non traités). Les problèmes les plus fréquents sont liés à l'analyse linguistique, en particulier à la gestion des négations, que nous sommes en train de mettre en place pour les corriger.
Conclusion
Nous avons montré que dans des collections de documents textuels semi-structurées et thé-matiquement homogènes, où chaque document décrit un concept d'un même type dans toute la collection, l'exploitation de la structure des documents peut être un atout majeur pour automatiser la modélisation du concept décrit dans chaque document. Nous avons ainsi défini une nouvelle approche pour enrichir une ontologie noyau en exploitant la structure des documents de la collection et leur annotation sémantique par les concepts du noyau. Les résultats obtenus 6. l'encyclopédie botanique du site "Jardin !L'encyclopédie" http ://nature.jardin.free.fr/

Introduction et présentation du problème
L'étude des populations par l'analyse de détails d'appel téléphonique est un problème auquel s'intéressent les opérateurs de téléphonie mobile depuis quelques années. Différentes études ont été menées sur le sujet, dont certaines s'intéressent à déterminer la structure communautaire des populations grâce au trafic inter-antennes (Blondel et al., 2010), (Guigourès et Boullé, 2011). Ces études de journaux d'appels téléphoniques permettent de mettre évidence une segmentation naturelle du territoire liée à la langue ou aux zones d'influences des grandes métropoles à l'échelle d'un pays, ou encore au profil économique et social des quartiers (bourgeois, populaire, étudiant ...) dans des études plus locales. De telles analyses intéressent les opérateurs de tous les pays, notamment ceux en voie de développement, où les besoins en terme de télécommunications sont amenés à être importants et où les usages demeurent actuellement encore inconnus.
Pour aller plus loin, on peut s'intéresser à délimiter des zones géographiques où le comportement des usagers de téléphones mobiles est différent en fonction de la période de temps étudiée. Une analyse temporelle du trafic au niveau des groupes d'antennes permet de caractériser les zones géographiques grâce aux excès et déficits d'appels dans chacune des périodes étudiées.
Une telle étude apporte aussi bien une information sur les différentes plages horaires qui structurent la journée, la semaine, le mois ou l'année des utilisateurs de téléphones portables, que sur les lieux où les phénomènes temporels sont observés.
Une des contraintes dont il faut tenir compte dans ce type d'études est la volumétrie des données. Les données dont nous disposons et sur lesquelles nous souhaitons étudier les corrélations spatio-temporelles, sont un enregistrement quotidien des appels inter-antennes passés en France métropolitaine entre le 13 Mai et le 13 Octobre 2007. Le nombre d'antennes réparties sur le territoire français est de 17895 entre lesquelles ont transités 1,12 milliards d'appels. Dans la section 2, nous présentons des méthodes adaptées à ce type d'analyse et justifions le choix de la solution utilisée. La section suivante présentera les résultats de l'analyse des corrélations spatiales, puis la section 4 les corrélations temporelles. Une dernière partie conclut cet article en faisant un bilan de l'analyse.
État de l'art et choix de la solution
La première question qu'il est important de se poser est la représentation des données. En effet, un appel est décrit par l'antenne source, l'antenne cible et le jour de l'appel. Une précédente étude (Blondel et al., 2010) propose de représenter ces données sous forme d'un graphe non orienté, définissant ainsi un réseau d'antennes liées par le nombre d'appels transitant entre elles. Nous préférons conserver le format tabulaire pour traiter ce problème, et ainsi garder l'orientation naturelle des appels entre les antennes. Blondel et al. (2010) proposent de partitionner le réseau d'antennes suivant une approche de maximisation de modularité. La modularité (Newman, 2006) évalue la qualité de la partition d'un graphe en cliques, ou communautés dans la terminologie des réseaux sociaux, qui sont des ensembles de noeuds fortement connectés entre eux. Cette technique possède l'avantage d'être efficacement optimisable en terme de complexité (Blondel et al., 2008) et donc de pouvoir traiter de très grands volumes de données, comme c'est le cas ici. Cependant, l'hypothèse faite lorsqu'on emploie ce type d'approches est très forte en supposant que le réseau est modulaire, c'est-à-dire qu'il peut se subdiviser en groupes de noeuds fortement connectés entre eux. Si le réseau possède en effet une structure communautaire, la maximisation de modularité sera un outil très efficace. Cependant, la structure sous-jacente du réseau dans le cas présent étant inconnue, on ne peut faire d'hypothèse sur la nature des motifs que nous souhaitons faire émerger des données. Nous devons donc trouver une alternative qui permette de considérer toutes les structures possibles du réseau.
État de l'art
Le concept de blockmodeling est à l'origine des premiers travaux d'analyse des structures dans les graphes menés par les sociologues dès les années 1950 dans le contexte de l'analyse des réseaux sociaux (Nadel, 1957). Pour utiliser ce type d'approches, nous devons étudier les données sous leur forme tabulaire. Les lignes et les colonnes de cette matrice représentent respectivement les antennes source et cible, et les valeurs indiquent la fréquence d'appels entre couples d'antennes. Il est ainsi possible de réorganiser les lignes et les colonnes dans le but de découper la matrice en blocs homogènes. Cette technique est appelée Blockmodeling. Une fois les blocs extraits, une partition des antennes représentées à la fois par les lignes et les colonnes peut être réalisée. Cette segmentation simultanée est appelée coclustering. En procédant ainsi, nous sommes capables de capturer des structures plus complexes que les structures retrouvées par maximisation de modularité, méthode qui correspond finalement à un blockmodeling diagonal.
De nombreuses méthodes de blockmodeling ont été proposées pour extraire des groupes dans les réseaux. Certaines se basent sur l'optimisation d'un critère (Doreian et al., 2004) permettant d'isoler des blocs homogènes en se focalisant sur les blocs vides comme il est suggéré dans (White et al., 1976). Des approches déterministes plus récentes se sont intéressées à l'optimisation de critères qui mesurent la qualité de la partition en blocs, en termes de résumé des données d'origine (Reichardt et White, 2007). D'autres utilisent le blockmodeling stochastique. Dans ces modèles génératifs, une variable latente indiquant l'appartenance ou non à un cluster est associée à chaque noeud. Conditionnellement à leur variable latente, la probabilité d'observer un arc entre deux acteurs suit une loi de probabilité (Bernoulli dans les cas les plus simples) dont les paramètres dépendent uniquement de la paire de clusters désignés par la variable latente. Les premières approches nécessitaient une paramétrisation du nombre de clusters par l'utilisateur (Nowicki et Snijders, 2001), alors que les méthodes les plus récentes préfèrent le déterminer automatiquement en utilisant un processus de Dirichlet (Kemp et Tenenbaum, 2006).
Outre la diversité des structures pouvant être inférées dans le réseau par les approches de coclustering, il est également possible réaliser un coclustering avec des variables numériques (Nadif et Govaert, 2010), (Boullé, 2012). Des blocs sont extraits des données et produisent une discrétisation de la ou des variables numériques. On peut ainsi appliquer, pour une seconde analyse, un blockmodeling sur des données tabulaires dont les lignes sont les antennes source et les colonnes une variable numérique temporelle. Ainsi, il est possible de trouver des structures temporelles par utilisation de la même méthode.
Dans le cas d'une analyse de comptes rendus d'appels, la technique employée doit présenter plusieurs propriétés :
-Passage à l'échelle : avec près de 18000 antennes et 1,12 milliards d'appels, on ne peut pas se permettre d'avoir une complexité algorithmique trop forte, ce qui est souvent le défaut des techniques de blockmodeling et de coclustering. -Généricité : les données traitées peuvent être aussi bien numériques que catégorielles.
Ce point est important car l'analyse que nous menons porte à la fois sur des variables catégorielles (les Antennes) que numériques (le temps). -Absence de paramétrage : les données sont complexes et leur structure inconnue, un paramétrage de la structure de coclustering (e.g le nombre de clusters de chaque variable ou la distribution des antennes dans les groupes) serait trop complexe avec un tel jeu de données. -Fiabilité : la méthode utilisée ne doit pas produire de résultats lorsqu'il n'existe aucune structure sous-jacente. Elle doit donc être résistante au bruit et ne pas surapprendre. -Finesse et interprétabilité : l'approche doit capturer toute l'information présente dans les données afin d'extraire des motifs fins. Des outils destinés à l'interprétation des résul-tats doivent également être proposés afin d'exploiter de manière efficace les résultats.
Étant donné le volume de données, la plupart des méthodes de coclustering ont une complexité algorithmique telle qu'on ne pourrait pas les appliquer directement sur la base de données. Une idée serait donc de travailler sur un échantillon de données. Cependant, avec 17895 antennes et 1,12 milliards d'appels, le nombre d'appels moyen entre 2 antennes est d'environ 3, 5. Ainsi, un échantillonnage entrainerait une perte d'information importante. Parmi les approches de coclustering, nous retiendrons l'approche MODL  1 .
L'approche MODL
Afin de bien formaliser le problème, le tableau 1 liste les caractéristiques des données ainsi que les paramètres de modélisation du coclustering que nous cherchons à déterminer.
D : Données
M S : modèle de coclustering spatial  L'analyse que nous menons est divisée en deux phases. Une première s'intéresse aux corrélations entre antennes source et cible alors que la seconde se focalise sur la dimensions temporelle des appels. C'est pourquoi nous introduisons deux modèles distincts : l'un sera dit spatial M S et l'autre temporel M T . Dans les deux cas, l'approche MODL cherche à déduire les paramètres du modèle M S (resp. M T ) à partir des données D.
Dans un premier temps, les deux variables étudiées sont catégorielles, il s'agit des antennes source et cible. Le but de l'étude est de grouper les antennes source dont les appels sont distribués de manière similaire sur les antennes cible et vice-versa. Dans un second temps, une variable est catégorielle, les antennes source, et l'autre numérique, le temps. On connait le nombre d'appels sortant quotidiennement des antennes. On va donc chercher à grouper les antennes et discrétiser en même temps la variable temporelle de manière à ce que le trafic sortant des groupes d'antennes soit stationnaire à l'intérieur de chaque intervalle de temps. -le prior : noté P (M S ) (resp. P (M T )), il pénalise le modèle en spécifiant la distribution a priori des paramètres de ce dernier. Il est construit hiérarchiquement et uniformément à chaque étape afin d'être non-informatif (Jaynes, 2003). -la vraisemblance : Une fois les paramètres du modèle spécifiés, la vraisemblance P (D|M S ) (resp. P (D|M T )) est définie comme la probabilité d'observer les données initiales connaissant les paramètres du modèle étudié.
Le produit du prior et de la vraisemblance resulte en la probabilité a posteriori du modèle. Le logarithme négatif de cette dernière probabilité est utilisé pour construire le critère.
Définition (Coût du Modèle spatial). Le modèle spatial M S , représentation synthétique des données D est optimal s'il minimise le critère suivant :
(1)
Définition (Coût du Modèle temporel). Le modèle temporel M T , représentation synthétique des données D est optimal s'il minimise le critère suivant :
est une somme des nombres de Stirling de second ordre, c'est-à-dire le nombre de manières de partitionner |V S | éléments en k sous ensembles non-vides.
Les deux premières lignes de l'équation 1 et la première de l'équation 2 représentent le prior alors que la dernière représente la vraisemblance dans les deux cas. D'un point de vue théorie de l'information, un logarithme négatif de probabilité correspond à une longueur de codage. Ainsi, le logarithme négatif du prior est la longueur de codage du modèle alors que le logarithme négatif de la vraisemblance est la longueur de description des données pour une paramétrisation du modèle donnée. Minimiser la somme de ces deux termes a donc une interprétation naturelle en terme de MDL (Minimum Description Length) (Grünwald, 2007). D'un point de vue algorithmique, l'optimisation est réalisée à l'aide d'une heuristique gloutonne ascendante démarrant du clustering le plus fin (une antenne par cluster) et réalisant à chaque étape la fusion de clusters qui décroit le plus le critère. Une post-optimisation améliore cette heuristique en effectuant des permutations au sein des clusters. Cet algorithme, de complexité en O(m ? m log m), est détaillé dans .
Analyse des corrélations spatiales
La première étape est une analyse des appels entre antennes source et antennes cible. On obtient un total 2141 clusters d'antennes source et 2107 clusters d'antennes cible. Le nombre d'antennes par cluster est compris entre huit et neuf, ce qui est très fin. La difficulté est donc de savoir interpréter les résultats. À l'échelle de la France, le nombre de clusters ne permet pas d'avoir une vision synthétique du regroupement d'antennes. Cependant, à huit antennes par cluster, des résultats de cette finesse peuvent être un atout pour une analyse locale.
Analyse à l'échelle nationale
Dans un premier temps, nous nous intéressons à une analyse à l'échelle du pays. Le niveau de finesse du modèle obtenu ne permet pas d'avoir une vue synthétique de la structure de coclustering à l'échelle du pays. C'est pourquoi nous proposons de construire une classification hiérarchique ascendante des clusters. Nous fusionnons pour cela deux à deux les clusters de manière à détériorer le moins possible le critère optimisé afin d'obtenir le modèle le plus probable suite à une fusion (de clusters source ou cible). Ce processus nous permet de simplifier notre modèle de manière maitrisée. Afin de quantifier la perte en terme d'informativité du modèle, nous introduisons une mesure que nous appelons taux d'informativité. Cette définition nous permet de construire une courbe du taux d'informativité en fonction du nombre de clusters du modèle simplifié. Cette courbe fait figure de courbe de Pareto du meilleur modèle pour un nombre de clusters donnés. On observe que l'impact des premières fusions sur le modèle est relativement faible. Ainsi, il est possible de réduire le nombre de clusters de plus de 2000 à 85 sur les deux dimensions (antennes source et antenne cible) tout en conservant environ 75% d'informativité du modèle. On utilisera ce niveau de grain pour notre étude à l'échelle nationale, le modèle restant ainsi informatif et suffisemment simple pour être étudié dans sa globalité. Les résultats sont présentés sur la Figure 2. La corrélation entre les clusters d'antennes et leur position géographique est très forte bien que la position des antennes ne soit pas une contrainte dans l'algorithme de coclustering. On en déduit alors que les habitants d'une même zone géographique appellent vers les mêmes endroits. On voit sur cette carte que la France peut être séparée en zones géographiques bien délimitées mais pas nécessairement corrélées avec les frontières des régions administratives ou des départements.
Une analyse locale
On se propose maintenant d'exploiter la finesse des résultats obtenus. Pour cela, nous utilisons le modèle optimal le plus fin (M * S ) et nous faisons un zoom sur une métropole française. L'agglomération de Toulouse est divisée en sept principaux clusters, affichés en Figure 3a. Un premier cluster regroupe les antennes du centre-ville (ronds jaune pâle), un autre cluster (ronds vert clair) correspond au quartier de la rive gauche de la ville qui est plus résidentielle que la rive opposée. Le cluster modélisé par des ronds rose pâle correspond au quartier étudiant de Rangueil ainsi qu'à des zones urbaines sensibles, comme Empalot. Les ronds vert pâle couvrent le quartier du Mirail qui possède les mêmes caractéristiques que le précédent cluster. Les ronds orange sont localisés dans des zones périphériques résidentielles aux caractéristiques socio-économiques relativement diverses, quartiers plus aisés vers le Sud Est et plus populaires vers le Nord-Est. Enfin les carrés rouges couvrent la commune de Blagnac qui regroupe plusieurs zones d'activités de l'agglomération Toulousaine.
Afin de mieux comprendre les raisons de ce regroupement d'antennes source, on s'intéresse à la distribution des appels sortants de ces groupes. L'information mutuelle quantifie la dépendance entre deux variables, ici les antennes source et cible. Cette mesure, notée MI, est définie de la manière suivante (Cover et Thomas, 2006) :  
Analyse des corrélations spatio-temporelles
Dans cette seconde étude, nous proposons d'effectuer un coclustering des antennes source et de la variable temporelle. Les données sont donc constituées de 17895 antennes émettrices et de 1,12 milliards d'appels enregistrés sur 5 mois avec une précision à la journée. Le groupement d'antennes source est différent du groupement obtenu dans la Section 3. Ici les antennes groupées entre elles sont similaires car leurs hausses et baisses de trafics sortants se font sur les mêmes périodes temporelles. Nous obtenons 6129 clusters d'antennes source et 117 intervalles de temps. Contrairement à l'analyse du trafic inter-antennes, les clusters sont dispersés sur l'ensemble du territoire Français, ce qui rend une projection sur une carte ininterprétable et ce, pour n'importe quel niveau de grain du modèle. Pour mieux comprendre les phénomènes qui ont mené à une telle structure du coclustering, nous allons étudier l'information mutuelle entre les clusters d'antennes sources et les périodes de temps trouvées. Pour visualiser cette information, nous avons simplifié de modèle de la même manière que dans l'étude précédente et nous avons tracé un calendrier des excès et déficits du trafic sur la Figure 4.
On observe du 13 Mai au 5 Juillet, ainsi que du 1er Septembre au 13 Octobre, une discréti-sation régulière et périodique qui correspond au découpage semaine/weekend. On voit qu'en semaine le trafic est en excès pour le cluster du milieu et en déficit pour le cluster du bas, le contraste est d'autant plus fort entre ces clusters qu'on étudie une période éloignée des vacances d'été. Pour les weekend, la tendance s'inverse mais dans une moindre mesure. Le trafic est donc mieux équilibré entre les différents clusters d'antennes. On peut expliquer ces phénomènes par l'activité concentrée sur des zones géographiquement restreintes et généralement urbaines pendant la semaine. Le cluster du haut reste, quant à lui, toujours en déficit en dehors de l'été, bien que ce déficit ait tendance à être plus léger pendant les weekends.
Pendant la période estivale, l'alternance semaine/weekend disparait. On observe alors un déficit d'appels sur toute la période dans le cluster du milieu, alors que le cluster du haut voit un excès significatif d'appels par rapport au trafic habituel et par rapport au trafic de la période. Quant à la dernière zone, on y observe un léger excès d'appels sortant. C'est à cette période de l'année que les contrastes sont les plus forts, c'est pourquoi on va s'y intéresser et tracer une projection géographique des clusters d'antennes sources sur une carte de France ( Figure 5). Pendant cette période des vacances scolaires (4 Juillet -4 Septembre), on observe un excès d'appels au niveau des côtes Atlantiques et du Languedoc-Roussillon principalement. Cela signifie qu'à cette période, un nombre d'appels bien plus important que le trafic habituel a été émis depuis ces zones. On peut donc qualifier ces zones de saisonniaires car elles sont caractérisées par une répartition des appels très déséquilibrée au cours de l'année, ce qui explique cet excès d'appels à cette période de l'année. C'est pour cette même raison que la Côte d'Azur ne connait pas un excès d'appels aussi fort que la Vendée par exemple. Le trafic y est mieux réparti sur l'année, bien qu'elle soit la première destination estivale des français. Les grandes agglomérations, et notamment Paris, sont colorées en bleu. Ceci peu s'expliquer par la baisse d'activité dans les villes à cette période. On peut donc supposer un déplacement des populations en été, depuis les centres urbains vers les côtes et les zones touristiques. Remarquons tout de même que la couleur tracée sur cette carte n'indique pas la fréquence des appels mais bien l'information mutuelle, les antennes parisiennes demeurant les antennes les plus émettrice, même à cette période de l'année.
Conclusion
Nous avons proposé une étude d'un compte rendu d'appels enregistré pendant cinq mois entre les 17895 antennes téléphoniques françaises, ce qui représente un total de 1,12 milliards d'appels. Après avoir présenté des études similaires, ainsi que des méthodes adaptées à ce type d'études, nous avons justifié le choix et détaillé la méthode utilisée : l'approche MODL. Nous avons pu mener deux études de nature différente en utilisant une unique méthode, possédant les propriétés de généricité et de passage à l'échelle suffisantes pour réaliser une analyse complète de nos données. On observe dans cette étude que les antennes groupées dans un même cluster, de part les distributions similaire des appels sortant (resp. rentrant), sont géographiquement très proches et dessinent des frontières précises, aussi bien au niveau national que local. D'autre part, lors d'une étude temporelle, nous avons pu dresser un calendrier de la période temporelle étudiée et déterminer des zones où les appels sortants sont distribués identiquement dans chaque intervalle de temps. Les zones obtenues ont perdu la proximité géographique observée dans la première partie de l'étude mais sont caractéristiques des zones qu'ils décrivent : urbaines, rurales ou touristiques. Les périodes quant à elles, montrent une différence de comportement entre les vacances d'été et les périodes scolaires où on observe une périodicité semaine/weekend où les excès et déficits de trafic s'inversent suivant la nature de la zone. Ainsi, par exemple, les excès de trafic se concentrent dans les zones touristiques en Août et les déficits dans les zones urbaines. Dans des prochains travaux, il serait intéressant de mener une étude où plusieurs dimensions temporelles sont embarquées simultanément dans l'algorithme de coclustering, afin de voir comment se caractérisent les comportements dans les zones géographiques en fonction du jour de la semaine et de l'heure de la journée.

Introduction
L'objectif de ces travaux est d'obtenir un classifieur évolutif pour faciliter les interactions homme machine sur interface tactile (tablette, smartphone, tableau interactif, etc.). Autrement dit, nous souhaitons que le classifieur soit capable de s'adapter à la manière d'écrire et de dessiner de l'utilisateur. Cette manière de dessiner va probablement évoluer au fur et à mesure que l'utilisateur va s'habituer à utiliser l'interface tactile. L'utilisateur commence par dessiner lentement et attentivement ses gestes lorsqu'il est novice, alors qu'il les fait d'une manière plus fluide et plus rapide quand il devient expert. Il faut alors que le classifieur s'adapte et suive cette évolution. Il est également souhaitable que le classifieur supporte l'ajout de nouvelles classes « à la volée » pour permettre à l'utilisateur de personnaliser et d'adapter l'application à ses besoins. Pour cela, il est nécessaire de disposer d'un système performant et réactif aux changements de son environnement.
Plusieurs systèmes de classification évolutifs sont apparus ces dernières années pour ré-pondre au besoin de classifieurs fonctionnant en environnement non stationnaire. Ils utilisent des algorithmes d'apprentissage incrémental pour améliorer leurs performances pendant leur utilisation et s'adapter à tout changement. Parmi ces classifieurs évolutifs, les systèmes d'infé-rence floue (SIF) se distinguent par leurs excellentes performances (Angelov et Zhou, 2008), leur capacité à mettre en oeuvre un apprentissage incrémental et leur bon comportement face à l'ajout de classe. Dans la plupart de ces approches, l'apprentissage incrémental des conclusions repose sur l'algorithme des moindres carrés récursifs. Or cet apprentissage incrémental lié aux moindres carrés récursifs à l'inconvénient de perdre ses capacités d'adaptation avec le temps. Cet article étudie les différentes possibilités existantes, dans la littérature dédiée au contrôle, pour introduire de l'oubli dans l'algorithme des moindres carrés récursifs, dans le contexte de la classification à base de SIF. L'objectif est de préserver dans le temps les capacités d'adaptation des SIF. Ces différentes techniques sont transposées dans le système d'inférence floue Evolve (Almaksour et Anquetil, 2011), puis sont comparées pour la reconnaissance de gestes manuscrits.
La section 2 présente l'architecture et l'apprentissage incrémental des systèmes d'inférence floue. Plusieurs approches pour intégrer de l'oubli dans l'algorithme des moindres carrés ré-cursifs sont ensuite présentées section 3. Ces différentes techniques sont évaluées et comparées sur la problématique de reconnaissance de gestes manuscrits en section 4. Enfin, la section 5 conclue et discute des perspectives de ces travaux.
Systèmes d'inférence floue d'ordre un
Cette section présente l'architecture de notre système d'inférence floue (SIF) Evolve. Nous détaillons ensuite l'algorithme des moindres carrés récursifs utilisé pour son apprentissage. Enfin, nous expliquons les limites de cet algorithme ainsi que la nécessité d'intégrer de l'oubli.
Architecture du système d'inférence floue
Nous travaillons ici avec des systèmes d'inférence floue (SIF) d'ordre un -dit de TakagiSugeno -qui offrent de bonnes performances face à des problèmes de classifications complexes. De plus, ils supportent bien l'ajout de nouvelles classes « à la volée » et peuvent facilement être entrainés de manière incrémentale en temps réel. Des classifieurs évolutifs basés sur des SIF ont été proposés par Angelov et Zhou (2008) et par Almaksour et Anquetil (2011).
Un système d'inférence floue d'ordre un se compose d'un ensemble de règles floues de la forme suivante :
où x ? R n le vecteur des caractéristiques etˆyetˆ etˆy (i) ? R c le vecteur d'appartenance au c diffé-rentes classes.
Les prémisses des règles sont définies par l'appartenance floue à des prototypes C (i) -des regroupements (clusters) dans l'espace d'entrée -qui sont caractérisés par un centre µ (i) et une matrice de covariance ? (i) . Le degré d'appartenance floue ? (i) (x) est calculé par une fonction à base radiale hyper-ellipsoïdale, en fonction de la distance de Mahalanobis d
Les conclusions des règles donnent l'appartenance floue aux différentes classes, et pour les SIF d'ordre un, ces degrés d'appartenance sont obtenus par des fonctions linéaires des entrées :
où l
k (x) représente la fonction conséquence linéaire de la règle i pour la classe k :
L'inférence floue de type somme-produit est ensuite utilisée pour calculer la sortie du système pour chaque classe :
où r représente le nombre de règles floues. La classe de x est choisie comme étant l'étiquette correspondant à la composante maximale de la sortie du systèmê
Apprentissage incrémental
Dans un problème d'apprentissage incrémental en-ligne, les données d'apprentissage ne sont disponibles qu'au fur et à mesure. Le système doit donc être appris en utilisant les premières données arrivées, puis continuer à évoluer, de manière transparente du point de vue de l'utilisateur, lorsque les données suivantes arrivent.
Ainsi, les prototypes sont adaptés incrémentalement à l'arrivée de chaque nouvelle donnée. Ce processus d'adaptation permet de mettre à jour les centres des prototypes en fonction de chaque nouvelle donnée d'apprentissage disponible, et de recalculer de manière récursive les matrices de covariances des prototypes. Un algorithme de clustering incrémental est utilisé pour créer de nouveaux prototypes, et donc de nouvelles règles, lorsque que cela est opportun.
Le problème de l'apprentissage incrémental des conséquences dans un SIF d'ordre un peut être résolu par la méthode des Moindres Carrés Récursifs (MCR) pondérés par les activations des règles (Angelov et Zhou, 2008). Soit ? (i) la matrice de tous les paramètres des consé-quences linéaires de la règle i :
où c représente le nombre de classes, et n la taille du vecteur des caractéristiques. Ces matrices peuvent être récursivement apprises :
Où les matrices de covariances
?1 sont également mises à jour récur-sivement :
Nécessité et principe de l'oubli
Plus le nombre de données d'apprentissage augmente, plus les modifications apportées aux conclusions par l'algorithme des MCR seront faibles. En effet, le poids accordé à chaque donnée étant équivalent, plus le nombre de données augmente, plus le poids de chaque donnée individuelle est faible. Cette propriété est illustrée par le fait que la matrice de covariance décroit avec le temps : P t < P t?1 . Il est dit que le gain de l'algorithme tend vers zéro.
Cette propriété a pour conséquence de réduire la réactivité du système -la capacité d'apprentissage de nouveautés -avec le temps. Ainsi, au bout d'un certain temps, l'algorithme ne sera plus capable de s'adapter aux changements dans les données, ni d'apprendre de nouvelles classes (dans un temps raisonnable). Il est donc nécessaire de limiter le poids des anciennes données, autrement dit d'introduire de l'oubli dans l'algorithme des moindres carrés récursifs, pour maintenir les capacités d'apprentissage du système.
L'introduction d'oubli dans l'algorithme des moindres carrés récursifs est un sujet qui a été beaucoup étudié dans la littérature dédiée au contrôle de systèmes physiques complexes (Lughofer et Angelov, 2011). Pour cela, il faut travailler sur la matrice de covariance qui repré-sente la distribution des données dans l'algorithme des moindres carrés récursifs. Le principe de l'oubli est d'introduire une mise à jour temporelle de cette matrice : Cette formulation de l'oubli -dite bayésienne -permet d'unifier les différentes techniques d'oubli pour l'algorithme des moindres carrés récursifs (Kulhavy et Zarrop, 1993). Chaque technique est alors caractérisée par sa fonction d'oubli F(·).
La propriété la plus importante d'un algorithme d'estimation récursif avec oubli est que sa matrice de covariance reste bornée (Salgado et al., 1988), (Parkum et al., 1992). En effet, sans borne inférieure à la matrice de covariance, le gain de l'algorithme va tendre vers zéro et le système va perdre ses capacités d'adaptation. Sans borne supérieure, la matrice de covariance peut exploser sous l'effet de d'oubli, et engendrer des instabilités qui mènent à l'écroulement du système.
Différentes formes d'oubli
Cette section présente les principales techniques pour introduire de l'oubli dans l'algorithme des moindres carrés récursifs. La plus simple est l'utilisation d'un facteur d'oubli exponentiel, mais elle est sujette au problème d'explosion de la matrice de covariance (covariance « wind-up » problem). Plusieurs autres techniques ont été proposées pour tenter de pallier ce problème comme utiliser un facteur d'oubli variable ou de l'oubli directionnel.
Facteur d'oubli constant
Le principe du facteur d'oubli exponentiel est d'introduire une pondération exponentiellement décroissante des anciennes données dans le calcul de la matrice de covariance : 
Cependant, le choix de ce facteur d'oubli est complexe. Si le facteur d'oubli est trop faible, la matrice de covariance va tendre vers zéro et l'algorithme va perdre ses capacités d'adaptation. Si le facteur d'oubli est trop fort, la matrice de covariance va exploser et l'algorithme va devenir instable jusqu'à provoquer l'effondrement du système. Ce phénomène -appelé covariance « wind-up » -est dû au fait que l'on oubli davantage d'information que l'on en reçoit.
Pour éviter ce problème, il est nécessaire d'ajuster le facteur d'oubli à la quantité d'information nouvellement disponible. C'est le principe du facteur d'oubli variable dans le temps.  et al. (1981) proposent ainsi d'utiliser l'erreur quadratique a posteriori (pondé-rée) comme métrique : ?
Facteur d'oubli variable
Une autre mesure possible, reflétant la quantité d'information acquise par l'algorithme, est la trace de la matrice de covariance :
Un tel facteur d'oubli variable permet à l'algorithme de conserver un gain non nul au cours du temps et le système peut ainsi s'adapter à une évolution des données. Cette approche permet aussi d'éviter l'explosion de la matrice de covariance, mais seulement si l'information est bien répartie selon les dimensions de l'espace d'entrée. Dans le cas contraire, certains éléments de la matrice de covariance peuvent tendre vers zéro pendant que d'autres explosent. Pour éviter cela, il faut non seulement faire varier le facteur d'oubli dans le temps mais également dans l'espace, c'est ce qu'on appelle l'oubli directionnel.
Oubli directionnel
L'oubli directionnel -Directional Forgetting (DF) -a été proposé par Hägglund (1985) et Kulhavy (1987) puis amélioré par Bittanti et al. (1990) et Cao et Schwartz (2000. C'est une technique qui fait varier l'oubli dans le temps mais aussi dans l'espace en introduisant de l'oubli seulement dans les directions où de l'information arrive.
La fonction d'oubli utilisée est donc paramétrée par la donnée courante :
L'avantage de cette technique est qu'elle maintient la matrice de covariance bornée, et garantit ainsi les performances du système. Dans les directions où beaucoup d'information arrive, on oublie beaucoup ; mais on oublie peu dans les directions qui sont peu excitées.
Résultats expérimentaux
Dans cette section, nous étudions l'impact de ces différentes approches d'oubli dans le cadre de l'apprentissage incrémental de systèmes d'inférence floue pour la classification. 
Protocole d'évaluation
FIG. 1 -Exemples de gestes -ILGDB groupe 1 (gestes libres).
Pour que l'évaluation soit représentative de l'utilisation d'un système évolutif, nous utilisons le mode d'évaluation prédictif séquentiel (Gama et al., 2009). Comme un classifieur évolutifs essaye d'abord de reconnaître un geste, puis s'en sert ensuite pour apprendre une fois qu'il dispose de sa vrai étiquette, nous évaluons nos systèmes d'une manière similaire. Ainsi, chaque donnée est d'abord utilisée comme donnée de test, puis ensuite comme donnée d'apprentissage. Les taux d'erreur sont ensuite calculés sur une fenêtre entre chaque point de test.
Performances en environnement non stationnaire
Nous évaluons ces différents systèmes sur un scénario non stationnaire simulant des changements de concept brusques. Pour simuler ces changements de concept, nous utilisons les 21 scripteurs du groupe 1 (gestes libres) à la suite. D'un scripteur à l'autre, les gestes d'une même classe sont différents ce qui oblige le classifieur à changer complètement son modèle.
Les taux d'erreur sont calculés sur les 56 derniers gestes (sur 173) de chaque scripteur pour évaluer les performances une fois le changement de concept appris. Les résultats (moyennés sur 20 ordres différents de présentation des scripteurs) sont présentés Figure 2.
Tout d'abord, les performances du système Evolve sans oubli sont sans équivoque. Sans oubli le système n'est pas capable à long terme de s'adapter aux changements de concepts. L'utilisation d'un facteur d'oubli exponentiel est indispensable, mais son choix est difficile. Si l'utilisation d'un facteur d'oubli variable dans le temps peut donner de bons résultats sur des problèmes de régression, ce n'est pas du tout le cas dans notre contexte. L'utilisation de la trace de la matrice de covariance comme mesure d'information (Evolve VF (trace)) n'empêche pas certains éléments de la matrice de tendre vers zéro et le système perd très vite ses capacités d'adaptation. Au contraire, l'utilisation de l'erreur a posteriori (Evolve VF (error)) n'empêche pas l'explosion de la matrice de covariance lorsque le système est inégalement excité suivant les directions de l'espace.
Enfin, l'oubli directionnel est la seule approche stable, ce qui est logique car elle garantit que la matrice de covariance reste bornée. On observe cependant une légère augmentation du taux d'erreur tout au long de l'expérience. Cela vient du fait que certaines valeurs de la matrice de covariance se stabilisent à des valeurs relativement faibles, ce qui réduit légèrement les capacités d'adaptation du système.
Conclusion
Dans cet article nous avons étudié comment introduire de l'oubli dans l'apprentissage incrémental de classifieurs évolutifs basés sur des systèmes d'inférence floue (SIF). Plusieurs approches existent, principalement tirées du domaine de la régression, pour introduire de l'oubli dans l'algorithme des moindres carré récursifs utilisé pour l'apprentissage des SIF. 
Summary
This paper study the use of forgetting for the incremental learning of evolving classifiers based on fuzzy inference systems. Several techniques are presented and compared on handwritten gesture recognition task in changing environement.

Introduction
Dans cet article, nous nous intéressons à la problématique d'évolution d'une ontologie dé-diée à la représentation de relations n-aires pour l'annotation de documents dans le cadre du Web Sémantique. Une relation n-aire est une relation qui est définie entre au moins deux arguments. Nous nous appuierons sur la représentation de relations n-aires sans arguments diffé-renciés telles que proposées par le W3C 1 , ce qui correspond au cas le plus général d'utilisation des relations n-aires, le cas 3. Nous avons de plus choisi d'utiliser le "patron 1" qui consiste à représenter une relation n-aire à l'aide d'un concept, relié à ses arguments par des propriétés, comme dans l'exemple de la figure 1. Nous nous intéressons plus particulièrement aux relations n-aires entre des données quantitatives expérimentales, ce qui suppose d'apporter une attention particulière à la gestion des arguments numériques (i.e. les quantités) et leurs unités de mesure. Supposons que l'on veuille ajouter la relation n-aire O2Permeability_Relation de la figure 1 à notre ontologie, MapOpt Ontology, dédiée à la représentation de relations n-aires entre des données expérimentales quantitatives dans le domaine du risque alimentaire microbiologique étendu aux emballages (Touhami et al., 2011). L'existence d'interdépendances entre la relation et ses six arguments nécessite d'effectuer un nombre important de changements dans l'ontologie pour y parvenir (e.g. ajout de concepts, de propriétés, de restrictions). En outre, ces changements peuvent engendrer des incohérences dans l'ontologie. Nous présentons ici la représentation formelle des changements applicables à notre ontologie permettant de modifier sa structure tout en maintenant sa cohérence.
Le processus global de gestion de l'évolution de notre ontologie ainsi que de ses artefacts dépendants est celui de Stojanovic (2004), et nous présentons dans cet article l'étape de la représentation du changement de l'ontologie et la sémantique du changement. Dans notre approche une relation n-aire constitue un ensemble d'interdépendances (entre la relation et ses arguments, entre une quantité et les unités de mesures associées) à représenter et à traiter conjointement dans le processus d'évolution de l'ontologie.
Dans la section 2, nous présentons la modélisation de l'ontologie dédiée à la représentation de relations n-aires et la notion d'ontologie structurellement cohérente. Les changements élé-mentaires et composés permettant de faire évoluer l'ontologie sont détaillés dans la section 3. Enfin, nous concluons et présentons les perspectives de ce travail dans la section 4. 2 L'ontologie dédiée à la représentation de relations n-aires D'après Reymonet et al. (2006), la modélisation d'une ontologie est fortement influencée par la tâche, le domaine d'intérêt et l'application pour laquelle elle a été conçue. Pour nous, la tâche à réaliser est l'annotation de relations n-aires dans des documents pour l'interrogation ; le domaine d'intérêt est l'étude de données expérimentales quantitatives des sciences du vivant ; l'application, enfin, est la construction d'un entrepôt de données ouvert sur le Web (Buche et al., 2012).
Définition 1 Pour un ensemble de concepts C, sa hiérarchie de spécialisation, notée H C , est une relation acyclique, H C ? C × C, telle que : si (c 1 , c 2 ) ? H C alors c 1 est un sousconcept (un fils) de c 2 , c 2 est un superconcept (ou père) de c 1 , H * C est la fermeture réflexive, antisymétrique et transitive de
Définition 2 Soit C un ensemble de concepts et quatre concepts de C, Simple_Concept, Quantity, Symbolic_Concept et Relation tel que Symbolic_Concept Simple_Concept et Quantity Simple_Concept. Nous définissons les ensembles des concepts suivants :
Définition 3 Une ontologie dédiée à la représentation de relations n-aires est définie par le tuple suivant : M O := (C, H C , P, H P , I, F) avec :
-C : l'ensemble des concepts de l'ontologie (cf. définition 2) et H C sa hiérarchie (cf. définition 1) ; -P : l'ensemble des propriétés telles que définie par le W3C 2 , où une propriété est définie comme une relation binaire entre des instances de concepts, des littéraux et des types de données primitifs, et H P sa hiérarchie telle que donnée dans la définition 1 où la notion de "concept" est remplacée par celle de "propriété" ; -I : l'ensemble des instances de concepts ; -F : un ensemble de fonctions retournant un sous-ensemble d'instances ou de concepts de l'ontologie.
Nous présentons ci-dessous plusieurs fonctions de F, utilisées dans la suite du papier : -la fonction signature de C rel dans 2 Carg qui permet d'associer à une relation r ? C rel le domaine de valeurs de chacun de ses arguments (c 1 , . . . , c n ) où c i ? C arg ; -la fonction dimension de C quantity dans I Dimension 3 qui permet d'associer à une quantité q ? C quantity sa dimension d ? I Dimension ; -la fonction linkconcepts de C rel ×P × R, où R = {allValuesFrom, someValuesFrom, hasValue}, dans C arg qui permet d'associer à un concept c 1 ? C rel un autre concept c 2 ? C arg via la propriété p ? P à l'aide d'une restriction de propriétés 4 . Pour la représentation du changement nous introduisons ci-dessous la notion d'ontologie CC-cohérente, en nous inspirant de Stojanovic (2004) pour adapter ses contraintes de cohé-rences à la structure particulière de notre ontologie. Définition 4 Une ontologie, définie par le tuple (C, H C , P, H P , I, F) cf. à la définition 3, est dite CC-cohérente si elle vérifie un ensemble de contraintes de cohérences structurelles, noté CC.
Nous avons identifié 39 contraintes de cohérences structurelles dont 16 portent sur les concepts, 7 sur les propriétés et 12 sur les instances. Nous présentons ci-dessous quelques exemples de ces contraintes :
-CC1 : une relation a au moins deux arguments : ?r ? C rel , |signature(r)| ? 2 -CC2 : Tous les concepts appartenant à la signature d'une relation doivent être définis : ?r ? C rel , ?c ? signature(r) =? c ? C arg -CC3 : Chaque quantité doit être associée à une seule valeur de dimension :
?q ? C quantity =? |dimension(q)| = 1 3 Évolution de l'ontologie dédiée à la représentation de relations n-aires L'évolution de la structure de l'ontologie telle que donnée dans la définition 4, se traduit par divers changements tels que l'ajout d'une relation n-aire, l'ajout ou la suppression d'un de ses arguments, etc. Pour des raisons de simplicité et de lisibilité, nous présentons l'évolution de la structure de notre ontologie à travers un cas d'utilisation, détaillé dans la sous-section 3.1. Puis, nous présentons dans les sous-sections 3.2 et 3.3 les changements élémentaires et composés qui permettent une telle évolution.
Cas d'utilisation : ajouter une relation n-aire à MapOpt Ontology
Supposons que la relation n-aire O2Permeability_Relation présentée dans la figure 1 ne soit pas définie dans MapOpt Ontology. L'ontologue doit commencer par ajouter ses six arguments : le nouveau concept Packaging comme sous-concept du concept Symbolic_Concept et les nouveaux concepts Partial_Pressure, Relative_Humidity, Thickness et O2Permeability comme sous-concepts du concept Quantity. L'ontologue doit ensuite définir le concept O2Per-meability_Relation comme sous-concept du concept Relation, ainsi que les propriétés et les restrictions de propriétés permettant de relier la relation à ses arguments.
L'ontologue doit donc, pour ajouter une nouvelle relation n-aire dans l'ontologie, effectuer un nombre important d'opérations de modification. Il doit également veiller à ce que les modifications effectuées respectent les contraintes de cohérences présentées dans la section 2. Afin d'alléger la tâche de l'ontologue et d'éviter des erreurs lors de manipulations fastidieuses de l'ontologie, nous définisson un ensemble de changements élémentaires et composés permettant de faire évoluer l'ontologie tout en préservant sa CC-cohérence.
Notion de changement élémentaire
Nous appelons changement élémentaire, une opération de modification de l'ontologie portant sur une seule entité : un concept, une propriété ou une instance. Pour faire évoluer l'ontologie, nous avons identifié 60 changements élémentaires dont 8 s'appliquent aux concepts (e.g. CreateHierarchyConceptLink), 41 aux propriétés (e.g. CreateConceptsLinked) et 5 aux instances de concepts (e.g. CreateInstance). Comme dans Stojanovic (2004), la formalisation des changements repose sur la définition d'un ensemble de pré-conditions (un ensemble d'assertions qui doivent être vraies pour pouvoir appliquer le changement) et de post-conditions (qui correspondent au(x) résultat(s) du changement) pour chaque type de changement, élémentaire ou composé. Les pré-conditions associées à un changement correspondent au sous-ensemble de contraintes de cohérences CC portant sur les parties de l'ontologie affectées par le changement considéré. La vérification des pré-conditions d'un changement permet d'éviter d'appliquer ce changement sur une ontologie non CC-cohérente. Les post-conditions garantissent que le changement a bien été appliqué, mais pas la CC-cohérence de l'ontologie.
Nous présentons ci-dessous l'exemple du changement élémentaire nécessaire pour faire évoluer MapOpt Ontology dans le cas d'utilisation décrit dans la section 3.1.
Définition 5 CreateLinkConcepts (c 1 , p, c 2 , allValuesFrom) : := Sémantique : associer un concept c 2 au concept c 1 via une propriété p en utilisant la restriction allValuesFrom Pré-condition : c 1 ? C rel , c 2 ? C arg , p ? P, {c | c ? C arg ? linkconcepts(c 1 , p, allV alues-F rom) = c} = ? Post-condition : c 1 ? C rel , c 2 ? C arg , p ? P, linkconcepts(c 1 , p, allV aluesF rom) = c 2 .
Ce changement est considéré comme élémentaire étant donné qu'il ne permet de manipuler qu'une seule entité : le concept c 1 . Comme pré-condition, il s'agit de vérifier : i) que les concepts c 1 et c 2 et la propriété p sont bien définies et ii) qu'aucun argument n'est déjà associé directement au concept c 1 via la propriété p à l'aide de la restriction allValuesFrom.
Changements composés
Un changement composé est une opération de modification de l'ontologie qui affecte plusieurs entités et peut se décomposer en une succession de changements élémentaires. Pour faire évoluer l'ontologie, nous avons identifié 12 changements composés (e.g. AddConcept, AddHierarchyOfSymbolicConcept). Nous présentons ci-dessous le changement composé né-cessaire pour faire évoluer MapOpt Ontology dans le cas d'utilisation décrit dans la section 3.1.
u 1 , . . . , u nu ? I U nit_Concept 5 , p 1 , . . . , p n ? P , o 1 , . . . , o n ? C arg , rs 1 , . . . , rs n ? R Sémantique : ajouter un concept newc c, qui peut être une relation ((c, Relation) ? H * C ) ou un concept simple ((c, Simple_Concept) ? H * C ) avec sa hiérarchie, et ses restrictions. Traitement du concept :
-Créer le concept newc -Créer un lien hiérarchique de subsomption entre newc et c : CreateHierarchyConceptLink(newc, c) Traitement des restrictions :
1. CreateLinkDimension (newc, hasDimension, d, hasV alue) 2. ?i ? [1, n u ], CreateLinkUnit (newc, hasU nitConcept, u i , allV aluesF rom) 3. CreateRestrictionMinMax (newc, hasN umericalV alue, min, max) 4. ?i ? [1, n], CreateLinkConcepts (newc, p i , o i , rs i )). Post-condition : newc ? C, (newc, c) ? H C .
Le changement composé AddConcept permet de créer soit une nouvelle relation, soit un nouveau concept, sous-concept de Simple_Concept, (i.e. une quantité, un concept symbolique ou un concept simple), qui sera utilisé comme argument d'une relation. Pour tous ces concepts, il y a un traitement en commun : créer le nouveau concept et le lier avec son concept père. Ensuite, il n'y a aucun traitement supplémentaire pour les concepts simples et les concepts symboliques. Pour les quantités, il s'agit de leurs associer leur dimension (traitement des restrictions 1), leurs unités de mesures (traitement des restrictions 2) et éventuellement un intervalle 5. Les unités de mesure sont définies comme des instances du concept Unit_Concept ? C

Introduction
L'extraction de motifs fréquents est une tâche importante dans le domaine de la fouille de données. Initialement centrée sur la découverte d'ensemble d'items (itemsets) fréquents (Agrawal et al., 1993), les premiers travaux ont été étendus pour extraire des motifs structurels comme les séquences (Agrawal et Srikant, 1995), les arbres (Chi et al., 2004a) ou les graphes (Washio et Motoda, 2003).
Alors que l'extraction d'itemsets fréquents recherche les combinaisons fréquentes d'items, l'extraction de motifs structurels recherche des sous-structures fréquentes. La plupart des travaux existants se focalisent sur un seul type de problème (fouille d'itemsets ou fouille structurelle). Toutefois, afin de représenter des données plus complexes, il semble naturel de considérer des collections structurées d'itemsets. Dans cet article, nous introduisons le problème de fouille d'arbres attribués. Les arbres attribués sont des arbres dans lesquels les noeuds sont associés à des itemsets.
Les arbres attribués peuvent être utilisés dans de nombreuses applications de fouilles de données spatio-temporelles. Dans le cas d'études épidémiologiques, par exemple, l'espace géographique peut être découpé en zones qui sont représentées par les noeuds de l'arbre, les itemsets décrivent les caractéristiques de ces zones à un temps donné et les arêtes symbolisent des relations de voisinage avec d'autres zones au temps suivant. Les motifs fréquent trouvés sont ainsi susceptible de donner un éclairage nouveau sur les déterminants d'une pathologie. D'autres applications peuvent être imaginées dans divers domaines comme l'analyse des arbres de retweet, la fouille d'arbres phylogénétiques, la fouille de documents XML ou l'analyse de journaux d'activité Web (Web log analysis).
Les contributions clés de notre travail sont les suivantes : 1) nous présentons le problème de la fouille de sous-structures ordonnées et non ordonnées dans une collection d'arbres attribués, 2) nous définissons les formes canoniques des arbres attribués, 3) nous proposons une méthode permettant l'énumération des arbres attribués qui est basée sur la combinaison de deux opéra-tions : l'extension de la structure arborescente et l'extension des itemsets associés aux noeuds, 4) nous présentons trois variantes d'un algorithme permettant d'extraire les motifs fréquents dans des arbres attribués, 5) nous montrons les résultats expérimentaux effectués sur plusieurs jeux de données artificiels et un jeu de données réel.
Cet article est organisé de la manière suivante : la section 2 présente les concepts de base et définit le problème, la section 3 propose un bref aperçu de l'état de l'art et met l'accent sur les quelques études qui combinent la fouille d'itemsets et la fouille structurelle, la section 4 décrit la méthode en insistant sur l'exploration de l'espace de recherche, le calcul des fréquences et l'élagage des candidats, la section 5 montre les résultats de la fouille de plusieurs jeux de données artificiels et réels et finalement, la section 6 conclut l'article et présente de possibles extensions de notre travail.
Concepts généraux et définition du problème
Dans cette section, nous introduisons les concepts et définitions nécessaires et présentons le problème de fouille d'arbres attribués.
Préliminaires
Soit I = {i 1 , i 2 , .., i n } un ensemble d'items. Un itemset est un ensemble P ? I. Les items appartenant à un itemset sont triés selon l'ordre lexicographique. L'ensemble D des itemsets présents dans une base de données est noté {P1, P2, ..., Pm} avec ?P ? D, P ? I. D est une base de données de transactions.
Un arbre S = (V, E) est un graphe orienté acyclique et connecté dans lequel V est l'ensemble des noeuds et E = {(u, v)|u, v ? V } est l'ensemble des arêtes. Un noeud particulier r ? V est considéré comme étant la racine, et pour chacun des autres noeuds x ? V , il existe un unique chemin allant de r à x. S'il existe un chemin allant d'un noeud u à un noeud v dans S = (V, E), alors u est un ancêtre de v (v est un descendant de u). Si (u, v) ? E (i.e. u est un ancêtre direct de v), alors u est un parent de v (v est un enfant de u). Dans un arbre ordonné, les fils de chaque noeuds sont ordonnés, sinon, l'arbre est non ordonné. Dans cette article, sauf spécification contraire, nous considérons que les arbres sont non ordonnés. Un arbre attribué est un triplet T = (V, E, ?) où (V, E) représente l'arbre sous-jacent et ? : V ? D est une fonction qui associe un itemset ?(u) ? I à chaque noeud u ? V . La taille d'un arbre attribué est le nombre des items associés à l'ensemble des noeuds.
Dans cet article, nous utilisons une représentation textuelle d'un arbre attribué basée sur celle définie par Zaki (2002) pour les arbres étiquetés. Notre représentation se distingue par l'écriture des noeuds qui est générée en listant tous les items présents dans l'itemset associé et par le fait que, par simplicité, nous omettons les $s finaux. Par exemple, la représentation textuelle de l'arbre attribué A2 illustré dans la figure 1 est "a c $ cde ab $ a".
Les arbres attribués peuvent être vus comme des itemsets organisés selon une structure arborescente. L'inclusion sur les arbres attribués peut donc être définie en considérant soit l'inclusion d'itemsets, soit l'inclusion structurelle. Pour l'inclusion portant sur les itemsets, on considère que l'arbre attribué T 1 est contenu dans un autre arbre attribué T 2 si les deux arbres attribués ont la même structure et que, pour chaque noeud de T 1 , l'itemset qui lui est associé est inclus dans l'itemset du noeud correspondant de T 2 . Plus formellement,
. L'inclusion structurelle est quant à elle représentée par le concept classique de sous-arbre (Balcázar et al., 2010;Chi et al., 2004a;Hido et Kawano, 2005;Nijssen et Kok, 2003;Termier et al., 2004;Xiao et al., 2003;Zaki, 2002).
A partir de la définition précédente, nous généralisons la notion de sous-arbre attribué de la façon suivante. T 1 = (V 1 , E 1 , ? 1 ) est un sous-arbre attribué d'un arbre attribué T 2 = (V 2 , E 2 , ? 2 ) et on note T 1 < T 2 si T 1 est un sous-arbre attribué isomorphe de T 2 , i.e. il existe une correspondance ? :
). Si T 1 est un sous-arbre attribué de T 2 , on dit que T 2 est un super-arbre attribué de T 1 . T 1 est un sous-arbre attribué induit de T 2 ssi T 1 est un sous-arbre attribué isomorphe de T 2 et ? préserve la relation parent-enfant. T 1 est un sous-arbre attribué enchâssé de T 2 ssi T 1 est un sous-arbre attribué isomorphe de T 2 et ? préserve la relation ancêtre-descendant. La figure 1 montre un exemple d'une base de données d'arbres attribués composée de trois arbres attribués différents ainsi que deux ensembles de motifs communs ; l'un contenant des arbres attribués induits et l'autre des arbres attribués enchâssés.
Tous les algorithmes de fouille d'arbres travaillant avec des arbres non-ordonnés doivent prendre en compte le problème d'isomorphisme. Pour éviter la génération redondante de solutions équivalentes, un arbre est choisi en tant que forme canonique et les formes alternatives sont écartées (Asai et al., 2002;Chi et al., 2004b;Nijssen et Kok, 2003;Xiao et al., 2003;Zaki, 2004). Dans de précédents travaux, les formes canoniques sont basées sur l'ordre lexi-cographique des étiquettes des noeuds. Dans notre travail, nous définissons un ordre basé sur l'ordre des itemsets associés aux noeuds. Etant donné deux itemsets P et Q (P = Q), on dit que
De la définition précédente, un ordre, ?, portant sur les arbres attribués peut être défini. A partir de cet ordre, une forme canonique pour les arbres attribués isomorphes est facilement définie en utilisant la méthode présentée par Chi et al. (2004a).
Le problème de la fouille d'arbres attribués est que le nombre de motifs fréquents est souvent très important. Dans des applications réelles, générer toutes les solutions peut s'avérer très coûteux ou même impossible. De plus, un nombre important de solutions contient des informations redondantes. Dans la figure 1, par exemple, l'arbre attribué "a e" est présent dans toutes les transactions mais le motif est déjà encodé dans "a cde".
Depuis la proposition de Mannila et Toivonen (2005) d'importants efforts on été consacrés à l'élaboration de représentations condensées qui résument toutes les solutions dans un ensemble restreint. L'ensemble des motifs fermés est un exemple d'une telle représentation condensée (Pasquier et al., 1999). Un arbre attribué T est un arbre attribué fermé si aucun de ses super-arbres attribués ne possède le même support que lui. Dans cet article, nous introduisons une autre représentation résumée des motifs qui est définie uniquement en fonction de la relation contenu dans. On dit que T est un arbre attribué c-fermé (contenu fermé) s'il n'est pas contenu (comme défini précédemment) dans un autre arbre attribué avec le même support.
Définition du problème
Soit une base de données B d'arbres attribués et un arbre attribué T , la fréquence par transaction de T est représentée par le nombre d'arbres attribués dans B pour lesquels T est un sous-arbre attribué. Un arbre attribué est fréquent si sa fréquence par transaction est supérieure ou égale à un seuil minimum. Le problème consiste à énumérer tous les motifs fréquents dans un ensemble d'arbres attribués.
Etat de l'art
Les premiers algorithmes de fouille d'arbres étiquetés sont dérivés de l'approche Apriori (Agrawal et al., 1993). Ceux-ci consistent en une succession d'itérations comprenant une génération des candidats suivie d'un calcul de fréquences à l'issue duquel les motifs non fréquents sont écartés. Deux stratégies sont possibles pour la génération des candidats : extension et jointure. Avec l'extension, un nouveau candidat est généré en ajoutant un noeud à un arbre fréquent (Asai et al., 2002;Nijssen et Kok, 2003). Avec la jointure, un nouveau candidat est créé en combinant deux arbres fréquents (Hido et Kawano, 2005;Zaki, 2004). La combinaison de ces deux principes a également été étudiée (Chi et al., 2004b).
D'autres algorithmes de fouille d'arbres sont dérivés de l'approche FP-growth (Han et al., 2000). Ces algorithmes, qui adoptent le principe de pattern-growth, permettent d'éviter le coûteux processus de génération de candidats. Cependant, cette approche ne peut pas être adaptée simplement au problème de la fouille d'arbres. Les implémentations existantes sont limitées dans le type d'arbres qu'elles peuvent manipuler (Xiao et al., 2003;Wang et al., 2004).
Trouver des représentations condensées des motifs fréquents est une extension naturelle de la fouille de motifs. Pour la fouille d'itemsets, la notion de fermeture a été formellement définie par Pasquier et al. (1999). Plusieurs travaux ont exploré ce thème dans le contexte de la fouille d'arbres et ont proposé des méthodes de fouille adaptées ainsi que diverses implémentations (Chi et al., 2004c;Termier et al., 2004Termier et al., , 2008. A notre connaissance, aucune méthode n'a été proposée dans le cadre général des arbres attribués.
Récemment, nous avons vu un intérêt croissant pour la fouille d'itemsets organisés selon une certaine structure. Miyoshi et al. (2009) travaillent sur des graphes étiquetés avec des attributs quantitatifs associés aux noeuds. Ce type de structure permet de résoudre le problème en combinant un algorithme "classique" d'exploration de sous-graphe pour le graphe étiqueté, et un algorithme existant de fouille d'itemsets pour les attributs quantitatifs. Cette approche ne peut pas être utilisée si les attributs associés aux noeuds ont tous la même importance. Fukuzaki et al. (2010) étudient des graphes dans lesquels les noeuds sont associés à des itemsets. Notre travail diffère de cette étude dans le sens où nous recherchons des motifs dans lesquels les itemsets associés aux noeuds ne sont pas forcément identiques.
Fouille d'arbres attribués fréquents
Nous nous intéressons principalement à l'identification de sous-arbres attribués induits, ordonnés ou non. Bien que l'on se focalise sur les sous-arbres attribués induits, nous définissons une méthode générale qui est capable d'extraire également les sous-arbres attribués enchâssés.
Énumération des arbres attribués
En utilisant l'opérateur ?, il est possible de construire un arbre des candidats Q représen-tant l'ensemble de l'espace de recherche (Ayres et al., 2002) de la manière suivante. La racine de Q est étiquetée avec ?. Récursivement, pour chaque noeud terminal n ? Q, des fils n sont ajoutés tel que n ? n . Les fils d'un noeud n ? Q sont générés soit par extension d'arbre, soit par extension d'itemset.
Extension de la structure arborescente
Pour l'extension de la structure arborescente (extension d'arbre), nous utilisons une variante de la technique connue sous le nom de rightmost path extension (Asai et al., 2002;Nijssen et Kok, 2003). Un arbre attribué T peut être étendu pour générer de nouveaux arbres attribués de deux façons différentes. Dans le premier cas, un nouveau fils N est ajouté au noeud terminal situé le plus à droite de T . Dans le second cas, un nouveau frère N est ajouté à l'un des noeuds situés sur le chemin le plus à droite de T (Chehreghani, 2011). Dans l'approche classique, N représente n'importe quel noeud valide figurant dans la base de données. Dans notre approche, de nouveaux noeuds N sont créés à partir des noeuds valides Q de la base de données. Dans les faits, chaque noeud Q, associé à un itemset de taille k, génère un ensemble de k noeuds N = {N 1 , .., N k } qui sont utilisés pour l'extension d'arbre. Chaque N i est associé à un itemset de taille 1 ; l'unique item étant le ième item de ?(Q). Par exemple, dans la figure 1, les noeuds qui peuvent être utilisés pour l'extension du motif "a cde" sont "ab", "a" (de A2), "abc" et "c" (de A3). A partir du noeud "abc", trois extensions sont générées ("a", "b" et "c") alors que le noeud "ab" génère "a" et "b". Les noeuds "a" et "c" génèrent respectivement les extensions "a" et "c". Trois nouveaux candidats différents sont ainsi obtenus en ajoutant chacune de ces extensions au motif candidat : "a cde a", "a cde b" et "a cde c" Pour les arbres ordonnés, la complétude et la non-redondance de cette méthode a été dé-montrée (Asai et al., 2002). Pour les arbres non ordonnés, la méthode peut générer des motifs redondants sous la formes d'arbres isomorphes. Les candidats dupliqués sont détectés et écartés avant la phase d'extension au moyen d'un test de canonicité.
Extension des itemsets associés aux noeuds
Pour l'extension des itemsets associés aux noeuds (extension d'itemset), nous utilisons une variante de la méthode présentée par Ayres et al. (2002). Grâce à cette variante, un nouvel item I est ajouté à l'itemset associé au noeud terminal le plus à droite de l'arbre attribué candidat T . Les items utilisés pour l'extension d'itemset sont dérivés de l'itemset associé à ce noeud dans la base de données. La contrainte est que le nouvel item doit être lexicographiquement situé après n'importe lequel des items associés au noeud terminal le plus à droite de T .
Calcul des fréquences
Nous organisons nos données dans une structure stockant toute l'information nécessaire au processus de fouille. Notre structure est une extension de la représentation verticale introduite par Zaki (2002Zaki ( , 2004. Brièvement, chaque sous-arbre attribué candidat est associé à son motif et à un ensemble de données annexes permettant de localiser précisément toutes ses occurrences dans la base de données. Les premiers candidats, composés d'un noeud unique associé à un seul item, sont générés en parcourant la base de données. En utilisant uniquement cette structure, il est facile de calculer le nombre d'occurrences de chaque motif. De plus, cette même structure est suffisante pour générer toutes les extensions possibles d'un motif donné. Quand un motif de taille k est traité, toutes ses occurrences sont étendues par les méthodes d'extension d'arbre et d'extension d'itemset décrites plus haut pour générer de nouveaux candidats de taille (k + 1) qui sont eux même stockés dans la structure de données.
Parcours de l'espace de recherche
Plusieurs techniques peuvent être utilisées pour élaguer l'espace de recherche.
Élagage des candidats
Le principe, énoncé par Agrawal et al. (1993) il y a une vingtaine d'années, peut être appliqué à la fouille d'arbres attribués : i) tout sous-motif d'un motif fréquent est fréquent, et ii) tout super-motif d'un motif non fréquent est non fréquent. Comme la fréquence suit une fonction anti-monotone (l'extension d'un motif ne peut pas donner un nouveau motif avec une fréquence supérieure), il est possible d'arrêter l'exploration d'une branche lorsque la fréquence d'un candidat est inférieure au support minimum. Par exemple, dans la figure 1, durant la fouille des arbres attribués, lorsque l'on détermine que la fréquence du motif "a c a" est inférieure au support minimum, il n'est pas nécessaire de générer des extensions de ce motif (e.g. "a c ab", "a c a $ b", "a c a $ $ c"). Dans le cas de la fouille d'arbres attribués non ordonnées, l'extension est également stoppée si le candidat examiné n'est pas sous forme canonique.
Énumération des arbres attribués c-fermés
En énumérant uniquement les arbres attribués qui ne sont pas contenus dans un autre arbre attribué ayant le même support, l'espace de recherche peut être considérablement réduit. L'énumération d'arbres attribués c-fermés nécessite de stocker tous les motifs fréquents identifiés ainsi que leur fréquence par transaction et leur nombre total d'occurrences.
Soit T un arbre attribué candidat, T l'ensemble de tous les sous-arbres attribués fréquents identifiés précédemment et X l'ensemble de tous les candidats générés par l'extension de T . Nous distinguons deux sous ensembles de X : X I , l'ensemble des motifs générés par extension d'itemset de T et X T , l'ensemble motifs obtenus par extension d'arbre de T . Nous définissons deux fonctions : f t qui donne la fréquence par transaction d'un sous-arbre attribué et f o qui retourne son nombre total d'occurrences. T est un arbre attribué c-fermé si ?T ? T U X I tel que
. Cependant, identifier une extension d'itemset de T avec la même fréquence par transaction que T ne permet pas de stopper l'exploration des autres candidats de X . La condition supplémentaire suivante doit également être satisfaite : ?T ? T U X I :
. Dans la figure 1, par exemple, le premier candidat à être examiné est "a" avec une fré-quence par transaction de 3. Par extension d'itemset, nous construisons X I = {"ab", "ac"}. Le candidat "ab" a une fréquence par transaction de 3, donc, comme "a" < I "ab", "a" n'est pas c-fermé. Cependant, le motif "a" apparaît 7 fois dans la base de données alors que la fréquence d'occurrence du candidat "ab" est 3. Les 4 occurrences où "a" apparaît dans un itemset qui ne contient pas "b" peuvent éventuellement générer des motifs qui eux sont cfermés. C'est effectivement le cas dans la figure 1 où l'extension du noeud droit du motif "a" génère le candidat "a e" avec un fréquence par transaction de 3.
Énumération d'arbres attribués fermés
On dit que T est un arbre attribué fermé si ?T ? T U X tel que
Il faut également supprimer les arbres attribués non fermés stockés dans T , i.e. tous les motifs qui sont des sous-arbres attribués de T avec la même fréquence par transaction. Le test de fermeture nécessite d'effectuer plusieurs tests d'isomorphisme de sous-arbres qui sont des opérations coûteuses.
Description des algorithmes
La figure 2 illustre la structure de l'algorithme IMIT. Pour commencer, un ensemble de sous-arbres attribués de taille 1 est construit en parcourant la base de données (ligne 1). Chaque candidat est alors traité dans une boucle. La fonction GetF irst retourne le plus petit candidat par rapport à l'opérateur ? (ligne 3). Le candidat traité est supprimé de la liste des candidats (ligne 4). S'il est fréquent et s'il est sous forme canonique (ligne 5), il est ajouté à la liste des solutions (ligne 6) et toutes ses extensions sont ajoutées à la liste des candidats (ligne 7).
Cet algorithme, qui implémente la méthode d'élagage proposée dans la section 4.3.1 est suffisant pour énumérer toutes les solutions mais son espace de recherche est immense. Pour limiter les redondances dans l'ensemble des solutions, nous avons développé un algorithme d'extraction des sous-arbres attribués fermés appelé IMIT_CLOSED reprenant les principes IM IT (D, minSup) 1: C ? {all asubtrees of size 1 in D} 2: while C = ? do 3:
if isCanonical(T ) and f t (T ) ? minSup then exposés dans la section 4.3.3 (figure 3). Comme cela est montré dans la section 5, l'algorithme est coûteux et ne passe pas à l'échelle.
Forts de cette constatation, nous avons développé IMIT_CONTENT_CLOSED, un algorithme permettant l'extraction des sous-arbres attribués c-fermés dont le principe est présenté dans la section 4.3.2. Faute de place, cet algorithme n'est pas présenté ici. Cependant, il peut être facilement déduit de l'algorithme IMIT_CLOSED (figure 3) en remplaçant < par < I et en supprimant les lignes 12 à 14. L'utilisation de la relation < I à la place de < permet de se limiter aux tests d'inclusion d'itemsets qui sont beaucoup moins coûteux que les tests d'isomorphismes. Les lignes 12 à 14 suppriment de l'ensemble des solution trouvées précédemment celles qui contiennent un sous-arbre attribué du candidat actuellement examiné. Ce test n'est pas nécessaire dans le cas de l'extraction de motifs c-fermés. Les expérimentations montrent que ce troisième algorithme constitue le meilleur compromis entre la non redondance des solutions et le temps d'exécution.
Résultats experimentaux
Tous les algorithmes sont implémentés en C++ avec la STL. Les expérimentations ont été effectuées sur un ordinateur avec Ubuntu 12.04 LTS basé sur un processeur Intel c TM i5-2400 @ 3.10GHz avec 8 Gb de mémoire. Tous les temps d'exécution incluent la phase de prétraitement et l'affichage des résultats.
Jeux de données artificiels
Nous avons modifié le programme proposé par Zaki (2002) pour générer des arbres attribués avec différentes tailles d'itemsets. Nous avons utilisé les mêmes paramètres que Zaki (2002) sauf pour le nombre de sous-arbres que nous avons fixé à 10000. Nous avons construit cinq jeux de données en faisant varier la taille des itemsets. Dans T10K, tous les noeuds sont associés à des itemsets de taille 1. Dans T10K-3 et T10K-5, les noeuds sont associés respectivement à des itemsets de taille 3 et 5. Dans T10K-1/10, les noeuds sont associés à des itemsets de taille variant de 1 à 10 alors que dans T10K-1/20, la taille des itemsets varie de 1 à 20.
IM IT _CLOSED(D, minSup)
1: C ? {all asubtrees of size 1 in D} 2: while C = ? do 3:
if isCanonical(T ) and f t (T ) ? minSup then 6:
if ?T ? T : T < T and f t (T ) = f t (T ) and ?T ? X I : f t (x) = f t (T ) then 
Données de journaux d'activité Web
Nous avons élaboré un jeu de données à partir du journal d'activité Web de notre université. Cependant, au lieu d'étiqueter les noeuds avec les URLs des pages consultées, nous leur avons associé un ensemble de mots clés portant sur le contenu des pages. Cette approche permet de capturer les habitudes de navigation des utilisateurs même dans le cas où la structure du site Web change. Le jeu de données est composé de 126 396 arbres annotés avec des itemsets de taille 10 (10 mots clés par page).
Évaluation des performances
La figure 4 montre les temps d'exécution d'IMIT_CONTENT_CLOSED pour la fouille de motifs c-fermés induits et non-ordonnés sur les 5 jeux de données artificiels. A titre de comparaison, nous avons ajouté dans la figure les temps d'exécution de SLEUTH (Zaki, 2004), une implémentation de référence du paradigme d'extension de classes d'équivalence, pour fouiller T10K. IMIT_CONTENT_CLOSED est environ deux fois plus lent que SLEUTH pour toutes les valeurs de support, sauf pour les plus petites pour lesquelles SLEUTH est pénalisé par le coût de la jointure sur des millions de motifs fréquents. Bien qu'IMIT_CONTENT_CLOSED soit en général plus lent que SLEUTH, les résultats sont satisfaisants car notre algorithme est conçu pour fouiller des arbres attribués. Il est normal d'être moins performant que des implé-mentations dédiées à la fouille d'arbres étiquetés.
La figure montre également que la fouille d'arbres attribués demande beaucoup plus de ressources que la fouille d'arbres étiquetés ; et la différence est largement sous-estimée car seuls les motifs c-fermés sont listés. Extraire tous les motifs génère un grand nombre de so-  lutions et prend énormément de temps. Pour donner une idée, la fouille du jeu de données T10K-3 avec un support minimum de 1% génère plus de 12 millions de motifs en 15 heures (figure 5). Rechercher uniquement les sous-arbres c-fermés permet de réduire à la fois le nombre de motifs et le temps d'exécution. Ainsi, avec un support minimum de 1% , 200 motifs c-fermés sont identifiés en 4 secondes. Comme illustré dans la même figure, la recherche de motifs fermés permet de réduire encore le nombre de motifs. Avec un support de 1%, par exemple, le nombre de motifs tombe à 103. Cependant, à cause des tests coûteux d'isomorphisme d'arbres, en contrepartie, les performances s'effondrent lorsque les motifs sont nombreux. La figure 6 montre les temps d'exécution et le nombre de motifs c-fermés trouvés dans les données de journaux d'activité Web. Ce jeu de données réel est plus important que ceux utilisés précédemment et la fouille ne peut être réalisée raisonnablement avec un support inférieur à 10%. La fouille avec un support de 6% retourne 360 motifs en 6 heures.
Conclusion et perspectives
Dans cet article, nous avons introduit le problème de la fouille d'arbres attribués. Nous avons exploré des méthodes permettant d'énumérer tous les motifs fréquents ou seulement les fermés. Ces méthodes se sont révélées inefficaces à cause, dans le premier cas, du nombre Remerciements. Ce travail a été financé par le contrat ANR-2010-COSI-012 FOSTER.

Introduction
L'extraction de motifs fréquents est une tâche importante dans le domaine de la fouille de données. Initialement centrée sur la découverte d'ensemble d'items (itemsets) fréquents (Agrawal et al., 1993), les premiers travaux ont été étendus pour extraire des motifs structurels comme les séquences (Agrawal et Srikant, 1995), les arbres (Chi et al., 2004a) ou les graphes (Washio et Motoda, 2003).
Alors que l'extraction d'itemsets fréquents recherche les combinaisons fréquentes d'items, l'extraction de motifs structurels recherche des sous-structures fréquentes. La plupart des travaux existants se focalisent sur un seul type de problème (fouille d'itemsets ou fouille structurelle). Toutefois, afin de représenter des données plus complexes, il semble naturel de considérer des collections structurées d'itemsets. Dans cet article, nous introduisons le problème de fouille d'arbres attribués. Les arbres attribués sont des arbres dans lesquels les noeuds sont associés à des itemsets.
Les arbres attribués peuvent être utilisés dans de nombreuses applications de fouilles de données spatio-temporelles. Dans le cas d'études épidémiologiques, par exemple, l'espace géographique peut être découpé en zones qui sont représentées par les noeuds de l'arbre, les itemsets décrivent les caractéristiques de ces zones à un temps donné et les arêtes symbolisent des relations de voisinage avec d'autres zones au temps suivant. Les motifs fréquent trouvés sont ainsi susceptible de donner un éclairage nouveau sur les déterminants d'une pathologie. D'autres applications peuvent être imaginées dans divers domaines comme l'analyse des arbres de retweet, la fouille d'arbres phylogénétiques, la fouille de documents XML ou l'analyse de journaux d'activité Web (Web log analysis).
Les contributions clés de notre travail sont les suivantes : 1) nous présentons le problème de la fouille de sous-structures ordonnées et non ordonnées dans une collection d'arbres attribués, 2) nous définissons les formes canoniques des arbres attribués, 3) nous proposons une méthode permettant l'énumération des arbres attribués qui est basée sur la combinaison de deux opéra-tions : l'extension de la structure arborescente et l'extension des itemsets associés aux noeuds, 4) nous présentons trois variantes d'un algorithme permettant d'extraire les motifs fréquents dans des arbres attribués, 5) nous montrons les résultats expérimentaux effectués sur plusieurs jeux de données artificiels et un jeu de données réel.
Cet article est organisé de la manière suivante : la section 2 présente les concepts de base et définit le problème, la section 3 propose un bref aperçu de l'état de l'art et met l'accent sur les quelques études qui combinent la fouille d'itemsets et la fouille structurelle, la section 4 décrit la méthode en insistant sur l'exploration de l'espace de recherche, le calcul des fréquences et l'élagage des candidats, la section 5 montre les résultats de la fouille de plusieurs jeux de données artificiels et réels et finalement, la section 6 conclut l'article et présente de possibles extensions de notre travail.
Concepts généraux et définition du problème
Dans cette section, nous introduisons les concepts et définitions nécessaires et présentons le problème de fouille d'arbres attribués.
Préliminaires
Soit I = {i 1 , i 2 , .., i n } un ensemble d'items. Un itemset est un ensemble P ? I. Les items appartenant à un itemset sont triés selon l'ordre lexicographique. L'ensemble D des itemsets présents dans une base de données est noté {P1, P2, ..., Pm} avec ?P ? D, P ? I. D est une base de données de transactions.
Un arbre S = (V, E) est un graphe orienté acyclique et connecté dans lequel V est l'ensemble des noeuds et E = {(u, v)|u, v ? V } est l'ensemble des arêtes. Un noeud particulier r ? V est considéré comme étant la racine, et pour chacun des autres noeuds x ? V , il existe un unique chemin allant de r à x. S'il existe un chemin allant d'un noeud u à un noeud v dans S = (V, E), alors u est un ancêtre de v (v est un descendant de u). Si (u, v) ? E (i.e. u est un ancêtre direct de v), alors u est un parent de v (v est un enfant de u). Dans un arbre ordonné, les fils de chaque noeuds sont ordonnés, sinon, l'arbre est non ordonné. Dans cette article, sauf spécification contraire, nous considérons que les arbres sont non ordonnés. Un arbre attribué est un triplet T = (V, E, ?) où (V, E) représente l'arbre sous-jacent et ? : V ? D est une fonction qui associe un itemset ?(u) ? I à chaque noeud u ? V . La taille d'un arbre attribué est le nombre des items associés à l'ensemble des noeuds.
Dans cet article, nous utilisons une représentation textuelle d'un arbre attribué basée sur celle définie par Zaki (2002) pour les arbres étiquetés. Notre représentation se distingue par l'écriture des noeuds qui est générée en listant tous les items présents dans l'itemset associé et par le fait que, par simplicité, nous omettons les $s finaux. Par exemple, la représentation textuelle de l'arbre attribué A2 illustré dans la figure 1 est "a c $ cde ab $ a".
Les arbres attribués peuvent être vus comme des itemsets organisés selon une structure arborescente. L'inclusion sur les arbres attribués peut donc être définie en considérant soit l'inclusion d'itemsets, soit l'inclusion structurelle. Pour l'inclusion portant sur les itemsets, on considère que l'arbre attribué T 1 est contenu dans un autre arbre attribué T 2 si les deux arbres attribués ont la même structure et que, pour chaque noeud de T 1 , l'itemset qui lui est associé est inclus dans l'itemset du noeud correspondant de T 2 . Plus formellement,
. L'inclusion structurelle est quant à elle représentée par le concept classique de sous-arbre (Balcázar et al., 2010;Chi et al., 2004a;Hido et Kawano, 2005;Nijssen et Kok, 2003;Termier et al., 2004;Xiao et al., 2003;Zaki, 2002).
A partir de la définition précédente, nous généralisons la notion de sous-arbre attribué de la façon suivante. T 1 = (V 1 , E 1 , ? 1 ) est un sous-arbre attribué d'un arbre attribué T 2 = (V 2 , E 2 , ? 2 ) et on note T 1 < T 2 si T 1 est un sous-arbre attribué isomorphe de T 2 , i.e. il existe une correspondance ? :
). Si T 1 est un sous-arbre attribué de T 2 , on dit que T 2 est un super-arbre attribué de T 1 . T 1 est un sous-arbre attribué induit de T 2 ssi T 1 est un sous-arbre attribué isomorphe de T 2 et ? préserve la relation parent-enfant. T 1 est un sous-arbre attribué enchâssé de T 2 ssi T 1 est un sous-arbre attribué isomorphe de T 2 et ? préserve la relation ancêtre-descendant. La figure 1 montre un exemple d'une base de données d'arbres attribués composée de trois arbres attribués différents ainsi que deux ensembles de motifs communs ; l'un contenant des arbres attribués induits et l'autre des arbres attribués enchâssés.
Tous les algorithmes de fouille d'arbres travaillant avec des arbres non-ordonnés doivent prendre en compte le problème d'isomorphisme. Pour éviter la génération redondante de solutions équivalentes, un arbre est choisi en tant que forme canonique et les formes alternatives sont écartées (Asai et al., 2002;Chi et al., 2004b;Nijssen et Kok, 2003;Xiao et al., 2003;Zaki, 2004). Dans de précédents travaux, les formes canoniques sont basées sur l'ordre lexi-cographique des étiquettes des noeuds. Dans notre travail, nous définissons un ordre basé sur l'ordre des itemsets associés aux noeuds. Etant donné deux itemsets P et Q (P = Q), on dit que
De la définition précédente, un ordre, ?, portant sur les arbres attribués peut être défini. A partir de cet ordre, une forme canonique pour les arbres attribués isomorphes est facilement définie en utilisant la méthode présentée par Chi et al. (2004a).
Le problème de la fouille d'arbres attribués est que le nombre de motifs fréquents est souvent très important. Dans des applications réelles, générer toutes les solutions peut s'avérer très coûteux ou même impossible. De plus, un nombre important de solutions contient des informations redondantes. Dans la figure 1, par exemple, l'arbre attribué "a e" est présent dans toutes les transactions mais le motif est déjà encodé dans "a cde".
Depuis la proposition de Mannila et Toivonen (2005) d'importants efforts on été consacrés à l'élaboration de représentations condensées qui résument toutes les solutions dans un ensemble restreint. L'ensemble des motifs fermés est un exemple d'une telle représentation condensée (Pasquier et al., 1999). Un arbre attribué T est un arbre attribué fermé si aucun de ses super-arbres attribués ne possède le même support que lui. Dans cet article, nous introduisons une autre représentation résumée des motifs qui est définie uniquement en fonction de la relation contenu dans. On dit que T est un arbre attribué c-fermé (contenu fermé) s'il n'est pas contenu (comme défini précédemment) dans un autre arbre attribué avec le même support.
Définition du problème
Soit une base de données B d'arbres attribués et un arbre attribué T , la fréquence par transaction de T est représentée par le nombre d'arbres attribués dans B pour lesquels T est un sous-arbre attribué. Un arbre attribué est fréquent si sa fréquence par transaction est supérieure ou égale à un seuil minimum. Le problème consiste à énumérer tous les motifs fréquents dans un ensemble d'arbres attribués.
Etat de l'art
Les premiers algorithmes de fouille d'arbres étiquetés sont dérivés de l'approche Apriori (Agrawal et al., 1993). Ceux-ci consistent en une succession d'itérations comprenant une génération des candidats suivie d'un calcul de fréquences à l'issue duquel les motifs non fréquents sont écartés. Deux stratégies sont possibles pour la génération des candidats : extension et jointure. Avec l'extension, un nouveau candidat est généré en ajoutant un noeud à un arbre fréquent (Asai et al., 2002;Nijssen et Kok, 2003). Avec la jointure, un nouveau candidat est créé en combinant deux arbres fréquents (Hido et Kawano, 2005;Zaki, 2004). La combinaison de ces deux principes a également été étudiée (Chi et al., 2004b).
D'autres algorithmes de fouille d'arbres sont dérivés de l'approche FP-growth (Han et al., 2000). Ces algorithmes, qui adoptent le principe de pattern-growth, permettent d'éviter le coûteux processus de génération de candidats. Cependant, cette approche ne peut pas être adaptée simplement au problème de la fouille d'arbres. Les implémentations existantes sont limitées dans le type d'arbres qu'elles peuvent manipuler (Xiao et al., 2003;Wang et al., 2004).
Trouver des représentations condensées des motifs fréquents est une extension naturelle de la fouille de motifs. Pour la fouille d'itemsets, la notion de fermeture a été formellement définie par Pasquier et al. (1999). Plusieurs travaux ont exploré ce thème dans le contexte de la fouille d'arbres et ont proposé des méthodes de fouille adaptées ainsi que diverses implémentations (Chi et al., 2004c;Termier et al., 2004Termier et al., , 2008. A notre connaissance, aucune méthode n'a été proposée dans le cadre général des arbres attribués.
Récemment, nous avons vu un intérêt croissant pour la fouille d'itemsets organisés selon une certaine structure. Miyoshi et al. (2009) travaillent sur des graphes étiquetés avec des attributs quantitatifs associés aux noeuds. Ce type de structure permet de résoudre le problème en combinant un algorithme "classique" d'exploration de sous-graphe pour le graphe étiqueté, et un algorithme existant de fouille d'itemsets pour les attributs quantitatifs. Cette approche ne peut pas être utilisée si les attributs associés aux noeuds ont tous la même importance. Fukuzaki et al. (2010) étudient des graphes dans lesquels les noeuds sont associés à des itemsets. Notre travail diffère de cette étude dans le sens où nous recherchons des motifs dans lesquels les itemsets associés aux noeuds ne sont pas forcément identiques.
Fouille d'arbres attribués fréquents
Nous nous intéressons principalement à l'identification de sous-arbres attribués induits, ordonnés ou non. Bien que l'on se focalise sur les sous-arbres attribués induits, nous définissons une méthode générale qui est capable d'extraire également les sous-arbres attribués enchâssés.
Énumération des arbres attribués
En utilisant l'opérateur ?, il est possible de construire un arbre des candidats Q représen-tant l'ensemble de l'espace de recherche (Ayres et al., 2002) de la manière suivante. La racine de Q est étiquetée avec ?. Récursivement, pour chaque noeud terminal n ? Q, des fils n sont ajoutés tel que n ? n . Les fils d'un noeud n ? Q sont générés soit par extension d'arbre, soit par extension d'itemset.
Extension de la structure arborescente
Pour l'extension de la structure arborescente (extension d'arbre), nous utilisons une variante de la technique connue sous le nom de rightmost path extension (Asai et al., 2002;Nijssen et Kok, 2003). Un arbre attribué T peut être étendu pour générer de nouveaux arbres attribués de deux façons différentes. Dans le premier cas, un nouveau fils N est ajouté au noeud terminal situé le plus à droite de T . Dans le second cas, un nouveau frère N est ajouté à l'un des noeuds situés sur le chemin le plus à droite de T (Chehreghani, 2011). Dans l'approche classique, N représente n'importe quel noeud valide figurant dans la base de données. Dans notre approche, de nouveaux noeuds N sont créés à partir des noeuds valides Q de la base de données. Dans les faits, chaque noeud Q, associé à un itemset de taille k, génère un ensemble de k noeuds N = {N 1 , .., N k } qui sont utilisés pour l'extension d'arbre. Chaque N i est associé à un itemset de taille 1 ; l'unique item étant le ième item de ?(Q). Par exemple, dans la figure 1, les noeuds qui peuvent être utilisés pour l'extension du motif "a cde" sont "ab", "a" (de A2), "abc" et "c" (de A3). A partir du noeud "abc", trois extensions sont générées ("a", "b" et "c") alors que le noeud "ab" génère "a" et "b". Les noeuds "a" et "c" génèrent respectivement les extensions "a" et "c". Trois nouveaux candidats différents sont ainsi obtenus en ajoutant chacune de ces extensions au motif candidat : "a cde a", "a cde b" et "a cde c" Pour les arbres ordonnés, la complétude et la non-redondance de cette méthode a été dé-montrée (Asai et al., 2002). Pour les arbres non ordonnés, la méthode peut générer des motifs redondants sous la formes d'arbres isomorphes. Les candidats dupliqués sont détectés et écartés avant la phase d'extension au moyen d'un test de canonicité.
Extension des itemsets associés aux noeuds
Pour l'extension des itemsets associés aux noeuds (extension d'itemset), nous utilisons une variante de la méthode présentée par Ayres et al. (2002). Grâce à cette variante, un nouvel item I est ajouté à l'itemset associé au noeud terminal le plus à droite de l'arbre attribué candidat T . Les items utilisés pour l'extension d'itemset sont dérivés de l'itemset associé à ce noeud dans la base de données. La contrainte est que le nouvel item doit être lexicographiquement situé après n'importe lequel des items associés au noeud terminal le plus à droite de T .
Calcul des fréquences
Nous organisons nos données dans une structure stockant toute l'information nécessaire au processus de fouille. Notre structure est une extension de la représentation verticale introduite par Zaki (2002Zaki ( , 2004. Brièvement, chaque sous-arbre attribué candidat est associé à son motif et à un ensemble de données annexes permettant de localiser précisément toutes ses occurrences dans la base de données. Les premiers candidats, composés d'un noeud unique associé à un seul item, sont générés en parcourant la base de données. En utilisant uniquement cette structure, il est facile de calculer le nombre d'occurrences de chaque motif. De plus, cette même structure est suffisante pour générer toutes les extensions possibles d'un motif donné. Quand un motif de taille k est traité, toutes ses occurrences sont étendues par les méthodes d'extension d'arbre et d'extension d'itemset décrites plus haut pour générer de nouveaux candidats de taille (k + 1) qui sont eux même stockés dans la structure de données.
Parcours de l'espace de recherche
Plusieurs techniques peuvent être utilisées pour élaguer l'espace de recherche.
Élagage des candidats
Le principe, énoncé par Agrawal et al. (1993) il y a une vingtaine d'années, peut être appliqué à la fouille d'arbres attribués : i) tout sous-motif d'un motif fréquent est fréquent, et ii) tout super-motif d'un motif non fréquent est non fréquent. Comme la fréquence suit une fonction anti-monotone (l'extension d'un motif ne peut pas donner un nouveau motif avec une fréquence supérieure), il est possible d'arrêter l'exploration d'une branche lorsque la fréquence d'un candidat est inférieure au support minimum. Par exemple, dans la figure 1, durant la fouille des arbres attribués, lorsque l'on détermine que la fréquence du motif "a c a" est inférieure au support minimum, il n'est pas nécessaire de générer des extensions de ce motif (e.g. "a c ab", "a c a $ b", "a c a $ $ c"). Dans le cas de la fouille d'arbres attribués non ordonnées, l'extension est également stoppée si le candidat examiné n'est pas sous forme canonique.
Énumération des arbres attribués c-fermés
En énumérant uniquement les arbres attribués qui ne sont pas contenus dans un autre arbre attribué ayant le même support, l'espace de recherche peut être considérablement réduit. L'énumération d'arbres attribués c-fermés nécessite de stocker tous les motifs fréquents identifiés ainsi que leur fréquence par transaction et leur nombre total d'occurrences.
Soit T un arbre attribué candidat, T l'ensemble de tous les sous-arbres attribués fréquents identifiés précédemment et X l'ensemble de tous les candidats générés par l'extension de T . Nous distinguons deux sous ensembles de X : X I , l'ensemble des motifs générés par extension d'itemset de T et X T , l'ensemble motifs obtenus par extension d'arbre de T . Nous définissons deux fonctions : f t qui donne la fréquence par transaction d'un sous-arbre attribué et f o qui retourne son nombre total d'occurrences. T est un arbre attribué c-fermé si ?T ? T U X I tel que
. Cependant, identifier une extension d'itemset de T avec la même fréquence par transaction que T ne permet pas de stopper l'exploration des autres candidats de X . La condition supplémentaire suivante doit également être satisfaite : ?T ? T U X I :
. Dans la figure 1, par exemple, le premier candidat à être examiné est "a" avec une fré-quence par transaction de 3. Par extension d'itemset, nous construisons X I = {"ab", "ac"}. Le candidat "ab" a une fréquence par transaction de 3, donc, comme "a" < I "ab", "a" n'est pas c-fermé. Cependant, le motif "a" apparaît 7 fois dans la base de données alors que la fréquence d'occurrence du candidat "ab" est 3. Les 4 occurrences où "a" apparaît dans un itemset qui ne contient pas "b" peuvent éventuellement générer des motifs qui eux sont cfermés. C'est effectivement le cas dans la figure 1 où l'extension du noeud droit du motif "a" génère le candidat "a e" avec un fréquence par transaction de 3.
Énumération d'arbres attribués fermés
On dit que T est un arbre attribué fermé si ?T ? T U X tel que
Il faut également supprimer les arbres attribués non fermés stockés dans T , i.e. tous les motifs qui sont des sous-arbres attribués de T avec la même fréquence par transaction. Le test de fermeture nécessite d'effectuer plusieurs tests d'isomorphisme de sous-arbres qui sont des opérations coûteuses.
Description des algorithmes
La figure 2 illustre la structure de l'algorithme IMIT. Pour commencer, un ensemble de sous-arbres attribués de taille 1 est construit en parcourant la base de données (ligne 1). Chaque candidat est alors traité dans une boucle. La fonction GetF irst retourne le plus petit candidat par rapport à l'opérateur ? (ligne 3). Le candidat traité est supprimé de la liste des candidats (ligne 4). S'il est fréquent et s'il est sous forme canonique (ligne 5), il est ajouté à la liste des solutions (ligne 6) et toutes ses extensions sont ajoutées à la liste des candidats (ligne 7).
Cet algorithme, qui implémente la méthode d'élagage proposée dans la section 4.3.1 est suffisant pour énumérer toutes les solutions mais son espace de recherche est immense. Pour limiter les redondances dans l'ensemble des solutions, nous avons développé un algorithme d'extraction des sous-arbres attribués fermés appelé IMIT_CLOSED reprenant les principes IM IT (D, minSup) 1: C ? {all asubtrees of size 1 in D} 2: while C = ? do 3:
if isCanonical(T ) and f t (T ) ? minSup then exposés dans la section 4.3.3 (figure 3). Comme cela est montré dans la section 5, l'algorithme est coûteux et ne passe pas à l'échelle.
Forts de cette constatation, nous avons développé IMIT_CONTENT_CLOSED, un algorithme permettant l'extraction des sous-arbres attribués c-fermés dont le principe est présenté dans la section 4.3.2. Faute de place, cet algorithme n'est pas présenté ici. Cependant, il peut être facilement déduit de l'algorithme IMIT_CLOSED (figure 3) en remplaçant < par < I et en supprimant les lignes 12 à 14. L'utilisation de la relation < I à la place de < permet de se limiter aux tests d'inclusion d'itemsets qui sont beaucoup moins coûteux que les tests d'isomorphismes. Les lignes 12 à 14 suppriment de l'ensemble des solution trouvées précédemment celles qui contiennent un sous-arbre attribué du candidat actuellement examiné. Ce test n'est pas nécessaire dans le cas de l'extraction de motifs c-fermés. Les expérimentations montrent que ce troisième algorithme constitue le meilleur compromis entre la non redondance des solutions et le temps d'exécution.
Résultats experimentaux
Tous les algorithmes sont implémentés en C++ avec la STL. Les expérimentations ont été effectuées sur un ordinateur avec Ubuntu 12.04 LTS basé sur un processeur Intel c TM i5-2400 @ 3.10GHz avec 8 Gb de mémoire. Tous les temps d'exécution incluent la phase de prétraitement et l'affichage des résultats.
Jeux de données artificiels
Nous avons modifié le programme proposé par Zaki (2002) pour générer des arbres attribués avec différentes tailles d'itemsets. Nous avons utilisé les mêmes paramètres que Zaki (2002) sauf pour le nombre de sous-arbres que nous avons fixé à 10000. Nous avons construit cinq jeux de données en faisant varier la taille des itemsets. Dans T10K, tous les noeuds sont associés à des itemsets de taille 1. Dans T10K-3 et T10K-5, les noeuds sont associés respectivement à des itemsets de taille 3 et 5. Dans T10K-1/10, les noeuds sont associés à des itemsets de taille variant de 1 à 10 alors que dans T10K-1/20, la taille des itemsets varie de 1 à 20.
IM IT _CLOSED(D, minSup)
1: C ? {all asubtrees of size 1 in D} 2: while C = ? do 3:
if isCanonical(T ) and f t (T ) ? minSup then 6:
if ?T ? T : T < T and f t (T ) = f t (T ) and ?T ? X I : f t (x) = f t (T ) then 
Données de journaux d'activité Web
Nous avons élaboré un jeu de données à partir du journal d'activité Web de notre université. Cependant, au lieu d'étiqueter les noeuds avec les URLs des pages consultées, nous leur avons associé un ensemble de mots clés portant sur le contenu des pages. Cette approche permet de capturer les habitudes de navigation des utilisateurs même dans le cas où la structure du site Web change. Le jeu de données est composé de 126 396 arbres annotés avec des itemsets de taille 10 (10 mots clés par page).
Évaluation des performances
La figure 4 montre les temps d'exécution d'IMIT_CONTENT_CLOSED pour la fouille de motifs c-fermés induits et non-ordonnés sur les 5 jeux de données artificiels. A titre de comparaison, nous avons ajouté dans la figure les temps d'exécution de SLEUTH (Zaki, 2004), une implémentation de référence du paradigme d'extension de classes d'équivalence, pour fouiller T10K. IMIT_CONTENT_CLOSED est environ deux fois plus lent que SLEUTH pour toutes les valeurs de support, sauf pour les plus petites pour lesquelles SLEUTH est pénalisé par le coût de la jointure sur des millions de motifs fréquents. Bien qu'IMIT_CONTENT_CLOSED soit en général plus lent que SLEUTH, les résultats sont satisfaisants car notre algorithme est conçu pour fouiller des arbres attribués. Il est normal d'être moins performant que des implé-mentations dédiées à la fouille d'arbres étiquetés.
La figure montre également que la fouille d'arbres attribués demande beaucoup plus de ressources que la fouille d'arbres étiquetés ; et la différence est largement sous-estimée car seuls les motifs c-fermés sont listés. Extraire tous les motifs génère un grand nombre de so-  lutions et prend énormément de temps. Pour donner une idée, la fouille du jeu de données T10K-3 avec un support minimum de 1% génère plus de 12 millions de motifs en 15 heures (figure 5). Rechercher uniquement les sous-arbres c-fermés permet de réduire à la fois le nombre de motifs et le temps d'exécution. Ainsi, avec un support minimum de 1% , 200 motifs c-fermés sont identifiés en 4 secondes. Comme illustré dans la même figure, la recherche de motifs fermés permet de réduire encore le nombre de motifs. Avec un support de 1%, par exemple, le nombre de motifs tombe à 103. Cependant, à cause des tests coûteux d'isomorphisme d'arbres, en contrepartie, les performances s'effondrent lorsque les motifs sont nombreux. La figure 6 montre les temps d'exécution et le nombre de motifs c-fermés trouvés dans les données de journaux d'activité Web. Ce jeu de données réel est plus important que ceux utilisés précédemment et la fouille ne peut être réalisée raisonnablement avec un support inférieur à 10%. La fouille avec un support de 6% retourne 360 motifs en 6 heures.
Conclusion et perspectives
Dans cet article, nous avons introduit le problème de la fouille d'arbres attribués. Nous avons exploré des méthodes permettant d'énumérer tous les motifs fréquents ou seulement les fermés. Ces méthodes se sont révélées inefficaces à cause, dans le premier cas, du nombre Remerciements. Ce travail a été financé par le contrat ANR-2010-COSI-012 FOSTER.

Introduction
Une approche récente de l'analyse exploratoire de données multidimensionnelles consiste en l'extraction d'invariants topologiques (Carlsson, 2009). Ces invariants permettent de caractériser la topologie de la population si l'on suppose qu'elle est définie par une collection de sous-variétés de l'espace R D dont sont issues les données. Les nombres de Betti sont de tels invariants : le premier encode le nombre de composantes connexes, le second le nombre de cycles indépendants, le troisième le nombre de cavités indépendantes, etc... Ces nombres expriment numériquement des caractèristiques topologiques que nous extrayons visuellement lors de l'analyse de nuages de points présentés dans un repère cartésien à deux dimensions. Ces invariants topologiques le sont par homotopie une classe très large de transformations non linéaires incluant les homéomorphismes, les similitudes et les isométries, on s'attend donc à ce que l'informaiton topologique soit plus robuste à la chaîne de mesure que l'information de nature géométrique.
Il existe des techniques d'extraction des nombres de Betti à partir d'un complexe simplicial (de Silva). Un complexe simplicial est un ensemble de simplexes tels que l'intersection de deux simplexes du complexe est soit vide soit fait aussi partie du complexe. Le plongement dans R D d'un k-simplexe est l'enveloppe convexe de (k + 1)-points de cet espace. Un complexe simplicial plongé est donc une collection de variétés qui peut servir de modèle topologique et géométrique à la population génératrice des données. Nous devons lui associer un modèle de densité de probabilité afin de modéliser complètement le processus de génération des données.
Les cartes de Kohonen (Kohonen, 1989) ou leur pendant génératif que sont les Generative Topographic Map (GTM) (Bishop et al., 1998) ainsi que les Generative Principal Manifolds (Tibshirani, 1992) sont des modèles topologique dont la dimension intrinsèque doit être fixée a priori. Les modèles de mélange de gaussiennes classiques (McLachlan et Peel, 2000) permettent de modéliser la densité du nuage de points, mais n'encodent aucune information topologique. Nos précédents travaux sur le Graphe Génératif Gaussien (Aupetit, 2005), que l'on peut voir comme une version générative des Topology Representing Network (TRN) de (Martinetz et Schulten, 1994) 
Hypothèses générales et description du modèle
Les données sont un nuage de points dans R D , supposées issues d'une collection de variétés, perturbées par un bruit centré gaussien isovarié, dont la variance ? 2 est inconnue. Nous supposons que cette collection de variétés peut être approchée par une complexe simplicial de Delaunay dont les sommets w = (w 1 , ..., w N ) appartiennent à R D . Nous définissons le Complexe Simplicial Génératif (CSG) comme la convolution d'un bruit gaussien isovarié à un complexe simplicial de Delaunay de sommets w. Les composants de ce modèle de mélange sont les différents simplexes du complexe : les sommets, les arêtes, les triangles, les tétra-èdres... Les paramètres de ce modèle sont la position des sommets du complexe, la proportion des composants du mélange, et la variance du bruit gaussien. Nous utilisons l'algorithme EM pour trouver le jeu de paramètres qui maximise la vraisemblance, et nous utilisons le critère BIC (Schwarz, 1978) pour sélectionner le modèle de complexité optimale. La complexité du modèle est déterminée par son nombre de sommets, d'arêtes, de triangles... de simplexes que l'on conserve. Optimiser BIC revient donc à retirer certains simplexes du modèle, et donc à sculpter le complexe de Delaunay pour obtenir un sous-complexe de Delaunay BIC-optimal, dont on suppose que la topologie modélise ainsi au mieux celle de la population. Nous dé-taillons le modèle CSG et l'algorithme d'apprentissage.
Le Simplexe Génératif (SG)
Un simplexe génératif (SG) est une fonciton densité de probabilité basée sur un simplexe plongé. Soit S 
Ceci peut être vu comme la convolution d'une densité gaussienne multidimensionnelle et d'un simplexe. Pour former le modèle de mélange nous remplaçons les composants gaussiens classiques, sources ponctuelles de données perturbées par un bruit gaussien, par des sources simpliciales. Le modèle de mélange associé est le Complexe Simplicial Génératif.
Le Complexe Simplicial Génératif (CSG)
Un Complexe Simplicial Génératif est un mélange de simplexes génératifs S = {S 
Expériences
Variétés topologiques connues : le tore et la sphère
Nous testons la technique sur des variétés plongée dans R 3 dont la topologie est connue : une sphère de rayon 1 et un tore de petit rayon 3 et de grand rayon 10. Les nombres de Betti attendus pour la sphère sont (1, 0, 1, 0...), et pour le tore (1, 2, 1, 0...). Le CSG est comparé au Witness Complex (WitC) tel qu'implémenté dans le code Javaplex (Adams et Tausz). Le nombre de prototypes (30 pour la sphère, 40 pour le tore) et leur position sont identiques pour le CSG et le WitC, ils sont obtenus par un modèle de mélange gaussien classique maximisant le critère BIC. La méthode infiniteBarcodes de Javaplex retourne les nombres de Betti recherchés à partir d'une filtration du complexe simplicial.
Pour la sphère, 1000 points sont tirés aléatoirement avec un bruit gaussien centré d'écart-type ? ? {0.05, 0.1, 0.2}. Pour le tore, 2000 points sont tirés aléatoirement avec un bruit gaussien centré d'écart-type ? ? {0.01, 0.05, 0.1}. Dans chaque cas, CSG et WitC sont relancés 100 fois à partir d'une initialisation aléatoire de leurs paramètres. Une réponse est considérée comme correcte uniquement si les nombres de Betti attendus sont trouvés. Le résultat calculé est le pourcenatge de réponses correctes sur les 100 essais.
Base d'images : COIL-100
COIL-100 (Nene et al., 1996) est un corpus d'images de 100 objets différents photographiés en rotation. Chaque image est prise après que l'objet a été tourné de 5 degrés autour d'un axe vertical, il y a donc 72 images par objet. Les images couleurs sont transformées en niveaux de gris et leur taille est réduite. Un objet est représenté par un nuage de 72 points dans l'espace de dimension le nombre de pixels des images. Afin de pouvoir calculer le complexe de Delaunay, nous projetons les données sur les 5 premières composantes principales du nuage de points. Du fait de la rotation complète de l'objet, ce nuage de points à la topologie d'un anneau dont les nombres de Betti sont (1, 1, 0...). Nous testons le CSG sur 5 objets (oignon, flacon, tomate, boîte, chat décoratif ), et nous le lançons 10 fois pour chacun.
Resultats
La sphère et le tore
On peut trouver les résultats obtenus sur la sphère et le tore dans les tableaux 1 et 2. Pour les variances les plus faibles, WitC domine le CSG, mais ils sont tous les deux fiables. Pour ? = 0.2, les performances de CSG diminuent alors que celles de WitC sont stables. Ici, la filtration avantage WitC : la cavité à l'intérieur de la sphère persiste. Alors que pour le CSG, si la variance grandit trop, une corde à l'intérieur de la sphère peut apparaître malgré les différentes étapes d'élagage et ajouter des cycles non désirés dans le modèle.
FIG. 1 -Exemple de 2000 points tirés uniformément à la surface du tore.
FIG. 2 -Cinq objets sélectionnés dans la base de données COIL-100.
Le tore est plus complexe à modéliser que la sphère : si la variance du bruit est trop grande, l'intérieur du tore se remplit. Des cycles et des cavités parasites peuvent apparaître entrainant une baisse de performance que l'on peut observer ici. Alors que les résultats sont toujours acceptables pour le CSG, ceux du WitC ne sont plus suffisants. Les erreurs du WitC se font en général sur le nombre de cycles, alors que le nombre de composantes connexes et de cavités sont corrects.
TAB. 2 -Tore.
COIL-100
La rotation de la boîte, du flacon et du chat décoratif génère pour chacun de ces objets des images nettement différentes les unes des autres, et crée donc pour chaque objet un nuage de points de topologie annulaire. Le CSG retrouve systématiquement les nombres de Betti attendus dans ce cas (1, 1, 0...). En analysant la structure du complexe simplicial, on observe qu'il est constitué de 11 sommets et 11 arêtes formant un cycle. La dimension maximale des simplexes vaut 1 montrant que la projection sur les 5 premières composantes principales n'a pas fait perdre d'information. Pour les deux autres objets, oignon et tomate, le CSG optimal contient seulement 5 sommets et tous les simplexes jusqu'à la dimension 5, avec la signature topologique d'une boule (1, 0, 0...) sans cycle ni cavité. Ce résultat peut s'expliquer par le fait que ces deux objets sont plutôt invariants par rotation autour de l'axe vertical, les 72 images associées à chacun sont donc très similaires et forment un nuage de points compact dans l'espace des pixels, sans structure topologique complexe.

Introduction
La plupart des Systèmes de Recherche d'Information (SRI) utilisent des termes simples pour indexer et retrouver des documents. Cependant, cette représentation n'est pas assez pré-cise pour représenter le contenu des documents et des requêtes, du fait de l'ambigüité des termes isolés de leur contexte : si l'on considère le mot composé « pomme de terre », les mots simples pomme et terre ne gardent pas leur propre sens que dans l'expression « pomme de terre » et si on les utilise séparément ils deviennent une source d'ambigüité.
Une solution à ce problème consiste à utiliser des termes complexes à la place des termes simples isolés (Boulaknadel, 2006). L'hypothèse est que les termes complexes sont plus aptes à désigner des entités sémantiques que les mots simples et constituent alors une meilleure représentation du contenu sémantique des documents (Mitra et al., 1997).
Notre objectif consiste à acquérir des termes complexes représentatifs du contenu informationnel du corpus. Les termes complexes extraits doivent représenter le contenu des textes sous une forme compréhensive par l'ordinateur et riche en information. Ces termes extraits sont utilisés pour effectuer l'indexation de corpus textuels, les termes d'indexation sont alors plus complets et plus précis, ils permettent d'atteindre une meilleure performance pour le SRI.
Les termes complexes peuvent être sélectionnés statistiquement, linguistiquement ou en combinant les deux approches. Les techniques statistiques permettent de découvrir des séries de mots ou de combinaisons de mots qui occurrent fréquemment dans un corpus. Les techniques linguistiques visent à extraire les dépendances ou les relations entre les termes grâce aux phénomènes langagiers.
Dans (Haddad, 2002), l'auteur fait l'indexation des documents et des requêtes après l'analyse linguistique et l'extraction des syntagmes nominaux (SNs). Les résultats des expérimen-tations montrent que l'intégration des SNs dans l'indexation permet d'obtenir de meilleures performances par rapport à l'utilisation des unitermes.
Le et Chevalet (Diem et Chevallet, 2006) utilisent une méthode d'extraction de connaissances hybride qui fusionne l'association entre les paires de termes extraits statistiquement avec les relations sémantique extraites linguistiquement. Les SNs sont organisés en réseaux de dépendance syntaxique (tête et expansion/ modificateur) en ajoutant les associations statistiques et sémantiques. L'information sémantique est étudiée à travers les relations : synonymie, hyperonymie, causalité.
Les auteurs dans (Woods et al., 2000;Haddad, 2003) ont montré que l'indexation avec des SNs extraits linguistiquement affecte plus positivement les résultats d'un SRI que celle avec des groupes de mots extraits statistiquement.
Méthodologie suivie
L'approche hybride d'extraction de connaissances montre son efficacité dans l'augmentation de la performance des SRIs. Nous choisissons alors de combiner entre une approche linguistique basée sur l'extraction des syntagmes nominaux (SNs) et sur un filtrage statistique basé sur l'information mutuelle (IM) car cette mesure est adaptée aux termes rares (Daille, 1994;Thanopoulos et al., 2002) ce qui est le cas des SNs, pour la représentation du contenu textuel du corpus.
Extraction des syntagmes nominaux
Nous effectuons, d'abord l'analyse linguistique avec un étiqueteur, qui génère une collection étiquetée. Ensuite, on utilise cette collection étiquetée et on en extrait un ensemble de SNs. Les syntagmes nominaux candidats sont extraits par repérage de patrons syntaxiques. Nous adoptons la définition des patrons syntaxiques dans (Haddad, 2002), où un patron syntaxique est une règle sur l'ordre d'enchainement des catégories grammaticales qui forment un SN :
-V : le vocabulaire extrait du corpus -C : un ensemble de catégories lexicales -L : le lexique ? V × C Un patron syntaxique est une règle de la forme :
..Y n Avec Yi ? C et X un syntagme nominal, exemples : SUBC ADJQ : « échelle planétaire », « relation diplomatique », etc. Nous nous basons dans nos travaux sur les 10 patrons syntaxiques les plus susceptibles de contenir le maximum d'informations (Haddad, 2002), alors on ne va étudier que les SNs composés de deux ou de trois termes.
Sélection des meilleurs descripteurs
Bien que les SNs soient généralement pertinents, il peut être nécessaire de n'en sélectionner que les « meilleurs » du corpus, pour cela nous utilisons un filtrage statistique qui consiste à employer une mesure statistique afin de leurs donner un score de qualité. Pour notre cas, ce filtrage statistique est effectué en calculant l'information mutuelle (IM) entre les composants de chaque SN et en fixant différents seuils de cette mesure pour ne conserver que les SNs plus pertinent pour l'indexation. La mesure de l'information mutuelle consiste à comparer la probabilité d'apparition des cooccurrences de mots (m1, m2) à la probabilité d'apparition de ces mots séparément. Cette mesure est donnée par (Church et Hanks, 1990) :
Où P est la probabilté. Dans notre cas l'IM est utilisée pour détecter les syntagmes nominaux les plus pertinents pour les utiliser ensuite dans notre processus de RI. Donc pour les syntagmes composés de deux mots X := Y 1 Y 2 Avec Yi ? C et X un SN, l'IM va être calculée de la façon suivante :
Pour les SNs composés de trois mots X := Y 1 Y 2 Y 3 , l'IM va être calculée de la façon suivante : Si Y 2 est une préposition, alors :
Sinon
Où P(Y i ) est une estimation de la probabilité d'apparition du mot Y i qui est calculée à partir de la fréquence d'apparition du mot Y i dans le document où il apparait, normalisée par N le nombre de mots contenu dans le même document. P(Y i , Y j ) est une estimation de la probabilité que les deux mots apparaissent ensemble dans le même document. Cette probabilité est estimée par la fréquence d'apparition du couple (Y i , Y j ) divisé par N.
Expérimentations et résultats
Pour tester notre approche hybride d'extraction de SNs, nous avons utilisé le SRI Lemur 1 , le modèle vectoriel et la mesure de pondération tf.idf (Salton et Yang, 1973). Nous enlevons par la suite les mots vides lors de l'indexation et cela avec le SRI Lemur. Pour évaluer notre approche d'indexation, on se focalise sur la précision à faible taux de rappel et cela en étudiant la précision à 3, 5 et 10 documents (P_3, P_5, P_10), la précision à 11 point de rappel (11 pt_avg) et la F-mesure sont aussi étudiés.
Pour ces expérimentations le corpus initial sera noté Corpus-I, le corpus qui contient les SNs sera noté Corpus-SN et le corpus qui contient des SNs après filtrage sera noté Corpus-Fi
Résultats du filtrage avec l'IM
Selon les expériences on a déterminé que le score de l'information mutuelle pour les SNs varie entre -2 et 12. Nous varions alors S_Min et S_Max entre cet intervalle pour déterminer les meilleurs paramètres pour le filtrage statistique.
Après analyse du Tableau 3.1, nous remarquons que l'approche de filtrage selon l'IM a permis l'amélioration de la P_3 lorsque S_Max= 7, 8 et 9 avec une amélioration de 1.4% par rapport au Corpus-I. Nous remarquons aussi l'amélioration de la P_5 10, et pour S_Min= 2 et 6, la meilleure amélioration est de 2.1% pour S_Max= 8 par rapport au Corpus-Fi. Pour la P_10, trois stratégies ont permis l'amélioration de cette mesure avec S_Max= 4, 6 et 7, la stratégie S_Max= 6 a permis une amélioration de 0.2% par rapport au Corpus-I. L'analyse de la 11 pt_avg, montre que cinq stratégies du Corpus-Fi ont permis l'améliora-tion de cette mesure lorsque S_Max= 2, 4, 6, 7, 8 et 10, la meilleure amélioration est obtenue pour S_Max= 8, ce qui a permis d'augmenter cette mesure de 4.6% par rapport au Corpus-Fi.
Pour la F-mesure le meilleur score est obtenu pour le Corpus-Fi lorsque S_Max= 8 avec une augmentation de 2% par rapport au Corpus-I et de 3.2% par rapport au Corpus-SN.
Nous remarquons que pour la 11 pt_avg la meilleure performance est obtenue par le Corpus-Fi pour S_Max=8, c'est à dire que les SNs les plus pertinents ont un score d'IM infé-rieur à 8. Nous remarquons aussi que les résultats obtenus par le Corpus-SN sont inférieurs à ceux obtenus par le Corpus-I, cela peut être expliquer par le fait que les SNs sélectionnés ne sont pas tous de bonne qualité.
Conclusion
Cet article présente notre méthode d'indexation basée sur la sélection et le filtrage des SNs. Nous avons opté pour une méthode hybride d'extraction des connaissances, qui combine à la fois l'analyse linguistique fondée sur l'extraction des SNs et l'analyse statistique. Les SNs candidats sont extraits d'un corpus étiqueté par repérage de patrons syntaxique. Nous procédons par la suite à un filtrage statistique basé sur l'IM pour ne sélectionner que les SNs les plus pertinents.
Les résultats ont montré que cette méthode permet d'améliorer les performances de notre SRI et que la meilleure performance est obtenue pour le cas où S_Max= 8, malgré que les SNs étaient de mauvaise qualité car la stratégie Corpus-SN qui utilise les SNs a dégradé les performances du SRI.
Pour les travaux futurs, c'est le processus de filtrage qui sera mis en question, que ce soit pour le filtrage linguistique qui permettra d'extraire des SNs de meilleurs qualités, que ce soit pour le filtrage statistique.

Introduction
L'extraction de règles d'association, consistant à découvrir des associations entre les conjonctions de variables binaires (ou motifs) d'une base de données, est une tâche importante en fouille de données. La recherche d'algorithmes efficaces de telles règles a été un problème majeur de cette communauté. Depuis le célèbre algorithme Apriori (Agrawal et Srikant, 1994), il y a eu de nombreuses variantes et améliorations. L'importance de l'extraction des règles négatives fut mise en évidence par (Brin et al., 1997) qui indiquent que de la connaissance précieuse peut se cacher dans ces règles. Ainsi (Brin et al., 1997) utilisent le test du ? 2 pour déterminer la dépendance entre deux motifs et ensuite une mesure de corrélation afin de trouver la nature de cette dépendance (positive ou négative). (Savasere et al., 1998) combinent les motifs fréquents 1 positifs avec la connaissance du domaine afin de détecter les associations négatives. Cette approche est difficile à généraliser puisqu'elle dépend de la connaissance du domaine. (Boulicaut et al., 2000) recherchent deux types de règles négatives, les règles du type X ? Y ? Z et X ? Y ? Z, et pour cela ils proposent une approche basée sur les contraintes. (Teng et al., 2002) proposent un algorithme détectant uniquement les règles négatives du type X ? Y . Quant à , (Antonie et Zaïane, 2004) et (Cornelis et al., 2006), ils extraient des règles négatives grâce à un algorithme basé sur l'algorithme fondateur Apriori (Agrawal et Srikant, 1994). ) utilisent en plus du couple de mesures (support 2 , confiance 3 ), les deux mesures suivantes : une mesure d'intérêt qui n'est autre que la valeur absolue de la nouveauté (Lavrac et al., 1999) et une mesure nommée ratio incrément de la probabilité conditionnelle qui n'est autre que la mesure de Shortliffe (Shortliffe, 1976). Quant à (Antonie et Zaïane, 2004), ils utilisent comme mesure supplémentaire, le coefficient de corrélation (Pearson, 1896). Nous nous sommes focalisés dans cet article sur les techniques basées sur l'algorithme pionnier Apriori, et plus particulièrement sur les travaux de , (Antonie et Zaïane, 2004) et (Cornelis et al., 2006). A l'issue d'une étude approfondie de chacune des trois techniques, nous avons mis en évidence essentiellement les deux failles suivantes : (1) un nombre encore trop important de règles inintéressantes et (2) un parcours de recherche des règles non optimisé. Pour remédier au premier problème (nombre important de règles inintéressantes), nous retenons un sous-ensemble de motifs fréquents, les motifs raisonnablement fréquents, en éliminant ceux qui vont conduire à des règles non pertinentes c'est-à-dire les règles éliminées par toute mesure d'intérêt évaluant l'écart à l'indépendance de la règle comme par exemple la mesure de Piatetsky-Shapiro (Piatetsky-Shapiro, 1991). L'avantage de ce choix est que l'élimination intervient dans la première phase de l'algorithme et non plus en deuxième phase (i.e. l'extraction des règles) ou dans une phase de post-traitement des règles. De plus, nous utilisons également une mesure supplémentaire au couple de mesures (support, confiance) pour sélectionner les règles valides 4 , la mesure M G (Guillaume, 2010) qui est une amélioration de la mesure de Shortliffe (Shortliffe, 1976) utilisée par , et qui évalue non seulement l'écart de la règle par rapport à l'indépendance 5 mais également par rapport au point d'équilibre 6 (Blanchard et al., 2005). L'intérêt de prendre en compte le point d'équilibre est développé dans (Blanchard et al., 2005). Cette mesure plus sélective que celle utilisée dans  va permettre d'éliminer une nouvelle caté-gorie de règles inintéressantes. Pour remédier au deuxième problème (parcours de recherche des règles non optimisé), nous démontrons que seulement la moitié des règles négatives potentiellement valides sont à étudier, et ceci en fonction de la valeur de la confiance de la règle positive par rapport au support de la conclusion. De plus, parmi les 4 règles à étudier, nous avons de nouveau utilisé la propriété d'anti-monotonicité de la confiance 7 , propriété abandonnée par (Antonie et Zaïane, 2004) et , à laquelle nous en avons ajouté une nouvelle dégagée par (Guillaume et Papon, 2012) et qui repose sur la mesure que nous allons utiliser, la mesure M G . L'article s'organise donc de la façon suivante. La section 2 présente et motive les choix retenus pour optimiser l'extraction des règles d'association positives et négatives. La section 3 développe l'algorithme proposé et la section 4 évalue notre technique sur plusieurs bases de données. L'article se termine par une conclusion et des perspectives. 
6. Le point d'équilibre est le cas où lorsque X est réalisé, il y a autant de chances de voir se réaliser Y que Y , ainsi nous avons les relations suivantes : 
sup(M2) ? 1 sans pour autant être pertinentes comme le montre la valeur de la nouveauté (Lavrac et al., 1999)  
Parcours optimisé pour la recherche des règles valides
Aucune technique d'élagage pour le parcours des règles n'est utilisée par (Antonie et Zaïane, 2004), (Cornelis et al., 2006) et . En effet la propriété d'antimonotonicité de la confiance n'est valable que pour les règles positives. Cependant, il est possible de restreindre et de diviser par 2 le nombre de règles négatives à étudier en fonction (1) soit du signe de la nouveauté ; (2) soit de la réponse à la question suivante "la réalisation de la prémisse augmente-t-elle les chances d'apparition de la conclusion ?". La réponse à cette question peut être obtenue grâce à la nouveauté puisque sup
8. On entend par motif omniprésent, un motif ayant une très forte valeur pour son support.
] ou grâce à la mesure de Shortliffe, propriété non exploitée par ) malgré une utilisation de cette mesure. Nous explicitons cette restriction du nombre de règles négatives à évaluer grâce aux liens suivants entre les différentes règles.
Nous avons le lien suivant entre les règles antinomiques X ? Y et X ? Y : si la règle X ? Y est potentiellement intéressante c'est-à-dire si la réalisation de X augmente les chances d'apparition de Y (question précédente) ou encore si la confiance de la règle est supérieure à la probabilité d'apparition du motif conclusion Y (autrement dit si conf (X ? Y ) > P (Y ) ), alors la règle antinomique X ? Y ne pourra pas être intéres-sante puisque dans ce cas-là, la confiance de la règle antinomique est inférieure à la proba-
9 . De la même façon, nous avons le lien suivant entre les règles
10 . Pour finir, nous avons le lien suivant entre les règles symétriques
11 . De ces trois liaisons précédemment établies entre les règles, nous pouvons en déduire que si la règle X ? Y est potentiellement intéressante alors les règles  (Antonie et Zaïane, 2004) considèrent ensuite le couple de motifs (Y, X) et refont le calcul du coefficient de corrélation
10.
Nous allons utiliser ce résultat des liaisons d'intérêt entre les règles négatives afin de diminuer le nombre de règles à évaluer en le divisant par 2. Pour cela, nous devons savoir si la confiance de la règle X ? Y est supérieure au support de la conclusion (c'est-à-dire si conf (X ? Y ) > sup(Y )), ce qui nous garantit d'obtenir une règle potentiellement intéressante.  vérifient également le potentiel intérêt des règles grâce à la valeur absolue de la nouveauté. Autrement dit, avant de tester la validité des règles au regard du support et de la confiance,  
Ainsi, la recherche de l'appartenance de la règle à cette zone où conf (X ? Y ) > sup(Y ) avant de tester la contrainte de la confiance nous assure d'éliminer une partie des règles inintéressantes. C'est ce que nous allons retenir dans notre proposition pour répondre non seulement à une exigence de rapidité d'exécution de l'algorithme (moins de règles à évaluer puisque nous divisons ce nombre par 2) mais également au problème du nombre important de règles restituées, règles pas toujours pertinentes (les règles inintéressantes qui sont éliminées sont celles où la prémisse n'augmente pas les chances d'apparition de la conclusion). Cependant cette zone où les règles sont potentiellement intéressantes est encore trop importante et peut générer encore des règles inintéressantes. C'est le cas où la confiance de la règle est bien supérieure au support de la conclusion mais également inférieure au point d'équilibre (Blanchard et al., 2005)  
) . Maintenant, nous devons optimiser le parcours des règles dans le cas répulsif et par conséquent, utiliser une méta-règle permettant de passer de la règle X ? Y à la règle X ? Y . Comme (Guillaume et Papon, 2012) ont dégagé des méta-règles uniquement à partir des règles positives X ? Y , pour pouvoir faire cette transition des règles X ? Y aux règles X ? Y , nous allons utiliser la méta-règle permettant de passer de la règle X ? Y à la règle X ? Y , et par conséquent, celle que nous venons de décrire précédemment. Nous résumons les deux méta-règles qui vont être utilisées pour optimiser la recherche des règles :
Pour finir, les algorithmes existants reposant sur le couple (support, confiance) extraient des règles du type Lors de la recherche des motifs raisonnablement fréquents, nous allons rechercher en même temps ces conjonctions de motifs négatifs, motifs que nous noterons¨Xnoterons¨ noterons¨X. Cette recherche simultanée va renforcer notre souhait d'extraire des règles les plus pertinentes possibles. En effet, la contrainte supplémentaire suivante sup( ¨ X) ? min¨supmin¨ min¨sup sur les motifs X impose, comme pour la deuxième contrainte des motifs raisonnablement fréquents (à savoir sup(X) ? max sup ), d'être en présence de motifs X non omniprésents. Pour un seuil d'exigence identique (c'est-à-dire min sup = min¨supmin¨ min¨sup et max sup = 1 ? min sup ), cette nouvelle contrainte est plus restrictive que la deuxième contrainte (à savoir sup(X) ? max sup ) puisque si nous avons sup(X) ? max sup , alors nous avons les équivalences suivantes : 
FIG. 1 -Exemple de règles où les motifs ont des supports relativement élevés (courbe de gauche) et où les motifs ont des supports proches du seuil minimal min sup (courbe de droite).
La contingence des ensembles X ei?BD 14 , Y ei?BD et (X ei?BD ? Y ei?BD ) est la même pour les deux courbes sauf pour l'ensemble (X ei?BD ? Y ei?BD ) qui est plus faible pour la courbe de gauche. Comme la contingence des ensembles X ei?BD et (X ei?BD ? Y ei?BD ) est la même dans les deux cas de figure, les deux règles X ? Y associées à ces deux contingences ont la même valeur pour la confiance. Cependant la règle associée à la courbe de droite de la Figure 1 est plus pertinente que celle de la courbe de gauche puisque la probabilité d'avoir une intersection aussi importante entre X ei?BD et Y ei?BD est plus faible que pour le cas de la courbe de gauche. Nous savons que la confiance ne peut pas discerner ces deux types de règles et l'ajout de cette nouvelle contrainte sur les motifs¨Xmotifs¨ motifs¨X nous assure d'éliminer un certain type de règles non pertinentes. Nous n'ajouterons pas, comme pour les motifs positifs, une valeur maximale à ne pas dépasser sur les supports des motifs¨Xmotifs¨ motifs¨X car elle est en partie présente avec la contrainte du support minimum sur les motifs positifs. Nous présentons maintenant notre algorithme.
Algorithme
Tout d'abord, nous définissons ce que nous entendons par règle valide et donc les 6 contraintes Ct 1 à Ct 6 que doivent vérifier les règles. 
else if attraction négative then 10: 
, et telle que Ct 6 : C 1 ? C 2 est minimal au regard des motifs négatifs raisonnablement fréquents X ou Y . La contrainte Ct 6 est celle présente dans (Cornelis et al., 2006) où les motifs C 1 et C 2 lorsqu'ils sont des motifs raisonnablement fréquents négatifs X et Y , doivent également être minimaux c'est-à-dire qu'il n'existe pas par exemple pour le motif X un sous-ensemble X ? ? X tel que X ? soit également raisonnablement fréquent.
L'algorithme d'extraction des RAPN (voir l'algorithme 1) commence par rechercher les motifs raisonnablement fréquents grâce à la fonction funct_RF (ligne 1). Cette recherche est similaire à celle utilisée par (Agrawal et Srikant, 1994) pour générer les motifs fréquents en rajoutant deux contraintes supplémentaires : un seuil maximal max sup qui ne doit pas être dépassé par le support de X et un seuil minimum min¨supmin¨ min¨sup pour le support des motifs¨Xmotifs¨ motifs¨X, ce qui permet de vérifier les contraintes Ct 1 et Ct 2 des règles valides définies dans cette même section. A partir des motifs raisonnablement fréquents, on va rechercher les motifs négatifs raisonnablement fréquents minimaux grâce à la fonction func_NRFM (ligne 2). Cette recherche sert ensuite à s'assurer que la règle vérifie la contrainte Ct 6 . Cette fonction est similaire à celle exposée dans (Cornelis et al., 2006) en rajoutant la contrainte du support maximum (i.e. 
Expérimentations
Les quatre algorithmes ont été développés en Java et incorporés au logiciel libre WEKA (Waikato Environment for Knowledge Analysis) (Witten et Frank, 2005). Les expérimentations ont été effectuées sur les 4 bases de données UCI KDD (Hettich et Bay, 1999)  . Tout d'abord, nous avons effectué une étude comparative des 4 algorithmes sur la base de données Abalone dont les résultats sont résumés dans la figure 2 et où nous avons fait varier les valeurs du seuil minimum pour le support et la confiance comme indiqué dans les deux premières colonnes du tableau. Pour chacun des algorithmes, nous avons restitué le temps d'exécution total en secondes (colonne Temps) et le nombre total de règles négatives extraites (colonne # Négatives). Par manque de place, nous n'avons pas fait resortir le nombre de règles positives. De plus pour notre algorithme, nous avons restitué dans la dernière colonne (colonne # Nouvelles R.) le nombre de règles extraites du typë X ? ¨ Y , règles non présentes dans les 3 algorithmes existants. Pour finir, nous avons retenu comme seuil minimum pour le coefficient de corrélation nécessaire pour l'algorithme de (Antonie et Zaïane, 2004), la valeur 0,60 et la valeur 0,10 pour la mesure d'intérêt utilisée dans . La mesure de Shortliffe utilisée dans ) a le même seuil que celui de la confiance. Quant à notre algorithme, nous avons retenu les valeurs suivantes pour les différents seuils : max sup = 0, 80, min¨supmin¨ min¨sup = min sup et min MG = 0, 60.
FIG. 2 -Etude comparative des 4 algorithmes sur la base Abalone.
Nous remarquons que c'est notre algorithme qui restitue, pour tous les cas de figure de cette base Abalone, le nombre le plus faible de règles négatives. Nous observons également que le nombre de règles du typë X ? ¨ Y restituées par uniquement notre algorithme est conséquent mais reste globalement inférieur au nombre de règles restituées par (Cornelis et al., 2006). Quant au temps d'extraction, notre algorithme arrive en première place suivi de (Cornelis et al., 2006), (Antonie et Zaïane, 2004) et .
FIG. 3 -Résultats de notre algorithme sur les 4 bases de données.
La deuxième étude réalisée s'est concentrée sur notre algorithme et nous avons souhaité connaître les temps d'extraction (colonne Temps) et le nombre de règles négatives (colonne # Négatif ) extraites sur différentes bases de données UCI, et cela pour différents seuils pour le support et la confiance comme indiqué dans la figure 3. Les autres seuils nécessaires pour notre algorithme sont les mêmes que pour l'étude précédente. Nous constatons des temps d'extraction raisonnables et le nombre de règles négatives extraites est raisonnable en général sauf pour ce nouveau type de règles¨Xrègles¨ règles¨X ? ¨ Y où une étude complémentaire est nécessaire afin de ne retenir que les plus pertinentes.
Conclusion
Dans cet article, nous avons proposé un algorithme d'extraction de RAPN optimisé par rapport à ceux présents dans la littérature et reposant sur l'algorithme fondateur Apriori. Les deux optimisations ont porté sur une diminution du nombre de règles et sur un parcours optimisé de recherche des règles valides. La diminution du nombre de règles a été rendue possible en éliminant certains motifs fréquents qui ne pouvaient pas conduire à des règles intéressantes car ayant soit une valeur pour la confiance trop faible, soit un écart à l'indépendance trop faible.

Introduction
De nombreux acteurs de l'informatique doivent faire face à l'arrivée massive de données. Les plus connus sont Google et Yahoo avec le traitement des logs pour la publicité en-ligne, Facebook et Twitter qui modélisent les données provenant de leurs centaines de millions d'utilisateurs, les opérateurs téléphoniques pour la gestion de réseaux de télécommunications. La volumétrie de ces données continue de croître rapidement et les quantités ne sont plus compatibles avec l'utilisation de la plupart des méthodes hors-ligne qui supposent de pouvoir accéder à toutes les données. Dans ces conditions, il est préférable de traiter les données à leur passage ce qui impose d'y accéder une seule fois et dans leur ordre d'arrivée. On parle alors d'un accès sous la forme d'un flux de données.
En classification supervisée, on appelle concept P (C|X) la probabilité conditionnelle de la classe C connaissant les données X. Les flux de données peuvent ne pas être stationnaires et comporter des changements de concept si le processus qui génère les données varie au cours du temps. Dans ce cas le modèle de classification supervisée doit être adapté au fur et à mesure que le concept change.
Cet article propose une nouvelle méthode de détection de changement basée sur l'observation des données du flux. Notre méthode utilise deux fenêtres et permet d'identifier si les données de ces deux fenêtres proviennent ou non de la même distribution. Elle est capable de détecter les changements de diverses natures (moyenne, variance...) sur la distribution des données conditionnellement ou non aux classes. Notre méthode a l'intérêt de n'avoir aucun a priori sur la distribution des données, ni sur le type de changement. De plus, à part la taille de la fenêtre, elle ne requiert aucun paramètre utilisateur.
Cet article présente tout d'abord, dans la section 2, les approches existantes de l'état de l'art auxquelles l'approche proposée sera comparée. La section 3 présente notre méthode de détec-tion ainsi qu'une validation expérimentale sur des données artificielles. La section 4 montre comment notre méthode de détection peut être utilisée au sein d'une méthode de gestion de la dérive de concept. La dernière partie conclut cet article.
2 Etat de l'art des méthodes de détection L'état de l'art sur la gestion de la dérive de concept est abondant (Bifet et al., 2009;Žlio-baite, 2010). Les méthodes de gestion de la dérive peuvent se diviser en plusieurs familles : détection de changement, ensemble de classifieurs, pondération des données selon leur ancienneté... Le but de cet article étant de présenter une nouvelle méthode de détection, l'état de l'art présenté ici se focalise donc sur la famille des méthodes de détection de changement.
Le but de la classification supervisée sur flux de données est d'optimiser les performances du classifieur. Dans ce cas, l'état de l'art peut encore être divisé en deux familles :
-méthodes sans classifieur : les auteurs s'intéressent directement aux distributions d'inté-rêt du flux de données : P (X), P (C), P (X|C) -méthodes avec classifieur : les auteurs s'intéressent à la performance, liée à P (C|X), d'un classifieur. D'un point de vue bayésien cela revient à détecter les variations d'une des quantités de la formule suivante : P (C|X) = (P (C)P (X|C)) /P (X) avec :
-P (C) la proportion des classes dans les données -P (X) la probabilité des données -P (X|C) la probabilité conditionnelle des X connaissant la classe C. A notre connaissance il n'existe pas de méthodes permettant de détecter directement les variations dans la distribution jointe P (X, C).
La méthode proposée dans cet article ayant vocation à détecter les changements sur les trois termes mentionnés ci-dessus, on présente dans les sous sections suivantes une brève description des méthodes de l'état de l'art testées de façon comparative dans cet article.
Méthodes sans classifieur
Les méthodes sans classifieur sont essentiellement des méthodes basées sur des tests statistiques qui sont utilisés pour détecter un changement entre différentes fenêtres d'observations. 
. On utilise le test t de Welch pour tester l'hypothèse nulle suivante : « les moyennes de deux populations sont égales ». Ce test retourne une p-value qui permet de rejeter ou non l'hypothèse nulle.
Test de Kolmogorov-Smirnov : Le test d'hypothèse de Kolmogorov-Smirnov est utilisé pour déterminer si un échantillon suit bien une loi donnée ou bien si deux échantillons suivent la même loi. Ce test est basé sur les propriétés des fonctions de répartition empirique. Nous utiliserons ce test pour vérifier si deux échantillons suivent la même loi. Soient deux échan-tillons de tailles N 1 et N 2 possédant respectivement les fonctions de répartition empirique F 1 (x) et F 2 (x). La distance de Kolmogorov-Smirnov est définie de la manière suivante :
L'hypothèse nulle, stipulant que les deux échantillons proviennent de la même distribution, est rejetée avec une confiance ? si :
K ? se retrouve à l'aide des tables de Kolmogorov-Smirnov.
Cette méthode a été proposée par (Bondu et Boullé, 2011)   (Boullé, 2006) appliquée à ces exemples. Si la variable X i est discrétisée en au moins 2 intervalles cela signifie qu'il y a au moins 2 zones où la distribution des exemples conditionnellement à la fenêtre W est significativement différente. Dans ce cas la méthode conclut qu'un changement s'est produit.
Méthodes avec classifieur
Les méthodes avec classifieur observent les performances du classifieur et détectent un changement quand les performances varient de manière significative. Ces méthodes ont comme hypothèses de départ que le classifieur est un processus stationnaire et que les données sont indépendantes et identiquement distribuées (iid). Bien que ces hypothèses ne soient pas toujours validées, ces méthodes ont prouvé leur intérêt sur diverses expérimentations (Gama et al., 2004;Baena-García et al., 2006;Bifet et al., 2009). Les deux principales méthodes avec classifieur référencées dans l'état de l'art sont décrites ci-dessous. DDM : La méthode DDM, proposée par Gama et al. (Gama et al., 2004), détecte les changements en observant l'évolution du taux d'erreur du classifieur. L'algorithme prend en entrée une distribution binomiale provenant de la variable binaire qui indique si l'exemple est bien classé (0) ou mal classé (1) par le classifieur. Cette loi binomiale est approximée par une loi normale après avoir vu 30 exemples. La méthode estime pour chaque exemple la probabilité qu'il soit mal classé 
Pour les expérimentations les paramètres sont fixés à ? = 90% et ? = 95%. Sur des jeux de données synthétiques et réels EDDM détecte plus rapidement que DDM les changements graduels.
3 Une nouvelle méthode de détection supervisée
Présentation
La méthode proposée dans cet article pour détecter les changements de concept s'inspire de la méthode MODL P (W |X i ) (Bondu et Boullé, 2011) présentée dans la section 2.1. Le flux de données est constitué de d variables explicatives (X i , i ? {1, ..., d}). Notre méthode fait l'hypothèse d'indépendance des variables conditionnellement aux classes. Le test de changement est donc réalisé par variable X i sur les données provenant de deux fenêtres. La première fenêtre W ref contient les données du concept de départ. La deuxième W cur est une fenêtre glissante/sautante sur le flux qui permet de capturer les données d'un éventuel nouveau concept. Le choix de la taille des fenêtres est un compromis entre la réactivité aux changements et le nombre de mauvaises détections. Une grande taille de fenêtre permet de détecter avec plus de confiance des motifs potentiellement plus complexes. Une plus petite taille permet d'être plus réactif. Fixer cette taille dépend du flux observé et du type de changements que l'on veut dé-tecter. De manière générale on peut traiter ce problème en utilisant plusieurs tailles de fenêtre en parallèle.
Les données sont étiquetées par fenêtre : W ? {W ref , W cur }. Notre intérêt porte sur la détection de la dérive du concept dans le cadre de la classification supervisée. Par conséquent on s'intéresse à la probabilité de la fenêtre connaissant à la fois la classe C et la variable
Parmi les méthode de l'état l'art de discrétisation/groupage bivarié capables de prendre en compte la variable X i et la classe C nous avons choisi d'utiliser l'approche MODL (Boullé, 2009) pour ses caractéristiques : pas d'a priori sur la distribution des données, faible sensibilité aux valeurs atypiques, pas de paramètres utilisateur, régularisation pour éviter le surapprentissage.
Un changement est détecté quand la variable à expliquer (dans notre cas W ) peut être discriminée à partir de l'observation de X i et C. Ceci se traduit dans l'approche MODL bivariée par l'observation d'une grille de discrétisation/groupage de plus d'une cellule. Si la grille n'a qu'une seule cellule, alors la distribution des données n'a pas changé entre les deux fenêtres.
La complexité algorithmique de cette méthode est en O(T ? T log(T )) où T est la somme de la taille des deux fenêtres (référence et courante).
Validation expérimentale : détection sans classifieur
Dans la suite de cette section on s'intéresse au comportement de la méthode proposée sur des données artificielles provenant de la simulation de différents types de changement. Le comportement dans le cas stationnaire est étudié dans la section 3.2.1, la capacité à détecter un changement dans la section 3.2.2 et la capacité à détecter différents types de changement dans la section 3.2.3. Les expérimentations comparent les méthodes de l'état de l'art présentées précédemment ainsi que notre approche : -test t de Welch avec signification statistique de 1%, 5% et 10% -test de Kolmogorov-Smirnov (KS) avec signification statistique de 1%, 5% et 10% -méthode supervisée MODL P (W |X i ) -méthode supervisée bivariée MODL P (W |X i , C)
Comportement dans le cas stationnaire
Le but de cette première expérimentation est d'étudier le comportement des méthodes dans le cadre d'un flux stationnaire où l'on ne doit détecter aucun changement. Les fenêtres de ré-férence et courante ont la même taille. Etant donné que la méthode fait l'hypothèse d'indépen-dance des variables, une seule variable numérique X a est utilisée pour la validation expérimen-tale. La distribution des classes se fait selon une gaussienne de paramètres (µ 1 = ?1, ? 1 = 1) pour la classe 1 et une gaussienne de paramètres (µ 2 = 1, ? 2 = 1) pour la classe 2. Diffé-rentes tailles de fenêtre ont été choisies entre 10 et 5 000 exemples. Les expérimentations sont réalisées 1000 fois.
Les résultats sont présentés dans le tableau 1. Ceux-ci confirment la robustesse de la méthode bivariée MODL car aucune détection n'est observée sur P (W |X a , C). Par contre quelques fausses alarmes ont lieu sur P (W |X a ) (Bondu et Boullé, 2011)   TAB. 1 -Nombre de mauvaises détections selon la méthode détection et la taille de la fenêtre pour 1000 expérimentations.
Grille bivariée pour la détection de changement
Capacité à détecter un changement
Le but de cette expérimentation est d'observer le temps nécessaire (en nombre d'exemples) pour détecter un changement en fonction de la taille de la fenêtre et des méthodes. La plupart des méthodes sont capable de détecter ce type de changement. Le point important est ici d'analyser la vitesse de détection.
Les fenêtres de référence et courante ont la même taille. Une seule variable X a est utilisée. Le concept 1 est défini ainsi :
-la classe 0 suit une distribution N 0 (µ = ?1, ? = 1) ; -la classe 1 suit une distribution N 1 (µ = 1, ? = 1). On simule ce changement en changeant la moyenne de la classe 0, qui passe de -1 à 2 et sa variance qui passe de 1 à 0,5. Pour la classe 1, seule la moyenne change en passant de 1 à 0. On obtient donc le concept 2 suivant :
-la classe 0 suit une distribution N 0 (µ = 2, ? = 0, 5) ; -la classe 1 suit une distribution N 1 (µ = 0, ? = 1). Différentes tailles de fenêtres ont été testées entre 10 et 5 000 exemples et les expérimentations sont réalisées 1000 fois. La position du changement dans la fenêtre est tirée de manière aléa-toire car dans un cas réel la position du changement n'est pas connue. Les résultats obtenus correspondent aux délais moyens de détection du changement en fonction de la taille de la fenêtre et des méthodes étudiées.
Les résultats sont présentés dans le tableau 2. On observe que jusqu'à une taille de fenêtre de 100 exemples la détection est difficile dans la fenêtre où a lieu le changement. A partir d'une taille de 200 exemples, le délai moyen est inférieur à la taille de la fenêtre et ce pour toutes les méthodes. L'augmentation du seuil de signification à 5% et 10% entraine, pour les tests de Welch et de Kolmogorov-Smirnov, une baisse du délai de détection. La méthode basée sur MODL P (W |X a ) est légèrement plus longue à détecter que les méthodes paramétriques à 1%. Cependant, si on prend notre méthode MODL P (W |X a , C) alors celle-ci est meilleure que les deux autres méthodes paramétrées à 1%. 15  14  14  15  15  15  15  15  20  29  26  24  29  28  26  28  29  30  41  36  33  43  40  36  41  40  50  62  53  49  67  57  54  65  58  100  103  90  86  110  96  90  109  96  200  178  160  150  185  166  160  195  163  300  241  218  211  252  227  214  269  218  500  367  337  321  375  344  330  404  325  1000  665  620  598  678  636  613  719  600  2000  1224  1188  1154  1260  1188  1174  1334  1120  5000  2886  2766  2741  2911  2781  2756  3081  2671 TAB. 2 -Délai moyen de détection d'un changement selon la méthode de détection et la taille de la fenêtre pour 1000 expérimentations.
Différents types de changements détectés
Les expériences de cette section ont pour but d'observer quels types de changement (moyenne, variance, inversion des classes) les différentes méthodes sont capables de détecter. Pour tous les types de changement testés, le concept de départ est le concept 1 défini ainsi : la classe 0 suit une distribution N 0 (µ = 0, ? = 0, 5) et la classe 1 suit une distribution N 1 (µ = 2, ? = 1). Différents changements sont appliqués au concept 1 pour expérimenter le comportement des différentes méthodes.
-Changement de moyenne. On simule ce changement en changeant la moyenne de la classe 0, qui passe de 0 à 1. On obtient le concept 2 : la classe 0 suit une distribution N 0 (µ = 1, ? = 0, 5) et la classe 1 suit une distribution N 1 (µ = 2, ? = 1). -Changement de variance. On simule ce changement en changeant la variance de la classe 0, qui passe de 1 à 0,5. On obtient le concept 2 : la classe 0 suit une distribution N 0 (µ = 0, ? = 1) et la classe 1 suit une distribution N 1 (µ = 2, ? = 1). -Inversion des classes. On simule ce changement en inversant les étiquettes des classes du concept 1. Pour ces expérimentations et pour les différents types de changement, les tailles des fenêtres ont été fixées à 1000. La fenêtre de référence contient le concept 1 et la fenêtre courante le concept 2. Le tableau 3 présente les résultats en termes de nombre de détections pour 1000 expérimentations. Les meilleures méthodes sont celles qui sont capables de réaliser le plus de détections. TAB. 3 -Nombre de détections par changement pour les différentes méthodes de détection.
Toutes les méthodes sont capables de détecter de manière fiable un changement dans la moyenne. Le test de Welch n'est pas capable de détecter les changements de variance et ces expériences le confirment. Quelques détections (19) pour le test à 10% se produisent, mais celles-ci correspondent à des fausses détections et non pas à une détection de changement de variance. Le test de Kolmogorov-Smirnov et la détection MODL P (W |X a ) se comportent très bien pour les détections dans les changements de moyenne et variance. Tous les tests précé-dents détectent seulement des changements dans la répartition des données mais sans s'inté-resser à la classe. Par conséquent aucun d'entre eux n'est capable de détecter un changement qui n'intervient que par rapport aux classes. Contrairement à ces tests statistiques, la méthode de détection bivariée MODL P (W |X a , C) est capable de détecter ce genre de changements. Les expérimentations confirment cette capacité.
Application à la gestion de la dérive de concept
Détecter les changements requiert ensuite de réagir à ces derniers. Si l'on sait détecter les changements de concepts au cours du temps on pourra alors (Žliobaite, 2010) : (i) soit réapprendre le modèle de classification à partir de zéro ; (ii) soit adapter le modèle courant ; (iii) soit adapter un résumé des données sur lequel se fonde le modèle courant ; (iv) soit travailler avec la séquence des modèles de classification appris au cours du temps. Cette section propose d'intégrer la méthode de détection précédente dans un algorithme de gestion de la dérive de concept afin de remplacer le classifieur après une détection de changement.
Algorithme MDD
Nous proposons d'intégrer notre méthode de détection à un nouvel algorithme que nous appelons MDD : MODL Drift Detection Method (Algorithme 1). Notre algorithme n'utilise pas les performances du classifieur pour détecter les changements contrairement aux algorithmes DDM et EDDM décrits dans la section 2.2. Il ne dépend donc pas du type de classifieur.
Le remplacement de l'ancien classifieur par un nouveau se fait suite à une détection sur au moins l'une des variables par la méthode bivariée MODL P (W |X i , C). Le remplacement n'est effectif que lorsque le taux d'erreur du nouveau classifieur est plus faible que l'ancien. Ce taux d'erreur est calculé, pour nos expérimentations, à l'aide de la méthode tauxErreur qui correspond à une moyenne mobile exponentielle de paramètre ? = 1/tailleW cur . Le paramètre n min est le nombre minimum d'exemples utilisé pour comparer les performances des deux classifieurs, sa valeur est fixée à 30 (n min = 30) pour toutes les expérimentations. Cette même valeur est utilisée par les méthodes DDM et EDDM avant qu'elles ne commencent à chercher un changement.
Protocole expérimental
Notre algorithme est configuré avec une même taille de fenêtre de 1000 pour la fenêtre de référence et la fenêtre sautante. La problématique du réglage de la taille de la fenêtre n'est pas abordée en détail dans cet article. Une grande taille de fenêtre permet de détecter avec plus de confiance ainsi que des motifs plus complexes. Une plus petite taille permet d'être plus réactif. Fixer cette taille dépend du flux observé et des changements que l'on veut détecter. De manière générale on peut traiter ce problème en utilisant plusieurs tailles de fenêtre en parallèle comme le propose (Lazarescu et al., 2004).
On peut Deux types de classifieur sont utilisés : -un classifieur bayésien naïf utilisant une estimation de densité conditionnelle des classes basée sur des résumés à deux niveaux (Salperwyck et Lemaire, 2012). Le premier niveau est un résumé de quantiles à 100 tuples et le second niveau la discrétisation MODL. -un arbre de Hoeffding (Domingos et Hulten, 2000) avec un résumé de quantiles à 10 tuples et un classifieur bayésien naïf utilisant la discrétisation MODL. On présente les résultats pour le générateur basé sur un hyperplan en mouvement proposée dans (Hulten et al., 2001). Ce générateur est configuré avec 10 attributs, une vitesse de changement de 10 ?3 et 10% de bruit de classe.
Une méthode de l'état de l'art pour l'évaluation en-ligne des classifieurs (Gama et al., 2009) est utilisée. La méthode choisie mesure la performance en utilisant les exemples du flux comme données de test avant qu'ils ne soient appris. La mesure utilisée est la précision moyenne du classifieur entre le début du flux et l'instant t.  Algorithme 1: Notre algorithme MDD (MODL Drift Detection) de remplacement du nouveau classifieur après changement de concept.
Résultats
La figure 1 présente les résultats pour le jeu de données basé sur un « hyperplan en mouvement ». Sur cette figure toutes les méthodes permettent au classifieur d'avoir les mêmes performances respectivement pour le naïf Bayes et l'arbre de décision jusqu'à 100 000 et 200 000 exemples appris. Pour le classifieur bayésien naïf, entre 100 000 et 200 000 exemples DDM devient meilleur, notre méthode MDD reste relativement stable et EDDM bien moins bon. Après 300 000 exemples alors que MDD reste stable, DDM devient bien moins bon et EDDM meilleur, mais moins bon que notre méthode. On observe que notre méthode de dé-tection est meilleure et beaucoup plus stable que les deux autres méthodes. Pour l'arbre de décision, entre 200 000 et 600 000 exemples DDM devient meilleur, la méthode de détection MDD reste relativement stable et EDDM bien moins bon. Après 600 000 exemples alors que MDD continue à s'améliorer, DDM devient bien moins bon et EDDM un peu meilleur. Comme pour le classifieur bayésien naïf, on observe que notre méthode de détection est bien meilleure et beaucoup plus stable que les deux autres méthodes sur ce jeu de données. La figure 2 présente le nombre de détections et d'alertes des différents méthodes. On observe que le comportement de DDM et EDDM n'est pas stable bien que la vitesse de changement (la vitesse de rotation de l'hyperplan) soit constante. Ces méthodes ont tendance à détecter beaucoup de changements sur certaines périodes et aucun sur d'autres. Leurs hypothèses de départ (processus stationnaire et données iid), ne sont sans doute pas valides, ce qui aboutit à un manque de robustesse. Notre méthode de détection n'utilise pas le classifieur pour la détection mais seulement les données du flux. On observe que son nombre de détections est relativement régulier pour un changement ayant une vitesse constante. 
Conclusion
Cet article présente une nouvelle méthode de détection de changement dans les flux de données. Celle-ci est basée sur l'observation du changement des distributions des exemples dans deux fenêtres. Notre méthode n'a pas d'a priori sur la distribution des données ni sur le type de changement à détecter. Elle est capable de détecter des changements rapides ou lents, que cela soit sur la moyenne, la variance ou tout autre changement dans la distribution. Notre méthode fait l'hypothèse d'indépendance des variables conditionnellement aux classes et est capable de détecter des changements sur les probabilités P (W |X i , C) en utilisant l'approche MODL. Chaque fenêtre est étiquetée et un changement est détecté s'il existe un modèle de discrétisation/groupage permettant de distinguer les deux fenêtres.
La nouvelle méthode que nous proposons utilise le critère bivarié MODL P (W |X i , C) qui, dans le cadre de notre méthode, est à la fois : (i) robuste dans le cas d'un flux stationnaire, (ii) rapide à détecter tous types de changements dans la distribution des données, (iii) capable d'utiliser l'information de classe.
Notre méthode a été comparée à deux méthodes de l'état de l'art détectant les variations de performance d'un classifieur. Nous avons proposé un nouvel algorithme, appelé MDD, basé sur notre méthode de détection permettant au classifieur de se mettre à jour. Celle-ci n'utilise pas le classifieur pour la détection mais seulement les données du flux. Ses performances, en termes de précision, sont meilleures et plus constantes que les deux méthodes de l'état de l'art testées.

Introduction
Notre travail se situe dans le cadre d'un système d'agrégation de descriptifs de commerces. Dans ce système, chaque commerce est décrit (entre autres) par un jeu de tags. Des connaissances sur la compatibilité ou l'incompatibilité de ces tags peuvent aider à prendre la décision d'agréger ou pas deux descriptifs. Par exemple, un descriptif contenant le tag fitness ne peut pas être agrégé avec un descriptif contenant le tag menuisier car, a priori, un même commerce ne peut pas à la fois rendre des services de menuiserie et avoir un lien avec le fitness. Par contre, les tags peinture et carrelage sont compatibles car ils peuvent, par exemple, être associés à un magasin d'ameublement. Pour acquérir ces informations de compatibilité, nous disposons de données dont la structure est identique à celle d'une folksonomie. Les folksonomies sont le produit de systèmes d'annotation collaborative tels Flickr ou Delicious qui permettent à une communauté d'utilisateurs d'annoter manuellement des ressources (urls, photos) à l'aide de descripteurs (tags). Le but de notre étude est de tester diverses approches pour l'acquisition de relations entre tags à partir de folksonomies. L'approche retenue sera ensuite intégrée au système d'agrégation de descriptifs décrit plus haut. En plus de tester les approches état-de-l'art, nous explorons aussi l'apport de l'apprentissage automatique pour la détection de relations sémantiques à partir de folksonomies, ce qui, à notre connaissance, n'a pas encore été proposé.
L'article comprend 5 sections. La section 2 décrit les approches existantes pour l'identification de relations entre tags à partir de folksonomies. Les expériences menées sont décrites en section 3. Les données expérimentales et les résultats obtenus sont présentés en section 4. Les perspectives de travail sont discutées en section 5. Hotho et al. (2006b) définissent une folksonomie comme un 4-uplet F := (U, T, R, Y ) où U = {u 1 , ...u n } est un ensemble d'utilisateurs, T = {t 1 , ...t m } est un ensemble de tags, R = {r 1 , ...r p } est un ensemble de ressources et Y ? U × T × R. Un triplet (u, t, r) ? Y correspond à l'attribution du tag t à la ressource r par l'utilisateur u. T ur := {t ? T |(u, t, r) ? Y } est l'ensemble des tags donnés à la ressource r par l'utilisateur u. Les travaux visant à identifier des relations entre tags à partir de folksonomies font état de trois types d'approches : statistiques, vectorielles et exploitation de ressources sémantiques 1 . Les approches statistiques sont les plus fréquemment employées. Elle portent sur la cooccurrence des tags, c'est-à-dire le nombre de fois où t 1 et t 2 ont été attribués à une même ressource par un même utilisateur, et qui est définie ainsi : w(t 1 , t 2 ) = |{(u, r) ? U × R|t 1 , t 2 ? T ur }|. Les mesures peuvent être asymétriques, comme dans le cas de Schmitz (2006) qui utilise la probabilité qu'une ressource ayant reçu le tag t 1 soit également annotée avec le tag t 2 ou symétriques comme la mesure Jaccard employée par Hassan-Montero et Herrero-Solana (2006) dans le but de construire une hiérarchie de concepts.
Travaux apparentés
Les approches vectorielles consistent à représenter chaque tag dans un espace vectoriel dont les dimensions correspondent soit aux autres tags, soit aux ressources, soit aux utilisateurs. Par exemple, dans l'espace vectoriel correspondant aux ressources, chaque tag t est représenté par un vecteur t = {w(t, r 1 ), ..w(t, r |R| )} où w(t, r) := |{(t, u) ? T × U |t ? T ur }|. Les vecteurs sont comparés avec la mesure Cosinus. Ces approches ont été employées par Specia et Motta (2007)  Les approches exploitant des ressources sémantiques existantes consistent à projeter les tags de la folksonomie dans une ressource sémantique structurée en graphe 2 afin d'en déduire des relations sémantiques entre les tags -voir par exemple les travaux de Djuana et al. (2011). Cette technique a également été employée par Cattuto et al. (2008) ainsi que Markines et al. (2009) pour évaluer les approches vectorielles et statistiques. Pour cela, ils se basent sur la corrélation entre le score donné par la mesure à évaluer et la distance taxonomique de Jiang et Conrath (1997) calculée à partir de WordNet.
Expériences
Nos expériences ont consisté à tester les approches statistiques et vectorielles ainsi qu'à tester l'apport de l'apprentissage automatique. Nous avons utilisé deux types de ressources :
(i) un ensemble de triplets (u, t, r) correspondant à l'attribution d'un tag t au lieu r par la source Internet u. La structure de ces données est identique à celle d'une folksonomie, à 1. Plus anecdotiquement, d'autres travaux exploitent la structure de graphe des folksonomies et utilisent une version adaptée de l'algorithme PageRank (Hotho et al. 2006).
2. Par exemple : taxonomie, ontologie, thésaurus.
ceci près que R est un ensemble de lieux (et non de documents) et que U est un ensemble de sources Internet décrivant les lieux (et non des utilisateurs). Toutes les approches testées s'appuient sur l'ensemble des tags (T ) et l'ensemble des ressources (R) ainsi que sur la fonction R(t) = {r ? R|t ? T ur , ?u} qui renvoie l'ensemble des lieux ayant reçu le tag t, quelles que soient les sources ayant attribué ce tag 3 .
(ii) un arbre de tags initialement conçu pour la catégorisation de commerces (exemple de chemin : RACINE > manger > restaurant > chic). Les relations hiérarchiques ainsi que la distance entre deux tags sont potentiellement des indicateurs de leur compatibilité.
Cinq expériences ont été menées :
• OVERLAP (approche statistique) : OVERLAP est une mesure de type statistique dérivée de Jaccard. Elle correspond à un coefficient de chevauchement entre les lieux ayant reçu t 1 et les lieux ayant reçu t 2 :
• COSINUS (approche vectorielle) : Dans cette approche, chaque tag t est représenté par un vecteur t = {inter(t, t 1 ), ...inter(t, t |T | )} où inter(t, t i ) = |R(t)?R(t i )|. Ici, nous considérons que deux tags sont d'autant plus compatibles qu'ils ont des patrons de co-occurrences similaires (i.e. ils ont tendance à apparaître avec les mêmes tags). Par rapport à OVERLAP, cette approche a l'avantage de permettre de rapprocher deux tags même s'ils n'ont pas ou peu été attribués aux mêmes lieux. La similarité des vecteurs de deux tags est évaluée avec la mesure Cosinus :
• ML_TAGTREE (apprentissage automatique à partir d'informations issue d'une ressource sémantique) : Cette approche exploite des informations tirées de l'arbre de tags. À partir de cet arbre de tags, nous pouvons extraire, pour chaque paire de tags (t 1 , t 2 ), 10 variables, décrivant soit des propriétés associées aux noeuds représentant les tags, soit des calculs de distance entre les tags : 10. nb. de tags dans {t 1 , t 2 } ayant pour propriété de déclencher, lors de l'ajout du tag à un lieu, l'ajout automatique de tags plus génériques que lui 4
Ces 10 variables ont été utilisées pour apprendre un modèle de classification à partir d'exemples de paires de tags annotées comme COMPATIBLE ou INCOMPATIBLE. L'apprentissage a été réa-lisé avec C5, un outil de génération d'arbre de décision 5 qui est une version améliorée de l'algorithme de Quinlan (1996).
• ML_SIMPLE : Dans cette expérience, nous utilisons toujours C5 auquel nous fournissons 4 variables très simples pour apprendre l'arbre de décision :
• ML_ALL : Dans cette expérience, nous utilisons toujours C5, auquel nous fournissons toutes les informations utilisées dans les expériences précédentes. Le modèle de classification est donc appris à partir de 16 variables : la mesure Overlap, la mesure Cosinus, les 10 variables de ML_TAGTREE et les 4 variables de ML_SIMPLE.
Données expérimentales et résultats Données
Nous disposons de 15 millions de lieux, 3696 tags et 100 sources fournis par la société Nomao (http://fr.nomao.com). Pour apprendre les différents modèles de classification et évaluer nos expériences, nous avons constitué un jeu de 590 paires de tags annotées avec 2 classes : COMPATIBLE ou INCOMPATIBLE. Un tiers des paires a été annoté par au moins deux annotateurs 6 . Le taux de désaccord entre annotateurs est de 12%, soit un Kappa (Carletta, 1996) de 0,77. La répartition COMPATIBLE/INCOMPATIBLE est de 41%/59% respectivement.
Résultats Les approches ont été évaluées par validation croisée à 10 blocs. Pour les approches OVERLAP et COSINUS, un seuil de compatibilité a été appris en faisant varier sur les données d'apprentissage le seuil à partir duquel on considère que deux tags sont compatibles ; puis ce seuil a été évalué sur les données de tests. Nous avons comparé les résultats des approches deux à deux et pour chaque couple, nous avons appliqué le t-test unilatéral apparié. Nous considérons qu'une approche est significativement meilleure que l'autre si la valeur p du t-test est en-dessous de 5%. Nous obtenons les classements suivants (les valeurs p sont indiquées entre parenthèses) :
- 
Conclusion et perspectives
Nous avons présenté une étude visant à identifier des compatibilités entre descripteurs de lieux dans le but de l'intégrer à un système d'agrégation de descriptifs de lieux : la présence de tags incompatibles empêchera l'agrégation de deux descriptifs. Nous avons introduit l'utilisation de l'apprentissage automatique et nous avons testé trois modes de représentation des données : informations issues d'une ressource sémantique, un jeu de 4 indices "simples" et la combinaison des informations utilisées dans toutes nos expériences. Au final, c'est l'apprentissage automatique basé sur le jeu des 4 indices qui a été choisi car cette approche se place parmi les meilleures en termes de taux d'erreur et qu'elle est également la plus robuste.
La première perspective de travail est l'intégration de la compatibilité des tags dans le système d'agrégation et l'évaluation de son apport. Pour autant, notre méthode de détection de tags compatibles reste perfectible. Nous pensons à l'exploitation de ressources linguistiques plus riches que notre arbre de tags, comme par exemple le réseau lexical JEUX DE MOTS (Lafourcade, 2007) qui a l'avantage d'indiquer des relations lexicales entre mots (synonymie, hyponymie, etc.). Les autres perspectives de travail concernent la variation des différents paramètres de C5, l'essai d'autres approches d'apprentissage automatique (SVM, Naive Bayes...) ainsi que l'annotation de nouveaux exemples sélectionnés via des approches d'apprentissage actif (Settles, 2009).
Références Carletta, J. (1996). Assessing agreement on classification tasks : The kappa statistic. Computational Linguistics 22(2), 249-254.

Introduction
L'étude des génomes nous apprend beaucoup sur le vivant. Chaque organisme possède un génome dont la composition est le résultat de l'histoire évolutive propre à cet organisme. Cette information biologique codée par l'ADN est divisée en unités discrètes, les gènes. Ces gènes codent pour les protéines, véritables "rouages" des réseaux biologiques au niveau cellulaire. Le projet de séquençage du génome humain (3, 4 Gb, 1 Gb représentant un milliard de paires de bases ou acides nucléiques A, C, G et T), initié en 1990, a duré 13 ans pour un coût total de 2, 7 milliards de dollars. Les techniques récentes (2nd Generation Sequencing) ou futures (3rd Generations sequencing) permettent des vitesses de séquençage bien plus élevées, de l'ordre de 50 Gb/ jour, soit un génome comme celui de l'Homme entièrement séquencé en moins de 3 heures (HiSeq Systems, Illumina Inc.).
On comprend aisément le déluge de données brutes qu'il faudra savoir stocker et analyser. Nous nous intéressons à l'exploitation de ces données pour en extraire des connaissances, notamment sur les protéines codées par ces gènes et leurs rôles dans la réalisation d'une action biologique.
Ces informations devraient permettre à terme, de concevoir de nouveaux médicaments spé-cifiques à un individu donné, individu pour lequel nous aurons identifié précisément les interactions entre les protéines de son génome. La plupart des interactions fonctionnelles entre protéines sont réalisées sous la forme de complexe protéique (assemblage macro-moléculaire de plusieurs protéines).
Nous proposons d'apprendre un modèle permettant de prédire un sous-ensemble de ces assemblages : les complexes protéine-protéine impliquant trois protéines ou trimères.
Cet article est organisé de la manière suivante : la section 2 présente le contexte de l'étude que nous avons réalisé ainsi que le protocole mis en oeuvre pour apprendre notre modèle pré-dictif. Les données utilisées pour évaluer et valider notre approche seront également présentées dans la section 2. Les résultats obtenus sont analysés selon deux visions : l'une purement informatique (sections 2.1 et 2.2) et l'autre plus orientée biologie (section 3). Enfin, nous présentons plusieurs perspectives à ce travail en fin d'article.
Contexte biologique de l'étude
Le postulat essentiel de ce travail est que deux protéines qui interagissent dans une espèce donnée vont être soumises, de manière corrélée (ou conjointe), aux contraintes évolutives imposées à l'une ou à l'autre. En d'autres termes, les deux protéines vont co-évoluer, permettant ainsi à l'interaction d'être maintenue et à la fonction biologique d'être conservée dans le temps. L'exemple extrême de coévolution entre protéines en interaction est le gain ou la perte conjointe de deux protéines : si l'association entre deux protéines est nécessaire pour qu'une fonction biologique soit réalisée, la perte d'un des deux partenaires (resp. le gain) entraînera la perte (resp. le gain) de l'autre. La comparaison des profils de présence/absence de protéines (nommés profils évolutifs ou profils phylogénétiques, ) chez différentes espèces permet donc de détecter la co-évolution entre protéines et donc de prédire si deux protéines sont susceptibles d'interagir. Ce type d'approche est au coeur de plusieurs démarches similaires , Marcotte et al. (1999), de Vienne et Azé (2012).
Afin de déterminer le profil évolutif d'une protéine d'intérêt, l'ensemble des protéines qui lui sont similaires, dans un ensemble d'espèces de référence, est calculé. Ces protéines similaires à une protéine donnée mais dans une autre espèce sont nommées des orthologues. Pour une protéine donnée, le profil évolutif que nous manipulons représente l'ensemble des espèces dans lesquelles notre protéine possède au moins un orthologue. Il peut être représenté sous la forme d'un vecteur de booléens comme montré sur la Figure 1.
Étant donné un ensemble de complexes protéiques avérés, nous réalisons l'extraction, pour chaque complexe, des interactions binaires entre protéines. Dans le cadre d'un apprentissage supervisé, ces interactions binaires entre protéines représentent l'ensemble des exemples positifs. À partir de l'ensemble des protéines présentes dans ces complexes, nous générons l'ensemble des paires de protéines n'appartenant pas à des complexes. Ces paires de protéines représentent alors les exemples négatifs.
Nous pouvons alors apprendre un modèle permettant de prédire l'interaction de deux protéines à partir de la connaissance de leur profil évolutif. Nous pouvons considérer les profils évolutifs associés aux différentes protéines comme des 1-motifs fréquents. L'étude des paires de protéines via leurs profils évolutifs, correspond alors à l'étude des 2-motifs fréquents que nous obtenons en calculant simplement l'intersection des profils associés à chaque protéine (voir Figure 1).
Rappelons que notre objectif est de pouvoir prédire si deux protéines vont interagir. Nous avons donc besoin de définir un ensemble de critères permettant d'associer, à partir des profils évolutifs des protéines, un score à chaque paire de protéines étudiée. Pour cela, nous proposons d'utiliser des métriques classiquement utilisées dans l'évaluation des règles d'association pour définir l'ensemble des critères numériques associés à une paire de protéines. Ces métriques, souvent appelées mesures de qualité, sont pour certaines d'entre elles non symétriques, i.e. elles n'évaluent pas de la même manière les règles A ? B et B ? A. Dans notre cadre de travail, les interactions entre protéines ne sont pas orientées et nous ne devons donc pas prendre en considération cette information. Ainsi, pour une paire de protéine (A, B) donnée, nous calculons les mesures de qualité associées aux règles A ? B et B ? A dans le cas des mesures dites non symétriques.
Les critères retenus pour évaluer l'interaction entre deux protéines A et B sont : -n A , n B , n AB qui représentent respectivement le nombre d'espèces ayant au moins un orthologue pour la protéine A, B et pour les protéines A et B -les mesures non symétriques : la confiance (et son symétrique : le rappel), la confiance centrée, la moindre contradiction, l'indice de Jaccard, Loevinger, TEC, LAP, GAN, Zhang, Pearl -les mesures symétriques : Lift, Dice, Pearson, GiniIndex, IQC Le Tableau 1 présente les expressions analytiques des mesures de qualité symétriques et non symétriques. Dans ce tableau, les notations suivantes sont utilisées :
Ainsi, une paire de protéines (A, B) est décrite par 28 valeurs réelles qui caractérisent le lien entre A et B. Afin de déterminer si ce lien est fonctionnel, i.e. se traduit par une interaction entre A et B, un modèle prédictif est appris.
Nous présenterons dans la suite les détails liés à la mise en oeuvre de l'apprentissage su-
Mesures non symétriques Confiance
Indice de Jaccard
TAB. 1 -Mesures de qualité utilisées pour décrire les profils évolutifs. Nous renvoyons le lecteur à (Lallich et al., 2007) et (Lenca et al., 2003) pour une étude détaillée de chacune de ces mesures. Les métriques n A , n B et n AB également utilisées pour décrire un profil évolutif ne sont pas rappelées dans ce tableau.
pervisé de ce modèle. Nous avons retenu une approche de type "ensemble learning" où un ensemble de classifieurs binaires sont combinés pour construire un méta-classifieur.
Notons que le choix des descripteurs (Tableau 1) et leur utilité pour décrire les interactions protéine-protéine ont été décrits et évalués dans un article précédent (de Vienne et Azé (2012)). Nous ne discutons donc pas ces choix dans le présent article.
Combinaison de classifieurs
Des travaux antérieurs (Juan et al. (2008), de Vienne et Azé (2012)) ont montré que les hypothèses biologiques sur lesquelles reposent notre travail permettent d'apprendre des modèles qui s'avèrent efficaces en prédiction d'interaction protéine-protéine. Ces modèles permettent d'ordonner efficacement les paires de protéines par probabilité décroissante d'être en interaction.
Par contre, ces approches ne permettent pas de reconstruire nativement les complexes protéiques associés à ces paires de protéines. Cette reconstruction n'est immédiate que dans le cas de complexes n'impliquant que deux protéines. Pour un trimère ABC, le nombre d'interactions potentiels est égal à 3 : AB, BC, AC et les approches actuelles ne garantissent pas que la totalité de ces interactions soient prédites, ni qu'elles aient des scores comparables permettant ainsi de les identifier facilement. Le travail présenté ici tente de répondre à cette question.
Pour traiter ce problème, nous avons utilisé comme données d'apprentissage l'ensemble des complexes binaires avérés de l'organisme modèle Escherichia Coli (noté E. coli dans la suite de l'article).
Cet organisme contient 4078 protéines réparties en de multiples monomères, 66 dimères, 45 trimères et quelques complexes impliquant plus de trois protéines.
Parmi l'ensemble des approches existantes pour calculer les orthologues, nous avons retenu celle proposée par Moreno-Hagelsieb et Latimer (2008). Ils ont proposé une combinaison de plusieurs méthodes afin de détecter avec une grande fiabilité les orthologues des protéines de E. coli chez N = 1050 espèces et ont rendu ces données publiques.
Nous (i) Les six classifieurs suivants ont été utilisés pour apprendre des méta-classifieurs : règles de décision (JRip, PART), arbres de décision (J48 et RandomForest), Bayésien Naïf et Ré-gression Logistique. Ces classifieurs ont été appris avec la boîte à outils Weka (Hall et al., 2009).
(ii) L'échantillonnage des données est réalisé d'une part de manière à contrôler le nombre de positifs et de négatifs dans les données et d'autre part, pour palier au faible nombre d'exemples positifs disponibles.
De nombreux travaux ont montré l'intérêt des approches de type "Ensemble Learning" par rapport à l'utilisation de classifieurs "classiques" (voir Quinlan (1996), Opitz et Maclin (1999), Bauer et Kohavi (1999) pour quelques approches de références). Nous avons utilisé comme données d'apprentissage les 66 dimères d'E. coli, soit 132 protéines différentes. À partir de ces 132 protéines, nous construisons l'ensemble des 8580 paires de protéines représentant des interactions négatives. Cet ensemble est réduit à 8279 paires de protéines négatives après application de la contrainte n AB ? 2 à chaque profil évolutif. Cette contrainte minimale permet d'assurer une certaine cohérence avec notre hypothèse de travail reposant sur la coévolution (dans au moins deux espèces différentes) des protéines en interaction.
Nous procédons de la même manière pour créer l'ensemble de test constitué uniquement des 45 trimères. Ces 45 trimères sont constitués à partir de 135 protéines différentes. Notre ensemble de test est donc constitué de 135 exemples positifs (paires de protéines impliquées dans un trimère) et de 8910 négatifs (le filtre n AB ? 2 ne rejette aucune paire).
Les six classifieurs listés précédemment sont utilisés pour apprendre deux méta-classifieurs différents :
M 1 ce premier méta-classifieur est construit de la manière suivante : (i) l'ensemble des données d'apprentissage (les dimères) est utilisé pour apprendre chacun des six classifieurs, (ii) chacun des six classifieurs obtenu est appliqué sur le jeu de test (les trimères), et (iii) pour chaque exemple, le nombre de votes positifs est utilisé pour évaluer l'appartenance à la classe positive. Ainsi, une valeur variant de 0 à 6 sera associé à chaque exemple.
M n 2 ce second méta-classifieur ne diffère que sur le point (i) où un ensemble de n échantillons des données d'apprentissage est construit par tirage sans remise et les six classifieurs sont appris sur cet échantillon. Chaque échantillon contient 50 exemples positifs et 1000 exemples négatifs. Afin d'éviter tout biais dû à l'échantillonnage aléatoire dans la construction des échan-tillons utilisés par l'approche M n 2 , nous avons itéré le processus 100 fois et les votes ont été moyennés. Ainsi, une valeur variant de 0 à 6 × n est associée à chaque exemple du jeu de test.
Ces deux méta-classifieurs sont ensuite utilisés pour deux tâches : (A) élaguer l'ensemble des paires potentiellement en interaction et (B) détecter les trimères.
L'élagage des paires potentiellement en interaction est simplement réalisée en utilisant la contrainte suivante : si score(exemple) = 0 alors élagage(exemple), où score(exemple) représente le nombre de votes positifs associés à l'exemple. Les performances des différents classifieurs pour la tâche (A) sont présentées dans le Tableau 2.
Le méta-classifieur M 1 se comporte comme le Bayésien Naïf, car le Bayésien Naïf est le classifieur qui prédit le plus de positifs et tous les positifs prédits par les différents classifieurs sont également prédits par le Bayésien Naïf.
Concernant le méta-classifieur M n 2 , nous ne présentons que les résultats pour n = 50 car nous avons constaté qu'à partir de n = 50 échantillons utilisés pour composer le méta-classifieur, les performances observées ne variaient plus (par manque de place, nous ne pouvons discuter plus précisément cet aspect de nos résultats). Pour la tâche (B), nous proposons l'algorithme pT ri, présenté dans la section suivante, qui permet de reconstruire les trimères, à partir des paires non élaguées.
Extraction des trimères à partir de l'évaluation des paires de protéines par le méta-classifieur
Sachant que nous nous focalisons sur la recherche de trimères, nous proposons l'algorithme pT ri (voir Algorithme 1) pour identifier ces trimères. pT ri exploite le nombre de votes positifs associés aux paires de protéines pour identifier les paires les plus prometteuses. Nous considérons la liste des paires ordonnées par nombre de votes positifs décroissant.
pT ri parcourt cette liste par valeur décroissante du nombre de votes positifs. Les paires de protéines sont extraites de la liste une par une pour reconstituer des trimères. L'étape initiale consiste à extraire la première paire de la liste. Puis la liste est parcourue en appliquant la règle suivante : lorsqu'une nouvelle paire de protéines AB est prise en considération :
-soit il existe une protéine X telle que la paire AX (resp. BX) a déjà été rencontrée précédemment (avec un nombre de votes positifs plus élevé que AB) alors : -s'il n'existe pas de protéine Y tel que BY (resp. AY ) ait été précédemment rencontré, alors le trimère AXB est prédit et toutes les paires impliquant l'une des trois protéines A, B ou X sont supprimées de la liste ordonnée des paires considérées. -soit il existe Y tel que BY (resp. AY ) ait été rencontrée alors la paire AB est simplement supprimée. L'hypothèse sous-jacente est que les paires AX (resp. BX) et BY (resp. AY ) appartiennent chacune à un autre trimère. -sinon, la paire AB est ajoutée à l'ensemble des paires prédites. Cette paire représente alors la première paire d'un trimère dont aucune des protéines n'a déjà été identifiée. Nous avons appliqué pT ri sur les données de test où les interactions entre protéines ont été évaluées d'une part en utilisant le méta-classifieur composé des 6 classifieurs appris sur l'ensemble des données relatives aux dimères et d'autre part, sur le méta-classifieur obtenu par combinaison des 6 × n classifieurs appris sur des échantillons aléatoires des données relatives aux dimères.
Le Tableau 3 présente les résultats obtenus pour les approches pT ri(M 1 ) et pT ri(M 50 2 ). Les performances de pT ri peuvent être résumées selon les deux axes suivants.
Réduction du nombre de paires de protéines à analyser
La quantité de paires à analyser a été réduite de manière drastique pour les deux méthodes. 
Augmentation du nombre de trimères correctement identifiés
Outre le nombre de paires de protéines correctement identifiées, l'approche pT ri(M 50 2 ) permet également de prédire correctement un plus grand nombre de trimères que l'approche pT ri(M 1 ). En effet, en utilisant les 64 paires de protéines correctement identifiées par l'ap-  
Application aux données biologiques
Nous nous sommes focalisés ici sur la prédiction de complexes hétérotrimèriques impliquant des protéines de structures et fonctions différentes. Ces trimères sont impliqués dans plusieurs rôles clés de la cellule : régulation des concentrations en différents substrats assurant le maintien de la cellule dans un état quasi-stationnaire, dégradation et synthèse de l'ADN/ARN.
Le méta-classifieur M 50 2 nous permet d'identifier correctement les trois protéines de 30 des 45 trimères. Ces 30 prédictions se décomposent en 27 trimères parfaitement prédits (l'intégra-lité des interactions) et 3 trimères partiellement prédits (2 interactions sur 3), ce qui s'avère parfaitement suffisant pour reconstruire le trimère original.
Pour 12 trimères, nous n'arrivons à identifier qu'une seule des trois interactions, enfin pour 3 trimères, aucune des interactions n'est identifiée.
Parmi les 19 paires de protéines incorrectement élaguées par le méta-classifieur M 50 2 (voir Tableau 2), nous retrouvons 8 des 9 paires impliquées dans les 3 trimères non identifiés. Ces 8 paires ayant été élaguées avant l'application de l'algorithme pT ri, il devient impossible d'identifier correctement les trimères concernés.
Prises en compte des spécificités structurales : cas des transporteurs membranaires type ABC
L'objectif initial de cette étude concerne la prédiction des interactions pour un trimère ABC, pour lequel le nombre d'interactions potentiels est égal à trois : AB, BC, AC. Néan-moins quels renseignements pouvons nous retenir des prédictions partielles de ces assemblages protéiques, i.e lorsque seule une voire deux interactions du trimère original sont correctement prédites ? le corollaire de cette question étant : quels enseignements pour la suite, nous fournissent ces trimères "atypiques" ?
Parmi les complexes pour lesquels la prédiction est incomplète (de 1 à 2 interactions sur les 3 potentielles), figurent les transporteurs ABC. Ces transporteurs ABC, situés au niveau des membranes de la cellule, permettent l'assimilation ou l'élimination au niveau cellulaire d'une large variété de métabolites et constituent une cible thérapeutique de choix comme les traitements antibactériens (Cangelosi et al., 1990).
Ce transporteur, qui est ATP-dépendant (i.e. nécessitant une source d'énergie ATP pour fonctionner), est représenté schématiquement Figure 2. Il est constitué d'une structure extramembranaire, récepteur des métabolites provenant de l'environnement, d'une sous-structure membranaire ou perméase permettant le transfert des métabolites au niveau de la membrane hydrophobe sans altérer leurs structures et enfin d'une partie intra-membranaire correspondant au site de fixation des molécules d'ATP (Dawson et Locher, 2006). Nous voyons ici que d'un point de vue structural, la partie extra-cellulaire (site de fixation des métabolites) et le site de fixation de l'ATP ne sont jamais connectés. Dès lors pour reprendre la notation utilisée précédemment pour un trimère ABC donné, (où A désigne le site extra-cellulaire, B la perméase et C le site de fixation de l'ATP), seules les interactions directes AB et BC s'avèrent pertinentes. L'information associée à la localisation de la protéine (intra ou extra cellulaire) peut être exploitée en post-traitement des prédictions effectuées par pT ri.
Prenons l'exemple présenté sur la Figure 3 pour illustrer ce post-traitement. La Figure 3-(a) représente un sous-graphe du graphe des prédictions effectués par pT ri. Nous pouvons y identifier une prédiction aberrante : proX ? modC. Ces deux protéines ne peuvent pas interagir physiquement car proX est une protéine intra-cellulaire et modC est une protéine extra-cellulaire. Une information évidente pour un expert du domaine qui peut donc supprimer manuellement le lien prédit entre proX et modC.
Si nous réappliquons pT ri uniquement sur les paires de protéines présentes dans ce sousgraphe (à l'exception de proX ? modC), alors nous obtenons les quatre trimères présentés sur la Figure 3-(b).
Nous pouvons voir qu'une simple connaissance expert injectée dans les prédictions permet automatiquement à notre algorithme d'effectuer les bonnes prédictions d'interactions.
Ce type de configuration se reproduit pour un autre sous-graphe pour lequel 2 trimères sont prédits après une simple intervention de l'expert.
Après application de ce nouveau post-traitement, 9 des 12 trimères partiellement prédits (1 interactions sur 3) sont alors extraits avec deux interactions prédites sur les trois, permettant ainsi d'identifier 39 des 45 trimères.
Conclusion
Dans ce contexte spécifique de la biologie, cette méthode par combinaison de classifieurs s'avère performante. Elle permet de réduire de manière drastique le taux de faux négatifs, rejetant ainsi 94, 74% des paires de protéines. Les résultats montrent que l'échantillonnage aléatoire des données avec renforcement du taux de positifs a un impact positif sur la qualité des prédictions observées. Concernant la reconstruction des trimères de protéines, l'extraction des 2-motifs fréquents en tenant compte du nombre de votes positifs associés à chaque paire de protéine, permet de retrouver le modèle mutlimérique dans 66, 67% des cas. L'étude des contacts structuraux directs a permis de montrer que ce taux pouvait atteindre 86, 67% de bonnes prédictions, en accord avec le principe même de coévolution où, en grande majorité, la modification évolutive d'une protéine "touche" principalement son interactant protéique direct. À très court terme, cette approche offre des perspectives remarquables dans les reconstructions des réseaux de signalisation cellulaire ou de docking, visant à prédire, au sein même de la cellule, quelles protéines interagissent entre elles et comment, d'un point de vue structural, celles-ci peuvent s'assembler. Une généralisation de l'algorithme pT ri pour pouvoir l'étendre aux multimères de plus de 3 partenaires s'avère essentielle. En outre, afin de réduire un peu plus le taux de faux positifs, nous souhaiterions utiliser l'ensemble des classifieurs pour vérifier la qualité de l'annotation faite sur les données en isolant les interactions indirectes entre protéines prédites dans un même complexe. Enfin à plus long terme, il serait intéressant d'introduire une procédure d'apprentissage actif en recoupant les votes associés au méta-classifieur et les données multi-sources mises à disposition par les experts en biologie.
Références
Bauer, E. et R. Kohavi (1999). An empirical comparison of voting classification algorithms : Bagging, boosting and variants. Machine Learning 36(Issue 1-2), 105-139.

Résumé
La réponse cellulaire d'un organisme vivant à un signal donné, hormone, stress ou médi-cament, met en jeu des mécanismes complexes d'interaction et de régulation entre les gènes, les ARN messagers, les protéines et d'autres éléments tels que les micro-ARNs. On parle de réseau d'interaction pour décrire l'ensemble des interactions possibles entre protéines et de ré-seau de régulation génique pour représenter un ensemble de régulations entre gènes. Identifier ces interactions et ces régulations ouvre la porte à une meilleure compréhension du vivant et permet d'envisager de mieux soigner par le biais du ciblage thérapeutique. Puisque les techniques expérimentales de mesure à grande échelle, récemment développées, fournissent des données d'observation de ces réseaux, ce problème d'identification de réseau, généralement appelé inférence de réseau en biologie des systèmes, s'inscrit dans le cadre général de la fouille de données et plus particulièrement de l'apprentissage artificiel. Voilà maintenant quelques années que cette problématique a été posée à notre communauté et durant lesquelles les échanges entre biologistes et informaticiens ont non seulement permis aux biologistes d'étoffer leurs boîtes à outils mais aussi aux informaticiens de concevoir de nouvelles méthodes de fouille de données.
En partant des deux problématiques distinctes que sont l'inférence de réseau d'interaction et l'inférence de réseau de régulation, je montrerai que ces deux tâches d'apprentissage posent, chacune de manière différente, la problématique de la prédiction de sorties structurées. L'inférence de réseau d'interaction entre protéines, vue comme un problème transductif de prédiction de liens, peut être résolue comme un problème d'apprentissage d'un noyau de sortie à partir d'un noyau d'entrée. L'inférence de réseau de régulation, impliquant la modélisation d'un système dynamique, peut être abordée par l'approximation parcimonieuse et structurée de fonctions à valeurs vectorielles. Je présenterai un ensemble de nouveaux outils de régression à sortie dans un espace de Hilbert, fondés sur des noyaux à valeur opérateur, qui fournissent d'excellents résultats en inférence de réseaux biologiques. Des expériences in silico sur des données artificielles, chez la levure du boulanger ou chez l'homme illustreront mes propos. En fin d'exposé, je tracerai quelques perspectives concernant les " nouveaux " défis dans le domaine de la bioinformatique et dans celui de la prédiction de sorties structurées. 

Introduction
Contexte. Depuis plusieurs années, dans les secteurs de l'Internet, de l'analyse décisionnelle ou encore de la génétique sont collectées et analysées des données de plus en plus volumineuses et complexes. Ce phénomène connu sous le nom de Déluge des données (ou Big Data) soulève de nombreuses problématiques. En particulier, être capable de stocker, partager et analyser de telles quantités de données constitue un enjeu d'étude essentiel, comme le soulignent Schuett et Pierre (2012). La théorie des graphes est particulièrement appropriée pour étudier les réseaux sociaux, où les connexions entre utilisateurs peuvent facilement être représentées et analysées en utilisant des graphes, le plus souvent orientés. Dans cet article, nous considérons le graphe des relations entre utilisateurs (anonymisés) de Twitter créé en 2009 par Cha et al. (2010). Ce graphe orienté contient plus de 50 millions de sommets et près de 2 milliards d'arcs.
Les capitalistes sociaux. Nous nous intéressons particulièrement au comportement d'utilisateurs particuliers nommés capitalistes sociaux, observé par Ghosh et al. (2012). Ces utilisateurs, qui ne sont ni des spammeurs, ni des robots, partagent un objectif commun : acquérir un nombre maximum d'utilisateurs qui les suivent -followers. En effet, plus le nombre de followers d'un utilisateur est élevé, plus il peut être influent sur le réseau. Au-delà de cet intérêt évident, le nombre de followers a un impact direct sur le classement des tweets de l'utilisateur sur le moteur de recherche de Twitter. Ces utilisateurs ne sont pas sains pour un réseau social : en suivant des utilisateurs sans regarder le contenu de leurs tweets, les capitalistes sociaux donnent de l'influence à des utilisateurs tels que les spammeurs.
Notre contribution. Les résultats que nous proposons dans cet article sont de deux types. Nous nous concentrons tout d'abord sur la recherche de méthodes efficaces et haut niveau permettant de stocker et manipuler le graphe des relations entre utilisateurs de Twitter en ayant recours à des ressources informatiques raisonnables 1 . Nous nous intéressons ensuite à la dé-tection des capitalistes sociaux. En particulier, nous montrons que ces derniers peuvent être détectés efficacement en appliquant des mesures de similarité sur les voisinages du graphe des relations. Pour notre étude, nous utilisons un graphe collecté en 2009 par Cha et al. (2010), contenant les utilisateurs anonymisés de Twitter ainsi que les relations qui existent entre eux. Plus précisément, pour mettre en oeuvre nos mesures, nous considérons le graphe des spammeurs de Twitter, qui contient près de 40000 spammeurs (détectés par Ghosh et al. (2012)) ainsi que leurs voisins, pour un total de 15 millions de sommets et 1 milliard d'arcs (Section 2). Nous verrons dans la suite de cet article que travailler sur un tel graphe est suffisant pour nos besoins. Finalement, pour valider notre méthode de détection, nous comparons nos résultats à une liste de 100000 capitalistes sociaux potentiels détectés de manière ad-hoc par Ghosh et al. (2012) lors de leur étude sur les spammeurs. Nous observons que nos algorithmes détectent une grande majorité de ces utilisateurs, qui ont la quasi-totalité de leur voisinage inclus dans le graphe des spammeurs (Section 3).
2 Graphe des spammeurs : stockage et définition Stockage. Dans leurs travaux respectifs, Cha et al. (2010) et Ghosh et al. (2012) ne donnent aucun détail sur la méthode qu'ils ont employée pour manipuler le graphe de Twitter. Dans cet article, notre premier objectif est ainsi de trouver un procédé haut niveau pour stocker et traiter le graphe des relations de Twitter en utilisant des ressources informatiques raisonnables. La méthode que nous suggérons dans cet article peut donc être reproduite facilement sur un simple serveur avec un unique processeur disposant d'une certaine quantité de mémoire vive (environ 24 Go pour étudier efficacement le graphe des spammeurs). Remarquons que les mesures développées dans l'article auraient pu être réalisées via des traitements sur la liste d'adjacence du graphe mais cela n'est pas une méthode haut niveau extensible à d'autres problèmes.
Pour parvenir à cet objectif, nous avons utilisé des bases de données pour stocker le graphe et l'analyser. Nous avons exploré la possibilité d'utiliser tout type de bases de données, qu'elles soient dites SQL, NoSQL ou orientées graphes. Les résultats de nos expérimentations montrent que, même pour de simples mesures telles que celles que nous mettons en oeuvre, il est plus efficace d'utiliser une base de données orientée graphes. Par exemple, si MySQL permet de rapidement charger les données, l'exécution de requêtes calculant l'intersection de deux voisinages (un outil nécessaire pour nos mesures) est relativement lente (plusieurs jours pour les exécuter sur tous les sommets ou plusieurs jours pour poser les index efficaces). Avec Cassandra (NoSQL), le simple fait d'obtenir le degré de chaque sommet peut demander de nombreuses heures de traitement. Par conséquent, nous avons essayé plusieurs bases de données orientées graphes, telles que OrientDB ou Neo4j. Dans les deux cas, nous n'avons pas été capables de charger le graphe en un temps raisonnable (moins d'une semaine). Finalement, Dex (voir Martínez-Bazan et al. (2012)) est apparue comme une solution viable pour plusieurs raisons : orientée graphes, haute performance et dotée d'une API haut niveau. Sur simple demande, nous avons obtenu une licence temporaire fournissant un accès complet à toutes les fonctionnalités. Cependant, si nous avons été capables de stocker le graphe des relations de Twitter en utilisant Dex (avec un temps de chargement de quelques heures), nos algorithmes n'ont parfois pas pu s'exécuter sur la totalité du graphe à cause de dysfonctionnements. Ces problèmes techniques -détectés par nos expérimentations-sont toujours en cours de correction. Cependant, ils ne sont pas apparus sur un graphe de plus petite taille (dit des spammeurs) qui reste cohérent avec nos objectifs, et nos algorithmes s'exécutent ainsi en quelques heures. Nous allons voir que détecter des utilisateurs particuliers dans ce graphe avec des mesures de similarité sur les voisinages peut permettre d'obtenir des informations pertinentes dans le graphe des relations. En effet, la majorité des utilisateurs détectés par nos algorithmes ont une grande proportion de leur voisinage dans le graphe des spammeurs.
Capitalistes sociaux
De façon identique aux comportements observés sur Internet, où les administrateurs de sites webs effectuent de l'échange de liens dans le but d'accroître leur visibilité, certains utilisateurs cherchent à obtenir un maximum de followers afin d'augmenter leur influence. Pour parvenir à cet objectif, ces utilisateurs exploitent deux techniques relativement simples et basées sur la réciprocation du lien follow : FMIFY (Follow Me and I Follow You -l'utilisateur assure à ses followers qu'il les suivra en retour) et IFYFM (I Follow You, Follow Me -ces utilisateurs suivent d'autres utilisateurs en espérant que ceux-ci les suivent en retour). Ces comportements ont été mis en lumière par Ghosh et al. (2012) : lors d'une étude sur les spammeurs, ils ont observé une classe d'utilisateurs réels (i.e. ni des spammeurs, ni des faux comptes) répondant beaucoup aux sollicitations des spammeurs, les qualifiant ainsi de capitalistes sociaux.
Définition 1 (Capitaliste social). Un capitaliste social est un utilisateur appliquant les principes FMIFY/IFYFM dans le but d'augmenter son influence sur le réseau social Twitter.
Il est intéressant de noter que plusieurs comptes célèbres sur Twitter (comme par exemple celui de Barack Obama) sont connus pour avoir appliqué ces principes.
Mesures de similarité. Dans le but de détecter des capitalistes sociaux, nous utilisons deux mesures de similarité sur le voisinage des utilisateurs, à savoir l'indice de chevauchement (introduit par Simpson (1943)) et le ratio. La première nous permet de détecter de potentiels capitalistes sociaux, alors que la dernière nous sert à classifier ces capitalistes sociaux selon leur utilisation de l'un ou l'autre des principes FMIFY et IFYFM. 
. Cela nous permet de détecter des utilisateurs susceptibles d'être des capitalistes sociaux. En effet, les voisinages entrants et sortants de ces derniers doivent être fortement liés ; en d'autres termes, ils doivent suivre la majorité de leurs followers (principe FMIFY), ou inversement être suivis par la majorité des utilisateurs qu'ils suivent (principe IFYFM). En particulier, cela signifie que leur indice de chevauchement doit être proche de 1, l'ensemble
, respectivement. Nous utilisons par la suite la Définition 3 pour classifier ces utilisateurs.
Intuitivement, les utilisateurs qui suivent le principe IFYFM doivent avoir un ratio supérieur à 1, tandis que les utilisateurs qui appliquent FMIFY doivent avoir un ratio inférieur à 1. Dans les deux cas, le ratio attendu doit être proche de 1. Nous observons cependant un comportement qui engendre un ratio très inférieur à 1 chez certains utilisateurs, que nous appelons passifs. Contrairement aux autres capitalistes sociaux, ces utilisateurs considèrent leur degré entrant comme suffisant. Ils cessent donc d'utiliser les principes mentionnés ci-dessus mais continuent à accumuler des followers, notamment grâce à l'influence dont ils disposent.
Détection dans le graphe des spammeurs. Nous prétendons que détecter les capitalistes sociaux dans un tel graphe peut fournir des informations pertinentes à propos des capitalistes sociaux dans le graphe des relations entre utilisateurs. Pour illustrer cela, nous utilisons une liste de 100000 utilisateurs considérés comme des capitalistes sociaux par Ghosh et al. (2012). Ces derniers ont été répérés de manière ad-hoc : en partant d'une liste de 40000 spammeurs potentiels, ils ont considéré comme capitalistes sociaux les utilisateurs répondant le plus aux sollicitations des spammeurs. Ces derniers doivent donc se retrouver dans le graphe des spammeurs. Plus précisément, comme l'illustre la Figure 1, la majorité de ces utilisateurs possède la quasi-totalité de leur voisinage dans le graphe des spammeurs. Ces premières observations nous permettent de valider la pertinence de nos mesures de similarité basées sur les voisinages, et donc de poursuivre l'étude des capitalistes sociaux sur le graphe des spammeurs. Quelle que soit la contrainte imposée sur le degré entrant, nous observons les comportements IFYFM et FMIFY : pour un degré supérieur à 500, 62% des utilisateurs ont un ratio supérieur à 1 et 24% ont un ratio entre 0.7 et 1, respectivement. Nous remarquons également que des capitalistes sociaux dits passifs sont présents lorsque le degré est supérieur à 10000 : 14% des 5344 sommets ayant un indice de chevauchement supérieur à 0.8 ont un ratio inférieur à 0.7.
Validation des résultats. Afin de confirmer nos observations, nous utilisons la liste de cent mille capitalistes sociaux détectés de manière ad-hoc par Ghosh et al. (2012). Rappelons que ces derniers devraient être détectés par notre méthode, un fait visible sur la table droite de la Figure 2. Nous aimerions mentionner qu'environ 12500 utilisateurs de la liste de Ghosh et al. (2012) possèdent moins de 500 followers. Pour mieux illustrer la cohérence de nos résultats, nous montrons maintenant que les utilisateurs détectés comme capitalistes sociaux potentiels par nos algorithmes ont leur voisinage presque entièrement contenu dans le graphe des spammeurs ( Figure 3). Ainsi, la plupart des utilisateurs que nous détectons peuvent être considérés comme des capitalistes sociaux dans le graphe des relations. En effet, par la Définition 2, tout utilisateur ayant moins de 10% de son voisinage à l'extérieur du graphe des spammeurs aura un indice de chevauchement d'au moins 0.72 dans le graphe des relations. 

Introduction
Les approches de bi-partitionnement sont devenues un sujet d'intérêt en raison de ses nombreuses applications dans le domaine de l'exploration des données. Une méthode de bipartitionnement, aussi appelée classification croisée, bi-clustering ou co-clustering, est une méthode d'analyse qui vise à regrouper des données en fonction de leur similarité. La stratégie classique des méthodes de bi-partitionnement cherche à trouver des sous-matrices ou des blocs, qui représentent des sous-groupes de lignes et des sous-groupes de colonnes. Depuis le premier algorithme de bi-partitionnement, appelé Block Clustering proposé par Hartigan (1972), de nombreuses techniques ont été proposées telles que l'énumération exhaustive (Tanay et al. (2002)), l'analyse spectrale (Greene et Cunningham (2010)), les réseaux bayésiens (Shan et al. (2010)) et d'autres (Angiulli et al. (2006), Charrad et al. (2008)). L'approche Block Clustering (Hartigan (1972)) permet de diviser la matrice des données en plusieurs sous-matrices correspondant à des blocs. Le principe de base de cette méthode est de faire des permutations des lignes et des colonnes afin de définir la structure de bloc. De plus, l'auteur Hartigan (1972) a proposé deux autres algorithmes de bi-partitionnement : le premier (One-Way Splitting) est principalement basé sur le partitionnement des observations en utilisant des fonctions ayant une variance intra-classe supérieure à un seuil donné afin de diviser la classe associée. Le second algorithme (Two-Way Splitting) procède par des divisions successives des lignes et des colonnes. Le même principe a été repris dans l'approche CTWC proposée par Getz et al. (2000a). CTWC consiste à appliquer un algorithme de classification hiérarchique, le SPC (Super Paramagnetic Clustering) introduit par Getz et al. (2000b) sur les colonnes en utilisant toutes les lignes et vice versa.
Les algorithmes de k-means ont longuement été utilisés dans le bi-partitionnement. En effet, Govaert (1983) a défini trois algorithmes de bi-partitionnement : Croeuc, Crobin et Croki2. Ces algorithmes consistent à déterminer une série de couples de partitions minimisant une foction de coût sur la matrice des données en appliquant la méthode des nuées dynamiques alternativement sur les lignes et les colonnes. Croeuc, Crobin et Croki2 diffèrent par le type des données à traiter. En effet, Croeuc est destiné à des données quantitatives. Crobin est appliqué sur des données binaires. Croki2 est utilisé pour un tableau de contingence.
Récemment, de nouvelles approches de bi-partitionnement basées sur la décomposition matricielle sont proposées (Paatero et Tapper (1994), Long et al. (2005), Yoo et Choi (2010), Labiod et Nadif (2011), Shang et al. (2012). Les auteurs de Long et al. (2005) ont proposé une approche nommée NBVD qui décompose une matrice des données en trois composantes en procédant par un algorithme itératif appliqué sur des données non négatives. L'approche nommée Coclustering Under Nonnegative Matrix Tri-Factorization (CUNMTF) introduite par Labiod et Nadif (2011) appartient à cette même famille. Les auteurs montrent que le double kmeans est équivalent à un problème algébrique de NMF sous certaines contraintes appropriées.
Les méthodes de bi-partitionnement utilisant des cartes auto-organisatrices (SOM) (Kohonen et al. (2001)) ont été définis par plusieurs auteurs. Nous citons l'approche DCC (Double Conjugated Clustering) de Busygin et al. (2002) et KDISJ (Kohonen for Disjonctive Table) de Cottrell et al. (2004) ainsi qu'une autre variante récente introduite par Benabdeslem et Allab (2012). L'inconvénient de la méthode DCC est l'utilisation de deux cartes (une carte pour les observations et une carte pour les variables). Ces cartes sont construites indépendamment avec la même dimension. En ce qui concerne KDISJ, cette méthode est uniquement dédiée aux données catégorielles.
Dans ce papier, nous proposons une nouvelle approche (BiTM) de bi-partitionnement utilisant les cartes topologiques. BiTM ne nécessite aucune pré-organisation de la matrice des données en utilisant une seule carte qui représente simultanément la partition des observations et la partition des variables. Notre approche permet aussi de fournir de nouvelles visualisations. Le reste de cet article est organisé comme suit : dans la section 2, nous présentons le modèle et l'algorithme, la section 3 est dédiée à la méthodologie et les résultats expérimentaux. Enfin, nous concluons cet article par une conclusion et quelques perspectives.
2 Bi-partitionnement topologique : modèle BiTM Le modèle BiTM est constitué d'un ensemble de cellules discrètes C de taille K appelées "carte". Cette carte a une topologie discrète définie comme un graphe non orienté, qui est généralement une grille à 2 dimensions. Pour chaque paire de cellules (c, r) de la carte, la distance ?(c, r) est définie par le plus court chemin reliant les cellules r et c sur la grille. Soit d l'espace euclidien des données et D la matrice des données où chaque observation
d . L'objectif de BiTM est de fournir des bi-clusters organisés dans une carte topologique. Pour cela, l'ensemble des lignes (observations) I = {1, . . . , N } de la matrice des données D est partitionné en K groupes {P 1 , P 2 , . . . , P k , . . . , P K }. De même, l'ensemble des colonnes
Nous définissons deux matrices binaires Z = (z ik ) et W = (w jl ) pour sauvegarder les informations associées respectivement aux observations et aux variables.
Où ? est la fonction d'affectation. Avec z ik et w jl , nous pouvons déterminer des blocs de données B l k = {x ij |z ik × w jl = 1}. Dans BiTM, chaque cellule c de C est associée à un prototype sous la forme d'un vecteur :
k . Nous proposons de minimiser la nouvelle fonction de coût suivante :
) la fonction de voisinage. T représente la fonction contrôlant le rayon du voisinage. De même que pour les cartes auto-organisatrices, nous utilisons la fonction
) pour définir le voisinage. La minimisation de˜Rde˜ de˜R(? w , ? z , G) est obtenue par l'exécution itérative de 4 étapes jusqu'à un nombre d'itérations prédéfini (algorithme 1).
L'ordre topologique dans le modèle BiTM
La décomposition de la fonction de coût˜Rcoût˜ coût˜R qui dépend de la valeur de T , peut être réécrite de la manière suivante :
La fonction de coût˜Rcoût˜ coût˜R est décomposée en deux termes. Afin de maintenir l'ordre topologique entre les blocs, la minimisation du premier terme entraîne le bloc qui correspond à deux cellules voisines. En effet, si les cellules c et r sont voisines dans la carte, la valeur de ?(r, k) est faible et dans ce cas, la valeur de K T (?(r, k)) est élevée. La minimisation du second terme correspond à la minimisation de l'inertie des données locales affectées à un bloc B j r , j = 1 . . . L. Pour différentes valeurs de T , chaque terme de la fonction de coût a une importance relative dans le processus de minimisation. On peut, donc, définir deux étapes pour l'exploitation de l'algorithme : Phase itérative 1-Affectation des observations : chaque observation x i est affectée au prototype g k le plus proche en utilisant la fonction d'affectation :
2-Mise à jour des prototypes : les vecteurs des prototypes sont mis à jour en fonction des affectations des observations :
3-Affectation des variables : chaque variable x j est affectée au prototype g l k le plus proche en utilisant la fonction d'affectation :
4-Mise à jour des prototypes : les vecteurs des prototypes sont mis à jour en fonction des affectations des variables :
RÉPÉTER les phases 1, 2, 3 et 4 jusqu'à t = t max .
-La première étape correspond à des valeurs élevées de T . Si le premier terme est dominant alors la priorité est de préserver la topologie. -La deuxième étape correspond à des valeurs faibles de T où le deuxième terme est pris en compte dans la fonction de coût. Par conséquent, l'adaptation locale et l'algorithme BiTM converge vers l'algorithme Crouec proposé par Govaert (1983).
Expérimentations
Nous avons testé l'algorithme BiTM avec des jeux de données de la base UCI (Frank et Asuncion (2010)). Le tableau 1 indique les paramètres de chaque jeu de données (nombre d'observations, nombre de variables et nombre de classes réelles, ainsi que la taille de la carte utilisée pour l'apprentissage). Afin d'évaluer les performances de BiTM, nous avons utilisé trois indices, la pureté, le rand et le NMI (Normalized Mutual Information) (Strehl et al. (2002) 
Comparaison de BiTM avec les approches de partitionnement
Pour cette première expérimentation, nous comparons les résultats de notre approche BiTM avec les approches suivantes : SOM classique (Kohonen et al. (2001)), HCL (Eisen et al. (1998)), NMF (Paatero et Tapper (1994)) et ONMTF (Long et al. (2005) 
Comparaison de BiTM avec les approches de bi-partitionnement
Afin de comparer BiTM avec les approches de bi-partitionnement, nous avons sélectionné trois approches : CTWC (Getz et al. (2000a)), NBVD (Long et al. (2005)) et CUNMTF (Labiod et Nadif (2011)). Les résultats expérimentaux sont présentés dans les tableaux 5, 6 et 7. Nous signalons que CTWC ne fournit pas de résultats avec la base Movement Libras.
Le tableau 5 résume les résultats expérimentaux de l'indice pureté. Nous remarquons que BiTM fournit les meilleurs résultats sur toutes les bases de données. Dans la plupart des cas, nous constatons une différence remarquable entre les résultats sur l'indice pureté obtenu avec notre méthode et les autres approches. En effet, pour la base Movement libra par exemple, BiTM obtient 0.712, NBVD 0.33 et CUNMTF 0.333. La même constatation pour la base LungCancer où BiTM obtient 1, CTWC 0.718, NBVD 0.875 et CUNMTF 0.843. Nous observons aussi la difficulté d'obtenir de grandes valeurs de l'indice pureté pour la base isolet5.
Comme indiqué dans le tableau 6, BiTM fournit un indice de rand similaire et même meilleur que celui obtenu par les autres méthodes dans la majorité des cas.
Le tableau 7 présente les résultats expérimentaux obtenus avec BiTM, CTWC, NBVD et CUNMTF avec l'indice NMI. Notre approche BiTM fournit les plus hautes valeurs de l'indice NMI pour toute les bases de données excepté pour la base glass. 
Visualisation
Dans cette section nous montrons l'apport visuel de l'approche proposée. Il est clair que BiTM se base sur les visualisations intuitives des cartes auto-organisatrices. Les figures 1(a), 1(b) et 1(c) sont des cartes de BiTM obtenues à partir du jeu de données isolet5. La figure 1(a) est dédiée à la visualisation de la base de données organisées en fonction des groupes de lignes et de colonnes. Cette figure peut être obtenue par toute méthode de bi-partitionnement. Cependant, en utilisant cette visualisation, il est difficile d'analyser les blocs ou les bi-clusters obtenus. Afin de faciliter cette tâche, nous proposons de visualiser les bi-clusters en utilisant l'organisation topologique du modèle de BiTM. Ainsi, chaque cellule de la carte est associée à la partition des observations et des variables. Cette organisation est illustrée par la figure 1(b). Par exemple, les groupes en haut à gauche de la carte ont des valeurs faibles représentées par des couleurs bleues. Tandis que ceux avec de fortes valeurs (au milieu en bas) sont représentés par des couleurs plus vives (rouge). La figure 1(c) indique la cardinalité de chaque cellule. Les cellules sont représentées par un carré dont la taille varie proportionnellement avec le nombre d'observations associées.
Il est également possible de zoomer sur chaque cellule de la carte pour analyser l'organisation des observations et des variables dans chaque cellule. Les résultats obtenus en zoomant sur la carte 1(b) sont représentés dans la figure 2. Comme nous utilisons la notion du voisinage dans le modèle BiTM, nous constatons que la couleur est relativement similaire lorsque les variables sont proches.
Finalement BiTM a l'avantage de proposer une visualisation de la base de données et des bi-clusters. Ce résultat permet aux utilisateurs/experts une meilleure compréhension de la cohérence des données.
Conclusion et perspectives
Nous avons constaté après l'étude comparative avec des méthodes de partitionnement et de bi-partitionnement que BiTM est une méthode de bi-partitionnement efficace. La princi-  pale nouveauté du BiTM est l'utilisation d'un modèle topologique pour organiser la matrice des données en blocs homogènes, tout en prenant en compte simultanément les lignes et les colonnes. La série d'expériences que nous avons réalisées nous ont permis de valider notre méthode et d'analyser ses performances à partir de nombreux critères. Ces résultats expéri-mentaux démontrent que notre algorithme identifie les bi-clusters et a de bonnes performances par rapport à certains algorithmes de classification croisée. Nombreuses sont les perspectives qu'offre notre approche telle que l'amélioration de l'utilité de BiTM en l'adaptant à des données binaires et mixtes.

Introduction
L'alignement d'ontologies se positionne comme une pierre angulaire du Web sémantique. Il facilite la réconciliation des ressources décrites par des ontologies différentes. Ce processus permet la production des correspondances entre les entités de deux ontologies. Dans ce contexte, une multitude de méthodes d'alignement ont émergé ces dernières années (Euzenat et al., 2011). Ces méthodes réussissent à produire une bonne qualité d'alignement en se basant sur une configuration adéquate des paramètres. Ces paramètres sont fixés en amont au processus d'alignement. Cependant, un tel paramétrage peut s'avérer parfois non adéquat puisqu'il ne prend pas en considération la nature intrinsèque des ontologies. A titre d'exemple, le système FALCON-AO (Hu et Qu, 2008) est un outil d'alignement comportant 21 paramètres différents, qui peuvent être fournis. Ces paramètres posent un problème quant à la détermination de leur combinaison optimale avant d'entamer la phase d'alignement.
En outre, il est à noter qu'un paramétrage particulier ne peut pas être universellement optimal. Dans ce qui suit, nous proposons une nouvelle approche pour le paramétrage automatique d'une méthode d'alignement. Cette approche repose sur l'exploitation de l'intégrale de Choquet dans le but de déterminer une configuration optimale des paramètres au cours du processus d'alignement en fonction des ontologies à aligner. L'opération de paramétrage est caractérisée par un aspect totalement automatique et ne nécessitant pas l'intervention de l'utilisateur. Ainsi, les résultats encourageants, fournis après la phase d'évaluation, montrent que le processus de paramétrage automatique s'avère très intéressant.
L'intégrale de Choquet
L'intégrale de Choquet est considérée comme un opérateur d'agrégation (Kaci, 2011). Il permet l'amélioration de la puissance de l'analyse multicritères par la prise en compte de l'interaction entre les critères (Grabisch, 1996). En effet, cette notion permet de modéliser les phénomènes d'interaction entre les critères et la dépendance préférentielle. Elle utilise des mesures floues pour prendre en compte l'importance relative de chaque critère ainsi que les interactions mutuelles entre eux.  
. ? a ?(nc) représentent les indices permutés des critères.
La plupart des méthodes d'agrégation multicritères se basent sur la somme pondérée, qui met en valeur l'importance de chaque critère indépendamment. L'intégrale de Choquet se base sur la notion d'interaction pour la résolution des problèmes multicritères. Elle permet de tenir compte de l'importance de chaque critère mais aussi de l'importance relative entre ces derniers. Pour cela, il faut distinguer la notion de l'importance globale de chaque critère et l'importance relative due à son interaction avec les autres. Outre les propriétés usuelles des opérateurs d'agrégation et la modélisation de l'importance relative des critères, la famille de l'intégrale de Choquet a la distinction de permettre la représentation de phénomènes d'interaction mutuelle qui peuvent exister. Cependant, pour interpréter le comportement de l'intégrale de Choquet, nous sommes amenés à calculer deux indices, à savoir : l'indice de Shapley et l'indice d'interaction entre les critères.
L'importance globale d'un critère i n'est pas déterminée uniquement par la mesure floue µ(i), mais elle prend en compte toutes les mesures µ(D) pour un sous-ensemble D ? N c de toutes les coalitions d pour i ? D. En effet, nous pouvons avoir, µ(i) quasiment nulle suggérant que le critère i est sans importance. Cependant, nous pouvons avoir en joignant i à une coalition D ? N c , une valeur de µ(D ? {i}) qui soit plus grande que µ(D) suggérant ainsi l'importance du critère i dans la décision. Le calcul de l'importance globale se base ainsi sur la notion d'indice de Shapley, issue de la théorie des jeux coopératifs (Grabisch, 1996). Pour tout critère i l'indice de Shapley est défini par : 
d'interaction entre les critères i et j est la moyenne de la quantité de synergie entre i et j en présence d'un groupe de critères D : 
Agrégation de similarité par l'intégrale de Choquet
La forme la plus simple de l'agrégation est la moyenne arithmétique ou pondérée. Ce type d'opérateur n'est pas adapté pour l'agrégation des mesures de similarités vu qu'il exige que les mesures donnent des valeurs de façon indépendante. De toute évidence, cette condition n'est pas satisfaite. Ces mesures présentent une très grande interaction entre elles. Ainsi, l'utilisation de la moyenne pondérée peut conduire à un résultat biaisé car un groupe de mesures très similaires peut facilement submerger d'autres. Par conséquent, l'utilisation de l'intégrale de Choquet avec une fonction de capacité appropriée permet d'éviter ce problème. L'étape primordiale dans l'utilisation de l'intégrale de Choquet est de modéliser l'interaction entre les mesures via une fonction de capacité adéquate. Pour réaliser cette tâche, nous avons opté à l'utilisation de l'approche proposée par (Marichal et Roubens, 2000). Les auteurs ramènent le problème à un programme linéaire qui tient compte des contraintes préférentielles du décideur. Pour maximiser la valeur de la correspondance, V corr = ? 1 .SimT erm + ? 2 .SimT opo + ? 3 .SimSemant (composante terminologique SimT erm, composante topologique SimT opo et composante sémantique SimSemant), suivant les propriétés de chaque ontologie, il faut tenir compte d'un certain nombre de contraintes : -(i) si les valeurs de SIMTERM et SIMTOPO sont plus importantes que celle de SimSemant, alors il est préférable de les favoriser, sinon favoriser SIMSEMANT ; -(ii) si la valeur de SIMTOPO est la plus petite, alors favoriser SIMTERM et SIMSEMANT ; -(iii) si la valeur de SIMTERM est la plus petite, alors favoriser SIMTOPO et SIMSEMANT.
Chacune de ces contraintes modélise l'interaction positive ou négative qui peut exister entre les critères SIMTERM, SIMTOPO et SIMSEMANT. Les préférences sus décrites peuvent être représentées en utilisant un système linéaire. En outre, l'utilisateur est amené à assigner un poids à chaque critère qui reflète son importance relative dans le cadre de chaque contrainte. En effet, la solution optimale à notre problème d'agrégation revient à résoudre à ce système linéaire sur notre ensemble N c = {SimT erm, SimT opo, SimSemant} :
La première contrainte donne une importance relative aux composantes terminologique et topologique ensemble. Pareillement, dans le cas d'une faible contribution de la composante topologique, la deuxième contrainte favorise la coalition entre la composante terminologique et celle sémantique. La troisième contrainte favorise la jointure entre SIMTOPO et SIMSEMANT au détriment de SIMTERM ou toute autre jointure. Les trois dernières inégalités favorisent une seule composante parmi les trois, à condition que l'interaction entre les deux composantes restantes soit négative. La résolution du système résultant de l'ensemble de ces contraintes a été menée en utilisant Kappalab R package 1 (Grabisch et al., 2008).
Étude expérimentale
L'agrégation par l'intégrale de Choquet a été appliquée sur trois mesures de similarité ré-sultant des trois modules d'alignement. La base Benchmark 2 de la campagne OAEI 2012 a été utilisée pour évaluer notre méthode de paramétrage. La base Benchmark comporte 111 tests. Chaque test permet d'évaluer la puissance de la méthode d'alignement sur un aspect particulier. Cette base comporte 4 sous cas, qui diffèrent selon la taille, tout en gardant les mêmes caractéristiques. Nous avons opté à l'utilisation de la sous base FINANCE. Les tests sont systématiquement dérivés d'une ontologie de référence et introduisent à un certain nombre de fluctuations. Ces modifications permettent d'évaluer le comportement de la méthode d'alignement face aux changements subis par les ontologies objet du processus d'alignement. En effet, ces ontologies montrent des altérations qui peuvent être catégorisées selon 6 niveaux, à savoir : -(i) les noms d'entités peuvent être supprimés, remplacés par des synonymes ou traduits ; -(ii) les commentaires peuvent être supprimés ou traduits ; -(iii) les liens hiérarchiques peuvent être supprimés, étendus (i.e., par rapport à l'ontologie 101) ou aplatis ; -(iv) les instances peuvent être supprimées ; -(v) les propriétés peuvent être supprimées ou ayant leurs restrictions de classes éliminées ; -(vi) les classes peuvent être multipliées ou réduites.
Une interaction positive (ou de moins en moins négative), reflète un indice de Shapley aussi important. La figure 1, montre que pour l'ontologie 101, SIMTERM a une importance qui dépasse SIMTOPO et SIMSEMANT. Avec l'absence des noms d'entités, la famille 20x marque une importance élevée pour SIMTOPO dépassant ainsi SIMTERM et SIMSEMANT. Les ontologies des familles 22x et 23x se distinguent par une morphologie structurelle pauvre (hiérarchie aplatie ou supprimée), ce qui défavorise la coalition de SIMTOPO avec les autres composantes. Les indices de Shapley pour les familles 24x, 25x et 26x sont très proches, vu que toutes les composantes interagissent négativement à ce niveau. De point de vue mesure de similarité, les trois composantes à la fois sont incapables de fournir de bonnes valeurs, puisque les ontologies des familles 24x, 25x et 26x sont doublement altérées, i.e., terminologiquement et structurellement. Comme l'illustre la figure 2, et par rapport à un paramétrage figée, la moyenne de l'amélioration varie entre 6% (la famille 25x) et 12% (la famille 20x).
FIG. 1 -Indices de Shapley par familles de tests.
FIG. 2 -Les valeurs de F-Mesure par famille de tests.

Introduction
La classification non supervisée est un problème étudié depuis plusieurs décennies, et récemment de nouvelles approches ont été développées pour s'adapter au challenge induit par les nouvelles méthodes d'acquisition automatique des données et le nombre croissant d'application produisant des données massives. Ces données doivent être étudiées par des algorithmes suffisamment efficaces afin de pouvoir exploiter les connaissances qu'elles contiennent. En procédant à une classification non supervisée, on cherche à construire des ensembles homogènes d'individus, c'est-à-dire partageant un certain nombre de caractéristiques identiques. Classiquement, les méthodes fonctionnent de la façon suivante : l'utilisateur fixe le nombre de classes, un partitionnement est ensuite généré puis évalué par l'utilisateur même ou par des critères d'homogénéité, le partitionnement obtenu pouvant être remis en question selon son évaluation. Nous proposons une approche différente qui consiste à présenter et évaluer une par une les classes sans en fixer préalablement le nombre. L'approche générique est basée sur un processus itératif qui va extraire les classes les unes après les autres permettant ainsi l'exploration pas à pas des données. L'approche propose à l'utilisateur en priorité les classes les plus pertinentes (selon un critère donné) et lui laisse le soin de décider quand arrêter le processus. L'approche peut être intégrée dans un système interactif qui lui permettra d'étudier les classes individuellement ou les unes par rapport aux autres. Cet article est organisé de la façon suivante. Nous allons d'abord présenter notre approche itérative ainsi que les critères utilisés pour l'extraction de classes puis présenter certains de nos résultats expérimentaux avant de conclure.
Méthode proposée
Nous proposons une approche itérative générique qui va extraire les classes les unes après les autres pour permettre une exploration pas à pas des données. Le processus itératif est répété à la demande de l'utilisateur. À chaque itération, une nouvelle classe est extraite : une méthode d'optimisation (un extracteur) recherche la meilleure classe à extraire selon un critère d'é-valuation donné afin d'obtenir un sous-ensemble de données homogène et séparé des autres objets. La classe est alors proposée à l'utilisateur, qui va pouvoir l'analyser à l'aide d'outils de visualisation interactifs. L'utilisateur pourra ensuite demander au système d'extraire une nouvelle classe, pour poursuivre son exploration des données. L'extraction d'une nouvelle classe prendra en compte les classes extraites précédemment.
Extraction d'une classe
Il existe plusieurs méthode pour extraire une classe homogène à partir d'un ensemble de données. Dans cet article, nous introduisons une méthode d'extraction de classes basée sur la détection de limite de classe. Une classe est extraite à partir d'un « centre » : nous calculons la distance entre chaque objet et le centre de la classe et cherchons alors la première augmentation abrupte dans ces valeurs qui indiquera la limite de la classe extraite. Un algorithme d'optimisation pourra être utilisé pour chercher un centre de classe, qui produit une classe homogène selon un critère donné. Nous avons testé deux méthodes pour détecter la limite de la classe, la méthode CUSUM Basseville et Nikiforov (1993) et une méthode de détection de pics présentée dans Palshikar (2009), appliquée sur le différentiel des distances. Une fois la limite de classe évaluée, tous les objets qui ont une distance inférieure à cette limite appartiennent à la classe extraite.
Évaluation d'une classe
Pour proposer à l'utilisateur les classes les plus pertinentes, il faut définir des critères d'évaluation. La plupart des critères d'évaluation en classification non supervisée évaluent l'ensemble de données dans son intégralité et ne donnent pas une évaluation des classes indépendamment les unes des autres. Nous proposons donc deux nouveaux critères d'évaluation pour évaluer les classes indépendamment les unes des autres, dérivés de critères classiques en classification non supervisée. Dans l'ensemble des critères définis, C k représente la k-ième classe extraite. Le premier critère est le rapport d'inertie IR (rapport entre l'inertie intra-classe et l'inertie totale des données, normalisé par le nombre d'objets dans la classe et dans l'ensemble de données :
Le second critère proposé est le rapport de limite de la classe CLR, représentant le rapport entre la distance du dernier objet de la classe sur la distance du premier objet hors de la classe :
Nous pouvons alors utiliser l'un ou l'autre des critères qui produisent une évaluation de la compacité d'une classe sphérique. Cependant, comme chaque classe est extraite individuellement, nous devons également nous assurer que les classes extraites sont différentes les unes des autres. Nous proposons donc d'ajouter une pénalisation des classes selon leur chevauchement avec les classes précédemment découvertes. Le critère de pénalité OP calcule une pé-nalité selon l'intersection et l'union de la classe extraite avec les classes précédentes (? ? 0 représente le poids de la pénalité selon l'importance que l'on donne aux chevauchements).
Méthode de détection de limite
Une première expérimentation a servie à étudier l'impact de la méthode de détection de limite de la classe. Nous avons, pour cela, utilisé des données artificielles en deux dimensions constituées de trois classes (figure 1), deux étant relativement proches, la dernière plus éloignée. Pour cet ensemble de données, nous illustrons la méthode de détection de limite de la classe centrée à l'origine et avons donc calculé les distances de chaque points à l'origine. Sur la figure 1, nous pouvons voir en haut les nuages de points des classes et en bas le graphique des distances calculées. La méthode de détection de limite de classes consiste, alors, à appliquer la méthode CUSUM (figure 2) ainsi que la méthode de détection de pics (figure 3) sur les distances. On remarque sur la figure 2 que la méthode CUSUM permet de détecter seulement le plus grand changement (minimum de la fonction CUSUM) qui ne correspond pas forcément à la limite de la classe considérée. Cependant, comme nous pouvons le voir sur la figure 3, l'approche basée sur les pics du différentiel de distances, permet de détecter plusieurs changements brusques sur les distances, il nous suffit, alors, de choisir le premier pic détecté pour déterminer la limite de la classe considérée.
Processus itératif
Une seconde expérimentation a servi à évaluer l'approche itérative d'extraction de classes avec les différents critères proposés. Nous avons donc appliqué notre approche sur un ensemble de données artificiel simple à deux dimensions pour en expliquer aisément le fonctionnement. Les données ont été créées avec quatre distributions gaussiennes, trois d'entre elles étant proches les unes des autres, la quatrième plus éloignée (cf. figure 4). Nous avons appliqué notre approche en utilisant la méthode de détection de pics pour déterminer la limite de la classe, le critère d'évaluation CLR + OP . Sur la figure 5, nous pouvons voir la première classe extraite, il s'agit de la classe isolée. Nous voyons, sur la figure 6, la deuxième classe extraite, qui représente un regroupement de trois petites classes proches les unes des autres. En continuant le processus, les trois classes (figures 7, 8 et 9) constituant la classe 2 sont extraites l'une après l'autre. On remarque alors (haut de la figure) que la pénalité de chevauchement s'applique sur ces classes puisqu'elles sont contenues dans la classe 2. Nous avons ensuite appliqué notre algorithme avec le critère d'évaluation IR, sans modifier les autres paramètres. Sur la figure 10, on observe les cinq classes obtenues. Ici les trois classes proches les une des autres ont été découvertes individuellement avant la classe qui les englobe. Blansché (2012) la méthode CUSUM a été utilisée pour détecter la limite de la classe. Une fois une classe extraite dans un sous-ensemble de données, des méthodes de visualisation peuvent être utilisées pour l'étudier. Nous présentons dans la figure 11, la première classe extraite sur l'ensemble de données Ionosphere de l' UCI Blake et Merz (1998)  
Application au biclustering
Conclusion
Nous avons proposé dans cet article une nouvelle approche de classification non supervisée où les classes sont obtenues les unes après les autres suivant un processus itératif sans préciser Concernant les travaux futurs, nous avons prévu de mettre en place une plate-forme interactive d'exploration pas à pas des données. L'approche propose à l'utilisateur en priorité les classes les plus pertinentes (selon un critère donné) et lui laisse le soin de décider, permettant ainsi à l'utilisateur d'étudier les classes individuellement ou les unes par rapport aux autres (treillis, visualisation,. . .).

Introduction
L'intégration de données vise à combiner des données issues de sources différentes afin de permettre leur exploitation à travers une interface d'interrogation appelée schéma global. Le travail que nous présentons ici est une des phases de réalisation d'un système d'inté-gration de type médiation (approche non matérialisée) sémantique. Ce dernier est un triplet J = G, S, MM (Lenzerini (2002)) où, dans notre proposition, le schéma global G et les schémas locaux S sont décrits par des ontologies. Nous construisons le schéma global semiautomatiquement à partir des schémas sources, ce qui permet de définir naturellement un mapping GAV. Pour autant, comme montré dans Niang et al. (2011), notre construction incrémen-tale de G induit que l'ajout et le retrait de sources ont un coût de mise à jour de G très limité. Nous montrons ici qu'en plus cela n'a aucun effet sur le mapping que nous définissons, car ce mapping n'est calculé qu'au moment où une requête doit être évaluée, en fonction de l'état courant de G et uniquement pour ce qui concerne la requête.
Notre contribution vise une intégration dynamique au sein de communautés web. Une source peut rejoindre le système intégré le temps d'échanger ses données ou de contribuer à une tâche particulière et peut se retirer à tout moment. Les collaborations qui se multiplient dans de nombreux domaines suscitent la constitution de solides ontologies de référence, publiquement accessibles. C'est dans ce cadre que nous avons mis au point dans Niang et al. (2011) notre construction du schéma global, en exploitant une ontologie de référence. Ce schéma global est une TBox spécifiée en DL-Lite A , une logique de description (LD) (Calvanese et al. (2007)) connue pour traiter des requêtes dans un système d'intégration avec une complexité LOGSPACE en la taille des données interrogées. Nous exploitons dans cet article cette LD pour la ré-écriture d'une requête q, exprimée dans les termes de G, en des sous-requêtes ciblant des sources susceptibles d'y répondre.
Processus de ré-écriture de requêtes
Notre proposition dans Niang et al. (2011) pour la construction semi-automatique d'une ontologie globale pouvant servir de support d'intégration dans un système de médiation, synthétisée dans la figure 1, est scindée en deux phases. D'abord, une ontologie intermédiaire appelée Accord (notée A dans la figure 1) est générée pour chaque source. Elle décrit la partie de l'ontologie locale concernée par le processus d'intégration. Cette phase s'appuie sur des techniques d'appariement d'ontologies : ontologie source OL et ontologie de référence OM . Ensuite, chaque Accord est automatiquement et incrémentalement concilié dans l'ontologie globale OG. La conciliation est faite en liant sémantiquement les concepts locaux via une taxonomie calculée à partir de l'ontologie de référence OM . Les sources restent ainsi indépendantes les unes vis-à-vis des autres tout en étant liées sémantiquement dans l'ontologie globale. Notre proposition a été d'abord construite dans le cadre du projet SIC-Sénégal ayant pour but de permettre à plusieurs partenaires travaillant sur le bassin du fleuve Sénégal de partager et intégrer leurs données, entre autres agricoles. Dans l'exemple donné dans le tableau 1 1 , l'ontologie globale G est représentée sous forme d'une TBox T g telle que T g = {T ai }, T m , où (i) {T ai } est l'ensemble des TBox des accords A si construits à partir des ontologies locales des sources S i , et (ii) T m est calculée à partir de l'ontologie de référence pour concilier les différents T ai dans T g . Ce tableau 1 montre un extrait de T g conciliant T a1 et T a2 , des deux accords A s1 et A s2 . Les expressions de T a1 et T a2 expriment le fait que les sources S 1 et S 2 , à partir desquelles ont été respectivement construits A s1 et A s2 , détiennent des informations sur (1) des quantités de production de variétés de culture, ici : tomate, oignon et sorgho, et (2) des statistiques agricoles concernant des tomates et du riz paddy, lesquels ont un attribut prix. Les expressions de T m , située en bas du tableau, concilient T a1 et T a2 par le fait que, par exemple : oignon de T a1 et tomate de T a2 sont des légumes, sorgho de T a1 et riz de T a2 sont des céréales, les céréales et les légumes sont des produits végétaux, mais sont disjoints.
TAB. 1 -Extrait d'une TBox globale T g conciliant les Accords de deux sources.
Considérons dans ce qui suit (i) l'ontologie globale G, dont T g est celle montrée dans le tableau 1 ; (ii) les deux sources S 1 et S 2 dont les accords A s1 et A s2 sont ceux conciliés par T g , et (iii) la requête conjonctive suivante exprimée dans les termes de G :
q(x, a, b) ? Legume(x), prix(x, a), production(x, b) Cette requête demande le prix et la production de légumes. Nous décrivons dans la suite comment nous pouvons y répondre en reportant l'interrogation sur les sources S1 et S2.
Définition de règles servant au calcul des mappings
En partant uniquement des informations présentes dans T a1 et T a2 des Accords A s1 et A s2 (tableau 1), ni S 1 ni S 2 ne peuvent répondre à q, ne serait-ce que parce que le prédicat Legume de q n'est défini dans aucun de leur vocabulaire. Cependant, si on considère T m -taxonomie située dans la partie basse du tableau 1 et reliant les concepts des T ai , montrant dans notre exemple que les oignons et les tomates sont des légumes -il devient envisageable que S 1 et S 2 puissent permettre de construire une réponse pour q. La première étape de notre processus de ré-écriture est d'utiliser les relations sémantiques définies dans T g = {T ai }, T m afin de ramener les atomes d'une requête q, exprimée sur T g , vers les sources qu'ils impliquent (S i est représentée par T ai ). Pour ce faire, nous définissons un ensemble de règles sémantiques qui permettent par un raisonnement par chainage avant de déduire de T g les expressions de mapping correspondant aux atomes de la requête. Ces expressions sont de la forme : Appliquées en chainage avant, les règles fournies en haut de la prochaine page permettent de calculer un ensemble M g d'expressions de mapping pour chaque atome g de q. A chaque fois, l'atome g est utilisé comme déclencheur du raisonnement et T g comme base de faits initiale. Le résultat est un ensemble d'expressions M g de la forme g(v) ? S i (?(w)).
Les règles (1) à (3) correspondent au cas où l'atome est un concept atomique A(x). Dans (1), concept(A, T ai ) est vrai si A apparaît en position de concept dans une expression de T ai .
Dans ce cas, S i peut renseigner sur A, ce qu'indique l'expression de mapping en conclusion de la règle. Ainsi toutes les sources contenant ce concept seront considérées par le processus de ré-écriture. (2) et (3) permettent de rechercher des sous-concepts qui seraient dans les sources. Grâce à elles, en partant de l'atome Legume(x) on peut arriver à Tomate(x) de S 2 et Oignon(x) et Tomate(x) de S 1 . Les règles (4) à (7) peuvent se comprendre comme les trois premières mais s'appliquent à un attribut atomique U (x, y). Elles servent à rechercher d'éventuels sousattributs au sein d'une même source. La règle (7) permet, dans une même source, d'interroger l'attribut d'un concept sur ses sous-concepts. Les règles (8) à (13) concernent un rôle atomique P (x, y), les 2 premières fonctionnant comme (1) pour le concept ou (4) pour l'attribut : elles s'appliquent s'il y a dans une T ai une expression impliquant ce rôle. (10) et (11) permettent de rechercher des sous-rôles (dans une même source) quand (12) et (13) servent à interroger des rôles sur des sous-concepts (dans une même source). Nous donnons ci-après les mappings obtenus en appliquant les règles (1) à (13) pour chaque atome de la requête q :
Les mappings de M prix ne ramènent qu'à S 2 qui est capable de fournir le prix d'instances de tomate et de riz, tandis que les mappings de M production indiquent que la source S 1 est capable d'apporter des réponses sur la production de variétés de cultures, d'oignons et de tomates. Les réponses qui peuvent être obtenues à partir de S 1 et S 2 sont partielles puisque S1 ne peut fournir que la production (de tomate ou d'oignon) et S 2 que le prix (de tomate ou de riz). C'est en combinant les différents mappings que l'on aura une réponse globale pour q, c'est l'objet de la prochaine sous-section.
Dépliage de requêtes globales et validation des ré-écritures obtenues
Le dépliage d'une requête q(x) ? g 1 (z 1 ), ..., g n (z n ) par rapport à un ensemble de mappings GAV g i (x i ) ? q i (x i , y i ) est la requête u obtenue en remplaçant dans q chaque atome g i (z i ) par q i (? i (x i , y i )), où ? i est une fonction qui associe x i à z i , et y i à de nouvelles variables. Chaque dépliage u sert à calculer une partie du résultat que le système intégré peut fournir pour q. L'union de ces parties constitue l'ensemble des réponses à q. L'ensemble des dépliages u de q est obtenu en calculant le produit cartésien R = M g1 × ... × M gn . Chaque requête q r de ce produit est une ré-écriture candidate. En considérant les mappings obtenus précédemment pour les atomes de la requête q, résumés dans le tableau 2, certaines des 18 ré-écritures candidates de q sont les suivantes :
TAB. 2 -Résumé des sources pouvant répondre pour chaque atome de la requête q.
Une ré-écriture candidate ne constitue pas forcément une ré-écriture valide de q. A titre d'exemple, q r15 qui demande pour S 1 la production de tomates et pour S 2 le prix d'oignons n'est pas valide. Les tomates et les oignons sont certes des légumes, mais l'expression Tomate ¬ Oignon déclarée dans la TBox T g indique que les tomates et les oignons sont disjoints. Pour vérifier la consistance des dépliages, nous utilisons l'algorithme Consistent de Calvanese et al. (2007) qui construit des requêtes booléennes q unsat à partir des inclusions négatives présentes dans T g , qui représentent des contraintes d'intégrité servant à assurer la consistance des réponses apportées par le système d'intégration. Considérons ci-après les requêtes q unsat obtenues à partir de T g du tableau 1 et qui concernent les atomes de q : q unsat1 ? Tomate(x), Oignon(x) -car Tomate ¬ Oignon est dans T g q unsat2 ? Tomate(x), Riz_paddy(x) -inféré de Legume ¬ Cereale q unsat3 ? Oignon(x), Riz_paddy(x) -idem Afin de vérifier la consistance d'une ré-écriture candidate q r , nous remplaçons d'abord dans q r les requêtes sur les sources par les requêtes correspondantes sur le schéma global. Dans notre contexte il suffit de retirer toute mention des sources. Soit q r le résultat de cette étape, il faut ensuite appliquer l'algorithme Consistent en évaluant les requêtes q unsat sur le corps de q r considéré comme une instance canonique. Si le résultat de l'évaluation d'une des requêtes q unsat est true alors la ré-écriture q r est invalide. A titre d'exemple, soit la requête q r15 suivante, obtenue à partir de q r15 : q r15 (x,y,z)?Tomate(x), prix(x,y), Tomate(x), production(x,z), Oignon(x) 

Introduction
Actuellement, de nombreuses recherches (Stein et al., 2007) traitent de la recherche de similitudes notamment à cause de l'augmentation importante du plagiat sous toutes ses formes et dans tous les domaines : l'enseignement avec les élèves et étudiants (quatre étudiants sur cinq déclarent avoir recours au copier-coller), la recherche scientifique avec les publications et thèses (Bao et Malcolm, 2006) (plagiat de thèses notamment) et l'industrie avec les problèmes de copie de brevets ou de codes sources. Les outils existants pour rechercher des documents similaires sont principalement basés sur la recherche de segments dits n-gram (n représentant la taille en mots du segments) identiques (Oberreuter et al., 2010) pour détecter les copies et commencent tout juste à proposer la détection de copie par traduction dite la copie inter-langue (Kent et Salim, 2009) à travers les travaux de ces dix dernières années. L'approche proposée consiste à prendre en compte le document comme une agrégation de documents plus petits et récursivement que chacun des documents le composant soit lui-même l'agrégation de documents plus petits. Cette hiérarchie permet de déterminer des mots-clés à chacun des niveaux et ainsi détecter des similitudes normalement indétectables à l'échelle globale. Cette approche repose sur l'hypothèse que lorsqu'on paraphrase ou reformule un texte, on garde le sens de celui-ci et ainsi on garde les mots-clés principaux, porteurs du plus haut niveau sémantique du texte.
Après avoir présenté rapidement l'état de l'art et l'approche, nous décrivons d'abord comment extraire et utiliser des mots-clés, puis nous présenterons les niveaux hiérarchiques proposés (taille, organisation, obtention). Enfin, nous présentons l'évaluation de notre approche en la comparant à la méthode classique n-gram.
2 État de l'art et approche
La notion de similitude
Une similitude est un rapport, une relation qui existe entre deux choses semblables. Cela peut aller de la simple ressemblance jusqu'à l'identité. Lorsqu'on parle de similitude textuelle ou de document similaire, on distingue plusieurs types de similitudes allant de la ressemblance à l'identité : la reformulation qui consiste à reprendre la sémantique d'un texte et à l'exprimer différemment ; la paraphrase qui consiste à reprendre les éléments d'un texte, dans l'ordre d'origine mais en les formulant différemment normalement dans le but d'éclairer, d'expliciter, ou pour développer certains points ; la citation qui consiste à faire la copie mots à mots d'une portion de texte en informant le lecteur de l'origine extérieure de celle-ci ; la copie qui consiste à faire la copie mots à mots d'un texte ou d'une partie d'un texte sans citer la source ; et la traduction qui consiste à faire la copie mots à mots d'un texte ou d'une partie d'un texte sans citer la source et en traduisant dans une autre langue. Selon la langue, la forme finale est donc potentiellement différente de l'originale.
Recherche et comparaison de documents
La recherche de documents s'effectue selon deux approches. La première est basée sur le style de l'auteur, elle est dite stylistique (Iyer et Singh, 2005), part du postulat (Jardino et al., 2005) que deux textes du même auteur contiennent un grand nombre de correspondance et qu'inversement, deux textes de deux auteurs différents contiennent un petit nombre de correspondances. On recherche de documents présentant un style identique ou approchant en utilisant la structure de phrase, la grammaire ou par observation stylistique (Stamatatos, 2009). Cette approche est plutôt récente et bien que très rapide, elle pose encore un certain nombre de problème notamment sur l'aspect de la précision. La seconde approche est basée sur le contenu du document (White et Joy, 2004;Iyer et Singh, 2005;Eissen et Stein, 2006)  
Notre approche
Notre approche consiste à extraire sur chaque document les mots-clés principaux d'un document dans sa globalité mais également dans les sous-documents construits par découpages successifs. Pour chaque niveau hiérarchique, on combine les mots-clés 3 par 3 pour former des triplets de recherche.
Extraction et utilisation de mots-clés
La notion de mots-clés
Le terme mot-clé désigne de manière générale un mot qui a une importance particulière et il est notamment utilisé lors des recherches d'informations. Il existe plusieurs manières d'extraire des mots-clés afin de garantir leur pertinence. La plus 'traditionnelle' est la méthode fréquentielle. Plus un mot-clé apparaît souvent, plus il est important. En ajoutant le regroupement par racine d'un même mot (regroupement de 'cheval' et de 'chevaux' sous le lemme 'cheva*' par exemple) et en n'utilisant que les mots les plus informatifs sémantiquement, on obtient les mots-clés théoriquement les plus représentatifs du contenu sémantique d'un texte.
Les groupes de mots-clés
A partir de la liste des mots-clés trouvés, il est important de savoir si il faut chercher les documents similaires avec un, deux, trois ou plus, mots-clés combinés. Afin de déterminer ce nombre, nous avons utilisé une petite base de documents sur laquelle nous avons extrait les 10 principaux mots-clés extraits sur l'analyse de l'intégralité du document. Puis ils ont été utilisés pour retrouver le document.
Le tableau 1 présente la pertinence de recherche d'un document par nombre de mots-clés utilisés dans la recherche. Le nombre de requêtes de recherche variant d'un minimum de 100 requêtes à un maximum de 25200. Au final, on observe un bon compromis entre résultats et nombre de requêtes effectuées en utilisant des groupes de 3 mots-clés et avec des résultats allant de 72 à 87% de récupération du document tout en étant à 120 requêtes par document. Le passage à 4 mots-clés ne fait gagner qu'0,5% de récupération supplémentaire alors qu'il double le nombre de requêtes (passage de 120 à 210 par document).
Niveaux hiérarchiques 4.1 Méthodes de segmentation
Il existe de nombreuses manières de procéder à la segmentation d'un texte. Bien qu'il existe des méthodes indépendante du formalisme, ou des méthodes par modèles thématiques, les principales sont encore le segmentation sémantique qui consiste à découper en fonction de bloc logique lié à la rédaction ou à la lecture (découpage par parties, par paragraphes, par sous-paragraphes, par phrases, découpage par ponctuation) et la segmentation dimensionnelle qui consiste à découper en bloc de dimension donnée sans prendre en compte le contenu du document. Cette segmentation peut elle-même être effectuée de manière absolue (découpage par bloc de 1000 mots, 500 mots...) ou de manière relative (découpage par bloc couvrant 25%, 10%...).
Taille et pertinence des différents niveaux
Les mots-clés sont extraits principalement par méthode fréquentielle ce qui implique qu'il soit nécessaire d'avoir un nombre minimum de mots pour générer des mots-clés. Statistiquement, des segments de moins de 40 mots ne comportent jamais de mots-clés intéressants, ceux de moins de 70 mots très rarement et au mieux, des mots représentés 2 fois au maximum. En effet, les auteurs ont tendance à éviter de faire des répétitions dans des zones trop proches. Ainsi, la méthode fréquentielle ne peut s'appliquer que sur des segments d'au moins 70 mots. Il est impossible de travailler avec des phrases, rarement avec des paragraphes. La taille minimale des segments doit donc être entre 70 et 100 mots afin d'obtenir un nombre minimal d'occurrences des mots.
Distribution des segments
Dans le cas de la segmentation dimensionnelle absolue, on peut légitimement se demander la pertinence de la position d'un segment de taille donnée dans l'ensemble du document qui pourrait avoir une multitude de positions différentes. Une approche naïve consiste à positionner un découpage donné comme une sous-partie directe d'une partie plus grande et de procéder par découpage successif c'est à dire un découpage par partie disjointe. Dans le but de couvrir des segments différents et potentiellement intéressants, une alternative est d'utiliser la taille du segment pour créer un masque de récupération que l'on utilise en balayant le texte pour géné-rer plusieurs segments non disjoints i.e. avec une intersection non vide. Dans l'approche par segments non disjoints, il convient de définir le déplacement du masque (pas de masque) ou le nombre maximal de parties acceptables par niveau hiérarchique (le pas étant alors égal au rapport taille du document / nombre de parties). L'expérimentation montre qu'un pas de masque du tiers de la taille du document permet d'obtenir des résultats satisfaisants sans augmenter de manière trop importante le nombre de segments générés.
Évaluation et tests
La base de tests et protocole
La base de tests est composée de 200 textes de 2500 mots environ dont on dispose des sources effectivement utilisées pour les écrire. Certains reprises sont de la copie au mot près (copie ou citation), d'autres sont des paraphrases ou même des reformulations. La taille des sections copiées allant de la simple phrase (6-8 mots) à celle d'une partie complète (1000 mots). On peut avoir plusieurs reprises dans un même document. Les sources utilisées sont disponibles en ligne et d'origine diverses : Wikipédia, publications de recherche, mémoires de stage, article de presse. Le protocole est composé d'une extraction de mots-clés de manière hiérarchique sans et avec glissement puis la recherche de documents en utilisant les triplets de mots-clés parmi ceux extraits. L'ensemble des documents utilisés est accessible librement sur internet. La recherche est également effectuée en utilisant la méthode traditionnelle des n-gram avec n égal à 8. Les résultats sont présentés dans le tableau 1 et permettent d'observer les différents résul-tats avec l'approche mots-clés et avec l'approche n-grammes. L'approche mots-clés présente plusieurs points plutôt positifs. Elle permet la détection des documents similaires présentant des similitudes supérieurs à 100 mots à 90% bien qu'il y ait des disparités entre types de document (83% copie, 69% paraphrase, 37% reformulation) mais aussi entre sources (94% internet, 75% articles et 60% mémoire). Elle permet également de trouver des similitudes de type paraphrase ou reformulations impossible à trouver avec l'approche n-gram (sauf si inférieur à 100 mots). Enfin, elle est trois fois plus rapide que l'approche n-gram en calcul et 40% plus rapide dans l'exécution des requêtes moteur (simplicité de requête). L'ajout du glissement permet l'amélioration des résultats de l'ordre de 5% mais en ajoutant environ 5% de requêtes.
Résultats

Introduction
Le contexte de nos recherches est celui de l'aide à l'observation de l'activité sur simulateurs pleine échelle du groupe EDF utilisés pour la formation et le perfectionnement des agents de conduite de centrale nucléaire (Pastré, 2005), (Champalle et al., 2011). Ce projet de recherche est mené avec l'Unité de Formation Production Ingénierie (UFPI) d'EDF. L'UFPI forme les personnels d'EDF dans les métiers de la production d'électricité. Parmi ses formations, l'UFPI forme des opérateurs de conduite de centrales nucléaires. Pour cela, les formateurs organisent des séances de simulation sur simulateurs pleine échelle, ré-plique intégrale à l'échelle 1 des salles de commande des centrales. Durant les simulations, les formateurs pilotent le simulateur et observent les réalisations des opérateurs. Ces derniers doivent réaliser un transitoire. Il s'agit de faire passer le simulateur d'un état initial e0 à un instant t0 à un état final n à tn. Pour cela, les opérateurs conduisent l'installation selon des actions attendues organisées en familles d'objectifs pédagogiques elles mêmes déclinées en objectifs pédagogiques de plus bas niveaux. Ces actions sont réparties dans des grilles d'observation complétées par les formateurs (Agency, 2004). L'observation et l'analyse des interactions individuelles et collectives des opérateurs est une activité critique et particulièrement dense (Samurçay et Rogalski, 1998). Afin de limiter la surcharge cognitive inhérente à ces tâches, les simulateurs disposent d'outils permettant d'enregistrer l'activité des opérateurs tels que les journaux de bord, la vidéo, la téléphonie, etc. Les données collectées permettent aux formateurs de revenir sur les difficultés rencontrées par les opérateurs afin de leur apporter des solutions pour améliorer leurs pratiques. Ces données sont cependant difficilement exploitables dû à leur grande quantité et à leur très bas niveau néces-sitant une expertise forte que ne possède pas les jeunes formateurs. Notre objectif est donc de proposer des modèles et des outils afin d'apporter aux formateurs une aide à l'observation à l'analyse et au débriefing des activités des opérateurs. Pour cela, notre approche est basée sur la modélisation, la représentation et la transformation des traces d'activités des simulations. Cet article est organisé comme suit : la section 2 présente un état de l'art et un positionnement par rapport aux systèmes d'observation et d'analyse des activités de opérateurs dans des environnements de formation. La section 3 présente le principe de notre approche d'observation et d'analyse des activités des opérateurs. La section 4 détaille les modèles de trace et de transformation que nous proposons. La dernière section présente nos conclusions et perspectives.
Travaux relatifs
Exploiter les traces numériques de l'activité de opérateurs est une pratique répandue dans les environnements informatisés dédiés à la formation. Dans cette partie nous étudions diffé-rentes approches exploitant les traces numériques d'activité suivant deux angles :
-l'assistance aux formateurs dans l'observation, l'analyse et la conduite du débriefing ;  (Dunand et al., 1989), assiste le formateur dans le débriefing des évaluations des agents de conduite d'EDF. Basé sur un système expert, SEPIA enregistre les paramètres du simulateur et les actions des opérateurs afin de fournir, lors du débriefing, une correction du scénario. Le projet PPTS (Pedagogical Platoon Training System) (Joab et al., 2002) assiste des formateurs dans le suivi et l'analyse des manoeuvres des équipages de simulateurs de char LE-CLERC. PPTS utilise un ITS afin d'exploiter les traces numériques de la simulation et faire émerger les niveaux de compétences attendus : technique, tactique et stratégique. A la fin de la simulation, PPTS génère une synthèse et des remarques sur les compétences des équipages. Les outils présentés ci-dessus utilisent les traces numériques de l'activité des stagiaires afin de diagnostiquer et analyser leurs comportements. Ils sont cependant basés sur des systèmes fermés dont la mise en oeuvre est généralement lourde, et nécessitent une longue et étroite collaboration avec les experts. Leurs connaissances sont de plus "statiques" et il n'est pas possible pour le formateur  -M-trace première : les observés sont issus des données collectées par les sources de traçage du simulateur tels que les journaux de bord et les annotations vidéo ; -M-trace des objectifs pédagogiques : les observés représentent les attendus que les opé-rateurs doivent valider tels que "acquitter l'alarme" ou "ajuster la pressions" ; -M-trace famille d'objectifs pédagogiques : les observés correspondent aux objectifs gé-néraux de la formation tels que "conduire l'installation" ou "travailler en équipe". Ces niveaux de m-trace sont obtenus via des transformations à base de règles. Comme le montre la figure 1, chaque observé d'une trace de niveau n est en relation avec ses observés d'origines de la trace de niveau n ? 1. Les observés de la M-trace première, sont quant à eux en relations avec les données collectées par le simulateur.
FIG. 1 -Principe d'analyse par transformation et visualisation de traces
Une telle organisation permet aux formateurs d'explorer, d'analyser et de mieux comprendre les raisons, collectives ou individuelles, des réussites et des échecs des opérateurs pour préparer et conduire les sessions de débriefing. Par exemple, si le formateur veut comprendre les raisons pour lesquelles l'observé "Gestes professionnels" de la trace "famille d'objectifs pé-dagogiques" est KO 2 (voir la figure 1), il peut naviguer dans ses différents observés origines, à savoir "Régulation tension alternateur", "Utiliser les bonnes consignes", "Régulation tempé-rature" et "Pression ajustée". Selon la règle 9, pour que l'observé "Gestes professionnels" soit OK, il faut que tous ses observés origines soient OK. Dans la mesure où un de ces observés est KO, l'observé "Gestes professionnels" est KO lui aussi.
Modèle de trace et de transformation.
Quel que soit le niveau de la trace, son modèle et le simulateur, nous considérons qu'une m-trace doit embarquer son identité afin d'être localisable et exploitable dans le temps pour des traitements statistiques ou des analyses à grande échelle. Ainsi, toute m-trace possède un identifiant unique, une date de début et de fin, un niveau, un type de simulateur, un type de formation (initiale ou maintien de compétences), une catégorie de formation (sommative ou formative) ainsi qu'un scénario de simulation (ilotage, perte d'alimentation, etc.) ( figure 2(1)). Chaque type d'observé recensé est caractérisé par un identifiant, une date de début et de fin, un label, l'identifiant du sujet générateur, sa nature (évalué ou non), son rôle (opérateur, superviseur,etc.) et un attribut de réalisation (OK ou KO). Ce modèle peut être spécialisé pour ajouter des attributs liés à l'action tracée. Afin de permettre l'exploration des niveaux de traces, chaque observé possède un lien vers sa règle et ses observés origines, ou vers les données collectées par les sources de traçage s'il s'agit de la trace première.
Une transformation génère une trace cible de niveau n à partir d'une trace source de niveau n ? 1. Comme le montre la figure 2(2), les transformations sont basées sur un ensemble de 
Conclusion et perspectives
Cet article traite de la problématique de l'observation et de l'analyse de l'activité sur simulateur pleine échelle. Ce travail de recherche, mené en partenariat avec l'UFPI d'EDF, est appliqué dans le cadre des formations et du maintien de compétences des opérateurs de conduite de centrale nucléaire. L'objectif est de de proposer des modèles et des outils pour assister les formateurs dans les phases de préparation et de conduite des débriefings. L'approche que nous avons proposée consiste à transformer les traces premières, issues des données collectées par le simulateur, afin d'extraire des informations de haut niveau sur l'activité des opérateurs. Nous avons pour cela proposé des modèles de trace et de transformation adaptés aux spécificités de nos propositions telles l'exploration de M-Traces et la saisie et le partage des connaissances d'observations des formateurs. Sur la base de nos modèles, nous avons également conçu un prototype, appelé D3KODE (Define, Discover, and Disseminate Knowledge from Observation to Develop Expertise), pour stocker, transformer et visualiser les traces. Ce prototype à été éva-lué sur la base d'un protocole comparatif mené avec une équipe de 8 formateurs de l'UFPI. Le dépouillement et l'analyse des résultats est en cours de réalisation. Nos travaux futurs visent à traiter le deuxième objectif du projet qui concerne l'exploitation des traces pour le retour d'expérience afin d'affiner les besoins et optimiser les programmes de formation des années à venir.

Introduction
La sélection de variables joue un rôle très important en classification lorsqu'un grand nombre de variables sont disponibles. Ainsi, certaines variables peuvent être peu significatives, corrélées ou non pertinentes. La sélection de variables permet également d'accélérer l'étape d'apprentissage et de réduire la complexité des algorithmes. Une méthode de sélection repose principalement sur un algorithme de recherche et un critère d'évaluation pour mesurer la pertinence des sous-ensembles potentiels de variables.
En apprentissage supervisé, la sélection de variables a largement été étudiée car il est connu que la sélection de variables peut améliorer la qualité d'un classificateur (Zhang et al., 2009). Parmi les méthodes supervisées, nous citons le coefficient de corrélation de Pearson (Rodgers et Nicewander, 1988), le score de Fisher (Duda et al., 2000) et le gain de l'information (Cover et Thomas, 2006). La sélection de variables a reçu peu d'attention en apprentissage non supervisé en comparaison au cas supervisé. Le problème devient plus difficile en raison de l'absence des étiquettes des classes pour guider la sélection. Ainsi se pose la question importante, comment évaluer la pertinence d'un sous-ensemble de fonctionnalités sans avoir recours aux étiquettes de classe ? Dans la littérature deux approches sont souvent utilisées pour éva-luer la pertinence d'un sous-ensemble de variables sélectionnées, (Kohavi et John, 1997;Yu et Liu, 2003) : l'approche de type filtrage (filter approach) et celle de type enveloppante (wrapper approach). Les approches enveloppantes évaluent les variables en utilisant un algorithme d'apprentissage qui sera finalement utilisé dans le processus de classement. Cependant, les mé-thodes enveloppantes sont généralement coûteuses en temps et ne peuvent pas être appliqués sur de grandes masses de données, (Kohavi et John, 1997). Ce sont les méthodes de type filtrage qui nous ont intéressés, car elles sont beaucoup plus efficaces. Les critères d'évaluation sont totalement indépendants du discriminateur utilisé. Les variables sont alors traitées avant le processus d'apprentissage. Les travaux de (Caruana et Freitag, 1994;John et al., 1994;Koller et Sahami, 1996) sur la sélection de variables montrent les différentes approches traitant ce problème d'optimisation. Parmi les méthodes de sélection d'attributs dans un contexte non supervisé, nous nous sommes intéressés principalement au score Laplacien qui est le critère d'évaluation le plus utilisé dans la littérature.
Plusieurs travaux ont tenté d'exploiter le principe du Score Laplacien (SL). Dans (Benabdeslem et Hindawi, 2011), les auteurs proposent une variante du SL qui utilise deux types de contraintes semi-supervisé sur les données : des contraintes Must-Link et des contraintes Cannot-Link. Ce score calcule la variance entre les données qui n'ont pas la même étiquette. Dans (Cai et al., 2010) les auteurs ont proposé un nouveau score appelé MCFS. Cette méthode vise à sélectionner les variables de manière à conserver la structure multi-cluster des données. MCFS mesure les corrélations entre variables d'une manière non supervisée, c'est une méthode efficace pour traiter de grande dimension, mais limitée par le choix du nombre de classes.
Dans (Zhang et hua Zhou Songcan Chen, 2007) les auteurs utilisent le même principe en proposant une nouvelle méthode appelée SSDR (Semi-supervised dimensionality reduction). Cette approche préserve la structure des données et utilise des contraintes semi-supervisés dé-finies par les utilisateurs. D'autres auteurs proposent une méthode de sélection de variables semi-supervisé en combinant des scores calculés sur la base de données étiquetées et non éti-quetés (Kalakech et al., 2011). La combinaison est simple, mais peut considérablement biaiser le résultat pour les variables ayant un meilleur score dans le cas supervisé et celles ayant de mauvais scores pour la partie non supervisée et vice-versa.
L'hypothèse sous-jacente au score Laplacien est que la structure des données dans l'espace des attributs est localement préservée dans l'espace d'attributs de sortie. En représentant cette structure par les graphes de similarité ou de distance, des données similaires dans l'espace d'entrée doivent aussi l'être quand elles sont projetées sur un vecteur d'attributs pertinents. Inspirés des travaux récents en classification non supervisée hiérarchique et aussi du modèle de classification hiérarchique AntTree (Azzag et al., 2003), nous nous sommes intéressés à l'étude du score laplacien auquel nous avons intégré de nouvelles contraintes non supervisées hiérarchiques. Le score que nous définissons est appelé SLH (Score laplacien hiérarchique). La principale contribution que nous proposons est d'utiliser une approche de construction de graphe, autre que celle qui se base sur le k-N N où k est fixé a priori. Dans notre approche nous utilisons un algorithme de classification hiérarchique autonome où la structure d'arbre fournie permet de définir un nouveau score intégrant des contraintes non supervisées basées sur l'arborescence.
2 Score Laplacien sous contraintes hiérarchiques 2.1 Le score Laplacien Soit un ensemble de N observations X = {x 1 , ..., x N }. Une observation x i est un vecteur de m dimensions (variables), f ri désigne le i ème échantillon de la r ème variable, r = 1, ..., m. Ainsi, nous définissons la r ème variable par
T . Le score Laplacien sélec-tionne les variables pertinentes qui préservent au mieux la structure locale et qui produisent de grandes valeurs de variances. Nous supposons que les données appartenant à la même classe soient proches les unes des autres. Le SL de la r ème variable doit être ainsi minimisé avec la fonction suivante (He et al., 2005) :
Le score Laplacien Hiérarchique
L'idée de notre approche pour la sélection de variables est d'utiliser le principe des k-plus proches voisins fournis par la structure d'arbre d'AntTree (Azzag et al., 2003). Dans la littéra-ture, de nombreux algorithmes d'apprentissage ont été proposés pour découvrir des structures sous-adjacentes dans les données en construisant un graphe de voisinage pour effectuer une analyse spectrale, (Belkin et Niyogi, 2001;Roweis et Saul, 2000;Tenenbaum et al., 2000). L'algorithme AntTree a l'avantage d'être complètement autonome et d'avoir une complexité très faible de ?(n log n). Dans le modèle AntTree, chaque noeud de l'arbre (interne ou feuille) représente une donnée x i . Ainsi, les noeuds de l'arbre seront successivement ajoutés du plus haut niveau vers les niveaux inférieurs (figure 1). Toutes les données doivent passer un test de similarité où leur propriété de voisinage est vérifiée.
Soit une observation x i qui va se connecter à un noeud de l'arbre x pos si et seulement si cette action augmente la valeur de T Dist (x pos ). T Dist (x pos ) est la valeur maximale de distance (distance euclidienne) entre les noeuds fils de x pos . La règle consiste à comparer x i avec son plus proche x i + (x i + est un noeud de x pos ). Dans le cas où les deux noeuds sont suffisamment
, alors x i se connecte à x pos . Sinon, x i se déplace vers x i + . Ainsi T Dist augmente localement à chaque fois qu'un noeud se connecte à l'arbre.
Avec le nouveau score SLH, nous souhaitons définir un algorithme complètement autonome pour la sélection de variables. L'algorithme SLH est essentiellement basé sur le score Laplacien auquel nous avons ajoutés des contraintes hiérarchiques. Ainsi, au lieu d'utiliser le graphe des k plus proches voisins, nous proposons d'utiliser AntTree qui avec sa structure hié-rarchique fournira automatiquement pour chaque observation connectés à x i . La figure 1 montre un exemple où l'observation x 1 a quatre voisins (k 1 = 4), x 2 a seulement deux voisins du niveau inférieur (k 2 = 2). En utilisant cette topologie, la nouvelle matrice d'adjacence est définie comme suit :
Ainsi, le critère SLH du score Laplacien sous contraintes hiérarchiques est défini comme suit :
2 S ij , nous donnons l'avantage aux attributs respectant la structure hiérarchique. En maximisant i (f rj ? ? r ) 2 , le score SLH sélectionne les variables ayant les plus grandes valeurs de variance locale, et qui sont les plus représentatives de la topologie de l'arbre construit. Le processus de démonstration est le même que celui présenté dans (He et al., 2005). L'Algorithme 1 présente les trois étapes nécessaires pour la sélection de variables par SLH.
Expérimentations
Dans cette section, plusieurs expérimentations ont été réalisées sur plusieurs bases de données réelles. Ces expérimentations sont présentées en deux parties : la qualité du clustering et la qualité de la classification supervisée en utilisant l'algorithme du plus proche voisin (1-N N  
Comparaison dans un cadre non supervisé
Pour évaluer la qualité du clustering, nous utilisons deux mesures : la pureté et l'Information Mutuelle Normalisée (NMI -Normalized Mutual Information) (Strehl et al., 2002) ; chacune doit être maximisée. Pour faciliter la comparaison entre les méthodes nous appliquons l'algorithme K-means sur la base de données en prenant en compte que les variables sélectionnées.
Dans ces expérimentations, pour construire le graphe k-NN du SL nous fixons le paramètre k = 5. Nous évaluons ensuite la qualité du clustering avec différentes valeurs pour le nombre de clusters de la manière suivante : Les figures 2 jusqu'à 6 montrent les courbes des performances sur le clustering (Pureté et NMI) par rapport au nombre de variables sélectionnées. De manière générale notre approche SLH obtient de meilleurs résultats par rapport aux autres méthodes. Dans la figure 2, nous observons que l'algorithme SLH fournit des résultats raisonnables par rapport aux critères de Pureté et du NMI. Pour les données Coil20 (Fig. 3), SLH est meilleur en terme de Pureté et NMI lorsque le nombre de variables est au alentour de 50 à 100 pour les trois expérimentations. Nous notons que pour K = 30, notre algorithme est nettement meilleur que les autres. De plus, dans la figure 4, la pureté de SLH augmente de manière constante. Les mêmes remarques sont observées pour la base Sonar (Fig.5). Pour Soybean (Fig.6) SLH atteint une valeur de Pureté = 100% pour la plupart des cas (K = 4, 6 et 8) en utilisant seulement 9 variables.
Dans les tables 3.1 jusqu'à 3.1, nous résumons les résultats du clustering obtenus sur toutes les bases testées. Les résultats numériques obtenus avec la base AR10P (Tab. 3.1) montrent une amélioration des performances du NMI de SLH. Pour la Pureté, SLH est de même qualité que SL et MaxVariance. Dans le tableau 3.1 nous remarquons que les résultats fournis par coil20, pour 20 clusters et 100 variables sélectionnées, donnent une valeur de NMI de 68% pour SLH, ce qui est mieux que si on avait utilisé les 1024 variables (66,0%). Dans le tableau 3.1, les résultats de Pureté et du NMI en utilisant 100 variables ne sont pas les meilleurs, mais restent proches de ceux utilisant 617 variables. Pour la base Sonar SLH est la seule méthode qui permet d'obtenir de meilleurs résultats.     
Comparaison dans un cadre supervisé
Dans cette partie nous souhaitons évaluer les différents critères de sélection de variables en utilisant le classifieur 1-NN. Pour chaque donnée x i , nous cherchons le plus proche voisin
. L'erreur de classification est calculée comme suivant : 
Conclusions et perspectives
Etudier la sélection de variables en mode non supervisé est un vrai challenge pour la communauté scientifique en raison du manque d'informations sur les labels des données. Pour relever ce défi, nous avons proposé une approche autonome de sélection de variables nommée SLH, qui est une variante du score Laplacien et utilise la structure et la topologie de l'arbre défini par AntTree. Notre algorithme est autonome et ne nécessite aucun paramètre. Les ré-sultats expérimentaux sur plusieurs jeux de données montrent que l'algorithme SLH réalise des performances plus élevées en mode supervisé et non supervisé. Comme perspective, nous nous sommes fixés comme objectif d'introduire de nouvelles contraintes non supervisées hié-rarchiques pour la sélection de variables. L'idée est d'utiliser un autre type de lien dans le graphe qui représenterait des liens faibles.

Travaux existants
Le subspace clustering est un domaine assez récent (Parsons et al., 2004), (Kriegel et al., 2009) qui vise à déterminer conjointement les clusters et leurs sous-espaces associés. Contrairement aux approches classiques de clustering dans lesquelles, la phase de partitionnement peut être précédée d'une phase de sélection ou de pondération des attributs, le subspace clustering ne dissocie pas la définition de l'espace et celle du groupe de données. En conséquence, une donnée peut théoriquement appartenir à plusieurs clusters, dès lors que ceux-ci sont définis dans un sous-espace qui leur est propre.
Le subspace clustering a été défini dans deux principaux travaux (Parsons et al., 2004), (Kriegel et al., 2009) qui clarifient la terminologie et distinguent le subspace clustering d'autres domaines proches comme le biclustering et le coclustering. On distingue plusieurs méthodes de subspace clustering en fonction du mécanisme de sélection des attributs lors de la construction des clusters. Certains algorithmes reposent sur des mécanismes de pondération des attributs, d'autres recherchent tous les sous-espaces potentiels de manière ascendante (des espaces de 1 dimension vers l'espace contenant toutes les dimensions) ou inversement descendante.
Les méthodes qui reposent sur un mécanisme de sélection/pondération des attributs appartiennent au domaine du soft subspace clustering (Gustafson et Kessel, 1979), (Candillier et al., 2005). L'idée principale de ces méthodes est d'affecter un poids à chaque attribut et d'utiliser une optimisation alternée pour rechercher un maximum local à une fonction objectif. Il existe toutefois de nombreuses limitations à ces méthodes : définition du nombre de clusters a priori, pas de garantie d'une convergence vers un optimum global, affectation (éventuellement floue) de chaque donnée à un (ou plusieurs) cluster(s) défini(s) dans un unique sous-espace.
D'autres approches reposent sur une exploration systématique de tous les sous-espaces éli-gibles en partant des sous espaces les plus petits. Ces approches ascendantes sont basées sur un mécanisme de recherche d'itemsets fréquents. Par exemple, l'algorithme CLIQUE (Agrawal et al., 1998) intègre un mécanisme d'agrégation de sous-ensembles denses de basse dimensionnalité pour retrouver les sous-ensembles denses de plus haute dimensionnalité. Toutefois, la complexité de ce type d'algorithme est grande par rapport au nombre d'attributs.
À l'inverse des méthodes ascendantes, les méthodes descendantes commencent par étudier l'ensemble des attributs avant de déterminer et de sélectionner les attributs caractéristiques pour réduire le nombre de dimensions. Ce type d'algorithme est efficace lorsque la répartition des données vérifie l'hypothèse de localité définie dans Kriegel et al. (2009) : "une sélection locale des données suffit à estimer une orientation locale des données".
Cette définition de localité repose sur des calculs de type k plus proches voisins qui utilisent l'ensemble des attributs pour définir le voisinage local. Cette hypothèse ne semble pas pertinente dans la pratique car, dans le cas d'un espace de grande dimension, de nombreux attributs non caractéristiques affectent le calcul du voisinage et donc le choix des attributs caractéristiques. De nombreux algorithmes utilisent l'heuristique des k plus proches voisins (Achtert et al., 2007), (Friedman et Meulman, 2004). Plusieurs paramètres sont estimés localement pour chaque cluster comme l'orientation du voisinage, et utilisés ensuite pour agréger au cluster les données vérifiant une relation de proximité. Toutefois, comme précédemment, ces algorithmes affectent une donnée à un unique cluster et son sous-espace.
Enfin, l'algorithme CASH  diffère des approches descendantes précé-dentes car il ne repose pas sur l'hypothèse de localité. Dans ce modèle, les clusters sont modéli-sés par des hyperplans. L'espace des hyperplans contenant au minimum une donnée est divisé en grille et parcouru afin de déterminer quels sont les hyperplans contenant de nombreuses données. La réitération de ce calcul et la modélisation en hyperplan permet de construire les sous-espaces. Cette méthode possède cependant une complexité rédhibitoire.
Nous décrivons dans la section suivante le modèle de l'algorithme SNOW qui, comme l'algorithme CASH ne repose pas sur l'hypothèse de localité et possède une complexité moindre.
3 Algorithme Snow SNOW est un algorithme qui détermine à chacune de ses itérations un cluster et son sousespace associé. Chaque itération est indépendante des précédentes et repose sur un processus en 4 étapes principales : (1) la génération aléatoire d'un cluster potentiel ; (2) la détermination de l'hyper-cube propre à ce cluster potentiel ; (3) le calcul d'un pas de densité des données pour chacun des attributs ; (4) l'extension de l'hyper-cube à partir du pas de densité pour obtenir un cluster maximal.
Génération aléatoire d'un cluster potentiel. Contrairement aux approches de l'état de l'art qui déterminent les clusters à partir du voisinage d'une seule donnée, notre approche se base sur une sélection aléatoire de plusieurs données pour former la graine du premier cluster potentiel. Cette sélection de plusieurs données amène plus de robustesse dans la détermination des attributs caractéristiques du cluster car, contrairement au voisinage local, elle permet de considérer des plages de valeurs plus importantes et d'être donc moins sensible aux variations locales de densité des attributs ou aux points aberrants. Enfin, à chaque itération, la distribution aléatoire initiale des points favorise l'émergence d'attributs caractéristiques différents ce qui assure une bonne couverture de l'espace des solutions. Comme CASH, SNOW recherche un modèle reliant les données et non les données mutuellement proches.
Détermination de l'hyper-cube du cluster potentiel. On définit l'hypercube H C du cluster potentiel C comme le produit des intervalles I C j sur chacun des attributs j de l'espace initial R m . Chaque intervalle I C j sur l'attribut j pour le cluster C est défini comme l'intervalle minimal englobant l'ensemble des valeurs des points x ? C sur l'attribut j : où x j désigne la valeur de l'attribut j du point x. À cette étape, on ajoute au cluster potentiel l'ensemble des données contenues dans l'hypercube.
Calcul d'un pas de densité. Cette étape vise à déterminer une densité locale au cluster potentiel. Pour chaque attribut j, on définit la séquence S I C j comme l'ensemble ordonné des valeurs x j de l'attribut j pour tout x ? C sur l'intervalle I C j . Le pas de densité ? j est ensuite simplement défini comme la distance maximale observée sur l'attribut j entre deux valeurs consécutives de S I C j (s i désigne le i ème élément de la séquence S I C j ) :
j Détermination du cluster et de l'hypercube maximal. Dès lors que le pas ? j est déterminé pour tout attribut j ? [1, m], notre algorithme agrège itérativement au cluster potentiel les points dont les coordonnées sont situées à une distance inférieure à ? j des frontières de son hypercube H C pour tous les attributs j. L'hypercube associé au cluster est ensuite mis à jour et le processus d'agrégation de nouveaux points est réitéré jusqu'à ce qu'aucun candidat ne puisse plus être ajouté au cluster, qui est alors maximal.
Paramétrage. L'algorithme SNOW repose sur deux paramètres fixés par l'utilisateur. Le premier est le nombre maximal d'itérations ? . Il permet d'optimiser la couverture, la qualité des clusters et de leurs sous-espaces associés par rapport au temps de calcul. Le second paramètre k est le nombre de données sélectionnées pour générer les graines de clusters potentiels. Une petite valeur de k diminue le temps de calcul mais une plus grande valeur de k permet une meilleure estimation de la densité des attributs du cluster potentiel et donc d'obtenir de meilleures performances en conjonction avec le paramètre ? .
Expérimentations
Cette section présente les deux expérimentations conduites pour valider notre algorithme. La première expérimentation propose la comparaison de notre algorithme SNOW avec la mé-thode COPAC (Achtert et al., 2007) à l'aide d'une mesure moyenne des F1-mesures des clusters découverts les plus pertinents. Nous utilisons l'implémentation de COPAC fournie par le framework ELKI . 2 bases de données sont utilisées : (1) un premier jeu de données artificiel de référence nommé P arsons (Parsons et al., 2004), dont les clusters rapprochés peuvent poser problème aux méthodes comme COPAC reposant sur l'hypothèse de localité ; (2) le même jeu de données modifié en séparant plus les clusters afin qu'il soit plus favorable à la méthode COPAC reposant sur l'hypothèse de localité.
La seconde expérimentation vise à évaluer la capacité de la méthode SNOW à produire des clusters interprétables sur la base de données réelles nommée Auto MPG (Quinlan, 1993).
Résultats comparatifs sur la base P arsons. Nous utilisons une méthode d'évaluation, qui consiste, à partir d'un ensemble de clusters cibles connus, à intégrer dans le score total chaque meilleur cluster généré par rapport à chaque cluster cible. Soit C 1 , . . . , C p la liste des clusters générés par l'algorithme et S 1 , . . . , S q les clusters cibles. Pour chaque cluster cible S i , on note C j le cluster ayant le meilleur score F1 par rapport à S i . Nous proposons comme score général de l'algorithme la moyenne des meilleurs scores F 1 par rapport à chaque cluster cible S i :
Discussion des résultats sur la base P arsons et P arsons modifiée. Les meilleurs résul-tats obtenus par l'approche COPAC lors de nos expérimentations sont de 49,84%. Ce résultat est dû à la proximité des clusters 2 à 2. En effet, le calcul du voisinage inclut un attribut dans le sous-espace, et par conséquent le calcul de l'orientation du voisinage intègre des données de clusters différents, faussant le résultat final. SNOW, quand à lui, n'est pas sensible à l'hypothèse FIG. 1 -Score de SNOW sur P arsons en fonction de ? et de k et par rapport à COPAC.
TAB. 1 -2 des clusters générés par SNOW sur la base Auto-MPG.
de localité. Pour k = 5 et ? = 10 5 (i.e. quand on génère 10 5 clusters candidats aléatoirement), le score est de 94.84%.
Pour P arsons modifiée, COPAC obtient un score de 93.16%. La proximité des clusters est donc bien la cause de la mauvaise performance de COPAC sur le premier jeu de données. SNOW obtient un score légèrement supérieur à COPAC, de 97.4%.
Jeu de données réelles Auto MPG. Contrairement aux données de P arsons, il n'existe pas d'étiquettes de clusters théoriques permettant une évaluation objective des résultats. Nous proposons donc, à l'image de la démarche suivie dans (Candillier et al., 2005), d'étudier avec cette base réelle la pertinence et l'interprétabilité des clusters découverts. SNOW produisant un grand nombre de clusters, nous avons retenu expérimentalement deux clusters parmi les plus denses dans leur sous-espace pour conduire notre interprétation. La densité des clusters a été calculée à partir du nombre de données du cluster divisé par le volume du cluster (la longueur des intervalles de l'hypercube est bornée pour ce calcul au minimum à 0.1). SNOW a été lancé avec comme valeurs de paramètres ? = 30000 et k = 10. Les clusters ayant un effectif inférieur à 50 sont supprimés et les clusters restants sont triés par ordre de densité décroissante. Les 10 premiers clusters de ce classement sont relativement homogènes. L'analyse rapportée dans le tableau 1 illustre les deux profils principaux de clusters ainsi découverts. Discussion des résultats de SNOW. D'après le tableau 1, le premier cluster représente le segment des petites voitures, légères et économiques, le deuxième cluster celui des grosses voitures plus puissantes. Toutefois, on remarque l'accélération, l'année et l'origine ne sont pas des attributs caractéristiques du premier cluster, tandis que pour le deuxième cluster seule l'accé-lération ne semble pas caractéristique. De manière inattendue et d'après SNOW, l'accélération n'est pas une caractéristique importante des voitures identifées comme puissantes.

Introduction
Au-delà de sa stricte définition d'entité administrative et politique, le territoire, selon Guy Di Méo, témoigne d'une "appropriation à la fois économique, idéologique et politique de l'espace par des groupes qui se donnent une représentation particulière d'eux-mêmes, de leur histoire, de leur singularité" (Di Méo (1998)). Dans ce contexte éminemment subjectif, la caractérisation et la compréhension des perceptions d'un même territoire par les différents acteurs est difficile, mais néanmoins particulièrement intéressante dans une perspective d'amé-nagement du territoire et de politique publique territoriale. Le travail présenté s'inscrit dans le cadre du projet Senterritoire 1 , qui adopte une démarche pluridisciplinaire, initiée à partir d'une méthode automatique et visant à fournir aux géographes et aux environnementalistes, une aide à la découverte de connaissances. Nos contributions portent, dans cette publication, sur l'accès à l'information spatiale et proposent (1) d'affiner et d'enrichir les patrons d'extraction d'informations existants dans la littérature afin d'améliorer l'identification du sens de l'entité spatiale extraite et (2) de définir une approche originale utilisant différentes techniques de fouille de textes afin de distinguer une entité spatiale d'une entité d'organisation. La suite de l'article est organisée de la façon suivante. En section 2, nous présentons les défini-tions préliminaires et les travaux du domaine. En section 3, nous décrivons la méthode hybride Text2Geo. En section 4, nous présentons les expérimentations réalisées sur le jeu de données du bassin de Thau et concluons dans la section 5.  (Maurel et al. (2011)). De nombreuses mé-thodes permettent de reconnaitre les ENs en général et les ES en particulier (Nadeau et Sekine (2007)). On trouve des approches statistiques consistant généralement à étudier les termes cooccurrents par analyse de leur distribution dans un corpus (Agirre et al. (2000)) ou par des mesures calculant la probabilité d'occurrence d'un ensemble de termes (Velardi et al. (2001)). Ces méthodes ne permettent pas toujours de qualifier des termes comme étant des ENs et notamment les ENs de type Lieu ou Organisation. On trouve également des méthodes de fouille de données fondées sur l'extraction de motifs. Ces derniers permettent de déterminer des règles de transduction utilisant des informations syntaxiques propres aux phrases pour repérer les ENs ). Pour la reconnaissance des classes d'ENs, de nombreuses approches s'appuient sur des méthodes d'apprentissage supervisé comme les SVM (Joachims (1998)). Les algorithmes exploitent divers descripteurs (positions des candidats, étiquettes grammaticales, informations lexicales, etc.) et des données expertisées/étiquetées. Dans cet article, nous combinons de telles méthodes d'apprentissage supervisé associées à des patrons linguistiques. phrase à partir de grammaires) ; (4) l'analyse sémantique (identifier de sens potentiel véhi-culé par des mots ou des groupes de mots sur la base des syntagmes retenus). Cette chaîne de TALN est définie avec Linguastream 4 qui intègre notamment l'étiqueteur grammatical TreeTagger 5 et le langage prolog pour la définition des grammaires DCG (analyses syntaxique et sémantique). Sur la base de la grammaire définie par (Lesbegueries (2007)) dans ces phases d'analyses syntaxique et sémantique, nous avons mis en place de nouveaux patrons dédiés à l'extraction des entités spatiales et des entités d'organisation. Cette extraction se fait selon deux étapes : L'étape 1 extrait les ESA qui constituent les types primitifs de notre processus d'extraction. Ces types primitifs sont soit des entités nommées de lieu (Montpellier, France...), soit des indicateurs spatiaux (la région, la ville) ou alors des indicateurs de relation (Le sud...). Ceci se traduit en logique par des règles comme :
Pour chaque règle définie ci dessus, "?" dans l'expression "ESA ? N omT oponymique" signifie que l'expression ESA est composée de l'expression NomToponymique, correspondant à un nom de lieu. Les deux premières définitions ESA sont récursives, ce qui permet de produire des patrons de tailles variables afin d'identifier des instances telles que : Les régions rurales du sud de la France, la ville de Madrid, les communes de l'agglomération du bassin de Thau. L'étape 2 extrait les instances les plus complexes : les entités spatiales relatives, composées d'une ESA et précédées d'une relation d'ordre topologique suivant des règles du type : ESR ? Relation, ESR. ; ESR ? Relation, ESA. Relation ? Adjacence |Orientation |Inclusion |Distance |F orme géometrique, Adjacence ? "prés" |"lapériphérie" |etc.
Dans cette chaîne de traitements, nous proposons deux contributions. Dans un premier temps, nous avons ajouté des règles à la grammaire afin d'améliorer l'identification des ESR et ESA. Dans un second temps, nous avons proposé un nouveau type de règles pour repérer de manière spécifique les entités nommées de type Organisation.
Définition de nouveaux patrons pour l'identification des ESA et ESR. Pour annoter les ENs spatiales, nous nous sommes appuyés sur la typologie classique du domaine qui identifie des sous-classes : les lieux géographiques naturels (lacs, mers, etc), les constructions humaines (buildings, installations, etc.), les axes de circulations (routes, etc.), les adresses (rue, code postal, etc.). Nous avons ajouté des règles (patrons) permettant d'améliorer l'identification des ESA et ESR. Par exemple, l'ajout d'un patron lié à la distribution des ES permet d'identifier le cas lié à la distribution des relations spatiales. Ainsi, dans la phrase Les environs de Lyon, Marseille..., nous identifions deux entités spatiales. Vers une méthode hybride. Nous proposons d'apprendre un modèle permettant de distinguer une entité de type Organisation et une ES. Pour cela, nous avons étiqueté manuellement un ensemble de phrases en deux classes correspondant aux deux types d'entités. Nous n'avons pas considéré les phrases dites ambiguës, c'est-à-dire présentant une ES et une Organisation. Pour l'apprentissage supervisé, nous avons utilisé la méthode classique SVM (Joachims (1998)). Les descripteurs utilisés sont les mots des phrases qui représentent un "sac de mots". Nous avons complété ces descripteurs en considérant les patrons définis dans les sections précé-dentes comme des descripteurs à part entière. Pour cela, dans la représentation vectorielle de nos textes, nous avons ajouté des attributs de type booléen signifiant qu'une phrase peut contenir un motif de type <ConceptOrg, Entité> (motif propre à une organisation) ou <ConceptSpa, Entité> (motif propre à une Entité Spatiale). ConceptOrg représente les prépositions typiques précédant une Organisation (avec, par, etc). ConceptSpa se décline en trois sous-concepts pré-cédant, en général, une Entité Spatiale : Préposition spatiale : en, sur, etc. ; Indicateur de relation : sud, vers, etc. ; Indicateur spatial : ville, région, etc. Cette représentation a deux avantages : 1) Elle donne plus de poids à certains mots propres au domaine de la Recherche d'Information Géographique (prépositions spatiales et d'organisation, indicateurs spatiaux et de relation). Dans un contexte plus général, de tels mots peu porteurs de sens sont souvent moins pris en compte voire supprimés ; 2) contrairement à l'approche sac de mots classique qui ne prend pas en considération l'ordre des mots, les nouveaux descripteurs prennent en compte un ordre partiel et se révèlent déterminants comme le montrent les expérimentations.  TAB. 2 -Classification des phrases.
Experimentations
Conclusion et perspectives
Dans le cadre du projet Senterritoire, nous avons proposé une méthode hybride qui permet l'extraction d'informations spatiales et la recherche d'informations. Ces approches exploitent 6. http ://www.cs.waikato.ac.nz/ml/weka/

Introduction
L'objectif de la détection de communautés dans les graphes, ou encore dans les réseaux sociaux, est de créer une partition des sommets, en tenant compte des relations qui existent entre ces sommets dans le graphe, de telle sorte que les communautés soient composées de sommets fortement connectés (Fortunato (2010)). Ainsi, les principales méthodes de détection de communautés proposées dans la littérature se concentrent sur la structure des liens, en ignorant les propriétés des sommets. Or dans de nombreuses applications, les réseaux sociaux peuvent être représentés par des graphes dont les sommets ont des attributs qui peuvent être pris en compte pour détecter plus efficacement les communautés. Ceci a conduit à revisiter cette probléma-tique afin d'opérer cette détection non seulement à partir des relations décrites par le graphe, mais aussi à partir d'attributs caractérisants les sommets et cela a donné lieu récemment à l'introduction de méthodes qui exploitent ces deux types de données (Moser et al. (2007);Zhou et al. (2009);Li et al. (2008); Cruz Gomez et al. (2011);Combe et al. (2012); Dang et Viennet (2012)).
Dans cet article, nous proposons ToTeM, une méthode de classification de graphes à vecteurs d'attributs qui reprend le principe de la méthode de Louvain, basée sur l'optimisation de la modularité, en l'étendant de façon à permettre la prise en compte d'attributs numériques d'une manière symétrique à ce qui existe pour les relations (Blondel et al. (2008)). Après avoir défini plus formellement le problème de la détection de communautés dans un réseau d'information dans la section 2, nous rappelons brièvement le principe de la méthode de Louvain et introduisons ToTeM dans la section 3 avant de décrire des critères globaux de partitionnement dans la section suivante.
Énoncé du problème et notations
Étant donné un graphe G = (V, E) où V = {v 1 , . . . , v i , . . . , v n } est l'ensemble des sommets et E ? V × V est l'ensemble des arêtes non étiquetées. On suppose que chaque sommet v i ? V est associé à un vecteur d i = (w i1 , . . . , w ij , . . . , w iT ) à valeurs réelles de sorte que G forme un réseau d'information (Zhou et al. (2009)). Dans un problème de partitionnement de réseau d'information, les liens et les attributs sont considérés, de telle sorte que d'une part il doit y avoir de nombreuses arêtes au sein de chaque classe et relativement peu entre elles et d'autre part, deux sommets appartenant à la même classe sont plus proches en termes d'attributs que deux sommets appartenant à des classes différentes. Ainsi, l'objectif est de partitionner l'ensemble V des sommets en r classes disjointes formant une partition P = {C 1 , . . . , C r } où r est a priori inconnu et de telle sorte que les sommets appartenant à un même groupe soient connectés et homogènes vis-à-vis des attributs. Dans la suite, on notera A la matrice d'adjacence de G telle que A ij indique la valuation de l'arête entre i et j si elle existe et vaut 0 s'il n'existe pas d'arête entre i et j. Le degré du sommet i, noté k i , est égal à j A ij et c i désignera la classe d'appartenance de i dans la partition P.
La méthode ToTeM
La méthode ToTeM que nous proposons est une extension de la méthode de Louvain qui consiste elle-même à optimiser le critère de modularité (Blondel et al. (2008); Newman et Girvan (2004)) :
où (i, i ) prend toutes les valeurs de V × V , m est la somme des poids de toutes les arêtes du graphe et ? est la fonction de Kronecker qui vaut 1 si ses arguments sont égaux et 0 sinon.
L'algorithme comporte deux phases. À partir de la partition discrète, la première phase consiste à essayer de déplacer successivement chaque sommet vers la classe de ses voisins et à l'affecter à la classe ayant apporté le plus fort gain de modularité. Lorsque plus aucune amélioration n'est possible, dans une seconde phase, un nouveau graphe pondéré est formé à partir des classes obtenues à l'issue de la première phase. Chaque classe devient un sommet du nouveau graphe et une arête entre deux sommets a pour poids la somme des poids des arêtes présentes entre des sommets contenus précédemment dans les classes correspondantes. Les deux phases sont répétées jusqu'à ce qu'il n'y ait plus de modification possible. Le gain de modularité induit par le déplacement d'un sommet isolé i vers une classe C l est égal à :
où in est la somme des poids des arêtes ayant leurs deux extrémités dans la classe C l , tot est la somme des poids des arêtes adjacentes aux sommets de C l , k i,in est la somme des poids des arêtes de i aux sommets de C l (Blondel et al. (2008)). La méthode ToTeM, que nous introduisons dans l'Algorithme 1, repose sur l'optimisation d'un critère global permettant de classer les sommets en se souciant à la fois de la qualité des classes d'un point de vue relationnel mais également du point de vue des attributs. Pour ce qui est de la qualité par rapport aux relations, on peut retenir la modularité. Le gain est alors mesuré suivant la formule 2. Pour ce qui est de la qualité par rapport aux attributs, plusieurs mesures sont envisageables comme le taux d'inertie inter-classes ou l'indice de Calinski, détaillés dans les sections suivantes.
Gain d'inertie et critères de qualité globale
Il est possible d'améliorer l'efficacité de l'algorithme ToTeM lorsque le critère global est basé sur l'inertie inter-classes d'une partition en remarquant que la variation d'inertie interclasses induite par la réaffectation d'un sommet peut être calculée uniquement à l'aide d'information locale. Étant donné V l'ensemble des n sommets du graphe représentés dans un espace vectoriel défini par les attributs et muni d'une distance euclidienne à laquelle est associée une norme A tout sommet x de V est associé un poids positif m x et, sans perte de généralité, on peut supposer qu'initialement tous les sommets ont le même poids. On note g le centre de gravité de V et pour toute classe C l de P, g l son centre de gravité et m l la somme des poids des éléments de cette classe C l . Considérons deux partitions P et P telles que P = (A, B, C 1 , . . . , C r ) et P = (A \ {x} , B ? {x} , C 1 , . . . , C r ). Par la suite, A \ {x} désigne la classe A privée du sommet x et B ? {x} la classe B augmentée du sommet x.
L'inertie inter-classes I inter (P) associée à la partition P est égale à :
. L'inertie inter-classes de la partition P obtenue en retirant x de sa classe A et en l'affectant à la classe B vaut :
La variation d'inertie inter-classes induite par le déplacement du sommet x de la classe A vers la classe B est donnée par :
g A\{x} et g B?{x} sont eux aussi calculés facilement en utilisant seulement l'effectif repré-senté par le sommet x et les classes A et B ainsi que leurs centres de gravités g A , g B :
Les valeurs des poids associés aux classes peuvent aussi être recalculées à l'aide de l'information locale :
4.1 Synthèse des informations du graphe et des attributs dans la seconde phase L'opération de synthèse des informations du graphe consiste, à l'instar de ce qui est opéré dans la méthode de Louvain, à fusionner les sommets affectés à une même classe de façon à n'en faire qu'un seul sommet. Ainsi, à partir de la partition P = (C 1 , . . . , C r ) obtenue à l'issue de la première phase, un nouveau graphe G = (V , E ) est crée. Ce graphe comporte autant de sommets qu'il y a de classes dans P et chaque sommet v l de V incarne une classe C l de P . La valuation de l'arête éventuellement présente entre les sommets v y et v z de V est égale à la somme des valuations des arêtes présentes entre des sommets de G appartenant aux classes C y et C z de P qui ont été représentées par v y et v z dans V . Soit ? la fonction qui indique, pour un sommet de V , par quel sommet de V il est représenté, alors le poids associé à une arête se calcule de la façon suivante :
Enfin, les arêtes internes aux classes de P deviennent des boucles dans G . De plus, il est nécessaire de transférer les informations relatives aux attributs sur le nouveau graphe G . Pour cela, on affecte les poids des classes d'origine aux sommets de destination et le centre de gravité de la classe d'origine devient le vecteur d'attributs du sommet de destination. Ainsi, pour tout sommet v
Critères de qualité globale
Le critère de qualité globale intervenant dans l'algorithme ToTeM doit être une fonction d'une mesure de qualité de la partition par rapport aux relations et d'une mesure de sa qualité par rapport aux attributs. La modularité peut être utilisée comme mesure de la qualité par rapport aux relations. Pour ce qui est de la qualité par rapport aux attributs, une première solution envisageable peut consister à prendre le taux d'inertie inter-classes. Ce qui conduit à une première mesure de qualité globale définie par :
où I(P) désigne l'inertie totale de V . Cependant, le taux d'inertie inter-classes n'est pas conçu pour comparer des partitions ayant un nombre de classes différent. En effet, il varie structurellement avec le nombre de classes de la partition de sorte qu'il est maximum pour la partition discrète. Une solution simple visant à palier ce biais structurel consiste à tenir compte du nombre de classes |P| de la partition pour définir un critère global :
Contrairement au précédent, ce critère donne un avantage aux partitions à faible nombre de classes. Une alternative pour palier cet inconvénient consiste à avoir recours à des indices conçus dans le but de déterminer le nombre de classes dans le cas du partitionnement de données vectorielles, comme par exemple l'indice de Calinski-Harabasz, celui de Dunn ou celui de Davies-Bouldin (Calinski et Harabasz (1974); Davies et Bouldin (1979)).
Une autre solution pour comparer deux partitions P et P de taille respective r et r consiste à utiliser la probabilité critique résultant de tests de comparaison de variance. En effet, sous l'hypothèse nulle selon laquelle les classes ne sont pas significativement différentes au sein de la partition P, la statistique F (P) définie par :
suit une loi de Fisher-Snedecor F (r?1, n?r) à (r?1, n?r) degrés de liberté où V inter désigne la variance entre les classes et V T la variance totale. On peut donc calculer la probabilité

Introduction
Tandis que dans les méthodes de fouille de données classiques, les données sont stockées dans une seule table, la Fouille de données mutli-tables (en anglais, Multi-Relational Data Mining, MRDM) s'intéresse à l'extraction de connaissances à partir de bases de données relationnelles multi-tables (Knobbe et al., 1999). Typiquement, en MRDM les individus sont contenus dans une table cible en relation un-à-plusieurs avec des tables secondaires. En apprentissage supervisé, une variable cible devrait être définie au sein de la table cible. La nouveauté en MRDM est de considérer les variables se trouvant dans les tables secondaires (variables secondaires) pour prédire la classe. Plusieurs solutions ont été proposée dans la littérature, notamment la Programmation Logique Inductive PLI (Džeroski, 1996) qui utilise le formalisme logique ou encore la propositionalisation qui opèrent par mise à plat afin de pouvoir utiliser un classifieur monotable classique (Kramer et al., 2001).
Dans cet article, nous introduisons un espace de modèles basé sur des itemsets de variables secondaires. Ces itemsets permettent de construire de nouvelles variables binaires dans les tables secondaires. Ensuite nous évaluons la pertinence de ces variables pour la tâche de classification supervisée. Afin de prendre en compte le risque de sur-apprentissage, qui augmente Un itemset permet de construire une nouvelle variable binaire A ? dans la table secondaire selon que les enregistrements de cette table sont couverts ou non par l'itemset. Le critère d'éva-luation proposé se décompose en la somme de deux termes : (i) un coût de codage évaluant la construction de l'itemset ?, et (ii) un critère qui estime la pertinence de la variable A ? par rapport à la variable cible, qui exploite l'approche d'évaluation des variables secondaires binaires introduite dans (Lahbib et al., 2011).
Le reste de cet article est organisé comme suit. La partie 2 introduit l'espace des variables construites à base d'itemsets de variables secondaires et présente son critère d'évaluation. Dans la partie 3 nous évaluons la méthode sur un jeu de données réelles. Enfin, la partie 4 conclut cet article et discute sur des travaux futurs.
Construction de variables à base d'itemsets
En se basant sur le modèle de classification de (Gay et Boullé, 2012), un itemset ? est une conjonction d'expressions de la forme (x ? S x ), où x est une variable secondaire et S x est soit un groupe de valeurs si x est catégorielle, soit un intervalle si x est numérique. À chaque itemset ? nous associons une variable secondaire binaire A ? . Celle-ci est évaluée à « vrai » pour les enregistrements secondaires couverts par ?, et à « faux » sinon. 
nombre d'intervalles (resp. groupes de valeurs) de la variable secondaire x ? X numérique (resp. catégorielle)
A priori hiérarchique d'un itemset de variables secondaires. Nous utilisons l'a priori hiérarchique défini ci-dessous. Soulignons qu'une distribution uniforme est utilisée à chaque étage 1 de la hiérarchie des paramètres des modèles.
1. le nombre k de variables secondaires qui constituent l'itemset est uniformément distribué entre 0 et m. 2. pour un nombre de variables k, chaque sous-ensemble de k variables qui constituent l'itemset est équiprobable dans un tirage avec remise. 3. pour une variable secondaire catégorielle qui figure dans l'itemset, le nombre de groupes est nécessairement 2 (I x = 2). 4. pour une variable secondaire numérique qui figure dans l'itemset, le nombre d'intervalles est soit 2, soit 3 de façon équiprobable. 5. pour une variable secondaire numérique (respectivement, catégorielle), et pour un nombre d'intervalles (respectivement, nombre de groupes) donné, toutes les partitions en I x intervalles (respectivement, en I x groupes de valeurs) sont équiprobables. 6. pour une variable secondaire catégorielle x appartenant à l'itemset, le choix du groupe de valeurs i x sur lequel porte la condition est équiprobable. 7. pour une variable secondaire numérique x appartenant à l'itemset, si la variable est discrétisée en deux intervalles, le choix de celui sur lequel porte la condition est équi-probable. Lorsqu'il y a 3 intervalles, celui qui figure dans l'itemset est nécessairement l'intervalle du milieu. En utilisant la définition de l'espace de modèles ainsi que sa distribution a priori, le coût de construction C c (A ? ) d'un itemset ? est donné dans l'équation 1.
1. Cela ne signifie pas que l'a priori hiérarchique est un a priori uniforme sur l'espace des itemsets, ce qui serait équivalent à une approche par maximum de vraisemblance.
Les deux premiers termes de l'équation 1 correspondent au choix du nombre de variables secondaires qui apparaissent dans l'itemset ainsi que le choix de ces variables parmi toutes les variables de la table secondaire. Le troisième termes représente le choix des partitions des valeurs des variables secondaires catégorielles ainsi que le choix des groupes impliqués dans l'itemset où S dénote le nombre de Stirling de deuxième espèce. La troisième ligne correspond au choix de la discrétisation des variables secondaires numériques ainsi que les intervalles sur lesquels portent les conditions de l'itemset. Le critère de l'équation 1 est un log négatif de probabilités, ce qui exprime une longueur de codage Shannon (1948). C c (A ? ) peut être donc interprété comme un coût de codage de l'itemset ?. Par ailleurs, il peut être vu comme un coût de construction de la variable A ? associée à ?.
Évaluation d'une variable secondaire binaire. La variable A ? associée à ? est une variable binaire construite dans la table secondaire. Lahbib et al. (2011) fournissent une approche d'estimation de densité de probabilité conditionnelle d'une telle variable vis-à-vis de la variable cible, ainsi qu'un critère permettant d'évaluer sa pertinence C e (A). Ce critère est rappelé dans l'équation 2. 
Le coût de construction agit comme un terme de régularisation afin de prévenir le risque de sur-apprentissage lié au grand nombre d'itemsets potentiellement considérés. Les variables secondaires A ? construites sur la base d'itemsets complexes, avec un nombre important de variables dans l'itemset, sont pénalisées par rapport à des variables construites plus simples. Soit ? ? un itemset vide, ne contenant aucune variable secondaire, où aucun enregistrement secondaire n'est couvert par l'itemset. Le coût d'évaluation global C r A ? ? de l'itemset vide est : 
Expérimentations
Nous avons évalué notre approche en utilisant la bases de données Digits (Lecun et al., 1998). Il s'agit de classer des images représentant des chiffres manuscrits de 0 à 9. Ces données initialement à plat 2 ont été reformatées afin d'obtenir un schéma relationnel (figure 2) constitué de deux tables : la table cible DIGIT et la table secondaire PIXEL décrivant les pixels qui composent chaque image. Cette dernière est décrite par trois variables secondaires : X_POSITION, Y_POSITION et GRAY_LEVEL qui représentent la position du pixel en abscisse et en ordonnée dans l'image originale ainsi que son niveau de gris.
Nous générons aléatoirement des itemsets basés sur des partitionnements (discrétisation dans le cas numérique et groupement de valeurs dans le cas catégoriel) des variables secondaires qui le constituent en 2, 4 et 8 partiles. Un classifieur Bayésien Naïf est employé en exploitant l'estimation de la densité de probabilité conditionnelle de la variable construite comme décrit dans (Lahbib et al., 2011).
La figure 2 illustre les performances de classification (Précision) du Bayésien Naïf utilisant les variables générées et ceci pour différents nombres d'itemsets (10, 1000, 1000 et 10 000). Ces résultats sont comparés à ceux obtenus avec le système Relaggs (Krogel et Wrobel, 2001). Relaggs est une méthode de propostionalisation qui consiste à générer pour chaque variable secondaire plusieurs agrégats (les effectifs, la somme, la moyenne, le min, le max, l'écart type,. . .). Ces agrégats sont ensuite ajoutés à la table cible et un Bayésien Naïf classique est employé. Nous reportons également les performances obtenues avec un Bayésien Naïf utilisant la représentation monotable initiale. On peut constater que notre approche dépasse largement Relaggs et ceci pour tous les nombres d'itemsets générés. Par ailleurs, avec suffisamment d'itemsets, notre approche atteint des performances comparables à celles d'un Bayésien Naïf qui utilise la représentation à plat.
Conclusion
Dans cet article, nous avons proposé une approche de prétraitement multivarié des variables secondaires dans le contexte de la classification de données multi-tables. La méthode consiste à construire de nouvelles variables à partir d'itemsets de variables secondaires. La pertinence 2. http://yann.lecun.com/exdb/mnist/

Introduction
Les ontologies ont contribué au succès des moteurs de recherche sémantique et sont de plus en plus utilisées pour améliorer la recherche d'information sur le web et la reformulation des requêtes. Cependant, la construction d'ontologies est généralement un processus long et coûteux, le recourt aux ontologies modulaires constitue une piste prometteuse. Une ontologie modulaire est une ontologie qui référence un fragment d'une ontologie de domaine et a l'avantage d'être réutilisée ultérieurement. La composition d'ontologies permet de construire une ontologie modulaire à partir d'un ensemble de modules ontologiques qui constituent un réseau, et permet d'améliorer l'organisation des concepts sémantiques.
Un état de l'art a permis de constater que les approches de composition d'ontologie proposées ne considèrent pas les relations sémantiques entre les termes, elles ne sont donc pas expressives et ne peuvent pas être efficacement utilisées pour la recherche sémantique sur le Web. Nos travaux précédents ont proposé en un premier lieu une approche de recherche d'information basée sur le RàPC pour reformuler la requête de l'utilisateur et lui recommander des résultats fondés sur les cas stockés (Elloumi-Chaabene et al., 2010), et par la suite une nouvelle méthode de composition de modules ontologiques basée sur des mesures de similarité séman-tique (Elloumi-Chaabene et al., 2011). L'objectif du présent travail est de proposer un système hybride de recherche d'information basée sur le RàPC et la composition d'ontologies, ayant pour but d'améliorer la précision des résultats fournis aux utilisateurs et de répondre à ses besoins. Notre contribution porte sur l'intégration de : 1/la composition de modules ontologiques pour construire une ontologie modulaire ce qui améliore le processus d'enrichissement de requête ; 2/ RàPC pour prendre en considération les préférences de l'utilisateur ; 3/une base de connaissances (BC) qui prend en considération le contenu des documents Web jugés pertinents par l'utilisateur lors des recherches précédentes afin d'enrichir la requête. Notre motivation est d'utiliser des requêtes passées afin d'améliorer la précision des résultats fournis aux utilisateurs et d'utiliser aussi les informations dans les documents Web pertinents pour enrichir la requête de l'utilisateur.
Ce papier est organisé comme suit : la section 2 introduit notre système qui intègre la composition d'ontologie et le RàPC, que nous détaillons dans les sous sections. Dans la section 3, nous présentons les expérimentations menées. La section 4 conclut et aborde nos perspectives.
2 Un système hybride de recherche d'information basé sur le raisonnement à partir de cas et la composition d'ontologies
Le système proposé se compose de quatre composants principaux : (1) un composant pour le RàPC (2) un composant pour la composition d'ontologies (3) un composant pour la base de connaissances et (4) un composant pour la classification de documents. L'architecture générale du système est présentée par la figure 1. Dans les sous-sections suivantes nous détaillons les différents éléments du système proposé. 
Première recherche
La première recherche sur le Web offre une reformulation de la requête initiale de l'utilisateur. En effet, le processus de recherche se déroule comme suit : l'utilisateur sélectionne les modules ontologiques nécessaires pour la composition, pose sa requête, qu'on enrichit par l'ontologie modulaire et la BC. En effet, la similarité sémantique est calculée entre les concepts de l'ontologie et les termes de la requête. Les concepts les plus similaires à la requête sont utilisés pour l'enrichir. La mesure de similarité utilisée est PMI_IR (Pointwise Mutual Information). Elle a été adaptée au Web par (Turney, 2001) en définissant p(a) la probabilité du terme " a " dans le web. Cette probabilité est estimée à partir du nombre de résultats retournés en cherchant le terme a sur le web. La mesure PMI_IR est calculée selon (1)
Un enrichissement par la BC est ensuite effectué. L'utilisateur valide la requête enrichie et il choisit le domaine de recherche. Une recherche sur le web commence en même temps que la construction de l'ontologie de domaine choisi. Le résultat de cette première recherche est donc un ensemble de documents et une ontologie de domaine qui vont permettre de passer à la recherche avancée.
La composition d'ontologies. La composition d'ontologies vise la construction d'une ontologie modulaire en utilisant un ensemble de modules ontologiques ceci permet d'améliorer leur structure en considérant les relations taxonomiques et non-taxonomiques implicites entre les concepts. Elle est évaluée par des mesures de cooccurrence basées sur le web. Les principales étapes de la méthode proposée sont les suivantes : (1) La réorganisation des modules ontologiques, qui consiste à déterminer les modules qui ont des concepts en communs et collecter l'ensemble des concepts qui se chevauchent pour chaque paire de modules. (2) La classification des concepts obtenus en modules en fonction de leur similarité sémantique en utilisant les mesures de cooccurrence (Turney, 2001). L'idée principale est de construire un graphe de cooccurrence et d'appliquer un algorithme de clustering pour réorganiser les concepts et déter-miner l'ensemble des nouveaux modules. (3) La construction d'une structure hiérarchique des modules ontologiques qui constituent l'ontologie modulaire.
La base de connaissances. Ce composant gère la BC composée d'une base de faits, une base de règles et un moteur d'inférence. Le système remplit automatiquement les règles de cette base par les termes les plus fréquents des documents jugés pertinents par l'utilisateur. Elle est utilisée pour enrichir la requête de l'utilisateur. En effet, suite au raisonnement fait par le moteur d'inférence sur les règles et les faits de la base, de nouvelles conclusions sont obtenues. Ces conclusions sont les termes significatifs extraits des documents pertinents de recherches antérieures qui vont être ajoutées à la requête pour l'enrichir. Les règles de cette base sont des règles de la logique d'ordre zéro écrites sous la forme :
Request? > BestT erm1, , , BestT ermi, , , BestT ermN. Avec N le nombre de documents choisis et BestTerm i le terme ayant la plus haute fré-quence dans le document i. Lorsqu'une nouvelle requête est soumise au système, le moteur d'inférence utilise ces mots clés pour faire un raisonnement : 1/ si la requête existe, il renvoie les termes significatifs extraits de documents pertinents retournés lors d'une recherche précé-dente pour cette requête (nouvelles conclusions) 2/ si la requête n'existe pas, la reformulation se fera seulement par l'ontologie modulaire.
Recherche avancée
L'utilisateur sélectionne les documents pertinents à partir de ceux récupérés dans la première recherche. Ensuite, le système insère d'une part un nouveau cas et sa solution, et d'autre part, utilise des techniques de fouille de texte pour extraire les termes les plus fréquents et les ajouter à la BC. A partir de l'ontologie du domaine construite, l'utilisateur choisit le concept de recherche. En utilisant WordNet (Miller, 1990), les synonymes, hyponymes et hyperonymes sont insérés dans la BDC pour mettre à jour la signature sémantique du cas( c'est une liste de termes qui apparaissent fréquemment avec le concept du cas). En se basant sur le concept choisi et la BDC, une nouvelle recherche est possible et comprend : 1/ une désambiguïsation : le système offre les sens du concept choisi à partir de WordNet et une recommandation basée sur un algorithme de désambiguïsation sémantique ; 2/ l'ajout de termes : l'utilisateur ajoute des concepts de son choix afin d'enrichir l'ontologie ; 3/ les recommandations : ce sont les cas similaires de recherche (des URLs recommandés qui correspondent à des cas pertinents de recherche similaires stockés dans la BDC).
Le raisonnement à partir de cas. La combinaison des ontologies et le mécanisme de RàPC peut améliorer les performances des recherches sur le Web sémantique. Le but ici est d'enrichir automatiquement la requête à l'aide de requêtes antérieures effectuées par l'utilisateur. Ce composant gère la BDC, en adoptant le modèle vectoriel pour représenter le cas. Typiquement un cas contient au moins deux parties : une description de situation représentant un "problème" et une "solution" utilisée pour remédier à cette situation. Le problème comprend le domaine général, le concept spécifique de la recherche ainsi que la signature sémantique. La solution correspondante est composée des documents jugés pertinents par l'utilisateur ainsi que deux vecteurs, domaine et module, associés aux concepts du domaine et concept de recherche respectivement. Ces deux vecteurs correspondent aux poids des concepts du domaine et de la signature sémantique calculés par la mesure TF*IDF dans les documents résultats. A l'aide de la BDC, on peut insérer un nouveau cas, le mettre à jour ou bien rechercher des cas similaires.
Classification des résultats
A partir des choix effectués par l'utilisateur au cours de l'étape précédente (la désambiguï-sation etc.) ainsi que les requêtes similaires dans la BDC, le système reformule la requête et recherche de nouveaux résultats sur le web. Les documents récupérés sont classés par ordre de pertinence par rapport à la requête et ceux sélectionnés par l'utilisateur sont ajoutés à la BDC et leurs termes les plus fréquents sont ajoutés à la BC.
La classification des documents. Afin de récupérer les documents les plus pertinents, le modèle de Salton (Salton et Rocchio, 1983) est utilisé (les termes remplacés par des concepts). Un filtrage par les vecteurs domaine et module est appliqué pour ne garder que les documents dans la même thématique et appartenant au même module. Chaque document est représenté par un vecteur Dj = (d1j ; d2j ; ... ; dnj). Où dij est le poids du concept ci dans le document Dj , N étant le nombre de concepts dans la signature sémantique. Le vecteur Q = (Q1 ;Q2 ; ... ;Qn) représente la requête, où Qi est le poids du concept ci dans la requête. La mesure de similarité entre un document et une requête est calculée avec la formule cosinus :
Expérimentation
Un prototype supportant le système proposé a été développé pour fournir une interface utilisateur qui permet la manipulation des ontologies, la gestion de la BC et l'affichage des résultats à partir du moteur de recherche sur le web (en utilisant l'API de Bing). L'évaluation expérimentale des performances du système proposé est menée en comparant les résultats de la précision moyenne des différents systèmes. Quatre scénarios ont été testés : 1/ une recherche classique (c'est une recherche par mots clés sans effectuer une reformulation de la requête), 2/ une reformulation de requêtes en utilisant l'approche de composition d'ontologies, 3/une recherche basée sur l'approche RàPC et finalement 4/ une recherche hybride. L'évaluation expérimentale a été conduite en utilisant le SRI expérimental LEMUR 1 , largement utilisé par la communauté RI. Les différents tests sont menés sur la collection INEX 2010 2 . Les résultats illustrés par la figure 2 représentent la précision exacte à 10, 30, 50 et 100 documents, et on observe une amélioration significative de la pertinence de l'information ré-cupérée lors de la reformulation de requête pour le quatrième scénario.  
Conclusion
Cet article présente notre système hybride pour la recherche d'information sur le Web inté-grant le RàPC et la composition d'ontologies. Tout d'abord, nous nous sommes appuyés sur le RàPC dans le but de prendre en considération les cas déjà rencontrés. Nous avons aussi utilisé une nouvelle méthode de composition des modules ontologiques pour améliorer, d'une part, la structure des modules d'ontologiques et leur organisation interne, et d'autre part, le degré de parenté sémantique entre les concepts et la structure globale de l'ontologie modulaire. Nous avons également intégré une BC pour enrichir la requête de l'utilisateur par les termes les plus fréquents extraits de documents pertinents. Plusieurs techniques de recherche d'information ont été intégrées pour améliorer les résultats de recherche sur le Web. L'expérimentation et l'évaluation menées montrent une amélioration du taux de précision. Dans nos travaux futurs, nous envisageons d'intégrer une nouvelle composante à notre système pour supporter le traitement des systèmes question-réponse afin d'améliorer les résultats de recherche en répondant automatiquement aux questions posées en langage naturel.

Introduction
La problématique de classification non supervisée (aussi appelée clustering) a été longuement étudiée pendant de nombreuses années avec des approches comme k-means et kmédoïdes. En général, le problème consiste à partitionner un ensemble de n objets en k classes non vides et deux à deux disjointes. C'est un champ de recherche difficile pour plusieurs raisons : le choix de la mesure de dissimilarité entre les objets dépendant principalement de l'application mais influant fortement sur les résultats, la définition du critère à optimiser, la taille de l'espace de recherche avec pour conséquence la nécessité de définir des heuristiques conduisant souvent à un optimum local. Poser des contraintes sur la solution recherchée permet d'une part, de modéliser plus finement les applications réelles et d'autre part de restreindre la taille de l'espace de recherche. Néanmoins, la plupart des algorithmes classiques n'ont pas été développés pour la classification non supervisée sous contraintes et doivent être adaptés, si possible, pour prendre en compte les contraintes posées par l'utilisateur. Développer des solveurs généraux applicables à une grande variété de problèmes pose de nouveaux défis.
D'autre part, des avancées récentes en Programmation par Contraintes (PPC) ont rendu ce paradigme beaucoup plus puissant. Plusieurs travaux (De Raedt et al. (2008) (Boizumault et al. (2011)) ont étudié l'intérêt de la PPC pour modéliser des problèmes de fouille de données et ont montré l'apport de la déclarativité inhérente à la PPC.
Dans ce papier, nous proposons un cadre pour modéliser la classification non supervisée sous contraintes en Programmation par Contraintes. L'intérêt de notre approche est de fournir un modèle déclaratif permettant de spécifier le problème de classification non supervisée et d'intégrer facilement des contraintes. Dans notre modèle, nous faisons l'hypothèse que nous disposons d'une mesure de dissimilarité entre les paires d'objets et que le nombre k de classes est fixé (en théorie aucune limite n'est donnée sur la valeur de k, mais plus k est grand, plus la complexité est élevée). Nous considérons, dans ce papier, le problème de trouver une partition minimisant le diamètre maximal des classes et nous montrons que notre cadre intègre naturellement des contraintes sur les instances ou sur les classes.
Il est reconnu en PPC que le choix du modèle est fondamental, mais nous insistons aussi sur le fait que la stratégie de recherche est tout aussi importante pour améliorer l'efficacité. Plusieurs stratégies sont étudiées, différant sur la manière d'ordonner les points ou reposant sur des résultats théoriques. Des expérimentations sur des bases de données classiques montrent l'intérêt de notre approche. Contrairement à la plupart des travaux existants, notre modèle permet de trouver un optimum global, et nous ne pouvons espérer qu'il soit compétitif en terme de temps de calcul avec des méthodes heuristiques. Nous comparons la qualité des solutions obtenues par notre méthode avec celles obtenues par la méthode FPF (Gonzalez (1985)), mé-thode très efficace pour la classification non supervisée optimisant le diamètre maximum des classes, mais conduisant à une solution approchée.
Des travaux récents (Guns et al. (2011)), (Métivier et al. (2012)) ont déjà proposé d'utiliser la PPC pour la classification non supervisée conceptuelle. Le problème est formalisé comme la recherche d'un ensemble de k-motifs fréquents, deux à deux non recouvrants, dont l'ensemble couvre toutes les données : les transactions sont vues comme des objets et les motifs comme des définitions en intension des classes. Plusieurs critères d'optimisation sont considérés comme maximiser la taille minimale d'une classe ou minimiser la différence entre les tailles des classes. Notons que ces approches sont donc adaptées à des bases de données qualitatives alors que notre approche peut traiter tout type de données dès lors que l'on dispose d'une mesure de dissimilarité entre les données. Davidson et al. (2010) propose un cadre SAT pour la classification non supervisée sous contraintes, mais uniquement pour un problème à 2 classes (k = 2) : il traite les contraintes sur les instances ("must-link" et "cannot-link") et des contraintes sur les classes (diamètre des clusters, séparation entre les clusters). Son algorithme converge vers un optimum global. Notre approche est plus générale dans la mesure où le nombre de classes n'est pas limité à 2.
Le papier est organisé comme suit. Dans la section 2, nous rappelons des notions sur la classification non supervisée à base de contraintes et sur la Programmation par Contraintes. La section 3 est dédiée à la présentation de notre modèle et la section 4 aux expérimentations. La conclusion et une discussion sur les travaux futurs sont données dans la section 5.
Préliminaires
Classification non supervisée
La classification non supervisée (ou clustering) consiste à regrouper les données dans des classes (ou clusters), de manière à regrouper dans un même cluster les données similaires et à séparer les données distantes dans des clusters différents. Etant donnés une base de données de n objets O = {o 1 , . . . , o n } d'un espace X et une mesure de dissimilarité d(o i , o j ) entre deux objets o i et o j , l'objectif est de regrouper les objets en différentes classes de telle manière que la partition obtenue satisfasse un critère donné. Le problème de clustering peut être donc formulé comme un problème d'optimisation.
Les problèmes de clustering sont variés, dépendant de différents critères, comme la structure souhaitée (une partition, une hiérarchie, des classes recouvrantes, . . .), ou encore le critère à optimiser. Dans ce papier, nous nous intéressons à la recherche d'une partition des objets en k classes C 1 , . . . , C k telle que : (1) pour tout i, (4) un critère E est optimisé. Le critère optimisé peut être, entre autres :
-Critère des moindres carrés :
Ce critère est équi-valent au critère des moindres carrés dans le cas de la distance euclidienne.
maximal des clusters, où le diamètre d'un cluster est la distance maximale entre chaque paire de ces objets. L'algorithme k-means représente chaque cluster par la moyenne des points du cluster et tend à minimiser le critère des moindres carrés. L'algorithme k-médoïdes choisit des objets pour représenter des clusters et cherche à minimiser le critère d'erreur absolue. A chaque ité-ration, les deux méthodes réduisent la valeur du critère jusqu'à un optimum, en général local.
L'algorithme FPF (Furthest Point First) (Gonzalez (1985)) minimise le diamètre maximal des clusters. L'algorithme commence par choisir un point comme le représentant du premier cluster et affecte tous les points à ce cluster. A l'itération suivante, le point le plus loin du premier représentant est choisi comme le représentant du second cluster. Les points qui sont plus proches du second représentant que du premier sont réaffectés au second cluster. L'algorithme réitère de cette façon : choisir comme nouveau représentant le point le plus loin des représen-tants existants et réaffecter les points. Il s'arrête après k itérations, ayant ainsi formé k clusters. La complexité en temps est O(nk). Si d opt est l'optimum global alors l'algorithme garantit de trouver un clustering avec le diamètre maximal des clusters d tel que d opt ? d ? 2d opt , si la mesure utilisée satisfait l'inégalité triangulaire. De plus, Gonzalez a prouvé que trouver d tel que d ? (2 ? opt est NP-Difficile pour tout > 0, lorsque les points sont dans un espace à 3 dimensions. L'algorithme modifié (M-FPF) proposé dans (Geraci et al. (2006)) est plus rapid en temps mais la solution trouvée est identique que celle de FPF.
La classification non supervisée sous contraintes tient compte des contraintes définies par l'utilisateur pour identifier les clusters. Ces contraintes peuvent être posées sur les clusters ou sur des instances (points). Les contraintes sur les clusters imposent des conditions sur la forme, la taille ou d'autres caractéristiques. Par exemple, la contrainte sur la taille exprime que chaque cluster doit avoir un nombre minimum ? de points (contrainte de capacité minimale) : ?c ? [1, k], |C c | ? ?, ou encore que chaque cluster doit avoir un nombre maximum ? de points (contrainte de capacité maximale) :
Un autre exemple est la contrainte de séparation minimale, qui impose que la distance entre chaque paire d'objets de différents clusters soit supérieure à un seuil ? :
Les contraintes sur des instances sont en général posées sur des paires d'objets individuels. Deux types de contraintes sont souvent utilisés : "must-link" et "cannot-link". Une contrainte must-link indique que deux objets o i et o j doivent être dans le même cluster : ?c ? [1, k], o i ? C c ? o j ? C c . Une contrainte cannot-link indique que deux objets ne peuvent pas appartenir au même cluster :
Les contraintes définies par l'utilisateur sont nécessaires dans les applications réelles. Cependant, peu d'algorithmes sont adaptables pour gérer de telles contraintes. Il n'existe pas de solution générale pour étendre un algorithme traditionnel (k-means, k-médoïdes, etc.) avec des contraintes. Notre approche se basant sur la Programmation par Contraintes permet d'ajouter directement des contraintes définies par l'utilisateur, sans modifier le modèle.
Programmation par contraintes
La Programmation par Contraintes (PPC) est un paradigme puissant pour résoudre des problèmes combinatoires, se basant sur des techniques issues de l'intelligence artificielle ou de la recherche opérationnelle. La PPC se base sur le principe suivant : (1) le programmeur spécifie le problème d'une façon déclarative comme un problème de satisfaction de contraintes ; (2) le solveur cherche des solutions en intégrant la propagation de contraintes à la recherche. Un problème de satisfaction de contraintes (Constraint Satisfaction Problem -CSP) est un triplet
.., C t } est un ensemble de contraintes où chaque contrainte C i est une condition sur un sous-ensemble de X.
Une solution d'un CSP est une affectation complète de valeur a i ? D i à chaque variable x i satisfaisant toutes les contraintes de C. Un problème d'optimisation sous contraintes (Constraint Optimization Problem -COP) est un CSP auquel est associée une fonction objectif. Une solution optimale d'un COP est une solution du CSP qui optimise la fonction objectif.
En général, les CSP sont NP-Difficiles. Cependant, les techniques utilisées par les solveurs permettent de résoudre un grand nombre d'applications réelles de façon efficace. Les techniques les plus connues reposent sur la propagation de contraintes et des stratégies de recherche.
La propagation de contraintes consiste, pour une contrainte c, à supprimer du domaine des variables de c des valeurs pour lesquelles on peut déterminer qu'elles ne peuvent participer à une solution de c. A chaque contrainte est associé un ensemble de propagateurs, dépendant du choix de consistance pour cette contrainte. Par exemple, si la contrainte c est posée avec la consistance d'arc, les propagateurs sont implantés de manière à supprimer toutes les valeurs inconsistantes avec c du domaine des variables. Si c est posée avec la consistance de borne, les propagateurs modifient seulement les bornes inférieures et supérieures des domaines de variables. Le choix de la consistance est indiqué par le programmeur lorsque les contraintes sont posées.
Le fait que chaque contrainte soit réalisée par un ensemble de propagateurs implique que toute formule ou relation mathématique ne peut être une contrainte, seules celles pour lesquelles on peut construire un ensemble de propagateurs sont disponibles. Nous disposons des relations arithmétiques, logiques et de relations plus complexes représentées sous forme des contraintes globales.
Le solveur recherche des (les) solutions en itérant deux étapes : propagation des contraintes et branchement. Le solveur propage toutes les contraintes jusqu'à un état stable, dans lequel soit le domaine d'une variable est réduit à l'ensemble vide, soit aucun domaine ne peut se réduire davantage. Dans le premier cas, il n'existe pas de solution et le solveur effectue un retour en arrière. Dans l'autre cas, si tous les domaines sont singletons, une solution est trouvée, sinon le solveur choisit une variable dont le domaine est non singleton et découpe le domaine en deux parties, ce qui crée deux nouvelles branches dans l'arbre de recherche. Le solveur explore ensuite chaque branche, la propagation de contraintes peut devenir de nouveau active suite aux modifications du domaine d'une variable.
La stratégie de l'exploration de l'arbre de recherche peut être déterminée par le programmeur. Avec la stratégie de recherche en profondeur d'abord, le solveur ordonne les branches par l'ordre spécifié par le programmeur et explore en profondeur chaque branche. Pour un problème d'optimisation, la stratégie de recherche en profondeur devient la stratégie branchand-bound : chaque fois qu'une solution du CSP est trouvée, la valeur de la fonction objectif sur cette solution est calculée et une nouvelle contrainte est ajoutée, imposant qu'une nouvelle solution soit meilleure que celle-ci. Le solveur effectue une recherche exhaustive, la solution retournée est donc garantie d'être optimale. Les choix de variables et de valeurs à chaque branchement sont extrêmement importants, car ils peuvent aider à réduire de façon drastique l'arbre de recherche. Pour plus de détails sur la programmation par contraintes, le lecteur est invité à consulter l'ouvrage de Rossi et al. (2006).
Afin d'illustrer un problème d'optimisation sous contraintes et les stratégies de recherche, considérons l'exemple suivant. un cercle bleu est un état stable mais non encore une solution, un carré rouge est un état échec (pas de solution), un losange vert est une solution intermédiaire et le losange orange est la solution optimale. Pour chaque état stable, la branche à gauche est le cas où la variable choisie reçoit la valeur choisie, la branche à droite est l'autre cas, où la valeur choisie est supprimée du domaine de la variable.
Modélisation des problèmes de clustering sous contraintes
Nous présentons dans cette section un modèle en PPC pour le clustering sous contraintes. Nous disposons d'une collection de n points (objets) et d'une mesure de dissimilarité entre ces objets. Sans perte de généralité nous supposons que les points sont indexés et nommés par leur indice. La distance entre deux points i, j est notée d(i, j). Nous considérons le cas où le nombre k de clusters est connu à l'avance. Le modèle a pour objectif de trouver une partition des points en k clusters minimisant le diamètre maximal des clusters. Affecter un point à un cluster devient associer ce point au représentant du cluster. Nous introduisons pour chaque point i ? [1, n] une variable entière G[i] qui donne la valeur du représentant associé.
Modèle
Nous introduisons également une variable D qui représente le diamètre maximal. C'est une variable entière, car nous utilisons des solveurs sur des variables entières. Le domaine de D est l'intervalle formé par la distance minimale et la distance maximale entre chaque paire de points. Dans nos expérimentations, lorsque la distance n'est pas entière, elle est multipliée par 100 et seule la partie entière est conservée.
Notre modèle permet de trouver la solution optimale, c'est-à-dire une affectation complète des variables I, G et D qui satisfait les contraintes suivantes. -Deux points à une distance supérieure au diamètre maximal doivent être dans des clusters différents. Ceci est représenté par les contraintes réifiées suivantes : -La contrainte de séparation minimale indique que la séparation entre deux clusters doit être au moins ?, ou que deux points à une distance inférieure à ? doivent être dans le même cluster. Pour chaque i < j ? [1, n] tel que d(i, j) < ?, nous posons la contrainte :
. -La contrainte de diamètre maximum indique que le diamètre de chaque cluster doit être au plus ?, autrement dit que deux points à une distance supérieure à ? doivent être dans des clusters différents. Pour chaque i < j ? [1, n] tel que d(i, j) > ?, nous posons les contraintes :
Pour les contraintes sur les couples de points :
-Une contrainte must-link sur i, j se traduit par le fait que les points ont le même repré-sentant :
. -Une contrainte cannot-link sur i < j est exprimée par :
Stratégie de recherche Les stratégies de choix des variables et des valeurs doivent être données au solveur. Les variables sont choisies dans l'ordre I, D puis G. Cet ordre indique que les représentants de cluster doivent être identifiés en premier, puis pour chaque borne sur le diamètre maximal le solveur essaye de déterminer l'affectation de points aux clusters. 
Améliorations du modèle
Les solveurs de PPC réalisant une recherche exhaustive, ce modèle permet de trouver la solution optimale. Afin d'améliorer l'efficacité du modèle, différents aspects sont considérés. La version 1.1 repose sur un ordonnancement préalable de points où : le premier point est loin des autres points, le deuxième point est loin du premier mais aussi assez loin des autres points, le troisième point est loin des deux premiers et aussi assez loin des restes, . . .Pour réaliser cette heuristique, supposons que les p premiers points soient identifiés, le (p + 1)-ème point est le arg max i?[p+1,n] f (i), où
La version 1.2 ordonne les points avec l'algorithme FPF, en prenant k = n (autant de classes que de points). Ainsi, chaque point est choisi par l'algorithme FPF et l'ordre de choix donne l'ordre des points.
Amélioration des contraintes Avec un k fixé et sans connaissances de contraintes utilisateur, il est prouvé dans Gonzalez (1985) que le diamètre d F P F calculé par l'algorithme FPF vérifie d opt ? d F P F ? 2d opt , avec d opt le diamètre optimal. Cette connaissance implique des bornes pour la variable D, à savoir [d F P F /2, d F P F ]. De plus, pour chaque couple de points i, j : -si d(i, j) < d F P F , nous ne posons pas la contrainte réifiée (1) sur i, j, Les résultats montrent que pour la base ionosphere, les contraintes ML aident en général à réduire l'espace et le temps de recherche, ce qui n'est pas le cas avec les contraintes CL. Cependant pour la base kdd_synthetic_control, les contraintes utilisateur diminuent la performance, car la différence entre les diamètres optimaux dans les cas avec ou sans contraintes est souvent significative. Le diamètre optimal dans le cas avec contraintes est souvent deux fois plus grand que celui sans contraintes. Nous pensons que le critère de diamètre maximal pourrait ne pas être le plus approprié pour la structure réelle de cette base, ce qui pourrait expliquer les résultats.
Conclusion
Nous présentons dans ce papier un modèle en programmation par contraintes pour la classification non supervisée. Le modèle trouve un optimum global qui minimise le diamètre maximal des clusters. Des données qualitatives ou quantitatives peuvent être considérées. Le modèle est extensible directement aux contraintes sur des clusters ou sur des instances. Nous présentons également des stratégies de recherche pour améliorer l'efficacité du modèle. Des expérimentations sur des jeux de données classiques montrent l'intérêt de notre approche.

Introduction
Les documents textuels sont tellement abondants sur le Web que l'information pertinente est souvent difficile à retrouver. Dans l'objectif d'offrir une meilleure navigation dans les corpus de documents, que ce soit pour l'exploration du contenu ou la recherche d'information, l'extraction de thématiques (topic extraction) se distingue comme une tâche de fouille de textes dont l'objectif est d'extraire, automatiquement et sans catégories données a priori, des théma-tiques (sujets) à partir de grands corpus de documents. L'extraction de thématiques a été étudiée par différentes communautés, que ce soit celle de la fouille de données (Anaya-Sánchez et al., 2008), du Traitement Automatique des Langues (Blei et al., 2003), de la linguistique computationnelle (Ferret, 2006) ou de la recherche d'information (Zamir et al., 1997), d'où l'existence de différentes méthodes dédiées à cette tâche. A ces méthodes spécifiques, peuvent s'ajouter des méthodes adaptées notamment de l'apprentissage automatique non supervisé. Les résultats produits par l'ensemble de ces méthodes prennent des formes hétérogènes : partitions, matrices, distributions de probabilités sur les mots, etc. Cela pose clairement un problème de comparaison de ces résultats. Dans cet article, nous proposons une nouvelle mesure de qualité, la Vraisemblance Généralisée, qui permet d'évaluer et de comparer différentes méthodes d'extraction de thématiques. La mesure proposée est calculée dans un nouvel espace de description où les documents sont décrits par les thématiques et que nous appelons l'espace latent. Nous proposons également des opérateurs pour transformer les résultats des méthodes d'extraction de thématiques vers l'espace latent afin de calculer cette mesure.
La section 2 est consacrée à la présentation des principales méthodes d'extraction des thé-matiques. La section 3 présente les principales mesures de qualité ainsi que la nouvelle mesure intitulée Vraisemblance Généralisée. Les expérimentations et les résultats sont ensuite présen-tés en section 4. La conclusion et les perspectives de recherche sont données en section 5.
Méthodes d'extraction de thématiques
La plupart des méthodes d'extraction de thématiques nécessite que le corpus de documents soit mis sous forme d'une matrice V où les lignes représentent les documents et les colonnes représentent les mots (modèle vectoriel de (Salton et al., 1975)). Chaque élément V ij de la matrice contient le poids du mot w j dans le document d i , qui reflète son importance dans le document. Le plus simple est de pondérer les mots par leurs fréquences d'apparition dans les documents (fréquence TF), même s'il existe d'autres types de pondération.
Il faut cependant noter que certaines méthodes d'extraction de thématiques, notamment celles issues de la linguistique computationnelle (Ferret, 2006), manipulent les documents sous forme d'un graphe où les noeuds représentent des unités linguistiques (mots, phrases, documents, etc.) et les arêtes représentent des relations entre elles, par exemple des relations sémantiques ou de co-occurrences. L'extraction des thématiques est ensuite effectuée en utilisant les algorithmes classiques issus de la théorie des graphes, comme le clustering spectral (Ng et al., 2002). Nous avons choisi de ne pas intégrer ce type de représentation dans notre travail, du moins pour le moment.
Dans cette présentation, nous proposons de regrouper les méthodes d'extraction de thé-matiques en trois grandes familles : les méthodes à base de distance, les méthodes à base de factorisation de matrices et les modèles de thématiques probabilistes.
Méthodes à base de distance
Les méthodes à base de distance se fondent sur le calcul d'une distance pour mesurer la similarité entre les documents. La plupart des méthodes de cette catégorie sont des méthodes de classification automatique non supervisée. Même si ce n'est pas leur vocation initiale, ces méthodes peuvent être utilisées pour l'extraction de thématiques en considérant que chaque classe définit une thématique et regroupe ainsi les documents qui y sont relatifs. La caractéri-sation des thématiques peut ensuite se faire en post-traitement en prenant par exemple les mots les plus fréquents dans chaque classe, ou en cherchant les mots les plus discriminants.
Dans les méthodes à base de distance, on trouve principalement les méthodes de partitionnement et les méthodes hiérarchiques. Les méthodes de paritionnement, comme l'algorithme des K-Means, commencent par répartir aléatoirement les documents sur un certain nombre de classes et, à chaque itération, les documents sont réaffectés de telle sorte que chacun soit dans la classe dont il est la plus proche (au sens de la mesure de similarité utilisée).
Plusieurs variantes des K-Means existent, comme FCM (Fuzzy C-Means), qui permet une classification floue des documents, c'est-à-dire un document n'est pas affecté à une seule classe mais il appartient à plusieurs classes avec différents degrés d'appartenance (Dunn, 1973). Les méthodes de partitionnement sont généralement de faible complexité, ce qui les rend adaptées aux grands volumes de données.
Les méthodes hiérarchiques procèdent à la construction des classes au fur et à mesure par agglomération, ou par division. En agglomération, chaque classe contient, au départ, un seul document. Les deux classes les plus proches, en termes de distance, sont ensuite fusionnées récursivement jusqu'à ce que tous les documents soient dans la même classe. En division, tous les documents sont dans une seule classe qui est divisée, récursivement, jusqu'à ce que chaque document soit dans une classe. Dans (Pons-Porrata et al., 2003), une méthode hié-rarchique est proposée pour la classification de documents en se basant sur une distance qui prend en compte les entités temporelles et les noms de lieux. Les méthodes de classification hiérarchiques offrent la possibilité de contrôler la granularité des classes, et d'avoir ainsi des classes aussi fines ou grandes que souhaité. En revanche, les méthodes hiérarchiques souffrent du problème de complexité, ce qui les rend inadaptées aux grands volumes de documents.
Qu'elles soient à base de partitionnement ou hiérarchique, ces méthodes n'ont pas été initialement créées pour extraire des thématiques. Cependant, un simple post-traitement permet d'extraire des thématiques dans le sens où les centroïdes correspondent à des vecteurs dans l'espace du vocabulaire de mots. Cela explique la présence de ces méthodes dans cette étude.
Méthodes à base de factorisation de matrices
En algèbre linéaire, la factorisation de matrices est une approche qui peut permettre l'extraction des thématiques. Le principe général est de partir de la matrice d'occurences V , puis de trouver une factorisation de la matrice V en un produit de deux matrices W et H. H est une matrice dont les lignes sont constituées par des combinaisons de mots. Ce nouvel espace est appelé "espace sémantique latent" et il est défini par les thématiques, chaque thématique étant une combinaison de mots. W est une matrice de projection des documents dans le nouvel espace sémantique, où chaque élément W ij représente le degré d'appartenance du document d i à la thématique c j . L'analyse sémantique latente, LSA (Latent Semantic Analysis), permet de faire cette factorisation en effectuant une décomposition en valeurs singulières (Deerwester et al., 1990). Cependant, comme cette dernière peut produire des valeurs négatives, la méthode pose un problème d'interprétabilité des résultats (Lee et Seung, 1999). Pour contourner ce problème, la factorisation non négative de matrices, NMF (Non-negative Matrix Factorization), a été proposée par (Lee et Seung, 1999). NMF permet de trouver une factorisation non-unique d'une matrice non-négative V en un produit de deux matrices non-négatives W et H, de telle sorte que V ? W H. L'objectif est de minimiser la fonction objectif J NMF suivante :
(1) (Lee et Seung, 2001) décrivent une méthode itérative basée sur NMF pour l'extraction de thématiques. Le problème de factorisation est ramené à un problème d'optimisation de la fonction J NMF sous les contraintes de non négativité. Le problème est ensuite résolu en utilisant la méthode de Lagrange qui donne lieu aux deux règles de mise à jour suivantes :
Modèles de thématiques probabilistes
Les modèles de thématiques probabilistes (probabilistic topic models) sont une famille de modèles graphiques qui ont pour objectif la découverte de thématiques dans des corpus de documents. Le principe est de considérer un document comme un mélange probabiliste de thématiques latentes, c'est-à-dire un document est composé de plusieurs thématiques avec différentes proportions. Parallèlement à cela, chaque thématique est définie par une distribution de probabilités sur les mots. Par exemple, une thématique relative à la génétique associe des probabilités plus importantes sur les mots ADN, gène, cellule, etc. que sur les autres mots. Un modèle de thématiques probabiliste peut être vu, d'un autre angle, comme un processus de génération de documents à partir d'un vocabulaire (ensemble fixe de mots, notés w i ), tout en prenant en compte le fait que chaque document est un mélange probabiliste de plusieurs thématiques, notées z j . En supposant que les distributions p(w i |z j ) sont connues pour tout i, j, le processus simplifié de génération d'un document d est le suivant :
1. Se fixer une distribution de probabilités sur les thématiques p(z j |d) 2. Pour chaque mot w à générer :
(a) Choisir aléatoirement une thématique z parmi les z j suivant la distribution fixée dans (1).
(b) Choisir un mot w parmi w i dans le vocabulaire suivant la distribution p(w i |z).
A partir de là, la procédure consiste à inverser le processus génératif en utilisant la loi de Bayes afin d'estimer les valeurs des paramètres p(w i /z j ) et p(z j /d). Ceci est réalisé en utilisant les techniques d'apprentissage et d'inférence des modèles graphiques probabilistes, comme le Gibb's sampling ou l'inférence variationnelle.
Les modèles de thématiques probabilistes proposés dans la littérature partagent globalement le principe génératif exposé ci-dessus, mais diffèrent principalement dans la manière de choisir les distributions de probabilités p(z i |d), p(w i |z j ). Dans PLSA (Hofmann, 1999), aucune hypothèse de la distribution des thématiques sur les documents n'est posée ; chaque document est traité à part. Dans LDA (Blei et al., 2003), chaque thématique est caractérisée par une distribution multinomiale sur les mots qui lui sont associés. LDA utilise une loi de Dirichlet pour permettre un choix judicieux des paramêtres des distributions multinomiales, et ainsi pallier les limites de PLSA.
Les modèles de thématiques probabilistes diffèrent également par leur structure. En effet, certains supposent l'existence d'autres variables latentes que les thématiques, par exemple des variables temporelles ou d'opinion, et permettent ainsi d'extraire ces connaissances en même temps que les thématiques.
Les trois familles de méthodes exposées ci-dessus ont des inspirations différentes. Néan-moins, il a été montré que des liens théoriques existent entre ces méthodes. La méthode NMF est équivalente à Kernel K-Means, une version des K-Means avec noyau (Ding et al., 2005). La méthode PLSA est équivalente à NMF en prenant la divergence de Kullback-Leibler dans la fonction objectif (Gaussier et Goutte, 2005).
Evaluation des méthodes d'extraction de thématiques
Les différentes méthodes d'extraction de thématiques produisent des résultats de forme hétérogène : partitions de documents, distributions de probabilités sur les mots, matrices, etc. Cela pose le problème de comparaison des résultats. Pour résoudre ce problème, nous proposons un nouvel espace de description commun aux différentes méthodes et une nouvelle mesure de qualité qui se calcule dans cet espace. Cela permet ainsi de comparer des approches de nature différente et de manière quantitative. Cette section présente les mesures de qualité existantes et la nouvelle mesure que nous proposons.
Mesures existantes
Les méthodes d'extraction de thématiques sont généralement évaluées de manière qualitative ou quantitative. L'approche qualitative a recours au jugement humain pour qualifier les thématiques sans donner aucun indice quantitatif pour comparer les méthodes entre elles. A contrario, l'approche quantitative permet de mesurer plus finement la qualité des modèles, qu'elle soit basée sur le jugement humain ou non. Le jugement humain est utilisé pour évaluer les thématiques selon deux critères : word intrusion et topic intrusion (Chang et al., 2009). Une des mesures quantitatives qui n'utilisent pas le jugement humain (automatiques) est la vraisemblance mais elle se calcule seulement sur les modèles probabilistes et sur les méthodes d'apprentissage de type EM (Dempster et al., 1977), ce qui n'est pas le cas de toutes les mé-thodes d'extraction de thématiques.
La problématique d'évaluation des thématiques se retrouve classiquement en apprentissage non supervisé avec les mesures recensées par (Halkidi et al., 2001). Celles-ci peuvent être réparties en deux catégories : mesures externes et mesures internes. Les mesures externes évaluent la qualité des résultats par rapport à une référence définie par les classes a priori des documents. Comme exemples de ce type de mesures on peut citer le F-score (moyenne harmonique du rappel et de la précision), l'entropie (mesure de désordre dans l'ensemble des thématiques) et la pureté (ratio moyen de la classe majoritaire dans chacune des thématiques). Les mesures internes ne font pas appel à des connaissances extérieures. Par exemple, l'inertie intra-classes est utilisée comme fonction objectif dans la méthode des K-Means, ou, la cohé-sion (Steinbach et al., 2000), qui mesure la similarité Cosinus entre les documents d'une même thématique.
Même si ces mesures peuvent être utilisées pour évaluer les thématiques, en considérant que chaque thématique correspond à une classe, elles ne sont pas dédiées à cette tâche. A notre connaissance, il n'existe pas à ce jour de mesure automatique qui permette d'évaluer toutes les méthodes présentées de manière uniforme, et ainsi de pouvoir les comparer.
Nouvelle mesure : la Vraisemblance Généralisée
La mesure que nous proposons, intitulée Vraisemblance Généralisée (V G), est une mesure quantitative interne qui permet d'évaluer plusieurs méthodes d'extraction de thématiques, même si ces dernières sont basées sur des modèles mathématiques différents. Le bien-fondé de la mesure V G repose sur le fait qu'il existe une analogie entre les différentes méthodes. En effet, toutes ces méthodes permettent, d'une manière ou d'une autre, de projeter les documents dans un espace de description formé par les thématiques et de décrire ces thématiques par des mots (cf. figure 1). Notre idée consiste à proposer la mesure V G qui se calcule dans l'espace latent ainsi que des transformations des résultats des méthodes vers l'espace latent. 
FIG. 1 -Espace latent : les documents sont projetés dans l'espace latent caractérisé par les thématiques z 1 , z 2 (matrice W ) et les thématiques sont décrites par les mots (matrice H).
La mesure V G est calculée à partir de deux matrices : la matrice de projection W , matrice de projection des documents dans l'espace latent et la matrice de l'espace latent H qui définit cet espace. La matrice H est caractérisée par un ensemble de vecteurs, pas nécessairement orthonormés, décrits dans l'espace des mots (cf. figure 1). Ce sont donc des vecteurs positifs ou nuls. La matrice W est caractérisée par un ensemble de vecteurs correspondant aux documents, décrits dans l'espace des thématiques. En d'autres termes, les matrices W et H sont telles que :
-W ik est le score d'appartenance du document d i à la thématique z k .
-H kj est le score d'appartenance du mot w j à la thématique z k .
Nous définissons trois transformations vers l'espace latent pour trois méthodes issues des principales approches présentées dans la section 2 (LDA, NMF et FCM). Pour la méthode LDA, W ik est la probabilité p(d i |z k ), et H kj est la probabilité p(z k |w j ). Pour la méthode NMF, les deux matrices sont directement obtenues par factorisation. Pour la méthode FCM, W ik est le degré d'appartenance du document d i à la classe z k , et le vecteur H k * est le centroïde de la classe z k . Les deux matrices W et H doivent être normalisées (si elles ne l'étaient pas déjà) afin d'avoir un même ordre de grandeur quelque soit la méthode utilisée et d'éviter ainsi un biais éventuel dans le calcul de la mesure : -|Z| k=1 W ik = 1, ?i ? {1..|D|} (normalisation des lignes de W ).
|Z|} (normalisation des lignes de H).
Où Z est l'ensemble de thématiques et D est l'ensemble de documents. Sous ces hypothèses, nous définissons score(d i , w j ) le score de vraisemblance d'une occurrence du mot w j dans le document d i comme suit :
En d'autres termes, score(d i , w j ) est obtenu en multipliant la ligne de la matrice W qui correspond au document d i (i ème ligne) par la colonne de la matrice H qui correspond au mot w j (j ème colonne). Ensuite, le score de vraisemblance d'un document d est défini comme suit, V étant l'ensemble des mots du corpus (vocabulaire) :
w?V Où n(d, w) est le nombre d'occurrences du mot w dans le document d. En passant au log :
w?V La mesure V G est basée sur la moyenne géométrique des scores individuels sur les documents, score(d i ), chaque score étant lui-même un produit calculé sur chaque mot du vocabulaire (équation 5). La multiplication géométrique a donc la forme d'un produit de produits. Pour normaliser, il suffit de mettre à la puissance inverse du nombre de termes dans la multiplication. Celui-ci est égal à la double somme n(d, w). Au final, la mesure V G est calculée avec d?D w?V la formule suivante :
La mesure V G peut être calculée sur un corpus de test différent du corpus sur lequel les thé-matiques sont extraites (corpus d'apprentissage) mais ceci suppose que le modèle soit prédictif, c'est-à-dire capable d'affecter les nouveaux documents de test aux thématiques déjà extraites. Ceci n'est malheureusement pas le cas de toutes les méthodes, notamment les méthodes d'apprentissage non supervisé. Afin de mieux interpréter le résultat de la mesure V G, nous avons donc choisi, pour le moment, de l'évaluer en ne travaillant que sur le corpus d'apprentissage.
Expérimentations
Dans cette section, nous présentons le protocole expérimental (corpus, prétraitements, outils, paramètres des méthodes, etc.), ainsi que les résultats et la discussion. 
Protocole expérimental
Les tests sont effectués sur deux corpus : AP et Elections. AP est un corpus de documents de l'agence de presse Associated Press (Harman, 1993), également utilisé dans (Blei et al., 2003). Elections est un corpus de documents Web (médias, blogs, réseaux sociaux, etc.), qui traitent des élections présidentielles françaises de 2012. Ces documents ont été collectés durant la période du 16/03/2012 au 16/04/2012 par la plateforme de veille AMIEI (http ://www.amisw.com). Le tableau 1 résume les contenus des deux corpus, après les pré-traitements suivants :
-Suppression de mots outils (stopwords), par exemple le, sur, dans.
-Racinisation (stemming), par exemple les mots logement, loger deviennent log.
-Suppression des mots qui occurrent une seule fois dans le document.
Les tests sont réalisés en choisissant une méthode de chaque famille : LDA pour les modèles de thématiques probabilistes, NMF pour les méthodes à base de factorisation de matrices et FCM pour les méthodes à base de distance. Afin de limiter le risque de tomber dans des optima locaux, le même test est réalisé 5 fois et la moyenne est retenue. Les paramètres de la méthodes LDA sont fixés comme suit : ? = 50, ? = 0.01, nombre d'itérations = 1000. Les paramètres de la méthode FCM sont fixés comme suit : m = 1.1, nombre maximum d'ité-rations = 20. Pour exécuter LDA, nous nous sommes appuyés sur l'outil Mallet (McCallum, 2002). Pour NMF, nous avons utilisé notre propre implémentation, et pour FCM, nous avons utilisé le langage R (R, 2012 
Résultats et discussion
Les résultats d'exécution des trois méthodes LDA, NMF et FCM sont représentés dans le tableau 2. La comparaison des trois méthodes par la mesure V G est représentée dans la figure 2. Les résultats sur les cas extrêmes sont représentées dans la figure 3. Les méthodes LDA et NMF présentent un comportement similaire au vu de la variation de la mesure V G en fonction du nombre de thématiques (cf. figure 2). En effet, cette dernière augmente avec l'augmentation du nombre de thématiques. Ceci est en concordance avec l'intuition car un trop petit nombre de thématiques mène à mélanger plusieurs thématiques dans une seule, et donne ainsi des résultats de moins bonne qualité. En revanche, un grand nombre de thématiques permet de mieux séparer les thématiques, permet aux thématiques de petite taille d'émerger, et donne ainsi un résultat de meilleure qualité. Si le nombre de théma-tiques est encore plus grand (proche du nombre de documents), les résultats convergent vers un modèle où une thématique est extraite pour chaque document. La valeur de la mesure V G continue à augmenter sans pour autant que le résultat soit forcément de meilleure qualité. Ce problème est similaire au problème de surapprentissage (overfitting) connu dans le domaine de l'apprentissage statistique. LDA demeure la méthode qui donne les meilleurs résultats, en termes de la mesure V G, par rapport à NMF et FCM, et ce sur les deux corpus (cf. figure 2). La qualité des résultats donnés par la méthode FCM est remarquablement inférieure, en termes de la mesure V G, à celle des deux autres méthodes. Ceci est conforme aux exemples donnés sur le tableau 2. En effet, les thématiques extraites par la méthode FCM sont mélangées et très difficiles à interpréter.
L'objectif du test sur les cas extrêmes est d'analyser le comportement de la mesure V G dans les deux cas extrêmes Crisp et Uniforme (cf. section 4.1). Suivant V G, Crisp et Uniforme sont des configurations moins bonnes que celle produite par NMF (cf. figure 3). Cela confirme que ces deux cas extrêmes ne donnent pas de bons résultats et qu'un bon ensemble de thématiques constitue en général un compromis entre les deux extrêmes, à savoir quelques thématiques pertinentes pour un document.
Conclusion
Les méthodes d'extraction de thématiques, étant issues de domaines variés, produisent des résultats de forme hétérogène, ce qui empêche leur comparaison de manière uniforme. Dans cet article, nous avons proposé une mesure d'évaluation, la Vraisemblance Généralisée, qui permet d'évaluer dans un cadre commun les méthodes d'extraction de thématiques. Pour calculer la mesure, les résultats de ces méthodes sont transformés dans un espace latent qui plonge les documents dans l'espace latent des thématiques.
La mesure de qualité V G a permis de comparer trois méthodes d'extraction de thématiques (LDA, NMF et FCM) sur deux corpus différents. Les résultats ont donné l'avantage à la mé-thode LDA, suivie de NMF puis de FCM. Les résultats donnés par la méthode d'apprentissage FCM étaient d'une qualité inférieure, en termes de la mesure V G, par rapport aux deux autres méthodes. Ceci nous semble conforme avec une analyse qualitative des thématiques extraites par cette méthode. En effet, ces dernières étaient mélangées et très difficiles à interpréter.
Il serait intéressant, en complément à ce travail, de tester le comportement de la mesure V G sur des corpus de test (différents des corpus d'apprentissage). Cela nécessiterait la définition des opérations de prédiction pour les méthodes d'extraction de thématiques afin de pouvoir affecter les nouveaux documents aux thématiques. Il serait également intéressant d'envisager des analyses plus poussées afin de vérifier que l'on n'introduit pas de biais dans la comparaison des méthodes avec notre approche de transformation des résultats vers l'espace latent. En effet, il se peut que les transformations employées jouent en désavantage de certaines méthodes en dégradant ainsi artificiellement la qualité de leurs résultats.

Introduction
Les cartes cognitives (Axelrod, 1976) sont un modèle de base de connaissances populaire pour aider à la prise de décision. Elles fournissent un moyen de communication visuel facile pour analyser un système complexe. Elles ont été utilisées dans de nombreux domaines, tels que la biologie, la sociologie, la politique. . . Une carte cognitive est un graphe où chaque noeud est un concept et des influences étiquettent chacune un arc avec une valeur. Celle-ci appartient à un ensemble prédéfini qui peut contenir des valeurs symboliques tel que {+, ?} (Axelrod, 1976) ou {nul, faible, moyen, fort} (Dickerson et Kosko, 1994) ou qui peut être un intervalle tel que [?1, 1] (Kosko, 1986). En combinant les valeurs des influences composant les chemins entre deux concepts, il est possible de calculer l'influence propagée entre ces deux concepts.
Il est difficile pour un concepteur de construire une carte cognitive tout en assurant sa qualité. En effet, il arrive parfois que deux chemins d'influence entre deux concepts mènent à des conclusions qui se contredisent. Cet article propose une méthode pour valider la qualité d'une carte cognitive. La validation est basée sur un critère de qualité qui est utilisé pour contrôler une base de connaissances. La validation est souvent partagée en deux catégories : la vérifica-tion et le test (Ayel et Laurent, 1991). La vérification est basée sur un critère qui ne nécessite pas d'information externe, autrement dit ce type de critère dépend uniquement de la cohérence interne de la base. Le test est basé sur un critère qui nécessite de l'information externe, comme une spécification de contraintes. Il y a eu de nombreux travaux sur des bases de connaissances mais il n'y a pas eu de travaux dédiés à la validation des cartes cognitives à notre connaissance. Pour vérifier une carte cognitive, cet article introduit la notion de carte non-ambiguë. Une carte cognitive est non-ambiguë si pour toute paire de concepts, les influences propagées sur les différents chemins entre ces deux concepts ne sont pas contradictoires avec l'influence propagée entre ces deux concepts. Pour tester une carte cognitive, cet article introduit la notion de carte cognitive cohérente. Pour cela, une spécification est définie. L'idée est de construire cette spécification sur une hiérarchie de concepts telle que les concepts les plus bas dans la hiérarchie sont les concepts de la carte. Grâce à cette hiérarchie et à la carte, on peut calculer l'influence hiérarchique (Chauvin et al., 2011) entre deux concepts de la hiérarchie selon les influences propagées entre les concepts spécialisant ces deux concepts dans la carte. Une spécification est un ensemble de contraintes. Une contrainte est un triplet fait d'un concept source et d'un concept destination appartenant tout deux à la hiérarchie ainsi que d'une valeur. Une carte cognitive est cohérente avec une contrainte si la valeur de cette contrainte n'est pas contradictoire avec celle de l'influence hiérarchique entre les deux concepts. Cette même carte est cohérente avec une spécification si elle est cohérente avec l'ensemble des contraintes.
Dans la partie 2, le modèle classique des cartes cognitives ainsi que l'influence propagée entre deux concepts sont rappelés. Dans la partie 3, nous décrivons comment vérifier une carte cognitive. Dans la partie 4, nous décrivons comment tester une carte cognitive.
Carte cognitive
Une carte cognitive est un graphe orienté dans lequel les noeuds sont des concepts, représentés par de brefs textes. Les arcs représentent les influences entre ces concepts et sont étiquetés par une valeur d'influence appartenant à un ensemble de valeurs prédéfini.
Definition 1 (Carte cognitive) Une carte cognitive définie sur un ensemble de concepts C et un ensemble de valeurs I est un graphe orienté CM = (C, A, etiq I ) où les concepts de C sont les noeuds du graphe, A ? C ×C est un ensemble d'arcs appelés influences qui lient les concepts et etiq I : A ? I est une application étiquetant chaque arc de A avec une valeur de I.
Route sinueuse (RS)
Route secondaire (R2)
Exemple 1 Carte1 (figure 1) est définie sur l'ensemble de valeurs I = {+, ?}. Cette carte représente quelques causes d'accident de la route. La valeur + représente une influence positive. La valeur ? représente une influence négative. Ainsi, si on considère les concepts P (Pluie) et RG (Route glissante), circuler sous la pluie augmente les risques que la route soit glissante.
L'influence propagée d'un concept sur un autre est calculée selon les chemins d'influences minimaux entre les concepts et l'influence propagée sur chacun de ces chemins.
de c 1 à c 2 tel que p est une sous-séquence de p. On note P c1,c2 l'ensemble des chemins minimaux de c 1 à c 2 .
L'influence propagée IP sur un chemin d'influence p est la valeur de l'influence d'un concept sur un autre suivant ce chemin (Genest et Loiseau, 2007).
Definition 3 (Influence propagée sur un chemin)
L'influence propagée I entre deux concepts c 1 et c 2 agrège les influences propagées sur chacun des chemins minimaux entre ces deux concepts. Cette influence propagée introduit deux nouvelles valeurs. La valeur 0 signifie qu'il n'y a pas d'influence entre les deux concepts. La valeur ? signifie que l'influence entre les deux concepts est contradictoire : il existe deux chemins entre les concepts tels que leurs influences propagées sont opposées.
Definition 4 (Influence propagée entre deux concepts)
Exemple 2 Dans Carte1, il y a deux chemins minimaux entre Pluie et Accident :
Pour vérifier une carte cognitive, deux critères de qualité sont proposés : la propreté et la non-ambiguïté. Avant de définir ces critères, nous devons définir la non-contradiction entre une valeur et une autre. Une valeur est contradictoire avec une autre si elle exprime une information qui est opposée à l'autre. Avec l'ensemble de valeurs {+, ?}, une contradiction apparaît donc dès que deux valeurs sont différentes 1 .
Definition 5 (Non-contradiction) Soient i 1 , i 2 ? I deux valeurs. i 1 est non-contradictoire avec i 2 , noté i 1 i 2 , ssi i 1 = i 2 .
Une carte cognitive est propre si l'influence directe entre deux concepts n'est pas contradictoire avec l'influence propagée entre ces deux concepts.
Definition 6 (Carte cognitive propre) CM est une carte cognitive propre ssi :
La non-ambiguïté vérifie que les valeurs d'influences propagées sur les chemins entre deux concepts sont non-contradictoires avec la valeur de l'influence propagée entre ces concepts.
Definition 7 (Carte cognitive non-ambiguë) CM est une carte cognitive non-ambiguë ssi : ?c 1 , c 2 ? C, ?p ? P c1,c2 , IP(p) I(c 1 , c 2 )
Exemple 3 La carte cognitive de gauche de la figure 2 est non-propre tandis que la carte cognitive de droite est propre mais ambiguë. 
Test d'une carte cognitive
Pour tester une carte cognitive, trois nouvelles notions doivent être introduites : les notions de hiérarchie, de contrainte et de spécification. Une spécification et une hiérarchie sont fournies par le concepteur en vue de la validation de la carte cognitive.
Une hiérarchie H = (C, est un ensemble de concepts C associé à une relation de pré-ordre de simple héritage. Les concepts élémentaires d'un ensemble de concepts selon une hiérarchie sont les feuilles des arbres représentant la relation d'héritage. 
FIG. 3 -Une hiérarchie de concepts (H1).
Exemple 4 H1 (figure 3) est une hiérarchie ordonnant quelques concepts. Les concepts élé-mentaires de H1 apparaissent doublement encadrés.
L'influence ontologique (Chauvin et al., 2011) permet de calculer l'influence entre deux concepts d'une ontologie dans une carte cognitive ontologique. Ces définitions peuvent s'appliquer à la notion de hiérarchie moyennant une adaptation. On note elem H (c) les concepts élé-mentaires plus petits qu'un concept c selon la hiérarchie H. L'influence hiérarchique entre deux concepts est définie comme une agrégation des influences propagées entre tous les concepts élémentaires du premier concept et ceux du second concept. L'influence hiérarchique introduit elle aussi deux nouvelles valeurs : la valeur ? signifie que l'influence entre deux concepts n'est pas négative et la valeur signifie que l'influence entre deux concepts n'est pas positive.
Definition 8 (Influence hiérarchique entre deux concepts selon une hiérarchie) Soient c 1 , c 2 ? C deux concepts. L'influence hiérarchique entre c 1 et c 2 selon H est : Exemple 5  Exemple 6 Soient les contraintes C 1 = A, + et C 2 = MCT, + S1 = {C 1 , C 2 } construite sur H1 et {+, ?, 0, ?, ?} est une spécification.
Pour tester une carte cognitive, deux critères de qualité sont proposés : la cohérence et la compatibilité. Une carte cognitive est cohérente avec une contrainte si l'influence hiérarchique entre les concepts d'une contrainte n'est pas contradictoire avec la valeur de cette contrainte.
Definition 9 (Carte cognitive cohérente avec une spécification selon une hiérarchie) CM est cohérente avec c = 1 , c 2 , s selon H ssi I H (c 1 , c 2 ) s. CM est cohérente avec Spec selon H ssi CM est cohérente avec toutes les contraintes de Spec selon H.
Parfois, imposer une valeur stricte à une influence hiérarchique par l'intermédiaire d'une contrainte peut être trop contraignant pour l'utilisateur. Celui-ci peut préférer indiquer simplement que l'influence doit être proche de la valeur de la contrainte. Ainsi, plutôt que de vérifier si deux valeurs sont non-contradictoires, nous allons vérifier leur compatibilité. Intuitivement, les valeurs positives sont incompatibles avec les négatives et inversement. Pour les valeurs extrêmes situées aux bornes telles que + ou 0, la notion de compatibilité devient plus discutable. Si un utilisateur spécifie une contrainte ayant pour valeur +, nous considérons qu'il ne s'attend pas à obtenir 0. Si c'était le cas, il aurait plutôt dû fournir par exemple une contrainte de valeur ?. Enfin, nous considérons que la valeur ? n'est compatible qu'avec elle-même.
Definition 10 (Compatibilité d'une valeur) Soient i 1 , i 2 ? I + deux valeurs. On définit que i 1 est compatible avec i 2 , noté i 1 i 2 , tel que :
On définit la compatibilité d'une carte cognitive avec une spécification de manière similaire à la cohérence en utilisant cette fois la notion de compatibilité.
Definition 11 (Carte cognitive compatible avec une spécification selon une hiérarchie) CM est compatible avec c = 1 , c 2 , s selon H ssi I H (c 1 , c 2 ) s. CM est compatible avec Spec selon H ssi CM est compatible avec toutes les contraintes de Spec selon H.
Exemple 7 Carte1 est cohérente avec C 1 selon H1 car I H (MCT, A) = +. Cependant, elle n'est pas cohérente avec C 2 car I H (MT, MCT) = ?. Ainsi, elle n'est pas cohérente avec S1. En revanche, elle est compatible avec C 1 puisqu'elle est cohérente avec C 1 et est aussi compatible avec C 2 car I H (MT, MCT) +. Ainsi, Carte1 est compatible avec S1.
Conclusion
Nous avons présenté une formalisation de la validation des cartes cognitives pour laquelle nous avons introduit les critères de propreté et de non-ambiguïté. Le modèle des cartes cognitives est associé à une hiérarchie de manière à pouvoir tester la cohérence ainsi que la compatibilité d'une carte par rapport à une spécification de contraintes.

Introduction
Dans de nombreuses applications pratiques de gestion de clusters, résultat d'une opération de classification non supervisée (clustering), les objets à classifier évoluent dans le temps. Le but est donc d'obtenir des clusters optimaux à chaque pas de temps (Falkowski et al. (2006)). Tang et al. (2008) et Zhang et al. (2009) ont proposés des méthodes de clustering évolutif dans le but de produire des clusters optimaux qui reflètent les dérives à long terme dans les objets tout en étant robuste aux variations à court terme. Chi et al. (2009) ont développé cette idée en proposant deux cadres évolutifs pour le clustering spectral : PCQ (Preserving Cluster Quality) et PCM (Preserving Cluster Membership). Les deux cadres ont été proposés afin d'optimiser la modification de la fonction de coût proposée initialement par Chakrabarti et al. (2006).
Notre travail adapte le principe d'incrémentabilité afin de le généraliser à un ensemble d'algorithmes de clustering. Le cadre proposé consiste à estimer les états de données à l'aide des proximités qui sont à la fois actuels et passés. Puis, il effectue un clustering statique sur les estimations de ces états. Ce cadre de suivi de clustering évolutif d'une manière incrémen-tale a été utilisé pour étendre une variété d'algorithmes de clustering statiques tels que les C-Moyennes Floues (CMF) de Bezdek (1984), les k-moyennes de Mac-Queen (1967) et les approches spectrales de clustering présentées par Filippone et al. (2008).
Le reste de ce papier est organisé comme suit : en section 2, nous présentons le cadre évolutif du clustering. La section 3 présente les résultats d'expérimentation du cadre proposé sur une variété différente d'algorithmes de clustering statiques. La section 4 conclue le papier et présente les travaux futures.
Nous traitons le clustering évolutif comme étant un problème de suivi par un regroupement statique ordinaire. Pour ceci, nous étudions des matrices de proximité, notées W t , comme la réalisation d'un processus aléatoire non stationnaire indexé par des mesures de temps discrètes. Elles sont données par l'équation (1).
Où W t est une matrice déterministe inconnue des états non observés et N t est une matrice de bruit de moyenne nulle. ? t change au fil du temps pour réfléchir à long terme des dérives dans les proximités. Nous présentons une approche plus simple qui implique une mise à jour recursive des estimations de ces états en utilisant un seul paramètre ? nommé facteur d'oubli.
Estimation de la matrice de proximité
Une meilleure estimation peut être obtenue en utilisant un lissage de la matrice de proximité W t définie dans l'équation (2).
Cette matrice lissée est un candidat dans l'estimation de ? t . Des méthodes d'estimation ont été proposées dans Ledoit et Wolf (2003), Schäfer et Strimmer (2005) et Chen et al. (2010). D'une manière générale, ils calculent la différence entre la matrice de proximité réelle et lissée donnée par l'équation (3).
Puisque N t , N t?1 , ..., N 0 sont mutuellement indépendants et ont une moyenne nulle et la variance conditionnelle de W t?1 est nulle, le risque de l'espérance conditionnelle de la fonction de perte peut être alors exprimé dans l'équation (4).
La dérivée première correspondante au facteur d'oubli est donnée par l'équation (5). Pour les k-moyennes, la figure 2 montre que si nous estimons ? t , la valeur de l'indice de Rand (1971) est plus grande pour cadre évolutif. Par contre avec ? t = 0.5, la figure 2 montre qu'il y a égalité entre l'algorithme des k-moyennes dans les cadres statique et évolutif.
D'après la figure 3, nous remarquons que lorsque nous estimons ? t , la valeur de l'indice de Rand est plus grande dans le cadre évolutif que celui statique. L'algorithme de CMF statique ne fonctionne pas bien dès que les clusters commencent à se chevaucher vers environ le 9 ème pas de temps. Avec ? t = 0.5, les deux cadres fournissent les mêmes résultats. Pour assurer le bon fonctionnement du cadre proposé, nous allons étendre sa comparaison avec les résultats de deux cadres PCQ et PCM de Chi et al. (2009). Le tableau 1 montre que le cadre proposé avec l'algorithme de CMF reste toujours plus efficace que les deux cadres PCQ et PCM puisque le résultat de la valeur de l'indice de Rand (1971)  De même, nous comparons l'approche spectrale de clustering évolutif avec celle statique avec un ? t estimé et fixe. La figure 3 montre que l'algorithme fonctionne bien avec l'aspect incrémental et donne une valeur de l'indice de Rand (1971) plus grande que celle obtenue avec le cadre statique. Ce qui implique le bon fonctionnement du cadre avec les approches spectrales.
Avec ? t = 0.5, nous remarquons, d'après 3, que le comportement est le même pour les deux cadres statique et évolutif.
Conclusion
Le cadre proposé dans ce travail surpasse généralement par celui statique en produisant des clusters qui reflètent les tendances à long terme tout en étant robuste aux variations à court terme. Il est universel dans le sens qu'il permet à n'importe quel algorithme de clustering statique d'être étendu à un caractère évolutif qui fournit une méthode explicite pour la sélection du facteur d'oubli, contrairement aux méthodes existantes. L'objectif était de suivre avec pré-cision la matrice réelle de proximité à chaque pas de temps. Cela a été accompli en utilisant une mise à jour récursive avec un facteur d'adaptation d'oubli qui contrôle la quantité de poids à appliquer aux données historiques.
L'expérimentation a donné des résultats reflétant la performance du cadre adapté dans la performance de l'opération du clustering par rapport à celle statique.
Comme perspectives de travail, nous proposons d'élargir les expérimentations sur d'autres jeux de données et la possibilité d'étendre ce cadre afin qu'il puisse supporter des larges BD par réduction de l'échelle et l'échantillonnage des données.

Introduction
Dans un projet de fouille de données, la phase de préparation des données vise à extraire une table de données pour la phase de modélisation (Pyle, 1999), (Chapman et al., 2000). La préparation des données est non seulement coûteuse en temps d'étude, mais également critique pour la qualité des résultats escomptés. La préparation repose essentiellement sur la recherche d'une représentation pertinente pour le problème à modéliser, recherche qui se base sur des étapes complémentaires de construction et de sélection de variables. La sélection de variables a été largement étudiée dans la littérature (Guyon et al., 2006). Dans ce papier, nous nous focalisons sur l'approche filtre, qui évalue la corrélation entre les variables explicatives et la variable cible indépendamment de la méthode de classification utilisée, et est adaptée à la phase de préparation des données dans le cas d'un grand nombre de variables descriptives.
La construction de variables (Liu et Motoda, 1998) est un sujet nettement moins étudié dans la littérature scientifique, qui représente néanmoins un travail considérable pour l'analyste de données. Celui-ci exploite sa connaissance du domaine pour créer de nouvelles variables potentiellement informatives. En pratique, les données initiales sont souvent issues de bases de données relationnelles et ne sont pas directement exploitables pour la plupart des techniques de classification qui exploitent un format tabulaire attributs-valeurs. La fouille de données relationnelle, en anglais Multi-Relational Data Mining (MRDM), introduit par (Knobbe et al., 1999) vise à exploiter directement le formalisme multi-tables, en transformant la représen-tation relationnelle. En programmation logique inductive (ILP) (Džeroski et Lavra?, 2001), les données sont recodées sous forme de prédicats logiques. D'autres méthodes, dénommées propositionalisation Kramer et al. (2001) effectuent une mise à plat au format tabulaire de la représentation multi-tables par création de nouvelles variables. Par exemple, la méthode Relaggs (Krogel et Wrobel, 2001) exploite des fonctions de type moyenne, médiane, min, max pour résumer les variables numériques des tables secondaires ou des comptes par valeur pour les variables catégorielles secondaires. La méthode Tilde (Blockeel et al., 1998), (Vens et al., 2006) permet de construire des agrégats complexes exploitant des conjonctions de condition de sélection d'individus dans les tables secondaires. L'expressivité de ces méthodes se heurte néanmoins aux problèmes suivants : complexité du paramétrage de la méthode, explosion combinatoire du nombre de variables construites difficile à maitriser et risque de sur-apprentissage croissant avec le nombre de variables produites.
Dans ce papier, nous proposons un cadre visant à automatiser la construction de variables, avec un objectif de simplicité, de maîtrise de la combinatoire de construction des variables et de robustesse vis-à-vis du sur-apprentissage. La partie 2 présente un formalisme de description d'un domaine de connaissance, basé sur un format des données multi-tables en entrée et une liste des règles de construction de variables. La partie 3 introduit un critère d'évaluation des variables construites selon un approche Bayesienne, en proposant une distribution a priori sur l'espace des variables constructibles. La partie 4 analyse le problème d'échantillonnage dans cet espace et propose un algorithme efficace et calculable pour produire des échantillons de variables construites de taille désirée. La partie 5 évalue l'approche sur de nombreux jeux de données. Finalement, la partie 6 conclut cet article et propose des pistes de travaux futurs.
Spécification d'un domaine de construction de variables
Nous proposons dans cette section un cadre formel pour spécifier un domaine de connaissance permettant de piloter efficacement la construction de variables potentiellement utiles pour la classification. L'objectif n'est pas ici de proposer un nouveau formalisme expressif et généraliste de description de connaissance, mais simplement de préciser le cadre sur lequel s'appuieront les algorithmes de construction de variables présentés dans la partie 4. Ce domaine de connaissance se décline en deux parties : description de la structure des données et choix des règles de construction de variables utilisables.
Structure des données
La structure la plus simple est la structure tabulaire. Une instance est représentée par une liste de variables, chacune étant définie par son nom et son type. Les types usuels, numérique ou catégoriel, peuvent être étendus à d'autres types spécialisés, comme par exemple les dates, les heures ou les textes. Les données réelles étant souvent issues de bases de données relationnelles, il est naturel d'étendre cette structure au cas multi-tables. On propose ici de prendre en compte ces structures en s'inspirant des langages informatiques structurés ou orientés objets. L'objet d'étude statistique (instance) appartient à une table principale. Un objet principal est alors défini par une liste de variables, dont les types peuvent être simple (numérique, caté-goriel...) comme dans le cas tabulaire, ou structuré : sous-objets (enregistrements d'une table secondaire en relation 0-1) ou tableau de sous-objets (enregistrement d'une table secondaire en relation 0-n). Dans le cas de la classification supervisée, la variable cible est une variable caté-gorielle de l'objet principal. La figure 1 présente un exemple de l'utilisation de ce formalisme.
L'objet principal est le client (Customer), avec des objets secondaires usages en relation 0-n. Les variables sont de type simples (Cat, Num ou Date) ou structurés (ObjectArray(Usage)). Les variables identifiant (ici préfixées par #) servent essentiellement à établir le lien avec une base relationnelle ; elles ne sont pas considérées comme des variables descriptives. -TableSelection(ObjectArray, Num)->ObjectArray : sélection d'objets du tableau selon une conjonction de critères d'appartenance à des intervalles ou d'égalité à des valeurs -TableCount(ObjectArray)->Num : effectif du tableau -TableMode(ObjectArray, Cat)->Cat : valeur la plus fréquente -TableCountDistinct(ObjectArray, Cat)->Num : nombre de valeurs différentes -TableMean(ObjectArray, Num)->Num : valeur moyenne -TableMedian(ObjectArray, Num)->Num : valeur médiane -TableMin(ObjectArray, Num)->Num : valeur minimum -TableMax(ObjectArray, Num)->Num : valeur maximum -TableStdDev(ObjectArray, Num)->Num : écart type -TableSum(ObjectArray, Num)->Num : somme des valeurs En exploitant la structure des données présentée sur la figure 1 et les règles de construction précédentes (enrichies ici de la règle YearDay pour les dates), on peut par exemple construire les variables suivantes pour enrichir la description d'un client :
-MainProduct = TableMode(Usages, Product), -LastUsageYearDay = TableMax(Usages, YearDay(useDate)), -NbUsageProd1FirstQuarter = TableCount(TableSelection(Usages, YearDay(useDate) ? [1 ;90] and Product = "Prod1").
Evaluation des variables construites
Il s'agit d'exploiter les connaissances du domaine pour piloter efficacement la construction de variables potentiellement utiles pour la prédiction de la variable cible. Dans le formalisme introduit en partie 2, la structure des données peut avoir plusieurs niveaux de profondeur, voire posséder la structure d'un graphe. Par exemple, une molécule est un graphe dont les noeuds sont les atomes et les arcs sont les liaisons entre atomes. Les règles de calcul peuvent être utilisées comme opérandes d'autres règles, conduisant à des formules de calcul de longueur quelconque. On est alors confronté à un espace de variables constructibles en nombre potentiellement infini. Cela pose les deux problèmes principaux suivants :
1. explosion combinatoire pour l'exploration de cet espace, 2. risque de sur-apprentissage.
On propose une solution à ces problèmes en introduisant un critère d'évaluation des variables selon une approche Bayesienne permettant de pénaliser les variables complexes. On introduit à cet effet une distribution a priori sur l'espace de toutes les variables, et un algorithme efficace d'échantillonnage de l'espace des variables selon leur distribution a priori.
Evaluation d'une variable
La construction de variables vise à enrichir la table principale par de nouvelles variables qui seront prises en entrée d'un classifieur. Les classifieurs usuels prenant en entrée uniquement des variables numériques ou catégorielles, on ne s'intéresse qu'à l'évaluation de ces variables.
Prétraitement supervisé. Le prétraitement supervisé MODL 1 consiste à partitionner une variable numérique en intervalles ou une variable catégorielle en groupes de valeurs, avec une estimation de densité conditionnelle constante par partie. Les paramètres d'un modèle de pré-traitement sont le nombre de parties, la partition, et la distribution multinomiale des classes dans chaque partie. Dans l'approche MODL, le prétraitement supervisé est formulé en un problème de sélection de modèles, traité selon une approche Bayesienne en exploitant un a priori hiérarchique sur les paramètres de modélisation. Le meilleur modèle est le modèle MAP (maximum a posteriori). En prenant le log négatif de ces probabilités, qui s'interprète comme une longueur de codage (Shannon, 1948) dans l'approche minimum description length (MDL) (Rissanen, 1978), cela revient à minimiser la longueur de codage d'un modèle d'évaluation M E (X) (via une partition supervisée) d'une variable X plus la longueur de codage des données D Y de classe connaissant le modèle et les données descriptives
On a c(X) ? N ent(Y |X) où N est le nombre d'individus de l'échantillon d'apprentissage et ent(Y |X) l'entropie conditionnelle de la variable de classe connaissant la variable descriptive. Le critère 1 et son optimisation sont détaillés dans (Boullé, 2006) pour la discrétisation supervisée et dans (Boullé, 2005) pour le groupement de valeurs supervisé.
Modèle nul et filtrage des variables. Le modèle nul M E (?) correspond au cas particulier d'une seule partie (intervalle ou groupe de valeurs) et donc de la modélisation directe des classes via une multinomiale, sans utiliser la variable descriptive. La valeur du critère c(?) revient au cout d'encodage direct des classe cibles : c(?) ? N ent(Y ). Le critère d'évalua-tion d'une variable est alors utilisé dans une approche filtre de sélection de variables Guyon et al. (2006) : seules les variables dont l'évaluation est meilleure que celle du modèle nul sont considérées comme informatives et retenues à l'issue de la phase de préparation des données.
Prise en compte de la construction des variables. Quand le nombre de variables natives ou construites devient important, le risque qu'une variable soit considérée à tort comme informative devient critique. Afin de prévenir ce risque de sur-apprentissage, on propose dans cet article d'exploiter l'espace de construction de variables décrit dans la partie 2 en introduisant une distribution a priori sur l'ensemble des modèles M C (X) de construction de variables. On obtient alors une régularisation Bayesienne pour l'obtention des variables, permettant de pé-naliser a priori les variables les plus "complexes". Cela se traduit par un coût de construction L(M C (X)) supplémentaire dans le critère d'évaluation des variables, qui devient celui de la formule 2.
L(M C (X)) est le log negatif de la probabilité a priori (longueur de codage) d'une variable X native ou construite, que nous définissons maintenant.
Distribution a priori des variables
Une variable à évaluer est une variable numérique ou catégorielle de la table principale, soit native, soit obtenue par application récursive des règles de construction de variables. L'espace des variables ainsi défini étant potentiellement infini, définir une probabilité a priori sur cet espace pose de nombreux problèmes et implique de nombreux choix. Afin de guider ces choix, on propose de suivre les principes généraux suivants :
1. la prise en compte de variables construites a un impact minimal sur les variables natives, 2. l'a priori est le plus uniforme possible, pour minimiser les biais, 3. l'a priori exploite au mieux les connaissances du domaine.
Cas des variables natives. Dans le cas où il n'y a pas de variables constructibles, le problème se limite au choix d'une variable à évaluer parmi K variables numériques ou catégorielles de la table principale. En utilisant un a priori uniforme pour ce choix, on a p(
Cas des variables construites. Dans le cas où des variables sont constructibles, il faut d'abord choisir si l'on utilise une variable native ou construite. Un a priori uniforme (p = 1/2) sur ce choix implique un surcoût de log 2 pour les variables natives, ce qui contrevient au principe d'impact minimal sur les variables natives. On propose alors de considérer le choix d'une variable construite comme une possibilité de choix supplémentaire en plus des K variables natives. Le coût d'une variable native devient alors L(M C (X)) = log(K + 1), avec un impact minimal de log(1 + 1/K) ? 1/K par rapport au cas sans construction de variables.
Choisir une variable construite repose alors sur la hiérarchie suivante de choix : -choix de construire une variable, -choix de la règle de construction parmi les R règles de construction applicables (de code retour numérique ou catégoriel), -pour chaque opérande de la règle, choix d'une variable native ou construite, dont le type est compatible avec le type de l'opérande attendu. En utilisant un a priori hiérarchique, uniforme à chaque niveau de la hiérarchie, le coût d'une variable construite se décompose sur les opérandes de la règle R utilisée selon la formule récursive 3, où les variable X op sont les variables natives ou construites applicables en opérandes de la règle.
op?R Le cas de la règle TableSelection qui extrait une sous-table d'objets selon une conjonction de critères exploite de façon similaire une hiérarchie de choix : nombre de variables de sélec-tion, liste des variables de sélection et pour chaque variable de sélection, choix de l'opérande de sélection (intervalle ou valeur), avec dans le cas des intervalles, choix de la granularité (nombre de partiles) et de l'intervalle (index du partile).
La figure 2 donne un exemple d'une telle distribution a priori sur l'ensemble des variables constructibles à l'aide des règles de construction TableMode, TableMin, TableMax et YearDay, pour le problème de gestion de la relation client présenté sur la figure 1. Par exemple, le coût de sélection de la variable native Age est L(M C (Age)) = log 3. Celui de la variable construite T ableM in(U sages, Y earDay(Date)) exploite une hiérarchie de choix, conduisant à L(M C (T ableM in(U sages, Y earDay(Date)))) = log 3+log 3+log 1+log 1+log 1.
FIG. 2 -Distribution de probabilité a priori pour la construction de variables, pour un problème de gestion de la relation client
L'a apriori de construction de variables ainsi défini correspond à une hiérarchie de distributions multinomiales de profondeur potentiellement infinie (HDMPI). Les variables natives sont obtenues dès le premier niveau de la multinomiale, alors que les variables construites ont des probabilités a priori d'autant plus faibles qu'elles sont complexes et exploitent les parties profondes du prior HDMPI.
Construction d'un échantillon de variables
L'objectif est ici de construire un nombre donné de variables pour créer une représentation des données potentiellement informative pour la classification supervisée. Nous proposons de construire un échantillon de variables par tirage selon leur distribution a priori. Nous présen-tons un premier algorithme "naturel" de construction d'échantillon, et démontrons qu'il n'est ni efficace ni calculable. Nous proposons alors un second algorithme répondant au problème.
Tirages aléatoires successifs
Algorithm 1 Tirages aléatoires successifs Require: K {Nombre de tirages} Ensure: V = {V }, |V| ? K {Echantillon des variables construites}
Tirer V selon le prior HDMPI 4:
Ajouter V dans V 5: end for L'algorithme 1 consiste à tirer successivement K variables selon le prior HDMPI. Chaque tirage consiste à partir de la racine de l'arbre du prior et à descendre dans la hiérarchie par des tirages multinomiaux successifs, jusqu'à l'obtention d'une variable native ou construite, ce qui correspond à une feuille de l'arbre du prior. Cet algorithme naturel ne peut être utilisé dans le cas général, car il n'est ni efficace, ni calculable, ce que nous démontrons ci-dessous.
L'algorithme 1 n'est pas efficace. Soit un domaine de connaissance comportant V variables natives évaluables dans la table principale et pas de variable constructible. Le prior HDMPI se réduit à une multionomiale avec V valeurs équidistribuées. Si l'on effectue K tirages selon cette multinomiale, l'espérance du nombre de variables différentes obtenues est V (1?e ?K/V ) (Efron et Tibshirani, 1993). Dans le cas ou K = V , on retrouve la taille d'un échantillon boostrap, à savoir 1 ? 1/e ? 63% des variables obtenues avec V tirages. Pour obtenir 99% des variables, il faut K ? 5V tirages, ce qui n'est pas efficace. Si de plus, il y a des variables constructibles, la multinomiale à la racine du prior HDMPI comporte cette fois K + 1 choix équidistribués. Les tirages ne permettent de construire des variables que dans 1/(K + 1)% des cas. Il est à noter que ce problème d'innefficacité se produit à tous les niveaux de profondeur du prior HDMPI pour le tirage des opérandes des règles en cours de construction.
L'algorithme 1 n'est pas calculable. Soit un domaine de connaissance comportant une seule variable native numérique x et une seule règle de construction f (Num, Num)->Num. Les variables que l'on peut construire sont x, f (x, , x), x))... Le nombre de Catalan C n = (2n)! (n+1)!n! permet de dénombrer de telles expressions. C n correspond au nombre de façons différentes de placer des parenthèses autour de n+1 facteurs ou au nombre d'arbres binaires à n + 1 feuilles. Chaque variable correspondant à un arbre binaire à n feuilles (nombre de x dans la formule) vient en C n?1 exemplaires différents, chacun avec un probabilité a priori de 2 ?(2n?1) selon le prior HDMPI. On peut alors calculer l'espérance de la longueur s(V ) d'une variable calculée (en nombre de feuilles dans son arbre binaire de calcul). En utilisant la formule de Stirling pour approximer les nombres de Catalan, on établit dans la formule 4 que cette espérance est infinie.
Cela signifie que si l'on tire au hasard une variable selon le prior HDMPI, parmi l'ensemble des expressions faisant intervenir f et x, l'algorithme 1 mettra en moyenne un temps infini avant de produire une variable. L'algorithme 1 n'est donc pas calculable dans le cas général. Propager la construction de façon récursive en distribuant les K i tirages sur la multinomiale du sous-niveau 9:
Tirages aléatoires simultanés
end if 10: end for Comme il n'est pas possible de tirer les variables individuellement, nous proposons de créer un échantillon directement sur la base de plusieurs tirages simultanés. Dans le cas d'une loi multinomiale simple ayant K événements de probabilités (p 1 , p 2 , . . . , p K ), la probabilité qu'un échantillon de n tirages ait pour effectifs n 1 , n 2 , . . . , n K par évènement est :
L'échantillon le plus probable est obtenu en maximisant l'expression 5, qui par maximum de vraisemblance correspond aux effectifs n k = p k n. Dans le cas par exemple d'une multinomiale équidistribuée avec p k = 1/K et de n = K tirages, on remarque que l'expression 5 atteint son maximum pour n k = 1 et que donc tous les évènements sont tirés, ce qui remédie au problème d'efficacité décrit en partie 4.1. L'algorithme 2 exploite ce tirage par maximum de vraisemblance récursivement. Les tirages sont répartis selon les variables natives ou construites à chaque niveau du prior HDMPI, ce qui fait que le nombre de tirages demandés diminue en descendant dans la hiérarchie du prior. En cas d'égalité entre plusieurs choix (par exemple, 1 tirage pour K variables) le choix est fait au hasard, avec priorité aux variables natives en cas de choix entre variables natives ou construites. En répartissant les tirages par branche de la hié-rarchie du prior HDMPI, avec des effectifs décroissants selon la profondeur de la hiérarchie, l'algorithme 2 est à la fois efficace et calculable. La taille de l'échantillon obtenu étant potentiellement inférieure au nombre de tirages demandés, on réitère l'appel à l'algorithme 2 en doublant le nombre de tirage demandés à chaque appel, jusqu'à obtenir le nombre de variables désiré ou qu'aucune variable supplémentaire ne soit produite entre deux appels successifs.
Evaluation
Nous évaluons la méthode proposée en nous focalisant sur les axes suivants : capacité à générer de grands nombres de variables sans problème d'explosion combinatoire, résistance au sur-apprentissage et apport pour la prédiction.
Bases multi-tables de petite taille
Dans cette première évaluation, nous utilisons 20 bases issues de la communauté du data mining multi-tables, en ignorant les variables de la table principale et nous focalisant sur la construction de variables par propositionalisation au moyen des règles de construction présen-tées dans la partie 2.2. Afin d'avoir un résultat de référence, on utilise une méthode apparentée à Relaggs (Krogel et Wrobel, 2001), basée sur l'utilisation des mêmes règles de construction (celles de la partie 2.2, excepté TableSelection), et de l'effectif par valeur pour chaque variable secondaire catégorielle. Suite à cette construction de variable, on exploite un prédicteur Bayesien naif avec sélection de variables et moyennage de modèles (SNB) (Boullé, 2007), qui est à la fois robuste et performant dans le cas de très grands nombres de variables.
Les bases utilisées 2 sont des bases issues du traitement d'image (bases Elephant, Fox, Tiger, et base Miml, avec les variables cibles Desert, Mountains, Sea, Sunset, Trees), de la chimie moléculaire (Diterpenses , Musk1, Musk2, et Mutagenesis avec trois représentations), de la médecine (Stulong, avec les variables cibles Cholrisk, Htrisk, Kourisk, Obezrisk et Rarisk), ainsi que la base TicTacToe (Asuncion et Newman, 2007) traitée en multi-tables avec les neuf cases du jeu en table secondaire. Ces bases sont de petite taille, avec de 100 à 2000 individus.
En utilisant l'algorithme 2, nous contrôlons la taille de la représentation en générant 1, 10, 100, 1000, 10000 et 100000 variables par jeu de donnée dans les échantillons d'apprentissage dans un processus en validation croisée stratifiée à 10 niveaux, ce qui représente au total environ 20 millions de variables construites.
Evaluation de la performance. Dans une première analyse, nous collectons l'AUC moyenne en test pour chaque nombre de variables générées. La méthode Relaggs, qui s'appuie sur une construction de variables par application systématique de règles, ne permet pas de contrô-ler la combinatoire du nombre de variables produites, qui ici varie d'une dizaine à environ 1400 variables selon la base. Elle reste néanmoins applicable dans le cas de petites bases et fournit ici une performance de base compétitive. Les résultats présentés sur la figure 3 montrent que la performance de notre approche croit systématiquement avec le nombre de variables construites, et atteint ou dépasse la performance de Relaggs sur 17 des 20 bases quand le nombre de variables construites est suffisant. Pour trois des bases (Fox, Musk1 et Musk2), les performance de notre approche sont nettement moins bonnes que celles de Relaggs. Ces trois bases sont soit très bruitées (Fox avec AUC=0.7) soit très petites (Musk1 et Musk2 avec moins de 100 instances) et correspondent exactement aux trois bases avec la plus grande variance des résul-tats pour Relaggs (entre 10 et 15% d'écart type de l'AUC). Pour ces trois bases, le nombre d'instances est insuffisant pour compenser le coût de régularisation (critère 2) : la plupart des variables construites sont alors éliminées, ce qui fait chuter la performance. 
FIG. 3 -AUC en test en fonction du nombre variables construites
Evaluation de la robustesse. Dans une seconde analyse, l'expérience est réalisée après ré-affectation aléatoire des classes pour chaque jeu de données, afin d'évaluer la robustesse de l'approche. Nous collectons le nombre de variables sélectionnées, avec (critère 2) ou sans (critère 1) prise en compte du coût de construction des variables. Sans régularisation de construction, en moyenne 0.2% des variables sont sélectionnées à tort, avec des pourcentages diminuant avec la taille de la base (de 0.8% pour les plus petites bases à 0.02% pour les plus grandes). Par exemple, pour la base Musk1 qui comprend 92 instances, plus de 700 variables parmi 100000 sont identifiées à tort comme informatives. Avec prise en compte de la régularisation de construction, la méthode est extrêmement robuste : les 20 millions de variables générées sont identifiées comme étant non informatives, sans aucune exception.
Bases de taille moyenne
Dans une seconde évaluation, nous utilisons trois bases de taille moyenne : Connect4, PokerHands et Digits (Asuncion et Newman, 2007). La base Connect4 correspond à l'ensemble des positions légales après 8 coups du jeu "Puissance 4". Il s'agit de prédire l'issue de la partie (victoire, nul, ou défaite) pour deux joueurs jouant les coups parfaits (le jeu a été "cassé"). Le format tabulaire initial est représenté sous forme multi-tables, avec une table secondaire comportant les 6 * 7 = 42 cellules du jeu avec trois variables secondaires X, Y et V al. 70% des 67557 instances sont utilisées en apprentissage et 30% en test. La base PokerHand correspond à une main de cinq cartes au poker, avec le rang et la couleur (variables numériques) de chaque carte. Il s'agit de reconnaitre le type de main parmi 10 classes (rien, paire, double paire, brelan...). Le format tabulaire initial est représenté sous forme multi-tables, avec une table secondaire comportant les cinq cartes d'une main. 25000 instances sont utilisées en apprentissage et 1000000 en test. La base Digits est une base d'images décrites par 28 * 28 = 784 pixels, avec pour objectif la reconnaissance d'un chiffre manuscrit de 0 à 9. Le format tabulaire initial est représenté sous forme multi-tables, avec une table secondaire comportant les 784 pixels avec trois variables secondaires X, Y et GrayLevel. 60000 instances sont utilisées en apprentissage et 10000 en test.
Nous évaluons le taux de bonne prédiction en test, plus discriminant ici que l'AUC. Les trois problèmes sont difficiles dans leur représentation initiale pour le prédicteur SNB, qui obtient des performances de 72.4%, 50.1% et 87.5%. La figure 4 montre que les performances de notre approche croissent avec le nombre de variables construites, et dépassent significativement celle du classifieur SNB pour la représentation initiale. 
Conclusion
Nous avons proposé dans cet article un cadre visant à automatiser la construction de variables pour la classification supervisée. Sur la base d'une description d'un format multi-table des données et d'un choix de règles de construction de variables, nous avons défini une distribution a priori sur l'ensemble de toutes les variables constructibles au moyen de ce formalisme. Nous avons démontré qu'effectuer des tirages successifs selon cette distribution a priori pose des problèmes critiques d'efficacité et de calculabilité, puis proposé un algorithme efficace permettant d'effectuer simultanément un grand nombre de tirages de façon à construire une représentation comportant le nombre de variables désiré. Les expérimentations montrent que l'approche résout à la fois le problème de l'explosion combinatoire qui intervient dans les approches de construction de variables par application systématique de règles, et le problème de sur-apprentissage quand les représentations comportent de très grands nombres de variables. Les performances obtenues en classification sont très prometteuses. Dans des travaux futurs, nous prévoyons d'améliorer la prise en compte des connaissances du domaine, en premier lieu en étendant la liste des règles de construction disponibles avec des spécialisations potentielles par domaine applicatif. Une autre piste de travail consiste à rechercher les meilleures variables à construire, en échantillonnant leur distribution a posteriori plutôt que leur distribution a priori. 

Introduction
Le volume de données numériques actuellement disponible nécessitent de disposer de mé-thodes efficaces permettant de les structurer, résumer, comparer et regrouper. Dans tous ces cas, il est indispensable de disposer d'une mesure de similarité permettant d'évaluer la proximité entre les objets considérés. Les illustrations les plus récentes se situent dans le domaine de la bioinformatique pour l'alignement des sous-séquences d'ADN ou d'acides aminés ((Sander et Schneider, 1991;Chothia et Gerstein, 1997)) ou dans la détection d'intrusion dans les réseaux où les différentes séquences d'accès sont analysées et comparées à une base de signatures de comportements malveillants. En ce qui concerne les données séquentielles, de nombreux travaux ( (Levenshtein, 1966;Herranz et al., 2011;Keogh, 2002;Wang et Lin, 2007)) se sont intéressés à des séquences simples, c'est-à-dire une liste ordonnée d'éléments atomiques. Or, dès lors que l'on s'intéresse à des séquences d'objets plus complexes, le calcul de similarité se confronte à la nature même des objets comparés. Les trajectoires d'objets mobiles, les informations topologiques en biologie moléculaire ( (Wodak et Janin, 2002)) sont des exemples de telles données. Pour illustration, supposons que nous souhaitons comparer les trois séquences complexes suivantes : S 1 = b}{a, c}}, S 2 = b}{a, c}} et S 3 = d}{a, b}{c}{d}}. Le calcul classique de la plus longue sous-séquence commune entre S 1 et S 2 , noté LCS(S 1 , S 2 ), est la sous-séquence c}} de longueur 3. De même, LCS(S 1 , S 3 ) = b}{c}} de longueur 3. La mesure de la plus longue sousséquence commune nous amène à conclure que la séquence S 1 peut être considérée équidis-tante des séquences S 2 et S 3 . Or, la séquences S 1 est quasi identique à la séquence S 2 (hormis l'interversion des deux premiers ensembles). Il est donc important de comparer autrement l'impact des sous-séquences pour réellement mesurer la similarité de séquences d'objets complexes. Il suffit de comparer le nombre de sous-séquences communes qui est de 40 entre S 1 et S 2 alors qu'il est de 14 entre S 1 et S 3 . Ce résultat reflète mieux la similarité entre S 1 et S 2 car il prend en compte les différentes structures et combinaisons présentes dans une séquence complexe. La problématique se confronte à la combinatoire associée, l'efficacité computationelle et nous amène à poser les questions suivantes : (i) Etant donné une séquence complexe, comment compter sans énumérer le nombre de sous-séquences distinctes ? (ii) Pour un couple de séquences, comment efficacement compter le nombre de sous-séquences communes ? Dans ce contexte, notre contribution est double : un cadre théorique pour définir une mesure de similarité pour les séquences complexes basée sur le nombre de sous-séquences communes et un algorithme qui met en oeuvre de façon efficace la mesure de similarité proposée. Cette approche est basé sur la technique de la programmation dynamique afin de compter efficacement toutes les sous-séquences communes entre deux séquences.
L'article est organisé de la façon suivante. La section 2 présente les définitions préliminaires à notre proposition. Les sections 3 et 4 détaillent notre contribution, présentent de nouveaux résultats combinatoires et discutent la complexité ainsi que la complétude de notre algorithme. La section 5 présente deux études expérimentales. La section 6 fait le bilan et dresse les perspectives associées à ce travail.
Concepts préliminaires
Définition 1 (Séquence) Soit I un ensemble fini d' items. Un ensemble (ou itemset) X est un sous-ensemble non vide de I. Une séquence S est une liste ordonnée 1 · · · X n telle que
Définition 2 La similarité entre deux séquences S et T , notée sim(S , T ), est définie comme le nombre de toutes les sous-séquences communes entre S et T divisé par le nombre maximal de sous-séquences de S ou T : sim(S, T ) =
De manière usuelle, l'ensemble des parties d'un ensemble X est noté P(X) et P ?1 (X) est l'ensemble des parties de X sans l'ensemble vide (i.e., P ?1 (X) = P(X) \ {?}).  
Cette notion d'ensemble de positions critiques est cruciale dans notre approche puisqu'elle permet de focaliser les calculs uniquement sur les dernières positions où une répétition apparaît pour une séquence S donnée. Le lemme suivant formalise cette intuition.
Preuve 1 Voir le rapport technique Egho et al. (2012).
Remarque. Les éléments de l'ensemble ?(S ) • P ?1 (S[ ? Y ) ne sont pas néces-sairement disjoints. Pour s'en convaincre, considérerons la séquence S = b}{b, c}} et Y = {a, b, c}, alors L(S, Y ) = {1, 2}, et la séquence est construite deux fois dans les ensembles ?(
. Afin de prendre en compte ce chevauchement d'éléments, nous nous appuyons pour le calcul de R(S, Y ) sur le principe d'inclusion-exclusion.
Preuve 2 Voir le rapport technique Egho et al. (2012).
Exemple 2 Nous illustrons le processus complet de comptage ?( b, d}}).
Comptage efficace de toutes les sous-séquences communes
Dans cette section, nous allons étendre les résultats théoriques précédemment énoncés afin de compter toutes les sous-séquences distinctes communes entre deux séquences S et T . Comme pour la Section 3, nous présentons dans un premier temps, l'idée générale avant d'énoncer les résultats formels ainsi que l'algorithme de comptage. Supposons que nous concaténons la séquence S avec un itemset Y et observons la variation des ensembles ?(S, T ) et ?(S • Y, T ). Deux cas sont possibles : Si aucun item dans Y n'apparaît dans les itemsets des séquences S et T , alors la concaténation de l'itemset Y avec la séquence S n'a aucun effet sur l'ensemble ?(S • Y, T ) (i.e., le nombre de sous-séquences communes ne change pas, ?(S • Y, T ) = ?(S, T )). Ou si au moins un élément de Y apparaît dans l'une des séquences S ou T (ou les deux), alors de nouvelles séquences communes apparaissent dans ?(S • Y, T ). De la même manière que pour la méthode de comptage des sous-séquences distinctes d'une séquence unique, des répétitions peuvent se produire et il est nécessaire de définir un terme de correction. De manière formelle,
et A(S, T, Y ) représente le nombre de sous-séquences communes supplémentaires qui devraient être ajoutées au compte après la concaténation de Y et R(S, T, Y ) le terme de correction.
De même que pour le problème des sous-séquences distinctes pour une seule séquence, l'ensemble des positions critique joue un rôle dans le calcul de A(S, T, Y ) et R(S, T, Y ). Le lemme suivant formalise cette observation :
Preuve 3 Voir le rapport technique Egho et al. (2012).
Comme pour le Lemme 1, le calcul de
Preuve 4 Voir le rapport technique Egho et al. (2012).
Le théorème 2 permet de concevoir un algorithme simple qui s'appuie sur la technique de programmation dynamique. Pour deux séquences données S et T , de tailles n et m respectivement, l'algorithme produit une n × m-matrice, notée M , tel que la valeur de la cellule M i,j correspond au nombre de sous-séquences communes entre S i et T j (i.e., M i,j = ?(S i , T j )). Considérons les deux séquences S 1 et S 2 dans D ex , alors ?(S 1 , S 2 ) = 21. Le {?} {a} {b, c, d} {a, d} 
Expériences
Nous invitons le lecteur à lire la section des expérimentations dans le rapport technique Egho et al. (2012). où nous étudions le passage à l'échelle de notre mesure ainsi que son application dans le domaine de regroupement de séquences de protéines dans le domaine biologique.

Introduction
Traiter des données de plus en plus volumineuses fait partie des défis que se donne la communauté des chercheurs en fouille de données. Une des manières de répondre à ce défi est de définir des méthodes de fouille de données mettant en oeuvre le parallélisme, sur une ou plusieurs machines, et en utilisant le processeur (CPU) et/ou le calculateur graphique (GPU). Un certain nombre d'algorithmes ont déjà donné lieu à parallélisation sur GPU à la fois en fouille de données (voir un aperçu dans (Jian et al., 2011)), en visualisation scientifique (Weiskopf, 2006) ou en visualisation d'informations et plus spécifiquement en affichage de graphes (Frishman et Tal, 2007).
Dans le domaine de la fouille visuelle de données, les approches pouvant afficher le plus de données relèvent soit des approches orientées pixels et assimilées (voir (Keim et al., 1995) où 530.000 valeurs sont visualisées), soit des approches utilisant des densités (voir (Fua et al., 1999) où les coordonnées parallèles peuvent afficher jusqu'à 200.000 données). Cependant on peut constater que les interactions sont le point d'achoppement de ces méthodes visuelles pour le traitement de grands volumes de données (voir discussion dans (Florek, 2006)). Lorsque l'utilisateur formule une requête graphique, le système doit bien souvent faire des calculs supplémentaires et réafficher les données, partiellement ou entièrement, ce qui peut ralentir l'utilisateur dans son processus d'exploration (et rendre la méthode inutilisable).
Des approches de visualisation ont été étudiées pour remédier à ces inconvénients, comme pour le MDS (Ingram et al., 2009) en abaissant à la fois la complexité algorithmique et en parallélisant la méthode sur GPU. Cependant cette complexité est encore importante si l'on veut traiter des millions de données et il s'agit d'une approche peu interactive (sans redéfini-tion de l'espace de visualisation). Dans (Florek, 2006), l'objectif a été cette fois d'obtenir des temps d'affichage permettant les interactions. Le facteur d'accélération entre l'implémentation sur CPU (non parallélisée) et l'implémentation sur GPU va jusqu'à 60 (150.000 données en dimension 4 sont affichées en 0.3s sur GPU au lieu de 20s sur CPU). 
FIG. 1 -
TAB. 1 -Complexité et parallélisation choisie pour chacune des étapes de notre méthode en fonction du nombre de données n, de la dimension des données m, du nombre de POIs k et du nombre d'interactions réalisées I au cours d'une session utilisateur.
Notre objectif consiste donc à proposer une approche de fouille visuelle de données qui 1) puisse visualiser des volumes de données multidimensionnelles dépassant les limites actuelles des approches existantes, 2) permette à l'utilisateur d'avoir des interactions graphiques les plus rapides possible. Ces objectifs passent par le choix d'une méthode visuelle ayant une complexité faible et pouvant se paralléliser sur CPU et GPU. Nous avons donc choisi comme visualisation candidate une méthode répondant le mieux possible à ces critères (Da Costa et Venturini, 2006). Elle fait partie des méthodes radiales comme RadViz (Hoffman et al., 1999). Ces approches considèrent que des points d'intérêt (POIs, appelés aussi "ancres dimensionnelles"), sont disposés sur un cercle par exemple, et que les données viennent se positionner à l'intérieur du cercle en fonction de leur ressemblance avec les POIs (voir figure 1). Ainsi, les POIs peuvent représenter des cas particuliers importants parmi les données, ou des hypothèses à tester. Chaque choix ou configuration des POIs vient donner une disposition particulière des données. L'utilisateur dispose de plusieurs interactions afin de modifier cette disposition et faire apparaître de nouvelles informations.
2 Parallélisation d'une méthode radiale : entre CPU et GPU Si l'on note d 1 , ..., d n les n données multidimensionnelles décrites selon m attributs, et P OI 1 , ..., P OI k les k points d'intérêt choisis, une session utilisateur se déroule avec notre visualisation selon 5 étapes décrites dans la première colonne de la table 1. La parallélisation des différentes étapes de cet algorithme est résumée dans ce même tableau 1. La lecture des données (étape E1), effectuée une seule fois, est traitée par le CPU avec plusieurs tâches de lecture en parallèle.
Le choix des POIs (étape E2) est effectué de manière automatique pour la première visualisation, ensuite c'est l'utilisateur qui ajuste les points d'intérêt grâce aux interactions. La méthode automatique est appelée une seule fois, et son exécution reste peu coûteuse car elle porte sur un échantillon de données de taille fixe. Cet algorithme est donc exécuté sur le CPU, sans parallélisation particulière.
Le calcul des similarités entre les données et les POIs (étape E3) est l'opération la plus coû-teuse puisque l'on doit calculer ?(n * k) distances en dimension m. Cette multitude de calculs numériques se prête bien à une parallélisation sur le GPU. La tâche générique traitée par un coeur du GPU porte sur plusieurs données, définies comme un sous-ensemble de {d 1 , ..., d n }, dont on va calculer la similarité avec les POIs ainsi que les coordonnées d'affichage. Chaque paquet est traité par une tâche concurrente. En CUDA, les tâches sont regroupées en plusieurs blocs qui forment une grille. Le résultat du traitement d'un bloc est stocké temporairement dans la mémoire du GPU. Une fois tous les blocs traités, le résultat final (coordonnées d'affichage des données) est recopié dans la mémoire principale et peut ensuite être utilisé par les autres étapes de notre méthode sur le CPU. L'utilisation des différents types de mémoire est un point clé d'un programme CUDA, car ces mémoires ont des rôles et des performances très différents. Principalement, notre programme a été optimisé afin de ne lire qu'une seule fois les données, et nous avons choisi de stocker les informations nécessaires aux calculs (POIs, leurs coordonnées, etc) dans la mémoire constante du GPU qui est très rapide (mais limitée à 64ko).
Dans l'objectif d'effectuer des comparaisons, cette étape E3 a également été parallélisée sur le CPU. Pour cela nous avons utilisé le même principe global : les données sont découpées en paquet, et chaque paquet est traité par des tâches concurrentes sur le CPU (par exemple, de 1 à 12 tâches).
L'affichage des données (étape E4) a un coût linéaire en n, et dans notre implémentation actuelle, nous avons réalisé cette étape de manière concurrente sur le CPU. En ce qui concerne les interactions, un pré-traitement a souvent lieu avant de renvoyer l'exécution vers une des étapes précédentes de l'algorithme (E3 ou E4). Le seul pré-traitement ayant un coût de calcul est lié à la sélection, puisqu'il faut détecter quelles données appartiennent au cadre dessiné par l'utilisateur. Ce calcul est principalement proportionnel au nombre de pixels sélectionnés, et reste dans un temps acceptable sur le CPU.
Résultats
Bases, matériel, paramétrage
Les bases utilisées pour nos tests sont représentées dans la table 2. Nous avons conçu un générateur de données qui génère n données en dimensions m selon 5 classes de même effectif. De cette manière, nous avons pu tester l'évolution de la complexité des opérations en fonction de n, m et k. Le matériel utilisé dans ces tests est courant : un portable Asus avec un i7 cadencé à 2,2GHz, un GPU Nvidia GTX560M (192 coeurs). Les résultats présentés dans la suite ont été calculés en moyenne sur 10 essais. Les temps sont exprimés en ms. Pour les étapes utilisant le CPU (E1 et E4, mais aussi une version CPU de l'étape E3 utilisée dans nos comparaisons CPU-GPU), nous avons déterminé le meilleur nombre de tâches concurrentes à utiliser en utlisant de nombreux jeux de données (y compris ceux de la table 2) et en faisant croître le nombre de tâches concurrentes de 1 à 12. Nous avons mesuré le temps mis par le CPU pour accomplir les travaux demandés. Par manque de place, nous ne donnons que les conclusions obtenues : à l'issue de ces tests, nous avons conclu que les meilleurs paramètres pour les étapes ayant lieu sur CPU sont 4 tâches pour E1, 8 pour E3 et 4 pour E4 (paramètres utilisés dans le reste de l'article).
Pour le GPU, le paramétrage consiste à déterminer deux valeurs : le nombre de blocs B et le nombre de tâches par bloc T . L'efficacité de ce découpage dépend du matériel. Nous avons sélectionné plusieurs bases de données et nous avons fait varier ces deux paramètres. Dans la suite, nous avons sélectionné les valeurs B = 256 et T = 128 pour les résultats de l'approche GPU.
Performances de la méthode et apports du parallélisme
Nous avons mesuré les temps mis par notre méthode et ses variantes pour traiter, étape par étape, différents jeux de données (voir tableau 3). Les deux premières colonnes du tableau montrent que le temps de lecture des fichiers au format CSV est globalement divisé par 2 grâce à la parallélisation. La colonne E2-CPU1 donne les temps nécessaires pour le choix initial des POIs. Ici il s'agit d'un algorithme heuristique qui échantillonne des points au hasard et choisit le groupe de points qui maximise une mesure donnée. Le nombre d'itérations de cet algorithme est suffisant, d'après nos expériences précédentes, pour obtenir un résultat satisfaisant. Cette étape E2 s'exécute donc en un temps négligeable par rapport aux autres étapes. C'est la raison pour laquelle nous n'avons pas réalisé sa parallélisation (qui serait possible cependant).
Les colonnes E3-CPU1, E3-CPU8 et E3-GPU permettent de comparer entre elles des approches utilisant un parallélisme d'intensité croissante pour le calcul des similarités. Les résul- Les deux dernières colonnes (E4-CPU1 et E4-CPU4) permettent de quantifier l'efficacité de la parallélisation de l'affichage sur CPU. La différence entre les colonnes permet de gagner quelques secondes sur les plus grosses bases, cependant, elle n'est pas aussi importante que prévue et doit pouvoir être améliorée.
Conclusions
Nous avons présenté dans cet article une parallélisation d'une méthode visuelle radiale afin de rendre possible le traitement de millions de données, notamment du point de vue de la vitesse d'affichage et du temps nécessaire pour les interactions. Cette parallélisation s'est appuyée à la fois sur le CPU et le GPU, en fonction des complexités a priori des opérations traitées.
Le principal résultat de l'article est de montrer que cette approche permet des affichages et des interactions dont les temps vont, pour des millions de données, de 0,2 à 10 secondes, tout en utilisant du matériel classique. Ce résultat montre que les approches radiales repré-sentent une catégorie de méthodes se prêtant bien au passage à l'échelle à condition d'utiliser des algorithmes parallèles. Nos résultats permettent aussi d'augmenter de manière significa-

