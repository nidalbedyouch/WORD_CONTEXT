[
  {
    "id": "801",
    "text": "Introduction\nLes algorithmes de Séparateurs à Vaste Marge proposés par (Vapnik, 1995) et les méthodes de noyaux permettent de construire des modèles précis et deviennent des outils de classification de données de plus en plus populaires. On peut trouver de nombreuses applications des SVM (réf. http://www.clopinet.com/isabelle/Projects/SVM/applist.html) comme la reconnaissance de visages, la catégorisation de textes ou la bioinformatique. Cependant, les SVM demandent la résolution d\u0027un programme quadratique dont le coût de calcul est au moins d\u0027une complexité égale au carré du nombre d\u0027individus de l\u0027ensemble d\u0027apprentissage et la quantité de mémoire nécessaire les rend impossible à utiliser sur de grands ensembles de données à l\u0027heure actuelle (Lyman et al., 2003). Il y a besoin de permettre le passage à l\u0027échelle des SVM pour traiter de grands ensembles de données sur des machines standard. Une heuristique possible pour améliorer l\u0027apprentissage des SVM est de décomposer le programme quadratique en une série de plus petits problèmes (Boser et al, 1992), (Chang et al, 2003), (Osuna et al, 1997), (Platt, 1999). Au niveau de la mise en oeuvre, les méthodes d\u0027apprentissage incrémental (Cauwenberghs et al, 2001), (Do et Poulet, 2006), (Do et Poulet, 2003), (Fung et Mangasarian, 2002), , (Syed et al, 1999) permettent de traiter de grands ensembles de données par mise à jour des solutions partielles en chargeant consécutivement les sous-ensembles d\u0027apprentissage en mémoire sans avoir à charger l\u0027ensemble total. Les algorithmes parallèles et distribués (Do et Poulet, 2006), ) utilisent des machines en réseaux pour améliorer le temps d\u0027exécution de l\u0027apprentissage. Les algorithmes d\u0027apprentissage actif (Do et Poulet, 2005), (Tong et Koller, 2000) choisissent un sous-ensemble d\u0027individus (ensemble actif) représentatif pour la construction du modèle.\nDans cet article, nous continuons à développer nos algorithmes de boosting de PSVM et LS-SVM   (Do et Poulet, 2007) et (Do \u0026 Fekete, 2007) pour classifier simultanément un grand nombre d\u0027individus et de dimensions. Nous présentons une classe d\u0027algorithmes de boosting se basant sur les NSVM (Mangasarian, 2001), PSVM (Fung et Mangasarian, 2001) et LS-SVM (Suykens et Vandewalle, 1999)   (Golub et Van Loan, 1996) pour traiter des ensembles de données ayant un très nombre de dimensions. Enfin nous avons construit les algorithmes de boosting, Adaboost (Freund et Schapire, 1995), Arc-x4 (Breiman, 1998) de NSVM, PSVM et LS-SVM pour la classification d\u0027ensembles de données ayant simultanément un grand nombre d\u0027individus et de dimensions. Les performances des algorithmes sont évaluées sur de grands ensembles de l\u0027UCI (Blake et Merz, 1998), comme Adult, KDDCup 1999, Forest Covertype, Reuters-21578 et de RCV1-binary. Les résultats sont comparés avec ceux obtenus avec LibSVM (Chang et Lin, 2003), Perf-SVM (Joachims, 2006) et CB-SVM (Yu et al., 2003).\nLe paragraphe 2 présente brièvement les algorithmes de NSVM, PSVM et LS-SVM, le paragraphe 3 décrit les extensions pour la classification d\u0027ensembles de données ayant un grand nombre de dimensions. Dans le paragraphe 4 nous présentons les algorithmes de boosting pour le traitement de très grands ensembles de données (simultanément en nombre d\u0027individus et de dimensions) puis quelques résultats dans le paragraphe 5 avant la conclusion et les travaux futurs.\nQuelques notations sont utilisées dans cet article. Tous les vecteurs sont représentés par des matrices colonne. Le produit scalaire de deux vecteurs x et y est noté x.y. La norme d\u0027un vecteur v est ||v||. La matrice inverse de X est notée par X -1 . e est un vecteur colonne de 1. I représente la matrice identité. (1) D(Aw -eb) + z ? e z ? 0 où une constante c \u003e 0 est utilisée pour contrôler la marge et les erreurs.\nAlgorithmes de NSVM, PSVM et LS-SVM\nLe plan optimal (w, b) est obtenu par la résolution du programme quadratique (1) dont la mise en oeuvre est coûteuse en temps et mémoire vive.\nAlgorithme de NSVM\nL\u0027algorithme de generalized SVM (GSVM) (Mangasarian, 1998)  T et E \u003d (A -e), on peut réécrire le problème (2) en : (Mangasarian, 2001) a proposé d\u0027utiliser la méthode itérative de Newton pour résoudre efficacement le problème d\u0027optimisation (3). Le principe de la méthode de Newton est de minimiser successivement les approximations au second ordre de la fonction objectif ? en se basant sur un développement de Taylor au second ordre au voisinage de X v . Le problème \nAlgorithme de PSVM\nL\u0027algorithme de proximal SVM (PSVM) (Fung et Mangasarian, 2001) modifie aussi l\u0027algorithme de SVM (1) en :\n-maximisant la marge par (1/2) ||w, b|| 2 -minimisant les erreurs par (c/2) ||z|| 2 sous la contrainte :\nEn substituant z dans la fonction objectif ? du programme quadratique (1), nous obtenons alors (7) :\nPour ce problème d\u0027optimisation (7), nous calculons les dérivées partielles en w et b. Cela nous donne le système linéaire de (n+1) inconnues (w 1 , w 2 , …, w n , b) suivant :\nL\u0027algorithme de PSVM ne demande que la résolution d\u0027un système linéaire (8) à n+1 inconnues au lieu du programme quadratique (1), donc il est capable de traiter un très grand nombre d\u0027individus en un temps restreint sur une machine standard. Par exemple, la classification d\u0027un million de points en dimension 20 est effectuée en 13 secondes sur un PC (P4, 2,4GHz, 512Mo RAM).\nAlgorithme de LS-SVM\nL\u0027algorithme de LS-SVM proposé par (Suykens et Vandewalle, 1999)  En substituant z dans la fonction objectif ? du programme quadratique (1), nous obtenons alors (9) :\nPour ce problème d\u0027optimisation (9), nous calculons également les dérivées partielles en w et b. Cela nous donne le système linéaire de (n+1) inconnues (w 1 , w 2 , …, w n , b) suivant :\nL\u0027algorithme de LS-SVM a la même complexité que celle du PSVM, il ne demande que la résolution d\u0027un système linéaire (10) à n+1 inconnues au lieu du programme quadratique (1), donc il est capable de traiter un très grand nombre d\u0027individus en un temps restreint sur une machine standard.\nExtensions pour un grand nombre de dimensions\nCertaines applications comme la bioinformatique ou la fouille de textes nécessitent de traiter des données ayant un nombre très important de dimensions et un nombre d\u0027individus plus réduit. Dans ce cas la matrice de taille (n+1)x(n+1) est trop importante et la résolution du système à (n+1) inconnues nécessite un temps de calcul élevé. Pour adapter l\u0027algorithme à ce type de données nous avons appliqué le théorème de Sherman-Morrison-Woodbury (11) aux systèmes d\u0027équations des algorithmes de NSVM, PSVM et LS-SVM.\n(\nPour l\u0027algorithme de NSVM, on note :\nl\u0027inversion de la matrice ?\u0027\u0027(X i ) dans la formule (6) est re-écrite par (12) :\nEnsuite nous appliquons la formule de Sherman-Morrison-Woodbury (11) dans la partie droite de la formule (12), nous obtenons (13) :\nL\u0027inversion de la matrice (I + P T P) de taille (n+1)x(n+1) est ramenée à inverser la matrice (I + PP T ) (de taille mxm). Cette nouvelle formulation permet à l\u0027algorithme de NSVM de traiter facilement des ensembles de données ayant un nombre de dimensions très important.  Entrée : \nRNTI -X -\nTAB. 2 -Algorithme Adaboost de NSVM, PSVM et LS-SVM\nRemarquons que l\u0027utilisation des algorithmes de NSVM, PSVM et LS-SVM dans l\u0027algorithme Adaboost est très intéressante parce que ces algorithmes traitent beaucoup plus rapidement les sous-ensembles (échantillons) de données qu\u0027avec la résolution d\u0027un programme quadratique d\u0027un SVM standard.\nRNTI -X -\nNous proposons également d\u0027utiliser l\u0027algorithme Arc-x4 (Breiman, 1998) avec les algorithmes de NSVM, PSVM et LS-SVM de manière similaire à l\u0027algorithme Adaboost. Le principe de l\u0027algorithme Arc-x4 est semblable à Adaboost. Il utilise le mécanisme de vote majoritaire au lieu d\u0027un vote avec des poids et il a également une manière différente pour prendre un échantillon en se concentrant sur les erreurs de toutes les étapes précédentes. L\u0027algorithme Arc-x4 de NSVM, PSVM et LS-SVM donne de très bonnes performances en classification de grands ensembles de données.\nQuelques résultats\nNous avons développé le programme en C/C++ sous Linux en utilisant la librairie Lapack++ (réf. http://math.nist.gov/lapack++/) pour bénéficier de bonnes performances en calcul matriciel. Nous avons aussi développé des algorithmes spécifiques pour les matrices creuses. Nous allons en présenter une évaluation en prenant en compte le taux de précision et le temps d\u0027apprentissage. Nous avons sélectionné 4 grands ensembles de données de l\u0027UCI (Blake et Merz, 1998)  Nous avons utilisé les algorithmes de boosting de NSVM (AdaNSVM, Arcx4NSVM), PSVM (AdaPSVM, Arcx4PSVM) et LS-SVM (AdaLSSVM, Arcx4LSSVM) pour effectuer la classification sur un PC (Pentium IV, 2,4GHz et 1024Mo RAM). Les résultats sont comparés avec ceux obtenus avec l\u0027algorithme SVM standard LibSVM (Chang et Lin, 2003) et deux nouvelles versions de SVM, Perf-SVM (Joachims, 2006) et CB-SVM (Yu et al., 2003). Nous mettons dans les tableaux les meilleurs résultats en caractères gras et les deuxièmes en souligné. Nous ne faisons que comparer les premiers résultats expérimentaux des algorithmes de boosting de NSVM, LS-SVM avec LibSVM parce que l\u0027algorithme de PSVM est très semblable à l\u0027algorithme de LS-SVM.\nLes ensembles de données Reuters-21578 et RCV1-binary sont utilisés pour évaluer les performances en classification d\u0027ensembles ayant simultanément un grand nombre d\u0027individus et dimensions. Nous avons utilisé Bow (réf. http://www.cs.cmu.edu-/~mccallum/bow) en prétraitement des données Reuters-21578. Chaque document est vu comme un vecteur de mots, nous avons obtenu 29406 mots (dimensions) sans sélection de dimensions. Nous avons effectué la classification des 10 classes les plus nombreuses. Cet ensemble de données ayant plus de deux classes nous avons utilisé l\u0027approche \"un contre le reste\". Les résultats sont présentés dans le tableau 4 avec la moyenne de la précision et du rappel (breakeven point) pour les 10 plus grandes catégories.\nOn remarque que nos algorithmes donnent toujours des résultats un peu meilleurs en ce qui concerne le taux de précision (jusqu\u0027à 5,32% d\u0027amélioration) au détriment, dans certains cas, de la rapidité de calcul.\nPrécision (  Reuters-21578 Pour l\u0027ensemble de données RCV-1, nous avons utilisé le même prétraitement que (Chang et Lin, 2003), CCAT et ECAT formant la classe positive et GCAT et MCAT la négative (en enlevant les instances apparaissant simultanément dans les classes positive et négative). Les résultats montrent que nos algorithmes permettent une petite amélioration de la précision tout en étant très significativement plus rapide (jusqu\u0027à 19 fois plus rapide). Ce facteur est encore plus significatif sur l\u0027ensemble de données Adult (200 fois plus rapide pour une précision équivalente).\nPour l\u0027ensemble de données Forest Cover Type, nos algorithmes ont effectué la classification des deux classes les plus nombreuses en moins de 30 secondes, LibSVM n\u0027a donné aucun résultat après 21 jours de calcul. Cependant des travaux récents ont montré que l\u0027algorithme SVM-Perf (Joachims, 2006) effectue la classification de cet ensemble de données en 171 secondes sur un Intel Xeon, 3,6GHz, 2Go RAM. En tenant compte des différences de rapidité des processeurs, nous pouvons estimer raisonnablement que nos algorithmes sont 8 fois plus rapides.\nRNTI -X -\nEnfin pour l\u0027ensemble de données KDD-Cup\u002799, CB-SVM (Yu et al, 2003) a effectué la classification en 4750 secondes sur un P-III, 800MHz, 1Go RAM, avec 90% de précision. Nos algorithmes obtiennent 92% de précision en seulement 180 secondes soit à peu près 10 fois plus rapidement que CB-SVM.  Une première extension de ces travaux va consister à étendre ces algorithmes pour en faire des versions parallèles et distribuées sur un ensemble de machines. Cette extension permettra d\u0027améliorer le temps de la tâche d\u0027apprentissage. Une seconde sera de proposer une nouvelle approche pour la classification non linéaire.\n"
  },
  {
    "id": "802",
    "text": "Introduction\nLes avancées technologiques en matière de haut débit favorisent l\u0027apparition de nouveaux services de vente ou location en ligne de fichiers vidéos et musicaux. De tels services se veulent pro-actifs et proposent, en plus des actes promotionnels classiques, des choix personnalisés de films (ou de musique). Des méthodes de recommandation sont déjà utilisées sur certains sites Internet de vente par correspondance (Amazon, Fnac, Virgin, etc.) ou encore sur les platesformes musicales (Lastfm, Radioblog, Pandora, etc.). Candillier et al. (2007) fait un panorama des techniques de recommandation : qu\u0027elles soient basées sur des notations d\u0027internautes ou des descriptions de contenus (techniques user-and item-based utilisant le filtrage collaboratif) ou des rapprochements thématiques de profils d\u0027internautes et de descriptions de contenus (filtrage de contenus), voire des techniques hybrides combinant les différentes approches, la problématique reste de gérer les matrices creuses. En effet, devant la variété d\u0027un catalogue et le grand nombre d\u0027utilisateurs, le faible nombre de notes qu\u0027un utilisateur donne rend la comparaison avec d\u0027autres utilisateurs risquée. Parmi les verrous bien connus du domaine de la recommandation figure la difficulté à recueillir des descriptions aussi bien des goûts des utilisateurs (notes, intérêts, usage) que des contenus (métadonnées). Ce problème est pourtant crucial au lancement d\u0027un service (bootstrapping), et se repose pour tout nouvel utilisateur, car c\u0027est un challenge d\u0027attirer et fidéliser les clients dès la prise en main du service.\nAfin de pallier ces problèmes, une nouvelle voie de recherche est ouverte : l\u0027exploitation de la richesse de l\u0027Internet ouvert au service d\u0027un site web fermé. Internet est devenu plus que jamais une source de connaissances. Avec le web 2.0 et l\u0027apparition de sites communautaires, les internautes partagent de plus en plus leurs photos, leurs signets, leurs nouvelles, leurs arnaques, leurs opinions. Considérer le web comme un catalogue permet de connaître les goûts d\u0027un nombre considérable d\u0027individus, et nous permet d\u0027envisager à plus ou moins long terme, de décrire des profils type de fans, une typologie des films, ou encore de découvrir de nouveaux descripteurs de films décisifs dans le choix de films. L\u0027objectif de l\u0027étude exploratoire décrite dans ce papier est de comprendre ce que l\u0027on peut dégager des commentaires de films publiés dans des sites communautaires 1 . Nous chercherons à identifier quel est le vocabulaire révélateur d\u0027opinions, mais également découvrir si un vocabulaire particulier permet de regrouper des films. Nous montrerons que certains mots sont caractéristiques d\u0027un groupe de films précis, tandis que d\u0027autres mots sont beaucoup mieux répartis dans les commentaires de l\u0027ensemble du corpus textuels.\nTravaux connexes\nDe nombreux systèmes de mesure d\u0027opinions dans les contenus textuels ont vu le jour ré-cemment. Deux grandes familles de méthodes existent, les méthodes basées sur des techniques de traitement automatique de la langue naturelle (TALN) et celles basées sur l\u0027apprentissage, ces deux types de méthodes pouvant également être combinées. Liu et al. (2005) décrivent le système Opinion Observer qui analyse finement les commentaires d\u0027usagers dans le but de produire automatiquement des comparatifs de produits commerciaux. Ils ont conçu une méthode de découverte de motifs linguistiques qui permet de trouver le vocabulaire décrivant des critères (comme la qualité des images pour un appareil photo) puis d\u0027en calculer l\u0027orientation. En comptabilisant les scores positifs et négatifs de chaque critère, pour chaque produit, le système produit un rapport détaillé comparant l\u0027opinion générale qui se dégage sur ces produits et facilite ainsi l\u0027achat d\u0027un appareil photo parmi les modèles décrits par les usagers eux-mêmes. Opinion Observer est un exemple de système complet basé sur l\u0027identification de mots \"porteurs d\u0027opinion\" dans les phrases, suivie par un décompte de ces mots. Dave et al. (2003) présentent une méthode plus simple, sans détection de motifs, mais qui, à partir d\u0027un dictionnaire, et selon une échelle d\u0027intensité des mots d\u0027opinions, attribue un score positif ou négatif à chacune des phrases. Grâce aux scores calculés, le système classifie automatiquement des commentaires textuels en 2 classes : positive et négative.\nMéthodes linguistiques d\u0027analyse d\u0027opinions\nComme beaucoup d\u0027autres systèmes (Morinaga et al., 2002;Turney, 2002;Wilson et al., 2004;Nasukawa et Yi, 2003), ils ont besoin de mots décrivant des opinions. Ce lexique peut être totalement construit à la main ; cependant devant le coût d\u0027un tel procédé, beaucoup de systèmes décrivent des méthodes d\u0027enrichissement plus ou moins automatiques d\u0027une mouture minimale créée manuellement. La constitution de lexique peut se faire grâce à des techniques d\u0027apprentissage. Par exemple, Hatzivassiloglou et McKeown (1997)  ou Turney et Littman (2004) utilisent un algorithme non supervisé pour associer de nouveaux mots à des mots pré-sélectionnés. Pereira et al. (1994) et Lin (1998) décrivent des méthodes permettant de découvrir des synonymes en analysant la collocation de mots. Des méthodes linguistiques exploitent l\u0027analyse syntaxique et grammaticale de corpus afin d\u0027étendre le lexique. Citons les travaux de Hatzivassiloglou et McKeown (1997) qui utilisent les conjonctions and et or pour déduire l\u0027orientation sémantique de vocabulaire ainsi associé à des mots déjà connus. Turney (2002) utilise des motifs un peu plus complexes tels que adverbe + adjectif non suivis par un nom. Benamara et al. (2007) se basent sur une classification d\u0027adverbes et attribue des scores à des adjectifs selon la catégorie de l\u0027adverbe auquel ils sont associés.\nCitons enfin les travaux de Google (Godbole et al., 2007) ou Hu et Liu (2004) qui utilisent le dictionnaire bien connu WordNet (Miller et al., 1993).\nMéthodes d\u0027apprentissage analysant les opinions\nLes systèmes utilisant des méthodes d\u0027apprentissage classifient des commentaires textuels en 2 classes (positive et négative), mais parfois cherchent à prédire des notes de 0 à 5. Ces méthodes de classification supervisées considèrent qu\u0027un commentaire décrit un seul film et cherchent à prédire une note donnée par l\u0027auteur du commentaire.\nBeaucoup de méthodes utilisent une préparation linguistique du corpus. Wilson et Wiebe (2003) exposent comment étiqueter les mots porteurs d\u0027opinions par une intensité, après quoi Wilson et al. (2004) testent 3 méthodes d\u0027apprentissage différentes (fréquemment utilisées par les linguistes) : BoosTexter (Shapire et Singer, 2000), Ripper (Cohen, 1996) et SVMlight (la version légère de Support Vector Machine de Joachims (1998)). Cette dernière obtient les meilleurs résultats sur leur corpus annoté. De la même manière, pour caractériser ce qui est ou non apprécié dans chaque phrase, Nigam et Hurst (2004) combinent une technique de parsing avec un classifieur bayésien pour associer la polarité à des thématiques.\nPourtant Pang et al. (2002) et Dave et al. (2003) montrent que la préparation des corpus par des lemmatiseurs par exemple, ou encore par la prise en compte des négations, s\u0027avère inutile. Afin de prédire l\u0027opinion de commentaires sur des films, ces deux papiers explorent quelques méthodes d\u0027apprentissage et démontrent qu\u0027elles sont plus performantes que les méthodes de parsing suivies d\u0027un décompte comme présentées ci-dessus, avec des résultats de l\u0027ordre de 83% de bonnes prédictions. Les commentaires sont vus comme des sacs de mots. Pang et al. (2002) utilisent un classifieur bayésien naïf et un classifieur maximisant l\u0027entropie.\nDans notre étude exploratoire, nous partons du principe que nous n\u0027avons aucun a priori sur les données. Volontairement, nous n\u0027avons procédé à aucun pré-traitement, et outre le fait que la technique peut de ce fait être utilisée sur toutes les langues, nous prenons l\u0027hypothèse que le vocabulaire dédié aux opinions n\u0027est pas le seul déterminant et utile pour faire des recommandations de films.\nTechnique utilisée\nOn présente dans cette section une extension au cas non supervisé des modèles en grilles introduits dans le cadre de l\u0027évaluation bivariée pour la classification supervisée (Boullé, 2007c,a).\nAprès avoir formalisé l\u0027évaluation d\u0027une grille dans le cas de deux variables catégorielles à expliquer, on montre que ce type de grille peut s\u0027interpréter comme un modèle non paramétrique de corrélation entre les valeurs de chaque variable. On décrit ensuite les algorithmes permettant d\u0027optimiser ce type de modèles. On montre enfin qu\u0027il peuvent s\u0027appliquer à l\u0027analyse exploratoire en utilisant un modèle de co-clustering des individus et des variables.\nGroupement de valeurs bivarié non supervisé\nOn cherche à décrire conjointement les valeurs des deux variables catégorielles à expliquer Y 1 et Y 2 , comme illustré sur la figure 1.\nFIG. 1 -Exemple de densité jointe pour deux variables catégorielles Y 1 ayant 4 valeurs a, b, c, d et Y 2 ayant 4 valeurs A, B, C, D. Le tableau de contingence sur la gauche ne contient des individus que sur la moitié des cases (marquées •), les autres cases étant vides. Suite au groupement de valeurs bivarié, le tableau de contingence sur la droite permet une description synthétique de la corrélation entre\nOn introduit en définition 1 une famille de modèles où chaque variable à expliquer est partitionnée en groupes de valeurs. On distribue les individus sur l\u0027ensemble des cellules de la grille bidimensionnelle résultant du produit cartésien des partitions univariées ainsi définies. Cette distribution étant spécifiée, on en déduit par sommation sur les cellules la distribution des individus sur les groupes de valeurs pour chaque variable à expliquer. Il ne reste qu\u0027à spécifier localement à chaque groupe la distribution des individus sur les valeurs du groupe pour obtenir une description complète de la distribution des individus sur les valeurs des deux variables conjointement. Définition 1. Un modèle de groupement de valeurs bivarié non supervisé est défini par : -un nombre de groupes pour chaque variable à expliquer, -la partition de chaque variable à expliquer en groupes de valeurs, -la distribution des individus sur les cellules de la grille de données ainsi définie, -la distribution des individus de chaque groupe sur les valeurs du groupe, pour chaque variable à expliquer.\nNotations 1.\n-N : nombre d\u0027individus de l\u0027échantillon -V 1 , V 2 : nombre de valeurs pour chaque variable (connu) -J 1 , J 2 : nombre de groupes pour chaque variable (inconnu)\nj1 , m j2 : nombre de valeurs du groupe j 1 (resp. j 2 ) -n\nj1 , N j2 : nombre d\u0027individus du groupe j 1 (resp. j 2 ) -N j1j2 : nombre d\u0027individus de la cellule (j 1 , j 2 ) de la grille Un modèle de groupement de valeurs bivarié non supervisé est entièrement caractérisé par le choix des paramètres de partition des valeurs en groupes\ndes paramètres de distribution des individus sur les cellules de la grille\net des paramètres de distribution des individus des groupes sur les valeurs des variables\nLes nombres de valeurs par groupe sont déduits du choix des partitions des valeurs en groupes, et les effectifs des groupes par comptage des effectifs des cellules de la grille.\nAfin de rechercher le meilleur modèle, on applique une approche Bayesienne visant à maximiser la probabilité P (M |D) \u003d P (M )P (D|M )/P (D) du modèle sachant les données. À cet effet, on introduit en définition 2 une distribution a priori sur les paramètres des modèles. Définition 2. On appelle a priori hiérarchique l\u0027a priori de modèle de densité par grille basé sur les hypothèses suivantes : -les nombres de groupes de valeurs J 1 (resp. J 2 ) des variables à expliquer sont indépen-dants entre eux, et compris entre 1 et V 1 (resp. V 2 ) de façon équiprobable, -pour un nombre de groupes donné J 1 de Y 1 , toutes les partitions des V 1 valeurs en J 1 groupes sont équiprobables, -pour un nombre de groupes donné J 2 de Y 2 , toutes les partitions des V 2 valeurs en J 2 groupes sont équiprobables, -pour une grille de taille donnée (J 1 , J 2 ), toutes les distributions multinômiales des N individus sur les G cellules de la grille sont équiprobables, -pour un groupe donné d\u0027une variable à expliquer donnée, toutes les distributions multinômiales des individus sur les valeurs du groupe sont équiprobables.\nCette distribution a priori sur les paramètres des modèles est hiérarchique, uniforme à chaque étage de la hiérarchie. Pour les distributions multinômiales, les cases vides sont considérées dans la distribution a priori. En utilisant la définition formelle des modèles et leur distribution a priori hiérarchique, la formule de Bayes permet de calculer de manière exacte la probabilité d\u0027un modèle connaissant les données, ce qui conduit au théorème 1. \nB(V, J) est le nombre de répartitions de V valeurs explicatives en J groupes (éventuel-lement vides). Pour J \u003d V , B(V, J) correspond au nombre de Bell. Dans le cas général, B(V, J) peut s\u0027écrire comme une somme de nombre de Stirling de deuxième espèce.\nLa première ligne de la formule (1) regroupe des termes d\u0027a priori correspondant au choix des nombres de groupes J 1 et J 2 et à la spécification de la partition de chaque variable à expliquer en groupes de valeurs. La deuxième ligne représente la spécification de la distribution multinômiale des N individus de l\u0027échantillon sur les G cellules de la grille, suivi de la spécification de la distribution des individus de chaque groupe sur les valeurs du groupe. La troisième ligne représente la vraisemblance de la distribution des individus dans les cellules de la grille, au moyen d\u0027un terme du multinôme. La dernière ligne correspond à la vraisemblance des valeurs localement à chaque groupe pour chacune des variables à expliquer.\nInterprétation\nDans le cas d\u0027une grille comportant une seule cellule, la formule (1) se réduit à :\n(2)\nce qui correspond à la probabilité a posteriori d\u0027un modèle multinômial, pour chacune des variables catégorielles Y 1 et Y 2 à expliquer. On peut alors interpréter le modèle de groupement de valeurs bivarié non supervisé de la définition 1 comme un modèle de description de la corrélation entre les deux variables à expliquer. En cas d\u0027indépendance entre les variables, la description des deux variables conjointement se réduit à la somme des descriptions de chaque variable individuellement. Le modèle en grille permet de capturer de façon non paramétrique des corrélations entre les valeurs des variables à expliquer. Le surcoût de description du modèle de corrélation en grille est alors compensé par une description plus concise des valeurs de chaque variable connaissant le modèle de corrélation. Le meilleur compromis est recherché suivant une approche Bayesienne de la sélection de modèles.\nExemple de deux variables catégorielles à expliquer corrélées. Prenons l\u0027exemple de deux variables catégorielles identiques, et d\u0027un modèle en grille M comportant autant de groupes que de valeurs (J 1 \u003d V 1 ), comme illustré sur la figure 2. Le coût de description de la grille provenant de la formule (1) est alors égal à :\nFIG. 2 -Grille de groupement de valeurs bivarié avec autant de groupes que de valeurs pour deux variables à expliquer identiques\nEn comparant les formules (2) dans le cas d\u0027indépendance et (3) dans le cas d\u0027égalité des variables à expliquer, on observe un surcoût de modélisation dans les termes d\u0027a priori (spécification de chaque groupement de valeurs et spécification de la distribution des individus sur la grille bidimensionnelle). En revanche, le coût de vraisemblance est divisé par deux : les corrélations étant capturées dans le modèle en grille, la description des deux variables à expliquer se réduit à la description d\u0027une seule variable.\nAlgorithme d\u0027optimisation\nNous proposons une heuristique gloutonne ascendante d\u0027optimisation, qui, partant d\u0027une solution initiale de partitionnement bivarié aléatoire, procède par fusion itérative des groupes de valeurs tant qu\u0027il y a amélioration du critère. Cet algorithme est précédé d\u0027une étape de pré-optimisation qui consiste à déplacer les valeurs entre les groupes de façon à améliorer le critère. Afin d\u0027améliorer la solution obtenue, une post-optimisation, basée sur le même algorithme de déplacement des valeurs, est également appliquée.\nL\u0027heuristique gloutonne a une complexité algorithmique de O(N 5 ) avec une implémenta-tion naïve. En effet, pour V ? N , il y a O(N ) étapes de fusions de groupes effectuée lors de l\u0027heuristique gloutonne, et chaque étape repose sur l\u0027évaluation de O(N 2 ) fusions de groupes potentielles impliquant des grilles de O(N 2 ) cellules. On montre dans (Boullé, 2007b) que cet algorithme peut être implémenté avec une complexité algorithmique de O(N ? N log N ) en partant d\u0027une solution initiale aléatoire de taille O( ? N ). Pour atteindre cette complexité, on exploite l\u0027additivité du critère d\u0027évaluation des grilles bivariées, qui se décompose sur les caractéristiques de la grille, des variables, des groupes de valeurs et des cellules. On utilise également la nature intrinsèquement creuse des grilles qui comportent au plus N cellules non vides (une par individu) pour une taille de O(N 2 ) cellules potentielles. Comme cette heuristique gloutonne est efficace en temps de calcul, nous l\u0027avons incorporée au sein de la méta-heuristique Variable Neighborhood Search (VNS) (Hansen et Mladenovic, 2001), qui consiste essentiellement à appeler l\u0027heuristique principale en partant de solutions aléatoires générées dans le voisinage de la meilleure solution. On obtient ainsi un algorithme de type \"anytime\", qui permet d\u0027améliorer la solution en fonction du temps de calcul disponible.\nCo-clustering des individus et variables\nUn co-clustering (Hartigan, 1972) est défini comme le regroupement simultané des lignes et des colonnes d\u0027une matrice. Dans le cas des jeux de données de faible densité, ayant de nombreux 0 dans le tableau croisé individus x variables, le co-clustering est une technique attractive pour identifier des corrélations entre groupes d\u0027individus et groupes de variables (Bock, 1979;Govaert et Nadif, 2006;Dhillon et al., 2003;Lechevallier et Verde, 2004).\nConsidérons un jeu de données binaire de faible densité avec N individus, K variables et V valeurs non nulles. Un tel jeu de données peut être représenté sous la forme d\u0027un tableau à V lignes et deux colonnes. Cela correspond a un nouveau tableau de données avec deux variables nommées \"ID Individu\" et \"ID Variable\" où chaque individu est un couple de valeurs (ID Individu, ID Variable), comme illustré sur la figure 3.\nFIG. 3 -Jeu de données binaire de faible densité : depuis la matrice creuse (individus x variables) au tableau bivarié dense.\nEn appliquant notre methode de groupement de valeurs bivarié non supervisée à ce tableau bivarié dense, on obtient une grille bivariée basée sur le groupement des individus d\u0027une part, le groupement des variables d\u0027autre part. Le critère d\u0027évaluation du groupement bivarié conduit à maximiser la corrélation entre groupes d\u0027individus et groupes de variables, ce qui correspond à l\u0027objectif général du co-clustering. Il est à noter que les co-clusters sont ici les cellules de la grille, et qu\u0027ils forment une partition sans recouvrement du jeu de données.\nRésulats sur l\u0027exemple d\u0027analyse d\u0027opinions\nNous avons effectué les premières analyses sur 50 000 commentaires portant indifférem-ment sur 7 114 films. Le vocabulaire est composé de 27 673 mots différents. En formatant les données sur le modèle de la figure 3, on obtient alors plus de 700 000 individus (film, mot).\nLes commentaires de films ont subi un pré-traitement minimal, sans lemmatisation, stematisation ni filtrage sur les mots. Les seuls traitements effectués ont été de mettre tous les caractères en minuscule et la ponctuation a été supprimée. Les commentaires traités sont donc tous de la même forme que les exemples suivants :\n-it was really good juss somethin u wouldnt expect from her \nClustering de mots\nNous pouvons observer dans les résultats que les mots sont, en général, classés entre termes de même nature. On retrouve par exemple des groupes de mots :\n- -etc. D\u0027autres caractéristiques ressortent de certains groupes. On retrouve par exemple les mots n\u0027appartenant pas à la langue anglaise classés dans des ensembles communs. On retrouve ainsi des groupes de mots français d\u0027une part et des groupes de mots espagnols d\u0027autre part. On retrouve aussi des groupes de mots ne contenant qu\u0027un seul terme comme \"it\", \"a\", \"of\", \"to\", \"movie\", etc., termes qui n\u0027apportent que peu de sens, voire pas du tout, et qui peuvent être employés quelque soit le contexte dans le langage courant. C\u0027est pourquoi l\u0027outil ne peut les rapprocher d\u0027aucun autre mot.\nClustering de films\nParmi les résultats du clustering de films, on trouve des groupes séléctionnés selon diffé-rentes caractéristiques.\nOn \nLiens entre clusters de mots et clusters de films\nNous pouvons observer au moins trois tendances lorsque que l\u0027on analyse les fréquences de chaque mot d\u0027un même ensemble dans tous les clusters de films. Certain mots, comme ceux référents à un acteur ou personnage, ne sont présents que dans un très faible nombre de clusters de films (exemple avec l\u0027ensemble de mots contenant \"sean, connery, james, bond, etc.\", voir figure 4). D\u0027autres sont présents dans un plus grand nombre, comme les termes décrivant un type de films : comédie, action, policier, etc. (exemple avec l\u0027ensemble de mots contenant \"funny, hilarious, comedy, etc.\", voir figure 5). Et enfin d\u0027autres groupes de mots contiennent des termes présents quasi uniformément dans une grande majorité des clusters de films, c\u0027est le cas pour les ensembles contenant des mots de liaison (in, film, with, from, he, his, etc.).\nConclusion, perspectives\nL\u0027avantage de notre technique de coclustering est qu\u0027elle est fine et fiable, entièrement automatique en ne nécessitant ni paramètre utilisateur, ni connaissance a priori sur le domaine, interprétable et efficace en temps de calcul.\nCes premières analyses de co-clustering de films et de mots nous montrent que les textes produits par les internautes permettent de catégoriser les films selon différents critères. Cette méthode permet notamment de répertorier la plupart des genres existants dans le domaine du cinéma et d\u0027associer ces genres à un vocabulaire précis employé par les amateurs de films. En plus du classement par genres, les films sont catégorisés en fonction des acteurs présents, du réalisateur et de l\u0027appréciation des auteurs des commentaires analysés. Pour chacune de ces caractéristiques, cette méthode nous permet de connaître le vocabulaire s\u0027y rapportant, vocabulaire qui ne paraît pas toujours informatif a priori et qu\u0027il serait donc difficile à dé-terminer par des méthodes linguistiques. Ainsi notre méthode peut être complémentaire avec des analyses linguistiques, en proposant un enrichissement des dictionnaires répertoriant le vocabulaire d\u0027opinion ou de genre cinématographique avec du vocabulaire fiable.\nOutre l\u0027exploration même des données, dans un contexte de service de recommandation, la méthode permet naturellement d\u0027associer tout nouveau commentaire (un film qui vient de sor-\n"
  },
  {
    "id": "803",
    "text": "Summary\nA method of automatically alignment of definitions is proposed in order to improve the fusion between specialized medical terminologies and a general one. An SVM classifier and a compact representation are used. The model trained on the nominal group of Noun-Adjectives reaches the best performances.\n"
  },
  {
    "id": "804",
    "text": "Introduction\nAcquérir de la connaissance à partir de textes est une nécessité qui s\u0027est accrue ces vingt dernières années, avec l\u0027essor considérable de la masse de documents disponibles en format électronique, qu\u0027il faut gérer afin d\u0027extraire ou de filtrer les informations pertinentes parmi toutes celles contenues dans ces documents (Faiz, 2006). A titre d\u0027exemple; les événements boursiers sont nombreux et diversifiés. Les experts de la bourse doivent analyser ces événe-ments en un temps relativement raisonnable pour prendre des décisions importantes. Il s\u0027agit, donc, d\u0027annoter les documents présentant des événements pour pouvoir extraire ceux qui sont pertinents. C\u0027est dans ce cadre que s\u0027inscrit notre travail dont l\u0027objectif est de dévelop-per une approche qui annote automatiquement ces articles de Presse.\nLa suite du document est organisée comme suit : nous commençons, dans la section 2, par décrire les principaux systèmes d\u0027annotations existants. Au cours de la section 3, nous présentons notre approche d\u0027annotation, qui a été validée par le système AnnotEv lequel sera présenté et évalué dans la section 4. Enfin, dans la section 5, nous présentons quelques perspectives de notre travail.\nPrésentation de quelques systèmes d\u0027annotation\nPlusieurs méthodes et techniques sont utilisées par les systèmes d\u0027annotations dédiés au Web sémantique telles que l\u0027Exploration Contextuelle (Desclés, 1997), les graphes conceptuels (Roussey et al, 2002), les méta-thésaurus (Khelif et al., 2004) et les indicateurs linguistiques (Muller et al., 2004). Nous pouvons citer :\nLe système EXCOM (Djaoua et al., 2006) utilise un ensemble d\u0027outils linguistiques qui visent à annoter un document par un ensemble de connaissances aussi bien internes qu\u0027externes. Le système prend en entrée des textes et procède à l\u0027annotation sémantique et discursive de certains segments à partir des points de vue de fouille. L\u0027annotation sémantique de ces segments fait appel à la technique linguistique et informatique d\u0027exploration.\nAnnotea (Kahan et al., 2001) est un système client-serveur collaboratif pour l\u0027annotation de documents. Les annotations, externes aux documents, sont écrites en RDF. Des utilisateurs, connectés à un serveur d\u0027annotations, peuvent les ajouter, les modifier et les consulter. Cependant, nous constatons que l\u0027affichage des annotations est séparé du document, ce qui devrait être résolu pour améliorer la compréhension et l\u0027efficacité de telle annotation.\nSyDoM (Roussey et al., 2002)  Setzer et Gaizauskas (2000) ont proposé un système d\u0027annotation qui se base sur la dé-termination des événements et les relations entre eux en se basant sur des connaissances linguistiques. Cependant, nous constatons, le système ne prend pas compte de l\u0027état et des questions de type Qui et Comment pour extraire un événement, et d\u0027autre part, il se base uniquement sur les marqueurs temporels pour déterminer les relations entre les événements, hypothèse qui n\u0027est pas tout à fait validée étant donné qu\u0027il existe des relations implicites inter-événements qui sont exprimées sans utiliser des marqueurs temporels.\nApproche proposée pour l\u0027annotation des événements\nNous constatons que les approches proposées pour l\u0027annotation de l\u0027information temporelle sont principalement linguistiques et se basent sur les indices temporels. Nous nous sommes intéressés plutôt à l\u0027annotation des événements sous forme de métadonnées liées aux documents. Notre travail ne se limite pas à la détection des événements, mais permet aussi de regrouper les événements similaires pour faciliter un traitement ultérieur (indexation, remplissage des formulaires, etc.). Le processus d\u0027annotation automatique des documents que nous présentons, s\u0027effectue en quatre étapes :\n1. Prétraitement : qui consiste d\u0027une part, à détecter les frontières des phrases dans un texte et d\u0027autre part, à nommer les entités. 2. Annotation des événements : qui utilise un classificateur jouant le rôle d\u0027un filtre pour les phrases non événementielles. 3. Clustering : qui consiste à regrouper les phrases se référant au même événement.\nNous avons proposé, pour cette étape, une nouvelle mesure de similarité entre les événements.\n4. Annotation du document : qui prendra différentes formes (phrase, formulaire, concept, etc.) selon le domaine d\u0027application.\nEtape 1 : Prétraitement\nDans notre étude, le prétraitement est l\u0027application de certains outils de Traitement Automatique des Langage Naturelles (TALN) au texte brut pour le segmenter en phrases et annoter les entités nommées.\nPour la tâche de segmentation, nous avons utilisé le système SegaTex développé par Mourad (2001) pour son aspect multilingue et sa disponibilité. Concernant la nomination des entités, nous avons utilisé le système GATE (Bontcheva et al., 2004).\nEtape 2 : Annotation des événements\nUn événement est un objet spécifique qui se produit à un instant spécifique et dans un endroit bien déterminé. Notre objectif est d\u0027identifier tous les événements présents dans un document. Nous marquons par une balise chaque événement détecté. Pour cela, un modèle de classification est construit automatiquement, il permet de prédire si une phrase contient un événement ou non. Nous avons utilisé, dans un premier temps, les attributs qui se rapportent aux événements tels qu\u0027ils sont définis par Naughton et al. (2006) Il s\u0027agit donc de classer une phrase comme étant événementielle ou non. Plusieurs techniques d\u0027apprentissage automatique peuvent être utilisées tels que les réseaux de neurones, l\u0027arbre de décision, les réseaux bayésiens, etc. Nous avons choisi l\u0027arbre de décision puisque la construction de l\u0027arbre est moins paramétrable, comparé aux autres techniques, ce qui permet la réduction de la complexité du système. L\u0027ensemble de l\u0027apprentissage (training set) a été annoté par un expert. Pour chaque article de presse, les événements sont annotés comme suit : L\u0027annotateur est amené à assigner des étiquettes à chaque phrase représentant un événement ; Si une phrase se rapporte à un événement, il lui assigne l\u0027étiquette \"Oui\" sinon \"Non\".\nNous avons appliqué à ce même ensemble d\u0027apprentissage, différents algorithmes de construction des arbres de décision. Puis, nous avons choisi le modèle qui a le plus grand PCC (Pourcentage de Classification Correcte). Le résultat de cette étape est l\u0027ensemble des phrases se référant à des événements.\nEtape 3 : Le Clustering\nAu cours de cette troisième étape, nous regroupons les phrases se référant aux mêmes événements par l\u0027application de l\u0027algorithme « Hierarchical Clustering (HAC) », qui assigne initialement chaque objet à un cluster, puis fusionne, à plusieurs reprises, les clusters jusqu\u0027 à ce qu\u0027un des critères d\u0027arrêts soit satisfait (Manning et Schutze, 1999).\nDans ce cadre, nous avons proposé une nouvelle mesure de similarité sémantique entre les événements. Nous signalons qu\u0027il existe plusieurs mesures de similarités entre les documents, telle que le Cosinus de Salton (1988), le Cosinus dans l\u0027espace distributionnel et la distance de khi-deux (Lebart et Rajman, 2000). D\u0027autres mesures, qui nous intéressent le RNTI -X -plus, portent sur la similarité entre les phrases, dont la plus récente est la mesure de Naughton (2006).\nAinsi, la nouvelle mesure de similarité que nous proposons, est inspirée de tf-idf (weight term frequency-inverse document frequency) et tient compte également de la position des clusters dans le document. Afin de pouvoir regrouper des phrases exprimant le même évé-nement par deux lexiques différents, nous avons utilisé une base de synonymes permettent le remplacement des instances par leurs classes. Par exemple, soient les deux phrases événe-mentielles suivantes, initialement considérées comme deux clusters C1  Avec Ctij une classe de la base des synonymes Par la suite nous présentons FSIM par la formule suivante : Faiz, 2006).\nEtape 4 : Annotation du document\nNous continuons l\u0027enrichissement du document par d\u0027autres métadonnées qui seront très utiles pour d\u0027autres applications (RI, Résumé Automatique, Question-Réponse, Indexation).\nUne forme possible de métadonnée est le remplissage de formulaires, c\u0027est-à-dire stocker les événements dans une base de données en répondant à des questions bien déterminées, par exemple : Keyword: Killed ; Location: Baghdad ; Time/date: 2 p.m ; Person: U.S. soldier.\nUne autre forme de métadonnée est le résumé automatique, qui consiste à marquer, par des balises, les phrases qui forment le résumé d\u0027un document. En général, le but d\u0027un système de résumé automatique est de produire une représentation condensée du contenu dès son entrée, où les informations importantes du texte original sont préservées, il faut prendre en considération les besoins de l\u0027utilisateur et de la tâche spécifiée (Minel et al., 2001).\nDans le cadre de notre étude, nous proposons un résumé informatif contenant les informations essentielles contenues dans l\u0027article. Ce résumé est sélectif puisqu\u0027il néglige les aspects généraux de l\u0027article. Mais il est ciblé étant donné qu\u0027il concerne les événements.\nA partir de chaque cluster généré par la troisième étape, nous annotons l\u0027article par les principaux événements qu\u0027il contient. Nous utilisons une heuristique qui est : La phrase ayant les attributs maximaux est la meilleure pour annoter l\u0027article. \nConclusion et perspectives\nNous avons présenté une approche d\u0027annotation automatique des événements qui se base sur l\u0027apprentissage automatique, accompagnée d\u0027une exploitation de l\u0027annotation qui constitue un résumé automatique. Nous avons, également, proposé une nouvelle mesure de similarité sémantique : FSIM entre les événements.\nNotre approche se compose de quatre étapes : en commençant, dans une première étape, par le prétraitement qui consiste à appliquer des outils de TALN pour préparer les données. Dans une deuxième étape, un classifieur qui permet de filtrer les phrases événementielles. Au cours de la troisième étape, les phrases sont regroupées dans des clusters selon leur degré de similarité (FSIM). Dans la dernière étape, un résumé est généré automatiquement et portant sur les principaux événements constituant l\u0027article. Nous avons validé notre approche sur un corpus d\u0027articles de presse. Une des perspectives que nous proposons est d\u0027adopter AnnotEv à la langue arabe.\n"
  },
  {
    "id": "805",
    "text": "Introduction et contexte général\nSavoir déterminer de manière à la fois efficace et exacte l\u0027identité d\u0027un individu est devenu un problème critique dans notre société. En matière de sécurité, la biométrie ne cesse d\u0027apporter des solutions de plus en plus efficaces. Elle consiste à identifier une personne à partir de ses caractéristiques physiques ou comportementales. Le visage, les empreintes digitales, l\u0027iris, etc, sont des exemples de caractéristiques physiques. La voix, l\u0027écriture, le rythme de frappe sur un clavier, etc, sont des caractéristiques comportementales.\nDans la littérature récente, les recherches portent sur plusieurs problématiques de l\u0027identification biométrique, et surtout sur la reconnaissance de visages qui s\u0027avère une méthode, d\u0027une part, simple pour l\u0027utilisateur puisqu\u0027une brève exposition devant une caméra permet de l\u0027identifier ou de l\u0027enregistrer dans le système et d\u0027autre part, la reconnaissance de visages n\u0027est pas encore un problème résolu comme l\u0027ont montré les évaluations conduites par NIST 1 (Phillips et al., 2003).\nNIST : National Institute of Standards and Technology\nLa recherche dans le domaine de la reconnaissance de visages profite des solutions obtenues dans le domaine de l\u0027apprentissage automatique. En effet, le problème de classification de visages peut être considéré comme un problème d\u0027apprentissage supervisé où les exemples d\u0027apprentissage sont les visages étiquetés. Notre article introduit dans ce contexte une nouvelle approche hybride de classification qui utilise le paradigme d\u0027apprentissage automatique supervisé. Il est organisé en cinq sections. Dans la deuxième section, nous présentons brièvement les principales approches de reconnaissance de visages. La section 3 présente un aperçu sur la classification supervisée et les treillis de Galois. En se basant sur le fondement mathéma-tique des treillis de Galois et leur utilisation pour la classification supervisée, nous décrivons un nouvel algorithme de classification baptisé CITREC dans la section 4. Son application pour la reconnaissance de visages et la validation expérimentale sont détaillés dans la section 5.\nLa reconnaissance de visages\nLes travaux qui se sont intéressés à la reconnaissance de visages dans un environnement sous différentes conditions d\u0027éclairage, d\u0027expressions faciales et d\u0027orientations, peuvent être classés en deux catégories suivant l\u0027approche suivie. Nous allons décrire dans ce qui suit ces deux principales approches sachant qu\u0027une revue détaillée des principales techniques de reconnaissance de visages est donnée dans (Enuganti, 2005).\nL\u0027approche géométrique\nElle englobe une famille de méthodes appelées méthodes analytiques ou à caractéristiques locales. Elles sont dites analytiques puisque, en vue de reconnaître un certain visage et le classer, elle procède par son analyse. En effet, il est possible d\u0027attribuer une certaine description du visage humain, et ce, en rappelant ses parties et leurs relations. Nombreuses sont les dé-marches qui ont essayé de modéliser et de classer les visages en se basant sur des mesures de distances normalisées et d\u0027angles entre des points caractéristiques du visage humain (Kamel et al., 1993). Ce qui rend ces méthodes intéressantes est qu\u0027elles prennent en compte la spé-cificité du visage en tant que forme naturelle à reconnaître. Elles considèrent aussi un nombre réduit de paramètres (entre 9 et 14 distances (Kamel et al., 1993)).\nL\u0027approche globale\nLes méthodes dites globales traitent les propriétés globales du visage. Elles sont fondées essentiellement sur l\u0027information pixel, un point singulier dans une image. Le visage est traité comme un tout. Le système (ou classifieur) apprend ce qu\u0027est un visage à partir d\u0027un ensemble d\u0027exemples. Ce type de systèmes de reconnaissance est très efficace, reste que la phase d\u0027apprentissage s\u0027avère lourde à mettre en oeuvre (Yang et al., 2002). Parmi les approches les plus utilisées, nous pouvons citer : l\u0027approche de visages propres, l\u0027approche stochastique, l\u0027approche statistique et probabiliste et l\u0027approche connexionniste.\nClassification Supervisée et Treillis de Galois\nUne des méthodes les plus utilisées de l\u0027apprentissage automatique est celle de la classification qui consiste à répartir systématiquement de nouveaux objets selon des classes établies au préalable. Classer un objet consiste à lui faire correspondre une classe marquant sa parenté avec d\u0027autres objets. Une classe désigne un ensemble défini de données et d\u0027objets semblables. Ces données et objets, éléments d\u0027une classe, sont des instances.\nLes treillis de concepts formels (ou treillis de Galois) sont une structure mathématique permettant de représenter les classes non disjointes sous-jacentes à un ensemble d\u0027objets (exemples, instances, tuples ou observations) décrits à partir d\u0027un ensemble d\u0027attributs (propriétés, descripteurs ou items). Ces classes non disjointes sont aussi appelées concepts formels, hyperrectangles ou ensembles fermés. Une classe matérialise un concept (à savoir une idée générale que l\u0027on a d\u0027un objet). Ce concept peut être défini formellement par une extension (exemples du concept) ou par une intension (abstraction du concept) (Nguifo et Njiwoua, 2005). Étant donné que, notre approche se base sur l\u0027approche de Xie, nous présentons brièvement dans ce qui suit les systèmes CL N N et CL N B présentés dans (Xie et al., 2002).\nLes systèmes CL\nL\u0027approche de Xie (Xie et al., 2002) consiste en deux algorithmes qui se nomment CL N B (Concept Lattices Naïve Bayes) et CL N N (Concept Lattices Nearest Neighbors). Ces deux systèmes incorporent respectivement un classifieur bayésien naïf (R. Duda, 1973) et un classifieur par plus proches voisins (Dasarathy, 1991) au sein des noeuds du treillis de concepts.\nL\u0027idée directrice de cette approche est le fait d\u0027associer à chaque concept du treillis vérifiant certaines contraintes de sélection un classifieur contextuel. L\u0027apprentissage de chacun de ces classifieurs est effectué seulement sur l\u0027extension du concept en question. Ensuite, lors du classement, on vérifie si la nouvelle instance à classer correspond bien à l\u0027intension du concept alors on utilise le classifieur correspondant pour déterminer sa classe. Il est à noter que les deux systèmes (CL N B \u0026 CL N N ) reposent sur le même principe, tant en apprentissage qu\u0027au classement. L\u0027unique différence réside dans le choix du type du classifieur contextuel utilisé par chacun de ces deux classifieurs. Le treillis de concepts est utilisé pour sélectionner le sousensemble d\u0027instances qui sera utilisé par le classifieur contextuel lors de l\u0027apprentissage et du classement (Nguifo et Njiwoua, 2005).\nIl ressort des résultats en prédiction présentés par l\u0027auteur dans (Xie et al., 2002), qu\u0027en gé-néral, les méthodes CL N B et CL N N améliorent respectivement la prédiction des méthodes NB et NN prises individuellement. En outre, CL N B obtient des résultats en moyenne supérieurs à ceux des trois autres systèmes sur ces jeux de données. Il est à noter que nous distinguons trois limites non négligeables de cette approche, à savoir :\n-Approche non incrémentale, ce qui signifie qu\u0027à chaque nouvelle instance ajoutée dans la base, le processus d\u0027apprentissage doit être relancé ; -Complexité élevée et lourdeur de la phase d\u0027apprentissage ; -Problème de mise à l\u0027échelle (Sclability), la construction du treillis dépend énormément du nombre d\u0027instances. Pour pallier certaines de ces limites avec un minimum de compromis, nous proposons dans la section qui suit un nouvel algorithme de classification que nous nommons CITREC et que nous appliquerons dans la problématique de la reconnaissance de visages.\nCITREC : Nouvelle approche hybride de classification à base de Treillis de Galois\nDans cette section nous allons introduire un nouvel algorithme, baptisé CITREC (Classification Indexée par le TREillis de Concepts), dont l\u0027objectif principal est de pallier les principales limites de l\u0027approche de Xie (Xie et al., 2002).\nPrincipe général de CITREC\nTout comme l\u0027approche de Xie (Xie et al., 2002), l\u0027idée principale de notre approche demeure le fait d\u0027utiliser le treillis de concepts pour sélectionner le sous-ensemble d\u0027instances qui sera utilisé par un classifieur contextuel lors de l\u0027apprentissage et du classement.\nL\u0027originalité de notre approche concerne essentiellement la phase d\u0027apprentissage et plus précisément la construction du treillis qui consiste à le construire à partir d\u0027un contexte réduit contenant seulement une instance représentative de chaque classe et non pas tout l\u0027ensemble d\u0027apprentissage. Ceci va nettement réduire le temps de construction en plus de l\u0027amélioration de la mise à l\u0027échelle (Scalability) sans perte d\u0027informations. Pour chaque noeud du treillis qui satisfait certaines contraintes que nous présentons plus loin dans cette section, un classifieur de base serait entrainé sur les instances de la base d\u0027apprentissage ayant la même classe que celles des instances représentatives dans l\u0027extension du concept.\nQuant à la phase de classement, des modifications ont été apportées à l\u0027approche de Xie afin d\u0027utiliser le treillis de concept comme étant un index.\nDans ce qui suit et afin d\u0027illustrer l\u0027explication de notre approche nous utilisons l\u0027ensemble de données IRIS présentée dans (Fisher, 1936).\nLa phase d\u0027apprentissage\nLa phase d\u0027apprentissage est une phase cruciale pour tout système de classification. En effet, la phase de classement ne fera qu\u0027utiliser les données préparées lors de cette phase. Par conséquent, les performances du système en dépendront d\u0027une façon remarquable. Lors de la phase d\u0027apprentissage, CITREC opère en quatre étapes successives que nous décrivons dans ce qui suit.\nPrétraitement de la base d\u0027apprentissage\nCette étape consiste à passer d\u0027une base d\u0027instances d\u0027apprentissage ayant des attributs numériques et nominaux à un contexte avec des attributs binaires uniquement. Les attributs numériques sont plus difficiles à traiter, car leur espace de valeurs est infini, d\u0027où la nécessité de discrétiser ces attributs pour aboutir à la fin à un espace de valeurs fini qui pourra ensuite être traité d\u0027une manière semblable à celle des attributs nominaux. Pour notre approche, et suite à une série d\u0027expérimentations, nous avons opté pour une méthode de discrétisation non supervisée générique qui consiste à diviser l\u0027espace de valeurs de l\u0027attribut sur k intervalles.\nFIG. 1 -Treillis d\u0027index relatif à la base illustrative IRIS\nRéduction du contexte global\nAprès avoir généré le contexte général, l\u0027idée clé de notre approche consiste à créer un nouveau contexte qui va servir à la construction du treillis d\u0027index. Le contexte réduit, contiendra un nombre d\u0027objets égal au nombre des classes des différentes instances de la base d\u0027apprentissage. La taille de ce contexte est par conséquent insensible au nombre d\u0027instances d\u0027apprentissage, ce qui nous fera gagner en matière de temps d\u0027exécution et de complexité lors de la construction du treillis et même lors du classement. L\u0027idée est de sélectionner une instance représentative de chaque classe. Le choix de cette instance est assez délicat, car c\u0027est elle qui servira à déterminer la classe de la nouvelle instance lors de la phase de classement. Nous notons que, pour mener à bien la phase de classement, cette instance devrait minimiser la somme des distances entre elle et le reste des instances de la même classe.\nConstruction du treillis d\u0027index\nQuant à la construction du treillis d\u0027index, nous notons que ce treillis (voir figure 1) sera construit sur la base du contexte réduit généré lors de l\u0027étape précédente. Nous avons opté pour un algorithme incrémental qui est une version améliorée de celui de Godin (Godin et al., 1995). Cet algorithme est implémenté au sein de la plateforme Galicia (Valtchev et al., 2003). Il est à noter que l\u0027incrémentalité est un aspect très important de l\u0027algorithme de construction du treillis utilisé au sein du système CITREC. Ce choix nous permet d\u0027ajouter de nouvelles instances à la base d\u0027apprentissage sans pour autant régénérer le treillis d\u0027index.\nCréation et affectation des classifieurs contextuels\nUne fois le treillis d\u0027index construit, il ne reste qu\u0027à sélectionner les noeuds pertinents et de leur affecter de nouveaux classifieurs. Par noeuds pertinents, nous désignons l\u0027ensemble des noeuds qui satisfont deux contraintes de sélection, à savoir la contrainte de support et la contrainte de non-inclusion. Dans ce qui suit nous désignons par CLS un classifieur contextuel.\nLa contrainte de support : Cette contrainte peut être formalisée comme suit : POUR chaque règle contextuelle C ? CLS, nous avons :\nLe paramètre ? désigne le pourcentage d\u0027instances représentatives qui doit exister dans l\u0027extension d\u0027un concept pour le sélectionner et lui affecter un classifieur contextuel. Nous avons fixé ce paramètre à 10% comme valeur par défaut 2 .\nLa contrainte de non-inclusion : La contrainte de non-inclusion permet d\u0027éviter de créer deux classifieurs dont le sous-ensemble d\u0027apprentissage du premier est inclus dans celui du deuxième. Dans ce cas, le premier n\u0027apportera pas de plus par rapport au deuxième. Ceci peut être formalisé comme suit : Étant donné deux règles contextuelles r 1 : C 1 ? CLS 1 et r 2 : C 2 ? CLS 2 , nous avons :\nAlgorithme 1: L\u0027algorithme d\u0027apprentissage de CITREC\nLa phase de classement\nLors de cette phase, CITREC reçoit en entrée une instance de classe inconnue et retourne la classe de cette instance après avoir effectué tout un processus de classification. Le processus de classification de CITREC peut être décomposé en trois étapes, à savoir : 1. Conversion de l\u0027instance, qui consiste à transformer l\u0027instance à classer de sa représen-tation d\u0027origine (avec des attributs numériques et/ou nominaux) vers la représentation conceptuelle basée uniquement sur des attributs binaires, en respectant les mêmes attributs considérés lors de l\u0027apprentissage. En d\u0027autres termes, cette conversion devrait respecter les intervalles de chaque attribut défini lors de la discrétisation.\n2. Recherche des classifieurs contextuels activés par la nouvelle instance. Cette étape consiste à ajouter l\u0027instance dans notre treillis d\u0027index et de marquer les noeuds auxquels cette nouvelle instance a été affectée. Sachant qu\u0027on ne doit marquer que les noeuds incorporant un classifieur. La façon la plus simple de marquer ces noeuds consiste à parcourir l\u0027ensemble des noeuds incorporant des classifieurs et de sélectionner les concepts dont l\u0027intension de l\u0027instance à classer est incluse dans l\u0027intension du concept à marquer.\n3. Vote majoritaire et décision, qui consiste à déclencher la classification de l\u0027instance originale (avant la conversion) par les différents classifieurs contextuels concernés par l\u0027instance à classer. Ensuite nous effectuons un vote majoritaire sur les résultats de chacun des classifieurs. La classe ayant eu plus de vote est celle de la nouvelle instance. Nous insistons sur le fait que la classification au sein des noeuds est effectuée avec l\u0027instance brute avant le prétraitement et ceci pour surpasser la perte de précision engendrée par la discrétisation. Cette étape sous-entend la conversion de ces instances en une représentation à attributs binaires.\nCITREC est incrémental\n2. Mise à jour du treillis d\u0027index. Cette étape nécessite un algorithme de construction du treillis de concept qui soit incrémental. Ce qui justifie notre choix pour l\u0027algorithme de Godin (Godin et al., 1995) qui présente cet avantage. En considérant l\u0027instance centrale déduite lors de la première étape, le treillis d\u0027index sera mis à jour par l\u0027ajout de cette instance.\n3. Mise à jour des noeuds affectés par l\u0027ajout.Il s\u0027agit de parcourir les différents noeuds du treillis d\u0027index affectés par l\u0027ajout et de sélectionner les noeuds incorporant des classifieurs contextuels pour mettre à jour leurs classifieurs de base respectifs. Un point important est que, le fait de mettre à jour les classifieurs de base, sous-entend l\u0027aspect incrémental de ceux-ci. Pour cela, nous avons opté pour des versions incrémentales de nos classifieurs de bases. \nAlgorithme 2: L\u0027algorithme de mise à jour de CITREC\nApplication de CITREC dans la reconnaissance de visages\nDans cette section, nous allons présenter l\u0027application de notre approche de classification supervisée CITREC dans la problématique de reconnaissance de visages. Cette approche nous fera profiter de l\u0027union de l\u0027analyse formelle de concepts avec l\u0027apprentissage à partir d\u0027instances (classifieur à PPV (Dasarathy, 1991)) ou la classification bayésienne (classifieur bayé-sien naïf (R. Duda, 1973)) pour contribuer à la résolution de la problématique de reconnaissance de visages.\nPrésentation du Benchmark\nNotre base de visages expérimentale est la base BioID qui consiste en 1521 images (384 x 288 pixels, niveaux de gris) de 23 personnes différentes et qui a été enregistrée durant plusieurs sessions dans différentes places et conditions d\u0027illumination.\nLe groupe de travail FGnet a procédé à l\u0027annotation des visages de la base BioID. Cette annotation consiste à la localisation de 20 points caractéristiques du visage humain (position des yeux,de la bouche, etc.). Dans le cadre de cet article, nous avons étendu l\u0027annotation fournie par FGnet de 20 points à 36 points caractéristiques. Les 16 points que nous avons ajoutés (voir figure 2) ont un impact remarquable sur les résultats, vu leurs emplacements stratégiques. Dans ce qui suit, nous désignons par annoter un visage le fait d\u0027en localiser les points spécifiques sur sa photo.\nModélisation du visage humain\nFormellement, le modèle du visage n\u0027est autre qu\u0027une traduction de la réalité qui nous permettra de l\u0027utiliser dans le cadre de notre algorithme de classification CITREC. Ce modèle sera construit à partir des points caractéristiques du visage. Nous notons que ce modèle sera basé sur les distances entre les points et non sur leurs positions. Pour nos expérimentations, nous avons généré en utilisant le modèle du visage et l\u0027annotation étendue que nous avons présenté dans la section précédente, un benchmark qui comporte 30 attributs numériques avec 1521 instances. Ces instances sont réparties sur 23 classes différentes.\nFormalisation mathématique\nSur un visage, nous considérons qu\u0027un point est représenté par deux coordonnées x et y, soit pt i un point, nous avons :\nComme nous l\u0027avons déjà présenté dans la section précédente, le modèle du visage se base sur les distances entre les points. Ainsi, nous désignons par d a,b la distance euclidienne entre deux points pt a et pt b , nous avons :\nNous notons que toutes les distances sont normalisées par la distance entre les deux pupilles des yeux (distance entre pt 1 et pt 2 ) pour assurer l\u0027invariance des mesures par rapport à la distance entre l\u0027individu et le dispositif de capture de la photo lors de la prise de vue.\nNous désignons par n d a,b la distance euclidienne normalisée entre les points a et b. De ce fait, nous avons :\nAinsi, nous désignons par v la représentation d\u0027un visage (voir figure 2) :\nNous passons dans ce qui suit à la validation expérimentale de CITREC dans la probléma-tique de la reconnaissance de visages.\nValidation expérimentale de l\u0027approche CITREC\nEn vu de valider notre approche hybride de classification supervisée et souligner son apport, nous présenterons une évaluation expérimentale de CITREC. Cette évaluation est enrichie par une suite de comparaisons avec d\u0027autres algorithmes existants dans la littérature, à savoir C4.5 (Quinlan, 1993), CART (Breiman et Friedman, 1984) et BFTree (Haijian, 2007). Les différentes comparaisons sont réalisées par rapport à trois variantes de CITREC qui sont CITREC P P V , CITREC RB et CITREC BN . Ces variantes incorporent respectivement un classifieur à plus proche voisins (Dasarathy, 1991), un classifieur par réseau Bayésien (Pearl, 1985) et un classifieur Bayésien naïf (R. Duda, 1973). Nous avons aussi comparé chaque variante de CITREC avec son classifieur de base correspondant. Concernant la méthode de validation,\nFIG. 2 -Modèle mathématique du visage\nnous avons opté pour la validation croisée d\u0027ordre 10 avec la technique leave-one-out pour la sélection des instances.\nAfin de fournir des résultats vérifiables, nous avons adapté CITREC pour pouvoir l\u0027intégrer dans l\u0027environnement d\u0027évaluation des algorithmes de datamining nommé WEKA (Witten et Frank, 2005).\nComparaison des résultats\nLa première étape de nos expérimentations porte sur la comparaison des résultats des différents algorithmes présentés précédemment par rapport à ceux des variantes de CITREC sur le benchmark des visages. Le tableau 1 présente ces résultats en termes de pourcentage de classifications correctes (PCC). Nous constatons un avantage des trois variantes de CITREC par rapport à leurs classifieurs de base respectifs, ainsi qu\u0027un avantage par rapport aux autres algorithmes de la littérature. Ceci confirme expérimentalement l\u0027adaptation de CITREC pour ce type de données, et par conséquent à la problématique de reconnaissance de visages. Nous constatons que la variante à plus proche voisins de CITREC l\u0027emporte en matière de PCC. Ceci est due aux bons résultats de prédiction réalisé par le classifieur à plus proches voisins (Dasarathy, 1991) pour ce type de données.\nImpact de l\u0027extension de l\u0027annotation sur les résultats des différents classifieurs\nNous avons présenté dans cet article, l\u0027extension de l\u0027annotation d\u0027origine de la base BioID. Cette extension nous a permis de créer 15 attributs supplémentaires dans le modèle du visage (de c16 à c30 (voir figure 2)). Le tableau 1 présente les résultats relatifs à l\u0027annotation de base et ceux de l\u0027annotation étendue. En comparant l\u0027annotation de base avec l\u0027annotation étendue, nous remarquons une amélioration claire des résultats de tous les algorithmes de classification en utilisant le modèle issue de l\u0027annotation étendue. Cette amélioration est due à la pertinence des attributs ajoutés, qui a augmenté la discrimination entre les personnes.\nConclusion et perspectives\nDans cet article, nous avons proposé une nouvelle méthode hybride de classification supervisée à base de treillis de Galois que nous avons nommé CITREC (Classification Indexée par le TREillis de Concepts) dont l\u0027originalité provient de la combinaison de l\u0027analyse formelle de concepts avec les approches de classification supervisée à inférence bayésienne (R. Duda, 1973;Pearl, 1985) ou à plus proches voisins (Dasarathy, 1991). Notre approche s\u0027inspire de celle proposée par Xie dans (Xie et al., 2002) en essayant de pallier ses limites majeures tel que l\u0027aspect non incrémental, la lourdeur de la phase d\u0027apprentissage et le problème de mise à l\u0027échelle (Scalability). Nous avons montré empiriquement sur un benchmark du domaine de la reconnaissance de visages que notre approche améliore les résultats de son classifieur de base et devance plusieurs algorithmes cités dans la littérature tels que : C4.5 (Quinlan, 1993), CART (Breiman et Friedman, 1984) et BFTree (Haijian, 2007).\nComme perspective, nous proposons d\u0027étudier la possibilité de recourir à la logique floue pour la représentation des instances centrales des classes, et ce afin de minimiser la dégrada-tion de la précision engendrée par la discrétisation des attributs numériques. Ce choix devrait surtout influencer la phase de génération du treillis d\u0027index.\nRéférences Breiman, L. et J. Friedman (1984). Classification and regression tree. In Wadsworth International, California. \n"
  },
  {
    "id": "806",
    "text": "Introduction\nExiste-t-il des séries typiques d\u0027événements qui structurent la vie familiale ? Est-ce que certaines séquences d\u0027événements sont typiques d\u0027une partie de la population ou d\u0027une souspopulation ? Pour répondre à ces questions, les sciences sociales ont besoin de méthodes pour analyser les parcours de vie dans leur totalité. Mais comment décrire ou comparer des sé-quences d\u0027événements ? Dans cet article, nous proposons de nous centrer sur les transitions dans les parcours de vie pour les décrire. Ainsi, l\u0027approche proposée adopte un point de vue complémentaire à l\u0027alignement de séquences, par exemple, qui se base sur des séquences d\u0027états.\nLes parcours de vie familiaux peuvent être compris comme des séries de transitions entre états de la vie familiale telles que fonder un nouveau foyer, l\u0027arrivée d\u0027un nouvel enfant ou le remariage d\u0027un parent... 2 Ces transitions peuvent être caractérisées par plusieurs événements simultanés, par exemple, lorsqu\u0027une personne fonde un foyer en quittant son domicile parental 1 Etude soutenue financièrement par le Fonds national suisse de la recherche (FNS) FN-100012-113998, et réalisée avec les données collectées dans le cadre du projet « Vivre en Suisse 1999-2020 », piloté par le Panel suisse de ménages et supporté par le FNS, l\u0027Office fédéral de la statistique et l\u0027Université de Neuchâtel.\n2 Dans cet article, nous nous centrerons sur la vie familiale, mais nous pourrions inclure d\u0027autres ensembles d\u0027évé-nements tels que ceux affectant la vie professionnelle.\nou encore lorsqu\u0027elle se marie en même temps que la naissance d\u0027un enfant. Nous nous inté-resserons à caractériser les parcours de vie familiaux des individus vivant en suisse en prenant les données de l\u0027enquête biographique rétrospective réalisée par le Panel suisse de ménages en 2002.\nL\u0027idée de départ de cet article est d\u0027explorer l\u0027applicabilité d\u0027une approche de type ngrammes pour caractériser les parcours de vie et en extraire des connaissances. Les n-grammes ont été utilisés dans des contextes multiples pour caractériser des textes (Damashek, 1995;Mayfield et McNamee, 1998). Cette méthode se base sur le découpage en courtes sous-séquences de caractères d\u0027un texte afin de pouvoir le caractériser et le comparer. Dès lors, il peut sembler logique d\u0027utiliser cette méthode pour caractériser les parcours de vie représentés sous forme de séquences d\u0027événements. Cependant, la méthode n\u0027est pas directement applicable à cause des différences dans les types de données. En effet, les n-grammes sont construits sur une séquence continue qui ne connaît ni la simultanéité des événements (les caractères forment une séquence stricte) ni les trous (absence de caractère pendant une période indéfinie). De plus, il importe ici de tenir compte de la variabilité des durées qui séparent des événements consécutifs. Nous proposons d\u0027utiliser la recherche de sous-séquences fréquente pour pallier ce manque.\nLe reste de l\u0027article est organisé de la manière suivante. Nous commençons par décrire plus précisément les données de type « parcours de vie » que nous avons à notre disposition. Nous passons ensuite en revue les principes de l\u0027analyse n-grammes et discutons des limites de leurs applications au parcours de vie. Nous présentons ensuite la recherche de sous-séquences fréquentes avant de discuter de son adaptation pour caractériser les parcours de vie. Finalement, nous appliquons la méthode décrite avant de discuter de ses apports par rapport à d\u0027autres méthodes plus classiques d\u0027analyse des parcours de vie.\nPrésentation des données\nNous utilisons les données de l\u0027enquête biographique rétrospective réalisée par le Panel suisse de ménages 3 en 2002. Nous utilisons les résultats du questionnaire sur les parcours de cohabitation. Pour chaque année, nous connaissons les personnes avec lesquelles habitaient le répondant. Ainsi, nous disposons, pour chaque individu, d\u0027une histoire de sa vie familiale. Nous avons choisi de centrer notre analyse sur la transition vers l\u0027âge adulte en prenant les parcours de vie depuis la naissance jusqu\u0027à trente ans. Afin de comparer des séquences similaires, nous n\u0027avons retenu que les répondants ayant trente ans lors de l\u0027enquête, soit 3557 individus.\nNotre base de données se présente sous la forme d\u0027une liste « individu-transition » où chaque transition est décrite à l\u0027aide d\u0027un ensemble d\u0027événements. Les événements considérés sont les arrivées et les départs de personnes qui compose le ménage dans lequel vit un individu donné. Le tableau 1 donne la liste des événements que nous avons utilisés dans notre analyse. Le codage des départs nous permet de capter les transitions en terme d\u0027un état vers un autre. En effet, la transition n\u0027est pas seulement caractérisée par son état de destination, mais également par son origine.\nNous présentons dans le tableau 2 un extrait de notre base de données. Dans cet exemple, nous avons représenté deux individus. Le premier n\u0027a connu que deux transitions : il naît en 1973 (« apparition » des parents dans le parcours) et quitte ses parents (événements L et Cette représentation des données -considérée notamment par Agrawal et Srikant (1995) -n\u0027est pas spécifique des données de type parcours de vie, mais peut représenter plusieurs types de séquence qui doivent prendre en considération la simultanéité des événements. Il en va de même pour le codage des événements que nous avons réalisé. En effet, dans une optique plus générale, on peut penser les événements comme l\u0027apparition ou la disparition d\u0027un attribut au cours de la vie d\u0027un individu.\nAprès avoir présenté les données et leur particularité, nous présentons les principes des n-grammes et nous discutons des limites de leurs applications à ces données.\nPrincipe des n-grammes\nNous reprenons ici la présentation des n-grammes de Damashek (1995). Un texte peut être représenté en utilisant un vecteur composé des fréquences relatives de chaque n-gramme distinct qui le compose. La liste exhaustive des n-grammes correspond à l\u0027ensemble des séquences de n caractères obtenus en faisant coulisser une fenêtre de n caractères le long du texte, un caractère à la fois. Ainsi, si n \u003d 3, le texte « abcde » sera décrit à l\u0027aide des tri-grammes « abc », « bcd » et « cde ».\nLe poids assigné à chaque n-gramme est égal à sa fréquence relative. Ainsi, si un texte comprend j n-grammes distincts, le poids x i du i-ième éléments est égal à :\navec m j , le nombre d\u0027occurrences du j-ième n-gramme. Par construction, on a j j\u003d1 x j \u003d 1. Cette pondération donne un poids identique à chaque caractère de la séquence -chacun apparaissant dans n n-grammes 4 . D\u0027autres types de pondération existent, tel que l\u0027utilisation du T F/IDF (Mayfield et McNamee, 1998).\nAfin de juger de la similarité de deux textes, Damashek (1995) propose d\u0027utiliser le cosinus de l\u0027angle entre deux vecteurs. Il note qu\u0027il peut être intéressant de mesurer cette différence par rapport au centroïde. En effet, ceci permet de mesurer les dissimilarités à partir d\u0027un point commun (qui pourrait former l\u0027origine « réelle ») plutôt que par rapport à l\u0027origine mathématique, nécessairement arbitraire.\nA première vue, l\u0027application des principes des n-grammes aux parcours de vie peut sembler directe. Il suffirait de coder les parcours de vie sous la forme de séquences de caractères. Cependant, ce codage n\u0027est pas possible pour deux raisons. Premièrement, les textes sont caractérisés par de très longues séquences alors que les parcours de vie ne contiennent que peu de caractères (ou d\u0027événements) en comparaison. Ainsi, l\u0027information apportée par un caractère dans un texte est bien moindre que celle d\u0027un événement dans un parcours de vie.\nDeuxièmement, la notion de succession des caractères est clairement établie dans le cas des textes, où chacun est précédé et suivi d\u0027un autre (hormis pour la fin du texte). Il n\u0027en va pas de même pour les parcours de vie où les événements peuvent être simultanés. De plus, la notion de succession est également différente dans une séquence d\u0027événements de vie. En effet, celle-ci n\u0027est pas complètement définie : est-ce qu\u0027un événement qui se produit vingt ans après un autre lui succède de la même manière que si l\u0027événement suivant se produit 3 ans après ?\nPour pallier ces problèmes, nous proposons de nous baser sur la notion de k-séquence plutôt que celle de n-grammes pour décrire les séquences à l\u0027aide de leurs sous-séquences. Cette notion a été introduite par Agrawal et Srikant (1995)  \nK-séquence\nLe concept de k-séquence et sa formalisation sont introduits par Agrawal et Srikant (1995). Nous utilisons ici la notation proposée par Zaki (2001) dans la présentation de son algorithme SPADE de recherche de sous-séquences fréquentes et de règles d\u0027association entre celles-ci.\nNous reprenons ici sa formulation en adaptant les termes à la terminologie que nous avons déjà introduite.\nUne séquence peut être comprise comme une liste ordonnée de transitions. Une séquence ? est notée (? 1 ? ? 2 ? ... ? ? q ), où chaque ? i désigne une transition. Les transitions sont des listes non ordonnées d\u0027événements distincts (c\u0027est-à-dire qu\u0027un événement ne peut apparaître deux fois dans la même transition). Une séquence composée de k événements est appelée une k-séquence.\nAgrawal et Srikant définissent alors la sous-séquence de la manière suivante : ? est une sous-séquence de ? si chaque transition de ? est un sous-ensemble de ? et que l\u0027ordre des transitions est conservé. On le note ? ?. Ainsi, par exemple, (B ? AC) est une sousséquence de (AB ? E ? ACD) puisque B ? (AB) -l\u0027événement B est contenu dans la transition (AB) -et que (AC) ? (ACD) -les événements A et C sont contenus dans la transition (ACD) .\nUne sous-séquence est dite fréquente si on l\u0027observe dans un nombre de séquences supé-rieures à un support minimum 5 défini au préalable. Une sous-séquence est dite maximale si elle n\u0027est incluse dans aucune autre sous-séquence fréquente.\nDans un but d\u0027interprétation des résultats, nous avons jugé utile de spécifier une fenêtre de temps maximale pour la recherche de sous-séquences (spécification qui existe également dans SPADE). Ainsi, il est possible de ne rechercher que les sous-séquences qui se déroulent dans un laps de temps donné. Cette spécification s\u0027avère très utile dans le cas de longue séquence avec peu d\u0027événements, car il est plus difficile d\u0027assumer une liaison entre des événements très éloignés dans le temps.\nLa formalisation d\u0027Agrawal et Srikant permet donc de distinguer l\u0027ordre d\u0027apparition des événements, mais également leurs simultanéités, en considérant les transitions comme des listes non ordonnées d\u0027événements. Les k-séquences nous permettent de caractériser les parcours de vie de la même manière que les n-grammes permettaient de caractériser des textes. Les parcours de vie peuvent être considérés comme des séquences caractérisées par des sousséquences. C\u0027est l\u0027aspect que nous présentons à présent.\nCaractérisation de parcours de vie à l\u0027aide de k-séquences\nEn suivant le principe des n-grammes, on peut représenter chaque séquence à l\u0027aide d\u0027un vecteur composé des fréquences relatives de chacune des sous-séquences. Pour cela, nous devons encore définir un critère pour choisir les sous-séquences que nous utiliserons. Nous devons également choisir une méthode de comptage des sous-séquences afin de calculer les fréquences relatives.\nChoix des sous-séquences\nEn suivant la méthode des n-grammes, on devrait utiliser un k fixé pour le calcul des sousséquences. Cependant, ceci n\u0027est pas pertinent pour les parcours de vie. En effet, certaines dynamiques ne peuvent être captées. Dans notre cas, un individu qui resterait chez ses parents jusqu\u0027à trente ans ne pourrait être décrit à l\u0027aide d\u0027une 3-séquence, car il n\u0027aurait connu que deux événements. Il en va de même pour une personne ayant un enfant après dix ans de mariage, si notre fenêtre de temps est plus restreinte. Dès lors, il faudrait prendre des séquences d\u0027ordre 1. Cette solution enlève toutes les notions de successions des événements et donc de séquence. Deux méthodes sont envisageables pour choisir les sous-séquences :\n1. On retient l\u0027ensemble des sous-séquences fréquentes qui sont applicables à une séquence donnée. Ainsi, si l\u0027on considère la séquence (A ? B ? C), on utilisera les sousséquences (A ? C) (A ? B) et (B ? C) ainsi que les « sous-séquences événements » (A),(B) et (C) pour autant qu\u0027elles satisfassent le support minimum.\n2. On ne retient que les sous-séquences maximales d\u0027une séquence donnée. Ainsi, la solution dépend du support minimum. Si l\u0027on considère la séquence (A ? B), on ne retiendra que (A ? B) si cette sous-séquence est fréquente ou les « sous-séquences événements » (A) et (B), pour autant qu\u0027elles soient fréquentes, sinon. Cette dernière méthode implique de recalculer plusieurs fois les sous-séquences fréquentes (puisque les fréquences dépendent de la sélection effectuée). Elle a cependant l\u0027avantage de ne pas multiplier le comptage de chaque événement entre les sous-séquences.\nMéthode de comptage et calcul des poids\nAfin de calculer les poids de chaque sous-séquence, il est nécessaire de choisir une mé-thode de comptage. Il existe plusieurs méthodes pour compter le nombre de fois qu\u0027une sousséquence apparaît dans un parcours de vie (Joshi et al., 2001). Nous proposons de compter le nombre d\u0027occurrences d\u0027une sous-séquence dans chaque séquence de vie, c\u0027est-à-dire le nombre de fois où l\u0027on observe la sous-séquence dans chaque parcours de vie. Par exemple, dans la séquence (A ? B ? B), on compte deux fois la sous-séquence (A ? B), une fois en liant (A) au premier (B) et une deuxième fois en le liant au second.\nSi l\u0027on s\u0027en tient aux fréquences relatives des sous-séquences, chaque événement et chaque transition peuvent avoir des poids différents en fonction de la séquence. En effet, un événement peut être décrit à l\u0027aide de plus ou moins de sous-séquences. Ce problème n\u0027apparaît pas explicitement dans le cas des n-grammes. En effet, on considère une fenêtre coulissante, ce qui implique que chaque lettre est comptée n fois. Il est vrai que les n ? 1 premiers et derniers caractères sont compté moins souvent. Cependant, cette différence de poids est compensée par la longueur du texte. Ainsi, l\u0027utilisation de la fréquence relative dans les n-grammes assure un poids plus ou moins équivalent à chaque caractère.\nPlusieurs solutions peuvent être adoptées : -Un poids identique à chaque sous-séquence.\n-Un poids établi de manière à ce que chaque événement ait un poids identique dans l\u0027ensemble. -Un poids établi de manière à ce que chaque transition ait un poids identique dans l\u0027ensemble. Il est nécessaire de fixer la base de calcul des poids accordés à chaque sous-séquence. Ils peuvent être calculés, par exemple, sur la base des événements concernés et du nombre de sous-séquences qui les décrivent. Ainsi si l\u0027on prend un ensemble de parcours de vie composé de K événements distincts, on évaluera le poids w s associé à une sous-séquence s, composée de E événements en utilisant :\nOù q s désigne le nombre d\u0027occurrences de la sous-séquence s, S e l\u0027ensemble des sous-séquences fréquentes contenant l\u0027événement e et q se le nombre d\u0027occurrences de la séquence s e qui exprime l\u0027événement e. Ainsi, chaque événement repartit sa contribution (soit 1 K ) entre l\u0027ensemble des sous-séquences qui le décrivent.\nLa caractérisation des séquences à l\u0027aide de sous-séquences nécessite de spécifier les trois points suivants : la méthode de sélection des sous-séquences fréquentes, la méthode de comptage ainsi qu\u0027une base permettant d\u0027assigner des poids à chaque sous-séquence.\nApplication : Analyse en composantes principales\nNous avons caractérisé l\u0027ensemble des séquences à l\u0027aide des sous-séquences fréquentes en choisissant un support minimum de deux pour cent. Ce seuil nous fait retenir 326 sousséquences différentes pour décrire les parcours de vie. Nous ne présentons pas ici les résultats obtenus avec la caractérisation à l\u0027aide des sous-séquences maximales. En effet, les résultats sont difficiles à interpréter, sans-doute à cause du manque de redondance de l\u0027information qui permet de calculer les proximités entre événements et sous-séquences. Pour cette première analyse, nous avons choisi d\u0027accorder un poids identique à chaque sous-séquence en suivant les principes utilisés dans le cas des n-grammes.\nA l\u0027aide de ces données, nous avons effectué une analyse en composante principale pour offrir une visualisation de l\u0027espace formé par les parcours de vie. Dans les graphiques qui suivent, les sous-séquences contribuant le plus significativement à la construction des axes sont représentées à l\u0027aide d\u0027un rond bleu. Nous avons également ajouté plusieurs variables supplémentaires : le sexe (triangle, pointe en haut, rouge), la langue d\u0027interview que nous considérerons comme un proxy pour aborder la culture d\u0027origine (triangle, pointe en bas, rouge clair), la cohorte de naissance -la période dans laquelle on est né -(carré vert) ainsi que la confrontation à des problèmes d\u0027argent dans la jeunesse (losange bleu). Ces différentes variables nous permettront de caractériser le contexte dans lequel ces séquences apparaissent.\nNous avons également ajouté comme variable supplémentaire (représentée à l\u0027aide d\u0027un pentagone vert) la classification des parcours de vie décrite dans Müller et al. (2007) obtenue à l\u0027aide de la méthode « optimal matching ». Cette méthode permet de classer les séquences d\u0027états en respectant les états présents ainsi que leurs temporalités. Cette classification est construite sur la base de quatre événements soit le départ du domicile parental, le mariage, l\u0027arrivée du premier enfant et le divorce. La méthode décrite ici permet de prendre en considération un plus grand nombre d\u0027événements, tel que la mise en couple, la cohabitation avec des amis, etc... Nous pourrons ainsi comparer la méthode présentée avec celle de l\u0027optimal matching.\nLa figure 1 nous montre les deux premières dimensions de l\u0027espace formé par les sé-quences. La première dimension oppose ceux qui sont restés chez leurs parents (à gauche) aux autres. Ainsi, on retrouve à gauche les séquences K, Q et KQ qui dénotent l\u0027arrivée des deux parents. De par notre système de pondération, ces sous-séquences ont un poids très élevé La classification de l\u0027optimal matching se conforme aux axes et à leur description. Ainsi, le premier axe oppose ceux qui ne partent pas ou tard de chez leurs parents aux autres, alors que le second distingue la temporalité du mariage. On remarque que cette temporalité est prise en compte à l\u0027aide des sous-séquences du départ du domicile parental qui marque ainsi des transitions différentes pour le mariage tardif et le mariage précoce.\nLa troisième dimension (figure 2) oppose ceux qui ont un poids fort sur les événements de type départ de chez les parents aux autres. Ainsi, le point le plus à droite correspond à ceux qui ne partent pas selon la classification « optimal matching ». C\u0027est le dernier axe à être corrélé FIG. 2 -Positionnement des séquences et des variables supplémentaires sur le plan des troisième (7.55%) et quatrième (5.45%) composantes principales.\navec le nombre d\u0027événements ou de transition (r \u003d 0.27). La quatrième dimension distingue ceux qui partent pour vivre en couple ou pour se marier, de ceux qui partent vivre seuls. Ainsi, les regroupements de trajectoires menant directement au mariage se situent dans les valeurs positives.\nLa cinquième dimension (figure 3) oppose ceux qui partent de chez leurs parents pour se mettre en couple aux autres formes de départ. Cette dimension montre une forte association avec la cohorte. On remarque que le départ du domicile parental pour se mettre en union sans mariage immédiat (ce qui ne signifie pas sans mariage) est une dynamique relativement récente. Cette dimension oppose les répondants originaires de Suisse romande (francophone) aux autres. Finalement, la sixième dimension distingue ceux qui se marient la même année où ils ont leur premier enfant des autres formes de départ. Ce modèle est relativement plus présent en Suisse italienne et dans les cohortes plus anciennes.\nIci encore, on trouve un lien avec la classification de l\u0027optimal matching. Cependant, les liens ne sont pas directs. Ainsi, la catégorie « Mariage tardif » est située tout à gauche montrant qu\u0027on se marie tard, mais qu\u0027on est déjà en union. Il en va de même pour les trajectoires « Modernes ». Les « Mariages précoces » se situent à l\u0027opposé et montre qu\u0027on se marie directement en quittant ses parents et qu\u0027on tend également à avoir des enfants à ce moment.\nNous ne présentons ici que les six premières dimensions. Cependant, il est possible d\u0027associer un sens aux dix premières dimensions. Ceci nous montre qu\u0027un grand nombre de dimensions sont nécessaires à la compréhension de l\u0027espace des séquences. Ce n\u0027est pas une surprise étant donné que nous avons regroupé un grand ensemble d\u0027événements qui sont souvent analysés séparément, tel que le départ du foyer parental, la mise en union ou encore l\u0027arrivée du premier enfant.\nLes résultats obtenus sont relativement similaires à ceux de l\u0027optimal matching, mais apportent un éclairage complémentaire en se centrant sur les transitions. Ainsi, on a pu voir des différences entre le modèle de mariage tardif et précoce en identifiant certaines transitions intermédiaires (notamment la mise en union). La méthode proposée a cependant l\u0027avantage de pouvoir intégrer un plus grand nombre d\u0027événements, surtout s\u0027ils peuvent être simultanés. Avec l\u0027optimal matching, cette situation implique un trop grand nombre d\u0027états et les résultats deviennent difficilement interprétables. Sans spécifier explicitement une temporalité, celle-ci se retrouve dans les axes. Cependant, ceci est dû aux séquences qui se centrent sur la période allant de la naissance à 30 ans ainsi qu\u0027à la présence de l\u0027événement départ du domicile parental que presque l\u0027ensemble des individus connaît pendant cette période.\nAinsi, la caractérisation à l\u0027aide des sous-séquences fréquentes permet d\u0027offrir une visualisation de l\u0027espace formé par les séquences et de dégager quelques dynamiques intéressantes sur les différences entre types de trajectoire, notamment avec l\u0027aide des variables supplémentaires. Toutefois, nous voyons deux limites à la méthode présentée qui nécessite des développements futurs. Le système de pondération des sous-séquences rend parfois l\u0027interprétation des axes difficile, puisqu\u0027elle dépend de la présence d\u0027autres événements dans la séquence. Il s\u0027agit d\u0027un poids relatif à la séquence. Il est ainsi nécessaire de développer d\u0027autres méthodes qui\n"
  },
  {
    "id": "807",
    "text": "Introduction\nL\u0027objectif de notre étude est de pouvoir semi-automatiser le processus de réponse aux plaintes exprimées en français, en langue naturelle et relatives à la pollution de l\u0027air au sein des logements. Ces plaintes reflètent chacune un cas particulier, cependant elles abordent des problèmes communs que les experts aimeraient identifier de manière objective. Notre démarche est de construire de manière automatique des scénarii. Dans la première étape nous établissons un modèle de représentation et de recherche en ne négligeant pas l\u0027aspect sémantique. Le choix de la ressource sémantique est guidé par l\u0027étude du vocabulaire du corpus, il est présenté dans la partie suivante. Enfin, nous présentons l\u0027évaluation de la qualité des partitions (scénarii) obtenues.\nModélisation de l\u0027espace des plaintes\nPar manque de place ici, nous ne pouvons rappeler de manière détaillée nos nombreuses positions pour formaliser les textes et pour définir les différentes mesures de similarité textuelle correspondantes. Néanmoins, nous pouvons noter que pour le traitement des textes longs, les modèles vectoriels sont les plus fréquemment utilisés, tandis que pour la comparaison des textes courts le modèle booléen flouifié est le mieux adapté. De manière générale, la plainte comporte des informations concernant les symptômes de l\u0027occupant, elle décrit également le logement et son environnement extérieur, ..etc. Nous avons traduit ces champs de conversation sous forme de modèles de balise (unités sémantiques) dans un document XML pour enregistrer les plaintes. Les balises que nous avons retenues pour le formalisme XML des plaintes sont: symptômes, habitat et environnement Zargayouna et Salotti (2004) ont étendu le modèle vectoriel de Salton en définissant le nouveau poids des termes TF-ITDF adapté à la structure XML des documents. Un vecteur des poids des termes correspond à une unité sémantique (contenu d\u0027une balise). La similarité entre deux unités sémantiques est calculée en fonction du cosinus de l\u0027angle formé par les deux vecteurs correspondants.\nLe modèle vectoriel étendu\nLe modèle de recherche basé sur la proximité floue des termes\nLe degré de proximité floue des termes de la requête permet d\u0027évaluer le taux de densité des termes de la requête dans les textes. Il permet ainsi de classer les documents en fonction de leur pertinence par rapport à la requête. Mercier et Beigbeder (2004) calculent la pertinence relative µ des termes de la requête aux différentes positions x dans un document d comme suit:\nLe paramètre k caractérise le degré d\u0027influence d\u0027une occurrence d\u0027un terme. La pertinence d\u0027une requête booléenne disjonctive et/ou conjonctive est calculée en prenant respectivement le maximum et/ou le minimum des pertinences locales. Le score d\u0027un document par rapport à une requête est calculé en agrégeant les pertinences relatives locales.\nLe modèle vectoriel sémantique\nLe poids sémantique SemW du terme t au sein d\u0027une balise b d\u0027un document d au niveau du vecteur sémantique définit par Zargayouna et Salotti (2004) correspond à la somme de son poids TF_ITDF et les poids des termes qui lui sont proches sémantiquement.\nNotre modèle de recherche\nLa mesure de Mercier-Beigbeder ne tient pas compte de la sémantique. Dans (Heddadji et al., 2007) nous augmentons cette mesure de manière à prendre en considération des liens de similarité latents entre documents et à l\u0027adapter au formalisme XML.\nUn seuil de similarité est nécessaire pour délimiter l\u0027ensemble des termes sémantiquement pertinents par rapport à t. Nous fixons cette limite à l\u0027ensemble des termes dont le degré de similarité avec t est au-delà du score de similarité de ce dernier avec le terme auquel est rattachée la balise correspondant à l\u0027unité sémantique où son occurrence apparaît. Dans le cas d\u0027un corpus annoté en XML, les similarités citées calculent des appariements locaux. Une agrégation des similarités locales est nécessaire pour généraliser ces mesures au niveau «document».\nGénération de la sémantique\nIl est généralement reconnu que l\u0027emploi d\u0027une ontologie apporte une solution élégante au problème de la gestion de la sémantique. Les plaintes sont formulées par des particuliers. De nombreuses marques de produits et autres sigles sont utilisés, le vocabulaire est très vivant, d\u0027où l\u0027interrogation de la possibilité de construire une ontologie.\nLe vocabulaire des plaintes\nNous avons étudié le vocabulaire des plaintes dont nous disposons. L\u0027expérimentation que nous avons menée a eu lieu sur un corpus de 655 documents formulant des plaintes provenant de 4 organismes différents. Pour étiqueter les textes des plaintes, nous avons utilisé l\u0027outil Tree-tagger adapté au français. Les sigles, abréviations et autres acronymes en relation avec le domaine de la pollution inconnus de Tree-Tagger sont récapitulés dans un fichier dédié. Les termes retenus dans ce fichier sont substitués automatiquement par leurs synonymes ou par des termes plus génériques compréhensibles par l\u0027étiqueteur. La compréhension de la langue naturelle est spécifiée formellement par la notion d\u0027ontologie. Dans des domaines précis la communauté professionnelle s\u0027accorde autour d\u0027une «ontologie métier». Nous analysons ici l\u0027évolution du vocabulaire de nos plaintes qui sont en rapport avec la pollution intérieure uniquement. L\u0027allure asymptotique de la courbe de la FIG. 1 est une preuve de l\u0027insuffisance d\u0027une éventuelle ontologie gérant la sémantique car nous ne connaissons pas le vocabulaire utilisé qui reste ouvert en l\u0027état actuel de la base des plaintes. Afin de permettre l\u0027usage de la langue naturelle dans la description des plaintes, il est nécessaire de construire un réseau conceptuel de façon à comprendre la langue française. A date, il n\u0027existe pas d\u0027ontologie universelle en français dans laquelle on pourrait retrouver de manière exhaustive les termes du langage naturel et qui puisse servir de base à un système de recherche implémentant la sémantique. Le résultat de notre étude conduit à considérer l\u0027utilité des dictionnaires électroniques des synonymes pour le contrôle de la sémantique tout en assurant une couverture la plus exhaustive possible du lexique des plaintes.\nRNTI -X -\nDICTIONNAIRE et codage des mots\nNous avons utilisé le dictionnaire électronique des synonymes du laboratoire CRISCO de l\u0027université de CAEN baptisé DICTIONNAIRE et qui regroupe les synonymes de 48 881 mots (vedettes) (Manguin, 2004). En plus de la connaissance sémantique que nous offre DICTIONNAIRE, nous l\u0027utilisons en tant que vocabulaire de base. Nous utilisons les vedettes de DICTIONNAIRE en tant que primitives vectorielles caractérisant les textes et permettant de les apparier à l\u0027aide du module de recherche implémentant les modèles vectoriels. La proximité sémantique entre deux termes A et B du dictionnaire correspond au rapport entre le nombre de synonymes communs et le nombre total de ces synonymes (indice de Jaccard). En principe, les formes fléchies d\u0027un terme partagent le même sens (combienmême elles ne sont pas de la même catégorie grammaticale). Le dictionnaire des synonymes est insuffisant (le taux de similarité entre « pollution » et « polluer » est de 0 dans la base sémantique inférée par DICTIONNAIRE ) et un système de codage des termes est nécessaire pour la gestion de la sémantique entre termes du même code. Pour coder les mots, nous avons choisi d\u0027utiliser une heuristique, certes imparfaite, mais qui permet de rapprocher des termes ayant la même racine. L\u0027heuristique d\u0027Enguehard (Enguehard ,1992) qui définit le code d\u0027un terme comme étant la sous-chaîne des premières lettres jusqu\u0027à l\u0027obtention de deux voyelles non consécutives nous a paru simple à mettre en oeuvre. Ayant fait ses preuves dans d\u0027autres applications (serradura et al., 2002) nous avons appliqué ce principe pour apparier les termes issus du même code. Pour chaque paire de mots de DICIONNAIRE de même code on a effectué un échange de synonymes avec une influence de ½. Le degré de similarité entre les éléments de ces paires est de 0,5. Cette définition du lien sémantique entre termes nous permet maintenant de pouvoir comparer sémantiquement deux plaintes.\nComparaisons\nEvaluation de la qualité des partitions obtenues\nPour définir les scénarii nous choisissons de réaliser une partition de l\u0027ensemble des plaintes. Pour ceci nous utilisons l\u0027algorithme des nuées dynamiques dans lequel nous avons choisi pour noyau itératif la plainte qui minimise la disparité autour d\u0027elle. Ce travail a été réalisé parallèlement par 3 experts du CSTB (Centre Scientifique et Technique du Bâtiment) sur 100 documents d\u0027entraînement. Les experts se sont entendus sur l\u0027existence de 3 thématiques traitant chacune d\u0027un phénomène de pollution isolé. Le but de cette étude est d\u0027évaluer d\u0027une part les performances des modèles de représentation et de recherche, et d\u0027autre part la qualité de la synthèse automatique de la base de plaintes en un ensemble de scénarii type de pollution. In fine, la solution affectée au scénario dont appartient la plainte la plus pertinente sera assignée au problème courant. Pour évaluer la qualité de nos différentes classifications, nous avons calculé le rapport entre la distance inter-classes et la distante intra-classe. De plus, nous avons comparé les partitions automatiques avec les partitions des experts en utilisant l\u0027indice de Rand-corrigé. \nRNTI -X -\nLe modèle vectoriel sémantique et le modèle flou sémantique donne de meilleures classes quand l\u0027espace de données est partitionné en 3 clusters. En effet, nous percevons dans FIG. 2 que les modèles sémantiques partitionnent moins strictement que les systèmes directs dans le cas où le nombre de classes est supérieur à 3. D\u0027un autre coté, les experts ont constaté l\u0027existence de 3 classes de plaintes disjointes. Ce qui explique que le meilleur score de partition soit au niveau du graphe à la position k\u003d3 pour les modèles sémantiques. Ailleurs, la partition sous les systèmes sémantiques constituent des clusters moins denses, ce qui explique la flexion enregistrée aux partitions supérieures.\nConclusion\nNous souhaitions à partir de la série d\u0027études présentée appuyer l\u0027idée de la correspondance existante entre le raisonnement des experts basé sur des faits et le raisonnement de nos systèmes de recherche basé sur les termes et leur sémantique. Pour améliorer encore les résultats, il nous semble pertinent de réaliser une classification floue des plaintes pour mettre en évidence des cumuls de thématiques dans une plainte particulière.\n"
  },
  {
    "id": "808",
    "text": "Introduction\nBien que les méthodes d\u0027analyse factorielle soient très puissantes et contribuent efficacement à la visualisation des données, les grands échantillons nécessitent de nouvelles méthodes mieux adaptées. En effet, les algorithmes de décomposition matricielle rencontrent leurs limites sur les grands tableaux numériques ; en outre, la construction de nombreux plans de projection, du fait des grandes dimensions, rend la tâche d\u0027interprétation difficile pour recouper les informations disséminées sur ces plans. Finalement une grande quantité de données implique une grande quantité d\u0027informations à synthétiser et des relations complexes entre individus et/ou variables étudiés. Il est alors possible, dans ce contexte, d\u0027utiliser les cartes de Kohonen ou cartes auto-organisatrices (SOM) (Kohonen, 1997) qui sont des méthodes de classification automatique utilisant une contrainte de voisinage sur les classes pour conférer un sens topologique aux partitions obtenues. La carte auto-organisatrice originelle peut être vue comme une variante de l\u0027algorithme des k-means (MacQueen, 1967) intégrant une contrainte d\u0027ordre topologique sur les centres.\nLorsque la matrice des données x est définie sur un ensemble I d\u0027objets (lignes, observations) et un ensemble J de variables (colonnes, attributs), différentes approches de classification automatique sont utilisées et la plupart des algorithmes proposés concerne généralement un des deux ensembles. Ces algorithmes peuvent être modélisés par différentes approches. Celle qui a suscité le plus d\u0027intérêt ces dernières années est incontestablement l\u0027approche modèle de mélange (McLachlan et Peel, 2000). Dans ce cadre, il a été proposé diverses versions probabilistes de SOM telles que dans (Lebbah et al., 2007;Verbeek et al., 2005;Luttrell, 1994).\nLe papier (Govaert et Nadif, 2003) présente une extension du modèle de mélange pour ré-pondre à l\u0027objectif de la classification croisée dite aussi classification par blocs qui permet de tenir compte de I et J simultanément. Différents modèles ont été proposés pour tenir compte de chaque type de données.\nCes méthodes (Dhillon, 2001) ont un grand intérêt en data mining car elles sont particuliè-rement appropriées pour les grands ensembles de données en grande dimension. Elles ne sont pourtant pas encore employées en visualisation alors qu\u0027elles ont le potentiel pour fournir un outil très efficace et parcimonieux. En effet, elles utilisent beaucoup moins de paramètres que les modèles connus usuels tels que les modèles classiques de mélange.\nPour analyser le contenu d\u0027un ensemble de données, la visualisation est une étape crucial pour laquelle les modèles génératifs sont devenus très utiles. En effet, la taille croissante des ensembles de données rencontrés permet une estimation pertinente de variables cachées synthétisant de manière interprétable l\u0027information contenue dans les données. Pour toutes ces raisons, nous proposons dans ce papier de traiter la question de la visualisation par une approche basée sur le modèle de mélange croisé parcimonieux et l\u0027algorithme GTM (Bishop et al., 1998), méthode de auto-organisatrice probabiliste basée sur un modèle gaussien.\nCe papier est organisé comme suit. Le deuxième paragraphe présente une brève introduction du modèle de mélange croisé et une description rapide de l\u0027algorithme Block EM. Le troisième paragraphe est consacré au développement, dans le cas binaire, de l\u0027algorithme Block Generative Topographic Model ou Block GTM. Cet algorithme peut être vu comme une extension efficace du GTM à un modèle de mélanges de Bernoulli par blocs. Un algorithme d\u0027estimation y est présenté. Le quatrième paragraphe présente des expériences numériques à partir de deux matrices binaires textuelles. Enfin, le dernier paragraphe résume les principaux résultats du papier et les perspectives originales de cette approche.\nDans la suite, la matrice de données est notée x \u003d {(x ij ); i ? I et ? J}, où I est un ensemble de n objets (lignes, observations) et J est un ensemble de d variables (colonnes, attributs). Une partition z en g classes de l\u0027échantillon I sera représentée par la matrice de classification (z ik ; i \u003d 1, . . . , n ; k \u003d 1, . . . , g) où z ik \u003d 1 si i appartient à la classe k et 0 sinon. Une notation similaire sera utilisée pour la partition w en m classes de l\u0027ensemble J. Par souci de simplification des formules, les intervalles de variation des indices ne seront pas spécifiés, par exemple, nous noterons\n.\nL\u0027algorithme Block EM\nL\u0027objectif de la classification par blocs est d\u0027essayer de résumer cette matrice par des blocs homogènes. Le problème peut être étudié sous l\u0027approche d\u0027une partition simultanée des deux ensembles I et J en g et m classes respectivement. Dans (Govaert, 1983(Govaert, , 1995 plusieurs algorithmes ont été proposés pour obtenir une classification par blocs sur des tableaux de contingence ou plus généralement sur des tables qui ont les même propriétés : des données binaires, continues ou catégorielles.\nDans (Govaert et Nadif, 2003), ces méthodes ont été modélisées par une approche basée sur des mélanges de lois. Dans le contexte du problème de la classification par blocs, la formulation du modèle de mélange classique peut être étendue pour proposer un modèle en blocs latents défini par une distribution en sommant sur l\u0027ensemble des affectations de I × J :\noù Z et W dénote les ensembles de toutes les affectations possibles z de I et w de J. Comme pour l\u0027analyse en classes latentes, les n × d variables aléatoires X ij générant les cellules x ij observées sont supposées être indépendantes lorsque z et w sont fixés ; nous avons alors\noù ?(., ? k? ) est une distribution définie sur l\u0027ensemble des réels R.\nPar exemple, lorsque les données sont binaires, en notant ? \u003d (p, q, ? 11 , . . . , ? gm ), où p \u003d (p 1 , . . . , p g ) et q \u003d (q 1 , . . . , q m ) sont les vecteurs de probabilités p k et q ? qu\u0027une ligne et une colonne appartienne au k e composant et au ? e composant respectivement, nous obtenons le modèle par blocs latents de Bernoulli défini par la distribution suivante :\nUtiliser ce modèle est nettement plus parcimonieux qu\u0027utiliser un modèle classique de mélange sur chaque ensemble I et J. Par exemple, avec n \u003d 1000 objets et d \u003d 500 variables et des probabilités de classes égales p k \u003d 1/g et q ? \u003d 1/m, si on a besoin de faire la classification automatique d\u0027une matrice binaire en g \u003d 4 classes en lignes et m \u003d 3 classes en colonnes, le modèle par blocs latents de Bernoulli impliquera l\u0027estimation de 12 paramètres (? k? , k \u003d 1, . . . , 4, ? \u003d 1, . . . , 3) au lieu de (4 × 500 + 3 × 1000) pour les deux modèles de mélange de Bernoulli appliqués à I et J séparément. Maintenant nous nous intéressons à l\u0027estimation d\u0027une valeur optimale de ? par l\u0027approche du maximum de vraisemblance associé à ce modèle de mélange par blocs. Pour ce modèle, les données complétées sont le vecteur (x, z, w) où les vecteurs non observés z et w sont les labels. La vraisemblance classifiante L(?; x, z, w) \u003d log f (x, z, w; ?) est notée L C (z, w, ?). L\u0027algorithme EM (Dempster et al., 1977) maximise la vraisemblance L M (?) par rapport à ? itérativement en maximisant l\u0027espérance conditionnelle de la vraisemblance des données complétées L C (z, w, ?) par rapport à ?, étant donné une estimation précédente courante ? (t) et les données observées x :\nj? , et e ikj? sont respectivement les probabilités a postériori sur les lignes, les colonnes, et les cellules, à l\u0027itération t.\nMalheureusement, la structure de dépendance des variables X ij du modèle entraine des difficultés pour la détermination de e (t) ikj? . Pour résoudre ce problème, une approximation variationnelle remplaçant e (t) (t) (t) ikj? par le produit c ik d j? permet de fournir une bonne solution (Govaert et Nadif, 2005).\nDans la section suivante, nous développons un algorithme d\u0027apprentissage intégrant une contrainte d\u0027ordre topologique sur les paramètres ? k? .\nModèle et estimation\nNous présentons le Binary Bock GTM, une carte auto-organisatrice générative par blocs pour une matrice binaire x dont chaque cellule x ij est un réel 0 ou 1. Pour induire une autoorganisation topologique des densités gaussiennes, une approche par le GTM considère des coordonnées 2d pour les noeuds d\u0027une grille rectangulaire imaginaire qui représente l\u0027espace de projection. Ce graphe planaire peut être vu comme une discrétisation d\u0027une partie du plan sur lequel les données, les n lignes de la matrice, vont être projetées. Comme chaque noeud doit correspondre à une classe, chaque point 2d sur le plan est alors sujet à une transformation non linéaire afin d\u0027être amené dans un espace de dimension h supérieure. Une projection linéaire permet alors d\u0027obtenir des centres de même dimension que les individus vectoriels.\nPlus formellement, afin d\u0027obtenir une auto-organisation des probabilités ? k? , ces dernières sont paramétrées par les g coordonnées s k dessinant une grille rectangulaire régulière sur le plan. Ces coordonnées sont projetées dans un espace de plus grande dimension h, soit en prenant pour les applications ? des bases fonctionnelles de type noyau,\n? où m ? est un centre posé ad\u0027hoc et ? ? une variance bien choisie. Finalement, la paramétrisation nécessite l\u0027estimation de m vecteurs de dimension h inconnus nommés w ? . On écrit les probabilités du BEM binaire original à l\u0027aide de fonctions sigmoïdes ? k? \u003d ?(w T ? ? k ) où ?(y) \u003d e y /(1 + e y ), comme montré en figure 1. Le vecteur de paramètres devient ? \u003d (p, q, w 1 , w 2 , · · · , w m ).\nFIG. 1 -A gauche, la grille rectangulaire des s k , sur la droite, l\u0027espace des distributions ?. Le graphique représente la paramétrisation non linéaire des sigmoïdes. Chaque coordonnée\ng, de la grille est transformée de façon non linéaire pour se retrouver dans l\u0027espace des distributions multivariées de Bernoulli par la transformation ?(w\nLa matrice g × m de probabilités est remplacée par la matrice h × m, et le modèle demeure parcimonieux puisque h est petit en pratique, quelques dizaines. Donc, en reprenant à nouveau l\u0027exemple d\u0027une matrice binaire de 1000 lignes et 500 colonnes de la section précédente, nous aboutissons à environ 100 paramètres, compte tenu du choix à effectuer sur la valeur de h, qui est toujours peu comparativement à une approche de mélange classique. Ensuite, les para-mètres inconnus sont estimés en trouvant un maximum local de la log-vraisemblance par un algorithme EM (Dempster et al., 1977).\nGrâce à l\u0027approximation variationnelle de e\nikj? , il peut être montré (Govaert et Nadif, 2005) que la maximisation de la log-vraisemblance du Block EM est réalisée en maximisant alternativement deux critères conditionnels Q(?,\n, m).\nA la convergence en une position stable de ?, le paramètre optimal est nomméˆ?nomméˆ nomméˆ?. Ici, considérant des paramètres w ? , ces deux critères prennent la forme suivante :\nik . La maximisation de ces deux espérances, effectuée à l\u0027aide de la méthode du gradient, conduit aux relations suivantes :\nEn dérivant les deux critères, on obtient les vecteurs de gradient Q\nv . Comme les hessiennes sont diagonales par blocs, la log-vraisemblance est augmentée à chaque pas EM par deux pas de montée de type Newton-Raphson, pour ? de 1 à m, ce qui correspond à un algorithme EM généralisé.\nT la g × h matrice des bases fonctionnelles, nous obtenons :\n, les valeurs consécutives courantes convergent vers un maximum d\u0027une approximation de L M (?). Un biais bayésien (Bishop et al., 1998) peut éventuel-lement être ajouté pour améliorer la stabilité numérique des estimations. La forme matricielle obtenue par une approche de gradient du second ordre est analogue à une étape d\u0027IRLS (McCullagh et Nelder, 1983). Une alternative serait un gradient au premier ordre sous optimal en pratique. On remarque enfin que la symétrie des formules du BEM originale est ici absente du fait que seules les lignes sont projetées par la méthode proposée.\nExpériences numériques\nNous évaluons notre nouvelle méthode de projection à partir de deux matrice binaires de données textuelles. Les paramètres utilisés dans nos expériences sont m \u003d 10, g \u003d 81 et h \u003d 28 pour les deux bases de textes.\nLa projection sur le plan d\u0027un échantillon de données binaires par le modèle Block GTM peut s\u0027effectuer de diverses manières, dont essentiellement : -La représentation matricielle qui place en s k * l\u0027ensemble des individus affectés à la classe associée au noeud, tels quê z i \u003d k * . Cette affectation obéit à la règle du maximum a postériori (MAP) doncˆzdoncˆ doncˆz i \u003d argmax k ˆ c ik . Dans le cas du SOM, l\u0027affectation utilise la distance euclidienne entre le vecteur centre et le vecteur donnée.\n-La deuxième représentation, que nous avons utilisée dans la suite car celle-ci est plus fidèle à la classification floue obtenue, consiste en une projection par position moyenne sur le plan :\nOn remarque que la projection MAP correspond à la projection moyenne dans laquelle on remplace la matrice de classification floue de cellules (ˆ c ik ) par la matrice de classification dure de cellules (ˆ z ik ).\nFIG. 2 -Projection par Binary Block GTM de la matrice textuelle 449 × 167 des données Classic 3.\nLa première matrice est projetée pour tester le modèle avec trois classes. Ces données correspondent à un échantillon de la matrice Classic 3 (Dhillon et al., 2003), qui est constituée de trois bases d\u0027articles scientifiques : M edline, Cisi, Cranf ield. Par tirage au hasard, 450 documents ont été sélectionnés avec 150 documents dans chaque classe. Seuls les mots les plus fréquents (au dessus du seuil 30) ont été retenus. La matrice finale est de 449 lignes et 167 colonnes. La projection sur la figure 2 sépare les classes sans erreur quasiment. Les outliers peuvent s\u0027expliquer par le fait que le tableau original est de contingence, et également que les classes ne sont pas exactement disjointes comme le révèle les benchmarks relatifs.\nLa seconde matrice textuelle présente quatre classes et compte 400 documents décrits par 100 termes (Girolami, 2001). Le vocabulaire a été choisi par tri selon l\u0027information mutuelle évaluée grâce aux labels des classes. La projection de ces textes, à la figure 3, révèle 4 clusters facilement reconnaissables et correspondant aux quatre groupes de discussion \"sci.crypt\", \"sci.space,\"sci.med\", et \"soc.religion.christian\" ; dans chacun, 100 news ont été tirés au hasard. Les classes sont bien séparées avec des frontières précises et les classes de mots obtenues peuvent être interprétées. Notons que la carte obtenue avec notre approche est assez similaire à la carte auto-organisatrice probabiliste basée sur un modèle multinomial asymétrique (Kabán et Girolami, 2001), qui est moins parcimonieux.\nFIG. 3 -Projection par Binary\nBlock GTM de la matrice textuelle 400 × 100 des données newsgroups.\nConclusion et perspectives\nNous avons proposé une carte auto-organisatrice probabiliste. Celle-ci est obtenue par l\u0027utilisation d\u0027un modèle de mélange de Bernoulli croisé et de l\u0027algorithme GTM. Notre méthode, appelée Binary Block GTM, est efficace et parcimonieuse. En effet, le nombre de paramètres inconnus est égal à h × m, ce qui est très peu comparativement à un modèle contraint de mélange de lois de Bernoulli (Girolami, 2001) ou une approche dyadique comme un pLSA binaire contraint (Priam et Nadif, 2006). Quelle que soit g la taille de la carte de projection, quel que soit n le nombre de lignes, le nombre de paramètres du modèle ne croît qu\u0027avec le nombre de classes en colonnes. En conclusion, le modèle présenté apparait clairement comme un excellent candidat pour s\u0027attaquer aux problèmes du data mining. Il serait intéressant d\u0027étendre cette approche au tableau de contingence en proposant un Block GTM adapté. \n"
  },
  {
    "id": "809",
    "text": "Introduction\nNautilus est un outil d\u0027analyse de bases de données qui facilite la préparation de données agrégées et formatées mises à disposition des outils de datamining et de restitution.\nNautilus est bâti autour d\u0027un système d\u0027abstraction des données couplé à un moteur de requêtes. L\u0027application modélise l\u0027environnement de données en construisant une pile de métadonnées, permettant de visualiser, documenter, et de manipuler des données du SGBD sous la forme de concepts métiers (produits, trafic, revenus, segment, etc.). Ce système de paramétrage est relié à un générateur de requêtes SQL et un gestionnaire des tâches conçus pour l\u0027agrégation de volumes importants de données. Nautilus permet ainsi de produire de manière rapide et fiable de grands volumes de données d\u0027analyse « à l\u0027intérieur » des SGBD, sans nécessiter d\u0027extractions de données. De ce fait, Nautilus remplace avantageusement les scripts et les datamarts métiers créés ad hoc tant pour les performances que la sécurité.\nNautilus, disponible depuis mai 2007, est développé sous la forme de plug-in Eclipse. La construction de jeux de données avec Nautilus se déroule en deux étapes : -Modélisation des données du SGBD ; -Construction des indicateurs La première étape consiste à modéliser les concepts métiers sur les données du SGBD. Nautilus se connecte à la base de données via JDBC et effectue les opérations de rétro-ingénierie du SGBD en important ses métadonnées.\nUne fois que le modèle de données est intégré, l\u0027utilisateur crée le dictionnaire des objets métier dont il se servira pour spécifier les indicateurs : par ex. la notion \"d\u0027appel entrant\".\nFIG. 1 -Edition d\u0027une règle métier dans le dictionnaire Nautilus.\nConstruction d\u0027indicateurs\nLa définition des éléments métier du dictionnaire permet de spécifier les champs à calculer. Ces calculs se font avec les fonctions natives du SGBD : agrégations, comptages, statistiques ainsi que les fonctions de datamining de plus en plus intégrées aux SGBD.\nNautilus génère automatiquement le SQL pour créer les indicateurs au sein du SGBD. Afin de répondre aux contraintes des départements informatiques de grandes entreprises, l\u0027application décompose les calculs en une série de tâches qui rendent plus lisibles les calculs et permettent d\u0027optimiser les traitements.\nL\u0027application maintient les paramètres des calculs effectués pour faciliter leur partage et leu réutilisation dans l\u0027entreprise.\nSummary\nNautilus is a database analysis software. The purpose of the application is to generalize the usage of customer data within the enterprise. It helps business users access data by mapping business concepts onto the database. Nautilus includes a metadata management module and an optimized SQL generator to compute large volumes of data. Nautilus is developped as Eclipse plug-ins.\n"
  },
  {
    "id": "811",
    "text": "Introduction\nLes systèmes de recherche d\u0027information préconisent une fonctionnalité très intéressante voire indispensable lors de tout processus de recherche : il s\u0027agit de la reformulation automatique de la requête. Cette fonctionnalité permet de rétablir les choix de l\u0027utilisateur dans la perspective de retrouver plus de documents qui répondent à son besoin en information. Il est à noter à ce niveau que le besoin en information de l\u0027utilisateur est très vague : l\u0027utilisateur ne sait en général pas ce qu\u0027il cherche. Par ailleurs, il peut tolérer un résultat initial imprécis sous réserve de l\u0027améliorer par feedback Rocchio (1971).\nFaire recours à de nouvelles méthodes d\u0027apprentissage est alors devenu une nécessité. Plusieurs modèles qui ont été auparavant délaissés, tels que la classification, sont repris en vu d\u0027améliorer l\u0027apprentissage en recherche d\u0027information. Nous proposons dans ce papier une méthode d\u0027apprentissage en faisant appel aux réseaux petits mondes (small worlds en anglais, Watts (1999)).\nNotre Approche\nLes propriétés des réseaux petits mondes paraissent intéressantes dans les problèmes de classification. D\u0027autant plus que ces propriétés sont valuées. Comme application à la recherche d\u0027information, nous présumons qu\u0027un ensemble de documents peut constituer des réseaux petits mondes pour moins qu\u0027ils parlent du même sujet, et qu\u0027une idée peut être transmise d\u0027un document à un autre document si les auteurs partagent le même intérêt.\nNos objectifs pour l\u0027intégration des small worlds en recherche d\u0027information ont deux effets : un effet de construction des small worlds par le bais de la classification; et un effet d\u0027estimation de pertinence sur d\u0027autres documents.\nEn partant de l\u0027hypothèse suivante : «une classe est raisonnable si elle admet certaines propriétés : celles des small worlds». Le premier effet va simplement faire une construction de small worlds de documents homogènes (pertinents ou non pertinents). Pour ce faire, nous proposons trois stratégies : une stratégie de construction de graphes de documents (1), une stratégie de propagation des liens (2), et une stratégie de construction des classes des documents (3). Pour la stratégie (3) nous utiliserons une méthode de classification hiérarchique, et l\u0027identification du nombre de classes dépend de la qualité de classification et de la nature de classes construites. A chaque itération nous calculons une valeur d\u0027inertie intra-classe qui permet de quantifier l\u0027homogénéité de la classification. Pour des classes réellement construites les coefficients de clustérisation et les distances moyennes montrent que les classes construites admettent les propriétés des small worlds.\nEn partant de l\u0027hypothèse suivante : «une classe est un small worlds, et qu\u0027une classe homogène (constituée de documents pertinents ou non pertinents) peut être utilisée comme moyen efficace pour bien constituer l\u0027estimation des scores d\u0027autres documents», le deuxième effet consiste à estimer la pertinence pour d\u0027autres documents. Pour traduire la pertinence pour un document il suffit d\u0027identifier la classe à laquelle il appartient et de juger de sa pertinence en fonction de la nature de la classe. Ce document est jugé pertinent si la classe résultat contient plus de documents pertinents que de documents non pertinents et est jugé non pertinent si non,\nConclusion\nNous avons présenté dans cet article une approche statistique de classification des documents. L\u0027approche consiste à définir un nouveau concept d\u0027apprentissage. L\u0027apprentissage consiste à construire des classes qui préservent les propriétés des réseaux petits mondes. Nous admettons que les classes préservant ces propriétés sont des estimateurs de pertinences d\u0027autres documents. L\u0027approche que nous avons proposée consiste à considérer tous les critères pouvant intervenir dans le jugement de l\u0027utilisateur et de leur affecter les meilleurs poids pour que la pertinence utilisateur soit proche de la pertinence système.\nLes poids des critères considérés sont ajustés par apprentissage. Chaque poids traduit l\u0027intérêt porté par l\u0027utilisateur à celui-ci. Les poids relatifs aux termes peuvent servir de moyen de construction de requête. Nous envisageons de tester l\u0027approche sur une base réelle de documents afin de mesurer l\u0027apport des réseaux petits mondes à la recherche d\u0027information. Nous envisageons également de tester la reformulation de la requête en se basant sur les poids des critères. Avec l\u0027effet petit monde, nous envisageons d\u0027autres méthodes telles que les méthodes d\u0027ordonnancement (ranking effect).\n"
  },
  {
    "id": "812",
    "text": "Introduction\nLe clustering consiste à découvrir automatiquement des groupes (\"clusters\") présents dans le jeu de données. Une littérature abondante existe sur le sujet (une revue des principales mé-thodes peut être trouvée dans Rui et Wunsch (2005)). Nous nous plaçons ici dans le cadre des \"cluster ensembles\" (Strehl et Ghosh (2002)). Les \"cluster ensembles\" sont une sorte de méta-clustering : à partir de plusieurs clusterings du même jeu de données, on déduit un clustering \"moyen\" (Strehl et Ghosh (2002)). Plusieurs alternatives ont été proposées pour trouver le clustering moyen (méthodes agglomératives, ou basées sur des graphes). Indépendamment de la méthode de synthèse choisie, il est clair que le clustering moyen dépend fortement de la qualité et de la diversité de chaque clustering individuel (Fern et Brodley (2003)). Par exemple, agréger plusieurs clusterings issus de l\u0027algorithme K-means avec des initialisations différentes atténuera les erreurs particulières dues à chaque clustering individuel ; cependant cela ne permettra pas de contourner les limitations fondamentales de l\u0027algorithme (clusters de forme sphé-rique, sensibilité à la dimension...). La situation idéale pour les cluster ensembles est celle où les clusterings individuels sont variés, de bonne qualité et obtenus à faible coût.\nL\u0027idée explorée par Topchy et al. (2003) est d\u0027obtenir ces clusterings individuels en projetant le jeu de données sur une direction aléatoire, et en faisant un clustering simple sur la projection (qui revient essentiellement à trouver les modes de la densité de la projection). L\u0027intuition est qu\u0027avec suffisamment de droites, les séparations entre clusters seront mises en évidence et permettront donc d\u0027obtenir un clustering moyen de bonne qualité. Dans cet article, nous proposons d\u0027améliorer cette méthode en construisant les clusterings individuels avec d\u0027autres projections, toujours à faible coût mais avec de meilleures performances.\nLe reste de cet article est structuré de la façon suivante : La section 2 explique notre approche, et décrit formellement les projections partielles des points, la recherche de modes et le clustering final. La section 3 compare les performances de la méthode avec celles d\u0027autres méthodes sur des jeux de données réels. La section 4 conclut et évoque des pistes de travaux futurs.\nCluster ensembles et projections\nProjections intéressantes pour le clustering\nLa question que nous nous posons est la suivante : comment trouver des projections intéressantes pour le clustering, qui soient variées et obtenues à faible coût ? On trouve dans la littérature de nombreux algorithmes de clustering qui utilisent des projections linéaires du jeu de données. Cela est le plus souvent justifié par la \"malédiction de la dimensionalité\" : quand la dimension augmente, les clusters deviennent épars (les points d\u0027un cluster sont moins concentrés) et les distances entre les points tendent à perdre leur signification. Les clusters sont également dans des sous-espaces de dimension assez basse par rapport à la dimension de l\u0027espace (Parsons et al. (2004)). Cette \"malédiction\" est un frein à l\u0027efficacité de plusieurs méthodes de clustering basées sur les distances et/ou la densité. L\u0027intérêt des projections est qu\u0027elles permettent de faire apparaître plus clairement les clusters. Traditionnellement, c\u0027est dans le domaine de la réduction dimensionnelle qu\u0027on a d\u0027abord cherché à trouver des directions \"intéressantes\" en fonction de certains critères (une direction est simplement une droite qui passe par l\u0027origine). La plus utilisée est sans doute l\u0027analyse en composantes principales (ACP), qui consiste à déterminer les k directions où le jeu de données présente la plus grande variance. La Projection Pursuit est une approche du même type, où un problème d\u0027optimisation est résolu pour trouver les directions qui maximisent un \"intérêt\" lié à l\u0027entropie. On peut aussi citer les projections aléatoires qui reviennent à projeter les données sur un ensemble de directions choisies uniformément sur la sphère unité.\nUne démarche répandue consiste à d\u0027abord réduire la dimension du jeu de données en projetant, puis d\u0027appliquer un algorithme de clustering quelconque sur cette représentation. Cependant, la difficulté de paramétrer la réduction dimensionnelle pour obtenir un bon clustering a conduit certains auteurs à intégrer cette réduction directement dans l\u0027algorithme de clustering. Par exemple, certains algorithmes de clustering divisif partitionnent récursivement le jeu de données en trouvant une direction \"intéressante\" et en coupant le jeu de données en deux par rapport à cette direction (Boley (1998) avec une ACP, Miasnikov et al. (2004) avec une Projection Pursuit). Les approches de Aggarwal et Yu (2000) et Ding et Li (2007) utilisent des ACP lors du clustering pour associer des sous-espaces \"intéressants\" à chaque cluster.\nDans l\u0027article de Topchy et al. (2003) cité dans l\u0027introduction, les droites de projection sont choisies au hasard et passent par l\u0027origine. Le problème est que lorsque la dimension augmente, la projection sur une droite aléatoire tend à avoir une densité gaussienne unimodale, donc inintéressante pour le clustering. Une amélioration qui vient immédiatement à l\u0027esprit est d\u0027utiliser par exemple une ACP, ou plusieurs ACP locales. Mais cette alternative est coûteuse : la complexité du calcul d\u0027une ACP est quadratique avec la dimension. La direction de plus forte variance n\u0027est d\u0027ailleurs pas forcément la meilleure pour séparer des clusters. Le même problème de complexité et de manque de garanties se pose pour les Projection Pursuit. De plus ces deux alternatives cherchent des directions, c\u0027est-à-dire essentiellement des droites passant par l\u0027origine. Il est probable cependant qu\u0027une direction intéressante pour le clustering à un endroit donné de l\u0027espace ne le soit pas à un autre.\nNotre approche\nFIG. 1 -Droites inter-points\nNous proposons deux idées simples pour obtenir des clusterings individuels de bonne qualité. La première est d\u0027utiliser des droites inter-points comme droites de projection. Si les deux points définissant la droite sont dans deux clusters différents, chacun dans son sous-espace, (comme c\u0027est le cas pour L 1?2 sur la Figure 1), les projections de ces deux clusters sur cette droite seront vraisemblablement bien séparés. Si en revanche les deux points définissant la droite sont dans le même sous-espace, comme L 1 ?1 par exemple, la droite pourra séparer des clusters appartenant au même sous-espace.\nFIG. 2 -Clusters, droite de projection et densité résultante\nLa deuxième idée est de ne pas projeter tout le jeu de données sur toutes les droites, afin de mieux faire ressortir les clusters. Considérons par exemple le cas de la Figure 1, où les clusters C 1 et C 2 sont dans deux sous-espaces différents. Si une droite appartient à un sousespace seulement, par exemple L 1?1 , il n\u0027est pas vraiment judicieux de projeter les points de C 2 dessus. Plus généralement, seuls les points proches de la droite (au sens de la projection orthogonale) sont intéressants pour une droite donnée. Ceci est illustré sur la Figure 2 : si on ne projette que les points des clusters C 1 et C 2 , la densité résultante permet de bien les séparer, alors que si on projette tous les points, la densité résultante (en pointillés) est unimodale et ne sépare plus rien.\nDans la suite, on note X \u003d {x 1 , . . . , x N } ? R d le jeu de données. La sphère unité est notée S d . Le produit scalaire est noté ··. Le cardinal d\u0027un ensemble E est noté |E|.\nProjections partielles\nDroites de projection Nous choisissons au hasard M droites inter-points comme droites de projection. Chaque droite\nLa distance d\u0027un point x à la droite D k est la distance orthogonale du point à la droite. On peut décomposer le vecteur Ox comme étant la somme vectorielle (\nProjections partielles Pour éviter les effets évoqués dans la sous-section 2.1, on ne projette réellement un point x ? X que sur les droites les plus proches de lui. Pour cela, on calcule les distances entre chaque point et les droites. On sélectionne pour x les m droites les plus proches de lui en triant ces distances par ordre croissant. A la fin de ce traitement, chaque droite D k est donc associée à un certain ensemble de points X k . On note X k la projection de X k sur D k .\nRecherche de modes\nPour produire un clustering individuel à partir de X k , nous devons identifier les modes de la densité de X k . Pour effectuer cet étape, nous utilisons un estimateur de densité à noyaux :\nk . La seconde étape consiste à identifier les modes de cette densité. Pour cela, nous devons identifier les minimums locaux dê f . Un minimum local est un point où la dérivéê f (t) est nulle et la dérivée secondê f (t) est positive. En pratique, il est suffisant de parcourir la droite et d\u0027identifier les zones où la dérivée devient tour à tour négative, nulle puis positive. Nous faisons un parcours linéaire de la densité (en discrétisant avec un pas ?) des valeurs dê f . La dérivée en un point y \u003d i · ? est approchée parˆf\n. L\u0027Algorithme 1 décrit en pseudocode la recherche de modes.\nAlgorithme 1 : Recherche de modes de la densité de X k\nClustering final\nAprès les étapes précédentes, on a identifié les modes sur chacune des droites D 1 , . . . , D M . On rassemble cette information de façon synthétique dans une matrice X modes (de N lignes et M colonnes) telle que\noù mode k (x i ) est le numéro du mode contenant la projection de x sur D k . La i-ème ligne de X modes indique donc dans quels modes et sur quelles droites se projette x i ? X. Pour terminer le clustering, nous devons maintenant effectuer la synthèse des clusterings individuels. Nous avons choisi la méthode la plus simple : on effectue un clustering agglomé-ratif de type average-link (Duda et al. (2000)) sur les lignes de la matrice X modes . La mesure de distance entre deux lignes doit refléter l\u0027intuition suivante : deux points sont \"proches\" si ils sont projetés dans les mêmes modes sur les mêmes droites. En comparant deux lignes, nous devons donc ignorer les colonnes où les deux lignes ont des composantes nulles, c\u0027est-à-dire les droites où aucun des deux points ne sont projetés. Sur chacune des droites restantes, si les deux points sont dans des modes différents, cela contribue à les éloigner. Cela nous amène naturellement à la distance de Jaccard qui est simplement la proportion de coordonnées non nulles qui diffèrent : Il arrive bien souvent, en particulier avec des données réelles, que les clusters trouvés par l\u0027algorithme contiennent des points venant de classes différentes. Un premier critère classique pour évaluer un clustering est la pureté. On la calcule de la façon suivante : on étiquette chaque cluster C ? Z par la classe dominante qu\u0027il contient cldom\nComplexité\nLa pureté PUR Y (Z) de tout le clustering Z est simplement la moyenne pondérée par la taille des clusters\nLe second critère est l\u0027information mutuelle normalisée (IMN) (voir Strehl et Ghosh (2002)\navec H(·) dénotant l\u0027entropie discrète. Ce critère est au maximum de 1 quand les deux étique-tages sont parfaitement semblables. Si chaque classe de Y a un cluster correspondant dans Z, à part l\u0027une d\u0027elles divisée en deux clusters dans Z, alors l\u0027IMN sera plus petite que 1. Ainsi, le critère pénalise un clustering qui n\u0027a pas la même structure que la \"vraie\" information de classe. Ceci permet de juger à quel point les deux étiquetages ont la même structure.\nJeux de données\nNous avons pris trois jeux de données répandus pour l\u0027évaluation des algorithmes. Le premier est CHART 1 : il s\u0027agit de séries temporelles de contrôle. Il comporte N \u003d 600 exemples en dimension d \u003d 60. Il contient c \u003d 6 sortes de séries temporelles différentes. Le second est \nAlgorithmes et paramètres\nPour chacun des jeux de données, nous avons évalué la performance de quatre méthodes : -CLIP (Clustering par Lignes Inter-Points) : notre méthode ; -RP : la méthode de Topchy et al. (2003) ; -ACP+EM : mixture de gaussiennes, précédée d\u0027une ACP gardant 85% de la variance des données ; -LAC : l\u0027algorithme de subspace clustering de Domeniconi et al. (2007) qui est un Kmeans qui pondère les dimensions pour localiser le sous-espace particulier de chaque cluster. Pour chaque méthode, nous avons effectué le clustering en paramétrant un nombre de clusters k variable. Cela permet de mieux se rendre compte de l\u0027efficacité de l\u0027algorithme (voir Fern et Brodley (2003)), sachant qu\u0027il est rare que le nombre de classes dans le jeu de données corresponde au nombre de clusters du point de vue géométrique du terme. Chaque expérience, pour un algorithme et un jeu de données fixés, a été effectuée 10 fois (avec des graines aléa-toires différentes pour CLIP et RP, et des initialisations différentes pour ACP+EM et LAC). Les tables de résultats affichent la moyenne et les écarts-types de la performance selon les deux critères de Pureté et d\u0027Information Mutuelle Normalisée décrits plus haut. Les résultats de notre méthode CLIP ont été obtenus avec les paramètres suivants : pour CHART, nous avons pris M \u003d 100 droites et projeté chaque point sur ses m \u003d 10 droites les plus proches. Pour USPS, on a pris M \u003d 400 et m \u003d 10. Pour Coil20, on a pris M \u003d 500 et m \u003d 10. On a évalué la méthode RP avec le même nombre M de droites que CLIP.\nRésultats\nPour chacun des jeux de données, les expériences montrent que notre approche CLIP donne des résultats sensiblement supérieurs aux autres approches. Ces bons résultats sont assez stables pour un nombre de clusters k différents, ce qui confirme la robustesse de la méthode. Les mauvais résultats de RP montrent quant à eux que des droites choisies complètement aléa-toirement et indépendamment des données ne permettent pas d\u0027obtenir des clusterings individuels intéressants. (Nous avons également testé RP avec des droites aléatoires non contraintes à passer par l\u0027origine, ce qui a donné des résultats légèrement moins bons que ceux de RP). L\u0027algorithme classique de clustering par mixture de gaussiennes (précédée d\u0027une ACP) est le deuxième meilleur après notre méthode. L\u0027algorithme LAC vient ensuite. Les Figures 3, 4 et 5 illustrent graphiquement un clustering obtenu avec notre méthode pour chaque jeu de données.\nConclusion et perspectives\nNous avons proposé une nouvelle méthode dans le cadre des clustering ensembles. Chaque clustering individuel est réalisé à partir d\u0027une projection partielle du jeu de données sur une droite reliant deux points de données pris au hasard. Nous avons évalué et comparé la méthode avec d\u0027autres approches ; les expériences montrent que notre méthode donne de meilleurs ré-sultats. Les perspectives de travail futur portent sur deux aspects : le mécanisme de sélection des droites de projection pour chaque point, et le clustering global. Pour qu\u0027un point influence les droites les plus proches de lui, on peut imaginer d\u0027utiliser tous les points pour chaque droite, mais en pondérant l\u0027influence de chaque point dans l\u0027estimateur de densité en fonction de sa distance à la droite. Cela permettrait de s\u0027affranchir du calcul des distances entre points et droites. Quant à la performance du clustering global, elle est très probablement due à la nouvelle représentation des données et non pas à la nature agglomérative de l\u0027algorithme de synthèse des clusterings individuels. Une variante de K-means avec la distance de Jaccard devrait donner des résultats similaires pour une complexité moindre. \n"
  },
  {
    "id": "813",
    "text": "Introduction\nObtenir un clustering efficace et de haute qualité sur des données de grande taille est un problème majeur pour l\u0027extraction des connaissances. Il existe une demande de plus en plus importante pour des techniques flexibles et efficaces de clustering capables de s\u0027adapter à des jeux de données de structure complexe. Un ensemble de données est typiquement représenté dans un tableau composé de N items (lignes) et d dimensions (colonnes). Un item représente un événement ou une observation, alors qu\u0027une dimension peut-être un attribut ou une caractéristique de l\u0027item. Dans un mode semi-supervisé ou supervisé, une partie ou tous les items peuvent être annotés par une classe. Les méthodes de clustering tentent de partitionner les items en groupes avec une mesure de similarité. Un ensemble de données peut être grand en termes de nombre de dimensions, nombre d\u0027éléments, ou les deux.\nL\u0027approche classique est basée sur des algorithmes de clustering, comme les K-moyennes, le clustering spectral ou hiérarchique ainsi que leurs multiples variantes (Hastie et al., 2001). Il existe cependant plusieurs inconvénients connus à ces méthodes. Premièrement, il n\u0027est pas toujours facile de déterminer, visualiser et valider les clusters de forme irrégulière. Plusieurs algorithmes sont efficaces pour trouver des clusters dans des formes elliptiques (donc convenant aux distributions normales multidimensionnelles), mais peuvent échouer à reconnaître des clusters de forme complexe. Deuxièmement, les algorithmes existant sont automatiques, ils excluent toute intervention de l\u0027utilisateur dans le processus jusqu\u0027à la fin de l\u0027algorithme.\nIl n\u0027y a aucune manière commode d\u0027incorporer la connaissance du domaine au sein de la phase d\u0027analyse ou de permettre à l\u0027utilisateur d\u0027orienter un processus de clustering lorsque l\u0027on utilise des algorithmes automatisés. Typiquement, l\u0027analyse de cluster continue après la fin de l\u0027algorithme, jusqu\u0027à ce que les utilisateurs soient satisfaits et acceptent le résultat. Cependant, lorsque le résultat n\u0027est pas satisfaisant, les utilisateurs veulent être étroitement impliqués dans le processus itératif de clustering et d\u0027évaluation, en fournissant leurs impressions et intuitions.\nUne alternative aux méthodes automatiques de clustering est le clustering visuel interactif (Chen et Liu, 2004;Kandogan, 2001;Seo et Shneiderman, 2002). Ici les clusters sont visualisés sur un plan 2D ou un espace 3D ; cependant, la réduction de dimension nécessaire à la visualisation n\u0027est pas effectuée en sélectionnant les dimensions \"les plus importantes\", mais par le principe du \"sweeping\" (Kandogan, 2001). Selon ce principe, les n dimensions sont représentées par n axes disposés sur le plan 2D ou l\u0027espace 3D. Les coordonnées cartésiennes de chaque point de données étant alors définies en fonction de la direction et de la longueur de chaque axe. Un exemple récent de clustering visuel est le système iVIBRATE (Chen et Liu, 2006) basé sur le principe des coordonnées en étoiles. Son composant principal est le rendu visuel du clustering qui aide l\u0027utilisateur dans le processus itératif de clustering au travers d\u0027une visualisation interactive. Il permet de produire des solutions guidées par la visualisation pour traiter efficacement les clusters de formes irrégulières. Les résultats montrent que iVIBRATE peut réellement impliquer l\u0027utilisateur dans le processus de clustering et générer des résultats de haute qualité sur de grands ensembles de données (Chen et Liu, 2006). À la différence des méthodes automatiques, le clustering itératif dans iVIBRATE est accompli essentiellement par la manipulation des paramètres (longueur et direction des axes). Cependant le clustering manuel devient difficilement possible lorsque que les données sont de grande dimension.\nDans ce papier, nous proposons de combiner les avantages des deux approches ci-dessus, l\u0027analyse avancée des données des méthodes de clustering automatique et la flexibilité et l\u0027interactivité du clustering visuel. Notre idée principale est l\u0027apprentissage semi-supervisé d\u0027une métrique de distance permettant une projection optimale pour les systèmes de visualisation en coordonnées en étoiles. De plus, nous avons étendu le principe des coordonnées en étoile 2D en 3D grâce à une disposition sphérique des axes. Cette extension améliore grandement le rendu visuel et facilite donc le clustering.\nDans la section suivante, nous présentons le principe des coordonnées en étoiles 2D et 3D. Puis, nous décrivons l\u0027interface Semi-Supervised Visual Clustering qui enrichie le clustering visuel de l\u0027apprentissage semi-supervisé de la métrique de distance optimale. Nous décrivons trois modes possibles disponibles dans l\u0027interface : manuel, automatique et hybride. Les modes manuels et automatiques sont inspirés des méthodes automatiques et du clustering visuel interactif mentionnés précédemment. Nous présentons aussi une approche hybride dans laquelle certains paramètres sont manuellement fixés par l\u0027utilisateur tandis que les paramètres restant sont déterminés par un algorithme de distance optimale, utilisant l\u0027ensemble des retours de l\u0027utilisateur. Nous concluons avec une évaluation de la métrique optimale sur la collection standard UCI. données ne sont pas nécessairement orthogonaux entre eux. La valeur minimale d\u0027une donnée pour une dimension est tracée à l\u0027origine, et la valeur maximale est tracée à l\u0027extrémité de cet axe. Ainsi les vecteurs unités de chacun des axes sont calculés de manière à permettre la correspondance des valeurs des données à la longueur des axes. Les systèmes de visualisation de clustering visuel (Chen et Liu, 2006;Kandogan, 2001;Seo et Shneiderman, 2002) fournissent un certain nombre de dispositifs d\u0027interaction, dont les utilisateurs peuvent se servir pour amé-liorer leur compréhension des données. Les fonctionnalités de bases du clustering visuel sont les suivantes : Redimensionnement Le redimensionnement permet à des utilisateurs de changer la longueur d\u0027un axe, en augmentant ou diminuant la contribution d\u0027un attribut particulier sur la visualisation résultante. Rotation La transformation de rotation modifie la direction du vecteur unité d\u0027un axe, rendant un attribut particulier plus ou moins corrélé avec d\u0027autres attributs (voir figure 1). Lorsque plusieurs axes sont tournés dans une même direction, leurs contributions sont agrégées dans la visualisation. Annotation Les utilisateurs peuvent annoter des points soit par la sélection individuelle de points, soit par la sélection d\u0027un ensemble de points au moyen d\u0027une enveloppe convexe (voir figure 2). La couleur des points change selon l\u0027annotation, ce qui permet de plus facilement les suivre dans la suite des transformations. Point de vue En 3D, les utilisateurs peuvent changer de point de vue et zoomer sur des données pour trouver une meilleure visualisation des clusters. Dans la suite, nous ferons principalement référence à l\u0027extension 3D des coordonnées en étoiles.  \nQ(x, y, z) est déterminé par la moyenne du vecteur somme des d vecteurs sc i · x i , où sc i sont les coordonnées sphériques qui représentent les d dimensions dans l\u0027espace visuel 3D. Selon la transformation A, la projection 3D d\u0027un point Q(x, y, z) est déterminée par :\n, est issu des paramètres ajustables de redimensionnement, pour chacune des d dimensions. Ces ? i sont initialement fixés à 1. Les paramètres de rotation ? i et ? i sont initialement fixés à 2i?/d et peuvent être ajustés par la suite. Le point o\u003d(x 0 , y 0 , z 0 ) fait référence au centre de l\u0027espace de visualisation. La transformation A est une transformation linéaire avec un ensemble fixé de valeurs ?, ?, ?. Si nous fixons le centre o la transformation A ?,?,? (x 1 , . . . , x d ) peut être représentée par la transformation M x T , dans laquelle :\nElle ne casse pas les clusters dans la visualisation. Ceci étant, l\u0027écart visuel entre des nuages de points reflète un réel écart entre les clusters dans l\u0027espace original de grande dimension. Néanmoins, celle-ci peut-être la cause d\u0027un chevauchement de clusters (Chen et Liu, 2006). La séparation de clusters imbriqués peut être accomplie grâce à la visualisation dynamique, au travers de la manipulation interactive. La transformation est ajustable par le redimensionnement des ? i et la rotation des ? i et ? i . Par la manipulation de ces paramètres, l\u0027utilisateur peut voir l\u0027influence de la ième dimension sur la distribution des clusters au travers d\u0027une série de changements de vue, lesquels sont d\u0027importants indices pour le clustering. La figure 1 montre l\u0027exemple du changement de forme d\u0027un cluster par la rotation d\u0027un axe (en gras).\nLes dimensions importantes pour le clustering vont être la cause de changements importants dans la visualisation puisque les valeurs des paramètres correspondant sont changées de manière continue. Les axes sont disposés autour du centre d\u0027affichage et les objets graphiques sont conçus pour ajuster interactivement chaque paramètre. Malheureusement, la conception visuelle limite tout de même le nombre de dimensions qui peuvent être visualisées et manipulées. La visualisation dans des systèmes en coordonnées en étoiles comme iVIBRATE, permettent aux utilisateurs de manipuler facilement jusqu\u0027à une cinquantaine de dimensions.\nClustering Visuel Interactif Semi-Supervisé\nLorsque les données deviennent trop difficiles à manipuler, nous proposons d\u0027améliorer le clustering visuel interactif par l\u0027automatisation de certaines sous-tâches, si l\u0027utilisateur pense que cela peut l\u0027aider. Nous avons développé l\u0027interface Semi-Supervised Visual Clustering (SSVC) laquelle implémente à la fois les coordonnées en étoiles et leur extension sphérique pour une visualisation sur un plan 2D ou un espace 3D. L\u0027interface SSVC offre un processus de clustering interactif au travers de la manipulation de paramètres et un processus d\u0027optimisation basé sur le rendu visuel 2D ou 3D. Le système itère jusqu\u0027à ce que les utilisateurs soient satisfaits. Les principaux composants et réglages du processus sont discutés ci-dessous en détails.\nSélection de dimensions. Si l\u0027ensemble de données contient trop de dimensions, les utilisateurs peuvent vouloir préliminairement écarter les moins pertinentes. Le nombre de dimensions peut en effet influencer la facilité de manipulation des paramètres et réduire la complexité des algorithmes. Ci-dessous nous considérons trois options pour la sélection d\u0027attributs :\nManuel. L\u0027utilisateur peut sélectionner ou désélectionner manuellement une ou plusieurs dimensions. Mode non supervisé. L\u0027analyse en composantes principales (ACP) (Hastie et al., 2001) peut être utilisée pour réduire la dimension des données par une approche linéaire. L\u0027ACP ordonne les dimensions selon les valeurs propres de la matrice de covariance et permet de sélectionner une par une les d \u003c d meilleures dimensions.\nMode semi-supervisé. Si une partie des données est déjà annotée, l\u0027analyse discriminante de Fisher peut être utilisée pour réduire les dimensions par une approche linéaire. L\u0027interaction basée sur l\u0027entropie entre les attributs et les classes d\u0027annotation (Yu et Liu, 2004) peut être une alternative non-linéaire, pour ordonner les dimensions et sélectionner les d \u003c d meilleurs.\nManipulation et optimisation. Le processus de clustering interactif couvre essentiellement trois actions différentes. Le mode manuel est analogue aux systèmes de visualisation existant, lorsque l\u0027utilisateur ajuste manuellement les valeurs ?, ? et ? pour modifier les clusters visibles. Le mode automatique fait référence au cas où les paramètres ?, ?, ? sont déterminés selon la métrique de distance optimale apprise dans le mode semi-supervisé à partir des annotations disponibles et des retours de l\u0027utilisateur. Le mode hybride fait référence à divers cas intermédiaires lorsque l\u0027utilisateur fixe quelques ? i , ? i , ? i et exige que les paramètres restant soient déterminés par une métrique de distance optimale.\nClustering semi-supervisé\nLe clustering semi-supervisé suppose qu\u0027une petite quantité de données annotées est disponible pour un meilleur clustering. Ces données souvent proviennent des retours de l\u0027utilisateur, soit sous la forme d\u0027annotations directes d\u0027un item par une classe, soit d\u0027indication \"plus lé-gères\" sur la similitude ou la dissimilitude de paires d\u0027éléments. En utilisant ces indices, un meilleur clustering peut être réalisé par l\u0027ajustement de la métrique de distance (Basu et al., 2004;Tang et al., 2007;Xing et al., 2003). Cette ajustement cherche à obtenir la vue utilisateur dans laquelle les items sont mis ensemble ou séparément. La représentation des données d\u0027origine ne peut pas être incluse dans un espace où les clusters ne sont pas suffisamment sépa-rés. Modifier la métrique de distance transforme cette représentation de sorte que les distances entre des éléments de même cluster sont réduites au minimum, alors que les distances entre des éléments de clusters différents sont maximisées.\nCependant, à la différence de Basu et al. (2004) qui apprend la métrique de distance dans l\u0027espace d-dimensionnel original, nous visons ici l\u0027espace visualisé en CE3D. L\u0027utilisateur peut alors exprimer au mieux ses impressions et intuitions car ce n\u0027est pas l\u0027espace d\u0027origine qui est optimisé mais la vue qu\u0027il en a. Nous unifions donc ainsi dans un même système le clustering semi-supervisé et le clustering visuel 3D. En d\u0027autres termes, nous recherchons une métrique de distance de projection optimale donnée par la matrice M en (2). Dans la matrice, la projection 3D du point x avec la projection M est donnée par Q(x, y, z) \u003d M x. Ci-dessous, nous considérons plusieurs alternatives pour la modélisation et l\u0027évaluation de la métrique de distance de projection optimale.\nCoordonnées sphérique vs analyse factorielle discriminante\nSi un sous ensemble d\u0027éléments est annoté, les coordonnées sphériques des axes pour la métrique de distance de projection optimale M peuvent-être obtenues par l\u0027utilisation de l\u0027analyse factorielle discriminante (Bishop, 1995) lesquelles sont une généralisation de l\u0027analyse discriminante de Fisher à plus de 2 classes et 1 dimension projetée. Selon Bishop (1995), nous obtenons les variables discriminantes pour la projection 3D comme suit :\n-Pour chaque classe nous formons la matrice de covariance V k et sa moyenne µ k . Puis nous définissons les poids de la matrice de covariance V \u003d c k\u003d1 N k V k , où N k est le nombre d\u0027éléments dans la classe k, et c est le nombre totale de classes. -En utilisant la moyenne µ de l\u0027ensemble des données et µ k , la moyenne pour chacune des classes k, nous formons la matrice\nT . -Pour projeter dans l\u0027espace 3D, nous construisons la matrice de projection optimale W 3 avec les 3 premiers vecteurs propres de V ?1 V B . Notons qu\u0027il peut être coûteux sur un ensemble de grande dimension d\u0027obtenir des vecteurs propres pour V ?1 V B .\nUne fois la matrice de projection W 3 \u003d {w ij }, i \u003d 1, 2, 3, j \u003d 1, . . . , d obtenue, nous résolvons la matrice équation M T \u003d W 3 par la décomposition :\nSi w i1 , w i2 , w i3 ne sont pas tous égaux à zéro, alors il existe une solution unique pour ? i , ? i et ? i dans (3). La solution est analogue pour la conversion des coordonnées cartésiennes aux coordonnées sphériques :\n, ? i \u003d arctan wi3 . En d\u0027autres termes, si la matrice de projection W 3 n\u0027a pas de colonne 0, alors il existe un ensemble unique de ?, ? et ? pour la visualisation en CE3D.\nApprendre la métrique de distance de projection\nL\u0027analyse discriminante de Fisher à partir des matrices de covariance, présentées précé-demment, fait l\u0027hypothèse implicite que la distribution des données est multinomiale. Dans cette section, nous suivons (Xing et al., 2003;Basu et al., 2004) en ne faisant aucune hypothèse spécifique sur la distribution des données. La métrique de distance est obtenue à partir de l\u0027ensemble des paires de similitude/dissimilitude par l\u0027optimisation d\u0027une fonction qui cherche à réduire les distances des éléments semblables tout en augmentant les distances entre éléments différents. Cependant, à la différence de (Xing et al., 2003;Basu et al., 2004), nous recherchons une métrique de projection de distance M qui sépare les données dans la projection 3D plutôt que dans l\u0027espace initial. Nous définissons la métrique de distance de projection M comme la distance entre deux éléments x 1 et x 2 dans l\u0027espace 3D projeté :\nNous supposons que nous disposons d\u0027un ensemble S de pairs de similarité ou de dissimilarité D. Si nous possédons les classes d\u0027annotation pour quelques items, alors les items ayant la même classe forme l\u0027ensemble de similarité S et les items ayant des classes différentes forme l\u0027ensemble de dissimilarité D.\nProblème d\u0027optimisation pour la métrique de distance de projection\nTrouver la métrique de distance de projection peut être formulé comme un problème d\u0027optimisation non contrainte. Nous avons deux termes sur les ensembles S et D. Ces deux termes sont le terme intra-groupe In(M ) pour toutes les paires de similitude données par l\u0027ensemble S et le terme extra-groupe Ex(M ) pour toutes les paires de dissimilarité données par D :\nNous considérons ensuite une fonction d\u0027optimisation J(M ) qui cherche à diminuer In(M ) et augmenter Ex(M ). Nous prenons en considération le fait que les ensembles S et D peuvent être de tailles différentes, en particulier dans le cas de l\u0027annotation multi-classes. Ci-dessous nous considérons 4 variantes de la fonction d\u0027optimisation J(M ) :\nDérivées partielles de J(M ). In(M ) et Ex(M ) ont tous deux des dérivées partielles. Pour développer les dérivées partielles pour le terme intra-groupe In(M ), nous représentons la matrice M comme M \u003d {m kl }, k \u003d 1, 2, 3, l \u003d 1, . . . , d. Puis nous réécrivons In(M ) comme ci-dessous :\nPour obtenir les dérivées, nous utilisons la symétrie des valeurs de ? lk ? S S ln \u003d ? nl , n, l \u003d 1, . . . , d. Nous avons :\nn\u003d1\nLes dérivées partielles pour Ex(M ) dans toutes les variantes de J(M ) sont développées de manière similaire. Nous pouvons ensuite utiliser la méthode de descente du gradient ou bien une de ses variantes pour trouver la valeur du minimum (local) J(M ) (Basu et al., 2004).\nOptimisation basée sur les contraintes\nL\u0027optimisation non contrainte présentée dans la sous-section précédente semble être sensible au déséquilibre de taille des ensembles S et D. Pour remédier à ce problème, nous allons adapter l\u0027optimisation basée sur les contraintes proposée dans Basu et al. (2004) au CE3D. Dans cette approche, la fonction J(M ) tente de minimiser le terme intra-groupe In(M ) tout en gardant le terme extra-groupe supérieur à 1 afin d\u0027éviter la solution triviale M \u003d 0 lorsque toutes les variables tombent à 0.\nLe choix de la constante 1 est arbitraire mais n\u0027a pas de réelle importance. La changer pour n\u0027importe qu\u0027elle constante m conduit à remplacer M par ? mM . En final, nous considérons une solution alternative, spécifique aux CE3D. Dans cette alternative, la fonction d\u0027optimisation reste la même que dans (5). Néanmoins afin d\u0027eviter les chevauchements de clusters, nous ajoutons des contraintes pour exclure les distributions de ? i qui tendent à ramener tous les axes vers une direction unique, d\u0027où :\nOù th est un seuil, 0 \u003c th \u003c 1.\nApprentissage partiel de la métrique de distance\nLes problèmes d\u0027optimisation discutés dans les sections précédentes font référence au mode entièrement automatique du clustering visuel interactif. Le dernier mode, hybride fait référence aux cas où les paramètres sont partiellement définis, c\u0027est à dire lorsque quelques (? i , ? i ,? i ) sont fixés par l\u0027utilisateur, et que l\u0027optimisation de la métrique de distance de projection M est limitée aux axes libres. La mise en place de l\u0027apprentissage partiel est directe. Dans toutes les méthodes d\u0027optimisation, tous les (? i , ? i , ? i ) sélectionnés sont fixés et l\u0027optimisation est alors réalisée sur l\u0027ensemble des variables laissées libres.\nLa figure 3 montre un exemple de clustering visuel pour l\u0027ensemble de données segment issu de la collection UCI. La figure 3.a montre le rendu visuel initial en CE3D. La figure 3.b est le résultat d\u0027une réduction de dimensions réalisée grâce à l\u0027analyse discriminante de 21 à 4 dimensions, suivie par l\u0027application de la métrique de distance obtenue par l\u0027optimisation de (9).  Il n\u0027est pas surprenant de voir qu\u0027en moyenne, les méthodes basées sur l\u0027analyse discriminante se comportent mieux. Pour expliquer ce phénomène, nous avons étudié de plus près la collection elle-même ainsi que la fonction d\u0027évaluation. En effet toutes les deux sont plus adaptées aux méthodes de clustering automatique : les groupes d\u0027éléments dans les données ont une forme elliptique et la fonction d\u0027évaluation favorise les résultats de clustering basés sur les centroïdes. Pour rendre l\u0027évaluation plus correcte, nous recherchons actuellement des fonctions d\u0027évaluation alternatives, soit directement définies sur les CE3D, soit personnalisées à partir de métriques génériques pour les contraintes sur les paires (Liu et al., 2007). Aussi nous cherchons à compléter notre évaluation sur une collection de documents techniques, qui est une tâche plus ambitieuse à cause de sa grande taille et de la forme complexe de ses clusters.\nConclusion\nNous avons décrit un système interactif dans lequel nous associons le clustering visuel dans un système 3D en coordonnées en étoiles avec le clustering semi-supervisé basé sur l\u0027apprentissage d\u0027une métrique de distance optimale à partir des retours de l\u0027utilisateur. Le processus de clustering est guidé par l\u0027utilisateur de façon intuitive et flexible ; ce dernier peut soit accomplir le clustering des données manuellement, soit le déléguer au système par l\u0027annotation d\u0027éléments ou l\u0027indication de paires de similarité/dissimilarité. Différentes méthodes d\u0027optimisation ont été comparées et une interface a été développée pour des tests en situation réelle. Il s\u0027avère qu\u0027associer le clustering visuel et les algorithmes automatiques offre un mécanisme puissant pour l\u0027analyse intelligente des données complexes ou de grandes tailles.\n"
  },
  {
    "id": "814",
    "text": "Introduction\nDans de nombreux domaines applicatifs, l\u0027analyste se trouve devant des jeux de données matriciels dans lesquels un certain nombre d\u0027objets sont décrits par un certain nombre d\u0027attributs qui prennent leurs valeurs dans un domaine numérique, éventuellement restreint au domaine 0/1. L\u0027une des techniques phares pour l\u0027étude exploratoire de tels jeux de données est la classification, i.e., le calcul de partitions, soit sur l\u0027ensemble des objets, soit sur l\u0027ensemble des attributs. On peut aussi vouloir faciliter l\u0027interprétation des groupements calculés en dé-veloppant des méthodes de co-classification. Dans ce cas, les partionnements selon les deux dimensions sont couplés et les algorithmes comme ceux présentés dans Robardet et Feschet (2001); Dhillon et al. (2003); Ritschard et Zighed (2003); Jollois et al. (2003) produisent une bi-partition, i.e., une collection de co-clusters. Chacun des co-clusters est un groupe d\u0027objets associé à un groupe d\u0027attributs et la co-classification apparaît comme une méthode de classification conceptuelle. La co-classification a été particulièrement étudiée dans le contexte de l\u0027analyse du transcriptome (voir, e.g., Cheng et Church (2000); Madeira et Oliveira (2004)). En effet, les technologies à haut débit permettent de construire des matrices d\u0027expression de (tous les) gènes d\u0027un organisme dans différentes situations expérimentales. Dans ce type de matrices Expériences × Gènes, un co-cluster apparaît comme un ensemble de gènes ayant des profils d\u0027expression similaires dans un ensemble de situations expérimentales. Il est alors possible de considérer chaque co-cluster comme un module de transcription putatif, i.e., une hypothèse sur un mécanisme de régulation génique et donc une réponse (partielle) au but de l\u0027analyse du transcriptome qui a motivé la collecte des données d\u0027expression.\nNous nous intéressons à la pertinence des bi-partitions. Il nous semble important que les analystes puissent spécifier leurs attentes (intérêt subjectif dérivé de la connaissance du domaine) au moyen de contraintes et que les techniques de co-classification puissent produire des résultats cohérents vis-à-vis de ces spécifications. Une co-classification est alors vue comme le calcul de {? ? L | q(r, ?) est vrai} où r est une matrice, L désigne le language des bipartitions sur r, et le prédicat q spécifie des propriétés attendues sur ?. Une vision classique est que q va exprimer une contrainte d\u0027optimisation sur une fonction objectif, e.g., la perte d\u0027information mutuelle dans Dhillon et al. (2003). On peut également trouver d\u0027autres contraintes comme la définition du nombre de co-clusters, le fait que certains objets (resp. attributs) doivent (resp. ne doivent pas) être ensembles (i.e., des contraintes habituellement désignées sous les noms de \"must-link\" et \"cannot-link\"). Combiner les méthodes d\u0027optimisation sur la fonction objectif avec la satisfaction des autres contraintes est clairement difficile. L\u0027introduction des contraintes comme \"must-link\" et \"cannot-link\" dans des processus de classification monodimensionnelle (e.g., K-Means, classification hiérarchique) a motivé de nombreux travaux ces 5 dernières années (Wagstaff et al., 2001;Klein et al., 2002;Bilenko et al., 2004;Davidson et Ravi, 2005a,b). Par contre, l\u0027exploitation de contraintes pour la co-classification n\u0027a été que très peu étudiée. Dans Pensa et al. (2006a,b), nous introduisions certains types de contraintes pour la co-classification. À coté des extensions de \"must-link\" et \"cannot-link\" pour qu\u0027elles s\u0027appliquent aux objets et/ou aux attributs, nous avons considéré le cas des dimensions ordonnées. Par exemple, dans le contexte de l\u0027analyse de données d\u0027expression, nous pouvons enregistrer l\u0027évolution de l\u0027expression des gènes au cours du temps. On peut alors spécifier des contraintes de contiguité (contrainte \"Interval\"). Dans nos travaux antérieurs, le calcul des bi-partitions était traité comme un post-traitement de collections de motifs qui capturent des associations localement fortes. Dans ce contexte, l\u0027idée était de pouvoir \"pousser\" certaines contraintes sur la bi-partition jusque dans l\u0027extraction des motifs locaux à post-traiter.\nNous présentons ici une approche de co-classification sous contraintes originale et alternative aux propositions décrites dans Pensa et al. (2006a,b). Ces dernières étaient réservées au traitement de données 0/1. Ici, nous travaillons sur des données numériques et sans passer par des collections de motifs locaux. Pour celà\" nous étendons la technique de minimisation des résidus quadratiques proposée dans Cho et al. (2004) à un cadre de classification sous contraintes. La Section 2 formalise le problème. La Section 3 présente notre algorithme de coclassification sous contraintes. La Section 4 est dédiée à une validation expérimentale sur deux jeux de données réelles en analyse du transcriptome. La Section 5 est une brève conclusion. colonne j. Par exemple, x ij peut contenir le niveau d\u0027expression du gène i dans la condition expérimentale j. Nous noterons x i. et x .j les vecteurs associés respectivement à la ligne i et à la colonne j. Une co-classification C k×l sur X produit simultanément un ensemble de k × l co-clusters (une partition C r en k groupes de lignes associée à une partition C c en l groupes de colonnes). Soit I r l\u0027ensemble des indices des lignes appartenant à la classe de lignes r, et J c l\u0027ensemble des indices des colonnes dans la classe de colonnes c. La sous-matrice de X déterminée par I r et J c est nommée co-cluster. Pour avoir un premier critère de qualité sur les co-classifications, nous cherchons toujours à optimiser une certaine fonction objectif.\nensemble de toutes les co-classifications possibles.\nDes exemples de fonction objectif sont le coefficient ? de Goodman-Kruskal utilisé dans Robardet et Feschet (2001) ou la perte d\u0027information mutuelle exploitée dans Dhillon et al. (2003). Dans cet article, nous utilisons la somme des résidus quadratiques introduite dans Cho et al. (2004). Pour des raisons de faisabilité calculatoire, les algorithmes de co-classification relaxent ces contraintes d\u0027optimisation avec une mise en oeuvre d\u0027heuristiques d\u0027optimisations locales. En sus des contraintes d\u0027optimisation qui sont souvent implicites, on veut pouvoir spécifier d\u0027autres types de contraintes qui sont maintenant définis.\n, elles ne peuvent pas être dans la même classe de\nCes formes de contraintes ont été très étudiées dans le cadre de la classification semisupervisée (Bilenko et al., 2004). Nous les généralisons ici pour qu\u0027elles puissent s\u0027appliquer aussi bien à l\u0027ensemble des lignes qu\u0027à l\u0027ensemble des colonnes. Dans une matrice d\u0027expression, on peut ainsi exploiter des connaissances sur les gènes et/ou sur les conditions expérimen-tales. Par exemple, si l\u0027on sait que le gène i a et le gène i b ont la même fonction (disons F ) dans un processus biologique, on peut vouloir forcer une contrainte must-link entre ces deux gènes afin de privilégier la recherche d\u0027un co-cluster associant des gènes ayant cette fonction F et donc d\u0027identifier un module de transcription qui serait à l\u0027origine de cette fonction. Nous pourrions également ajouter des contraintes cannot-link pour éviter d\u0027associer dans les co-clusters des situations expérimentales que l\u0027on souhaite séparer (e.g., séparer différents type de tissus ou de lignées cellulaires).\nNous faisons aussi l\u0027hypothèse qu\u0027une valeur réelle s c (j) (resp. s r (i)) est associée à chaque colonne j (resp. ligne i). Nous avons donc s r : {1, 2, . . . , m} ? R et s c : {1, 2, . . . , n} ? R. Par exemple, s c (j) (resp. s r (i)) pourrait être une mesure temporelle ou spatiale liée à j (resp. i). Dans des données issues de Puces ADN, où chaque colonne correspond à une puce (i.e., une expérience) et chaque ligne désigne un gène, s c (j) pourrait être le temps d\u0027échantillonnage liée à la puce ADN j. Un second exemple serait de considérer s r (i) comme une mesure de la position (spatiale) absolue du gène i dans la séquence d\u0027ADN de l\u0027organisme étudié. Les deux fonctions s r et s c permettent de définir un ordre sur l\u0027ensemble des colonnes et/ou des lignes. On dit alors que j a j b ssi s c (j a ) ? s c (j b ). Dans la suite, on considère que, si une fonction s c existe, alors tous les éléments j sont ordonnés, i.e., ?j a , j b tels que j a \u003c j b , s c (j a ) ? s c (j b ) (idem pour les lignes). Il devient alors intéressant de rechercher des co-clusters qui soient cohérents avec les ordres définis par les fonctions s r et s c . Par exemple, si l\u0027on s\u0027intéresse aux différentes étapes du développement d\u0027une cellule et si l\u0027on veut découvrir les gènes majoritairement impliqués dans chaque étape, nous pouvons chercher des classes qui soient contiguës dans le temps. Pour cela, on peut forcer la contrainte d\u0027intervalle introduite dans Pensa et al. (2006a,b). \nLa satisfaction des contraintes must-link, cannot-link et interval entraîne en général une diminution de l\u0027optimum théorique de la fonction objectif. Nous voulons un algorithme de coclassification qui puisse prendre en compte de telles contraintes tout en essayant d\u0027optimiser la fonction objectif retenue. Notez que, la satisfaction d\u0027une conjonction de contraintes c \u003d , c \u003d et c int n\u0027est pas toujours faisable. Par exemple, pour trois objets i 1 , i 2 , i 3 tels que\nne peut jamais être satisfaite même si les sous-contraintes de cette conjonction ne posent aucun problème. Dans cet article, nous ferons l\u0027hypothèse que les conjonctions de contraintes traitées par notre approche sont toujours faisables. Le problème de la faisabilité des contraintes pour les méthodes partitionnelles et hiérarchiques a été largement traité dans Davidson et Ravi (2005a,b).\nExploitation de la somme des résidus quadratiques\nNous présentons notre approche de co-classification sous contraintes en proposant un algorithme itératif qui minimise la somme des résidus quadratiques. Cette fonction objectif a été introduite dans Cho et al. (2004) pour la co-classification sans contraintes appliquée au contexte des matrices d\u0027expression. Il s\u0027agit d\u0027une adaptation de la mesure introduite dans Cheng et Church (2000) pour la découverte de motifs locaux dans les matrices d\u0027expression.\nSoit X ? R m×n la matrice de données, nous cherchons une partition de X en k classes de lignes, et l classes de colonnes. Utilisons la définition de résidu de Cheng et Church (2000).\nm×n la matrice des résidus calculés avec la définition précédente. La fonction objectif que l\u0027on veut minimiser est la somme des résidus quadratiques de Cho et al. (2004) calculée de la manière suivante :\nr,c i?Ir,j?Jc\nOn peut réécrire la matrice des résidus dans une forme plus compacte si l\u0027on introduit les deux matrices\nsi i est dans la classe r (m r \u003d |I r | étant le nombre de lignes dans la classe r), 0 autrement. Chaque élément (j, c) (1 ? c ? l) de la matrice C est égal à n ?1/2 c si j est dans la classe c (n c \u003d |J c | étant le nombre de colonnes dans la classe c), 0 autrement. La matrice des résidus dévient alors :\nLa démonstration de la validité de cette équation est dans Cho et al. (2004). Les auteurs\nIrJc , avant de montrer que (3) est vraie. Ils concluent que, si l\u0027on considère la projection (I ? RR T )X de la matrice X, alors ||H|| 2 donne la fonction objectif de K-MEANS pour cette matrice modifiée.\nConsidérons maintenant notre contribution algorithmique. Notre approche utilise une technique dite \"ping-pong\" pour traiter de manière alternée (avec la méthode K-MEANS) les colonnes et les lignes. Ainsi, la matrice C n\u0027est mise à jour qu\u0027après que chaque colonne ait été affectée à la classe de colonnes la plus proche (similairement pour les lignes). Nous proposons donc de décomposer le calcul de la fonction objectif. Soit\nT , nous obtenons la réécri-ture suivante :\nDe la même manière, en posant\nnous obtenons comme décomposition en termes de lignes :  \nL\u0027algorithme 1, traite la co-classification en présence de conjonctions de contraintes mustlink et cannot-link (la partie concernant le traitement des lignes ( est omise). Il commence par initialiser (e.g., aléatoirement) les matrices C et R. À chaque itération, l\u0027algorithme affecte chaque colonne (ligne) à la classe de colonnes (lignes) la plus proche qui n\u0027introduit pas une violation des contraintes cannot-link. Si une colonne (ligne) est impliquée dans une contrainte must-link, nous affectons l\u0027ensemble des colonnes (lignes) impliquées par la fermeture transitive de cette contrainte à la classe de colonnes (lignes) pour laquelle la moyenne des distances est minimum, en contrôlant toujours qu\u0027il n\u0027y ait pas de contrainte cannot-link violée par cette opération. Ensuite, l\u0027algorithme met à jour la matrice C (R) selon le schéma d\u0027affectation résultant des opérations décrites précédemment. Le processus est réitéré jusqu\u0027à ce que la diminution de la fonction objectif devienne très petite (i.e., inférieure à un facteur de tolérance ? ). Notons que la phase d\u0027initialisation peut ne pas prendre en compte les contraintes, car leur satisfaction est assurée par la première itération. Une amélioration possible consiste à utiliser un meilleur critère d\u0027affectation pour les objets impliqués dans des contraintes cannot-link. On RNTI -X -6 sait d\u0027ailleurs que la satisfaction d\u0027un ensemble de contraintes cannot-link pour un nombre de classes donné est un problème NP-complet (Davidson et Ravi, 2005b).\nSatisfaction de la contrainte \"interval\"\nAlgorithme 2 : IntCoClust(X,k,l)\nEntrées : Une matrice de données X, k et l, int r , int c Sorties :\nL\u0027algorithme 2 permet de résoudre le problème de la satisfaction de la contrainte \"interval\" (la partie concernant le traitement des lignes ( est omise). Dans ce cas, l\u0027initialisation ( des partitions concernées par cette contrainte doit produire un nombre l (resp. k) d\u0027intervalles sur les colonnes (resp. les lignes). Ensuite, le processus d\u0027affectation s\u0027intéresse uniquement aux frontières entre les intervalles. Plus particulièrement, il traite itérativement d\u0027abord la frontière gauche, puis la frontière droite. Une colonne (resp. ligne) peut être affectée à l\u0027intervalle adjacent si la distance est inférieure à celle calculée sur l\u0027intervalle de départ. Dans ce cas, on continue à traiter les colonnes (resp. lignes) restantes. Lorsque la frontière gauche et la frontière droite d\u0027un intervalle correspondent à la même colonne (resp. ligne), l\u0027algorithme passe à la frontière suivante. Dans le cas où il n\u0027est pas nécessaire de réaffecter la colonne (resp. ligne), l\u0027algorithme termine le traitement de cette frontière et passe à la frontière suivante. Notez que, contrairement à Pensa et al. (2006b), la satisfaction de la contrainte interval est bien garantie sur la co-classification résultat.\nRNTI -X -7\nCo-classification sous contraintes par la somme des résidus quadratiques L\u0027intégration des deux algorithmes pour traiter une conjonction de contraintes must-link, cannot-link et interval n\u0027a pas encore été traitée. Donnons cependant une piste de recherche. Il s\u0027agit d\u0027abord de faire en sorte que chaque ensemble M r ? M r (ou M c ? M c ) soit un intervalle. Par exemple, pour un ensemble d\u0027objets {i 1 , i 2 , i 3 , i 4 , i 5 }, et un ensemble M r \u003d {i 2 , i 4 }, nous serions obligés d\u0027inclure l\u0027objet i 3 dans M r par la définition même d\u0027un intervalle. Ensuite, il faut que l\u0027initialisation produise une partition qui prenne en compte l\u0027ensemble des contraintes (notons que la satisfaction d\u0027une conjonction de contraintes cannot-link est un problème NP-complet). Enfin, il est possible d\u0027utiliser la stratégie de l\u0027algorithme 1 uniquement sur les frontières, suivant le schéma présenté dans l\u0027algorithme 2.\nComplexité Concernant l\u0027algorithme 1, notons que pour calculer (I ? RR T )X(I ? CC T ), le nombre d\u0027opérations nécessaires est celui qu\u0027il faut pour calculer R T XC, i.e., kn(m + l) opérations. Ce calcul peut donc être effectué dans un temps O(N ) (quand N \u003d mn) dans l\u0027hypothèse vraisemblable où k l \u003c\u003c m n. La phase d\u0027affectation des lignes et colonnes aux nouvelles classes, nécessite un temps O(N (k + l) à chaque itération. La complexité totale de l\u0027algorithme est donc en O(N (k + l)t), où t est le nombre total d\u0027itérations nécessaires à l\u0027algorithme pour compléter la co-classification. La complexité de l\u0027algorithme 2, est trivialement la même que celle de l\u0027algorithme 1. Noter que, en général, le fait de travailler uniquement sur les frontières, se traduit par une meilleure efficacité dans la phase d\u0027affectation.\nValidation expérimentale\nNous avons étudié le comportement de nos algorithmes dans deux jeux de données \"Puces ADN\" nommés plasmodium et drosophila. Le premier décrit dans Bozdech et al. (2003) concerne le transcriptome du cycle de développement intraerythrocytique du Plasmodium Falciparum, i.e., un agent responsable de la malaria humaine. Les données fournissent le profil d\u0027expression de 3 719 gènes dans 46 échantillons biologiques. Chaque échantillon correspond à un moment dans le cycle de développement : il commence avec l\u0027invasion des globules rouges du sang par le mérozoïte, et il est divisé en trois phases : anneau, trophozoïte et schizonte (concernant respectivement le moustique, le foie, et le sang). Après 48 heures, le cellule se réplique et se divise. Aux instants marqués 17h et 29h, on observe deux transitions brusques. Le second jeu de données est décrit dans Arbeitman et al. (2002). Il concerne l\u0027expression des gènes de la Drosophile melanogaster durant son cycle de vie. Les niveaux d\u0027expression de 3 944 gènes sont mesurés pour 57 périodes séquentielles de temps divisés en stade embryonnaire, larvaire et pupaire.\nDans toutes nos expériences, la valeur du paramètre d\u0027arrêt ? a été fixée à 10 ?4 ||X|| 2 . L\u0027initialisation des partitions étant aléatoire, nous avons exécuté nos algorithmes 20 fois pour chaque groupe de contraintes.\nContraintes must-link et cannot-link\nNous avons d\u0027abord étudié le traitement des contraintes must-link et cannot-link sur l\u0027ensemble des gènes uniquement. Pour cela, nous avons utilisé le jeu de données plasmodium, pour lequel on dispose d\u0027un certain nombre d\u0027information sur les gènes impliqués dans les différentes étapes du développement. En particulier, nous avons considéré le groupe cytoplasmic RNTI -X -8 translation machinery (159 gènes), actif dans la première phase du cycle de vie de la bactérie, le groupe merozoite invasion (87 gènes) actif dans la seconde phase, et le groupe early ring transcripts (34 gènes), caractérisant la dernière phase de son développement. Tous ces groupes fonctionnels sont décrits dans Bozdech et al. (2003). Nous avons sélectionné aléatoirement 20 ensembles de contraintes sur la base des trois groupes de gènes précédemment décrits. Chaque ensemble contient un nombre variable de contraintes, et le nombre de gènes impliqués dans chaque ensemble varie entre le 20% et le 50% des gènes impliqués dans les trois groupes fonctionnels. Pour cette expérience, nous avons utilisé k \u003d 3 and l \u003d 3, afin de pouvoir identifier les trois étapes du développement de Plasmodium Falciparum. \nTAB. 1 -Pourcentage de gènes dans les trois classes pour l\u0027approche sans contrainte (a) et avec contraintes (b).\nLes résultats présentés dans le tableau 1 montrent, pour chaque groupe fonctionnel, le pourcentage de gènes impliqués dans chaque classe. L\u0027amélioration est beaucoup plus sensible pour le second groupe de gènes (merozoite invasion). Le dernier groupe (early ring transcripts) ne semble pas bénéficier de l\u0027exploitation des contraintes.\nPour mesurer l\u0027impact de l\u0027utilisation combinée de contraintes sur l\u0027ensemble des lignes et l\u0027ensemble des colonnes, nous avons choisi une co-classification cible parmi les résul-tats obtenus sans l\u0027utilisation des contraintes. En particulier, nous avons sélectionné la coclassification avec la valeur minimale de la fonction objectif obtenue à la fin du processus itératif. Cette valeur était de 1.99×10\n4 . Nous avons ensuite généré aléatoirement 20 ensembles de contraintes impliquant gènes et conditions expérimentales. Le nombre de gènes impliqués dans les contraintes varie entre 5% et 10% de la taille totale de l\u0027ensemble de gènes. Pour les conditions expérimentales, ce nombre varie entre 15% et 25%. Pour évaluer la conformité entre la bi-partition choisie et les partitions découvertes par l\u0027algorithme de co-classification, nous avons utilisé l\u0027indice de Rand corrigé (Hubert et Arabie, 1985). Si C \u003d {C 1 . . . C z } est la structure issue de la classification et que P \u003d {P 1 . . . P z } est une partition prédéfinie, chaque paire de points peut être affecté à la même classe ou à deux classes différentes. Soit a le nombre de paires appartenant à la même classe de C et à la même classe de P, soit\nRNTI -X -9 la valeur maximale de a. La conformité entre C et P peut être estimée au moyen de la formule :\nLorsque RC(C, P) \u003d 1, les deux partitions sont identiques. Nous avons comparé les résultats obtenus avec l\u0027utilisation des contraintes avec ceux qui ont été obtenus sans aucune spécification de contrainte. Le résumé de cette expérience est présenté dans le tableau 2. On voit que l\u0027utilisation de contraintes produit une amélioration très nette de la conformité des deux partitions, en entraînant une légère diminution du nombre moyen d\u0027itérations nécessaire pour compléter la co-classification. Dans le même temps, la spécification de contraintes entraîne une amélioration de la valeur finale de la fonction objectif (diminution d\u0027environ 2%).\nRC\nContrainte interval\nPour évaluer la valeur ajoutée de la contrainte interval, nous avons appliqué notre algorithme au jeu de données drosophila. Notre objectif est ici de redécouvrir les trois phases du cycle de vie de la drosophile en utilisant comme seule connaissance le nombre de classes (k \u003d l \u003d 3). \nTAB. 3 -Index de Rand corrigé, valeur finale de la fonction objectif et nombre d\u0027itérations (valeurs moyennes).\nNous avons donc comparé l\u0027index de Rand obtenu avec et sans l\u0027utilisation de la contrainte interval pour une collection de 20 exécutions. Les résultats (voir tableau 3) montrent clairement que l\u0027utilisation de la contrainte interval permet de retrouver de façon plus nette les trois phases du cycle de vie de la drosophile (l\u0027amélioration mesurée sur l\u0027index de Rand est d\u0027environ 85%). De plus, le nombre d\u0027itérations nécessaires pour compléter la co-classification est sensiblement inférieur à celui que l\u0027on obtient si l\u0027on n\u0027utilise pas la contrainte interval. Notons que la valeur finale de la fonction objectif et meilleure dans la version non contrainte. Cela signifie que la structure découverte par notre algorithme est loin d\u0027être l\u0027optimum global pour ce jeu de données. Notons aussi que, dans aucun des cas, l\u0027algorithme sans contrainte n\u0027a pu retrouver des intervalles.\nConclusion\nLa co-classification permet une interprétation plus facile des groupements qu\u0027une classification mono-dimensionnelle. Nous nous sommes posés le problème de l\u0027exploitation de contraintes dans une co-classification. Permettre la définition de contraintes, c\u0027est autoriser l\u0027exploitation de connaissances du domaine pour obtenir des groupements plus pertinents. Encore faut-il être capable de combiner l\u0027optimisation des fonctions objectifs (au coeur des algorithmes de classification) et la satisfaction de contraintes comme des contraintes must-link ou cannot-link étendues au contexte de la co-classification. Contrairement à la seule proposition de co-classification sous contraintes que nous connaissons (Pensa et al., 2006a,b), nous proposons ici une méthode qui travaille directement sur les données numériques et qui garantit le respect des contraintes spécifiées. À cette fin, nous nous appuyons sur la fonction objectif des résidus quadratiques introduite dans Cho et al. (2004). L\u0027une des perspectives à court terme de ce travail consiste à valider les pistes identifiées pour l\u0027intégration entre la résolution des contraintes d\u0027intervalle et celle des autres types de contraintes.\n"
  },
  {
    "id": "815",
    "text": "Introduction\nDans le domaine des systèmes d\u0027informations géographique et, plus généralement, des systèmes d\u0027informations spatialisées, certaines recherches (Timpf, 2001, Laurini et Servigne, 2007, montrent depuis une dizaine d\u0027années que ces applications doivent prendre en considération une variété plus large d\u0027influences et de limitations que celles utilisées dans la cartographie conventionnelle (de nature matérielle, mais aussi de nature cognitive et sémantique), liées aux utilisateurs, aux dispositifs d\u0027accès, à l\u0027environnement, et de s\u0027adapter afin d\u0027être utilisables par leur différents utilisateurs finaux.\nCes préoccupations se sont multipliées avec l\u0027émergence de l\u0027informatique mobile et ubiquitaire. L\u0027augmentation de l\u0027autonomie des dispositifs mobiles, de leur taille mémoire et de leur puissance de calcul a donné la possibilité à la communauté de chercheurs en géomatique d\u0027explorer de nouvelles applications des systèmes d\u0027information géographique (Anegg, 2002. De nouvelles perspectives s\u0027ouvrent pour les utilisateurs qui peuvent accéder à des informations spatialisées n\u0027importe quand, n\u0027importe où, à travers des dispositifs d\u0027accès très variés en termes de puissance de calcul et capacité d\u0027affichage (par exemple les services localisés et les systèmes de navigation). Cet accès accru et universel aux applications spatiales accentue les problèmes d\u0027adaptation, liés tant aux limitations intrinsèques des dispositifs mobiles, qu\u0027aux problèmes cognitifs soulevés par l\u0027interaction entre l\u0027utilisateur et ces types de dispositif dans un environnement mobile.\nInitialement, les recherches se sont focalisées sur le contournement des limitations techniques des dispositifs mobiles (ou DM), comme les PDA (Personal Digital Assistant) et les smartphones (téléphones portables de dernière génération dotées d\u0027écrans tactiles). Parmi les objectifs visés nous pouvons citer l\u0027optimisation de l\u0027utilisation de la bande passante dans des réseaux à bas débit (Follin et al., 2005), la généralisation du contenu pour réaliser l\u0027adaptation à la capacité de calcul et à la capacité mémoire des DM (Lee et al., 2003), la généralisation dynamique pour adapter la présentation de cartes sur les DM à affichage réduit (Sester et Brenner, 2004).\nDans des recherches plus récentes, l\u0027attention s\u0027est portée vers les aspects cognitifs de l\u0027adaptation à l\u0027utilisateur (personnalisation) et à son contexte (sensibilité au contexte). Les travaux sur la personnalisation visent à éviter les phénomènes de fatigue et désorientation (Conklin, 1987). En transformant l\u0027application de manière à prendre en compte le profil de l\u0027utilisateur, il devrait avoir le sentiment que le système a été conçu spécialement pour lui. Parmi les caractéristiques des utilisateurs considérées pour l\u0027adaptation, on trouve le plus couramment les préférences et intérêts de l\u0027utilisateur (Malaka et Zipf, 2000), sa nationalité (Sarjakoski et al., 2002) ou son âge (Nivala et Sarjakoski, 2004).\nD\u0027autres recherches mettent en évidence l\u0027influence que peut avoir le contexte de l\u0027utilisateur sur l\u0027adaptation (Zipf, 2005, Reichenbacher, 2001) en soulignant que les SIST doivent être capables de présenter à l\u0027utilisateur une information pertinente, au moment opportun et au bon endroit, en fonction de son environnement. Plus précisément, la notion de contexte de l\u0027utilisateur définit l\u0027ensemble d\u0027entités qui peuvent avoir une influence significative sur l\u0027interaction entre l\u0027utilisateur et l\u0027application. Parmi les caractéristiques du contexte le plus souvent prises en compte pour l\u0027adaptation, on peut citer la localisation (Anegg, 2002, l\u0027activité en cours de réalisation par l\u0027utilisateur , Petit et al., 2006, la saison (Nivala et Sarjakoski, 2004) ou encore la direction de mouvement de l\u0027utilisateur (Roth, 2002).\nCes travaux présentent toutefois certains inconvénients. Ainsi, les applications existantes utilisent des mécanismes d\u0027adaptation assez simplistes, qui ne tiennent pas compte des aspects plus complexes de la cartographie (Reichenbacher, 2001) comme, par exemple, les représentations multiples. De même, la dimension temporelle n\u0027est généralement pas prise en compte, tant au niveau de la gestion de données qu\u0027au niveau de l\u0027adaptation (Schwinger et al., 2005). Enfin, les approches existantes n\u0027offrent pas de support pour la conception de SIST adaptatifs. La mise en oeuvre des mécanismes d\u0027adaptation se confond avec la logique interne de l\u0027application, ce qui rend ces approches très difficilement réutilisables et extensibles.\nEn constatant ce manques d\u0027outils et approches génériques pour l\u0027adaptation dans les SIST, notre objectif est d\u0027offrir aux concepteurs de SIST des outils de conception et des logiciels afin de leur faciliter les démarches de conception et de développement de SIST adaptables. Cet article présente un framework, appelé ASTIS, dédié à la conception et la -64 -RNTI-E-13 réalisation de systèmes d\u0027information spatio-temporelle adaptatifs et interactifs. ASTIS adopte une approche de conception et de mise en oeuvre d\u0027applications basée sur des modèles. Nous avons préalablement proposé une architecture modulaire appelée GenGHIS (Moisuc et al., 2005), séparant contenu et présentation et permettant aux concepteurs de SIST de générer des applications à partir de modèles de données conçus par eux. GenGHIS est composé de deux modules principaux : l\u0027un assure la gestion du contenu, l\u0027autre la gestion de la présentation, en s\u0027appuyant sur un système de représentation de connaissances par objets spatio-temporel appelé AROM-ST (Moisuc et al., 2004 \nL\u0027architecture générale de la plate-forme\nNotre approche pour la conception et le développement d\u0027applications spatio-temporelles adaptées est basée sur la génération de code à partir de modèles. L\u0027idée est de permettre aux concepteurs de créer des applications adaptatives en se basant sur une approche déclarative, de conception de modèles orientés objet. L\u0027architecture est formée de composants paramétrables qui opèrent une suite de transformations et de conversions, depuis des formats et sources externes de données (aux formats relationnels les plus répandus : mif/mid, excel, mdb, etc.) jusqu\u0027au format final, affichable à l\u0027écran (documents SVG et XHTML avec une structure particulière). Le framework contient également des composants visuels génériques, capables d\u0027afficher de manière interactive les documents structurés résultants du processus de transformation.\nL\u0027architecture est composée de trois modules principaux : un module de données, un module de présentation et un module d\u0027adaptation. Le module de données est chargé du stockage, de l\u0027interrogation et de l\u0027analyse des données en conformité avec les modèles spécifiés par les concepteurs. Il permet aux concepteurs de décrire leurs modèles de données et de les instancier. Le module de présentation permet aux concepteurs de créer des modèles de présentation adaptés et de générer des présentations conformes à ces modèles. Il contient aussi les composants visuels nécessaires pour afficher ces présentations. Le module d\u0027adaptation est chargé de la gestion du contexte, des utilisateurs, et de l\u0027application dynamique des mécanismes d\u0027adaptation choisis en fonction de ces cibles.\nLe fonctionnement de l\u0027architecture est décrit de manière simplifiée dans la FIG. 1. La partie gauche de la figure illustre la partie statique de l\u0027architecture, c\u0027est-à-dire les composants impliqués dans les processus de conception. La partie droite contient la partie -65 -RNTI-E-13\nConception de systèmes d\u0027information spatio-temporelle adaptatifs avec ASTIS dynamique de l\u0027architecture, c\u0027est-à-dire les composants chargés de la génération à la volée et de l\u0027affichage de contenus SVG adaptés. Les composants représentés par des rectangles à angles droits sont des composants paramètrisables via des modèles. Les composants représentés sous forme de rectangles à coins arrondis sont les composants chargés des conversions de format.\nFIG. 1 Schéma du fonctionnement d\u0027ASTIS.\nPour créer une nouvelle application, les concepteurs doivent d\u0027abord spécifier le schéma de données de l\u0027application. Ensuite, ils doivent spécifier différentes transformations de ce schéma afin de l\u0027adapter aux caractéristiques des utilisateurs et du contexte. De la même manière, la troisième étape consiste à spécifier des adaptations de présentation, c\u0027est-à-dire différentes options de style et d\u0027affichage applicables. La dernière étape de conception permet de définir et faire la liaison entre les différentes caractéristiques du contexte de l\u0027utilisateur et les adaptations correspondantes. Cela revient à spécifier un modèle d\u0027adaptation comme celui de la FIG. 2. A présent, il n\u0027existe pas un consensus sur une terminologie de l\u0027adaptation. Nous avons adopté la terminologie de (Reichenbacher, 2003). Ainsi, la cible de l\u0027adaptation (target) définit les caractéristiques significatives de l\u0027utilisateur ou de son contexte auxquelles l\u0027application doit s\u0027adapter (par exemple, le niveau d\u0027expertise de l\u0027utilisateur ou sa localisation). Une composante à adapter (adaptee) représente une caractéristique du contenu ou de la présentation qui peut être ajustée en adéquation avec le contexte. Le mécanisme d\u0027adaptation (adapter) est la fonctionnalité du système qui permet d\u0027ajuster les caractéristiques des composantes à adapter en fonction des caractéristiques connues des cibles.\n-66 -RNTI-E-13\nFIG. 2 Modèle d\u0027adaptation d\u0027ASTIS.\nAfin de finaliser la réalisation d\u0027une nouvelle application les concepteurs doivent procéder à l\u0027acquisition de données, c\u0027est-à-dire peupler avec des instances la base de connaissances AROM-ST de l\u0027application, en utilisant le composant de conversion de formats externes vers AROM-ST. Notre approche de conception s\u0027inscrit donc dans le cadre de l\u0027adaptativité (MacEachren, 1998), les concepteurs sont amenés à décrire des adaptations en fonction des caractéristiques de l\u0027utilisateur et de son contexte, qui seront appliquées automatiquement, sans l\u0027intervention des utilisateurs, au moment de l\u0027exécution de l\u0027application. Il est important de noter que, du point de vue technique il est possible de mettre en oeuvre de l\u0027adaptabilité, c\u0027est à dire de permettre aux utilisateurs de décrire et appliquer eux-mêmes ces adaptations, de manière dynamique, pendant l\u0027exécution de l\u0027application. Nous considérons que, pour que notre approche soit utilisable, il est nécessaire de créer de nouvelles interfaces et des manières simplifiées de contrôler les applications, parce qu\u0027il est raisonnable d\u0027imaginer que les utilisateurs finaux sont moins avisés en ce qui concerne la modélisation de données et de présentations spatio-temporelles. Une des perspectives de notre travail vise la mise en oeuvre d\u0027interfaces et modalités de présentation simplifiées pour rendre notre architecture plus utilisable aussi dans une approche basée adaptabilité.\nAu moment de l\u0027exécution, le gestionnaire d\u0027adaptation choisit, en fonction des caractéristiques concrètes du contexte, les modalités adéquates d\u0027adaptation à appliquer, décrites par le modèle d\u0027adaptation. Ces modalités sont appliquées au moment de la génération du code SVG de l\u0027application, au niveau de la présentation (structure de la  \nFIG. 4 Exemple de schéma de base dans une application dédiée aux risques d\u0027éboulements.\nAdaptation de la présentation\nLes points de vue peuvent influencer non seulement les données à présenter aux utilisateurs, mais également la manière dont les données sont présentées. La plate-forme ASTIS permet de générer des interfaces de visualisation qui rendent compte des dimensions diverses de l\u0027information (dimensions spatiale, temporelle et attributaire). Celles-ci sont affichées dans des fenêtres spatiales (cartes), temporelles (diagrammes temporels) ou attributaires (tableaux), dont les contenus sont synchronisés. Les mécanismes de synchronisation (ou linking, voir Buja et al., 1991)  Le modèle de présentation d\u0027ASTIS étend le standard SLD sur deux aspects : la prise en compte des présentations temporelles et attributaires, et non pas seulement spatiales, ainsi que la prise en compte des éléments dynamiques de présentation, afin de permettre des présentations interactives. ASTIS permet de définir des visualisations complexes et multidimensionnelles (classe Vizualisation) contenant plusieurs fenêtres (classe Frame) de type spatial, temporel ou attributaire, chaque fenêtre affichant un certain nombre de couches (de type spatial, temporel ou attributaire). De nouveaux types de symboles graphiques temporels (1D, classe Temporal Symbol) et attributaires (0D, classe Plain Symbol) ont été ajoutés aux types existants en SLD (2D, classe Spatial Symbol). Nous pouvons apercevoir un exemple de visualisation multidimensionnelle en FIG. 6 à gauche, contenant une fenêtre temporelle, une fenêtre attributaire et deux fenêtres spatiales, chacune des fenêtres étant peuplée de symboles de type correspondant : symboles spatiaux (polygones colorées en aplats, cercles proportionnels, signes conventionnels, texte ancré, etc.), symboles temporels (histogrammes), symboles attributaires (texte simple).\nAfin de permettre la génération de présentations dynamiques et interactives, le modèle de présentation d\u0027ASTIS étend la notion de couche décrite en SLD en ajoutant des informations concernant les liens existants entre les différents types d\u0027objet (classe Association Path). Par exemple, dans une application de gestion d\u0027avalanches ( voir FIG. 7), en cliquant sur le contour d\u0027une commune sur la carte (la commune avec un remplissage rouge) il est possible -72 -RNTI-E-13 de visualiser (toujours sur la carte) quels sont les sites avalancheux susceptibles de l\u0027affecter (en violet), quelles sont les caractéristiques temporelles des événements avalancheux qui se sont produits sur ces sites (sur le diagramme temporel, en bas) ainsi que leurs caractéristiques attributaires (fenêtre attributaire, à gauche).\nFIG. 7 Exemple de synchronisme interactif dans une application de visualisation de risques avalancheux.\nFIG. 8 Exemple de visualisation pour le point de vue « touriste ».\nNous reprenons l\u0027exemple de l\u0027application dédiée au suivie des risques d\u0027éboulements pour montrer comment peuvent être adaptées les présentations aux besoins de l\u0027utilisateur pour un certain point de vue. Considérons la conception des présentations selon un point de vue « touristique » (voir la FIG. 8) et selon un point de vue « cyndinicien » (voir la FIG. 9). La visualisation pour le point de vue « cyndinicien » est conçue pour un groupe d\u0027utilisateurs habitués aux outils de type SIST, alors que la visualisation selon le point de vue « touriste » s\u0027adresse sans doute à des utilisateurs moins avisés.\nCertaines différences de contenu entre les deux fragments de cartes sont visibles. Le point de vue « touristique » contient, par exemple, des informations sur des objectifs touristiques (musées, monuments, restaurants). Le point de vue « cyndinicien » contient plus d\u0027informations liées à l\u0027occupation des sols et au réseau hydrographique.\nAu niveau de la structure, les deux visualisations contiennent des fenêtres attributaires, spatiales et temporelles. Pourtant, si pour le PDV « touriste » comporte plus de détails attributaires sur les points d\u0027intérêt touristique, les tableaux attributaires pour les cyndiniciens apportent plus d\u0027informations sur les risques mêmes. Au niveau de la fenêtre temporelle, les graphiques temporels permettent aux cyndiniciens d\u0027avoir une perspective historique sur plusieurs décennies des phénomènes liés aux risques. La fenêtre temporelle de la visualisation dédiée aux touristes leur permet tout simplement d\u0027accéder aux prévisions des dangers éventuels menaçant leurs points d\u0027intérêt pour les prochains jours.\nFIG. 9 Exemple de visualisation pour le point de vue « cyndinicien ».\nLa représentation du concept de risque est très simplifiée dans l\u0027application dédiée aux touristes. Une zone rouge permet de mettre en évidence les différents types de zones d\u0027éboulements qui sont à éviter par les touristes, permettant ainsi de les informer des risques existants lors de la préparation de leurs balades en montagne. Les différentes composantes d\u0027une zone d\u0027éboulements (pied de dépôt, niche de dépôt, niche composite, etc.) sont, au contraire, présentées en détail par des signes conventionnels familiers aux cyndiniciens.\nEn ce qui concerne les niveaux d\u0027expertise, la visualisation s\u0027adressant aux touristes est plus simple : la densité visuelle affichée (et, respectivement, la quantité totale d\u0027information) est plus faible. Son style de présentation s\u0027appuie sur l\u0027utilisation de symboles visuels. Au contraire, la visualisation pour les cyndiniciens est plus dense et utilise des signes -74 -RNTI-E-13 conventionnels qui permettent d\u0027afficher une quantité plus importante d\u0027information, au détriment de la lisibilité.\nConclusions et perspectives\nNous avons présenté dans cet article une plate-forme pour la conception et la génération de SIST adaptables à l\u0027utilisateur. Cette architecture implémente une approche conceptuelle de l\u0027adaptation à l\u0027utilisateur qui permet aux concepteurs de SIST de générer des applications adaptables en décrivant simplement des modèles conceptuels. Ces modèles doivent décrire la transformation applicable à la composante à adapter (les éléments du système qui sont transformés afin de répondre aux besoins de l\u0027utilisateur) et la cible de l\u0027adaptation (les caractéristiques de l\u0027utilisateur ou de son environnement que le système doit prendre en compte pour l\u0027adaptation). Afin d\u0027aider les concepteurs dans la modélisation, la plate-forme offre des modèles génériques pour chaque type de cible et de composante à adapter, qu\u0027il suffit aux concepteurs d\u0027instancier. Cela permet aux mécanismes génériques d\u0027adaptation implémentés au sein de notre plate-forme de réaliser l\u0027adéquation aux types d\u0027utilisateurs au niveau de l\u0027application finale. Notre approche met en oeuvre tant l\u0027adaptation du contenu (les données), que l\u0027adaptation de la présentation (les interfaces de visualisation).\nPlusieurs perspectives sont possibles, en vue d\u0027améliorer notre proposition et de la rendre plus utile pour la conception et la réalisation d\u0027applications pour des SIST mobiles. La première vise la réalisation d\u0027un module d\u0027acquisition du contexte. Ce module devrait utiliser un modèle compréhensif de contexte et devrait permettre aux applications d\u0027acquérir un maximum de paramètres du contexte (localisation, temps, paramètres du dispositif d\u0027accès, historique de l\u0027utilisateur, agenda, etc.) sans aucune intervention de la part de l\u0027utilisateur ou du concepteur. La deuxième perspective est de compléter la plateforme avec un module d\u0027adaptation dynamique, chargé de détecter les changements de contexte et de déclencher (en utilisant un mécanisme de type ECA -Event Condition Action) les adaptations nécessaires, c\u0027est-à-dire régénérer de manière adéquate les parties de l\u0027application influencées par le changement de contexte.\nSummary. The joint development of the Web and of ubiquitous computing increases constantly the potential diversity of the users, access devices and contexts of use of spatiotemporal information systems accessed via these means. Adapting these systems to their users becomes a necessity and a guarantee of usability and longevity. This paper presents a generic approach for the design and generation of adaptive spatio-temporal information systems. The presented framework integrates generic user adaptation mechanisms, aimed at adapting both the content and the presentation of the applications. It allows designers to integrate these mechanisms into adaptable applications. In order to define the needs and the adaptations of their applications, designers only have to create conceptual models, by instantiating generic models supplied by our framework.\n"
  },
  {
    "id": "817",
    "text": "Introduction\nPour faire face aux besoins des nouvelles applications (médicales, suivi de consommation, suivi des navigations sur un serveur Web, etc), de plus en plus de données sont stockées sous la forme de séquences. Pour traiter ces bases et en extraire des connaissances pertinentes, les motifs séquentiels ont été proposés Agrawal et Srikant (1995). Ils permettent, étant donnée une base de données de séquences, de trouver toutes les séquences maximales fréquentes au sens d\u0027un support minimal défini par l\u0027utilisateur.Si la découverte de corrélations dans les données séquentielles est primordiale pour le décideur, il n\u0027en reste pourtant pas moins que certains problèmes ne peuvent être résolus par la recherche de tendances. De nouveaux motifs intéressent le décideur : les motifs inattendus qui contredisent les croyances acquises sur le domaine pour, par exemple, détecter des attaques sur un réseau.\nRappelons que notre objectif n\u0027est pas de trouver les motifs rares, mais bien les motifs contredisant une connaissance, ce qui n\u0027existe pas dans la littérature. La recherche de connaissance inattendue à partir d\u0027une base de croyance a été introduite dans Silberschatz et Tuzhilin (1995) et Padmanabhan et Tuzhilin (2006) présentent une approche de découverte de règles d\u0027association inattendues. Spiliopoulou (1999) propose un cadre basé sur la connaissance du domaine et des croyances pour trouver des règles séquentielles inattendues à partir de séquences fréquentes. Même si ces travaux considèrent des séquences inattendues, ils sont différents de notre problématique dans la mesure où la notion d\u0027inattendue concerne des sé-quences fréquentes sur la base afin de trier les résultats obtenus. Notre objectif est d\u0027extraire, à partir d\u0027une base, toutes les séquences inattendues et d\u0027obtenir des règles elles mêmes inattendues.\nDans cet article, nous définissons donc la notion de base de croyances et de contradiction dans le contexte des séquences. Nous parlons alors de séquence inattendue, et nous introduisons les méthodes de découverte de telles séquences. Etant donné que les motifs séquentiels traditionnels ne mettent pas en évidence des règles du type \"antécédent-conséquent\", nous étendons la notion de séquences inattendues à celles de règles inattendues. Pour extraire ces règles à partir d\u0027une base de données de séquences et d\u0027une base de croyances, nous proposons l\u0027approche USER (Unexpected Sequence Extracted Rules). \nUSER : Motifs séquentiels et règles inattendus\nim . Dans un ensemble de séquences, si une séquence s n\u0027est une sous-séquence d\u0027aucune autre, elle est dite maximale ; sinon, si s est contenue dans s , on dit que s supporte la séquence s. Soit une séquence s \u003d 1 I 2 . . . I k un segment g s est une sous-séquence qui contient des itemsets contigus i I i+1 . . . I i+n avec i ? 1 et i + n ? k. Le support d\u0027une séquence est défini comme la proportion de séquences dans D qui supportent cette séquence.\nNous définissons, dans cet article, la longueur d\u0027une séquence comme le nombre d\u0027itemsets qu\u0027elle contient, noté |s|. Nous considérons également la séquence vide et la concaténation de séquences. Une séquence vide est notée ?, avec s \u003d ? ?? |s| \u003d 0. Soient deux séquences s 1 et s 2 , la concaténation s 1 · s 2 de ces deux séquences correspond à s 1 complétée par s 2 en fin de séquence. Nous avons alors |s 1 · s 2 | \u003d |s 1 | + |s 2 |.\nPour simplifier les notations et la lecture, dans la suite de cet article, nous utilisons les majuscules A, B, C . . . pour décrire des items, et la notation (ABC) pour désigner des itemsets. La notation désigne une séquence (A puis A et C puis B et C).\nNous notons n une contrainte sur la longueur de séquences, avec op ? {{ \u003d, \u003d, \u003c, ? , \u003e, ?} et n ? N. La notation |s | |\u003d n signifie que la longueur de la séquence satisfait n Dans le cas où l\u0027on a n \u003d 0 on note * . Soit une séquence s, avec s 1 et s 2 deux sous-séquences de s, i.e. s 1 , s 2 s, telles que s 1 apparaisse avant s 2 dans s.\n, avec g vérifie n Dans le cas où n \u003d 0 (g \u003d ?), nous notons s 1 ? s 2 le fait que s 1 soit directement suivi de s 2 dans la séquence s. Nous avons alors :\nDans le cas le moins contraint, l\u0027écriture s 1 ? * s 2 désigne le fait que s 2 apparaît après s 1 dans s (s \u003d s 1 · s · s 2 sans contrainte sur s ). Une croyance représente une connaissance décrite sous la forme d\u0027une relation de causalité temporelle entre des occurrences d\u0027éléments dans une séquence.\nRNTI -G -\nDans notre approche, nous utilisons des règles pour décrire les relations de causalité entre séquences. De manière similaire à Spiliopoulou (1999), nous notons s ? la prémisse et s ? la conclusion. Ceci signifie que s ? ? s ? est satisfaite dans s, si l\u0027occurrence de s ? s implique l\u0027occurrence d\u0027une sous-séquence s ? s telle que s ? · s ? s.\nDans notre approche, ce sont les experts qui définissent les croyances à prendre en compte.\nDéfinition 1 (Croyance). Une croyance b sur une séquence est un couple (p, C) tel que : Notons qu\u0027une séquence inattendue peut être liée à deux contraintes violées. Ainsi une séquence peut être à la fois ?-et ?-inattendue, ou être à la fois ?-et ?-inattendue.\nIl est à présent intéressant de découvrir les tendances au sein de ces séquences (par exemple pour caractériser une attaque ou une fraude). Nous utilisons pour cela le cadre des motifs séquentiels. Afin de mieux identifier les segments de séquence correspondant à la partie \"souche\" et aux parties de contradiction, nous introduisons la notion de séquence inattendue bornée. \nL\u0027approche USER\nNous supposons connue une base de croyances et cherchons les comportements inattendus dans une base de données de séquences. L\u0027approche décrite ici s\u0027articule autour de deux phases. Dans la première phase, l\u0027algorithme USE (Unexpected Sequence Extraction) extrait toutes les séquences inattendues pour chaque type de violation de contrainte et pour chaque croyance. Dans une seconde phase, l\u0027algorithme USR (Unexpected Sequence Rules) trouve tous les motifs séquentiels inattendus et les règles associées à partir des séquences inattendues trouvées par USE, à partir de seuils de support/confidence définis a priori.  \nConclusion\nDans cet article, nous avons introduit la problématique de la recherche de motifs séquen-tiels et règles séquentielles inattendus qui trouve de très nombreuses applications dans les bases de données réelles (détection de pannes, de fraudes, de niches commerciales, etc.) L\u0027approche USER est proposée, décomposée en différentes étapes successives (USE/USR). for all u b do for all s a ? P \nRNTI -G -\nMotifs séquentiels et règles inattendus\n"
  },
  {
    "id": "818",
    "text": "Résumé. Dans le contexte de la gestion de flux de données, les données entrent dans le système à leur rythme. Des mécanismes de délestage sont à mettre en place pour qu\u0027un tel système puisse faire face aux situations où le débit des données dépasse ses capacités de traitement. Le lien entre réduction de la charge et dégradation de la qualité des résultats doit alors être quantifié. Dans cet article, nous nous plaçons dans le cas où le système est un cube de données, dont la structure est connue a priori, alimenté par un flux de données. Nous proposons un mécanisme de délestage pour les situations de surcharge et quantifions la dégradation de la qualité des résultats dans les cellules du cube. Nous exploitons l\u0027inégalité de Hoeffding pour obtenir une borne probabiliste sur l\u0027écart entre la valeur attendue et la valeur estimée.\nLa gestion de flux de données\nLes avancées de l\u0027électronique et de l\u0027informatique enrichissent continuellement la pratique de la récolte et de la gestion des données. La constante est l\u0027accroissement des capacités de traitement, tant au niveau de l\u0027acquisition que du stockage et de l\u0027accès aux données. Mais lorsque l\u0027information doit être extraite instantanément de données récoltées continuellement, le modèle relationnel basé sur des tables atteint ses limites. C\u0027est là qu\u0027interviennent les flux de données.\nUn flux de données est une suite de tuples ayant tous la même structure. Cette structure est représentée par un schéma, comprenant le nom des champs du tuple et leur type. La différence entre un flux et une table est le caractère ordonné des tuples. L\u0027ordre est souvent déterminé par un champ d\u0027agencement (typiquement la date, mais pas nécessairement). On entre dans le cadre de la gestion de flux dès lors que -les données du flux n\u0027ont pas vocation à être stockées, -les données nécessitent un traitement immédiat, -les requêtes sont exécutées continuellement (i.e. les flux de données donnent naissance à d\u0027autres flux de données). La gestion de flux de données repose sur un modèle \"data push\" : les données se présentent d\u0027elles-mêmes, à leur propre rythme. En conséquence, le système ne maîtrise pas et ne connaît pas à l\u0027avance le rythme et ses fluctuations. Des mécanismes d\u0027adaptation sont à mettre en place pour faire face à toutes les situations, notamment lorsque le rythme s\u0027accélère au point que le système ne peut plus traiter instantanément toutes les données. Dans ce cas, un mécanisme de réduction de la charge procède à une élimination des tuples en différents points du processus de traitement. On parle de mécanisme de délestage (ou : load shedding). Le système entre alors dans un mode \"dégradé\" et il convient de mesurer l\u0027impact du délestage sur les critères de qualité.\nNous étudions ici un tel mécanisme dans le contexte suivant : le système est un cube de données alimenté par un flux de données. On parle à ce sujet de stream cubing (Han et al. (2005)). La structure du cube est supposée connue a priori et ne fluctue pas avec les données. Notre apport réside dans la proposition d\u0027une mesure probabiliste de l\u0027erreur commise sur le calcul de la valeur moyenne dans chacune des cellules du cube.\nLiaison entre erreur et taux d\u0027échantillonnage\nAfin d\u0027étudier l\u0027impact de la réduction de la charge sur la qualité du résultat dans chaque cellule du cube, il nous faut lier taux d\u0027échantillonnage et erreur sur le calcul de la moyenne. Nous exploitons pour cela l\u0027inégalité de Hoeffding. Pour le confort du lecteur, nous présentons notre résultat indépendamment de son application. Notons que notre application de la borne de Hoeffding différe de celle de Babcock et al. (2004). Il s\u0027agit ici de formuler une borne sur l\u0027erreur d\u0027estimation d\u0027une moyenne à partir de données échantillonnées suivant des taux d\u0027échantillonnage différents.\nSoit N un entier et soient M 1 , . . . , M N des variables aléatoires indépendantes deux à deux. Soient B 1 , . . . , B N des variables de Bernoulli indépendantes deux à deux et indépendantes des M n . Ces variables sont réparties dans L paquets, les N l variables du l eme paquet suivant une loi de Bernoulli de paramètre p l .\nNotons\nSi on suppose les M n toutes de même espérance µ et b n ? a n \u003d ? pour tout n, en notant\nEn utilisant l\u0027inégalité triangulaire, l\u0027additivité des probabilités et l\u0027inégalité de Hoeffding par deux fois (pour les Y n et pour les M n ), on obtient les majorations\nOn déduit de cette inégalité que, sous les hypothèses données, on commet une erreur inférieure à ? \u003d N ? p avec une probabilité supérieure à 1 ? ? en remplaçant\nNous proposons maintenant un mécanisme de délestage pour gérer la surcharge d\u0027un système composé d\u0027un cube de données alimenté par un flux de données. Nous montrons comment le résultat de la section 3 conduit à une caractérisation de la dégradation de la qualité du cube suite à l\u0027échantillonnage de données. \nAutrement dit, on souhaite que le délai de traitement global des tuples qui seront conservés ne dépasse pas le délai d\u0027acquisition des tuples du paquet. On obtient la probabilité d\u0027échantillon-nage p l dans le buffer l : p l \u003d ? l N l ? . Dès lors, chaque tuple du buffer l est indépendamment soumis à un tirage aléatoire avec probabilité p l .\nNous proposons deux stratégies pour fixer la taille des buffers : fixer la taille des buffers de manière explicite (i.e. en posant N l \u003d N pour tout l) ou implicite (i.e. en posant ? l \u003d ? pour tout l). La présentation qui suit est indifférente au choix de stratégie et nous adoptons arbitrairement ici la seconde.\nCaractérisation de l\u0027erreur. -Considérons une cellule c du cube à son niveau le plus fin. Cette cellule contient un ensemble de vecteurs de mesures (m 1 , . . . , m v ). Ces mesures sont relatives à un certain nombre de caractéristiques dimensionnelles, notamment la caractéris-tique temporelle : elles relèvent toutes de la même période de longueur T . Pour simplifier la description, supposons v \u003d 1.\nLa valeur d\u0027intérêt pour chaque cellule est la moyenne m \nPropriété. -Nous avons jusqu\u0027ici considéré le niveau le plus fin du cube. On obtient plus gé-néralement une estimation de l\u0027erreur sur toute agrégation de données contenues dans plusieurs cellules. Deux cas sont à distinguer successivement. Pour deux (ou plusieurs) cellules dont les données correspondent à une même période de longueur T , la borne se calcule à partir des nombres de tuples tombant dans le groupe de cellules avant et après échantillonnage. Pour une agrégation sur deux (ou plusieurs) périodes, la borne se calcule à partir de l\u0027ensemble des buffers constitué au cours des différentes périodes.\nConclusion\nDe nombreuses applications nécessitent le traitement continuel de données qui se pré-sentent à leur propre rythme. Les systèmes ayant à gérer de tels flux de données doivent intégrer des mécanismes de gestion de la surcharge.\nDans cet article, nous avons proposé un mécanisme de délestage pour un système composé d\u0027un cube de données alimenté par un flux de données. La nouveauté majeure consiste en l\u0027exploitation de l\u0027inégalité de Hoeffding pour quantifier l\u0027erreur sur le résultat agrégé dans chaque cellule du cube.\nIl reste à passer en phase expérimentale. Notamment, il faut résoudre la question de l\u0027estimation du temps de traitement moyen d\u0027un tuple. Plusieurs politiques existent : calculer un temps moyen absolu, considérer le temps de traitement moyen sur le paquet précédent, opérer une modélisation auto-régressive du temps de traitement, employer un modèle de régression liant le temps de traitement à différentes caractéristiques du système. Seule la pratique permettra de trancher entre ces différentes politiques. Babcock, B., S. Babu, M. Datar, R. Motwani, et J. Widom (2002). Models and issues in data stream systems. In PODS \u002702 : proceedings of the twenty-first ACM SIGMOD-SIGACT-SIGART symposium on principles of database systems, New York, NY, USA, pp. 1-16. ACM Press.\n"
  },
  {
    "id": "819",
    "text": "Introduction\nUn problème important en analyse des données est la compréhension de comportements inattendus ou atypiques de groupes d\u0027individus. Quelles sont les catégories d\u0027individus qui gagnent de particulièrement forts salaires ou au contraire, quelles sont celles qui ont de très faibles salaires ?\nNotre but est de détecter automatiquement tous les groupes d\u0027individus ayant un comportement différent de celui de l\u0027ensemble d\u0027apprentissage pour une variable quantitative donnée et plus particulièrement pour les faibles et les fortes valeurs d\u0027un intervalle déterminé par l\u0027utilisateur. Nous recherchons donc les motifs ou conjonctions de variables dont la distribution diffère significativement de celle de l\u0027ensemble d\u0027apprentissage pour les faibles et fortes valeurs de l\u0027intervalle de cette variable cible.\nUn domaine d\u0027étude proche de notre travail est l\u0027extraction des règles d\u0027association . Les règles d\u0027association sont des relations entre les variables de la forme X?Y. Dans le cadre des bases de données transactionnelles, X et Y sont des items (articles) comme par exemple, cidre ou crêpes et dans le cadre des bases de données relationnelles, X et Y sont des paires d\u0027attribut-valeur comme par exemple salaire\u003e20K€ ou profession\u003d?cadre?. Pour extraire des règles d\u0027association, on doit tout d\u0027abord rechercher les motifs (ou conjonctions de variables) ayant un support (ou taux de couverture) supérieur à une certaine valeur définie par l\u0027utilisateur. Ces motifs vérifiant ce support minimum sont dits des motifs fréquents. Ensuite, à partir des motifs fréquents, on recherche les règles dont la confiance (ou probabilité conditionnelle) est supérieure à un autre seuil défini par l\u0027utilisateur. Pour extraire ces groupes atypiques, nous recherchons non seulement les motifs fréquents (pour obtenir des groupes d\u0027individus dignes d\u0027intérêt) mais également les motifs dont le support est surprenant, c\u0027est-à-dire étonnamment faible ou au contraire étonnamment élevé par rapport à ce qui est attendu. Cette extraction de motifs surprenants est réalisée grâce à l\u0027adaptation d\u0027une mesure statistique existante, l\u0027intensité d\u0027inclination (Guillaume, 2002). Cette mesure a également l\u0027avantage de nous libérer de l\u0027étape de transformation des variables quantitatives, à savoir l\u0027étape de discrétisation suivie d\u0027un codage disjonctif complet, puisqu\u0027elle affecte un poids plus ou moins important aux individus répondant au critère recherché, comme par exemple les individus gagnant un fort salaire.  effectuent une telle transformation pour extraire des règles d\u0027association quantitatives en réalisant une discrétisation automatique capable de maîtriser la perte d\u0027information engendrée par cette transformation. Kuok et al. (1998), Zhang (1999) et Subramanyam et Goswami (2006 utilisent la technique des ensembles flous pour rechercher de telles règles. Notre approche s\u0027apparente à la technique des ensembles flous avec l\u0027attribution d\u0027un poids aux individus, poids qui peut être rapproché d\u0027un degré d\u0027appartenance. La recherche des bons intervalles est un problème majeur pour extraire des règles pertinentes en présence de variables quantitatives. Ludl et Widmer (2000) ainsi que Bay (2001) ont montré qu\u0027une discrétisa-tion de ces variables sans tenir compte du contexte, peut conduire à des solutions non optimales. Mehta et Parthasarathy (2005) ont donc proposé une discrétisation qui prend en compte la distribution de chacune des variables mises en jeu. D\u0027autres auteurs ont opté pour l\u0027optimisation des mesures : Fukuda et al. (1996) optimisent le support et la confiance alors que Brin et al. (2005)   Auman et Lindell (1999) proposent des règles qui s\u0027appuient sur la distribution des valeurs des variables quantitatives. Ainsi, une règle est jugée intéressante pour ces auteurs si la catégorie d\u0027individus vérifiant la prémisse a une moyenne pour la variable quantitative cible significativement différente du reste de l\u0027ensemble d\u0027apprentissage. Rückert et al. (2004) recherchent des règles à partir d\u0027hyperplans et obtiennent, non plus des règles à partir des motifs (règles à partir d\u0027hyperrectangles), mais des règles du type : si la somme pondérée d\u0027un ensemble de variables quantitatives est supérieure à un seuil donné, alors une autre somme pondérée de variables sera plus grande qu\u0027un autre seuil avec une confiance digne d\u0027intérêt.\nCet article s\u0027organise de la façon suivante. La section 2 définit la notion de groupes atypiques et la section 3 présente la mesure utilisée pour extraire de tels groupes. La section 4 expose deux critères qui vont permettre d\u0027obtenir des groupes atypiques réellement intéres-sants, critères qui seront ensuite utilisés dans la section 5 lors de la présentation de l\u0027algorithme pour réduire la complexité du problème. La section 6 évalue l\u0027approche retenue sur une base de données standard : la base IPUMS du bureau de recensement américain. Nous terminons par une conclusion et des perspectives.\nGroupes atypiques\nDans cette section, nous allons définir la notion de groupes atypiques, c\u0027est-à-dire des groupes qui sont étonnamment sur-représentés ou au contraire étonnamment sous-représentés dans une zone de la variable cible définie par l\u0027utilisateur.\nIl nous a semblé intéressant de laisser à l\u0027utilisateur la possibilité d\u0027étudier cette variable quantitative cible dans une zone ou sur un intervalle particulier pour plusieurs raisons. \nSoit M un motif ou une conjonction de variables. Nous travaillons sur deux types de motifs : les motifs qualitatifs X, c\u0027est-à-dire les motifs composés uniquement de variables qualitatives ayant subi un codage disjonctif complet (i.e. Dans le calcul du support d\u0027un motif M , le même poids est attribué à chacun des individus vérifiant le motif. Nous nous intéressons aux associations ciblées ZM, dans lesquelles aucune transformation n\u0027a été effectuée sur Z. Nous souhaitons connaître les catégories d\u0027individus qui ont un support étonnamment élevé, ou au contraire étonnamment faible, pour les deux zones de l\u0027intervalle de Z : les faibles et les fortes valeurs de cet intervalle. Pour cela, nous attribuons un poids aux individus, poids qui sera d\u0027autant plus important que l\u0027individu est en adéquation avec le critère recherché, comme par exemple ?toucher RNTI -X -un fort salaire?, donc attribution d\u0027un poids plus important aux individus ayant de fortes valeurs pour la variable ?salaire?. Nous sommes donc amenés à définir deux nouveaux supports : le support positif qui se focalise sur les fortes valeurs de l\u0027intervalle de Z et le support négatif qui se focalise sur les faibles valeurs de l\u0027intervalle de Z.   Nous allons maintenant exposer la mesure utilisée pour extraire ces groupes atypiques. Pour cela, nous avons utilisé une mesure existante : l\u0027intensité d\u0027inclination.\n? ?\n? ?\nMesure d\u0027extraction des groupes atypiques\nDans un premier temps, nous exposons la mesure existante, l\u0027intensité d\u0027inclination (Guillaume, 2002), et ensuite, nous allons voir comment nous l\u0027avons utilisée afin d\u0027extraire les groupes atypiques. \nIntensité d\u0027inclination\nSi la probabilité Pr(T ? t o ) d\u0027avoir un nombre inférieur ou égal à t o est élevée, nous pouvons en conclure que t o n\u0027est pas significativement faible car pouvant se produire assez fré-quemment.\nAfin de mesurer la ?petitesse? de cet écart de façon croissante, l\u0027indice ?(Z,Y) \u003d Pr(T \u003e t o ) est retenu. Ainsi, la faiblesse de cet écart est admissible au niveau de  Le tableau 1 résume l\u0027utilisation de l\u0027intensité d\u0027inclination pour détecter les groupes atypiques positifs et négatifs.\nDétection des groupes atypiques\nGroupe atypique négatif\nGroupe atypique positif supAbs(ZXY-,+)\nTAB. 1 -Utilisation de l\u0027intensité d\u0027inclination pour détecter les groupes atypiques.\nGroupes atypiques intéressants\nnégatifs) G n\u0027est présent de façon exclusive (ou respectivement fortement absent) dans cette même zone, c\u0027est-à-dire n\u0027a de support supérieur (ou respectivement inférieur) à un certain seuil :\nGroupe négatif :\nSelon la zone étudiée, nous avons deux supports possibles : supAbs(ZM,-) pour la zone des faibles valeurs de l\u0027intervalle cible (i.e. Après avoir défini ces deux critères qui vont nous permettre d\u0027obtenir des groupes inté-ressants, nous allons voir comment nous les avons utilisés afin de réduire l\u0027espace de recherche des motifs.\nAlgorithme\nDans cette section, nous présentons l\u0027algorithme dans le cas de la détection des groupes atypiques intéressants positifs pour la zone des fortes valeurs de l\u0027intervalle cible. Nous pouvons transposer sans difficulté cette recherche au cas des groupes négatifs pour toute zone de l\u0027intervalle ainsi que pour la zone des faibles valeurs dans le cas de groupes positifs.\nL\u0027algorithme d\u0027extraction des groupes atypiques positifs présenté en figure 1 est basé sur Apriori  et plus particulièrement sur la première partie de cet algorithme, à savoir l\u0027extraction des motifs fréquents. Notre algorithme prend en entrée une table de données dans laquelle les variables qualitatives ont subi un codage disjonctif complet, une variable quantitative cible Z ainsi que la zone de l\u0027intervalle d\u0027étude [z 1 , z 2 ], le risque de première espèce, un support minimum et un support maximum, et retourne l\u0027ensemble des groupes atypiques positifs intéressants pour la zone concernée.\nComme l\u0027algorithme Apriori, notre algorithme effectue un parcours par niveau du treillis de l\u0027ensemble des parties de l\u0027ensemble des motifs de taille 1 (i.e. motifs composés d\u0027une seule variable) et utilise l\u0027anti-monotonie du support ainsi que la propriété 1 présentée cidessous afin d\u0027effectuer des coupures. La complexité de notre algorithme est identique à celle de l\u0027algorithme Apriori, c\u0027est-à-dire linéaire en nombre de passes sur la table de données mais exponentielle par rapport au nombre de motifs de taille 1.\nCette propriété 1 utilise le critère 2 présenté dans la section 4, qui nous indique que si un motif M est très fréquent (ou respectivement très peu fréquent) pour une zone z donnée alors tout sur-motif M\u0027 conduira à l\u0027obtention d\u0027un groupe G\u0027 non informatif. Propriété 1.\nL\u0027algorithme commence par rechercher les groupes atypiques positifs les plus généraux c\u0027est-à-dire les groupes dont le motif associé est composé d\u0027une seule variable (étapes 1 à 3). L\u0027étape 1 recherche les motifs qualitatifs fréquents. L\u0027étape 2 recherche les groupes atypiques positifs parmi les motifs qualitatifs fréquents et les variables quantitatives. L\u0027étape 3 détecte les motifs fortement fréquents (critère 2 de la section 4) qui seront ensuite éliminés de l\u0027ensemble des motifs fréquents servant à générer les motifs candidats. \nFIG. 1 -Algorithme d\u0027extraction des groupes atypiques positifs intéressants pour la zone des fortes valeurs de l\u0027intervalle de la variable cible quantitative.\nEnsuite, les étapes 4 à 7 vont extraire les sous-groupes atypiques positifs intéressants et ces quatre étapes vont être répétées pour chaque niveau k (un niveau k correspond à la recherche des motifs composés de k variables) jusqu\u0027à ce que l\u0027ensemble des motifs qualitatifs fréquents de niveau inférieur soit non vide (car nous ne pourrions pas générer de motifs candidats). L\u0027étape 4 génère à la fois : (1) des motifs qualitatifs candidats (suivant le même principe que l\u0027algorithme Apriori) à partir des motifs fréquents de niveau inférieur privé des motifs fortement présents, et (2) des motifs quantitatifs candidats composés d\u0027une variable RNTI -X -quantitative non fortement présente et d\u0027un motif qualitatif fréquent de niveau inférieur non fortement présent également. L\u0027étape 5 calcule les motifs qualitatifs fréquents à partir des motifs candidats trouvés à l\u0027étape 4. L\u0027étape 6 extrait les groupes atypiques positifs intéres-sants à partir des motifs fréquents. L\u0027étape 7 détecte les motifs fortement présents parmi les motifs qualitatifs afin de les supprimer de l\u0027ensemble des motifs qualitatifs fréquents qui vont servir à générer les candidats de niveau supérieur.\nL\u0027algorithme a été implémenté en Java et intégré au logiciel libre d\u0027extraction de connaissances WEKA (Waikato Environment for Knowledge Analysis) (Witten et Franck, 2005), logiciel développé par l\u0027université de Waikato en Nouvelle-Zélande.\nExpérimentations\nPour évaluer notre approche, la recherche des groupes atypiques a été effectuée sur la base de données IPUMS (Integrated Public Use Microdata Series), données du bureau de recensement américain. Ces données sont disponibles sur UCI KDD archive.\nCette quantitative en nous dispensant de l\u0027étape de discrétisation des variables quantitatives. Cela élimine les erreurs liées à une discrétisation a priori et nous permet d\u0027avoir une vue globale de l\u0027association avec ces variables quantitatives et non plus un émiettement de la connaissance dû à cette transformation. La complexité du problème s\u0027en trouve légèrement réduite, surtout pour les bases possédant de nombreuses variables quantitatives, puisqu\u0027il n\u0027y a pas multiplicité des variables. Cette extraction des groupes atypiques prend comme référence l\u0027ensemble d\u0027apprentissage. Il pourrait être intéressant d\u0027introduire un autre paramètre : un ensemble de référence, ce qui nous permettrait de rechercher tous les groupes ayant un comportement identique ou différent de cet ensemble de référence.\n"
  },
  {
    "id": "820",
    "text": "Introduction\nIn the process of knowledge discovery from a raw data set, we first preprocess the data to remove noise and handle missing data fields. Then data transformation, such as the reduction of the number of variables and the discretization of attributes defined on a continuous domain, is often performed, which is later provided to a data mining algorithm. One of the most important and complex issues in data mining is related to the transformation process such as discretization which consists of converting numerical data into symbolic or discrete form. Kusiak [9] emphasized that the quality of knowledge discovery from a data set can be enhanced by discretization because many of the knowledge discovery techniques are very sensitive to size of data in terms of complexity. Thus, the choice of discretization technique has important consequences on the induction model used such as CART [2].\nIn addition, numerical value ranges are not easy enough for evaluation functions to handle in a nominal domain ; for example, the original versions of the popular machine learning algorithms ID3 could be used only for categorical data and Quinlan [11] had to transform continuous ones into discrete values in his C4.5 decision tree learner. Many real-world classification algorithms are hard to solve unless the continuous attributes are discretized. It is hard to determine the intervals for a discretization of numerical attributes that has an infinite number of candidates. A simple discretization procedure divides the range of a continuous variable into equal-width intervals or equal-frequency intervals. Fayyad et al. [6] suggested a class dependent algorithm which reduce the number of attributed values maintaining the relationship between the class and attribute values. Liu et al. [10] classified discretization methods from five different viewpoints : supervised vs. unsupervised, static vs dynamic, global vs local, topdown vs bottom-up, and direct vs incremental. Unsupervised methods do not make use of class information in the discretization process while supervised methods utilize it. If no class information is available, unsupervised discretization is the only method possible. Dynamic methods perform discretization of continuous values during classification process, while static methods preprocess discretization before classification process. Local methods use the local region of the instance space while global methods use the entire space. Top-down methods as FUSBIN, MDLPC and CONTRAST [5][6][7] start with one interval and split intervals in the process of discretization and are based mostly on binarization within a subset of training data. While, bottom-up methods like FUSINTER [5] and Chi-Merge [4] split completely all the continuous values of the attribute and merge intervals in the process of discretization. In this article, we focus on these two types of strategies in determining better discretization points and providing comparisons in terms of quality and prediction rates [1].\nOur goal is find a way to produce better discretization points. Previously, various studies have been done to estimate the discretization points from samples. Significantly, in [1], a set of learning samples are used to approximate the best discretization points of the whole population, but also argue that the learning sample is an approximation of the whole population, so the optimal solution built on a single sample set is not necessarily the global one. This interpretation leads us to use a resampling approach [3] to determine better distributions of the discretization points, where each point has a probability to be the exact discretization point towards the whole population. By doing so, we attempt to improve on the quality of discretization and better estimation of the discretization points of the entire population, thus, treating the discretization problem in the statistical area with new results. In this paper, we show that by performing resampling using bootstrap [8] we determine a better estimate of discretization point distribution over the entire population, which is shown improving the prediction rate of the achieved discretization. Moreover, we further improve on the quality and mean prediction rate obtained from resampling by applying a discretization point selection protocol. This protocol selects the cut points according to some criteria (e.g. entropy) from the resampling bootstrap frequency point distribution obtained from resampling n times and improves further on the prediction rate. Furthermore, we compare the prediction rates of different top-down and bottom-up strategies by using resampling. In section 2, we lay out the framework for discretization and define the data sets used in our calculations. In 3, we give an illustration of our work and results by applying the methodology to an example data set and then to a much more detailed, Breiman\u0027s wave dataset [2]. We also compare several top-down and bottom-up strategy based criteria as in [1], such as Chi-merge based on ? 2 Statistical Law, FUSBIN and FUSIN-TER based on the uncertainty principle, MDLPC based on information gain and CONTRAST that takes into account the homogeneity of the classes and also the point density. In the end we conclude with observations, deductions and proposals for future work.\nDefinitions and Notations\nFramework and Formulation : Let X(.) be an attribute value on the real line For each example ? of a learning set ?, X(?) is the value taken by the attribute X(.) at ?. The attribute C(.) is called the endogenous variable or class and is usually symbolic and if an example belongs to a class c, we have C(?) \u003d c. We also suppose that C(?) is known for all ? of the learning sample set ?. Thus, we try to build a model, denoted by ?, such that ideally we have : C(.) \u003d ?(X 1 (.), ..., X p (.))). The discretization of X(.) consists in splitting the domain D x of continuous attribute X(.), into k intervals I j , j \u003d 1, ...., k, with k ? 1. We denote\n.).\nPrediction Rate : We measure the quality of discretization by taking into account the prediction rate, which is calculated as follows :\nWe denote by ? js the good prediction rate resulting from the discretization of X j obtained by applying the method q on the sample ? s or ? jt by applying on the test sample ? t .\nData Set : In this article, we use two different data sets. First, we use a small data set of 110 individuals corresponding to a two-class problem shown in figure 1. The second large data set used for comparisons and results is the Breiman\u0027s waveform dataset [2] having 4590 individuals and 21 attributes X(.), that correspond to a three class problem.\nFIG. 1 -Runs R i and boundary points d j for a sample of 2 classes \"x\" and \"o\".\nResults and Comparisons\nIllustration using Example Data of Figure 1\nConsider a data set of figure 1 of 110 individuals having two classes. We perform FUSBIN discretization with ? \u003d 0.91 on each random and bootstrap sample of size 30 and generate 500 samples. Figure 2 gives us the discretization point distribution from 500 bootstrap and random samples. We see that the discretization achieved from bootstrap is seem to be a little more generalized and well defined over the four small intervals ; 4.5 to 6, 6.5 to 9, 12.5 to 14.5 and 22.5 to 27. While, in random sampling, the point distribution does seem to be poorly defined in a large region of values from 18 to 27. We further argue that this difference increases as the data set becomes larger which we shall see in with Breiman\u0027s data set. We also calculated the mean prediction rate i.e. Next, we improve the quality and prediction rate by introducing a notion of discretization point selection protocol. This protocol selects the discretization points from a given point frequency distribution, having higher probability of occurrence, and splits on those points if a certain criterion (e.g. entropy) is met. To illustrate, from figure 2 we see that the highest probable point is 25.5 ; so we take that point and split the population if a certain criterion (FUSBIN entropy) is met. We continue our process on the obtained splits in a top-down manner, until the criterion allows further splitting or all the points from the frequency distribution have been already chosen. We applied this protocol on both the bootstrap and random samples and it selected 6 out of 30 and 8 out of 36 discretization points from both the frequency point distributions respectively. We calculated the prediction rate as 22 for bootstrap and 19.5 for random sampling, demonstrating the better quality of discretization achieved by selection from bootstrap. We further argue that sampling gives us a lot of variation in prediction rates i.e. for bootstrap samples the prediction rate varies from 17 to 26 and thus, it is difficult to obtain a generalized estimate of the discretization points of original population. Here, our protocol achieves well defined discretization points and thus, give better estimate of the original discretization points.\nAnalysis and Results using Breiman\u0027s Waveform Data\nFor this section we use the Breiman\u0027s waves data set. We generated 100 bootstrap and random samples ? b and ? s ; s \u003d 1,...,100 of 300 points each and ? t a test sample of 4590 points. For any ? taken from the sample, we have a vector of 21 components denoted as (X 1 (?), ..., X 2 1(?)) and a label C(?). We repeated the process described above with the waveform data set. We took each variable from the data set and generated bootstrap and random samples as above. Then, we performed FUSBIN on both the 100 bootstrap and random samples and obtained mean prediction rates of 196 and 180 respectively, showing a better performance with bootstrap sampling. Then, we applied our discretization point selection protocol on the point distribution obtained and selected the best points (using FUSBIN criterion) from both sampling methods. We found a prediction rate of 309 for points obtained from bootstrap distribution and a lesser value of 271 for random sampling showing a significant amount of improvement in the prediction rate by using resampling (or bootstrapping).\nFinally we compare FUSINTER, FUSBIN, CONTRAST, MDLPC and Chi-Merge by resampling. This is done in two by two according to the following procedure ; Let u and v be the two methods to compare. First, we obtain discretization points from 100 bootstrap samples and create a frequency point distribution for each variable. Then, using our selection protocol, we select discretization points from those point distribution frequencies, by applying the criterion of the respective method (from which the initial discretization points were obtained). We then compute prediction rates ? j t of the selected discretization points from each method in relation to the whole test sample ? t . We form the difference ? uv of the two prediction rates obtained and conclude that u is better than v if ? uv is significantly superior to 0. Table 1 presents the comparison in terms of the difference of the means µ uv of prediction rates from all the variables. Positive values of µ uv indicate that the method in the row is better than the method in the column. Aside from Chi-Merge method whose results are relatively poor, all the other methods have relatively smaller differences. However, among those methods MDLPC seemed to be the best with a much lesser time complexity. FUSBIN and FUSINTER also had a smaller time complexity in comparison to CONTRAST which had a quadratic complexity which had to be taken into account when the number of examples becomes too high.\nConclusion\nThe learning sample is an approximation of the whole population, so the optimal discretization built on a single sample set is not necessarily the global optimal one. Resampling gives a better estimate of the discretization point distribution in terms of achieving a well-defined distribution. Applying our discretization point selection protocol on the frequency distribution achieved by resampling, significantly improves the quality of discretization and prediction rate and thus, nearing to a global optimal solution. Moreover, the same protocol when applied to the frequency point distribution of random samples, achieved much lesser improvements in the prediction rate as compared to bootstrap. We applied our protocol (after resampling) to various methods. Except for Chi-Merge, all the other methods provide small variations in terms of prediction rates. MDLPC performs the best and FUSBIN achieves the best time complexity, which is a key point when dealing with a lot of examples. As future work, we shall apply this discretization approach in the context of decision trees, to see whether it improves the global performance or not. But, at the same time carrying out this approach needs to answer some other questions such as time complexity. This may lead also to apply the potential discretization points in the context of fuzzy or soft discretization [12] in decision trees.\n"
  },
  {
    "id": "822",
    "text": "Introduction\nLa problématique de l\u0027extraction de motifs séquentiels dans de grandes bases de données intéresse la communauté fouille de données depuis une dizaine d\u0027années et différentes mé-thodes ont été développées pour extraire des séquences fréquentes. L\u0027extraction de tels motifs est toutefois une tâche difficile car l\u0027espace de recherche considéré est très grand. Afin de gérer au mieux cet espace de recherche, différentes stratégies ont été proposées. Les plus traditionnelles utilisent une approche à la Apriori Srikant et Agrawal (1996) et diffèrent principalement par les structures de données utilisées (vecteurs de bits, arbres préfixés, ...). Les approches les plus récentes considèrent, quant à elles, des projections multiples de la base de données selon le principe de pattern-growth proposé dans Pei et al. (2001) et évitent ainsi de générer des candidats. Outre ces différentes stratégies, les propositions les plus efficaces considèrent comme hypothèse que la base de données peut être chargée directement en mémoire centrale. Cependant, avec le développement des nouvelles technologies, ces dernières se trouvent de plus en plus mises en défaut dans la mesure où la quantité de données manipulées est trop volumineuse et qu\u0027il devient irréaliste de stocker l\u0027intégralité de la base en mémoire centrale.\nLe développement des nouvelles technologies permet également de générer de très grands volumes de données issues de différentes sources : trafic TCP/IP, transactions financières, en-registrements médicaux, capteurs. Les données apparaissent alors sous la forme d\u0027un flot (data stream) de manière continue, à un rythme rapide et éventuellement de manière infinie. L\u0027extraction de connaissances à partir de tels flots a récemment donné lieu à de nombreux travaux de recherche qui se sont focalisés sur la découverte d\u0027itemsets fréquents (e.g., Chi et al. (2004); Giannella et al. (2003); Manku et Motwani (2002)) en utilisant des méthodes telles que le landmark, la fenêtre glissante ou les modèles de pondérations temporelles et peu de travaux se sont intéressés à l\u0027extraction de motifs séquentiels dans les flots de données Raissi et al. (2006). Il est vrai qu\u0027outre l\u0027espace de recherche, les approches traditionnelles nécessitent de faire plusieurs passes sur la base ou de stocker cette dernière en mémoire.\nLe reste de l\u0027article est organisé de la manière suivante. Les travaux liés à l\u0027extraction de motifs séquentiels sont présentés dans la section 2. Dans la section 3, nous introduisons plus formellement le problème et les concepts préliminaires de l\u0027extraction de motifs. La section 4 présente l\u0027échantillonnage dans le cadre d\u0027une base de données statique avec les résultats théo-riques sur la précision de l\u0027échantillon et du seuil d\u0027erreur. La section 5 étend ces résultats aux flots de données en présentant notre algorithme de maintien de synopsis. Les expérimentations sont décrites dans la section 6 et une conclusion est proposée dans la section 7.\nTravaux antérieurs\nDifférentes approches efficaces d\u0027extraction de séquences ont été proposées ces dernières années (e.g. PrefixSpan Pei et al. (2001), SPADE Zaki (2001), SPAM Ayres et al. (2002)). PrefixSpan est un algorithme basé sur la représentation de motifs au moyen d\u0027un arbre préfixé qui dans son implémentation publique charge la base de données en mémoire centrale et utilise différentes projections afin d\u0027éviter de générer des candidats. SPADE propose de transformer la base de données en une représentation verticale afin d\u0027appliquer rapidement des opérations de jointure. Dans le cas de SPAM l\u0027originalité réside dans la représentation des données sous la forme de bitmaps. Les expérimentations montrent que SPAM est plus efficace que PrefixSpan et SPADE sur de grandes bases de données. Cependant, ces trois algorithmes sont mis en défaut lorsqu\u0027ils essayent de charger des bases trop volumineuses, de l\u0027ordre de quelques gigaoctets.\nLe volume de données important et croissant dans les bases de données statiques ou dans les flots de données impose de nouvelles contraintes à prendre en compte par les algorithmes d\u0027extraction. Dans ce cas, il devient acceptable d\u0027obtenir des réponses avec des approximations Aggarwal (2007). En d\u0027autres termes, il devient indispensable de trouver un équilibre entre l\u0027efficacité et la précision des résultats. De nombreuses structures de synopsis ont été développées ces dernières années comme les sketches, l\u0027échantillonnage, les wavelets et les histogrammes. Toutes ces différentes structures possèdent les mêmes propriétés de grandes applicabilités (elles peuvent être utilisées pour répondre à divers problèmes), d\u0027efficacité mé-moire (elles sont capables de résumer de manière significative de grandes quantité de données) et de robustesse. En outre, toutes ces méthodes ne nécessitent qu\u0027une passe sur la base de données ce qui les rend particulièrement utiles dans le cas des flots de données. Dans les travaux que nous avons menés, nous nous sommes focalisés sur les classes d\u0027échantillonnage par ré-servoir dans la mesure où elles sont aisées à mettre en oeuvre et garantissent un échantillon représentatif de bonne qualité. Cette approche a initialement été introduite dans Vitter (1985). Le principe est le suivant : nous maintenons un réservoir de taille n, les premiers n points dans l\u0027ensemble des données sont stockés lors de l\u0027étape d\u0027initialisation. Lorsque le (t+1)\nFIG. 1 -Une base de données exemple D point est traité, il remplace aléatoirement l\u0027un des points du réservoir avec une probabilité de n t+1 . Ainsi, plus la taille de l\u0027ensemble de données augmente plus la probabilité d\u0027inclusion se réduit. Ceci est bien entendu un inconvénient majeur dans le cas des flots de données qui considèrent souvent que les informations les plus récentes dans le flot sont les plus pertinentes Giannella et al. (2003). Une solution pour répondre à ce problème est d\u0027utiliser des fonctions biaisées pour réguler l\u0027échantillon du flot de données Aggarwal (2006).\nConcepts préliminaires\nDans cette section nous présentons la problématique de l\u0027extraction des motifs séquentiels Srikant et Agrawal (1996); Srikant (1995)  \nL\u0027échantillonnage par réservoir biaisé fut introduit dans Aggarwal (2006). L\u0027idée principale est de réguler l\u0027introduction des points dans le réservoir et ce afin de maîtriser la fraîcheur de l\u0027échantillon produit. En d\u0027autres termes, la fonction de biais permet de moduler l\u0027échantillon de manière à se focaliser sur des comportements récents ou anciens en fonction des contraintes de l\u0027application. La fonction de biais est définie de la manière suivante : f (r, t) \u003d e ?(t?r) où le paramètre ? correspond au taux de biais. Cette fonction est proportionnelle à p(r, t) avec r \u003c t qui est la probabilité qu\u0027un point introduit dans le réservoir à l\u0027instant r soit encore présent à l\u0027instant t. En outre, l\u0027inclusion d\u0027une fonction de biais exponentielle rend possible l\u0027utilisation d\u0027algorithmes de remplacement simples et surtout cette classe spéciale de fonctions de biais implique aussi une borne supérieure sur la taille du réservoir qui est indépendante de la longueur du flot. Pour un flot de longueur t, soit R(t) la taille maximale du réservoir qui satisfait la fonction de biais exponentielle, nous avons R(t) ? 1 ? .\nEchantillonnage et base de données statique\nEtant donné que l\u0027un des facteurs clés pour l\u0027extraction est la taille de la base considérée, l\u0027intuition sous-jacente est que l\u0027algorithme d\u0027extraction pourrait être lancé sur un échantillon de la base de données originale afin d\u0027avoir des résultats de manière plus rapide et plus facile.\nLa première question à laquelle nous devons répondre si nous voulons extraire des motifs à partir d\u0027un échantillon est : à quel point l\u0027échantillon est-il pertinent par rapport au jeu de données original ? Nous répondons à cette question en exhibant une garantie sur le taux d\u0027erreur du support d\u0027une séquence. Notons qu\u0027une approche similaire a été proposée pour l\u0027extraction d\u0027itemsets fréquents dans Toivonen (1996). \nPosons X i,s une variable aléatoire indépendante définie telle que :\nreprésente le nombre de clients supportant la séquence s présents dans l\u0027échan-tillon S D . Ces clients peuvent être réécrits de la manière suivante :\nNous voulons estimer la probabilité que le taux d\u0027erreur e(s,\nPour répondre à cette estimation, nous utilisons une méthode connue en statistiques : les inégalités de concentration (et plus précisément les inégalités de Hoeffding ;Hoeffding (1963)) qui permettent de borner la valeur réelle d\u0027une variable aléatoire par rapport à son espérance et un terme d\u0027erreur. Le théorème suivant exhibe une borne inférieure sur la taille de l\u0027échantillon (ou réservoir) :\nL\u0027inégalité de Hoeffding énonce que pour n variables aléatoires indépendantes, X 1 , X 2 , . . . , X n tel que \nMotifs séquentiels, échantillonnage et flots de données\nUn des problèmes majeurs qui rend la pratique de l\u0027échantillonnage difficile sur les flots est que l\u0027on ne sait pas à l\u0027avance la taille du flot. Il faut donc développer des algorithmes d\u0027échantillonnages dynamiques qui prennent en compte l\u0027évolution et les changements dans la distribution des données transitant sur le flot. Dans cette section, nous étendons les résultats précédents et présentons un algorithme de maintien d\u0027échantillon (biaisé ou non) de manière dynamique et qui prend en compte les différentes évolutions du flot de données. L\u0027algorithme présenté peut être vu comme une étape de pré-traitement nécessaire afin de permettre l\u0027extraction de séquences fréquentes. Afin que cette étape de pré-traitement soit pertinente, elle doit respecter les conditions suivantes : (i) L\u0027échantillon doit avoir une borne inférieure sur sa taille afin de minimiser le taux d\u0027erreur absolu en terme d\u0027estimation du support. (ii) A cause de la nature même des séquences, les opérations d\u0027insertions et d\u0027enlèvements, nécessaires pour la mise à jour de l\u0027échantillon, doivent se faire au niveau des clients, mais aussi de leurs itemsets. Pour s\u0027en convaincre, il suffit de considérer un ensemble fini de clients avec pour chacun d\u0027eux un grand nombre d\u0027itemsets qui se rajoutent à chaque instant t. Cet ensemble ne peut bien sûr pas être considéré comme un échantillon ou un réservoir car il n\u0027est pas borné et ne fait qu\u0027augmenter avec le flot.\nDans notre modèle de flot de données, un point de données apparaissant à chaque instant t est défini comme un couple constitué d\u0027un identifiant de client et d\u0027une transaction.\nPartant de ces contraintes et des résultats théoriques obtenus dans la section 4, nous proposons un algorithme simple de remplacement issu de l\u0027approche de réservoir biaisé proposé dans Aggarwal (2006) qui permet de réguler l\u0027échantillonnage des transactions des clients sur le flot grâce à une fonction de biaisage temporelle exponentielle. L\u0027idée est la suivante : nous commençons avec un réservoir vide, de capacité maximale 1 ? (nous discutons un peu plus tard de la valeur du taux de biais ?) et chaque itemset d\u0027un client apparaissant sur le flot est inséré de manière probabiliste dans le réservoir après une opération de lancer de pièce : soit par un remplacement des itemsets d\u0027un client déjà présent dans le réservoir, soit par un ajout direct dans une des places encore vacantes. Comme discuté précédemment, nous devons aussi bien contrôler la taille du réservoir en terme de nombre de clients qu\u0027en nombre d\u0027itemsets. Cette opération de contrôle est appliquée grâce à une approche de fenêtre glissante (e.g. Aggarwal (2007), Babu et Widom (2001)) qui permet de garder uniquement les itemsets les plus récents pour un client donné dans le réservoir. Une fenêtre glissante peut être définie soit comme une fenêtre basée sur les séquences de taille k, contenant les k points les plus récents apparus sur le flot, soit comme une fenêtre basée sur un intervalle de temps de taille t contenant tous les points apparus sur le flot sur une durée de temps t. Dans notre approche nous utilisons des fenêtres glissantes basées sur des séquences afin de garder uniquement les transactions les plus récentes pour les clients présents dans l\u0027échantillon. Ce type de fenêtre glissante permet l\u0027extraction de séquences sur un horizon récent du flot. De plus, la fonction exponentielle de biais permet à l\u0027utilisateur de choisir la taille de son réservoir (avec des contraintes sur l\u0027(?, ?)-approximation) et ainsi, un échantillon représentatif du flot peut être construit et mis à jour en mémoire selon les besoins de l\u0027application et de l\u0027utilisateur. Le corollaire suivant, issu du théorème 1 exhibe le lien qui existe entre le taux de biais ? et les seuils d\u0027erreurs ? et ? :\nCorollaire 1 Soient ? le taux de bias, ? le seuil d\u0027erreur et ? la probabilité maximale telle que e(s, S D ) \u003e ?, alors :\nPreuve 2 D\u0027après Aggarwal (2006), pour un flot de taille t, supposons R(t) la taille maximale possible du réservoir qui satisfait la fonction de biaisage exponentielle, on a alors par définition : R(t) ? 1 ? . Nous considérons le réservoir entier comme l\u0027échantillon cible pour l\u0027extraction de motifs, on a donc : R(t) \u003d |S D |. Par substitution dans le théorème 1, nous avons le résultat énoncé. 2\nLa table 2 montre quelques valeurs du taux de biaisage ? et la taille minimale du réservoir nécessaire pour la bonne approximation du support des séquences. 2. Voir si le client C i est déjà dans le réservoir, si oui, rajouter l\u0027itemset dans la fenêtre et voir s\u0027il y a un décalage nécessaire à faire sinon passer à l\u0027étape 3.\n3. Faire un lancer de pièce (tir aléatoire) en cas de succès remplacer un des clients déjà présent dans le réservoir de manière aléatoire (le client remplacé est alors mis dans la liste noire). En cas d\u0027échec, rajouter le client et son itemset dans le réservoir sans rien remplacer.\nLa partie la plus importante dans l\u0027algorithme est la gestion des décalages des fenêtres glissantes et la mise à jour de la liste noire. Un problème peut apparaître lors de l\u0027étape de remplacement des clients présents dans le réservoir : nous devons détecter si un client était déjà présent dans le réservoir, car le réintroduire sans aucun test préalable rendrait les résultats de l\u0027extraction des motifs séquentiels inconsistants avec la réalité du flot. Le problème d\u0027inconsistance apparaît lorsqu\u0027un client est remplacé par un autre et qu\u0027il revient dans le réservoir à l\u0027instant suivant. Au moment de l\u0027extraction, ce client n\u0027aura pas tous les itemsets qu\u0027il aurait dû avoir dans la fenêtre glissante actuelle. Les points de certains clients doivent donc être ignorés. Mais d\u0027un autre côté, ignorer des points sur le flot peut introduire un nouveau biais, puisque seuls les clients ayant remplacé d\u0027autres clients seront présents dans l\u0027échantillon. Afin de résoudre ce problème d\u0027inconsistance entre les itemsets des différents clients nous introduisons un système de liste noire qui permet d\u0027interdire l\u0027échantillonnage à un certain nombre de clients indésirables. Cette liste noire n\u0027est pas irréversible et est réactualisée à chaque glissement de la fenêtre. Notons que cet algorithme peut être très facilement implémenté. Cependant, dans la section 5 nous avons supposé implicitement que l\u0027algorithme 1 construisait un réservoir biaisé respectant la fonction de biaisage exponentielle avec le paramètre ?. Nous allons maintenant présenter une preuve formelle de cette proposition. Comme dans Aggarwal (2006), nous allons démontrer que la politique de remplacement appliquée dans notre algorithme permet de construire un réservoir biaisé de taille\nProposition 1 L\u0027algorithme 1 construit un réservoir biaisé respectant la fonction de biaisage temporel f (r, t) \u003d e ??(t?r) avec ? \u003d 1 SD .\nPreuve 3 Soient K le nombre total de clients, q t le nombre de points déjà insérés dans le réservoir à l\u0027instant t et b t la taille de la liste noire à l\u0027instant t. Posons n \u003d |S D |, le nombre possible de points dans l\u0027échantillon. La probabilité que le client arrivant sur le flot soit dans la liste noire est bi K et la probabilité que le client soit déjà dans le réservoir est qi K . De plus, la probabilité que le lancer de pièce soit un succès après le test d\u0027appartenance à la liste noire est qi n , la probabilité dans le cas d\u0027un lancer fructueux est 1 qi donc la probabilité qu\u0027un point dans le réservoir soit éjecté à l\u0027instant donné i est qi 1 1 n × qi \u003d n (la probabilité qu\u0027un point soit encore à l\u0027instant donné i dans le réservoir est alors (1 ? 1 n ), étape 2 de l\u0027algorithme). De plus, sachant que r \u003c t nous devons calculer toutes les combinaisons possibles (r, t) avec t fixé de la probabilité qu\u0027un client inséré à l\u0027instant r soit encore dans le réservoir à l\u0027instant t :\nPour de grandes valeurs de n, 1 ? 1 n est approximativement égal à e ?1 . Par substitution dans l\u0027équation 2, nous avons le résultat énoncé.  Pei et al. (2001), qui permet l\u0027extraction des motifs séquentiels. Les tests ont été lancés sur plusieurs jeux de données synthétiques qui sont générés par le logiciel QUEST 1 . Les différents jeux de données utilisés dans ces expérimentations et leurs caractéristiques sont regroupés dans le tableau 3. Afin d\u0027échantillonner les bases de données statiques nous avons implémenté l\u0027algorithme d\u0027échantillonnage par réservoir de Vitter (1985).\nExpérimentations\nNous avons testé la pertinence de nos calculs théoriques pour l\u0027échantillonnage des bases de données statiques (section 4). Nous avons utilisé dans nos expérimentations les jeux de données CL6MTR2.5SL10IT20K, CL1MTR10SL20IT10K et CL0.5MTR20SL20IT10K afin de comparer les résultats aux estimations de l\u0027(?, ?)-approximation du théorème 1. Les résul-tats des différentes expérimentations sont listés dans les tableaux 4 et 5. Nous avons construit des échantillons de différentes tailles allant de 25000 à 500000 clients en utilisant l\u0027approche d\u0027échantillonnage par réservoir. Les expérimentations ont été répétées 5 fois pour chaque échantillon et les valeurs présentées sont les moyennes des résultats. La colonne Erreur dans les tableaux décrit le nombre de séquences extraites dont le support dépasse nos (?, ?)-approximations. On remarque que l\u0027échantillonnage sur les bases de données statiques reste très précis, comme cela a été présenté dans les résultats théoriques de la section 4 et ce même avec de très petites tailles de résumés. Le temps d\u0027extraction de motifs et l\u0027utilisation mémoire pour ce processus est diminué de plusieurs ordres de magnitudes ce qui permet de pousser le processus global d\u0027extraction vers des supports très bas. L\u0027expérimentation sur les flots de données utilise le jeu de données CL1MTR2.5SL50IT10K en faisant varier différents paramètres : la valeurs de biais (?) et la taille de la fenêtre glissante. Dans ces expérimentations nous mettons en valeur l\u0027efficacité de l\u0027échantillonnage et montrons empiriquement que la liste noire reste bornée dans le temps. La figure 2 montre que la liste noire, qui est la garante de la consistence de l\u0027échantillon pour le processus d\u0027extraction de motifs reste assez limitée en terme d\u0027espace mémoire utilisé et ce grâce aux différentes mises à jours faites à chaque décalage des fenêtres glissantes des clients présents dans le réservoir. De plus, si la taille de la fenêtre glissante est petite, la liste noire tend à être très petite aussi, ceci est du aux fréquents décalages qui permettent de mettre à jour les clients ignorés dans le réservoir. La figure 2 montre que le réservoir se remplit très vite, l\u0027étape d\u0027extraction de motifs séquentiels peut donc être lancé à partir de la 50 eme seconde. De plus, comme on peut le voir sur la figure 3, le réservoir reste borné et stable en terme d\u0027occupation mémoire.\nConclusion\nDans cet article, nous nous sommes intéressés à de nouvelles techniques de résumés pour représenter des bases de données de motifs séquentiels. Nous avons montré qu\u0027une approche basée sur des échantillons était tout à fait adaptée pour des bases de données statiques et que nous étions capables de maîtriser les taux d\u0027erreur dans les résultats d\u0027extraction de motifs séquentiels. A notre connaissance, ce travail est le premier à utiliser des techniques d\u0027échan-tillonnage pour extraire des motifs séquentiels dans des bases de données. Nous avons égale-ment montré qu\u0027une approche d\u0027échantillonnage basée sur des réservoirs pouvait être adapté au contexte des flots de données et avons proposé un algorithme de remplacement des éléments du réservoir qui permet de réguler l\u0027échantillonnage des transactions des clients sur le flot via une fonction de biaisage temporelle exponentielle.\nNous avons vu précédemment que les approches d\u0027extraction de motifs sur les flots néces-sitent, pour des raisons de capacités mémoire, de supprimer des connaissances acquises préa-lablement (par exemple, une séquence n\u0027est plus fréquente sur un petit intervalle de temps). Au travers de notre approche, il n\u0027est plus besoin d\u0027exécuter l\u0027algorithme d\u0027extraction en continu mais plutôt à la demande pour fournir au décideur toute la connaissance extraite du flot en lui garantissant une marge d\u0027erreur. Les perspectives associées à ces travaux sont nombreuses. Dans un premier temps, nous souhaitons offrir la possibilité de stocker les évolutions des données dans le réservoir et lors des étapes d\u0027extraction de connaissances afin de déterminer les tendances dans le flot. Pour cela, nous souhaitons étendre la notion de tilted-time window introduite dans Giannella et al. (2003) pour ne stocker que les variations de fréquences et non plus toutes les fréquences des motifs agrégées sur un intervalle de temps. Dans un second temps, nous souhaitons intégrer cette approche dans un outil complet de suivi de flots de données permettant ainsi de pouvoir effectuer non seulement de l\u0027extraction mais aussi des requêtes sur les données du flot.\n"
  },
  {
    "id": "825",
    "text": "Introduction\nLa classification recouvrante (en anglais overlapping clustering) constitue un domaine de recherche étudié depuis les années 60 et relancé par des besoins applicatifs dans des domaines importants tels que la Recherche d\u0027Information ou encore la Bioinformatique.\nLe but recherché est alors d\u0027extraire une collection de classes recouvrantes à partir d\u0027une population d\u0027individus de telle manière que : chaque individu appartienne à une ou plusieurs classes, les individus d\u0027une même classe soient similaires, et deux individus n\u0027appartenant pas au moins à une classe commune soient dissimilaires. Différentes directions ont été prospectées afin d\u0027obtenir ce type de schéma de classification.\nDes modèles hiérarchiques ont été proposés ; Jardine et Sibson (1971) ont permis, en introduisant les k-ultramétriques, d\u0027envisager des structures hiérarchiques (ou pseudo-hiérarchiques) moins contraignantes que les arbres, par exemple des pyramides (Diday (1984)) ou encore des hiérarchies dites \"faibles\" étudiées par Bertrand et Janowitz (2003) notamment. L\u0027un des avantages de ces modèles est de proposer une interprétation visuelle des classes et de leur organisation. En revanche, ces modèles ne permettent pas de prendre en compte la globalité des schémas de recouvrements possibles ; par exemple Bertrand et Janowitz (2003) montrent que dans une k-hiérarchie faible (le modèle hiérarchique le moins contraignant), \"l\u0027intersection de (k + 1) classes arbitraires peut être réduite à l\u0027intersection de k de ces classes\".\nLes approches par partitionnement proposées ont consisté dans un premier temps à déter-miner des centres, des axes ou des représentants de classes auxquels les individus sont affectés relativement à un seuil d\u0027appartenance. Il s\u0027agit des travaux de Rocchio (1966) repris par Dattola (1968) et plus récemment de la méthode des k-moyennes axiales proposée par Lelu (1994). Ces approches, tout comme l\u0027algorithme CBC (Pantel (2003)), sont motivés par le besoin de modèles spécifiques pour traiter les données textuelles mais souffrent d\u0027un problème commun récurrent que constitue la détermination du seuil (similarité ou probabilité d\u0027appartenance) qui décidera de l\u0027affectation des individus aux classes extraites. Cleuziou et al. (2004) proposent alors l\u0027algorithme POBOC pour se dégager de la contrainte du seuil ; l\u0027affectation des individus est effectuée indépendamment d\u0027un seuil fixé a priori, uniquement par l\u0027étude de la distribution de leurs proximités avec l\u0027ensemble des classes.\nLes méthodologies de partitionnement mentionnées jusqu\u0027ici s\u0027appuient implicitement sur une hypothèse forte qui considère qu\u0027un \"bon\" schéma de classification recouvrante (ou recouvrement) peut être obtenu par l\u0027extension 1 d\u0027un \"bon\" schéma de classification stricte (ou partition). D\u0027autres pourront penser que de façon analogue, un \"bon\" recouvrement peut être obtenu par restriction 2 d\u0027un \"bon\" schéma flou. La notion de \"bon\" schéma restant subjective à ce stade, Cleuziou (2007) propose un critère objectif générique de qualité d\u0027un schéma de classification (recouvrant ou non) et montre que pour ce critère, il n\u0027existe pas toujours une partition optimale qui, par extension, permettrait d\u0027aboutir à un recouvrement optimal.\nLes remarques précédentes nous amènent à considérer une nouvelle voie pour les mé-thodes de classification recouvrante : celle qui consiste à rechercher un \"bon\" schéma directement dans l\u0027ensemble des recouvrements possibles. Cette démarche a été adoptée par Banerjee et al. (2005a) et par Cleuziou (2007) dans les algorithmes MOC et OKM respectivement. MOC (Model-Based Overlapping Clustering) peut être considéré de façon simplifiée comme une gé-néralisation de la méthode EM (Dempster et al. (1977)) pour la classification recouvrante ; cette approche que nous détaillerons s\u0027appuie en effet sur les modèles de mélanges qui s\u0027avèrent être très performants pour les problématiques de classification stricte. OKM (Overlapping-kMeans) est une généralisation de l\u0027algorithme bien connu des k-moyennes (MacQueen (1967)) qui allie simplicité et rapidité pour traiter des problèmes concrets de manière efficace.\nEn notant que la variante classificatoire (CEM) de EM se ramène, sous certaines conditions restrictives (lois normales, variances sphériques et égales, proportions égales) à l\u0027algorithme des k-moyennes (Celleux et Govaert (1992)), il nous a semblé indispensable d\u0027étudier les analogies entre les deux approches recouvrantes MOC et OKM. L\u0027objectif de cet article est alors de proposer une formulation de OKM en terme de modèles de mélanges puis de la comparer théoriquement et expérimentalement à MOC. L\u0027article est organisé comme suit : les deux prochaines sections sont dédiées à la présen-tation des deux approches MOC et OKM respectivement ainsi qu\u0027à la reformulation de OKM permettant une comparaison analytique des deux modélisations. La section 4 présente une discussion sur les modèles permettant d\u0027identifier leurs principales différences et leurs consé-quences prévisibles sur les schémas de classification. Cette section sera illustrée par quelques expérimentations sur des données textuelles et biologiques et sera suivie d\u0027une synthèse de l\u0027étude permettant de dégager les principales pistes de réflexions à mener. Banerjee et al. (2005a) ont proposé récemment un modèle général pour le problème de classification recouvrante en utilisant les modèles de mélanges. Leur proposition s\u0027appuie sur les modèles probabilistes relationnels ou PRM (Friedman et al. (1999)) d\u0027une part, et sur des hypothèses de génération probabiliste des observations d\u0027autre part. Nous décrivons l\u0027essentiel de la méthode dans cette section en laissant le soin au lecteur de se reporter aux références citées pour en obtenir une présentation approfondie.\nLe modèle MOC\nUn modèle inspiré de la BioInfo\nLe modèle MOC (Model-based Overlapping Clustering) peut être vu comme une instanciation du modèle PRM permettant de modéliser les relations entre des gènes, des processus et des valeurs d\u0027expressions de ces gènes mesurées sur des puces ADN. Cette instanciation repose sur l\u0027hypothèse que le niveau d\u0027expression d\u0027un gène (observé dans une certaine condition expérimentale) dépend des processus auxquels le gène participe et de leur niveau d\u0027influence (dans cette même condition expérimentale).\nFIG. 1 -Instanciation du modèle PRM.\nLa figure 1 illustre l\u0027instanciation du modèle PRM permettant de modéliser le fait que l\u0027expression X ij d\u0027un gène X i dans une condition j dépend :\n-des niveaux d\u0027influence {A hj } h des processus {A h } (dans la condition j), -de la participation M ih (ou non) du gène i à un processus A h . Dans cet exemple on dispose de 2 gènes (données), 2 conditions (dimensions), 3 processus (classes).\nSous certaines hypothèses de distribution des observations {X ij }, la détermination des paramètres {A hj } et {M ih } s\u0027apparente à un problème de classification où les processus s\u0027identifient aux classes ; plus précisément à un problème de classification recouvrante puisque un gène peut participer naturellement à plusieurs processus différents (donc appartenir à plusieurs classes).\nHypothèses de distribution et modèles associés\nUn premier modèle classique consiste à faire l\u0027hypothèse que les observations X ij suivent des lois normales, de variances constantes ?. Selon le modèle général présenté précédemment, pour un nombre fixé de processus, les moyennes des gaussiennes associées sont déterminées par la somme des activités A hj des processus auxquels X i participe :\nLe problème de classification sera alors résolu par la recherche des paramètres M et A maximisant la vraisemblance du modèle. Segal et al. (2003) montrent que sous certaines condi-\n, le problème revient alors à minimiser l\u0027expression (2) ci-dessous par un algorithme du type EM (Dempster et al. (1977)).\nBanerjee et al. (2005a) généralisent ce modèle aux familles de distributions exponentielles en s\u0027appuyant sur le fait qu\u0027il existe une bijection entre les distributions exponentielles et les divergences de Bregman (Banerjee et al. (2005b)). Ainsi, quelque soit la loi exponentielle considérée, la distribution des observations peut s\u0027exprimer par\navec d ? la divergence de Bregman associée à la densité exponentielle choisie, en particulier : -la distance euclidienne (élevée au carré) pour des densités Gaussiennes, -la I-divergence permettant d\u0027explorer pour chaque M i un sous-ensemble des 2 k ?1 vecteurs M i possibles pour retenir la solution minimisant le critère ou à défaut conserver les anciennes appartenances.\nLa mise à jour de A peut être réalisée dans un cas général (pour toute divergence de Bregman) au moyen d\u0027un algorithme de descente de gradient de la forme :\navec ? la fonction identifiant la divergence de Bregman utilisée et ? un coefficient d\u0027apprentissage fixé. Dans les cas particulier de divergences simples qui nous intéresseront plus particulièrement dans les expérimentations à venir, le problème de minimisation peut être ré-solu plus directement.\n-pour la distance euclidienne il s\u0027agit d\u0027une minimisation de type moindres carrés résolue par (6) \n-pour la I-divergence, les auteurs s\u0027appuient sur des techniques de factorisation de matrices non-négatives pour aboutir à la règle de mise à jour suivante\nDans cette dernière variante, on peut observer que la règle de mise à jour proposée correspond à une simplification de la règle plus générale (8) réétudiée récemment par Finesso et Spreij (2006) :\nDans le cas particulier où chaque individu X i n\u0027appartient qu\u0027à une seule classe A h alors (M A) ij \u003d M ih A hj ; en ajoutant à cela le fait que la I-divergence mesure l\u0027écart entre deux distributions p et q telles que j p j \u003d j q j \u003d 1 la simplification (7) devient en effet possible ( j X ij \u003d 1). Cependant la méthode MOC s\u0027intéressant justement aux cas où chaque individu peut appartenir à plusieurs classes, cette simplification devient fausse ; c\u0027est la raison pour laquelle nous proposerons de conserver la règle de mise à jour originelle (8) afin d\u0027assurer la décroissance du critère (4).\nLes trois étapes de mises à jour que nous venons de présenter permettent d\u0027assurer la dé-croissance du critère (4) et par conséquent d\u0027améliorer à chaque itération (et après chaque étape de mise à jour) la vraisemblance du modèle probabiliste. L\u0027initialisation du modèle (matrices M et A) est effectuée au moyen d\u0027une étape de partitionnement (k-moyennes). Enfin, de façon assez classique l\u0027algorithme MOC itère le processus de mises à jour un nombre maximum de fois ou jusqu\u0027à observer une variation epsilonesque du critère objectif.\nLe modèle OKM\nL\u0027algorithme des k-moyennes présente un modèle théorique simple et intuitif, facilement appréhendé par les praticiens de domaines d\u0027application variés qui, de surcroît, apprécient gé-néralement l\u0027efficacité de cette méthode en terme de rapidité et de qualité des classes obtenues.\nL\u0027approche OKM, proposée par Cleuziou (2007), est le résultat d\u0027une volonté de répondre de manière pragmatique aux besoins applicatifs actuels, en proposant d\u0027étendre l\u0027algorithme des k-moyennes à la recherche de recouvrements plutôt que de partitions.\nCritère objectif et heuristique d\u0027optimisation\nLe critère des moindres carrés sur lequel repose l\u0027algorithme des k-moyennes est une formalisation fidèle de l\u0027objectif visé par les méthodes de partitionnement à savoir faire en sorte que deux individus d\u0027une même classe soient similaires et deux individus de classes différentes soient dissimilaires. Comme nous l\u0027avons mentionné en introduction, on peut assez naturellement considérer qu\u0027un bon recouvrement sera caractérisé par :\n-des individus similaires lorsqu\u0027ils appartiennent plutôt aux mêmes classes, -des individus dissimilaires lorsqu\u0027ils appartiennent plutôt à des classes différentes. Le critère utilisé dans OKM pour formaliser les caractéristiques précédentes introduit la notion d\u0027image (que l\u0027on notera ?(X i )) d\u0027un individu X i dans une classification I 1 , . . . , I k . L\u0027image de X i correspond à un point, dans l\u0027espace de représentation initial, et représentatif des classes auxquelles X i appartient. Par exemple, en reprenant les notations utilisées précé-demment, on pourra définir l\u0027image de X i dans un espace euclidien (R m , d) par\nDans (9), ? j (X i ) désigne la j ième composante de l\u0027image, M ih ? {0, 1} l\u0027appartenance de X i à la classe I h et A h correspond ici à la position du centre de la classe I h . Le critère objectif que l\u0027on cherchera à minimiser, évalue la qualité d\u0027un recouvrement par la variance entre les individus et leur image dans la classification :\ni Pour ?(.) bien choisie, on peut noter que ce critère se ramène exactement au critère des moindres carrés lorsque l\u0027on oblige chaque individu à n\u0027appartenir qu\u0027à une seule classe ( h M i,h \u003d 1). L\u0027heuristique de minimisation du critère objectif (10) dans OKM s\u0027apparente à l\u0027algorithme des k-moyennes. Après une étape d\u0027initialisation aléatoire des centres de classes A, deux étapes de mises à jour (de M puis de A) sont itérées jusqu\u0027à la vérification d\u0027un critère d\u0027arrêt portant sur le nombre d\u0027itérations ou la variation du critère objectif. Mise à jour de M . Pour chaque individu X i , la mise à jour de M i est réalisée en considérant qu\u0027un individu ne doit appartenir qu\u0027aux classes dont il est le plus proche au sens de la métrique choisie. Ce principe guide la construction du nouveau vecteur d\u0027appartenance : initialisation avec affectation au plus proche centre de classe puis ajout de nouvelles affectations dans l\u0027ordre de proximité des centres {A h } h avec X i tant que le critère objectif s\u0027en trouve amélioré ; le nouveau vecteur ainsi obtenu ne remplaçant le vecteur M i initial que s\u0027il permet d\u0027améliorer le critère objectif. Mise à jour de A. Cleuziou (2007) montre que pour la distance euclidienne on peut définir localement le nouveau centre A h de la classe I h de façon optimale au sens du critère objectif. \nReformulation de OKM\nEn reprenant l\u0027inspiration bioinformatique du modèle MOC, on considère qu\u0027une observation X ij est le résultat d\u0027une certaine combinaison des processus auxquels l\u0027individu X i participe. Plutôt que de choisir comme combinaison l\u0027addition des processus, nous en choisissons la moyenne. Ainsi sous l\u0027hypothèse d\u0027une distribution gaussienne (de variance constante ?) des valeurs X ij nous obtenons\navec µ un vecteur totalement défini par M tel que µ i \u003d 1/ h M ih . Dans un processus de classification, l\u0027objectif consiste par exemple à rechercher les paramètres M (matrice binaire) et A (matrice réelle) maximisant la (log-)vraisemblance des paramètres étant données les observations : log L(M, A|X) \u003d log p(X|M, A). Le modèle MOC fait l\u0027hypothèse qu\u0027il y a indépendance des observations X ij conditionnellement aux M i et A .j . Sous ces mêmes hypothèses, la vraisemblance du modèle peut se décomposer ainsi\nEn considérant à présent la log-vraisemblance et en introduisant l\u0027hypothèse de distribution gaussienne (11), on note que\nEn observant que µ i (M A) ij \u003d ?(X i ) avec ?(X i ) l\u0027image de X i telle que définie en (9), on montre que minimiser le terme ||X ? µ (10)). Nous avons donc démontré que l\u0027approche OKM, dans sa version initiale utilisant la distance euclidienne, peut être réécrite comme un modèle de mélanges recouvrant faisant l\u0027hypothèse que chacune des observations suit une lois normale dont la moyenne correspond à la moyenne des processus (ou classes) auxquelles l\u0027individu participe.\nDiscussion et analyse comparative des modèles\nNous mènerons la discussion en deux temps : nous relèverons dans un premier temps les différences majeures et les limites théoriques des deux modèles MOC et OKM ; dans un second temps nous proposerons une étude comparative expérimentale des deux approches.\nDiscussion sur les modèles\nNous avons choisi de comparer analytiquement les deux modèles en les exprimant tous les deux en terme de modèles de mélanges recouvrants, plutôt que simplement comme des techniques de réallocation dynamique minimisant un critère d\u0027inertie. On peut ainsi envisager plus facilement d\u0027extraire ultérieurement des classes de formes, volumes et orientations variées. Cependant en l\u0027état actuel des modèles, ces variations ne sont pas permises (hypothèse des variances toutes identiques) et les deux formalismes sont strictement équivalents.\nCe qui différencie les modèles MOC et OKM concerne la méthode de combinaison des \"processus\" déterminant les paramètres de la distribution d\u0027une observation : pour une distribution exponentielle, c\u0027est la moyenne de la distribution qui résulte de cette combinaison.\n-Le modèle MOC propose une combinaison par addition, justifiée par le modèle bioinformatique sous-jacent qui suppose que l\u0027expression observée d\u0027un gène est le résultat de l\"\u0027addition\" des processus dans lesquels ce gène intervient. -Le modèle OKM utilise la notion d\u0027image qui correspond effectivement à une combinaison des représentants des classes auxquelles l\u0027individu appartient. La définition de l\u0027image dépend de la métrique considérée et s\u0027exprime comme une moyenne plutôt qu\u0027une somme : moyenne arithmétique ou quadratique par exemple (cf. tableau 1)). Ce choix de combinaison n\u0027est pas anodin et peut avoir des conséquences notables sur la validité théorique du modèle d\u0027une part, sur sa sensibilité aux données d\u0027autre part.\nLa validité théorique du modèle peut être remise en cause lorsque la combinaison utilisée n\u0027est pas un endomorphisme car le modèle implique de considérer la distance entre chaque individu et la combinaison associée. Par exemple, la I-divergence permet de comparer deux\nLa combinaison peut également jouer un rôle important dans la sensibilité du modèle, notamment dans la répartition des données pour certaines hypothèses de lois de mélange. Prenons un exemple jouet composé de quatre individus décrits dans R {X 1 \u003d (1.0), X 2 \u003d (4.0), X 3 \u003d (5.0), X 6 \u003d (6.0)} avec les hypothèses suivantes : distributions gaussiennes des observations résultant de deux processus/classes (k \u003d 2). La figure 2 illustre la configuration des individus à classer et, pour chacune des deux approches MOC et OKM, la projection des paramètres A dans le même espace de description après optimisation.\nFIG. 2 -Observation du paramètre\nLa classification finale retournée par OKM s\u0027obtient simplement en affectant chaque individu X i à I 1 , I 2 ou aux deux classes selon que l\u0027individu est plus proche de A 1 , A 2 ou (A 1 + A 2 )/2 respectivement ; sur l\u0027exemple OKM retournera donc les classes I 1 \u003d {X 1 , X 2 } et I 2 \u003d {X 2 , X 3 , X 4 }. Dans l\u0027approche MOC on obtient cette fois la classification finale en comparant les distances de X i avec A 1 , A 2 et A 1 + A 2 , aboutissant ainsi aux classes\nIl suffirait sur cet exemple de recentrer les individus en zéro tout en conservant les distances entre individus inchangées pour obtenir avec MOC les mêmes classes que pour OKM.\nCet exemple nous a donc permis de mettre en évidence que MOC peut être sensible aux translations de données, en particulier sous des hypothèses de distributions gaussiennes observable, on remarque que l\u0027intersection des deux classes (points violets) est cohérente sur les données initiales (figure 3) et de nouveau injustifiées sur les données translatées (figure 4). Il est donc important de préciser qu\u0027en pratique, MOC produit des résultats intéressants sur des 5 On peut montrer que cette sensibilité est annihilée pour d\u0027autres types de distributions telles que la I-divergence grâce aux contraintes vérifiées par les individus (e.g. P j X ij \u003d 1). 6 Cette expérience a été réalisée dans le cadre du projet ANR/ARA Masse de données \"Genomic data to Graph Structure\" : http ://gd2gs.ibisc.univ-evry.fr/ données d\u0027expressions de gènes, précisément parceque ces données sont par nature centrées globalement autour de zéro 7 .\nComparaisons expérimentales\nPour achever la comparaison des deux approches de classification recouvrante étudiées, nous présentons les résultats obtenus par MOC et OKM sur une expérience de classification de documents textuels qui correspond à un domaine d\u0027applications cible. L\u0027étude est menée sur un sous-ensemble des documents du corpus Reuters, utilisé et présenté de façon détaillée par Cleuziou (2007). Ce corpus présente l\u0027intérêt que chaque document possède une ou plusieurs étiquettes de classes. Nous n\u0027utilisons pas cette information dans le processus de classification mais seulement pour évaluer la qualité des schémas de classification recouvrants générés par les méthodes. L\u0027évaluation opérée consiste à mesurer l\u0027écart entre les associations de documents connues (pré-étiquetage) et les associations effectivement retrouvées dans la classification, en utilisant les indices classiques de précision, rappel et F-mesure Nous présentons les résultats de l\u0027évaluation des classifications en utilisant la I-divergence d\u0027une part (tableau 2) et des distributions gaussiennes ou métrique euclidienne d\u0027autre part (tableau 3). Les valeurs reportées dans les tableaux correspondent à des moyennes de 10 exé-cutions de chaque méthode dans des conditions initiales identiques.\nTout d\u0027abord, nous retrouvons un phénomène connu en Recherche d\u0027Information qui est que la I-divergence donne de meilleurs résultats que la métrique euclidienne pour la classification de documents. Ceci est notamment dû au fait que la I-divergence compare des distributions de mots plutôt que des vecteurs de fréquences, réduisant ainsi les effets liés aux variations de tailles entre documents. Le second résultat attendu est de constater que la précision augmente et que le rappel diminue quand on augmente le nombre de classes ; ceci s\u0027explique par le fait que lorsque le nombre de classes augmente, le nombre de paires de documents associés diminue automatiquement.\nEnfin, en comparant les résultats obtenus par les approches MOC et OKM on observe que pour les deux métriques utilisées, OKM génère d\u0027avantage de recouvrements que MOC 9 , ce qui se traduit par un taux de rappel plus élevé sans pour autant entraîner un fléchissement de la précision (qui reste d\u0027ailleurs plutôt à l\u0027avantage de OKM). Il semblerait donc, au regard de la globalité des résultats de cette expérience que : les deux approches de classification recouvrante étudiées permettent de générer des recouvrements pertinents (comparaison avec l\u0027algorithme des k-moyennes) et que le gain obtenu soit plus net dans le cas de l\u0027approche OKM notamment en utilisant la I-divergence. Cette dernière remarque corrobore les limites théoriques énoncées précédemment concernant le modèle MOC, et en particulier sous les hypothèses de distributions gaussiennes.\nConclusion et prespectives\nDans cet article nous avons étudié deux approches de classification recouvrante : l\u0027approche MOC (Banerjee et al. (2005a)) et l\u0027approche OKM (Cleuziou (2007)). En proposant une formalisation de OKM en terme de mélange de lois, nous avons pu constater les fortes analogies qui existent entre les deux approches. Nous avons détaillé leurs différences fondamentales et relevé quelques limites théoriques concernant le modèle MOC. Ces limites sont susceptibles de produire des résultats incohérents que nous avons observés expérimentalement.\n"
  },
  {
    "id": "826",
    "text": "Introduction\nL\u0027extraction de connaissances est un processus qui permet d\u0027analyser une masses de données importante afin d\u0027en extraire des connaissances nouvelles, valides et utiles. Ces connaissances sont ensuite présentées sous différentes formes notamment sous forme de règles d\u0027association. Une règle d\u0027association (RA) (Agrawal et al. (1993)) est une implication de la forme C 1 ? C 2 , où C 1 et C 2 sont des conditions C sur les attributs de la base. Soient minsup et minconf des seuils prédéfinis. Une RA est dite forte si elle satisfait deux contraintes :\n-son support supp(C) ? minsup, avec supp(C) : nombre de transactions dans la base qui satisfont l\u0027ensemble des conditions C tel que supp(C 1 ? C 2 ) \u003d supp(C 1 ? C 2 ) ; -sa confiance conf (C 1 ? C 2 ) ? minconf , avec conf (C 1 ? C 2 ) \u003d supp(C1?C2) supp(C1) .\nDans cet article, nous nous intéressons tout particulièrement à l\u0027extraction de règles d\u0027association quantitatives (RAQ). Ce type de règles prend en considération tout type de variables, quantitatives ou catégorielles. Un certain nombre de travaux relatifs à l\u0027extraction de RAQ sont proposés dans (Srikant et Agrawal (1996), Hong et al. (1999), Aumann et Lindell (1999), Miller et Yang (1997), Lent et al. (1997), Fukuda et al. (1996), Rastogi et Shim (1999), Mata et al. (2002)). L\u0027interprétation des résultats obtenus est basée sur la sémantique choisie. Cette sémantique exprime la nature des liens existants entre les différentes variables de la base. On peut alors exprimer le lien qui maximise le nombre d\u0027éléments en commun entre deux variables. Ce type d\u0027extraction est connu sous le nom d\u0027extraction de règles d\u0027association positives : une personne qui achète du lait achète aussi du café. De la même façon, on peut exprimer le lien qui unit deux variables et qui prend en considération le changement de comportement d\u0027une variable spécifique lors de l\u0027introduction d\u0027une nouvelle variable. Cet aspect est particulièrement subtil dans le sens où il considère les variables qui ont un comportement opposé lors de leur union. Les règles extraites à partir de ce type d\u0027union sont connus sous le nom de règles d\u0027association négatives : une personne qui achète du Pepsi n\u0027achète pas de Coca Cola, ; Teng et al. (2002); Antonie et Zaane (2004); Wu et al. (2004); Savasere et al. (1998);Yuan et al. (2002); Yan et al. (2004)). Une règle d\u0027association négative (RAN ) est présentée sous l\u0027une des formes suivantes :\nDans (Teng et al. (2002)), la distance du ? 2 permet d\u0027évaluer l\u0027indépendance des variables mais sans donner aucune indication sur la dépendance. Dans (Antonie et Zaane (2004)), la mesure d\u0027inégalité de Cauchy Schwarz traduit, plus ou moins, la dépendance linéaire entre deux variables, mais reste cependant incapable de juger de la pertinence réelle de la règle d\u0027association. De plus, elle ne permet pas d\u0027exprimer l\u0027effet d\u0027influence recherché dans notre travail. ` A travers cet article, nous présentons une nouvelle méthode d\u0027extraction de règles d\u0027association quantitatives positives et négatives. Les règles visées ont une sémantique particulière qui permet, par ailleurs, de faire ressortir l\u0027effet qu\u0027une variable nommée influent pourrait avoir sur une autre nommée influé. Cet effet est exprimé par un changement significatif du comportement de l\u0027influé. Plus précisément, par une chute importante du nombre d\u0027individus dans l\u0027intersection de l\u0027association étudiée. L\u0027évaluation du degré de changement de comportement est réalisée grâce à une nouvelle mesure nommée Influence. Cette évaluation vient affiner les résultats d\u0027une autre étape exécutée en parallèle et qui par l\u0027utilisation des tableaux de contingence permet une sélection et un regroupement tabulaire de zones qui constituent la source de règles potentiellement intéressantes. Grâce aux filtrages réalisés dans ces deux étapes, des règles que nous avons nommées règles d\u0027influence sont extraites basées sur des contraintes spécifiées. Cet article s\u0027organise de la façon suivante. Dans la Section 2, nous faisons état de quelques travaux sur l\u0027extraction de RAQ positives et négatives. La section 3 expose notre stratégie de génération de règles d\u0027influence. Enfin, nous présentons nos conclusions dans la section 4.\nLes règles d\u0027association quantitatives positives et négatives\nLes RAQ positives prennent en considération les variables qui apparaissent simultané-ment. Intéressons nous à l\u0027exemple suivant adapté de (Brin et al. (1997) \u003d 0,875 \u003e minconf . Dans ce cas la règle A ? ¬B serait valide pour la base de données. Toutefois, quelle que soit la nature de la RA, positive ou négative, le support et la confiance utilisés comme uniques contraintes sont insuffisants pour une extraction optimale des RA. Afin de remédier à l\u0027extraction prohibitive des RAQ, plusieurs mesures évaluant la qualité des règles ont été proposées (Lif t IBM (1996), Conviction Brin et al. (1997)). Dans ce papier, nous élargissons l\u0027extraction à ces règles qui sont complémentaires aux règles positives. Dans (Wu et al. (2004)), les RAQ négatives sont extraites à partir d\u0027ensembles de motifs non fréquents. Leur objectif est d\u0027identifier les caractères qui peuvent être ignorés. Dans (Antonie et Zaane (2004)), une nouvelle mesure statistique appelée l\u0027inégalité de Cauchy Schwarz symbolisée par ? est utilisée à des fins d\u0027organisation dans le rayonnage des supermar-\nUn coefficient ? 0 signifie l\u0027absence de relation linéaire entre A et B mais ne donne aucune indication sur l\u0027indépendance. Si le coefficient ? +1 ou ? ?1, A et B sont fortement corrélées. Ceci dit, la corrélation ne doit aucunement être confondue avec la causalité. La corrélation entre deux caractères n\u0027implique absolument pas que l\u0027un cause l\u0027autre. Des problèmes similaires sont apparus avec l\u0027utilisation de certaines mesures statistiques. La distance du ? 2 (Teng et al. (2002)) détecte les variables qui expriment de fortes liaisons. Elle permet de déterminer si deux variables sont indépendantes. Malheureusement, elle est incapable de donner la direction de la dépendance : A ? B ou B ? A ? La section suivante introduit une nouvelle sémantique proche des règles d\u0027impacte proposées par Webb dans (Webb (2001) \nPréparation des données\nLes données présentes dans les bases de données sont sous forme brute. Antérieurement à leur exploitation, il est nécessaire de les traiter. Ce traitement, dans le cas des variables qualitatives, consiste à éclater une variable en un ensemble de variables correspondantes aux différentes valeurs distinctes de la variable traitée. Dans le cas des variables quantitatives, le traitement des données est plus complexe, et émane des nouveaux problèmes principalement liés au nombre important de valeurs distinctes que peut prendre une seule variable quantitative. La discrétisation fût une réponse naturelle aux problèmes rencontrés (Srikant et Agrawal (1996); Aumann et Lindell (1999)). Notre travail de préparation des données consiste à réaliser un codage disjonctif complet pré-cédé d\u0027une étape de discrétisation sur les variables quantitatives. La discrétisation est réalisée en utilisant une méthode de clustering basée sur le calcul des plus proches voisins. Les clusters sont constitués d\u0027individus dont les distances des uns par rapport aux autres n\u0027excèdent pas un seuil limite prédéfini le M axDif . L\u0027exemple suivant est extrait de la base de données W ages 1 . Les clusters de la variable âge sont calculés avec M axDif \u003d 5 ans et sont listés dans la table 1. \nÉvaluation des taux d\u0027influence\nAfin d\u0027évaluer l\u0027influence d\u0027une variable sur une autre, le rejet de l\u0027hypothèse d\u0027indépen-dance doit être prouvé. Ceci peut être réalisé grâce à différentes mesures (? 2 , coefficient de corrélation linéaire . . .). Suite à cela, une analyse de la nature de la dépendance est réalisée grâce à notre mesure l\u0027Inf luence. \noù |X | symbolise la cardinalité de l\u0027ensemble des individus qui vérifient la condition X. Les résultats obtenus sont filtrés afin de ne retenir que les taux d\u0027influence supérieur au seuil d\u0027influence négative minimum fixé, symbolisé par S ? , tel que : \nFIG. 1 -i)Influence\nii). Les règles d\u0027influence attendus sont de la forme\nLes résultants obtenus, comme dans le cas négatif, sont filtrés par rapport à un seuil d\u0027influence positive minimum fixé et symbolisé par S + , tel que :\nn\u0027est pas intéressante.\nConsolidation de la mesure d\u0027influence\nLa dépendance entre deux variables doit être évaluée objectivement. Les ensembles avec peu de représentants sont exclus pour n\u0027inclure que ceux avec un taux d\u0027influence excédant le seuil minimum fixé. Or, si nous fixons le seuil à S\u003d70%, par exemple, alors que le taux d\u0027influence calculé d\u0027une règle est égal à 69%, ne devrions nous pas prendre cette dernière en considération ? Dans notre approche, nous nous sommes intéressées à la pertinence de ce genre de règles et au moyen de les inclure dans notre système. Nous avons, donc utilisé le théorème de la limite centrale (Macfie et Nufrio (2005)) qui répond parfaitement au problème moyennant des intervalles de confiance et une estimation du risque ? pour la proportion. Cette évaluation dote la mesure d\u0027influence d\u0027une meilleure sensibilité en permettant une étude dans des intervalles de la forme\navec ? suivant la loi normale N (0; 1), ? suivant la loi normale inverse et |n| la taille de la base de données. L\u0027exemple qui suit est tiré de la base W ages. Cet exemple permet d\u0027évaluer l\u0027influence né-gative de la variable sexe représentée par deux clusters (masculin et féminin) sur la variable salaire qui est représentée par sept clusters listés dans la table (2), avec un risque ? \u003d 0,05, ? ?/2 \u003d 1.96 et S ? \u003d 0,70. \nTAB. 3 -Exemple d\u0027estimation des taux d\u0027influence négatifs et des intervalles de confiance\nDétermination des zones intéressantes\nAu niveau de cette étape, le test d\u0027indépendance entre les variables A et B est réalisé. Cette indépendance peut être évaluée en utilisant la table de contingence (T C) des variations . Cette table est calculée en comparant les effectifs observés et les effectifs théoriques, (voir table (4)) (|ef f ectif theorique| ? |ef f ectif observe |) Guillaume (2002). Si (|ef f ectif observe| \u003d |ef f ectif theorique|) alors A et B sont indépendants, sinon, l\u0027hypothèse d\u0027indépendance est rejetée si cet écart est sigificatif. La T C des variations de l\u0027association (sexe,salaire) est présentée table 5. Les cellules nulles dans T C des variations sont ignorées. Les résultats de cette table sont ensuite juxtaposés aux tables des taux d\u0027influence positifs et négatifs pour constituer la source de nos RI. Cette opé-ration est réalisée dans l\u0027étape de coordination des résultats.\nCoordination des résultats\nL\u0027étape de coordination des résultats consiste à superposer la T C des variations, (voir exemple dans la table (5)   \nExtraction des règles d\u0027influence\nEn se basant sur les résultats des deux étapes précédentes, les RI positives et négatives sont extraites à partir de la table de coordination. Les règles sont de la forme A ? B ou A ? ¬B avec A l\u0027influent et B l\u0027influé. Les RI générées pour l\u0027association (sexe,salaire) sont listées dans la table (7).\nGenInf représente l\u0027intervalle de confiance généralisé pour les taux d\u0027influence [A; B] de la règle. GenInf est construit à partir des intervalles de confiance [A i ; B i ] spécifiques aux différentes valeurs des variables avec i la ième classe de l\u0027association, tel que A \u003d min{A i } et B \u003d max{B i }. les RI peuvent alors être présentées sous une forme générale comme dans l\u0027exemple présenté ci-après. sexe \u003d masculin ? (¬salaire ? [1,00; 6,00]) ? (salaire ? [11,11; 16,  [0,53; 0,61]. Ces règles expriment que le fait d\u0027être un homme augmenterait les chances d\u0027obtenir un salaire entre 11,11K et 26,00K et de ce fait, les hommes ont peu de \u0027chance\u0027 d\u0027avoir un salaire entre 1,00K et 6,00K. Inverssement, être une femme réduit les chances d\u0027obtenir un salaire entre 11,11K et 26,00K, ainsi une femme à \u0027plus de chance\u0027 d\u0027avoir un salaire entre 1,00K et 6,00K.\nConclusions et perspectives\nDans ce papier, nous proposons une approche pour l\u0027extraction de règles d\u0027influence quantitatives positives et négatives. La stratégie que nous adoptons combine une méthode pour l\u0027identification des zones d\u0027intérêt grâce aux tableaux de contingence des variations et une mesure d\u0027élagage l\u0027Inf luence qui analyse le comportement des variables et détermine la nature de leur dépendance. Ce genre d\u0027analyse est très utile, notamment dans notre cadre de travail qui s\u0027appuie sur une sémantique spécifique du type Inf luent ? Inf lué. Le système de règles obtenu peut servir de base à l\u0027utilisateur afin d\u0027établir des prédictions et éventuellement lancer\n"
  },
  {
    "id": "827",
    "text": "Introduction\nL\u0027apprentissage supervisé sur données déséquilibrées fait l\u0027objet de nombreux travaux (Provost (2000)). Pour le cas des arbres de décision, différents auteurs ont proposé d\u0027utiliser des mesures d\u0027entropie prenant en compte l\u0027asymétrie pour la recherche du meilleur éclate-ment. Nous avons ainsi proposé une axiomatique permettant de définir une famille de mesures asymétriques (Zighed et al. (2007)). Comment évaluer la qualité des arbres construits avec de telles mesures ? En effet, les critères de performances globaux (comme le taux d\u0027erreur) ne prennent pas en compte l\u0027asymétrie des classes. Ceux qui évaluent les performances du modèle sur une seule classe sont tributaires de la règle d\u0027affectation d\u0027une classe dans chaque feuille. Or, dans le cas de données déséquilibrées, la règle majoritaire utilisée habituellement ne convient pas. Nous proposons donc une méthodologie et une évaluation des arbres construits avec une entropie asymétrique.\nMéthodes d\u0027évaluation\nNous avons retenu deux méthodes pour évaluer les arbres de décisions asymétriques : les courbes ROC et les graphes rappel / précision. Les courbes ROC permettent d\u0027évaluer la structure des arbres indépendamment du déséquilibre des classes (Provost et Fawcett (1997)). Les graphes rappel / précision permettent quant à eux d\u0027évaluer les performances du modèle sur une classe, en faisant varier la règle d\u0027affectation. Ces deux méthodes permettent ainsi de tenir compte des deux problèmes cités en introduction.\nNous avons mené des tests sur 11 jeux de données, dont 2 sont des données réelles issues du dépistage du cancer du sein. La proportion de la classe minoritaire varie de 4% à 35%. Nous avons construit deux modèles en 10-validation croisée : un arbre de décision utilisant l\u0027entropie quadratique, et un arbre utilisant l\u0027entropie asymétrique. Le critère d\u0027arrêt a été fixé à 3% de gain d\u0027information, et la distribution de référence de l\u0027entropie asymétrique au déséquilibre du jeu de départ. Nous observons les résultats sur la classe minoritaire. Nous pouvons résumer les résultats obtenus en trois points. Premiérement, le critère AUC calculé à partir des courbes ROC est systématiquement supérieur en utilisant l\u0027entropie asymétrique. Deuxièmement, la comparaison des courbes ROC entre les deux types d\u0027arbres montre que la courbe ROC de l\u0027entropie asymétrique est dominée sur la partie gauche du graphique (seuil d\u0027acceptation élevé) mais domine sur la partie droite. Enfin et de la même manière, les graphes rappel / précision montrent qu\u0027à rappel égal, l\u0027entropie asymétrique est moins précise pour les forts seuils d\u0027acceptation mais permet une meilleure précision sur les faibles seuils.\nConclusion et perspectives\nAinsi si on utilise un seuil d\u0027acceptation bas pour classer les feuilles d\u0027un arbre de décision, les courbes ROC comme le critère rappel / précision encouragent l\u0027utilisation d\u0027une entropie asymétrique lorsque les jeux de données sont déséquilibrés. Nous considérons quatre pistes pour étendre notre travail. D\u0027une part, l\u0027utilisation d\u0027un critère adaptatif cherchant à s\u0027écarter à chaque éclatement de la distribution du noeud parent. Deuxièmement, l\u0027élaboration d\u0027un critère d\u0027arrêt adapté aux données déséquilibrées. Le présent travail nous permettra de proposer une méthode pour le choix de la règle d\u0027affectation. Enfin, les outils que nous proposons pour adapter les arbres de décision aux données déséquilibrées seront étendus pour les cas à plus de deux modalités.\nRéférences Provost, F. (2000). Learning with imbalanced data sets. \nSummary\nTo build decision trees on imbalanced datasets, authors proposed asymmetric entropies. Then the problem of evaluating those trees has to be solved. This paper proposes to evaluate the quality of decision trees based on asymmetric entropy measure.\n"
  },
  {
    "id": "828",
    "text": "Introduction\nLe domaine de la classification de données textuelles se décline en de nombreux axes parmi lesquels la classification conceptuelle. Cette dernière consiste à regrouper des termes dans des concepts définis par un expert. Citons par exemple les termes pot d\u0027échappement, pare-brise et essuie glace qui peuvent être classés dans le concept automobile. Afin d\u0027établir une telle classification sémantique, la proximité de chacun des termes issus des textes doit être mesurée. Ces termes sont ensuite classés en fonction de leurs proximités sémantiques par un algorithme de fouille de données tels que les Kppv (K plus proches voisins) ou bien les K moyennes (Cornuéjols et Miclet (2002)).\nNous nous focalisons dans cet article sur la première étape de la réalisation d\u0027une classification conceptuelle : l\u0027étude de la proximité des termes. Afin de calculer une telle proximité, nous nous appuyons sur une méthode appelée Latent Semantic Analysis (LSA) développée par Landauer et Dumais (1997) 1 . La méthode LSA est uniquement fondée sur une approche statistique appliquée à des corpus de grande dimension consistant à regrouper les termes (classification conceptuelle) ou les contextes (classification de textes). Une fois l\u0027analyse sémantique latente appliquée à un corpus, un espace sémantique associant chaque mot à un vecteur est retourné. La proximité de deux mots peut alors être obtenue par un calcul de similarité comme le cosinus entre deux vecteurs. L\u0027objectif de nos travaux est d\u0027améliorer les performances de LSA par une approche nommée ExpLSA (Expansion des contextes avec LSA).\nL\u0027approche ExpLSA consiste à enrichir le corpus qui constituera l\u0027entrée d\u0027une analyse sémantique latente classique. Cet enrichissement utilise les informations sémantiques obtenues grâce à la syntaxe, ce qui permet d\u0027utiliser ExpLSA aussi bien avec des corpus spécialisés ou non. Il n\u0027est en effet pas utile d\u0027utiliser un corpus d\u0027apprentissage et donc pas nécessaire de connaître le thème général du corpus.\nDans cet article, nous allons nous appuyer sur un corpus des Ressources Humaines de la société PerformanSe 2 écrit en français 3 . Notons que les premiers travaux sur ce corpus ont été initiés dans l\u0027équipe IA du LRI (Roche et Kodratoff (2003)). Une caractéristique essentielle de ce corpus est qu\u0027il utilise un vocabulaire spécialisé. Par ailleurs, il contient des tournures de phrases revenant souvent, ce qui peut influencer positivement le traitement avec LSA. Ce corpus a fait l\u0027objet d\u0027une expertise manuelle nous permettant ainsi de valider nos expérimen-tations.\nNous proposons dans la section suivante de détailler les caractéristiques théoriques de la méthode LSA ainsi que les limites d\u0027une telle analyse. La section 3 propose un état de l\u0027art dans le domaine de l\u0027utilisation de connaissances syntaxiques associées à LSA. Nous présen-tons ensuite notre méthode en y développant ses différentes étapes (section 4). Nous décrirons également (section 5) le protocole expérimental utilisé pour finalement présenter les résultats obtenus.\nLSA\nLa méthode LSA qui s\u0027appuie sur l\u0027hypothèse \"harrissienne\" est fondée sur le fait que des mots qui apparaissent dans un même contexte sont sémantiquement proches. Le corpus est représenté sous forme matricielle. Les lignes sont relatives aux mots et les colonnes représen-tent les différents contextes choisis (un document, un paragraphe, une phrase, etc.). Chaque cellule de la matrice représente le nombre d\u0027occurrences des mots dans chacun des contextes du corpus. Deux mots proches au niveau sémantique sont représentés par des vecteurs proches. La mesure de proximité est généralement définie par le cosinus de l\u0027angle entre les deux vecteurs.\nCaractéristiques théoriques de LSA\nLa théorie sur laquelle s\u0027appuie LSA est la décomposition en valeurs singulières (SVD). Une matrice A \u003d [a ij ] où a ij est la fréquence d\u0027apparition du mot i dans le contexte j, se décompose en un produit de trois matrices U SV T . U et V sont des matrices orthogonales et S une matrice diagonale.\nSoit S k où k \u003c r la matrice produite en enlevant de S les r ? k colonnes qui ont les plus petites valeurs singulières. Soit U k et V k les matrices obtenues en enlevant les colonnes correspondantes des matrices U et V . La matrice U k S k V T k peut alors être considérée comme une version compressée de la matrice originale A. Les expériences décrites dans la section 5 ont été menées avec un nombre de facteurs k égal à 100, facteur faible qui est davantage approprié à des contextes de taille réduite.\nIl est coutume de dire que LSA est une méthode statistique ou numérique car elle s\u0027appuie sur une théorie mathématique bien connue. Cependant, on peut également dire que LSA est une méthode géométrique car seuls des résultats d\u0027algèbre linéaire sont utilisés.\nNous précisons qu\u0027avant d\u0027effectuer la décomposition en valeurs singulières, une première étape de normalisation de la matrice d\u0027origine A est exécutée. Cette normalisation consiste à appliquer un logarithme et un calcul d\u0027entropie sur la matrice A. Ainsi, plutôt que de se fonder directement sur le nombre d\u0027occurrences de chacun des mots, une telle transformation permet de s\u0027appuyer sur une estimation de l\u0027importance de chacun des mots dans leur contexte. De manière similaire aux travaux de Turney (2001), cette étape de normalisation peut également s\u0027appuyer sur la méthode du tf×idf, approche bien connue dans le domaine de la Recherche d\u0027Information. Précisons de plus que nous ne prenons pas en compte les ponctuations ainsi qu\u0027un certain nombre de mots non significatifs du point de vue sémantique tels que les mots \"et\", \"à\", \"le\", etc.\nLes limites de LSA\nLSA offre des avantages parmi lesquels, la notion d\u0027indépendance par rapport à la langue du corpus étudié, le fait de se dispenser de connaissances linguistiques ainsi que de celles du domaine tels que des thésaurus. Bien que cette approche soit prometteuse, il n\u0027en demeure pas moins que son utilisation soulève des contraintes.\nNotons tout d\u0027abord l\u0027importance de la taille des contextes choisis. Rehder et al. (1998) ont montré lors de leurs expérimentations que si les contextes possèdent moins de 60 mots, les résultats s\u0027avèrent être décevants. Il a également été mis en évidence par Roche et Chauché (2006) que l\u0027efficacité de LSA est fortement influencée par la proximité du vocabulaire utilisé.\nPour résoudre de tels problèmes, une des solutions peut consister à ajouter des connaissances syntaxiques à LSA, comme cela est décrit dans la section suivante.\n3 État de l\u0027art sur l\u0027ajout de connaissances syntaxiques à LSA  posent le problème du manque d\u0027informations syntaxiques dans LSA en comparant cette méthode à une évaluation humaine. Il est question de proposer à des experts humains d\u0027attribuer des notes à des essais sur le coeur humain de 250 mots rédigés par des étudiants. Un espace sémantique a été créé à partir de 27 articles écrits en anglais traitant du coeur humain \"appris\" par LSA. Les tests effectués concluent que la méthode LSA obtient des résultats satisfaisants comparativement à l\u0027expertise humaine. Il en ressort que les mauvais résultats étaient dus à une absence de connaissances syntaxiques dans l\u0027approche utilisée. Ainsi, les travaux qui sont décrits ci-dessous montrent de quelle manière de telles connaissances peuvent être ajoutées à LSA.\nLa première approche de Wiemer-Hastings et Zipitria (2001) utilise des étiquettes grammaticales (Brill (1994)) appliquées à l\u0027ensemble du corpus étudié (corpus de textes d\u0027étudi-ants). Les étiquettes étant rattachées à chaque mot avec un blanc souligné (\"_\"), l\u0027analyse qui s\u0027en suit via LSA considère le mot associé à son étiquette comme un seul terme. Les résul-tats de calculs de similarités obtenus avec une telle méthode restent décevants. Notons que de telles informations grammaticales ne sont pas des connaissances syntaxiques proprement dites contrairement à la seconde approche de Wiemer-Hastings et Zipitria (2001) décrite ci-dessous. Cette seconde approche se traduit par l\u0027utilisation d\u0027un analyseur syntaxique afin de segmenter le texte avant d\u0027appliquer l\u0027analyse sémantique latente. Cette approche est appelée \"LSA structurée\" (SLSA). Une décomposition syntaxique des phrases en différents composants (sujet, verbe, objet) est tout d\u0027abord effectuée. La similarité est ensuite calculée en traitant séparé-ment par LSA les trois ensembles décrits précédemment. Les similarités (calcul du cosinus) entre les vecteurs des trois matrices formées sont alors évaluées. La moyenne des similarités est enfin calculée. Cette méthode a donné des résultats satisfaisants par rapport à \"LSA classique\" en augmentant la corrélation des scores obtenus avec les experts pour une tâche d\u0027évaluation de réponses données par des étudiants à un test d\u0027informatique. Kanejiya et al. (2003) proposent un modèle appelé SELSA. Au lieu de générer une matrice de co-occurrences mot/document, il est proposé une matrice dans laquelle chaque ligne contient toutes les combinaisons mot_étiquette et en colonne les documents. L\u0027étiquette \"préfixe\" renseigne sur le type grammatical du voisinage du mot traité. Le sens d\u0027un mot est en effet donné par le voisinage grammatical duquel il est issu. Cette approche est assez similaire à l\u0027utilisation des étiquettes de Brill (1994) présentée dans les travaux de Wiemer-Hastings et Zipitria (2001). Mais SELSA étend ce travail vers un cadre plus général où un mot avec un contexte syntaxique spécifié par ses mots adjacents est considéré comme une unité de représentation de connaissances. L\u0027évaluation de cette approche a montré que la méthode LSA était plus pertinente que SELSA dans un test de corrélation avec des experts. Cependant, SELSA se révèle plus précise pour ce qui est de tester les bonnes et mauvaises réponses (c.-à-d. SELSA fait moins de fautes que LSA mais en retourne de plus nuisibles).\nL\u0027approche ExpLSA que nous présentons dans cet article se place dans un contexte différent. En effet, dans notre cadre de travail, les contextes sont représentés par des phrases. Ceux-ci ont donc une taille réduite ce qui a tendance à donner des résultats décevants avec l\u0027utilisation de LSA (Rehder et al. (1998); Roche et Chauché (2006)). Dans notre approche, nous proposons d\u0027utiliser la régularité de certaines relations syntaxiques afin d\u0027enrichir le contexte comme nous allons le monter dans la section suivante.\nNotre Approche : ExpLSA\nLe but final que nous nous fixons consiste à regrouper automatiquement des termes extraits grâce à des systèmes tels que ACABIT (Daille (1994)), LEXTER (Bourigault (1993)), SYNTEX (Bourigault et Fabre (2000)), EXIT (Roche et al. (2004)). Dans notre cas, nous nous proposons de regrouper les termes nominaux extraits avec EXIT. Les termes extraits avec ce système sont des groupes de mots respectant des patrons syntaxiques (nom-préposition-nom, adjectif-nom, nom-adjectif, etc.). Par ailleurs, EXIT s\u0027appuie sur une méthode statistique afin de classer les termes extraits et utilise une approche itérative pour construire des termes complexes.\nLa première étape de ce regroupement pour finalement construire une classification conceptuelle sera effectuée par ExpLSA dont le principe est décrit dans la section suivante.\nPrincipe général d\u0027ExpLSA\nNotre approche vise à enrichir le corpus initial lemmatisé en faisant une expansion des phrases (d\u0027où le nom ExpLSA) fondée sur une méthode syntaxique. Il en ressort un contexte plus riche. Celui-ci est construit en complétant les mots du corpus par des mots jugés séman-tiquement proches.\nCitons par exemple la phrase : \"Vos interlocuteurs seront donc bien inspirés de placer les échanges ...\". Nous la transformons tout d\u0027abord en phrase lemmatisée via le système SYGFRAN (Chauché (1984) \nAvec log Asium (x) valant : -pour x \u003d 0, log Asium (x) \u003d 0 -sinon log Asium (x) \u003d log(x) + 1\nUne mesure d\u0027Asium proche de 1 implique une importante proximité sémantique. L\u0027exemple de la figure 1 illustre l\u0027application de la mesure d\u0027Asium pour les verbes écouter et convaincre. Nous considérerons par la suite plusieurs seuils de similarité, appelés SA, signifiant qu\u0027au delà de ceux-ci, les verbes seront considérés comme proches par la mesure d\u0027Asium. La méthode d\u0027expansion utilisant la méthode d\u0027Asium est décrite dans la section suivante.\nÉtapes d\u0027ExpLSA\nAprès avoir explicité la mesure d\u0027Asium permettant de mesurer la proximité des verbes du corpus, nous proposons de détailler les différentes étapes définissant ExpLSA afin d\u0027étendre les contextes.\nLa première étape de l\u0027approche ExpLSA identifie les différents termes extraits par EXIT. Cette identification consiste à représenter le terme par un seul mot (par exemple, le terme attitude profondément participative issu du corpus des Ressources Humaines devient nom234 qui représente le 234ème terme parmi une liste extraite par EXIT).\nAprès extraction des relations syntaxiques Verbe-Objet par le biais d\u0027une analyse syntaxique, la phase suivante de notre approche consiste à étudier la proximité sémantique entre les L\u0027étape suivante a pour but de regrouper tous les objets communs dont les verbes ont été jugés proches sémantiquement par le seuil de similarité le plus élevé parmi l\u0027ensemble des couples de verbes.\nNous considérons deux possibilités de regroupement. La première consiste à considérer les objets communs aux deux verbes (interlocuteur et collaborateur dans l\u0027exemple de la figure 1). La seconde considère les objets communs et les complémentaires aux deux verbes comme dans les travaux de Faure et Nedellec (1999)  Notons qu\u0027une liste de noms non porteurs de sens ne sont pas pris en compte pour enrichir le contexte (par exemple, les mots \"chose\", \"personne\", etc.). Cette liste a été constituée manuellement.\nL\u0027évaluation, qui va être présentée dans la section suivante, consiste à comparer les résultats obtenus automatiquement en nous appuyant sur ExpLSA avec ceux d\u0027un expert qui a associé manuellement les termes pertinents à des concepts.\nExpérimentations\nPour discuter de la qualité des résultats retournés avec notre approche, nous nous appuyons sur le protocole expérimental décrit dans la section suivante.\nProtocole expérimental\nDans nos expérimentations nous nous appuyons sur le corpus des Ressources Humaines expertisé manuellement. De cette expertise ressort une classification conceptuelle de l\u0027ensemble des termes extraits par EXIT ; les concepts ayant été définis par l\u0027expert. Par exemple, l\u0027expert a défini le concept \"Relationnel\" dont les termes confrontation ouverte, contact superficiel et entourage compréhensif sont des instances. L\u0027objectif de nos expérimentations est d\u0027évaluer les similarités entre les termes retournées par les méthodes automatiques ci-dessous :\n-M1 : LSA 4 -M2 : la méthode des intersections de ExpLSA -M3 : la méthode des complémentaires de ExpLSA -M4 : LSA + Tree-Tagger La méthode de LSA + Tree-Tagger consiste à utiliser un étiqueteur grammatical, le TreeTagger (Schmid (1995)) comme dans l\u0027approche de Wiemer-Hastings et Zipitria (2001) présen-tée dans la section 3. Ainsi, nous appliquons LSA sur un corpus qui a été au préalable étiqueté par le Tree-Tagger.\nPour comparer ces méthodes, nous avons évalué deux à deux les termes des concepts. Pour cela, nous avons sélectionné parmi les concepts, ceux étant les plus représentés dans le corpus, c\u0027est-à-dire les concepts regroupant un minimum de 200 termes distincts selon l\u0027expertise manuelle. Cela nous laisse un total de quatre concepts. Nous proposons ainsi de comparer deux à deux chaque concept. L\u0027objectif de nos expérimentations consiste à évaluer si les termes appartenant à un concept sont correctement associés aux termes de ce même concept par les quatre méthodes (M1 à M4) décrites ci-dessus.\nPour effectuer une telle comparaison, tous les termes d\u0027un concept C1 sont pris en compte. La similarité (cosinus) est alors calculée entre l\u0027ensemble des termes du concept C1 et les autres termes des concepts à comparer (par exemple, les termes de C1 + C2, C1 + C3 ou C1 + C4). Les couples de termes ainsi obtenus sont classés par valeur décroissante avec les quatre méthodes M1 à M4. Un système retourne des résultats de bonne qualité si les couples pertinents sont placés en début de liste. Un couple est pertinent si les deux termes appartiennent au même concept. Pour évaluer la qualité de la liste, nous calculons la précision des premiers couples retrournés, le rappel n\u0027ayant pas été jugé adapté 5 . La précision permet d\u0027évaluer la proportion de couples pertinents retrouvés par le système. Notons que la réalisation de ces expérimentations avec deux concepts est assez conséquente puisqu\u0027elle produit plus de 60 000 calculs de similarité.\nComparaison des deux méthodes ExpLSA\nCette première évaluation propose de comparer les deux méthodes de ExpLSA utilisées pour l\u0027enrichissement du corpus, la méthode des intersections (M2) et la méthode des complé-mentaires (M3). Le tableau 1 compare la moyenne des précisions des 100 6 premiers couples de termes avec un seuil pour la mesure d\u0027Asium à 0,6. Avec une telle valeur de SA, nous faisons une large expansion du corpus. Ce tableau montre que les approches ExpLSA utilisant TAB. 1 -Précision en fonction des 100 premiers couples de termes. M1 : LSA, M2 : ExpLSA avec la méthode des intersections, M3 : ExpLSA avec la méthode des complémentaires. SA \u003d 0,6. la méthode des intersections (M2) ou bien celle des complémentaires (M3) sont inférieures à LSA et n\u0027améliorent que rarement la précision. Ces résultats s\u0027expliquent par la quantité importante de données non pertinentes utilisées pour l\u0027enrichissement. En effet, un seuil SA faible a tendance à ajouter du bruit comparativement à l\u0027utilisation d\u0027un seuil plus élevé qui permet une expansion quantitativement plus faible mais souvent plus pertinente. Ceci confirme les résultats préliminaires présentés dans (Béchet et al. (2007)). Le meilleur compromis entre la quantité de données ajoutées et la qualité de celles-ci a été experimentalement établi avec un seuil SA égal à 0,8 sur notre corpus. Nous constatons par ailleurs que la méthode M3 donne globalement des résultats plus faibles que la méthode M2 (bien que toutes les deux soient inférieures à M1 Nous proposons de comparer dans cette section deux méthodes utilisant des connaissances syntaxiques, ExpLSA (M2) et LSA + Tree-Tagger (M4). LSA + Tree-Tagger consiste à ajouter des connaissance grammaticales au corpus en complétant les mots par une étiquette grammaticale. Cette approche permet de lever les ambiguités de certains mots pouvant appartenir à des catégories grammaticales différentes. Par exemple, le mot bien peut-être un adverbe, un nom ou un adjectif. Avec la méthode M4, nous considérons dans cet exemple trois formes distinctes pour représenter ce mot : bien_ADV, bien_NOM et bien_ADJ.\nLa figure 3 montre que la méthode LSA + Tree-Tagger améliore les résultats de LSA pour les derniers couples ce qui ne correspond pas aux attentes de l\u0027utilisateur. En effet, une fonction de rang est en général satisfaisante si un nombre important d\u0027exemples positifs sont placés en tête de liste. Cette tendance se généralise pour les autres concepts comme le montre le tableau 3. La méthode LSA + Tree-Tagger (M4) reste cependant presque toujours inférieure à notre approche ExpLSA (M2). Ces résultats nous encouragent à envisager dans de futurs travaux une hybridation des méthodes M2 et M4 afin de conserver les résultats de ExpLSA et de bénéficier des améliorations de la méthode LSA + Tree-Tagger pour les derniers couples. LSA est une méthode statistique utilisée notamment pour regrouper des termes afin d\u0027établir une classification conceptuelle. Néanmoins, cette méthode donne des résultats parfois déce-vants. Ceux-ci s\u0027expliquent entre autres par l\u0027absence de connaissances linguistiques. La qualité de ces résultats peut également être influencée par la taille des contextes utilisés, LSA obtenant de meilleurs résultats avec des contextes de grande taille.\nC\u0027est pourquoi nous nous sommes intéressés dans nos travaux à améliorer les performances de LSA avec des contextes assez courts (phrases) en proposant une approche, ExpLSA, consistant à effectuer une expansion des contextes avant d\u0027appliquer LSA. Nous rendons de ce fait les contextes plus riches en utilisant des outils syntaxiques afin d\u0027y parvenir.\nNous avons présenté deux expérimentations pour comparer l\u0027approche ExpLSA. Nous avons conclu dans la première expérience qu\u0027avec ExpLSA fondée sur la méthode des complémen-taires, l\u0027expansion réalisée n\u0027était pas pertinente et ajoutait une quantité importante de bruit. ExpLSA utilisant la méthode des intersections donne quant à elle des résultats satisfaisants pour les concepts discriminants d\u0027un point de vue sémantique. Les termes des concepts pouvant générer des ambiguités se révelent plus ou moins difficiles à traiter automatiquement par notre approche. L\u0027inconvénient de la méthode ExpLSA est qu\u0027elle nécessite un temps d\u0027exé-cution conséquent (environ deux heures pour traiter un corpus de 1,2 Mo). Ce temps important s\u0027explique principalement par la durée d\u0027execution de la tâche d\u0027extraction des relations syntaxiques. La seconde expérimentation a montré que LSA + Tree-Tagger améliore rarement LSA et que notre approche ExpLSA donne de meilleurs résultats.\nNous envisageons comme futurs travaux d\u0027approfondir les expérimentations en identifiant plus précisément dans quels cas ExpLSA donne de meilleurs résultats comparativement à LSA. Ceci permettra de mettre en place une approche hybride qui utilise LSA et/ou ExpLSA et/ou LSA + Tree-Tagger selon les situations les plus appropriées. De plus, nous souhaiterions valider le regroupement des mots avec ExpLSA en nous appuyant sur des mesures statistiques et des données numériques issues des moteurs de recherche du web (Turney (2001)). Par ailleurs, nous proposerons d\u0027autres méthodes afin d\u0027ajouter des connaissances syntaxiques à LSA. De plus, nous validerons notre méthode d\u0027expansion des contextes en la confrontant à des problèmes de classification de textes. Nous envisageons enfin d\u0027utiliser des vecteurs sé-mantiques avec SYGMART (Chauché (1984)) en considérant un terme comme produit d\u0027un ensemble de concepts issus du thésaurus Larousse.\n"
  },
  {
    "id": "829",
    "text": "Introduction\nLe problème de la recherche de règles d\u0027association, introduit dans Agrawal et al. (1993), est basé sur l\u0027extraction de corrélations fréquentes entre les enregistrements et connaît de nombreuses applications dans le marketing, la gestion financière ou l\u0027analyse décisionnelle (par exemple). Au coeur de ce problème, la découverte d\u0027itemsets fréquents représente un domaine de recherche très étudié. Dans l\u0027analyse du panier de la ménagère, par exemple, les itemsets fréquents ont pour but de découvrir des ensembles d\u0027items qui correspondent à un nombre significatif de clients. Si ce nombre est supérieur à un support défini (par l\u0027utilisateur) alors cet itemset est considéré comme fréquent. Cependant, dans la définition initiale des itemsets fréquents, l\u0027extraction est effectuée sur la base de données toute entière (i.e. soit min supp , le support minimum donné par l\u0027utilisateur, les itemsets extraits doivent apparaître dans au moins |D| × min supp enregistrements de D). Toutefois, il est possible que des itemsets intéressants reste ignorés malgré des caractéristiques particulières (y compris de support). Effectivement, les itemsets intéressants sont souvent liés au moment qui correspond à leur observation. On pourrait prendre pour exemple le comportement des utilisateurs d\u0027un site de commerce en ligne pendant une offre spéciale sur les DVD et les CD vierges pour laquelle une publicité est faite par mailing. De la même manière, le site Web d\u0027une conférence peut voir le nombre de connexions augmenter dans une fenêtre de quelques heures avant la date limite de soumission. Une condition nécessaire à la découverte de ce type de données est liée à l\u0027aspect temporel des données. Cet aspect a déjà été abordé pour les règles d\u0027association dans Ale et Rossi (2000); Lee et al. (2001). Dans Ale et Rossi (2000), les auteurs proposent la notion de règles d\u0027association temporelles. Leur idée consiste à extraire les itemsets qui sont fréquents sur des périodes définies par la durée de vie de chaque item (les périodes ne sont donc pas découvertes mais utilisées comme contrainte).\nDans cet article, nous proposons de découvrir les itemsets qui sont fréquents sur un sousensemble contigü de la base de données. Par exemple, les navigations sur les pages Web des DVD et CD vierges apparaissent de façon distribuée sur toute l\u0027année. Cependant, la fréquence de ce comportement augmente très certainement pendant les quelques heures (ou jours) qui suivent le mailing. Ainsi, le défi consiste à trouver la fenêtre temporelle qui va optimiser le support de ce comportement. Considérons que le mailing soit envoyé le 3 mars et qu\u0027il a influencé les clients pendant deux jours. Notre but est de découvrir que : \"25% des clients, entre le 3 et le 5 mars, ont demandé la page des CD vierges, la page des DVD vierges et finalement la page des offres spéciales\". Le support de ce comportement est sûrement trop faible pour permettre son extraction sur l\u0027année toute entière mais cette connaissance (i.e. le comportement et la période sur laquelle il est fréquent) peut être très utile pour les décideurs qui veulent certainement découvrir ce comportement et sa période de fréquence pour finalement faire le lien avec le mailing.\nDefinitions\nLa définition 1 reprend le concept d\u0027itemset fréquent de Agrawal et al. (1993). Nous y avons ajouté la notion d\u0027estampille (donc une transaction peut couvrir plusieurs dates).\nDéfinition 1 Soit\nDéfinition 2 L\u0027ensemble F des itemsets fréquents de D avec un support minimum ? est noté\nÉtant donnés un ensemble d\u0027items I, une base de transactions D et un support minimum ?, le problème de l\u0027extraction d\u0027itemsets fréquents vise à trouver F (D, ?) ainsi que le support des itemsets de F . L\u0027exemple 1 donne une illustration des concepts définis dans cette section. figure 1(a)   figure 1(a) \nExemple 1 La\nFIG. 1 -itemsets fréquents et itemsets compacts sur\nNotre problème est basé sur les estampilles et vise à extraire des itemsets qui sont fréquents sur des périodes particulières de D. Nous présentons maintenant les notions d\u0027itemset temporel et d\u0027itemset compact, qui sont au coeur de cet article.\nDéfinition 3\nUne période P \u003d (P s , P e ) est définie par une date de départ P s et une date de fin P e . L\u0027ensemble des transactions qui appartiennent à une période P est défini par T r(P ) \u003d {T /T ? D, ?i ? T, P s ? P i ? P e } avec P i l\u0027estampille de l\u0027item i dans la transaction T . Enfin, P R est l\u0027ensemble des périodes possibles sur D.\nEn d\u0027autres termes, l\u0027ensemble des transactions qui appartiennent à P est l\u0027ensemble des transactions dont tous les items sont estampillés dans les limites de P .\nDéfinition 4 Un itemset temporel x est un tuple\nSoit ?, le support minimum, nous présentons les caractéristiques d\u0027un itemset compact dans la définition 5.\nDéfinition 5 Soit x un itemset temporel. x est un itemset compact (IC) ssi les conditions suivantes sont respectées :\nSoit k la taille de x i , alors x est un k-itemset compact. Enfin, SI k est l\u0027ensemble de tous les k-itemsets compacts.\nLa première condition de la définition 5 assure que x représente un itemset qui est fréquent sur sa période. La seconde condition assure que la taille de x p est maximale. En fait, si une période plus grande existe, alors, sur cette période, x i n\u0027est pas fréquent ou la couverture de x i reste identique (i.e. étendre la période de x p à p 2 n\u0027apporte rien au support). Enfin, la troisième condition assure que la taille de x p est également minimale. En effet, si x i est supporté par la première et la dernière transaction de x p , alors si il existe un période plus petite sur laquelle x i est fréquent, la couverture sera plus faible (i.e. passer de x p à p 2 implique d\u0027ignorer des transactions qui supportent x i et doivent donc être gardées). Une illustration de cette définition est donnée dans l\u0027exemple 2.   \nDéfinition 6 L\u0027ensemble des Itemsets Compacts Maximaux (ICM) est défini comme suit : soit x un IC, x est un ICM si les conditions suivantes sont respectées :\nDans la suite de ce papier, nous proposons un algorithme optimisé pour la découverte de l\u0027ensemble des ICM, comme décrits par la définition 6.\n3 DeICo : principe général DEICO introduit un nouveau principe de comptage pour les itemsets candidats. Considérons un itemset temporel t qui n\u0027est pas compact (i.e. t ? \u003c ?). Tout surensemble u \u003d (u x , u p , u ? )/t x ? u x ? u p ? t p de t ne peut pas être un itemset compact (i.e. u ? \u003c ?). DEICO étend le principe d\u0027apriori afin de générer des itemsets compacts candidats et compter leur support. Le principe de génération est modifié par l\u0027ajout d\u0027un filtre sur les intersections possibles entre candidats (i.e. si deux itemsets compacts de taille k ont un préfixe commun mais ne partagent pas la même période, alors leur croisement ne peut pas générer un itemset compact). Cependant, l\u0027étape de comptage d\u0027apriori ne peut pas s\u0027appliquer directement dans notre cas. Considérons c, un itemset temporel candidat. Une solution consisterait à compter le nombre d\u0027apparitions de c dans c p . Ce n\u0027est pas une solution correcte. Considé-rons en effet le candidat c \u003d ((a b), [1..10], c ? ) (généré à partir de x \u003d ((a), [1..10], En effet, la façon dont une passe est effectuée (soit dans l\u0027ordre séquentiel des transactions) implique de découvrir les kernels \"à la volée\".\nDéfinition 7\nUn kernel est une période. L\u0027ensemble K(x, P, ?) des kernels de l\u0027item x sur la période P pour un support ? est défini comme suit : Soit k ? P une période telle que x ? T r(k s ) ? T r(k s ) est la première apparition de x sur P . Si k n\u0027existe pas, alors K \u003d ?. Sinon, soit N l\u0027ensemble des estampilles telles que ?n ? N, n ? P ? n \u003e k s ? f requence(x, [k s ..n]) \u003c ? (en d\u0027autres termes, N est l\u0027ensemble des estampilles de P telles que toute extension de k à une estampille de N implique la perte de fréquence pour x). Si N est vide, alors k e est défini comme la dernière apparition de x dans  \nFIG. 2 -Kernels et période de l\u0027itemset (b)\nEnd while End Algorithm MERGEKERNELS Notre algorithme d\u0027extraction se base sur le principe de la génération de candidats. Pendant la passe sur les données, le but de l\u0027algorithme d\u0027extraction est de mettre à jour les informations sur les kernels des itemsets temporels candidats dont la période englobe l\u0027estampille de la transaction courante. A la fin de chaque passe de l\u0027algorithme nous obtenons tous les kernels de chaque candidat pour cette passe. A la fin de chaque passe, les kernels obtenus pour chaque itemset temporel candidat sont fusionnés pour obtenir des itemsets compacts.\nConclusion\nNous avons proposé une nouvelle définition pour la découverte d\u0027itemsets qui correspondent à des fréquences élevées sur des périodes précises sans connaissance préalable sur ces périodes. Cette découverte posait la difficulté de découvrir en même temps les itemsets et leurs périodes optimales de fréquence. De plus, le nombre de combinaisons possible devait être réduit et nous avons apporté les bases théoriques nécessaires à la résolution du problème. Notre algorithme, basé sur la découverte de \u0027kernels\u0027 et leurs fusions s\u0027est révélé efficace et capable d\u0027extraire ce nouveau type de connaissance de manière précise et exhaustive. D\u0027après nos expérimentations, les itemsets compacts constituent un résultat lisible et instructif pour mieux comprendre les données étudiées.\nRéférences\nAgrawal, R., T. Imielinski, et A. N. Swami (1993). Mining association rules between sets of items in large databases. In SIGMOD, Washington, D.C., USA, pp. 207-216. Ale, J. M. et G. H. Rossi (2000). An approach to discovering temporal association rules. pp. 294-300. Lee, C.-H., C.-R. Lin, et M.-S. Chen (2001). On mining general temporal association rules in a publication database. pp. 337-344.\nSummary\nFrequent pattern mining is a very important topic of knowledge discovery, intended to extract correlations between items recorded in databases. However, those databases are usually considered as a whole and hence, itemsets are extracted over the entire set of records. In this paper, we introduce the definition of solid itemsets, which represent a coherent and compact behavior over a specific period.\n"
  },
  {
    "id": "830",
    "text": "Introduction\nSuivre et modéliser les ravines d\u0027érosion en montagne est un défi nouveau, nécessaire pour développer des modélisations distribuées précises basées sur les processus hydrologiques élémentaires. Les avancées récentes en modélisation distribuée exigent de plus en plus une connaissance fine des processus élémentaires associée à une description physique précise des objets d\u0027intérêt (Hessel et al., 2003). Le mode de transport liquide et solide est à la base de la réaction des bassins versants pour ces modélisations, elle se décline de plus en plus en interrogations sur les « chemins de l\u0027eau », point qui apparaît à la fois fondamental et mal connu, et souvent très simplifié dans les modèles. En particulier en zones de badlands, avec un relief très tourmenté, il s\u0027agit de la caractérisation physique des ravines et du modelé du relief à différentes échelles.\nPour atteindre ce but, les techniques de télédétection semblent appropriées, mais des questions de résolutions spatiales et temporelles se posent. La précision exigée par la thématique varie de quelques mètres pour la description numérique des très grosses ravines, mais elle devient de quelques décimètres quand on s\u0027intéresse aux pentes locales ou aux -79 -RNTI-E-13 ravines et rigoles élémentaires. La précision peut encore atteindre quelques centimètres, voire des millimètres pour les analyses des dépôts et ablations de versants. Les photographies aériennes classiques prises par avion donnent des informations métriques. Afin d\u0027obtenir une résolution au sol de l\u0027ordre de quelques centimètres, notre choix s\u0027est porté vers un vecteur lent (drone avion) pour éviter les effets de filés sur les photos, volant à très base altitude, et utilisant un appareillage photographique « grand public » donc facilement reproductible. L\u0027utilisation de drone semble tout indiquée pour cela, bien qu\u0027elle présente deux difficultés : une connaissance de la position exacte de l\u0027appareil en vol, et une limitation de la charge utile. La facilité initiale de prise de vue est alors compensée par une nécessité de travaux associés : repérage de mires in situ pour effectuer une « aérotriangulation » correcte, rectifications de lentille et corrections diverses, que nous allons détailler.\nEn outre, la validation des modèles numériques de terrain (MNT) très détaillée n\u0027est pas une tâche facile. La quantité et la précision des données « vérité terrain » requises ainsi que la souplesse d\u0027utilisation de l\u0027appareillage sur place sont souvent hors de portée pour les outils couramment disponibles : GPS différentiel (DGPS) ou tachéométrie. Ces méthodes pourraient être assez précises mais elles ne produisent pas assez de données pour une validation approfondie. Néanmoins, le LiDAR terrestre a la potentialité pour satisfaire les conditions de validation d\u0027un MNT très détaillés.\nL\u0027objectif de cet article est de quantifier à chaque étape de la chaîne de traitement stéréophotogrammétrique, le gain de précision obtenu dans la construction d\u0027un MNT, pour un vecteur et un milieu particuliers et peu favorables.\nSite d\u0027étude et données disponibles\nLe Cemagref suit depuis 1975 des bassins versants expérimentaux d\u0027érosion dans des zones de badlands à Draix, dans les Alpes de haute Provence, récemment transformés en Observatoire de Recherche en Environnement -ORE Draix (Mathys et al., 2003). Les bassins sont situés sur des marnes noires très érodables, l\u0027érosion y est particulièrement importante : les valeurs d\u0027ablation sont supérieures à 100 t/ha/an (Oostwoud et Ergenzinger, 1998). Dans cette zone les chemins de l\u0027eau prédominants se présentent sous forme de ravines d\u0027érosion, voire de rigoles ou griffures élémentaires. Notre site porte sur une partie du bassin du Moulin (figure 1) avec des dénivelées très fortes : 40 m entre les points haut et bas, à une distance de 100 m (ravines de 10 à 20 m de large et 5 à 10 m de profondeur).\nLes données utilisées sont essentiellement 1) des photographies dans le visible prises par drone « Avion jaune » (www.lavionjaune.fr), avec un appareil photo numérique Sony DSC-P150 (à 7.2 méga pixels) ; 2) des repères topographiques au sol ou points de contrôle terrestre (PCT) et des points de vérité terrain (PV) sur les thalwegs et les crêtes, par une campagne mixte DGPS-tachéométrie ; 3) un scan LiDAR terrestre (ILRIS-3D) dans une ravine du site d\u0027étude (mai-juin 2007), avec une direction de balayage vers le Nord-Est. Dans la figure 1 nous pouvons observer la direction du scan (ligne noire pointillée en haut), ainsi que le nuage de points LiDAR (en bas de l\u0027image) et la ravine sélectionné pour le contrôle de qualité du MNT drone (polygone noir) à partir des données LiDAR.\n-80 -RNTI-E-13 -81 -RNTI-E-13\nMéthodologie\nLes grandes étapes de la méthode stéréophotogrammétrie numérique utilisée sont celles des techniques habituelles (Kraus \u0026 Waldhäusl, 1998) : 1) préparation du vol et prise de vues stéréoscopiques, 2) géométrie (orientation interne et externe), 3) aérotriangulation, et 4) restitution numérique du relief.\nLa première étape est le point du départ du développement d\u0027un MNT et comprend la sélection du vecteur, d\u0027après les objectifs de la mission, des dates et des heures de vol, du type de chambre photo, de la hauteur et la vitesse de vol, ainsi que le nombre et la précision des points de contrôle terrestre nécessaires. L\u0027étape suivante consiste à définir les caractéristiques de la chambre photo de prise de vue, ainsi que les conditions de prise de vues (position et orientation de l\u0027appareil photo). Ces paramètres d\u0027orientation interne et externe, généralement inconnus pour les appareils photo grand public et les vecteurs légers ou drones, sont à la base des calculs à développer dans la prochaine étape d\u0027aérotriangulation, où un lien entre l\u0027appareil photo, les photos et le terrain doit s\u0027établir (colinéarité). La dernière étape consiste à restituer le relief à partir des couples stéréoscopiques par corrélation locale des points homologues.\nLes paramètres d\u0027orientation inconnus entraînent des erreurs géométriques importantes pendant la restitution du relief. Il est alors évident que la souplesse de la méthode entraîne des approximations et des incertitudes à résoudre. Notre but est alors de préciser les points où la méthode devient particulière, et de vérifier jusqu\u0027où les incertitudes citées sont surmontables et à quel compromis « qualité -souplesse d\u0027utilisation » il est possible d\u0027accéder.\nL\u0027ensemble des éléments présentés correspond à des schémas classiques de constitution de MNT par couple stéréoscopique. C\u0027est l\u0027application et l\u0027adaptation de ces méthodes à des zones érodées avec de fortes pentes et avec des appareils photo grand public qui constituent le point original de notre contribution, avec un grand effort de validation de la précision sur le terrain.\nPréparation du vol et prise de vue stéréoscopique\nUn véhicule aérien non-piloté (UAV, Unmanned Aerial Vehicle, ou drone avion), est un vecteur piloté au sol. Il supporte une charge utile de l\u0027ordre de 3 à 4 kg, ce qui permet d\u0027embarquer un ou deux appareils photos « grand public », ainsi que des éléments de vol comme un GPS légers notamment (Raclot et al., 2005).\nPour une utilisation en stéréoscopie, la précision requise pour la position du capteur à l\u0027instant de la prise de vue est toutefois nettement plus forte que celle permise par ces GPS légers mono-fréquence et non-différentiels. Une qualité de positionnement requise de quelques centimètres impose la prise de points d\u0027appui au sol, afin de pouvoir faire un calcul a posteriori plus précis de la position du vecteur en fonction des éléments de l\u0027image.\nLes éléments de stéréophotographie sont particuliers dans les zones de ravines. En effet, dans la littérature on cite couramment le rapport B/h comme élément de qualité stéréoscopique (Kraus \u0026 Waldhäusl, 1998). Dans le triangle formé par le point photographié au sol et les deux positions de l\u0027appareil photo au moment des prises de vues, h représente -82 -RNTI-E-13 l\u0027altitude de la prise de vue et B est la base, c\u0027est-à-dire la distance entre les deux positions de prise de vue (figure 2).\nClassiquement un rapport B/h proche de 1 est préconisé pour une stéréoscopie « optimale ». Néanmoins, dans le cas de zones très tourmentées où les pentes des versants analysés sont excessivement fortes, les lignes d\u0027incidence de la prise de vue doivent être nettement plus verticales pour éviter des parties cachées. Dans ces conditions nous préconisons un rapport B/h plus petit (de l\u0027ordre de 0,2 à 0,3), ce qui permet un bon compromis entre altitude, distance focale, et recouvrement entre clichés.\nPar rapport au calcul des coordonnées de PCT, plusieurs solutions sont possibles : DGPS en mode RTK, qui présente des avantages en terme de rapidité de mesure et de contrôle de qualité en temps réel (Mora et al., 2003) ; tachéomètre avec une qualité millimétrique. \nGéométrie\nLa géométrie interne de l\u0027appareil photo est définie par la distance focale, le point principal de symétrie, ainsi que les déformations de lentilles. L\u0027orientation interne est fondamentalement utilisée pour transformer système de coordonnées pixel en système de coordonnées spatiales de l\u0027image (x, y, et z).\nAfin de calibrer la chambre photo en ce qui concerne l\u0027orientation interne, nous analysons des photographies particulières représentant des zones dont la géométrie est parfaitement connue. Ces prises de vue particulières peuvent être réalisées sur des mires planes parfaitement quadrillées. Néanmoins, il existe aussi des méthodes qui s\u0027appuient sur des couples de prises de vue réalisées dans des bâtiments particuliers munies des repères (mires 3D, parking sous-sol à l\u0027IGN, Paris), ce qui conduit à une expression plus stable et rapide de la déformation de lentille.\nCette méthode permet d\u0027accéder aux déformations de l\u0027image liées aux imperfections de la lentille. Ce résultat peut être modélisé par une fonction de déformation généralement polynomiale et exprimée en fonction de la distance au centre de l\u0027image, que l\u0027on peut ensuite appliquer à l\u0027image brute pour obtenir une image « théoriquement » corrigée des déformations. L\u0027équation du polynôme de distorsion radial est de la forme : F(r) \u003d a*r3+b*r5+c*r7 (1) où r est la distance radiale en pixels, a, b et c sont des coefficients.\nL\u0027orientation externe définit la position et l\u0027orientation angulaire de l\u0027appareil photo au moment de la prise de vue. Les paramètres de position de l\u0027orientation externe incluent Xo, Yo, et Zo, position du centre de perspective (O) par rapport au système de coordonnées terrestre (X, Y, Z). Zo est généralement égal à la hauteur de l\u0027appareil photo au-dessus du niveau de la mer.\nEn utilisant les trois angles de rotation : omega (?), phi ( Le vecteur image et le vecteur terrain sont seulement situés sur la même ligne droite si l\u0027un est le multiple scalaire de l\u0027autre (k) :\nUne expression matricielle de cette même relation dans le système de coordonnées image va alors s\u0027écrire sous la forme :\nL\u0027équation finale qui définit le rapport entre le centre de perspective O de l\u0027appareil photo au moment de la prise de vue et le point P sur le terrain (point p sur l\u0027image) est la suivante :\noù xp et yp sont les coordonnées image du point terrain, xo et yo sont les coordonnées images du point principal et f est la distance focale, Xp, Yp et Zp sont les coordonnées terrain du point terrain, Xo, Yo et Zo sont les coordonnées terrain du centre de perspective (figue 3). Cette équation est à la base de la colinéarité qui est employée dans la plupart des opérations photogrammétriques (ERDAS OrthoBASE user\u0027s guide, 2002).\n-84 -RNTI-E-13\nDes points additionnels communs aux deux images stéréo (dits points homologues ou PH), connus uniquement dans le système de coordonnées image, sont intégrés afin d\u0027améliorer l\u0027ajustement du modèle d\u0027aérotriangulation. Ces PH peuvent être repérés manuellement ou de façon automatique par corrélation d\u0027image.\nFIGURE 3 -Paramètres d\u0027orientation externe (d\u0027après OrthoBASE user\u0027s guide, 2002).\nAérotriangulation\nAfin d\u0027accomplir la restitution d\u0027un couple stéréo, une relation entre les paramètres de l\u0027appareil photo, des photos et du terrain doit être établie. Pour définir cette relation on utilise les paramètres d\u0027orientation interne et externe de chaque photo et une représentation précise du terrain par un ensemble de points de contrôle terrestres (PCT).\nLes paramètres d\u0027orientation externe d\u0027une photo aérienne sont normalement inconnus et le calcul de ces paramètres est l\u0027obstacle les plus important de la stéréorestitution. Selon les données d\u0027entrée disponibles, plusieurs techniques sont employées pour définir les variables exigées dans la stéréophotogrammétrie.\nParmi eux, l\u0027ajustement par paquet de blocs est le plus rigoureux considérant la minimisation d\u0027erreurs et la distribution des résidus. Casson (2002) signale que -85 -RNTI-E-13 l\u0027aérotriangulation en bloc présente les meilleures précisions et que ce résultat peut s\u0027expliquer par la condition de coplanéité et la présence des PH distribués de manière homogène sur la zone de recouvrement des clichés stéréoscopiques (ce qui compense une répartition des PCT parfois hétérogène). La condition de coplanéité établit que les deux positions du capteur d\u0027un couple stéréoscopique, ainsi qu\u0027un point au sol quelconque et sa position image correspondante sur les deux images, doivent se situer dans un plan commun.\nUne fois que chaque équation d\u0027observation est formulée, la condition de colinéarité peut être résolue en utilisant l\u0027ajustement par moindres carrés. Cette technique statistique est employée pour estimer les paramètres inconnus associés à une solution, tout en minimisant l\u0027erreur. En ce qui concerne l\u0027aérotriangulation, cette technique est employée pour : a) estimer ou ajuster les paramètres d\u0027orientation interne et externe ; b) estimer les coordonnées X, Y, et Z des PH ; c) minimiser l\u0027erreur et distribuer les résidus dans l\u0027ensemble des PCT.\nDes erreurs systématiques subsistent à l\u0027issue de l\u0027aérotriangulation et se répercutent sur le MNT et les images associées. En mode numérique il s\u0027agit des effets de la non isotropie de la déformation de lentille ainsi que des effets d\u0027une mauvaise disposition des mires dans la zone couverte par les clichés. Maatouk (2004) a réalisé des essais par rapport à la disposition des mires sur modèle réduit. Il montre des conséquences fortes sur la stabilité de l\u0027aérotriangulation (basculement par exemple). Une dissymétrie dans la disposition des mires a entraîné un déplacement en X jusqu\u0027à 5,2 cm dans l\u0027estimation de la position d\u0027un appareil photo situé à 1,60 m d\u0027altitude.\nIl existe plusieurs manières de réduire les erreurs systématiques, comme la compensation a posteriori, le calibrage par données complémentaires de terrain, et l\u0027approche par autocalibrage. Tandis que les deux premières méthodes sont coûteuses en temps, les approches par auto-calibrage emploient des paramètres additionnels dans le processus de triangulation pour minimiser les erreurs systématiques.\nLes paramètres additionnels sont un moyen extrêmement efficace pour compenser les erreurs systématiques, quelle que soit leur provenance, qu\u0027il s\u0027agisse d\u0027une distorsion résiduelle de l\u0027objectif, de déformations du film (en cas de photo argentique), d\u0027anomalies de réfraction, ou de toute autre source d\u0027erreur (Kraus \u0026 Waldhäusl, 1998).\nLa recherche et le développement intensif des modèles « auto-calibrage » dans la stéréophotogrammétrie se sont intensifiés pendant les années 1970 et 1980, et plusieurs de ces résultats sont opérationnels. Parmi eux, la solution d \u0027Ebner (1976) consiste en une recherche systématique de déformations types sur la restitution finale des images, en utilisant 12 paramètres additionnels. L\u0027efficacité de cette correction dépend de l\u0027importance relative entre erreur systématique et erreur aléatoire, donc exige une bonne qualité des mesures des PCT. Un nombre plus important de PCT et de PH est rendu nécessaire par l\u0027introduction de ces paramètres additionnels.\nRestitution numérique et création du MNT\nLa restitution numérique cherche à retrouver de manière automatique la position de chaque pixel de l\u0027image 1 représentant le même objet sur l\u0027image 2, dans la zone de recouvrement d\u0027un couple stéréoscopique. La corrélation d\u0027images permet l\u0027identification et la mesure automatiques des points homologues situés dans les zones de recouvrement des deux images.\n-86 -RNTI-E-13 La méthode de corrélation d\u0027images à une dimension utilise la géométrie épipolaire comme contrainte géométrique. Pour chaque point p1 dans une image, les différentes positions possibles du point homologue p2 dans l\u0027image associée varient selon l\u0027altitude et le lieu de ces points constitue la courbe épipolaire. La géométrie épipolaire est associée à la condition de coplanéité. La recherche de l\u0027optimum du coefficient de corrélation d\u0027un pixel est donc orientée : chaque point est cherché sur sa courbe épipolaire dans l\u0027autre image.\nLa corrélation automatique d\u0027images peut s\u0027effectuer sur une large fenêtre (par exemple 7*7 pixels) pour une solution rapide mais qui implique un adoucissement local du relief (Casson et al, 2002). Elle est implémentée dans les logiciels de stéréophotogrammétrie disponibles ce qui fournit une solution valable pour la plupart des applications.\nLe MNT final est obtenu au moyen d\u0027une compensation par moindre carré qui s\u0027appuie sur la relation de coplanéité. Les résidus sont minimisés en ajustant chaque position sol. A chaque pixel ne correspond pas forcément un couple de points homologues corrélés.. Le résultat se présente sous la forme d\u0027un ensemble de points irrégulièrement distribués (X, Y et Z). L\u0027efficacité de la corrélation dépend du pas de la corrélation et de la taille de la fenêtre de corrélation, et est également fonction du nombre de pixels effectivement corrélés.\nAfin d\u0027obtenir un MNT en surface continue il est nécessaire de faire une interpolation. Ceci peut être effectué dans une grille régulière, mais il est possible aussi de convertir le nuage de point 3D en format TIN par triangulation.\nApplication à Draix\nLe vol\nUne mission UAV drone a été effectuée sur la zone du Moulin à Draix le 15 juin 2005 à 11h30, avec des photographies numériques en couleurs naturelles. Aucun système de guidage précis (GPS embarqué) n\u0027était disponible à l\u0027époque des prises de vue. Le guidage du drone a été opéré de façon manuelle. La répartition des mires dans les images n\u0027a donc pu être optimisée « a priori ».\nNous avons préconisé finalement une altitude de vol entre 100 et 200 m selon la résolution spatiale au sol recherchée, une focale courte (40 à 80 mm) et un recouvrement entre clichés de 60%. La vitesse de vol du drone est de l\u0027ordre de 30 à 40 Km/h, ce qui permet d\u0027obtenir des photos nettes (sans effet de filé). 39 photos ont été acquises en plusieurs passages avec une hauteur de vol d\u0027environ 150 m. Nous avons sélectionné le couple photo (figure 1) qui remplit au mieux la demande de recouvrement établie précédemment. La résolution au sol ou taille de pixel sur les images est d\u0027environ 6 cm.\nLes points de terrain\nLe DGPS ne travaille pas avec la même précision dans toutes les conditions de relief, notamment en fond des ravines profondes. Etant donné cette limitation du système DGPS, nous avons choisi une solution mixte DGPS-Tachéométrie pour une précision satisfaisante (? 3 cm) dans toutes les configurations de terrain.\n-87 -RNTI-E-13\nA l\u0027intérieur de la zone de recouvrement on a 8 mires-PCT et 388 PV sur sol nu, obtenus par une campagne mixte DGPS-Tachéométrie. La qualité de la donnée de terrain a été analysée en utilisant le levé tachéométrique comme référence. Les résultats exprimés en erreur quadratique moyenne (RMSE) montrent un décalage des positionnements acquis par DGPS en mode RTK de 0,025 m selon les axes Est-Ouest et Nord-Sud, et de 0,030 m en Altitude.\nGéométrie\nLes paramètres d\u0027orientation interne ont été calculés au préalable (avant aérotriangulation), en utilisant le logiciel Poivilliers E (Etalon, IGN). Une série de photos a été prise dans un bâtiment muni de 65 cibles disposées sur différents plans de façade, avec la chambre Sony DSC-P150 utilisée pendant le vol.\nLes distorsions obtenues sur le bord de l\u0027image sont beaucoup plus importantes qu\u0027au centre. L\u0027impact du modèle de déformation de lentille calculé ci-dessus sur la qualité finale du MNT sera évalué suivant le calcul des erreurs par rapport aux données de vérité terrain.\nLes paramètres d\u0027orientation externe sont inconnus et ils ont été calculés pendant l\u0027aérotriangulation.\nAérotriangulation\nPour l\u0027aérotriangulation, nous disposons de 8 PCT (en 3D) et 326 PH détectés automatiquement par corrélation d\u0027images à l\u0027intérieur de la zone de recouvrement du couple choisi (ainsi que 4 PCT à l\u0027extérieur, figure 4). La distribution des PH n\u0027est pas homogène : le coté Nord-Ouest des images, plus végétalisé, a un faible potentiel de corrélation ce qui se traduit par une densité plus faible de PH que la zone non végétalisé. Nous avons utilisé le modèle orthogonal d \u0027Ebner (1976)  Deux PCT ont été rejetés pendant l\u0027ajustement du modèle d\u0027aérotriangulation, sur un total de 8 disponibles, ainsi que 8 PH sur un total de 326 disponibles. Dans le tableau 2 nous pouvons observer les résidus des PCT non rejetés après l\u0027aérotriangulation.\nLe PCT mire45 présente les résidus les plus importants, ce qui est cohérent avec la position de la mire (figure 4) au bord de la zone de recouvrement de deux images, où les effets de la déformation de lentille sont plus forts. La mire32, encore plus au bord de l\u0027image, a été rejetée pendant l\u0027aérotriangulation.\nMNT par corrélation d\u0027images\nEtant donné son meilleur contraste par rapport aux autres canaux disponibles, le canal rouge a été utilisé pour la corrélation automatique des images. La taille finale du pixel MNT -89 -RNTI-E-13 a été fixée à 15 centimètres (soit trois fois la taille originale moyenne du pixel dans le couple stéréoscopique).\nID PCT rX (m) rY ( \n-Résidus des PCT non rejetés après l\u0027ajustement du modèle d\u0027aérotriangulation (en mètres).\nAfin d\u0027apprécier le détail atteint dans la restitution du relief par stéréoscopie drone, nous comparons qualitativement sur une ravine un MNT issu de restitution manuelle de photos argentiques au 1/4000°, avec notre résultat. Les courbes de niveau du MNT existant sont à espacement 2 m, celles issues du couple drone sont à espacement 20 cm (figures 5a et 5b, respectivement). La finesse de détail et le plan des courbes de niveau démontrent les potentialités de la restitution détaillée du relief à partir d\u0027images drone.\n3 915 pixels ont été effectivement corrélés dans une fenêtre de 285 m 2 (points noirs, figure 5b). Cette densité représente environ 14 points par mètre carré, mais elle n\u0027est pas homogène sur la surface. Ceci correspond à une grille régulière de 20 cm approximativement, mais dans les zones densément corrélées la distance entre points peut atteindre la taille du pixel des images (autour de 6 cm). Il est important de rappeler qu\u0027un couple de pixels effectivement corrélés n\u0027exclut pas une fausse corrélation, ce qui peut être à l\u0027origine d\u0027une anomalie du MNT (un pic ou un fossé).\nFIGURE 5 -(a) courbes de niveau sur une ravine à partir de photos aériennes 1/4000 ; (b) courbes de niveau et points effectivement corrélés (en noir), à partir d\u0027images drone.\nLes points effectivement corrélés suivent les zones de rupture radiométrique dans les images, telles que les lignes d\u0027ombres. En absence de végétation, les zones de rupture radiométrique sont les lignes de contraintes classiques d\u0027un MNT : les thalwegs et les crêtes.\n-90 -RNTI-E-13\nEn présence de végétation les points effectivement corrélés sont bien contrôlés par les bordures de la végétation et ses ombres. La dépendance de la corrélation sur les points de rupture abrupte de la radiométrie laisse une grande partie de la surface des photos inexploitée dans la restitution du relief (environ 2% du total des pixels disponibles dans la zone de recouvrement ont été effectivement corrélés). La figure 6 montre une vue 3D du MNT calculé.\nFIGURE 6 -Vue 3D du MNT drone, démarquée en noir la ravine analysée dans la figure 5 (en rouge les points de validation).\nEvaluation de qualité\nPour tester l\u0027effet des différents traitements pendant l\u0027aérotriangulation dans la qualité de la restitution du relief, trois MNT ont été construits à partir du même couple stéréoscopique d\u0027images drone, avec le même jeu de PCT et de PH : a) MNT sans correction de lentille et sans auto calibrage (MNT.o) ; b) MNT avec correction de lentille mais sans auto calibrage (MNT.L) ; et c) MNT avec correction de lentille et avec auto calibrage (MNT.LA). Les résultats de l\u0027évaluation de qualité altimétrique des MNT par rapport aux données de vérité terrain DGPS-tachéométrie, sont présentés dans la figure 7.\nFIGURE 7 -Statistiques de contrôle de qualité (a) et histogrammes de fréquence des erreurs en Z (b), pour chaque MNT analysés (unités en mètres).\nL\u0027amélioration la plus importante dans la qualité du MNT provient de la correction de la déformation de lentille (cf. figure 7a et 7b). Avec l\u0027auto calibrage (toutes corrections),\n-92 -RNTI-E-13 l\u0027erreur quadratique moyenne (RMSE) en Z pour le MNT.LA diminue d\u0027environ 80% par rapport à la RMSE du MNT.o (aucune correction)..\nEn effet l\u0027application de ces corrections permet de :\n• ramener la moyenne de l\u0027erreur pour le MNT.LA à 8 cm, ce qui est à peine supérieur à la taille initiale du pixel de l\u0027image drone (6 cm).\n• Améliorer fortement la précision du MNT (écart-type), ce qui est garant d\u0027une meilleure cohérence de la représentation du relief.\n• Réduire fortement la plage de variation de l\u0027erreur (5,11 m pour le MNT.o contre 1,85 m pour le MNT.LA). La mise en oeuvre de l\u0027auto calibrage sur les images (MNT LA) nous permet d\u0027améliorer encore de manière significative, notamment en ce qui concerne les erreurs systématiques. Ces affirmations sont bien évidentes en observant les histogrammes de fréquence des erreurs pour chaque MNT analysé (figure 7b). L\u0027histogramme des erreurs pour le MNT.o (ligne bleue) est plat et étendu avec une plage de variation très importante (figure 7a). Avec l\u0027application de corrections (MNT.L et MNT.LA, lignes verte et rouge respectivement) il se déplace vers l\u0027axe des ordonnées et il se resserre, les valeurs des erreurs sont plus concentrées.\nDans la figure 8 nous pouvons observer les séries des erreurs (résidus) correspondants à un profil de crête et un profil de thalweg. La tendance générale des séries des erreurs montre l\u0027effet de déformation radiale de lentille surtout pour le MNT sans aucune correction (MNT.o, carré clair dans la figure 8), dans chaque profil. Les résidus du profil thalweg sont d\u0027une ampleur plus importante que ceux de la crête.\nAprès la correction de la déformation de lentille, les résidus qui correspondent au MNT.L (cercle foncé, figure 8) montrent déjà une amélioration significative, surtout pour la crête. Par rapport au thalweg, nous pouvons observer toujours des erreurs systématiques même si elles ont été réduites en ampleur. L\u0027utilisation de l\u0027approche par auto calibrage (MNT.LA, triangles clairs dans la figure 8) ne représente pas une amélioration significative pour le profil qui correspond à la crête. Toutefois, ce n\u0027est pas le cas du profil thalweg où le profil MNT.LA semble plus affecté par l\u0027erreur systématique.\nAnalyse selon la distance au centre de l\u0027image\nAfin d\u0027avoir une vision générale de la distribution spatiale des erreurs, nous avons calculé la distance au centre de l\u0027image (rayon) de chaque point de validation (figure 9), pour les trois niveaux de traitement étudiés. L\u0027effet de la déformation de la lentille dans la distribution des erreurs est bien évident, l\u0027erreur est plus forte dès qu\u0027on s\u0027éloigne du centre de l\u0027image (figure 9a). L\u0027application du modèle de déformation de lentille (figure 9b) montre une amélioration significative de la qualité du MNT. L\u0027effet d\u0027erreur systématique du MNT est encore visible (forme de la ligne de tendance), mais cet effet a été significativement diminué.\nLa figure 9c correspond au MNT.LA avec toutes corrections. La distribution des erreurs est plus resserrée autour de l\u0027axe des X, correspondant à la baisse de la moyenne des erreurs en Z de 0,19 à 0,08 cm (figure 7). L\u0027effet des erreurs systématiques a été aussi diminué, la pente de la ligne de tendance (0,0033, figure 9c) est plus faible que dans le cas sans auto calibrage (0,0069, figure 9b).\nFIGURE 8 -Séries des erreurs « vérité terrain (DGPS-tachéométrie) -différents MNT » selon un profil de crête et un profil de thalweg.\nLa RMSE totale finale obtenue (0,22 m), et la présence de résidus systématiques laisse encore la possibilité d\u0027améliorations à faire, surtout autour d\u0027une estimation plus performante de paramètres d\u0027orientation interne de l\u0027appareil photo (modèle de déformation de lentille, distance focale).\nAnalyse d\u0027un versant (validation par LiDAR terrestre)\nPour avoir une évaluation de qualité « surface continue » du MNT.LA, nous avons utilisé un scan LiDAR terrestre comme vérité terrain. Le balayage au sol du faisceau laser a produit un nuage dense de données, même s\u0027il y a beaucoup d\u0027espaces vides dus aux effets d\u0027ombre portée. Nous avons sélectionné une ravine où les données LiDAR étaient plus denses, en enlevant aussi la végétation de l\u0027information originale. Cette ravine est délimitée par un polygone ( figure 1 en bas, et figure 10).\n-94 -RNTI-E-13 -95 -RNTI-E-13\nUne nouvelle grille régulière a été calculée à l\u0027intérieur du polygone LiDAR sélectionné. Cette grille a été ré-échantillonnée avec une maille de 5 cm en utilisant la valeur moyenne comme filtre de bruit dans l\u0027information brute et puis une interpolation, afin de créer un MNT-LiDAR.\nUne évaluation de qualité a été faite dans le polygone sélectionné (figure 10), en comparant le MNT.LA drone aux deux sources de données de vérité terrain : 1) 30 points de contrôle de qualité DGPS-Tachéométrie, et 2) 101 912 pixels du MNT-LiDAR terrestre (pixel 5 cm). Les résultats (tableau 4) montrent que même si l\u0027écart type en Z est similaire pour les deux jeux de données de vérité terrain, la RMSE est plus importante pour le LiDAR terrestre. Malgré une évaluation un peu moins précise avec le LiDAR terrestre, celui-ci reste très intéressant parce qu\u0027il propose un très grand nombre de points de validation, y compris sur des zones d\u0027accès difficile où on ne peut aller avec le GPS.\nDGPS-\nNous avons établi une différence entre le MNT.LA drone et le MNT-LiDAR de référence, afin d\u0027effectuer une évaluation de qualité en « surface continue » (MNT.LA moins MNT-LiDAR, figure 10).\nLes valeurs positives de cette différence sont principalement situées en tête de ravine, et moins évidemment le long de son côté droit, dénotant une sur estimation de Z pendant la corrélation d\u0027images drone. Nous pouvons trouver une situation inverse au côté gauche de la même ravine, où la sous estimation est plus évidente.\nLes nombreuses taches bleues et rouges intenses sont associées à des anomalies locales de type « pics et fossés », respectivement, dus aux fausses corrélations pendant la restitution numérique du relief. Ces nombreuses taches isolées affectent la qualité finale du MNT mais aussi sa continuité et sa cohérence. Elles pourraient être éliminées dans notre cas par un post traitement de type algorithme de lissage. D\u0027autres solutions, en développement, intègrent ces régularisations directement dans l\u0027algorithme de corrélation d\u0027image, par exemple le logiciel MicMac (Pierrot D., 2007).\nCes appréciations n\u0027auraient pas été possibles en utilisant seulement les points de validation pris par DGPS-Tachéométrie.\nL\u0027évaluation d\u0027erreur en Z dépend non seulement du décalage en Z lui-même mais également des décalages selon les axes X et Y, particulièrement dans un relief très accidenté comme Draix. Une optimisation 3D (MNT.LA au MNT-LiDAR) a été effectuée pour calculer le décalage moyen global dans chaque axe : -0,004, 0,014, et -0,272 (tous en mètres) pour X, Y et Z respectivement. Ces résultats montrent un décalage significativement plus important en Z, par rapport aux axes X et Y.\n-96 -RNTI-E-13 \nConclusion\nLe problème traité dans l\u0027article concerne l\u0027utilisation de drone dans l\u0027acquisition d\u0027images stéréoscopiques pour le développement de MNT précis et cohérents par stéréophotogrammétrie. Les vecteurs légers ne permettent pas d\u0027utiliser des appareils photo professionnels, certes plus précis mais trop lourds pour être placé sur des drones. Les prises de vue par appareil « grand public » permettent une grande souplesse d\u0027utilisation. Notre objectif était de préciser les points où la méthode devient particulière, et de vérifier jusqu\u0027où les incertitudes citées sont surmontables et à quel compromis « qualité -souplesse d\u0027utilisation » on peut arriver. L\u0027application de la méthode IGN (Poivilliers E) pour le calcul des paramètres d\u0027orientation interne a été traduite par une amélioration très significative dans la précision du MNT, en passant d\u0027une RMSE totale de 1,09 à 0,31 mètre. A ce stade, il reste toujours des erreurs systématiques résiduelles. Elles sont probablement dues à plusieurs effets : instabilité de vol et disposition non régulière des mires dans les images ; déformations de lentille non parfaitement corrigées liées au modèle (polynomial) de correction utilisé. L\u0027approche par auto calibrage (Ebner, 1976) compense en partie ces erreurs systématiques. La moyenne finale de l\u0027erreur est de 8 cm, proche de la taille du pixel de l\u0027image drone (6 cm). La RMSE totale obtenue est alors de 22 cm, soit trois fois et demie la taille de pixel initial de l\u0027image.\nToutefois, il faut remarquer que l\u0027analyse de distribution d\u0027erreurs par rapport à la position dans le paysage, reflète que les observations sur les crêtes atteignent une précision significativement meilleure que sur les fonds des ravines. Ceci est dû au contraste entre les images et les effets d\u0027ombres portées, toujours plus présentes au fond des ravines très profondes.\nAfin de bien développer le potentiel des images drone pour la restitution du relief par stéréophotogrammétrie dans des zones à fortes pentes (sans sacrifier la souplesse d\u0027utilisation), on doit surmonter plusieurs contraintes comme l\u0027instabilité de vol et les déformations de lentille. Un autre facteur perfectible est l\u0027efficacité de la corrélation d\u0027images, qui contrôle la qualité de la restitution de relief. La distribution de points effectivement corrélés montre en effet la dépendance pratiquement exclusive de la corrélation sur les points de rupture abrupte de la radiométrie, en laissant ainsi une grande partie de la surface des photos inexploitée. L\u0027algorithme de corrélation d\u0027images disponible sur ERDAS Imagine LPS ne dispose pas de mécanismes pour contrôler les fausses corrélations entre pixels d\u0027un couple stéréoscopique, résultant en nombreuses irrégularités dans la surface du MNT (pics et fossés). Les données « vérité terrain » par LiDAR terrestre ont permis de détecter ces irrégularités grâce à l\u0027énorme densité de l\u0027information disponible, et elles constituent un excellent complément aux données DGPS-Tachéométrie dans la validation de MNT très détaillés.\nLe développement de routines plus performantes destinées à augmenter la densité de points effectivement corrélés en modifiant le critère de sélection des points, seraient certainement une avancée dans ce domaine : par exemple le corrélateur MEDICIS (CNES), qui recherche un optimum local en analysant le rapport signal à bruit des images (Casson et al., 2002).\nNos résultats montrent une amélioration significative par rapport aux résultats de Henry et al. (2002), Maatouk (2004) et Raclot et al. (2005), dans les mêmes conditions de relief. Les résultats révèlent le potentiel du vecteur drone pour le développement des MNT peu coûteux, avec une très haute résolution spatiale, malgré de nombreuses contraintes.\n"
  },
  {
    "id": "831",
    "text": "Introduction\nLes motifs séquentiels sont étudiés depuis plus de 10 ans (Agrawal et Srikant (1995)). Ils ont donné lieu à de nombreuses applications.Des algorithmes ont été proposés, basés sur le principe d\u0027Apriori (Masseglia et al. (1998); Zaki (2001); Ayres et al. (2002)) ou sur d\u0027autres propositions (Pei et al. (2004)). Récemment, les motifs séquentiels ont été étendus aux motifs séquentiels multidimensionnels par Pinto et al. (2001), Plantevit et al. (2005), et Yu et Chen (2005) dans l\u0027objectif de prendre en compte plusieurs dimensions d\u0027analyse. Par exemple, dans Plantevit et al. (2005), les règles telles que Un client qui achète une planche de surf avec un sac à NY achète plus tard une combinaison à SF sont découvertes. Toutefois, le nombre de motifs extraits dans une base de données peut être très important. C\u0027est pourquoi des représen-tations condensées telles que les motifs clos ont été proposées pour l\u0027extraction des itemsets (Pasquier et al. (1999); Pei et al. (2000); Zaki et Hsiao (2002); El-Hajj et Zaïane (2005)) et des séquences (Yan et al. (2003); ). Les clos permettent de disposer à la fois d\u0027une représentation condensée des connaissances extraites et d\u0027un mécanisme d\u0027extraction plus efficace afin d\u0027élaguer significativement l\u0027espace de recherche. Néanmoins, ces propositions ne peuvent pas être directement appliquées aux motifs séquentiels multidimensionnels pour la raison suivante : une super séquence peut être obtenue de deux façons (1) une plus longue séquence (plus d\u0027items) ou (2) une séquence plus générale (plus de valeurs non spécifiées) ce qui modifie les définitions des méthodes précédemment introduites.\nNotre contribution majeure est la définition d\u0027un cadre théorique pour l\u0027extraction de motifs séquentiels multidimensionnels clos ainsi qu\u0027un algorithme permettant de rechercher de tels motifs. Nous adoptons une méthode basée sur le paradigme \"pattern growth\" (Pei et al. (2004)) afin de proposer une solution d\u0027extraction de motifs séquentiels multidimensionnels clos efficace. De plus, nous souhaitons définir un algorithme qui se dispense de gérer un ensemble de clos candidats, seules les séquences closes étant ajoutées à l\u0027ensembles des clos.\nMotivations et Problématique\nNous définissons ici le cadre théorique de l\u0027extraction des motifs séquentiels multidimensionnels clos à partir de l\u0027approche définie par Plantevit et al. (2005).\nDéfini par Pasquier et al. (1999), un motif clos est un motif qui n\u0027a pas le même support que tous ses super-motifs. Les motifs clos permettent de représenter les connaissances extraites de manière compacte sans perte d\u0027information et sont généralement associés à des propriétés qui permettent de réduire sensiblement l\u0027espace de recherche à l\u0027aide d\u0027opérations d\u0027élagage autres que l\u0027élagage élémentaire des motifs infréquents. Dans un contexte multidimensionnel, une séquence peut être plus spécifique qu\u0027une autre si elle contient plus d\u0027items (séquence plus longue), ou si elle contient des items plus spécifiques (moins de valeurs *).\nSi ? est plus spécifique que ?, nous notons ? ?S ? où ?S représente la relation de spécialisation. Soient s1 \u003d b1, c1), (a2, * , c1)}{( * , b2, c2)} s2 \u003d * , * ), (a2, * , c1)}{( * , b2, c2)} et s3 \u003d b1, c1)}{( * , b2, c2)} trois séquences multidimensionnelles. On a s2 ?S s1 et s3 ?S s1. A partir de cette définition, nous pouvons définir une séquence multidimensionnelle close. Le contexte multidimensionnel rend l\u0027approche générer-élaguer très difficile en raison du très grand nombre de combinaisons d\u0027items possibles. Nous utilisons donc le paradigme \"pattern growth\" introduit par Pei et al. (2004) s\u0027appuyant sur un parcours en profondeur de l\u0027espace de recherche. L\u0027extraction des motifs se fait en concaténant à la séquence traitée (préfixe) les items fréquents sur la base de données projetée par rapport à cette séquence préfixe. Le terme de g-k-séquence désigne les séquences composées de k items au sein de g itemsets. \n}, {e\n1 , e 2 , . . . , e\nLorsqu\u0027on considère des séquences d\u0027itemsets, l\u0027opération de concaténation peut s\u0027effectuer de deux façons différentes : (1) concaténation inter itemset où l\u0027item est inséré dans un nouvel itemset (le (g + 1)\n` eme itemset de la séquence) :\n(2) concaté-nation intra itemset où l\u0027item est inséré dans le dernier itemset de la séquence (le g ` eme itemset de la séquence) : S ? \u003d s 1 , s 2 , . . . s g ? {e ? } . Ordonner les items au sein des itemsets est un des moyens d\u0027améliorer le processus d\u0027extraction en éliminant de façon efficace des cas déjà examinés. La valeur joker * n\u0027existe pas comme valeur réelle dans la base de données. Ainsi, les solutions proposées dans un contexte classique par Yan et al. (2003) (CloSpan) et  (BIDE) ne sont pas directement applicables au contexte multidimentionnel avec valeur joker.\nTAB. 1 -Contre exemple : ordre dans les itemsets\nLe Tab. 1 illustre le fait que la valeur joker n\u0027est pas explicitement présente dans les nuplets, il n\u0027est pas possible de définir un ordre lexicographique total. Ainsi, il n\u0027est pas possible d\u0027obtenir la séquence 1 , b 2 ), ( * , b 1 )} CloSpan extrait l\u0027item (a 1 , b 2 ) avec un support de 2 et construit ensuite la base projetée à partir de la séquence 1 , b 2 )} qui contient les sé-quences et 2 , b 1 )} L\u0027item ( * , b 1 ) n\u0027apparaîtra donc pas comme fréquent dans cette base projetée alors qu\u0027il l\u0027est dans la base initiale. Il est donc nécessaire d\u0027ordonner les sé-quences en prenant en compte le caractère joker (*) comme valeur de dimension possible.\nUn pré-traitement sur la base de données par extension à l\u0027ensemble des n-uplets contenant la valeur joker étant trop coûteux, nous souhaitons traiter cette particularité à la volée pendant le processus d\u0027extraction de motifs séquentiels multidimensionnels clos. C\u0027est pourquoi nous introduisons un ordre lexico-graphico-specifique(LGS) qui est un ordre alpha-numérique par rapport au degré de précision des items (nombre de * dans l\u0027item). La priorité est ainsi donnée aux items les plus spécifiques.Nous tentons de matérialiser localement cet ordre au sein de chaque transaction à l\u0027aide d\u0027une fonction LGS-Closure qui est une application d\u0027un itemset i vers la fermeture de i en respectant l\u0027ordre LGS \u003c lgs .\nExtensions et clos\nActuellement, la plupart des algorithmes d\u0027extraction de motifs clos ont besoin de maintenir l\u0027ensemble des clos (ou juste candidats) en mémoire et vérifier en post traitement si un motif peut être absorbé ou non par un autre motif. Mais la maintenance d\u0027un tel ensemble est très coûteuse, c\u0027est pourquoi notre objectif est d\u0027éviter une telle gestion. D\u0027après la définition d\u0027un motif séquentiel multidimensionnel clos, si une g-k-séquence S \u003d s 1 , . . . , s g n\u0027est pas close alors il existe une séquence S ? de même support telle que S ? S S ? . La définition 4 présente les cinq différents types de construction d\u0027une séquence plus spécifique à partir d\u0027une séquence préfixe. \nExtraction de Motifs Séquentiels Multidimensionnels Clos\nNous verrons que le dernier point peut être facilement détecté grâce à l\u0027ordre de parcours dès lors que les précédents le sont.\nThéorème 1 (Extension bi-directionnelle). Une séquence S est close si et seulement si elle n\u0027accepte aucune extension vers l\u0027avant, ni extension vers l\u0027arrière, ni spécialisation.\nPour déterminer si une séquence préfixe est close, nous devons donc vérifier si elle ne peut pas avoir d\u0027extension vers l\u0027avant ou vers l\u0027arrière ainsi que de spécialisation d\u0027item. Le lemme suivant facilite l\u0027étude des extensions vers l\u0027avant.\nLemme 1. Pour une séquence S, l\u0027ensemble complet des extensions vers l\u0027avant est équi-valent à l\u0027ensemble des items localement fréquents sur la base projetée par rapport à S ayant un support égal à support(S).\nPour les extensions vers l\u0027arrière, la recherche d\u0027extension est moins triviale. En effet, une extension vers l\u0027arrière peut être réalisée de deux façons différentes :\n. . , s g . Soit un item s\u0027insère dans un nouvel itemset, entre deux itemset s i et s i+1 existants (inter-itemsets), soit il s\u0027insère dans un itemset existant (intra itemset). Comme une séquence peut se répéter plusieurs fois à l\u0027intérieur d\u0027une séquence de données, on peut identifier g intervalles pour localiser les possibles insertions vers l\u0027arrière d\u0027une g-k-séquence. Il faut maximiser ces intervalles afin de détecter toutes les extensions possibles vers l\u0027arrière.\nDéfinition 5.\nEtant données une g-k-séquence préfixe S p \u003d 1 , s 2 , . . . , s g et une séquence de données S, le i ` eme intervalle maximal se définit de la façon suivante : pour i \u003d 1 : la sous-séquence du début de S jusqu\u0027à strictement avant da(s 1 ) la dernière apparition de s 1 dans S telle que da(s 1 ) \u003c da(s 2 ) \u003c . . . \u003c da(g) pour 1 \u003c i ? g : la sousséquence entre la première apparition de la séquence 1 , s 2 , . . . , s i?1 notée pa( 1 , s 2 ,-. . . , Une séquence préfixe ne peut pas être close s\u0027il existe une spécialisation d\u0027un item de la sé-quence préfixe. L\u0027ordre LGS, que nous adoptons, nous permet d\u0027extraire les séquences closes en commençant par celles qui contiennent les items les plus spécifiques (le moins de valeurs * ). Ainsi, s\u0027il existe une spécialisation possible d\u0027une séquence préfixe considérée, alors la \"séquence spécialisée\", qui contient au moins un item plus spécifique, sera déjà présente dans l\u0027ensemble des clos déjà extraits. Ainsi, si une séquence est potentiellement close (pas d\u0027extensions vers l\u0027avant ou l\u0027arrière), il suffit de vérifier qu\u0027il n\u0027existe pas de séquence plus spécifique dans l\u0027ensemble des séquences closes déjà extraites.\nElagage de l\u0027espace de recherche\nTout en recherchant les nouvelles séquences fréquentes avec l\u0027algorithme d\u0027énumération des séquences, nous pouvons utiliser la propriété de fermeture bidirectionnelle pour vérifier si la séquence est close dans le but de générer un ensemble non redondant de connaissances. Bien que la propriété de fermeture retourne un ensemble plus compact, cela ne permet pas d\u0027extraire les séquences plus efficacement. Par exemple, il peut n\u0027y avoir aucun clos au delà d\u0027un certain noeud dans l\u0027arbre des préfixes, il faudrait donc éviter de parcourir inutilement la branche et réduire ainsi significativement l\u0027espace de recherche.\nComme nous l\u0027avons dit précédemment, une séquence peut apparaître plusieurs fois dans une séquence de données. Dans la définition 5, nous avons introduit la notion d\u0027intervalle maximal afin de pouvoir détecter toutes les extensions vers l\u0027arrière. Nous désirons minimiser ces intervalles afin de détecter les séquences \"non-prometteuses\". Nous définissons ainsi la notion d\u0027i ` eme intervalle minimal. \nTravaux Connexes\nNos travaux sont au carrefour de plusieurs problématiques : (1) l\u0027extraction de séquences multidimensionnelles, (2) l\u0027extraction de séquences closes. Pinto et al. (2001) sont les premiers à aborder le problème de l\u0027extraction de motifs séquen-tiels dans un contexte multidimensionnel. Les séquences extraites ne contiennent pas plusieurs dimensions puisque la relation d\u0027ordre (temps) concerne uniquement la dimension produits. Les autres dimensions sont \"statiques\" et seulement utilisées pour caractériser le profil des utilisateurs. Yu et Chen (2005) proposent d\u0027extraire des séquences dans un contexte de web usage mining en considérant trois dimensions (pages, sessions, jours) qui appartiennent à une même hiérarchie. Ainsi, les séquences extraites décrivent des corrélations temporelles entre objets en considérant une seule dimension (pages). Plantevit et al. (2005) proposent des règles définies sur plusieurs dimensions d\u0027analyse non \"statiques\".\nMême s\u0027il existe de nombreux travaux pour l\u0027extraction d\u0027itemsets clos (Pasquier et al. (1999); Pei et al. (2000); Zaki et Hsiao (2002);El-Hajj et Zaïane (2005)), il n\u0027y a, à notre connaissance, que deux propositions pour les motifs séquentiels clos : BIDE de  et CloSpan de Yan et al. (2003). CloSpan et BIDE ne peuvent pas être directement adaptés dans notre contexte multidimensionnel à cause de la valeur joker. De plus CloSpan gère un ensemble de séquences closes candidates et effectue un post-traitement coûteux (quadratique en la taille de l\u0027ensemble).\nNous pouvons également citer les travaux de Songram et al. (2006) qui abordent le problème des motifs séquentiels clos dans un contexte multidimensionnel en proposant une repré-sentation condensée des motifs définis par Pinto et al. (2001). Cependant, il s\u0027agit de séquences définies sur une seule dimension où les autres dimensions sont \"statiques\".\nConclusion\nDans cet article, nous avons proposé une approche complète (définitions et algorithmes) pour l\u0027extraction de motifs séquentiels multidimensionnels clos. Ces motifs permettent d\u0027obtenir une représentation condensée de l\u0027ensemble des motifs séquentiels multidimensionnels sans aucune perte d\u0027information. De plus, ceci permet de calculer différentes mesures (e.g. la confiance pour les règles séquentielles) sans passe supplémentaire sur la base de données puisque tous les supports sont connus. Outre leur puissance représentative, les motifs multidimensionnels clos permettent d\u0027utiliser des propriétés supplémentaires d\u0027élagage, ce qui est prépondérant pour assurer le passage à l\u0027échelle de telles techniques d\u0027extraction. Notre approche adopte le paradigme pattern growth et permet l\u0027apparition de valeurs joker * dans les motifs pour une extraction plus pertinente.\nLes perspectives associées aux motifs séquentiels multidimensionnels clos sont nombreuses : prise en compte des hiérarchies, autres représentations condensées (non-dérivables Calders et Goethals (2002), k-libres Boulicaut et al. (2003)) et extraction de motifs séquentiels multidimensionnels sous contraintes (top k).\n"
  },
  {
    "id": "832",
    "text": "Introduction\nL\u0027annotation sémantique est devenue l\u0027une des approches privilégiées par les travaux sur le web sémantique. Les travaux visant à extraire semi-automatiquement ces annotations, plus particulièrement à partir de textes, ont connu ces dernières années une avancée importante. Dans ce contexte, des outils de traitement automatique de la langue naturelle (TALN) sont proposés. Ces outils reposent en général sur des méthodes linguistiques telles que la projection de patrons morpho-syntaxiques ou des méthodes statistiques (fréquence d\u0027apparition). Les méthodes de TALN peuvent être semi-automatiques (l\u0027intervention de l\u0027expert du domaine est alors requise) ou automatiques (dans ce cas, les approches proposées requièrent une certaine spécialisation dans un domaine particulier (Aussenac-Gilles et al., 2006)). Les approches utilisées jusqu\u0027à présent reposent en général sur l\u0027extraction de termes, certaines permettent également l\u0027extraction de relations entre ces termes, mais en ignorant en général le contexte de leur apparition.\nDans le cadre de cette problématique, nous proposons une approche de modélisation, d\u0027extraction et d\u0027exploitation des annotations, qui prenne en compte leurs contextes. La limite observée, concernant les approches d\u0027extraction des termes pour l\u0027annotation, a été notre principale motivation pour offrir des annotations qui représentent au mieux le contenu d\u0027un document. Nous considérons l\u0027annotation sémantique d\u0027un document comme une image par un annotateur (humain ou programme) du contenu de ce document. Cette annotation sémantique doit être exploitable par la machine et de la qualité de cette image dépend son exploitation par l\u0027application visée. Ce travail s\u0027inscrit dans le cadre du projet SEVENPRO qui a comme objectif de développer, en reposant sur des technologies et des outils qui aident à la fouille de connaissances sur un produit, des corpus de textes multimédia et sur la réalité virtuelle 3D enrichie sémantiquement.\nTout d\u0027abord, dans la section 2, nous allons analyser quelques travaux sur l\u0027extraction des annotations à partir du texte. Puis dans la section 3, nous aborderons notre proposition sur la modélisation de la notion du contexte. Dans la section 4, nous proposerons notre approche d\u0027extraction automatique des annotations contextuelles illustrée par des exemples issus du projet SEVENPRO et nous étudierons les possibilités pour inférer/exploiter les annotations contextuelles. La section 5 présentera nos conclusions.\nEtat de l\u0027art\nNous nous intéressons plus particulièrement aux travaux qui permettent de produire des annotations d\u0027une manière automatique ou semi-automatique et qui concernent la notion de contexte. Cependant, pour une vue plus approfondie sur l\u0027annotation sémantique, citons (Prié Y. et al., 2004) ou encore (Amardeilh F., 2007).\nPar rapport à notre problématique d\u0027utilisation de la notion de contexte lors de l\u0027extraction des annotations, nous avons classé les travaux en deux grandes catégories : extraction d\u0027annotation par le contenu du document et extraction d\u0027annotation à partir de sources externes au document.\nExtraction d\u0027annotation par le contenu du document\nOn distingue deux types de techniques d\u0027annotation de documents par le contenu (ou indexation) : la technique classique, qui consiste en général à attribuer un ensemble de mots clés (ou termes) à chaque document, et la technique sémantique qui attribue une annotation basée sur des concepts (et non de simples mots-clés) et éventuellement sur les relations entre eux. Les travaux visant à extraire des annotations par le contenu se focalisent généralement sur l\u0027extraction des termes. (Guarino et al., 1999)  (Khelif et al., 2005) prennent également en compte les relations sémantiques entre termes. Dans (Guarino et al., 1999), les auteurs décri-vent OntoSeek un système de recherche documentaire en ligne pour les « pages jaunes ». Afin de construire automatiquement des résumés, (Berri J., 1996) utilise la méthode d\u0027exploration contextuelle (Desclés J. P. et al., 1994) qui repose sur des critères linguistiques et qui consiste à affecter des étiquettes sémantiques aux phrases contenant des indicateurs pertinents. Dans (Desmontils et al., 2002), est proposée une approche supervisée pour indexer des ressources web en reposant sur le contenu des pages web à l\u0027aide d\u0027une ontologie ; les termes sont pondérés par rapport à leur importance dans la page (titre, paragraphe,…). D\u0027autres travaux utilisent les techniques d\u0027extraction d\u0027information pour l\u0027annotation de textes dans un domaine particulier : par exemple, la génomique (Nédellec C., 2004).\nExtraction d\u0027annotation à partir de sources externes au document\nLes travaux cités dans cette partie font appel à des ressources externes au document. (Njmogue, et al. 2004) propose une approche basée sur un référentiel métier. L\u0027idée principale est que l\u0027indexation d\u0027un document dépend des activités de l\u0027entreprise et non pas des mots clés du document. Cette approche utilise à la fois une analyse linguistique et statistique du document et un traitement sémantique. Nous pouvons souligner que les activités de l\u0027entreprise peuvent être considérées comme un contexte d\u0027utilisation des documents. Dans (Abrouk, 2006), l\u0027auteur propose une approche pour l\u0027annotation semi-automatique de ressources selon les liens de référencement et ce, sans connaissance préalable du contenu du document. D\u0027autres travaux se sont intéressés à modéliser le processus de recherche d\u0027information et la modélisation de l\u0027utilisateur. Nous citons à titre d\u0027exemple (Hernandez, 2005)  \nType d\u0027objets manipulés\nNous nous intéressons à l\u0027extraction d\u0027annotations contextuelles à partir de textes : par conséquent, les objets que nous manipulons sont de type textuel. Un « Objet Textuel » (OT) est défini comme un élément du texte (mot, terme, phrase, titre, texte mis entre parenthèses, paragraphe, section, partie de phrase,…) qui transmet une sémantique. Nous définissons aussi un « Objet Sémantique » (OS) comme la sémantique transmise par un objet textuel et dont nous cherchons à déterminer le contexte. D\u0027une manière plus simple, un objet sémanti-que est la représentation sémantique associée à un objet textuel. Nous avons adopté dans notre approche, la représentation de la sémantique par des concepts et des relations provenant d\u0027une ontologie. L\u0027utilisation de la structure du texte, à travers les objets textuels, nous permet de choisir le niveau de détail à étudier ou « granularité ». En effet, nous pouvons étudier, à titre d\u0027exemple, d\u0027une part les « paragraphes » et les RC entre eux, et d\u0027autre part les RC entre « phrases ». La difficulté réside principalement dans le choix de la bonne granularité à étudier. En outre, la notion de granularité vient conforter notre vision sur la récursivi-té entre les contextes et ainsi assurer la présence du contexte à tous les niveaux d\u0027abstraction.\nPar conséquent, nous pouvons étudier les RC, non seulement entre les objets textuels du même niveau d\u0027abstraction, mais entre les objets appartenant à des niveaux différents.\nLa portée de validité d\u0027une sémantique\nDéfinir une portée de validité d\u0027un objet sémantique donné revient à chercher un ensemble d\u0027objets sémantiques dans lequel il est utilisable pour raisonner sans produire des incohé-rences sémantiques. Du point de vue des objets sémantiques, la notion de validité permet de distinguer : les objets sémantiques valides quel que soit le contexte et les objets sémantiques valides dans un (ou plusieurs) contexte(s) particulier(s). Pour le contexte, la notion de validité offre la possibilité d\u0027étudier la portabilité entre contextes, c\u0027est-à-dire la validité dans un autre contexte, des objets sémantiques valides dans un contexte donné.\nJusqu\u0027à présent, nous avons montré notre vision du contexte et sa modélisation avec les annotations contextuelles. Reste à élaborer une approche permettant d\u0027extraire ces annotations contextuelles.\nExtraction et exploitation des annotations contextuelles\nAvant de proposer l\u0027approche d\u0027extraction des annotations contextuelles, définissons les différentes relations contextuelles structurelles/sémantiques que nous cherchons à extraire.\nRelations contextuelles structurelles/sémantiques\nComme souligné précédemment, plusieurs niveaux de granularité existent, pour les annotations contextuelles, selon les objets textuels que nous voulons étudier (phrase, paragraphe,…). Nous allons proposer une approche basée sur un niveau de granularité particulier que nous jugeons riche sémantiquement. Les objets textuels, que nous manipulerons à ce niveau, ont les « liens logiques » comme délimiteurs. Les liens logiques (Asher et al., 2003) (relation rhétorique, connecteurs logiques ou relations de discours,…) sont définis comme les liens qu\u0027entretiennent les idées entre elles dans un texte argumentatif et servent aussi à assurer les articulations et la progression d\u0027un texte.\nDes travaux se sont déjà intéressés à identifier ces relations dans un texte, nous citons (Saito et al., 2006) qui décrit un système permettant d\u0027identifier les relations de discours entre deux phrases qui se suivent en langue japonaise. Aussi, (Marcu et al., 2002) propose une approche non supervisée pour identifier les catégories des relations de discours (CONTRAST,…). D\u0027autres (Teufel et al., 2000) proposent une approche qui détecte les actions et les affecte à leurs types en exploitant la structure des documents scientifiques (Aim, Own,…). Une autre approche (Desclés J. P., 2006)  \nRelations contextuelles spatiales\nNous définissons une relation contextuelle spatiale comme toute relation exprimant la position d\u0027un objet (textuel ou sémantique) par rapport aux autres objets de même granularité ou non. Pour les objets textuels, nous pouvons prendre à titre d\u0027exemple les relations de succession, appartenance,…. Dans le cas des objets sémantiques nous aurons par exemple des relations exprimant un rang (devant, derrière, après...) ou un lieu (dans, chez, sous...).\nRelations contextuelles temporelles\nNous définissons une relation contextuelle temporelle comme toute relation exprimant la notion de temps entre un objet (textuel ou sémantique) et d\u0027autres objets du même niveau de granularité ou non. Pour les objets sémantiques, nous pouvons avoir à titre d\u0027exemple comme liens logiques (avant, depuis, pendant,...). Dans le cas des objets textuels, nous pouvons considérer le temps des verbes (présent, futur,…) comme une relation temporelle qui exprime le moment où la sémantique de ces objets sera manifestée.\nRelations contextuelles diverses\nNous appelons relations contextuelles diverses des relations exprimant une notion séman-tique quelconque (ni spatiale ni temporelle) d\u0027un objet (textuel ou sémantique) par rapport aux autres objets du même niveau de granularité ou non.\nDans le cas d\u0027un objet textuel, un exemple de ce type de relation contextuelle peut être : le degré d\u0027importance entre un paragraphe et son titre. Pour les objets sémantiques, toutes les relations de lien logique qui n\u0027expriment pas l\u0027espace ou le temps peuvent être prises en compte : une addition (de plus, d\u0027ailleurs,…), une illustration (ainsi, comme,…).\nApproche d\u0027extraction des annotations contextuelles\nNous proposons une approche d\u0027extraction d\u0027annotations contextuelles répartie en deux grandes parties : manipulation textuelle et manipulation sémantique ( voir FIG. 1).\nFIG. 1 -Etapes d\u0027extraction des annotations contextuelles\nExtraction et exploitation des annotations contextuelles\nManipulation textuelle\nCette partie a pour objectif d\u0027obtenir un ensemble d\u0027OT et les RC structurelles entre eux. Identification des objets textuels. Identifier ces objets revient à identifier les titres, phrases, lien logique, mots,… ainsi que les arguments de chaque lien logique présent dans le texte. Nous avons utilisé dans cette étape la bibliothèque de développement d\u0027ingénierie linguistique GATE (Cunningham al., 2002) qui repose sur l\u0027application successive (chaîne de traitement) de transducteurs 1 aux textes. Suivant une liste de liens logiques, des règles JAPE 2 sont gérées automatiquement pour obtenir la position dans le texte des liens logiques. D\u0027autres règles sont construites manuellement pour identifier les marqueurs des indices numériques qui précèdent certaines phrases. Les règles JAPE sont appliquées en tant que transducteur dans la chaîne de traitement. D\u0027autres transducteurs fournis par défaut dans GATE nous ont permis d\u0027identifier les phrases et les paragraphes dans le texte. Nous avons exploité, à ce niveau, les marqueurs de position tels que le début et la fin dans le texte (d\u0027une phrase, d\u0027un lien logique,…) pour identifier les arguments des liens logiques dans le texte. Identification des relations contextuelles structurelles. Afin d\u0027identifier les RC structurelles citées dans (4.1.1), nous avons (a) identifié les titres à l\u0027aide des marqueurs numériques, qui précèdent certaines phrases tels que « les numérotations de type \u00276.2.1\u0027», ainsi que des heuristiques telles que « une seule phrase existante dans le paragraphe qui contient le titre (cette phrase représente le titre lui même) ». Nous soulignons que le transducteur fourni par défaut dans GATE pour identifier les titres, donne une faible précision, ce qui nous a menés à introduire ces heuristiques pour les identifier automatiquement; (b) calculé la portée des titres et construit les imbrications entre eux (nous aurons ainsi défini quel paragraphe et quel sous-titre appartiennent à quel titre); (c) construit les imbrications entre les paragraphes, les phrases et les arguments en utilisant les marqueurs de position dans le texte. Une fois la structure hiérarchique du texte construite, les RC structurelles peuvent être déduites.\nManipulation sémantique\nNous nous intéressons dans cette partie aux OS et aux RC sémantiques qui les relient. Identification des objets sémantiques. Identifier ces objets signifie représenter la sémanti-que des OT, par un formalisme de représentation des connaissances. Nous avons opté pour une représentation avec le standard RDF(S) reposant sur la notion des triplets (ressource, propriété, valeur). Dans notre approche, nous supposons qu\u0027une ontologie est déjà construite. Pour associer les OT à des triplets RDF en se référant à l\u0027ontologie, nous proposons d\u0027identifier les ressources (ou concepts), les propriétés et les valeurs (instances) dans le texte. Pour cela, nous proposons de construire des règles JAPE d\u0027une manière automatique. En effet, l\u0027idée principale est de bénéficier de la propriété rdfs:label, dans un schéma RDFS, pour construire des règles JAPE qui détectent les différentes manifestations d\u0027un concept (ou propriété) dans le texte. La propriété rdfs:label représente le nom lisible par un humain d\u0027un concept (ou propriété) dans le texte. Par ailleurs, les règles JAPE qui détectent les instances sont construites en se référant à un document RDF qui contient la liste d\u0027instances pour cha-1 Un transducteur est un automate à états finis qui, pour chaque état parcouru, produit une ou plusieurs informations. 2 JAPE (Java Annotation Patterns Engine) est un langage d\u0027expression de grammaires pour le TALN (un exemple est donné dans 4.2.3). que concept. Souvent, la détection des instances est difficile puisque nous ne connaissons pas auparavant les instances et à quel concept elles réfèrent. Le document RDF qui contient les listes des instances est une particularité propre aux données que nous manipulons dans notre expérimentation. Par la suite, les règles JAPE sont introduites dans la chaîne de TALN pour produire des marqueurs, qui permettent de connaître la position des concepts, propriétés et instances dans le texte. Enfin, il reste la construction des triplets RDF associés à chaque OT. Des problèmes d\u0027ambiguïté peuvent surgir dans cette étape. Ils sont dus à l\u0027identification de plusieurs concepts, propriétés ou instances dans un même OT. Nous proposons d\u0027ajouter la prise en compte des contraintes range et domain pour déterminer quelle propriété correspond à quel concept. Néanmoins, nous avons remarqué que l\u0027ambiguïté est généralement inexistante si la fréquence des liens logiques est grande dans le texte. En effet, plus la fréquence des liens logiques est grande et plus la taille 3 des arguments est relativement petite. Par conséquent, le même argument correspond moins souvent à plusieurs concepts, propriétés ou instances. Quant à l\u0027objet textuel de type « titre » sa taille est généralement petite. Identification des relations contextuelles sémantiques. Cette étape consiste à attribuer le rôle sémantique aux liens logiques déjà détectés. Des travaux (Sporleder et al., 2005)  (Marcu et al., 2002) identifient ces rôles d\u0027une manière automatique : par exemple, une addition pour les liens logiques (de plus, d\u0027ailleurs,…). Néanmoins, des problèmes d\u0027ambiguïté persistent dans certains liens logiques plus complexes. Nous nous sommes contentés, à ce stade de notre travail, des liens logiques qui ne représentent pas des ambiguïtés sémantiques tels que les liens logiques «because, compared to, except,…».\nDescription of the mill internal elements\nInlet Headliners\nThis new design is composed of 3 thicker bolted rings, compared to the original design of 2 rings. The liners have a thickness of 70mm. except for the area of most wear (R/2-R/2+R/3), where the thickness is 85mm.\n\u003ctitle Id\u003d\"OT1\"\u003e Description of the mill internal elements \u003ctitle Id\u003d\"OT12\"\u003e Inlet Headliners \u003cparagraph\u003e \u003csentence\u003e \u003ccontextualRelation Id\u003d\"OT121\" type\u003d\"comparedTo\"\u003e \u003cargument1 Id\u003d\"OT1211\"\u003e This new design is composed of 3 thicker bolted rings, \u003c/argument1\u003e \u003cargument2 Id\u003d\"OT1212\"\u003ethe original design of 2 rings \u003c/argument2\u003e \u003c/contextualRelation\u003e \u003c/sentence\u003e \u003csentence\u003e \u003ccontextualRelation Id\u003d\"OT122\" type\u003d\"except\"\u003e \u003cargument1 Id\u003d\"OT1221\"\u003e The liners have a thickness of70mm. \u003c/argument1\u003e \u003cargument2 Id\u003d\"OT1222\"\u003e for the area of most wear (R/2-R/2+R/3), \u003ccontextualRelation Id\u003d\"OT12221\" type\u003d\"where\"\u003e \u003cargument Id\u003d\"OT122211\"\u003e the thickness is 85mm. \u003c/argument\u003e \u003c/contextualRelation\u003e \u003c/argument2\u003e \u003c/contextualRelation\u003e \u003c/sentence\u003e \u003c/paragraph\u003e \u003c/title\u003e \u003c/title\u003e\nFIG. 2 -Exemple de partie de texte dans un document FIG. 3 -Arborescence des objets textuels\nDéroulement de l\u0027approche proposée\nNous soulignons que la langue que nous prenons en compte actuellement est l\u0027anglais. L\u0027approche que nous proposons est expérimentée sur un document texte (3768 mots et 1862 autres unités linguistiques tel que les : chiffres, virgules, parenthèses,…) issu des partenaires industriels dans le cadre du projet SEVENPRO.. Nous exposons dans ce qui suit le déroule-ment des étapes de l\u0027approche sur une partie de ce document FIG. 2. phase: first options: control \u003d appelt Rule:JRuleRingOfDiaphragmMill ( ({Token.lemma \u003d\u003d\"Ring\"}({SpaceToken})?{Token.lemma \u003d\u003d\"of\"}({SpaceToken})?{Token.lemma \u003d\u003d\"mill\"}({SpaceToken})? {Token.lemma\u003d\u003d\"diaphragm\"}({SpaceToken})?)| ({Token.lemma\u003d\u003d\"Ring\"}({SpaceToken})? {Token.lemma\u003d\u003d\"of\"} ({SpaceToken})?{Token.lemma\u003d\u003d\"diaphragm\"} ({SpaceToken})? )| ({Token.lemma \u003d\u003d\"Ring\"}({SpaceToken})? ) ):RingOfDiaphragmMill --\u003e: RingOfDiaphragmMill.Concept \u003d {kind \u003d\"RingOfDiaphragmMill\", rule\u003dJRuleRingOfDiaphragmMill } Rule: JRulehasPart ( ({Token.lemma\u003d\u003d\"has\"}({SpaceToken})? {Token.lemma \u003d\u003d\"part\"})| ({Token.lemma\u003d\u003d\"is\"}({SpaceToken})?\n{Token.lemma\u003d\u003d\"composed\"} ({SpaceToken})?{Token.lemma\u003d\u003d\"of\"} ({SpaceToken})?)| ):hasPart --\u003e:hasPart.property \u003d {kind \u003d\"hasPart\", rule\u003dJRulehasPart} \u003crdfs:Class rdf:ID\u003d\"RingOfDiaphragmMill\"\u003e \u003crdfs:subClassOf rdf:resource\u003d\"#Item\"/\u003e \u003crdfs:label xml:lang\u003d\"en\"\u003eRing of mill diaphragm \u003c/rdfs:label\u003e \u003crdfs:label xml:lang\u003d\"en\"\u003eRing of diaphragm \u003c/rdfs:label\u003e \u003crdfs:label xml:lang\u003d\"en\"\u003eRing\u003c/rdfs:label\u003e \u003crdfs:label xml:lang\u003d\"fr\"\u003eAnneau diaphragme \u003c/rdfs:label\u003e \u003crdfs:comment xml:lang\u003d\"en\"\u003edenotes a part of a diaphragm mill (i.e. ring). \u003c/rdfs:comment\u003e \u003c/rdfs:Class\u003e \u003crdf:Property rdf:ID\u003d\"hasPart\"\u003e \u003crdf:type rdf:resource\u003d\"http://www.w3.org/ 2002/07/owl#TransitiveProperty\"/\u003e \u003cowl:inverseOf rdf:resource \u003d\"http://www.sevenpro.org/ ontologies/2006/estanda#partOf\"/\u003e \u003crdfs:domain rdf:resource\u003d\"#Item\"/\u003e \u003crdfs:range rdf:resource\u003d\"#Item\"/\u003e \u003crdfs:label xml:lang\u003d\"en\"\u003ehas part\u003c/rdfs:label\u003e \u003crdfs:label xml:lang\u003d\"en\"\u003eis composed of \u003c/rdfs:label\u003e \u003crdfs:comment xml:lang\u003d\"en\"\u003e hasPart is transitive and also reflexive, and anti-symmetrical. \u003c/rdfs:comment\u003e \u003c/rdf:Property\u003e\nFIG. 4 -Représentation rdfs d\u0027un Concept et d\u0027une relation dans l\u0027ontologie et les règles JAPE permettant de les identifier dans le texte\nEn appliquant l\u0027étape « manipulation textuelle » et ses différentes identifications et constructions en utilisant les marqueurs extraits à partir des règles JAPE, nous obtenons un ensemble d\u0027OT, selon une structure arborescente sous format XML (FIG. 2).\nLes exemples de RC structurelles pouvant être extraites à ce niveau sont : OT1 imbrique OT12; OT122 succède OT121; OT12 plus Important que 4 OT121; … Les étapes de la manipulation textuelle sont implémentées sous forme de requêtes XQuery 5 . Les objets textuels étant identifiés, il faut leur associer des objets sémantiques gérés à l\u0027étape de manipulation sémantique. Selon l\u0027ontologie associée 6 , un ensemble de règles JAPE est construit automatiquement. La FIG. 4 montre une description RDFS du concept «RingOfDiaphragmMill» et la relation «hasPart» dans l\u0027ontologie ainsi que les règles JAPE associées permettant de les identifier dans le texte. Nous avons obtenu 65 règles gérées automatiquement et qui correspondent à 65 concepts dans l\u0027ontologie. Cependant, comme l\u0027ontologie est amenée à évoluer, il faudra relancer l\u0027opération de génération pour avoir des règles qui reflètent l\u0027état de l\u0027ontologie. L\u0027ontologie étant en cours de construction, le nom-bre de propriétés/instances construit n\u0027est pas conséquent. De ce fait, les règles associées aux propriétés et instances ne sont pas générées dans l\u0027expérimentation. L\u0027utilisation de ces règles dans la chaîne de TALN permet de marquer tous les concepts et propriétés. Nous utilisons « Token.lemma » pour faire référence à toute variante d\u0027un mot donné. Les concepts, propriétés et instances identifiés dans l\u0027exemple de la FIG. 2  \n-Evaluation des étapes d\u0027extraction\nLes résultats d\u0027évaluation partielle des étapes de l\u0027extraction (TAB. 1) sont très satisfaisants. Cependant la construction des triplets RDF (FIG. 5) n\u0027a pas été expérimentée encore vu que l\u0027ontologie est en cours de construction (pas assez de propriétés et instances).\nA la différence des simples relations prises en compte par le formalisme RDF reliant des concepts, les RC que nous proposons sont des relations entre OS. Par conséquent, cela se traduit en relations entre triplets RDF (c\u0027est-à-dire relations entre des annotations sémanti-ques), or RDF ne permet pas la représentation de telles relations. Afin de pallier à ce problème de représentation, nous attribuons des identifiants aux objets sémantiques. Ces derniers sont mis dans un document RDF à part. Nous remplaçons les OT dans le fichier XML, qui représente la structure du document, par une référence (avec les identifiants) vers les OS.\nExploitation des annotations contextuelles\nDans cette partie, nous allons donner quelques exemples d\u0027utilisation d\u0027annotations contextuelles ainsi que des orientations de nos travaux futurs sur leur exploitation. L\u0027exploitation des annotations contextuelles repose essentiellement sur la notion de portée de validité. La portée de validité d\u0027un objet sémantique donné est calculée suivant un contexte donné modélisé par les RC structurelles et sémantiques. Prenant l\u0027exemple de la FIG. 3, la portée de validité de l\u0027objet sémantique associé à l\u0027objet textuel «OT1» est l\u0027ensemble d\u0027objets sémantiques associés aux objets textuels «OT12, OT121, OT122, OT1211, OT1212, OT1221, OT1222, OT12221, OT122211». Les conséquences de cette portée sont que : nous pouvons déduire, par exemple, que le concept «newDesign» cité dans l\u0027objet «OT1211» correspond à la nouvelle conception du concept «mill» citée dans l\u0027objet «OT1». Par ailleurs, la notion de portée de validité nous permettra d\u0027étudier la portabilité entre contextes, ce qui conduit à une complexité supplémentaire dans l\u0027inférence.\nLe challenge qui reste à surmonter réside dans la difficulté du raisonnement sur les RC spatiales et temporelles. Nous proposons de nous inspirer des travaux sur les SIG (systèmes d\u0027informations géographiques) pour raisonner sur l\u0027aspect spatial et temporel. En effet, nous pouvons utiliser le raisonnement dit « de propagation de contraintes », et plus particulière-ment en utilisant les relations (ou intervalles) d\u0027Allen (Allen, 1984). Les relations d\u0027Allen (FIG. 6) peuvent être utilisées pour représenter les liens logiques exprimant des RC temporelles. Par exemple les liens logiques avant, et durant peuvent être représentés respectivement par les relations (A before B ou A meets B), et (A during B). Ainsi nous pouvons faciliter l\u0027inférence sur l\u0027aspect temporel. Nous pouvons exploiter les ontologies déjà construites et qui modélisent l\u0027aspect temporel (Santos et al., 2003). Cependant, nous proposons de simplifier ces propositions et de les adapter à la manipulation du texte. Les limites de RDF concernant la représentation des relations entre triplets, ouvrent des perspectives de recherche sur une extension de ce formalisme et ceci nous pousse à utiliser un formalisme de description plus puissant (OWL), notamment pour représenter le rôle sémantique des RC. La  FIG. 7 donne un exemple d\u0027une représentation OWL de la relation temporelle before.\n\u003crdf:Property rdf:ID\u003d\"Before\"\u003e \u003crdfs:subPropertyOf rdf:resource\u003d\"#TemporalProperty\"/\u003e \u003crdf:type rdf:resource\u003d\"\u0026owl;TransitiveProperty\"/\u003e \u003cowl:inverseOf rdf:resource\u003d\"\u0026TempSchema;After\"/\u003e \u003crdfs:domain rdf:resource\u003d\"#SemanticObject\"/\u003e \u003crdfs:range rdf:resource\u003d\"#SemanticObject\"/\u003e \u003c/rdf:Property\u003e\nFIG. 6 -Les relations d\u0027Allen FIG. 7 -La relation temporelle «Before»\nDe la même manière, nous proposons d\u0027exploiter les relations d\u0027Egenhofer (Egenhofer et al., 1991) pour représenter les RC spatiales (4.1.1). Les annotations contextuelles que nous proposons ouvrent des perspectives de raisonnement sur le texte qui auparavant n\u0027étaient pas envisageables.\nConclusions\nNous avons proposé, dans cet article, une approche qui modélise le contexte à partir de sources textuelles, en prenant en compte les différents types de relations structurelles/sémantiques (spatiale, temporelle, et diverse). Dans l\u0027approche d\u0027extraction des annotations contextuelles que nous proposons, les étapes automatisées sont : la détection des liens logiques, des titres et leurs portées, des imbrications entre « titre, paragraphe, phrase et argument », des concepts. Les étapes qui restent à automatiser sont : l\u0027identification des propriétés et des instances ainsi que la construction des triplets. Un prototype est implémenté pour évaluer partiellement les différentes étapes de l\u0027extraction. Les résultats de l\u0027évaluation sont très satisfaisants. Cependant, l\u0027ontologie à laquelle nous nous référons est incomplète et a besoin d\u0027être enrichie. Aussi l\u0027attribution des rôles pour les RC reste à généraliser sur des liens logiques plus complexes. Par conséquent, nous envisageons d\u0027introduire l\u0027inférence spatiale et temporelle pour mieux exploiter la richesse sémantique considérable des liens logiques. Afin de valoriser ce travail, un outil qui regroupe toutes les étapes d\u0027extraction est en cours d\u0027élaboration. Aussi une ontologie regroupant les relations contextuelles (temporelles, spatiales et autres) est en cours de construction, pour faciliter leur réutilisation. L\u0027approche d\u0027extraction et d\u0027exploitation des annotations contextuelles sémantiques offre des perspectives prometteuses dans des domaines d\u0027extraction de connaissances à partir du texte. Néanmoins, i) l\u0027utilisation des relations contextuelles pour déduire les dépendances d\u0027inférence (portée de validité) pourra peser lourd sur le temps d\u0027exécution de l\u0027inférence ; ii) aussi, nous avons constaté des redondances d\u0027annotations contextuelles dans un même contenu textuel. Pour ces deux problèmes, nous envisageons de proposer des solutions techniques pour optimiser l\u0027inférence et réduire la taille des annotations.\n"
  },
  {
    "id": "833",
    "text": "Introduction\nFace à de grandes quantités de documents web, notre objectif est d\u0027extraire et de valider semi-automatiquement des relations d\u0027un domaine. Dans l\u0027état de l\u0027art, l\u0027extraction des relations a été faite soit par une approche statistique, une approche linguistique ou une approche hybride. De plus, l\u0027intérêt a été toujours porté sur un voire deux types de relations. A contrario, notre objectif est d\u0027extraire des relations de différents types en combinant des analyses de textes et en considérant les caractéristiques des mots. Dans cet article, nous avons défini un algorithme contextuel de découverte de relations qui combine différentes analyses (lexicale, syntaxique et statistique) pour définir des processus complémentaires qui assurent l\u0027extraction de relations variées et pertinentes. Notre algorithme établit des opérations de croisements entre analyses afin de pouvoir valider certaines relations. Les relations valides, comme celles invalides, seront présentées à l\u0027expert du domaine mais séparément.\nLa découverte des relations\nLa notion de contexte. Pour l\u0027extraction des relations, nous souhaitons trouver les mots qui sont reliés au mot étudié. Donc, nous cherchons des contextes qui contiennent ces mots reliés. Pour cela, nous avons défini différents contextes et nous les avons catégorisés en quatre types: le contexte structurel, le contexte linguistique (centré autour du verbe, globalement syntaxique et lexical), le contexte documentaire (paragraphe) et le contexte fenêtre (avec un degré de proximité). Notre approche utilise toutes ces analyses afin d\u0027extraire de nouvelles relations (en plus de celles existantes dans la hiérarchie) et de les valider automatiquement. L\u0027algorithme contextuel de découverte des relations. Il applique différents types d\u0027analyses pour extraire et évaluer les relations. Il dépend de certains paramètres comme le degré de confiance (DC), NO est le pourcentage d\u0027occurrences de mots dans le corpus (NO) et FN est la fréquence normalisée des mots dans le corpus (FN). Ces paramètres sont utilisés lors du filtre statistique ainsi que la validation. Le DC doit être défini par l\u0027utilisateur vu qu\u0027il explique sa confiance en l\u0027application. Par contre, NO et FN peuvent être définis soit par l\u0027expert du domaine, soit par le système en les déduisant de la valeur de DC ou par défaut (valeur définie par le concepteur du système). Dans le cas où le système est utilisé pour calculer les valeurs de NO et FN, si la valeur de DC est supérieure à 50% leurs valeurs (par défaut) seront maintenues, sinon elles seront multipliées par deux. Notre algorithme catégorise quatre types de relations extraites : valides, invalides, déduites et étiquetées. Une relation valide est celle qui est récupérée après une opération de croisement entre analyses. Une relation invalide est celle qui n\u0027a pas été retrouvé dans deux analyses.\nNotre algorithme est composé de cinq étapes. Une première étape applique les différentes analyses pour extraire les relations. Une seconde étape applique un filtre interne pour élimi-ner les relations qui représentent les liaisons des mots à l\u0027intérieur des classes validées. L\u0027étape trois applique un filtre par croisement des relations résultantes des différentes analyses. Nous proposons deux types de croisements complets (qui nécessitent que la relation existe dans les deux analyses pour qu\u0027elle soit retenue) pour la première étape de validation : un croisement au sein de l\u0027analyse statistique. Ce croisement est fait entre les relations structurées et les relations paragraphes vu qu\u0027une structure telle que définie dans notre démarche (contexte structurel) n\u0027est pas systématiquement incluse dans un paragraphe. D\u0027où l\u0027intérêt de recueillir ces relations qui se trouvent dans les deux résultats de nos contextes de même nature ; un croisement hybride réservé pour les relations provenant de l\u0027analyse fenêtre par proximité et celles des analyses syntaxiques et lexicales. La quatrième étape prend en compte l\u0027ensemble des relations invalides et applique un filtre statistique. Ce dernier est fait en défi-nissant la valeur de deux paramètres à savoir le nombre d\u0027occurrences NO et la fréquence normalisée FN. L\u0027étape 5 et 6 s\u0027occupent respectivement d\u0027établir les validations par degré de confiance et les déductions de nouvelles relations à partir de l\u0027existant et d\u0027étiqueter ces relations qu\u0027elles soient valides, invalides ou déduites.\nExpérimentations. Après avoir appliqué notre algorithme sur un corpus de 565 documents HTML en langue française relatif au domaine du tourisme, nous avons pu extraire: relations centrées autour du verbe (2251) ; relations globalement syntaxiques (34439) ; relations lexicales (5793) ; relations paragraphe (72476) ; relations structurelles (16966) ; relations fenêtres (206010). Par la suite, nous avons établit deux types de croisements à savoir un croisement entre les relations structurelles et paragraphes, et un second entre les relations fenêtres et lexicales. Le premier croisement nous a permis de retenir 372 relations (Hôtelle-rie/ hébergement, Réservation/hébergement, Camping/dormir). Quant au second croisement, nous avons pu avoir 268 relations (Catholicisme/christianisme, Ethnographie/paléontologie), sachant que dans les deux croisements nous avons supprimé certaines relations contenant des noms propres afin de minimiser le bruit. Après l\u0027étape de filtre statistique, nous n\u0027avons pas pu retenir des relations valides sur celles lexicales, globalement syntaxique et centrée autour du verbe vu que la relation la plus récurrente ne dépasse pas les 20 fois ; ce qui est largement loin de nos critères définis. Par contre, selon notre algorithme, pour les relations fenêtres (Activité/sport, Nautique/sport, Patrimoine/histoire, Plonger/sport) et structurelles (Casino/divertissement, Festival/musique, Vigne/vignoble), nous avons obtenu respectivement 24818 et 15257 relations validées. Pour les relations paragraphe, le résultat des validations a été négatif. Les relations qui n\u0027ont pas été validées tout au long de notre démarche seront les relations invalides. Celles-ci seront présentées à l\u0027expert en cas de besoin.\nSummary\nIn this research, we focus on extracting relations among concepts in order to build a domain ontology. For this, we define a contextual relation discovery algorithm that applies different textual analyses in order to extract, deduce, label and validate the domain relations. Our algorithm is based on a rich contextual modelling that takes into account the document structure and strengthens the term co-occurrence selection, a use of the existent relations in the concept hierarchy and a stepping between the various extracted relations to facilitate the evaluation made by the domain experts. Our main perspective is using these relations for the concept hierarchy evaluation and enhancement.\n"
  },
  {
    "id": "834",
    "text": "Introduction\nLe FIA ? est un nouvel automate qui permet de traiter de façon efficace la problématique de l\u0027extraction des itemsets fréquents dans les flots de données. FIASCO est l\u0027algorithme qui permet de construire et de mettre à jour le FIA ? en effectuant un seul passage sur les données. Notre objectif dans cet article est de présenter et d\u0027illustrer par l\u0027expérimentation l\u0027applicabilité et le passage à l\u0027échelle de FIASCO dans le cas des flots de données.\nFIASCO (Frequent Itemset Automaton Stepwise Construction Operator)\nLe FIA ? est un automate déterministe et acyclique, ce qui nous permet d\u0027établir une relation d\u0027ordre sur ses états (notée De par cette relation d\u0027ordre, nous introduisons un algorithme en deux passes pour la construction de cet automate, en utilisant des bits positions : FIASCO2. Cet algorithme utilise les propriétés d\u0027Apriori afin d\u0027optimiser sa construction, ce qui le rend efficace dans le cas d\u0027une base de données (cf. section 3). Nous proposons aussi un algorithme en une passe (FIASCO1), pour les flots de données, permettant de mettre à jour incrémentale-ment le FIA ? , item par item, avec une phase d\u0027élagage en utilisant un support statistique.\nExpérimentations\nLes expérimentations ont été réalisées sur les jeux de données 2 kosarak et T10I4D100K, sur une machine munie d\u0027un bi-processeur AMD ATHLON 3600+ 64 bits, avec 1Go de RAM. \nConclusion\nNous présentons dans cet article un nouvel algorithme, FIASCO, qui permet de construire et de mettre à jour incrémentalement le FIA ? appliqué aux flots de données. Cet algorithme est en une passe, avec une granularité par item. Les expérimentations, avec une analyse en temps et en espace, montrent l\u0027applicabilité et le passage à l\u0027échelle de l\u0027algorithme.\nSummary\nWe present in this paper a new algorithm for constructing and incrementally updating the FIA ? : FIASCO. Our algorithm only needs one scan over the data and takes into account the new batches, itemset per itemset and for each itemset, item per item.\n"
  },
  {
    "id": "835",
    "text": "Introduction\nEn transcription automatique de la parole, de grands corpus audio (incluant généralement des centaines d\u0027heures de parole) servent à estimer des modèles acoustiques précis de phonèmes contextuels. Ces modèles de sons élémentaires sont ensuite concaténés pour aboutir à des modèles de mots en s\u0027appuyant sur la connaissance de leur prononciation. Cette connaissance est incomplète à l\u0027heure actuelle et une partie importante de l\u0027information caractérisant les variantes de prononciations se trouve encodée implicitement dans les modèles acoustiques. L\u0027objectif de ce travail est de s\u0027appuyer sur les techniques de fouille de données afin d\u0027extraire des connaissances relatives aux spécificités acoustiques et prosodiques caractéri-sant les prononciations. Cette approche a déjà pu montrer son intérêt pour la caractérisation des accents étrangers (Vieru-Dimulescu et al., 2007). Nous nous intéresserons ici aux mots considérés comme homophones, i.e. phonémiquement pareils, et qui sont de ce fait sujets à de nombreuses erreurs de confusion lors de la transcription automatique. Partant de ces constats, nous nous sommes interrogés si les mots homophones ne déploieraient pas de particularités acoustiques/prosodiques qui n\u0027ont été prises en compte ni par les paramètres acoustiques classiques (vecteurs de cepstres), ni par les modèles acoustiques (Modèles de Markov Cachés à trois états) et qui permettrait leur discrimination. Nous faisons ainsi l\u0027hypothèse que des informations prosodiques (concernant durée, fréquence fondamentale notée f0, cooccurrence avec des pauses, etc.) puissent contribuer à lever certains types d\u0027homophonie, en particulier s\u0027il s\u0027agit d\u0027homophones issus de classes syntaxiques différentes (hétéro-syntaxiques). Nous avons fait appel aux techniques de fouille de données afin de classer automatiquement ces mots grâce à un ensemble d\u0027attributs acoustiques/prosodiques spécifiques développés pour ce travail.\nComme déjà évoqué ci-dessus, de nombreuses erreurs de transcription concernent des mots, considérés comme homophones, par exemple une confusion entre un nom au singulier et un nom au pluriel (table, tables), un verbe au participe passé ou à l\u0027infinitif (allé, aller) ou des mots-outils homophones (à, a). Ces derniers, par leur fréquence d\u0027occurrences dans la langue participent de manière significative aux erreurs de transcription automatique.\nDans la section 2, nous allons présenter les corpus utilisés et les analyses menées concernant : la fréquence des mots, la durée, la f0, en nous limitant à deux paires de mots homophones choisies pour cette étude préliminaire, i.e. à (préposition) vs. a (verbe) et et (conjonction) vs. est (verbe). Ces mesures visent à étudier les réalisations acoustiques de ces mots homophones hétéro-syntaxiques dans le but d\u0027identifier d\u0027ores et déjà des attributs potentiellement pertinents lors de la classification automatique. Nous allons ensuite essayer de mettre en évidence des traits spécifiques différenciant ces mots fréquents en utilisant la classification automatique et la fouille de données (section 3), avant de conclure et d\u0027ouvrir quelques perspectives (section 4).\nAnalyses acoustiques de mots (quasi-)homophones\nLors de la campagne ESTER (Evaluation des Systèmes de Transcription enrichie d\u0027Emissions Radiophoniques) (Galliano et al., 2005), financée par le programme interministériel français TECHNOLANGUE et organisée par l\u0027AFCP (Association Francophone de la Communication Parlée), la DGA (Délégation Générale pour l\u0027Armement) et ELDA (European Language Resources Distribution Agency), le système de transcription automatique du LIMSI a obtenu environ 11% de taux d\u0027erreur de mots (Galliano et al., 2005). Cependant de nombreuses erreurs portent sur les mots outils, qui sont les mots les plus fréquents et qui sont de plus souvent monosyllabiques comme et, est, a, à, un, que, qui, il, y, etc. (Adda-Decker, 2006). Dans l\u0027étude présentée ici, deux paires de mots et (conjonction)/est (verbe être) et à (préposition)/a (verbe avoir), qui sont parmi les mots les plus fréquents du français et qui sont souvent confondues lors de la transcription automatique, sont choisies pour définir et examiner des descripteurs ou attributs acoustiques permettant potentiellement de distinguer ces paires. Il faut souligner que la paire et/est n\u0027est pas vraiment homophone au sens phonologique, car le /E/ y correspond à deux degrés d\u0027aperture différents: la réalisation /e/ (e fermé) caractérise le mot et, tandis que la prononciation canonique du verbe est est /E/ (e ouvert). Cependant, dans la parole spontanée, la réalisation acoustique en tant que [e] (fermé) du verbe est fréquente, ce qui entraîne l\u0027homophonie des deux mots dans ce cas.\nCorpus utilisés\nCette étude part de deux types de corpus en français : l\u0027un est composé de journaux radiodiffusés d\u0027environ 55 heures et appelé BN (Broadcast News), provenant de différentes stations de radios (France Inter, Radio France International (RFI), France Info et Radio Television du Maroc (RTM)). Le style de ces corpus est (semi-) préparé. L\u0027autre corpus, appelé PFC (Phonologie du Français Contemporain) (Durand et al., 2003) contient des enregistrements de variétés de français de régions différentes et en différents styles de parole : parole spontanée (entretiens) et lecture (liste de mots et texte). La partie du corpus PFC utilisé contient environ 32 heures, dont le style « entretien » couvre 20h. Nous avons retenu ce sous-ensemble d\u0027entretiens correspondant à de l\u0027oral spontané.\nAlignement automatique\nL\u0027alignement automatique vise à localiser dans le signal acoustique les mots prononcés, de déterminer leur prononciation et de segmenter le flux audio en phones. Pour ce faire le système d\u0027alignement utilise des transcriptions manuelles, un dictionnaire de prononciation et des modèles acoustiques de phones indépendants du contexte. Le système de transcription automatique de parole du LIMSI (Gauvain et al., 2005)  \nExtraction automatique des paramètres acoustiques\nLe logiciel PRAAT (Boersma et Weenink, 1999) a été utilisé afin d\u0027extraire un nombre de paramètres acoustiques intervenant par la suite dans la définition d\u0027attributs pour la classification des mots homophones. Ces paramètres concernent le mot même (mot cible) et le contexte immédiat gauche/droite (i.e. la voyelle ou la pause précédant/suivant le mot cible). Nous avons ainsi extrait les trois premiers formants (F1, F2, F3) et la fréquence fondamentale (f0). Les formants (F1, F2 et F3) apportent des informations relatives à la qualité vocalique des segments: globalement le premier formant (F1) correspond à l\u0027articulation sur un axe ouvert/fermé du segment, le second (F2) à la réalisation sur un axe antérieur/postérieur et le troisième (F3) au caractère étiré/arrondi de la voyelle. La fréquence fondamentale (f0), c\u0027est-à-dire la fréquence de vibration des cordes vocales, détermine la hauteur du son. Elle permet essentiellement de distinguer les grandes classes de sons (voisés/non-voisés), mais donne aussi des informations quant au genre du locuteur, à la structuration temporelle du discours et à l\u0027accentuation. Quant à la durée, il a été montré qu\u0027elle peut être associée au style de parole ou bien à l\u0027appartenance des mots à une classe lexicale ou fonctionnelle (Adda-Decker, 2006 \nRNTI -X -\nPour chaque segment aligné, correspondant à l\u0027un des 4 mots cibles et leur contexte, les mesures sont effectuées toutes les 5ms. Un segment comprend ainsi au moins 6 points de mesures, étant donné que la durée minimale d\u0027un segment est de 30ms. Pour chaque segment un taux de voisement peut être calculé, correspondant au rapport donné par le nombre de points de mesure avec f0\u003e0 sur le nombre total de points de mesure. La détermination automatique des valeurs de formant est sujette à erreurs. Afin d\u0027éliminer des erreurs de mesure de formants, un filtrage a été mené: un segment est retenu à condition que son taux de voisement n\u0027y soit pas nul.\nPour le mot est, il y a la prononciation canonique /?/ (e ouvert) et sa variante /e/ (e fermé) dans le dictionnaire de prononciation utilisé pour la transcription automatique. Ces deux phonèmes sont perceptivement ainsi qu\u0027acoustiquement différents (cf. Fig. 1). Ci-dessous, les courbes détaillent les distributions des mots analysés selon leurs durées respectives. La distribution de la paire de mots et/est pour des durées allant de 30ms jusqu\u0027à 200ms est montrée dans la Figure 2 (corpus BN à gauche et corpus PFC-entretien à droite). Pour le mot est, trois courbes sont présentées : la première courbe (losanges) illustre la prononciation majoritaire /e/ (variante de prononciation), la deuxième (carrés) -le phonème /?/ (prononciation canonique), tandis que la troisième courbe (triangles) réunit les distributions des deux prononciations possibles. Chacune des courbes représentées somme à 100%.\nAnalyse des paramètres\nUne première tendance qui se dégage est que la durée peut être mise en relation avec le style de parole : les mots sont plus courts dans le corpus PFC (maximum à 30ms) que dans le corpus BN (maximum autour de 60-70ms). Pour ce qui est des différences entre les homophones et et est, on observe que la conjonction et (courbes rouges à croix) a tendance à avoir une durée plus importante que le verbe. L\u0027intersection des deux mots et/est est à 80ms pour les deux styles de corpus. Après 80ms, le taux de et est plus important que celui du mot est. L\u0027évolution des trois courbes du mot est est similaire dans les deux corpus.\nLa Figure 3 montre des décomptes similaires pour les mots à (préposition) et a (verbe avoir) pour le corpus BN (à gauche) et le corpus PFC-entretien (à droite). Par rapport à la paire de mots et/est (cf. Fig. 2), la paire à/a observe la même tendance de durée plus courte pour le style de parole spontané (PFC-entretien). La préposition à est légèrement plus longue que le verbe a. Par exemple on peut remarquer dans la Figure 3 à droite (PFC-entretien), qu\u0027un écart d\u0027environ 8% existe pour les deux items à/a pour la durée minimale de 30ms.\nPour conclure sur cette partie, nous avons analysé les durées de deux paires de mots (quasi-)homophones. Une différence qui semble s\u0027imposer pour les deux paires de mots et les deux styles de corpus est que la durée de mots-outil de types conjonction ou préposition (surtout et, dans une moindre mesure à) est plus longue que celle des verbes auxiliaires est et a. Cette information contribuera éventuellement à différencier nos paires de mots.  \nFIG. 3 -Distributions de durée de « à (préposition) » et « a (verbe) » du corpus BN (gauche) et PFC (droite).\nFréquence fondamentale (f0)\nLes paramètres prosodiques généralement considérés pour caractériser un mot sont la f0, la durée et l\u0027intensité. D\u0027après Selkirk (1996), les mots peuvent être distingués selon la caté-gorie grammaticale à laquelle ils appartiennent, e.g. fonctionnelle (le déterminant, la préposi-tion, l\u0027auxiliaire, le complément, la conjonction et la particule, etc.) ou lexicale (le nom, le verbe et l\u0027adjectif). Ce statut (fonctionnel vs. lexical) peut être mis en relation avec le paramètre appelé fréquence fondamentale. En effet, on pourrait émettre l\u0027hypothèse qu\u0027un verbe se trouvant à l\u0027intérieur d\u0027un mot prosodique réalise un f0 moyen différent de celui d\u0027une pré-position ou d\u0027une conjonction se trouvant en début de mot prosodique. De la même manière, on pourrait penser qu\u0027en début de mot prosodique on peut observer des zones de voisement partielles, précédées éventuellement de césures ou de pauses. Nous avons donc analysé la f0 en tant qu\u0027information prosodique pour nos paires de mots et/est et à/a. La question s\u0027est posée si le taux de voisement (i.e. la proportion de valeurs non nulles de f0) joue un rôle important dans l\u0027articulation des mots homophones analysés. Ainsi, selon le degré de voisement des segments vocaliques correspondant aux mots cibles, les données ont été divisées en trois classes :\n1.  \nFIG. 5 -Distribution de pourcentage d\u0027occurrences et valeurs moyennes de f0 de « à (prépo-sition) » et « a (verbe) » selon le taux de voisement du corpus BN (gauche) et PFC (droite).\nLa comparaison des mots à/a est montrée dans la Figure 5 pour le corpus BN (à gauche) et pour le corpus PFC-entretien (à droite). De gauche à droite, le mot à est illustré en prune, le verbe a en bleu. Comme pour la conjonction et, la préposition à est plus susceptible d\u0027apparaître dans les groupes à voisement partiel. De manière réciproque, le verbe a est mieux représenté dans la classe Voisé (les paquets d\u0027au moins 80% de voisement), même si cette tendance est moins forte que pour la paire et/est. Cela nous informe donc que dans les deux types de corpus et pour les deux paires de mots, le verbe est plus fréquemment complètement voisé que la préposition ou la conjonction. Le taux de mots dans la classe Voisé est plus faible dans le corpus PFC (entretien) que dans le corpus BN, ce qui peut être à nouveau mis en lien avec le style de parole spontanée.\nÀ partir de l\u0027analyse de la f0 et du taux de voisement, on peut remarquer que les verbes auxiliaires sont en général plus voisés que la préposition et la conjonction faisant partie de la catégorie fonctionnelle. Ces résultats indiquent qu\u0027en français, les mots dans la catégorie lexicale pourraient être plus accentués que ceux dans la catégorie fonctionnelle, confirmant ainsi ce qui a été mis en évidence en anglais (Selkirk 1996). Ces mesures permettent d\u0027envisager des attributs permettant de distinguer nos paires de mots homophones. Dans la section suivante nous nous intéressons à ces paires de mots en contexte, afin de trouver des descripteurs distinctifs supplémentaires.\nCooccurrence de pauses (gauche/droite)\nSelon Beaugendre et Lacheret-Dujour (1999), les pauses jouent un rôle très important dans le processus d\u0027extraction automatique d\u0027informations prosodiques, et ceci est plus particulièrement vrai pour ce qui est de la parole spontanée. Quant à la perception humaine, les pauses permettent entre autre, de repérer différents contours dans la substance verbale. Nous nous sommes intéressées au rapport qui existe entre les pauses au sens large (qu\u0027il s\u0027agisse d\u0027un silence, d\u0027une pause remplie, i.e. une hésitation, ou d\u0027une respiration) et les paires de mots homophones analysés ici. On examine leurs cooccurrences à gauche et à droite du mot cible.\nMots On peut remarquer que les taux de la catégorie « pause gauche » pour la conjonction et sont plus importants que ceux pour le verbe est, et ceci dans les deux styles de corpus. Cela suggère que de manière générale, les locuteurs introduisent une césure plus souvent devant la conjonction que devant le verbe et ceci d\u0027autant plus lorsqu\u0027il s\u0027agit de parole conversationnelle (PFC). Pour ce qui est de la paire de mots à vs. a, des différences comparables sont observées: les pauses sont plus nombreuses devant le mot outil que devant le mot lexical mais dans un degré moindre.\nPour finir, notons donc que la principale différence entre mots fonctionnels (à, et) et mots lexicaux (a, est) concerne l\u0027occurrence de pauses surtout à gauche (et plus légèrement à droite) du mot cible. Ainsi, de manière générale, les verbes est et a sont rarement précédés d\u0027une pause, au contraire des mots fonctionnels.\nDiscussion\nÀ l\u0027issue de cette section, on peut observer que les caractéristiques acoustiques et prosodiques des deux paires de mots homophones établies à partir de dizaines d\u0027heures de parole et quelques milliers d\u0027occurrences pour chaque mot examiné, présentent quelques différen-ces. Des paramètres tels que la durée et le taux de voisement permettent de les distinguer au moins partiellement. La cooccurrence de pauses à gauche et à droite des mots cibles change aussi selon la nature fonctionnelle ou lexicale des homophones considérés. Ainsi, on peut s\u0027interroger si ce type d\u0027attributs acoustiques ne pourrait pas être utilement exploité dans la discrimination de telles paires de mots.\nPartant de là, nous nous sommes posé comme objectif de définir des attributs qui pourraient caractériser les mots à travers des techniques de fouille de données. Dans la section suivante, nous décrivons d\u0027abord les descripteurs acoustiques et prosodiques mis en oeuvre, avant d\u0027aborder la méthode adoptée pour discriminer les paires d\u0027homophones analysées ici.\nClassification des mots homophones par fouille de données\nUn ensemble de tests de classification automatique a été mené, visant à déterminer à la fois l\u0027algorithme de classification et les attributs acoustiques les mieux adaptés à distinguer les deux paires de mots homophones et/est et à/a. Lors de cette étude préliminaire, nous avons fait appel au logiciel Weka (développé à l\u0027université de Waikato, en NouvelleZélande) pour classifier automatiquement ces mots grâce à quelques descripteurs acoustiques et prosodiques pressentis. Le logiciel Weka est destiné à résoudre une variété de problèmes rencontrés dans le cadre de la fouille de données, et le système est équipé d\u0027une large gamme d\u0027algorithmes d\u0027apprentissage et de classification.\nDéfinition d\u0027attributs\nPour la classification automatique, 41 attributs acoustico-prosodiques ont été définis. Ils ont été choisis pour modéliser à la fois le mot cible (attributs intra-phonème) et sa relation au contexte (attributs inter-phonème). Ces attributs sont :\nAttributs intra-phonème (33) : durée, f0 (moyenne/segment, début, milieu, fin), les formants (F1, F2 et F3) (valeurs moyennes par segment ainsi qu\u0027en début, milieu et fin de segment), taux de voisement (moyennes par segment ainsi qu\u0027en début, milieu et fin de segment). Nous avons également calculé les différences (notées ?) début-milieu, milieu-fin et début-fin pour les formants et pour la f0.\nAttributs inter-phonème (8) : durée, f0, pause. Le paramètre durée est mesuré ici comme suit : la différence entre la durée au centre du segment correspondant au mot cible et le centre de la voyelle précédente/suivante, même s\u0027il y a des consonnes ou des pauses entre ces phonèmes. Pour la f0 au niveau inter-phonémique, ?f0 a été calculée comme différence entre la valeur moyenne de f0 du phonème du mot cible et celle de la voyelle précédente et suivante, et entre ces deux voyelles précédant et suivant le mot cible. Les paramètres « pause gauche » et « pause droite » ont été également rajoutés.\nExpériences de classification\nPour classifier automatiquement les mots à partir de ces attributs, nous avons testé 17 algorithmes implémentés dans le logiciel Weka (classification bayésienne, arbres, règles et fonction etc.), avec le but de trouver l\u0027algorithme le plus performant. Les expériences de classification sont effectuées à l\u0027aide de la méthode de validation croisée, comprenant la division du corpus dans une partie « entraînement » et une partie « test ». Les résultats obtenus sur la partie « test » sont évalués en termes de classification correcte et de coefficient kappa 1 . En fonction des scores de classification et du coefficient kappa, les meilleurs algorithmes sont sélectionnés pour chaque paire de mots. Le tableau 2 ci-dessous montre l\u0027algorithme ayant permis la meilleure discrimination de chaque paire et la moyenne des 5 meilleurs algorithmes/paire de mots. Les résultats montrent que la paire et/est est nettement mieux classifiée que la paire à/a. Cela va dans le sens des résultats de la section précédente où l\u0027on observait que la paire et/est se distinguait mieux que la paire à/a. Cela s\u0027explique également en partie par le fait qu\u0027un tiers environ des occurrences du verbe est ne sont pas de vrais homophones (prononciation /?/ pour est) de la conjonction et ce qui engendre des attributs plus discriminants. Les résultats pour et/est sont particulièrement prometteurs pour le corpus PFC. La parole spontanée présente en général plus d\u0027erreurs lors de la transcription automatique, ces résultats montrent cependant que les homophones analysés sont en grande partie distinguables dans ce type de parole. \nSélection d\u0027attributs\n41 attributs ont été utilisés pour classifier les paires de mots. Nous pourrions faire l\u0027hypothèse que parmi ces attributs, certains sont plus pertinents que d\u0027autres. Les 10 meilleurs attributs ont été ainsi retenus (cf. tableau 4) à partir des résultats données par l\u0027algorithme le plus performant, i.e. l\u0027algorithme LMT (Logistic Model Trees). À partir de ces 10 attributs sélectionnés, nous avons recalculé les pourcentages de classification correcte avec l\u0027algorithme LMT en utilisant la méthode de la validation croisée. Les attributs sélectionnés et leurs résultats sont présentés dans le tableau 3. Les résultats à partir de 10 attributs sont légèrement moins performants que l\u0027utilisation de tous les attributs, surtout pour la paire et/est dans le corpus PFC. Le résultat du corpus BN montre que la différence entre 10 attributs et 41 attributs reste marginale (1%). Ceci montre que seuls 10 attributs suffisent pour 1 Le coefficient kappa mesure ici la concordance entre la classification automatique par Weka et les deux classes réelles de paramètres caractérisant les mots et/est, à/a. Il varie entre -1 (désaccord total) et 1 (accord total), en passant par 0 (classification au hasard). En fonction des résultats de la classification, les seuils correspondant aux meilleurs algorithmes sont : k\u003e0.50 pour et/est et k\u003e0.25 pour à/a. Cela montre d\u0027emblée une différence entre les deux paires : le kappa de et/est est très bon tandis que les résultats moins bons pour à/a montrent que cette paire est beaucoup plus difficile à discriminer.\nRNTI -X -produire des résultats pratiquement équivalents à ceux obtenus avec 41 attributs. Il y a donc des traits acoustiques et prosodiques particulièrement significatifs qui permettent de distinguer les mots homophones et ces traits sont encodés en grande partie dans les 10 attributs. \n-10 attributs (intra-segmentaux en italiquer et inter-segmentaux en gras) mieux classés par l\u0027algorithme LMT.\nEn regardant de plus près les attributs les plus discriminants, un certain nombre de tendances se dégage. En ce qui concerne la paire et/est, on remarque que les paramètres concernant les durées (durée du phonème, ?durée inter-phonèmes) et l\u0027existence d\u0027une pause devant le mot cible s\u0027avèrent particulièrement discriminants. Les paramètres liés au voisement (taux de voisement du phonème, ?f0 inter-phonèmes) pour le corpus BN et aux formants (F2, F2 milieu, F3 début, ?F1, ?F2) pour le corpus PFC-entretien sont aussi discriminants. Les paramètres inter-phonémiques sont surtout pertinents pour le corpus BN.\nPour la paire à/a, on peut observer d\u0027abord que les paramètres inter-phonémiques sont importants : ceux liés aux durées (?durées inter-phonèmes) et au voisement (?f0 interphonémes). Ensuite, le paramètre « pause gauche » est également utile. Enfin, les paramètres concernant le deuxième formant (F2 début, F2 milieu, F2 fin, ?F2), ainsi que le voisement jouent un rôle non négligeable. Il est aussi à noter qu\u0027il y a beaucoup de paramètres communs aux deux corpus qui permettent de distinguer les deux mots.\nCette analyse, bien que préliminaire, a permis de mettre en évidence que les mots homophones peuvent être différenciés grâce à certains paramètres acoustiques et prosodiques. Ce fait est valable pour les deux paires et surtout pour et/est dans les deux corpus. Les résultats obtenus suggèrent des pistes intéressantes pour mieux traiter ces mots lors de la transcription automatique de la parole. Afin de valider cette approche et les paramètres retenus, l\u0027analyse devrait s\u0027étendre à d\u0027autres mots homophones et d\u0027autres types de corpus.\nConclusion et perspectives\nDans ce travail nous avons cherché à étudier les réalisations acoustiques des mots homophones dans des grands corpus oraux de différents styles de parole (parole préparée, parole conversationnelle) avec le but à plus long terme de pouvoir contribuer à la modélisation acoustique des variantes de prononciation dans les systèmes de transcription automatique de la parole. Notre objectif était ensuite d\u0027examiner les réalisations acoustiques de mots homophones fréquents, sources de nombreuses erreurs de transcription, de définir et d\u0027évaluer des attributs acoustiques, permettant éventuellement de classifier correctement ces homophones. À cet effet nous avons utilisé différents outils (STK-LIMSI pour l\u0027alignement automatique, Praat pour l\u0027extraction d\u0027attributs acoustiques, Weka pour la classification et la fouille de données).\nNous nous sommes servies de l\u0027expérience acquise sur ces grands corpus de parole variés, pour définir des attributs acoustiques et prosodiques potentiellement utiles pour la discrimination des mots, en particulier des mots (quasi-)homophones. Ainsi nous avons pu étu-dier plus particulièrement les paires de mots outils homophones, et/est et à/a, qui sont les couples de mots qui produisent le plus d\u0027erreurs lors de la transcription automatique. La comparaison de durées entre et (conjonction) /est (verbe être) montre que le mot est a tendance à être réalisé avec une durée beaucoup plus faible, alors que la conjonction et se trouve souvent allongée. Cette simple mesure suggère que les homophones, réalisés a priori avec les mêmes phonèmes (par exemple, les mêmes valeurs de formants pour les voyelles), peuvent différer dans leur réalisation prosodique. Par la « réalisation prosodique » nous entendons tout ce qui concerne durée, fréquence fondamentale, voisement des segments, ainsi que leur articulation avec les contextes droite et gauche, incluant des mesures de pauses. On aboutit ainsi à un ensemble de mesures intra-et inter-phonèmes, servant à définir les 41 attributs acoustiques utilisés pour la classification. Pour les expériences de classification, nous avons testé les différents algorithmes proposés dans Weka : classification par arbres de décision, classification par règles, classification par MVS (machines à vecteurs support), classification bayésienne. Le résultat de la classification automatique par les 17 algorithmes testés dans le logiciel de fouille de données montre que les attributs impliquant les durées intra-et intersegmentales de la paire et/est sont parmi les plus importants pour la classification. La classification automatique s\u0027est avérée particulièrement prometteuse pour la paire et/est : les résultats sont supérieurs à 70% (BN) voire 90% pour le corpus PFC. Les mots à/a sont discriminables aussi, même si de manière moins aisée que le paire et/est. Ces résultats montrent qu\u0027il existe des informations acoustiques pertinentes pour la discrimination des homophones. Leur utilisation explicite dans les systèmes de transcription devrait contribuer dans le futur à réduire le taux de confusions observées.\nNous rappelons ici les attributs qui se sont avérés les plus utiles pour la classification: les durées inter-segmentales (en lien avec un phonème vocalique précédent ou/et suivant) sont plus efficaces que la durée du segment propre. La même remarque vaut également pour la f0.\nDans des travaux futurs, nous allons essayer de mieux caractériser les homophones en rajoutant de nouveaux attributs pour la discrimination, et ensuite tenter d\u0027implémenter efficacement les attributs identifiés (en particulier mesures de durée et de f0 inter-segmentales) pour améliorer la transcription automatique. Nous comptons également, dans le futur, éten-dre ce type d\u0027études à plus de mots, et intégrer alors des informations morpho-syntaxiques, afin de mieux factoriser les variantes observées dans la parole.\n"
  },
  {
    "id": "837",
    "text": "La génération des séquences résumées proposée\nLa génération de résumé vidéo est une technique alternative prometteuse utilisée dans l\u0027indexation et la recherche vidéo (Truong et Venkatesh, 2007). L\u0027objectif de cet article est de proposer une approche de génération des séquences résumées utilisant le soft computing vue son efficacité dans les systèmes tolérant l\u0027imprécision et l\u0027incertitude (Zadeh, 1956). Etant donné une longueur cible du condensé vidéo, on veut calculer les segments vidéo qui couvrent le maximum du visuel en respectant la longueur du condensé. Les segments vidéo sont représentés par des images clés et décrites par un histogramme de couleur. L\u0027histogramme est un outil très ordinaire pour résumer visuellement la distribution d\u0027un échantillon de données. Pour maximiser le visuel on doit maximiser la présence de l\u0027information de la couleur et sa distribution. Ce pendant, le problème revient à maximiser l\u0027occurrence et la distribution du visuel représenté par l\u0027information de couleur du contenu vidéo. En effet, l\u0027occurrence et la distribution de ce contenu visuel représente des variables linguistiques, c.-à-d. \"l\u0027occurrence est maximale\" et \"la distribution est maximale\". L\u0027occurrence et la distribution représentent deux ensembles flous qui nécessitent une détermination de leur fonction d\u0027appartenance.\nDans notre proposition, nous segmentons les séquences vidéo en plans (shots) comme unité de base en utilisant une transformation de couleur RGB réversible comme une représentation du contenu vidéo (Hadi et al., 2006b). Après, nous sélectionnons les images représentatives (keyframes) à partir des plans vidéo on se basant sur l\u0027estimation du mouvement local (Essannouni et al., 2006). L\u0027algorithme d\u0027extraction des images représentatives utilisé est basé sur un Séquence résumé par le Soft Computing algorithme de classification k-medoid pour rechercher les images les plus représentatives pour chaque plan Hadi et al. (2006a). Comme résultat, les images extraites représentent le centre des sous segments dans un plan.\nDans l\u0027optique de créer des condensés vidéo à partir des segments résultats, nous représen-tons les images clés calculées par leur histogramme de couleur utilisé dans (Hadi et al., 2006b). La création est réalisée en maximisant l\u0027occurrence et la distribution de la coleur considéraient comme des ensembles flous. Afin déterminer leurs fonctions d\u0027appartenance, on cherche les instances d\u0027histogrammes liées aux occurrences maximales et aux distributions maximales en utilisant les algorithmes génétiques (AG). Avec ces deux instances fictives d\u0027histogrammes, nous calculons leurs distances avec les histogrammes des images clés calculées dans la première étape d\u0027analyse vidéo. On obtient ainsi deux vecteurs de distance pour l\u0027ensemble des segments à sélectionner pour la création du condensé vidéo. Afin de chercher la valeur d\u0027appartenance du visuel maximal, nous utilisons les théories des ensembles flous. Le condensé final sera généré à partir de la courbe des valeurs d\u0027appartenance du visuel maximal en sélec-tionnant les segments ayant les plus grandes valeurs (visuel maximal) en respectant la longueur du condensé qui représente la sommes des longueurs des segments sélectionnés.\n"
  },
  {
    "id": "838",
    "text": "Introduction\nD\u0027un point de vue linguistique, la pragmatique s\u0027intéresse aux éléments du langage dont la signification ne peut être comprise qu\u0027en fonction d\u0027un contexte d\u0027interprétation donné. Dans le cadre des ontologie de domaine (qui sont des spécifications formelles de conceptualisations partagées Gruber (1993)), il s\u0027agit d\u0027enrichir la sémantique formelle intrinsèque à une ontologie de domaine (OD) à l\u0027aide d\u0027éléments caractéristiques d\u0027un contexte de création ou d\u0027usage comme la culture, le mode d\u0027apprentissage, ou encore l\u0027état émotionnel. Pour ce faire, nous prenons comme point de départ les ontologies telles quelles sont définies aujourd\u0027hui au moyen du langage OWL. Nous y ajoutons deux gradients, dont le but est de modéliser une différence intuitive de degré de vérité dans le processus de catégorisation : (1) une pondération des liens « is-a » dans la hiérarchie des concepts, les gradients de prototypicalité conceptuelle, (2) une pondération des synonymes du label utilisé pour dénoter un concept donné, le gradient de prototypicalité lexicale. Ces gradients permettent de prendre en considération différents points de vue pour une même ontologie, et par la-même d\u0027enrichir la sémantique formelle initiale intrinsèque à l\u0027ontologie d\u0027une pragmatique inhérente au contexte d\u0027usage considéré (fonction de la culture, des modes d\u0027apprentissage ou encore du contexte émotionnel). Après avoir exposé notre approche 1 dans les sections 2.1 et 2.2, les gradients de prototypicalité conceptuelle et de prototypicalité lexicale, fondés sur les processus cognitifs de catégorisation, sont explicités dans les sections 2.3 et 2.4.\nGradients de prototypicalité\nNos travaux reposent sur l\u0027idée fondamentale que tous les concepts ne sont pas constitués de membres équidistants par rapport à la catégorie qui les subsume, mais qu\u0027ils comportent des membres qui sont de meilleurs représentants que d\u0027autres (Kleiber, 2004). Ce phénomène est également applicable pour l\u0027ensemble des termes 2 désignant un concept. C\u0027est sous cette hypothèse, validée par des travaux de psychologie cognitive tels que Cordier (1985), que nous proposons deux mesures : (1) le gradient de prototypicalité conceptuelle, qui correspond à une pondération des liens « is-a » dans la hiérarchie des concepts, et qui permet de mesurer les différentes représentativités des sous-concepts ; (2) le gradient de prototypicalité lexicale, qui, pour un concept donné, correspond à une pondération des synonymes du terme saillant.\nApproche objective, fondée sur les propriétés\nLa composante objective de notre gradient vise à prendre en compte l\u0027aspect intensionnel d\u0027une conceptualisation aux travers des propriétés de concepts. Le rôle des attributs d\u0027une caté-gorie en vue d\u0027un rattachement à une autre catégorie a été développé dans Smith et al. (1974). Nous appelons cette pondération objective, car elle est issue du courant objectiviste selon lequel la catégorisation s\u0027opère sur la base de propriétés communes (Kleiber, 2004). Il s\u0027agit d\u0027un courant classique qui se fonde sur une vue catégorielle de l\u0027univers tel qu\u0027a pu le définir Aristote. Cette objectivité provient uniquement de la conceptualisation élaborée par l\u0027endogroupe, conceptualisation capturée dans l\u0027ontologie via la hiérarchie des concepts et leurs propriétés.\nL\u0027idée directrice est que plus un concept va ajouter d\u0027attributs à ceux hérités de son père, plus il sera sur la voie de la spécialisation et moins il sera prototypique, i.e. représentatif de sa catégorie. Nous considérons cette valeur comme étant le rapport entre le nombre d\u0027attributs du fils et le nombre d\u0027attributs du père. De manière formelle, la fonction objectif : C ×C ? [0, 1] est définie comme suit :\nOù (1) attributs(c p ) est le nombre d\u0027attributs du concept parent, (2) attributs(c f ) le nombre d\u0027attributs du concept fils, et (3) n le nombre de concepts fils du concept c p .\nNous élevons le rapport entre le nombre d\u0027attributs des deux concepts à la puissance n de manière à pouvoir tenir compte de la structure de l\u0027héritage et ainsi favoriser davantage les élé-ments dont la valeur objectif est forte. Le concept le plus prototypique d\u0027une décomposition se trouve ainsi renforcé proportionnellement au nombre d\u0027éléments de cette décomposition.\nApproche subjective, fondée sur les fréquences\nLa composante subjective de notre gradient vise à prendre en compte l\u0027aspect extensionnel d\u0027une conceptualisation à travers les termes utilisés pour dénoter les concepts. Cette approche est fondée sur la fréquence d\u0027apparition d\u0027un concept appartenant à un domaine D dans l\u0027univers d\u0027un endogroupe G. De la sorte, plus un élément sera fréquent dans cet univers, plus il sera jugé comme représentatif / typique de sa catégorie. Cette notion de typicalité fait l\u0027oeuvre des travaux d\u0027Eleanor Rosch (Rosch, 1973). Dans notre contexte, l\u0027univers de l\u0027endogroupe va être constitué par l\u0027ensemble des documents contenus dans ? (D,G) .\nDe manière formelle, la fonction subjectif G,\nAvec : -f requence(terme) retourne la fréquence d\u0027apparition du terme dans les documents appartenant à ? (D,G) ; -count(document, terme) retourne le nombre de documents appartenant à ? (D,G) où le terme apparaît ; -count(document) retourne le nombre de documents appartenant à ? ( le rapport entre le nombre de documents où le terme est présent et le nombre total de document. Une idée présentée fréquemment dans peu de documents est moins influente en terme de connaissance qu\u0027une idée défendue et citée peu de fois dans chacun des documents mais de façon uniforme dans tout le corpus 4\nGradients de prototypicalité conceptuelle\nLes gradients de prototypicalité conceptuelle prennent en compte, dans le processus de catégorisation et de classement, deux aspects des concepts : intensionnel d\u0027un côté par leur structure, extensionnel de l\u0027autre par leur fréquence d\u0027évocation et d\u0027utilisation dans l\u0027univers de l\u0027endogroupe. \nGradient de prototypicalité conceptuelle locale (GPCL). Soit protconloc\n.5] une pondération de la composante objective, (2) ? ? [0, 0.5] une pondération de la composante subjective, et (3) ? ? 0 une pondération de l\u0027état mental de l\u0027endogroupe G.\nLes valeurs de ? et de ? peuvent varier suivant le domaine traité, la volonté des experts lors de la création de l\u0027ontologie, le contexte d\u0027usage de l\u0027ontologie considérée, etc. Ils permettent ainsi d\u0027accorder plus ou moins d\u0027importance à l\u0027aspect structurel de la conceptualisation par rapport à l\u0027évocation des concepts dans les documents. Quant au facteur ?, il a pour objectif de tenir compte de l\u0027état mental de l\u0027endogroupe. Selon Mikulinger et al. (1990), un état mental négatif favorise la diminution de la valeur de représentation, et inversement pour un état mental positif. Nous caractériserons (1) un état mental négatif par une valeur de ? ?]1, +?[, (2) un état mental positif par une valeur de ? ?]0, 1[, et (3) un état mental neutre par une valeur de 1.\nAinsi, une très faible valeur de ? va avoir pour effet d\u0027augmenter considérablement la valeur de ce gradient pour des concepts initialement placés comme peu représentatifs (un état positif facilite l\u0027ouverture d\u0027esprit, la valorisation etc.) A l\u0027inverse, une très forte valeur de ?, pour un état mental fortement négatif, va avoir pour effet de sélectionner uniquement les concepts à forte typicalité, éliminant de facto les autres concepts. Une ontologie pragmatisée vernaculaire de domaine est par conséquent une ontologie vernaculaire de domaine placée dans un contexte particulier, défini par les trois paramètres ? pour la composante objective, ? pour la composante subjective et ? pour l\u0027état mental.\nGradient de prototypicalité lexicale (GPL).\nCe gradient a pour but de valuer le fait que tous les synonymes d\u0027un terme considéré comme le terme saillant, i.e. le label préconisé pour un concept, n\u0027ont pas forcément la même repré-sentativité au sein de l\u0027endogroupe. La question est en effet « pourquoi nommons-nous plus le concept x avec le vocable y plutôt que z ? » Pour définir ces variations de représentativité lexicale, nous reprenons le gradient calculé précédemment, à la différence près que nous n\u0027allons pas utiliser la composante objective liée aux propriétés (du fait que nous sommes sur la même catégorie). Nous allons une fois encore nous inspirer de la formule calculant le contenu en information d\u0027un concept, en prenant pour évaluer ce gradient le rapport entre la fréquence d\u0027utilisation de ce terme et la somme de fréquences de tous les termes du concept dans ? (D,G) . \n.\nConclusion\nLa finalité de nos travaux, focalisés sur la notion d\u0027ontologie pragmatisée vernaculaire de domaine, est de prendre en compte la subjectivité de la connaissance via sa spécificité à un endogroupe et un domaine, son aspect écologique, et l\u0027importance de son contexte émotion-nel. Cet objectif nous conduit à étudier la dimension pragmatique d\u0027une ontologie. En nous inspirant des travaux d\u0027E. Rosch sur la prototypicalité, nous avons développé deux mesures identifiant deux gradients de prototypicalité, l\u0027un conceptuel et l\u0027autre lexical, de manière à pouvoir -entres autres -pondérer les liens « is-a » des hiérarchies catégorielles. Ces pondéra-tions permettent, dans un premier temps et au niveau local, d\u0027ordonner les sous-concepts d\u0027une catégorie donnée pour le premier gradient, la liste des synonymes d\u0027un terme saillant pour le second. Bien évidemment, ces gradients ne modifient en rien la sémantique formelle inhérente à l\u0027ontologie considérée ; les liens de subsomption restent valides. Ces gradients ne sont que le reflet de la prise en compte de la pragmatique, à savoir une sur-couche pragmatique sur la sémantique.\nA partir de ces informations, plusieurs applications peuvent être envisagées :\n-évaluation / validation d\u0027ontologies. Les gradients de prototypicalité peuvent représenter des indicateurs de qualité de catégorisation, et par là même de qualité de la modélisation inhérente à une ontologie de domaine formalisée (au moyen d\u0027OWL par exemple).Par exemple, il peut se poser le problème des concepts jugés très peu typiques. Que faut-il en faire ? Sont-ils à la bonne place dans la hiérarchie catégorielle ? -recherche d\u0027information. Les gradients de prototypicalité peuvent permettre de classer les résultats d\u0027une requête (sous-entendue d\u0027une requête dite étendue) suivant un ordre de pertinence, en plaçant au premier plan les éléments les plus représentatifs d\u0027une caté-gorie ou d\u0027un terme donné.\n"
  },
  {
    "id": "839",
    "text": "Introduction\nLes avancées dans le domaine du Web ont ouvert de nouvelles perspectives dans le domaine de la cartographie interactive et dynamique (Koben, 2001, Josselin et Fabrikant, 2003. Aujourd\u0027hui le web propose de multiples cartographies qui s\u0027adaptent aux besoins d\u0027un utilisateur, qui peut être, tour à tour, décideur, citoyen, voyageur… Les services d\u0027itinéraires routiers en sont un exemple typique. Dans ce contexte, le groupe de recherche pluridisciplinaire HyperCarte 1 s\u0027est donné pour objectif de concevoir et implémenter une collection cohérente de plates-formes interactives d\u0027analyse spatiale et de représentations cartographiques de phénomènes sociaux, économiques, environnementaux, etc. (Grasland et al., 2005-b). Les applications visées se situent principalement dans le domaine socio-économique ou 1 http://www-lsr.imag.fr/HyperCarte/ -19 -RNTI-E-13 environnemental et l\u0027aide à la décision en matière de prospective territoriale pour un public large : chercheurs en géographie, en sciences sociales et humaines, mais aussi décideurs politiques et grand public. L\u0027enjeu est de proposer une cartographie s\u0027adressant à des utilisateurs aux compétences diverses, néophytes ou spécialistes pour une cartographie exploratoire (Antoni et al, 2004). L\u0027évolution des technologies associées au Web offre d\u0027énormes possibilités à ce type d\u0027approche, en particulier grâce à la souplesse qu\u0027apporte une interactivité de haut niveau.\nL\u0027exposé de cet article concerne plus précisément la réalisation d\u0027un environnement, basé sur l\u0027infrastructure du Web, capable de générer dynamiquement des cartes continues, c\u0027est-à-dire affranchies d\u0027éventuels maillages d\u0027observation (administratifs, grilles de collecte, etc.). L\u0027objectif est de visualiser la distribution spatiale des phénomènes analysés à un niveau macroscopique, c\u0027est-à-dire largement supérieur aux maillages d\u0027observation. La méthode que nous proposons, appelée méthode de transformation par potentiel, conserve la masse totale des données et en donne une représentation non biaisée. Définie empiriquement dans les années 1990 par les travaux de l\u0027UMR Géographie-cités sur les conséquences de la réunification allemande (Grasland, 1991) et de la chute du mur de Berlin en Europe (Boursier-Mougenot et al. 1993), l\u0027étude de cette méthode en vue de la création d\u0027un outil d\u0027analyse spatiale multiscalaire a abouti à une publication méthodologique de référence (Grasland et al., 2000). Le lissage est un cas particulier d\u0027application de cette méthode globale, lorsque la portée du potentiel est petite.\nUne des difficultés de diffusion de cette méthode d\u0027analyse spatiale et de représentation cartographique vient, d\u0027une part du coût élevé du calcul 2 qui empêche de répondre à l\u0027exigence d\u0027interactivité et, d\u0027autre part de la contrainte sur la sécurité et la confidentialité des données analysées.\nCet article présente la solution conçue pour répondre à ces besoins. Elle s\u0027est développée suivant trois axes : (i) distribution des calculs sur un serveur multiprocesseur détenant les données, (ii) parallélisation des tâches de calcul, (iii) visualisation des cartes sur un client web interactif avec connexion sécurisée.\nCette présentation s\u0027organise autour du plan suivant : dans un premier temps, les fondements théoriques et les applications possibles de la méthode du potentiel sont rappelés, et cette méthode est située en regard de ce qui existe. Suivent ensuite la justification de l\u0027architecture retenue et la description de la stratégie d\u0027optimisation des calculs. Enfin, les résultats de cette réalisation sont présentés ainsi que les perspectives d\u0027amélioration qu\u0027elle offre. 2 Il faut plusieurs heures pour calculer une carte de résolution moyenne visualisant la densité de population en France, à partir du recensement sur les 36000 communes.\n-20 -RNTI-E-13 2 La méthode de transformation par potentiel\nPrincipe\nLes représentations cartographiques continues de phénomènes spatiaux discrets sont nécessaires lorsque l\u0027on souhaite s\u0027abstraire d\u0027un maillage spatial, parce que le maillage est hétérogène ou qu\u0027il n\u0027est pas signifiant pour le phénomène étudié, afin de ne garder que l\u0027organisation spatiale du phénomène, sans référence au découpage sous-jacent du territoire.\nLa méthode considère un certain espace géographique, sur lequel est plaqué un maillage constitué d\u0027unités territoriales auxquelles sont associés des stocks, c\u0027est-à-dire des variables statistiques résultant de dénombrement, comme la population, le nombre d\u0027actifs, etc. Le maillage territorial peut être, par exemple, l\u0027ensemble des communes françaises et leur équivalent européen dans la nomenclature NUTS de niveau 5, adoptée par l\u0027European Spatial Planning Observation Network. Ce type de maillage s\u0027emboîte généralement dans un maillage de plus haut niveau, par exemple le maillage départemental (NUTS-3) : un département est composé d\u0027un sousensemble de communes. Les indicateurs considérés pour cette méthode possèdent une propriété additive, c\u0027est-à-dire que la valeur d\u0027un stock d\u0027une maille de niveau supérieur est égale à la somme des valeurs des mailles qui la composent au niveau inférieur. L\u0027objectif est de calculer en tout point de l\u0027espace discrétisé la valeur du potentiel de chaque stock. La discrétisation est une division de l\u0027espace en parcelles régulières, formant une grille, par exemple. En tout point de cet espace géométrique, la valeur du potentiel doit être comprise comme la valeur probable de l\u0027indicateur considéré, qui dépend de la contribution de chaque maille de l\u0027espace géographique, pondérée par sa distance au point calculé.\nSi on note A l\u0027ensemble des unités territoriales, a un élément de cet ensemble, Sa la valeur du stock sur cette unité, alors sachant que les effets des stocks s\u0027additionnent et sont liés à la distance ? entre a et le point M, le potentiel ?(M) est défini en tout point M de l\u0027espace géométrique par :\n[ 1]\nA a \" # Par exemple, considérons A, l\u0027ensemble des communes européennes, et S le nombre d\u0027habitants centenaires par commune, et supposons que l\u0027on veuille calculer le potentiel de centenaires en tout point de l\u0027espace. Pour chaque commune a, Sa est le nombre de personnes centenaires, et g a le centre de la commune.\nOn spécifie alors la distance ?(a,M) en mesurant la distance d entre M et g a, un point représentatif de a (qui peut être son centre de géométrie, son centre administratif ou industriel, etc.). La contribution au potentiel de chaque élément a est pondérée par une fonction f de la distance d, car l\u0027effet d\u0027un stock diminue usuellement avec la distance : il est maximal à une distance nulle et nul à une distance infinie. Pour que le potentiel ait du sens, en particulier lorsqu\u0027il dépendra d\u0027un paramètre, on normalise celui-ci par l\u0027équation [2], en tout point O de l\u0027espace:\nLa somme totale des stocks est égale à l\u0027intégrale du potentiel, on obtient ainsi une redistribution de la masse sur l\u0027espace considéré.\nDans une métaphore du modèle de gravité, ?(M) s\u0027interprète comme l\u0027attraction exercée par l\u0027environnement sur un mobile placé en M, dont le vecteur de déplacement serait alors -grad ?. Une interprétation duale serait aussi que ?(g a ) mesure l\u0027influence d\u0027une masse placée en g a sur l\u0027ensemble des points M de son voisinage. Par exemple, le nombre de centenaires vivant à Nuoro, une ville de la Sardaigne, contribuera beaucoup plus à l\u0027estimation d\u0027un point M situé dans son voisinage que les centenaires habitant Rome. Du point de vue méthodologique, la méthode s\u0027apparente aux méthodes de traitement du signal par déconvolution du signal échantillonné (Grasland, Vincent, 2006).\nDe l\u0027équation [1], il découle que la complexité du calcul dépend à la fois de la taille de l\u0027espace administratif (le nombre n d\u0027éléments a), et de la résolution de l\u0027image à produire (le nombre m de points M que l\u0027on estime).\nLe calcul dépend principalement de la fonction f, fonction d\u0027interaction spatiale. Elle intègre les hypothèses concernant les lois de diffusion dans l\u0027espace associées au phénomène étudié. Trois modèles de fonction paramétrée sont proposés : un modèle à support limité (disque et disque amorti), un modèle exponentiel (Gaussienne) pour des interactions proches -la décroissance de f se fait alors selon une exponentielle négative -; et enfin un modèle à interaction de longue portée (Pareto) -la décroissance suit une puissance inverse. Cette méthode permet par exemple de modéliser et d\u0027étudier la propagation d\u0027épidémies : leur diffusion pourra se faire soit sur de longues distances, soit sur de courtes distances, suivant le rayon d\u0027action de l\u0027élément contaminant (Grasland et al., 2005-a). L\u0027utilisateur peut tester différents modèles en choisissant la fonction d\u0027interaction à appliquer.\nL\u0027analyse du phénomène dépend aussi de la portée p de la fonction d\u0027interaction. La portée est définie comme la distance moyenne d\u0027action d\u0027une masse sur son voisinage. Elle est reliée à la forme de la fonction d\u0027interaction par l\u0027équation suivante en tout point O de l\u0027espace (après passage en coordonnées paramétriques, r représente le rayon) :\nLa portée peut être interprétée comme l\u0027échelle spatiale de représentation choisie. Le tandem (fonction, portée) traduit concrètement les hypothèses économiques et sociologiques associées aux interactions entre les acteurs sur le territoire. À portées identiques, le calcul du ratio de deux potentiels de stocks différents s\u0027interprète comme une densité. Par exemple, le potentiel ? P de population peut se rapporter au potentiel ? S de superficie, ce qui nous donne une densité de population sur une portée donnée et uniforme. (Voir figure 1). D\u0027autre part, il faut choisir le type de distance utilisée. Celle-ci dépend de la taille de l\u0027espace couvert par la carte. Par exemple, à l\u0027échelle d\u0027un continent, on ne peut pas utiliser la distance euclidienne sans introduire des déformations. La - \nDeux exemples d\u0027application empirique\nNous avons déjà dressé un récapitulatif concernant des usages relativement classiques de la méthode, (Plumejeaud et al., 2007). Nous présentons ici des usages plus complexes, mettant en oeuvre une modélisation de flux associés aux différentiels locaux, ou des distances non-isotropiques avec des effets de barrière.\nLe premier exemple concerne la recherche d\u0027une mesure des polarisations locales en Europe à partir d\u0027une information très pauvre (population et superficie des communes en 1999) mais très détaillée spatialement, correspondant au niveau communal, (Dubois, Gloersen \u0026 al. 2007  (Dubois, 2007).\n-24 -RNTI-E-13\nLe calcul des potentiels de population et de superficie pour des voisinages gaussiens de portée croissante (10,20,40,80, 160 km) a permis de résoudre le premier problème (élimination de l\u0027hétérogénéité liée au découpage initial) tandis que la comparaison des densités de population dans des voisinages de portées successives a permis de résoudre le second problème en repérant les zones localement plus denses que les espaces environnants. La série de cartes ainsi obtenues (voir figure 2) permet de mettre en valeur les pics relatifs de concentration de population à différents niveaux de généralisation et d\u0027observer comment des pics locaux (différentiels de densité entre les lissages à 10 et 20 km) fusionnent progressivement dans des pics régionaux ou globaux. Ce résultat est fondamental en terme d\u0027aménagement du territoire européen car il permet de donner une formalisation objective d\u0027un concept politique central qui est celui du polycentrisme. Même si celui-ci n\u0027est envisagé ici que sous une forme morphologique (l\u0027approche devant également être menée en termes de flux et de réseau) la méthodologie proposée ouvre de très importantes perspectives pour l\u0027identification de réseaux de villes et de coopérations transfrontalières.\nLe second exemple concerne l\u0027étude de la vulnérabilité des régions européennes face à la mondialisation 3 . La méthode des potentiels a été utilisée en tenant compte de l\u0027anisotropie de l\u0027espace puisque les distances ont été mesurées en temps routier et que d\u0027éventuels effets de barrières liés au franchissement des frontières internationales ont été introduits. A partir d\u0027un indicateur de vulnérabilité connu pour l\u0027ensemble des régions européennes (part des actifs dans les secteurs industriels menacés tels que le textile, l\u0027électronique et la mécanique), la vulnérabilité de chaque région a été comparée à celle des régions voisines en tenant compte d\u0027une part, de leur accessibilité routière, (matrice de distance temps), et d\u0027autre part, du fait que ces régions voisines étaient situées dans le même pays ou dans un pays voisin. Nous avons ainsi défini 7 niveaux de vulnérabilité correspondant à une discrétisation de l\u0027espace autour de la région elle-même selon différent temps de trajets (moins de 2h, entre 2 et 4h, entre 4 et 8h), y compris le franchissement d\u0027une frontière. Comme l\u0027illustre la figure 3, on identifie ainsi la région elle-même (Reg), une auréole de régions situées à moins de 2h du même pays (N1nat) ou d\u0027un pays voisin (N1int), puis une deuxième zone de régions situées entre 2 et 4h (N2nat, N2int) et enfin une troisième zone de régions situées entre 4 et 8 h (N3nat, N3int). Les seuils de 2h, 4h et 8h ont été choisis pour tenir compte des différents points de vue des voyageurs considérés :\n-pour les travailleurs : 2h représente le temps maximal acceptable pour se rendre à son travail -pour les entreprises : 4h représente le temps maximal acceptable pour effectuer un aller-retour dans une journée -pour les fournisseurs : 8h représente le seuil maximum de transport franchissable en une journée.\nLa distinction national/international est quant à elle fondamentale en raison des politiques plus ou moins protectionnistes de chaque pays en matière de compétition industrielle et d\u0027ouverture du marché de l\u0027emploi.\nFIG. 3 -Définition de voisinages tenant compte des temps de trajets et des franchissements de frontières.\nLes figures 4 et 5 présentent les résultats pour deux régions typiques permettent de saisir l\u0027intérêt de l\u0027approche ainsi proposée par rapport à la thématique de la vulnérabilité régionale face à la mondialisation. La région française des Ardennes et la région italienne de Modène présentent grosso modo des niveaux voisins de vulnérabilité, chacune ayant environ 15-16% de ses emplois dans des secteurs industriels menacés. Toutefois, elles s\u0027inscrivent dans des contextes radicalement différents si on examine la situation dans leurs différents voisinages fonctionnels comme définis précédemment.\nFIG. 4 -Situation de la région des Ardennes (France).\n-26 -RNTI-E-13\nLa région des Ardennes (voir figure 4) est un pic isolé de vulnérabilité, entouré de régions qui ont très peu d\u0027emplois dans les secteurs vulnérables, aussi bien à l\u0027intérieur du territoire français que dans les espaces voisins de Belgique, Luxembourg, Belgique ou Pays-Bas. Une crise des secteurs industriels vulnérables pourra donc être compensé par la recherche d\u0027emplois dans les régions voisines. Mais les entreprises touchées par la crise ne pourront pas compter sur une solidarité des territoires voisins.\nFIG. 5 -Situation de la région de Modène (Italie).\nLa région de Modène (voir figure 5) se situe au contraire à l\u0027intérieur d\u0027un véritable bastion de régions vulnérables dans un cadre qui est essentiellement national (aucune région étrangère à moins de 2h de route). Les régions les plus proches (moins de 2h de route) sont d\u0027ailleurs encore plus vulnérables que celle de Modène. Plus on s\u0027éloigne, plus la vulnérabilité diminue. À l\u0027inverse de la région des Ardennes, les employés des secteurs vulnérables risquent d\u0027avoir des difficultés à trouver un emploi dans les régions voisines si la crise frappe l\u0027ensemble du secteur. D\u0027un autre côté, les entreprises pourront plus facilement faire bloc à l\u0027échelle de plusieurs régions voisines…\nSituation par rapport à d\u0027autres méthodes\nLes vertus de la méthode ne s\u0027expriment pas sur un maillage régulier (taille des unités quasi-égales, et distance relative des centres toujours identique), mais au contraire lorsque l\u0027on traite des données issues de recensements sur des maillages irréguliers. En effet, la forme et l\u0027échelle du support des données jouent un rôle très important sur l\u0027estimation produite : ces facteurs sont la source de problèmes identifiés comme le Modifiable Areal Unit Problem (MAUP) ou plus généralement le Change Of Support Problem (COSP), qui ont donné lieu à divers développements méthodologiques et discussions sur leurs apports et contraintes respectives [Gotway et al., 2002], et qui sont liés à la nature des données traitées.\nLe premier facteur d\u0027erreur est un effet de l\u0027agrégation : les résultats de l\u0027estimation de la répartition spatiale des données varient en fonction de la taille des unités. Par exemple, le traitement d\u0027un jeu de données communales au niveau de son découpage initial ne donne pas des résultats identiques à celui qui serait fait en regroupant les données au niveau départemental. Ce qu\u0027on appelle communément -27 -RNTI-E-13 l\u0027« effet d\u0027échelle » (« scale effect ») s\u0027explique par le fait que la variance de l\u0027échantillon initial (communal dans l\u0027exemple) est mathématiquement plus élevée que la variance de la moyenne de ces échantillons. Ce qui signifie que sur des maillages fins, on perçoit une hétérogénéité des valeurs plus forte que sur des maillages plus grossiers, effets du pour partie à un simple effet mécanique, et pour partie à l\u0027organisation du phénomène à cet échelon. Le second facteur, dénommé « effet du découpage » (« zoning effect »), dérive du mode de regroupement des données à une échelle fixée : on prouve que la corrélation entre des données spatiales varie en fonction du découpage territorial (comparaison faite entre entités de surfaces comparables) (Openshaw et al., 1979). Les méthodes classiques d\u0027interpolation telles que triangulation, moyenne locale, interpolation polynomiale (splines, Bezier) ou méthode de Shepard subissent les effets du MAUP : elles sont bien adaptées à l\u0027estimation de variables continues dans l\u0027espace pour lesquelles il existe des points de mesures (comme la température, ou l\u0027altitude), mais ne le sont pas pour des variables résultant d\u0027un comptage sur une zone délimitée à l\u0027intérieur de laquelle la répartition effective de la population est inconnue. Notre démonstration (Grasland, et al, 2006) s\u0027appuie sur la comparaison de cartes obtenues à partir d\u0027un maillage de niveau NUTS 2 ou NUTS 3 en employant la méthode de Shepard. Elles ne font pas disparaître le maillage sousjacent, et au contraire, elles pourraient faire croire que les disparités observées sont le fait du phénomène étudié, alors qu\u0027en réalité, elles sont la résultante de l\u0027hétérogénéité du maillage. Ces méthodes sont les plus usuellement implémentées dans les SIG.\nPour solutionner le problème du COSP, des auteurs ont discuté les avantages des méthodes géostatistiques (krigeage) qui modélisent la variabilité spatiale des observations, et utilisent ces résultats pour inférer une surface continue, (Journel et al., 1978). Ces méthodes ont l\u0027avantage de fournir une mesure sur l\u0027incertitude des résultats. Elles s\u0027implémentent via un filtre de Kalman, et sont implantées dans de nombreux SIG. A contrario, la méthode du potentiel propose un modèle de diffusion a priori défini par un couple (fonction, portée) proposé par l\u0027utilisateur, et la structure spatiale du phénomène n\u0027entre pas en compte. Cependant si les méthodes géostatistiques ont un caractère prédictif efficace, elles restent des méthodes locales qui contrairement à la méthode du potentiel ne permettent pas de prendre en compte l\u0027impact de toutes les mesures sur un point donné de l\u0027espace géographique. D\u0027autre part, les modèles géostatistiques doivent êtres simplifiés lorsque la quantité de données à traiter est importante.\nLes méthodes basées sur les modèles bayésiens hiérarchiques sont aussi des solutions envisageables pour l\u0027estimation de variables dans le cadre du MAUP. Les hypothèses sous-jacentes sont que la variable à estimer dépend (et ceci s\u0027exprime sous forme de probabilité conditionnelle) d\u0027autres variables dont on fournit une loi de dispersion spatiale (loi de Poisson, ou Gaussienne par exemple), et dont les données de recensement sont fournies. Il est ainsi possible de modéliser des interactions complexes entre variables pour affiner l\u0027estimation (Wickel, 2002). Ce type de méthodes s\u0027inscrit donc dans des démarches plus explicatives que celle que nous proposons. Le cadre proposé pour la méthode du potentiel ne nécessite pas de statistiques auxiliaires pour l\u0027estimation des variables d\u0027intérêt.\n-28 -RNTI-E-13\nLa méthode la plus similaire est la méthode pycnophylactique (Tobler, 1979) qui, elle aussi, conserve la masse des données. Elle réalloue les stocks mesurés à l\u0027intérieur de chaque maille de façon à :\n(i) garantir la continuité avec une maille voisine, (ii) conserver sur chaque maille la masse mesurée. La courbe continue de la figure 6 montre comment peut se passer le réajustement de la répartition des stocks à l\u0027intérieur de chaque unité. C\u0027est un cas simple, on peut employer des méthodes d\u0027ajustement plus complexes où les surfaces obtenues sont au moins dérivables, voire de classe supérieure. Rase donne des développements intéressants de cette méthode (Rase, 2001). Mais en raison du prédicat (ii), la visualisation dépend alors du maillage utilisé pour l\u0027analyse : les résultats sont différents selon le niveau d\u0027agrégation des données, alors que ce n\u0027est pas le cas avec le potentiel. La méthode des potentiels s\u0027apparente aux méthodes de traitement du signal par transformation de Fourier, visant à découvrir la structure du signal échantillonné et ceci indépendamment de la fréquence d\u0027échantillonnage. Dans le cadre de l\u0027analyse spatiale, cette fréquence doit être comprise comme la mesure de la régularité des recensements dans l\u0027espace, évaluée via la distance entre chacun des centres administratifs des unités spatiales du maillage support de l\u0027étude. Pour cette raison, la méthode des potentiels s\u0027inscrit dans la lignée des méthodes proposées pour solutionner les problèmes d\u0027instabilité des résultats, notamment les corrélations, en fonction de la résolution spatiale choisie.\nCependant, en dessous d\u0027une certaine portée, la méthode devient imprécise ; la portée minimale peut être calculée par l\u0027analogue du théorème de Nyquist : elle vaut deux fois la taille maximale des mailles (Nyquist, 1928). Enfin, un maillage trop hétérogène conduit à faire un compromis entre les portées minimum associées à chaque classe de taille d\u0027unité.\n-29 -RNTI-E-13\nPar ailleurs, la méthode de transformation par potentiel n\u0027intègre pas encore les effets de barrières (cet aspect fait partie de nos futurs travaux de recherche). Par exemple, les montagnes sont des barrières géographiques qui jouent un rôle dans la dispersion spatiale des phénomènes étudiés. De même, la mer est un espace géographique non constructible, et en ce sens, les côtes maritimes pourraient être modélisées comme des barrières infranchissables lorsque on étudie la densité d\u0027habitat.\nRéalisation d\u0027un prototype\nUn prototype a été réalisé pour des utilisateurs disposant de données socioéconomiques géo-référencées, des géographes par exemple, et désireux de tester leurs hypothèses de diffusion spatiale des phénomènes étudiés. Pour l\u0027instant, ces analyses utilisent la distance orthodromique, qui permet de travailler sur de vastes espaces d\u0027étude, et requiert un format de données suffisamment simple. Le prototype se base sur une architecture distribuée client serveur.\nEn effet, le volume de données à traiter et les ressources exigées pour les calculs sont, de fait, importants. Par conséquent, la partie graphique (qui inclue la visualisation et la configuration de l\u0027analyse) est déportée sur un client Web Java, tandis que les calculs et le traitement lourd de données sont effectués sur un serveur accessible à distance, via le protocole SOAP (Simple Object Access Protocol) avec sécurisation des échanges via SSL (Secure Socket Layer). Le traitement des données coté serveur est optimisé de façon à produire des résultats intermédiaires dans des temps n\u0027excédant pas quelques secondes. La figure 7 donne une vue globale de cette architecture.\nFIG. 7 -Vue générale de l\u0027infrastructure distribuée.\nL\u0027infrastructure de gestion de données, d\u0027authentification et de gestion d\u0027erreurs restant encore à développer, l\u0027installation des fichiers de données dans un répertoire dédié à l\u0027utilisateur se fait manuellement du côté du serveur. Le format du fichier de données est le suivant : il contient une liste des points de mesure connus par leur latitude et leur longitude, et la valeur du stock associé, et il est nommé d\u0027après le nom du stock. Le client Web est fourni à l\u0027utilisateur avec le certificat de sécurité adéquat, avec le fond de carte approprié qu\u0027aura fourni l\u0027utilisateur au format -30 -RNTI-E-13 MIF/MID. Ainsi, il peut utiliser librement et à tout moment l\u0027infrastructure, sur l\u0027espace d\u0027étude qui l\u0027intéresse. Le serveur de calcul est installé sur une machine de type SMP, dont les ressources sont partagées par d\u0027autres projets de recherche, et qui est accessible à travers cette connexion Web sécurisée. Le serveur est prévu pour le partage de charge dynamique, il n\u0027y a donc pas en théorie de limitations sur le nombre d\u0027utilisateurs, même si dans les faits, il est encore restreint à la communauté des chercheurs en géographie, analyse spatiale et aménagement du territoire qui doivent s\u0027adresser au projet HyperCarte pour son utilisation.\nCommunication entre le serveur et le client\nLe format des données renvoyées par le serveur est contraint par les fonctionnalités attendues sur le client. Outre l\u0027interactivité sur le choix des palettes, du nombre de classes et du type de progression dans la distribution des couleurs, on souhaite que le client puisse générer à la demande un rapport numérique (sous forme de fichier texte ou bien HTML) contenant les coordonnées géographiques de chaque point M, avec la valeur de son potentiel. Le client a donc tout avantage à récupérer et sauver la grille matricielle des valeurs de potentiel calculées plutôt qu\u0027une image au format PNG, JPEG, ou GIF. Il peut ainsi redessiner une image rapidement en cas de changement de préférences graphiques. \nStratégie d\u0027optimisation\nRemarquons, en premier lieu, que les stratégies classiques de gestion de cache ne sont pas adaptées à notre cas car chaque requête doit générer un résultat global qui ne peut être pré-calculé puisqu\u0027il dépend des paramètres de l\u0027analyse. Cependant un examen approfondi des tâches de calcul sur le serveur montre qu\u0027il existe des redondances que nous pourrions exploiter pour optimiser certaines parties du calcul. (Grasland, 2005 -32 -RNTI-E-13\nLa visite de chaque branche de niveau n-1 dépend de la réussite du test suivant, qui vérifie si le poids des enfants du noeud n-1 est négligeable ou non :\n[ 4]\nnoeud n\"1 ! Le point délicat est d\u0027ajuster la valeur de l\u0027epsilon de manière à ne pas négliger trop de points. Par défaut, la valeur est fixée à un 1/1000 de la somme totale des stocks. Des études plus approfondies restent à mener sur l\u0027influence d\u0027epsilon sur, d\u0027une part la précision des calculs obtenus, et, d\u0027autre part, la durée du calcul. Intuitivement, on suppose qu\u0027une petite valeur d\u0027epsilon allongerait le temps de calcul, mais aussi augmenterait la précision des résultats. Inversement, un grand epsilon provoquerait une forte simplification du calcul, qui serait alors ramené à une moyenne mobile sur moments gaussiens, et diminuerait l\u0027intérêt de la méthode.\nLe second facteur de lenteur de calcul (l\u0027évaluation d\u0027expressions contenant des termes en arccosinus, cosinus et sinus d\u0027angles) peut être contourné en tabulant de façon fine ces fonctions. C\u0027est-à-dire que les valeurs des fonctions sur des angles correspondant à une division régulière et fine d\u0027un intervalle I donné sont pré-calculées, et pour tout angle la valeur de la fonction est approximée par la borne inférieure de la division à laquelle il appartient. Le grain de cette tabulation est fixé à l\u0027avance par un paramètre du programme.\nVisualisation interactive\nLa méthode prend tout son intérêt dans un contexte d\u0027utilisation interactive, qui permet une exploration des structures spatiales révélées selon un certain nombre de paramètres :\n-la fonction d\u0027interaction : une liste déroulante présente la liste des fonctions implantées sur le serveur : disk, amortized_disk, gaussian, pareto, exponential.\n-la portée moyenne en kilomètres de l\u0027analyse, sélectionnée via une barre de glissement, ou par saisie numérique. Ce choix n\u0027est pas borné.\n-la résolution de visualisation définie en nombre de points en largeur et hauteur désirés pour la grille, et qui se rapporte au pas de discrétisation de l\u0027espace étudié.\n-le cadrage (le territoire ciblé) pour l\u0027analyse, correspondant actuellement à l\u0027espace visualisé. Un zoom ou un déplacement modifie donc le cadrage.\n-les stocks proposés dans deux listes déroulantes séparées, une pour le numérateur, l\u0027autre pour le dénominateur, afin de pouvoir ensuite calculer un ratio de potentiel. Le client interroge le serveur lors de l\u0027authentification de l\u0027utilisateur pour connaître la liste des stocks disponibles.\nUne collection d\u0027onglets est créée, chacun contenant une carte spécifique : le premier présente simplement l\u0027aire d\u0027étude et le maillage administratif. Les trois suivants présentent pour une portée V 1 donnée le potentiel du numérateur, du dénominateur et leur ratio Z 1 . Les trois autres suivants de même mais sur une portée différente V 2 . Enfin le dernier onglet compare les densités entre les deux voisinages en visualisant leur différence : Z 2 -Z 1 .\nL\u0027utilisateur dispose en plus des fonctions de base de l\u0027interactivité : zoom, déplacement dans une carte, navigation dans l\u0027atlas de cartes ainsi produit. La visualisation d\u0027une carte de potentiel se fait en superposition avec un fond vectoriel présentant les limites administratives de l\u0027espace analysé. Ces cartes présentent une gradation de couleurs, suivant l\u0027intensité du phénomène, paramétrable indépendamment pour chaque onglet, via une proposition à gauche de la zone de visualisation pour le choix de palette, le type de progression, et le nombre de classes dans la distribution. La figure 10 donne un aperçu de l\u0027interface du client.\nFIG. 10 -Aperçu de l\u0027interface, avec un ratio du PNB par habitant sur un calcul gaussien de portée 50 km.\nBilan de la réalisation\nPerformances et expérimentations pour un usage local\nLe serveur peut s\u0027utiliser de façon autonome par rapport au client. Les expérimentations suivantes ont été réalisées sur un serveur bi-processeurs sous Linux (Pentium 4 à 2,6 Ghz, avec 1 Go de mémoire), une machine de travail pour un utilisateur ordinaire, avec comme jeu de test la population en Europe recensée au niveau communal, ce qui représente 116203 entités géographiques. Par exemple, -34 -RNTI-E-13 une carte de population est calculée pour une fonction gaussienne de portée 80 km avec la résolution forte de (800 x 600) en 1 minute 38 secondes.\nLa complexité de l\u0027algorithme est vérifiée lors des mesures sur les temps de calcul, comme le montre les mesures du tableau 1. Mathématiquement, elle est linéairement proportionnelle à la résolution (nombre de points n à calculer sur l\u0027image). Elle est aussi linéairement proportionnelle à la portée p. La connaissance de complexité en O(np) permet d\u0027estimer le temps de calcul d\u0027une carte en fonction de la résolution demandée n et de la portée p requise. Cette durée reste raisonnable pour un usage sur des machines classiques : 2 minutes environ pour une longue portée (100 km) et une résolution fine de (800 x 600).\nRésolution\nPortée ( \nPerformances et expérimentations pour un usage serveur\nNous avons implanté des options pour l\u0027exécution parallèle du calcul sur un ensemble de processeurs, dans le cas où le serveur de calcul est partagé par plusieurs usagers, et où il se déploie sur une machine multi-processeurs. L\u0027algorithme de calculs des potentiels est une itération sur l\u0027ensemble des points de la grille qui sont arrangés dans un tableau, dont la taille n égale la valeur de la résolution. Chaque point peut-être calculé indépendamment des autres : pour cette raison la parallélisation du programme est très aisée. Elle est fondée sur la distribution de tâches (le calcul d\u0027une entrée du tableau) à un nombre k de processeurs. Le nombre de points dévolus à un processeur est donc de n/k, et le calcul se termine lorsque chaque processeur a terminé le calcul de sa portion de tableau. Cette distribution naïve des tâches montre cependant deux limitations majeures. En premier lieu, un tel algorithme n\u0027est pas résistant aux perturbations : si les capacités d\u0027un processeur tombent à 50% brusquement, le temps de calcul est alors rallongé du temps équivalent à la moitié de sa tâche, puisqu\u0027il n\u0027y a pas de procédé de rééquilibrage des charges de travail automatique entre processeurs. D\u0027autre part, notre grille est très hétérogène : les points situés dans des zones à faible densité de mesure sont calculés bien plus rapidement grâce au procédé d\u0027élagage, et certains processeurs terminent donc leur travail plus vite.\nUne solution pour la répartition de charge dynamique a été imaginée, par un mécanisme de transmission de charge adaptatif (Roch, 2006) : chaque processeur « libre » vole la moitié de la tâche de travail restante p au processeur occupé. Cette redistribution n\u0027a lieu que lorsque le processeur occupé achève la portion de travail -35 -RNTI-E-13\nindivisible qui lui est attribuée ? log(p). ? est un paramètre configurable, qui est ajusté de manière à ce que le temps de contention (lock pour accéder au tableau de données) soit largement inférieur au temps de calcul de la portion du tableau. Cette notion de quantité minimale de travail est introduite pour éviter les problèmes de contention lors de l\u0027accès au tableau de points : lorsque qu\u0027un processeur prend une tâche, il actionne un sémaphore sur le tableau et bloque l\u0027accès des autres processeurs au tableau. Ce temps d\u0027inter-blocage est court, et la définition d\u0027une charge de travail minimale optimisée évite la répétition fréquente d\u0027inter-blocages. Cet algorithme a été mis en oeuvre avec PTHREAD dans notre implémentation et, testé sur une machine multi-processeurs de type SMP, huit coeurs avec mémoire partagée sur la population européenne, à une résolution fixée de 800*600 et une portée gaussienne de 100 km. Il donne les résultats confirmant l\u0027efficacité de la méthode, (voir la figure 11). En outre, nous signalons que cet algorithme a été implémenté avec deux librairies différentes pour l\u0027ordonnancement de tâches, TBB 6 (Intel Threading Building Blocks, http://threadingbuildingblocks.org/) et Kaapi (Danjean, 2007). Quelque soit la librairie, les performances sont améliorées de façon similaires : le gain en temps de calcul est linéaire avec le nombre de processeurs mobilisés, tant que la redistribution n\u0027est pas saturée (observée à partir de 5 processeurs avec PTHREAD, elle est repoussée avec Kaapi car l\u0027algorithme de répartition de tâches est mieux optimisé).\nSaturation de la redistribution FIG 11 -Accélération du calcul des potentiels en fonction du nombre de processeurs\nLe mode de répartition dynamique de charge décrit ne tient pas compte de la dimension spatiale des données : une répartition de charge basée sur un découpage -36 -RNTI-E-13 spatial statique paraît plus efficace. Afin de vérifier cette hypothèse, une seconde version de l\u0027algorithme de répartition de charge a été implémentée et testée : l\u0027espace d\u0027étude est découpé en tuiles contiguës de tailles égales, et chaque tuile est traitée en parallèle par un des processeurs mobilisés pour le calcul. La figure 12 illustre les résultats comparés des deux modes de répartition de charge sur un banc d\u0027essai de 16 processeurs. Les points des courbes situent le taux d\u0027occupation des processeurs au cours du calcul, en fonction du temps. On constate que l\u0027algorithme de répartition dynamique de charge termine plus tôt (une centaine de secondes d\u0027écart), et occupe avec un meilleur rendement les processeurs que l\u0027algorithme de répartition statique. Une première explication est suggérée par l\u0027interprétation du palier observé au niveau du taux d\u0027occupation des processeurs pour l\u0027algorithme statique : certains blocs de calculs avec une densité de données moindre auront terminés plutôt, laissant des processeurs inactifs, dans l\u0027attente de la fin du calcul.\nFIG. 12 -Comparaison des performances de deux modes de répartition de charge pour le calcul d\u0027une carte de potentiel sur un ensemble de 16 processeurs.\nLa version statique pourrait être améliorée en découpant les tuiles proportionnellement à la densité de données présente, de manière à équilibrer la charge sur les différents processeurs. Cependant, un partitionnement statique, même prenant en compte la répartition spatiale des données, ne permet pas de s\u0027adapter dynamiquement aux variations de charges CPU induites par les autres utilisateurs de l\u0027environnement de calculs intensifs, contrairement à l\u0027algorithme adaptatif. Par contre, la combinaison d\u0027un partitionnement statique pour la première partie du -37 -RNTI-E-13 calcul et d\u0027un équilibrage de charge dynamique pour la fin du calcul est une stratégie qui porte souvent ses fruits lorsqu\u0027on travaille sur des volumes importants de données (Jafar, 2006).\nLes temps de latence réseau sont bons, malgré le fait que nous n\u0027avons pas encore mis en oeuvre la compression des données échangées via gzip. En fait, on mesure un temps correspondant à l\u0027emballage de la réponse, l\u0027encryptage et le déballage de 4s localement avec une résolution de 300 x 400. Côté client, la reconstruction de l\u0027image à partir d\u0027un tableau de flottants prend très peu de temps (128ms). Cette matrice est re-calculée dès lors que l\u0027utilisateur change un des paramètres d\u0027analyse, agrandit, réduit ou se déplace dans la zone de visualisation. Elle est cachée avec les paramètres de la requête correspondante afin de pouvoir l\u0027exploiter ultérieurement. La résolution demandée étant le plus souvent inférieure à celle de l\u0027image vectorielle (1027 x 688), une interpolation bi-quadratique est appliquée sur l\u0027image produite afin de la caler sur le fond vectoriel.\nPerspectives d\u0027amélioration\nPlusieurs pistes s\u0027offrent pour améliorer les performances du serveur de calcul. Par exemple, le calcul de la distance orthodromique peut bénéficier d\u0027un pré-calcul supplémentaire. Sans entrer dans les détails, il faut encore exploiter les travaux de recherche concernant la quête du compromis idéal entre consommation d\u0027espace mémoire et gain de temps obtenu par tabulation. De même, la troncature des flottants transmis dans la grille matricielle réduirait le volume des données échangées, et accorderait donc un gain de temps sur la transmission des données. Un algorithme qui automatise la détermination du niveau de troncature est en cours d\u0027élaboration.\nD\u0027autres pistes, moins classiques, sont liées à l\u0027usage de notre algorithme de cutoff. Il s\u0027agit de préparer une stratégie de sous-échantillonage des données : par exemple, adapter la valeur du seuil d\u0027élagage dynamiquement au lieu de fixer arbitrairement sa valeur, ou bien limiter la visite de l\u0027arbre à un certain niveau, sans aller jusqu\u0027aux feuilles, lorsque que le niveau d\u0027information requis est très grossier. Dans ce but, l\u0027usage de cubes de données organisés en fonction de la hiérarchie spatiale des maillages territoriaux, tels ceux développés dans le domaine de l\u0027analyse de données en ligne (OLAP) serait d\u0027un très grand intérêt (Rigaux, 2005).\nEnfin, la sélection de la distance pourrait se faire de manière adaptative, selon l\u0027échelle d\u0027étude retenue. Par exemple, employer la distance euclidienne sur la région Rhône-Alpes n\u0027introduirait pas de biais notables.\nDe façon identique, quelques améliorations pour le client sur le plan de l\u0027ergonomie sont prévues. Comme, par exemple, décorréler la zone de visualisation et la zone de traitement, en utilisant un cadre de sélection de la surface à traiter. Nous souhaitons donner une interprétation plus directe de la résolution : indiquer un pas en nombre de kilomètres, plutôt qu\u0027un nombre de points sur une grille. Enfin, le calcul et la représentation de courbes de niveau à partir de l\u0027image matricielle amélioreraient sensiblement le fondu avec le fond vectoriel représentant le maillage administratif.\n-38 -RNTI-E-13\nLe client offre aussi un champ de réflexions en ce qui concerne l\u0027usage de caches autorisant un raffinement progressif des images, grâce à l\u0027émission de requêtes successives. En effet, tant que l\u0027utilisateur ne modifie pas ses souhaits, il serait judicieux de construire une image dont la résolution augmente dans le temps, et d\u0027anticiper un zoom sur la zone lissée.\nConclusion\nCet article présente HyperSmooth, une mise en oeuvre du calcul et de la visualisation de potentiels dans un contexte de cartographie interactive. L\u0027objectif étant de fournir un outil accessible à des utilisateurs via le Web, tout en assurant la sécurité et la confidentialité de ces données. Nous montrons que la méthode est adaptée pour étudier la propagation spatiale de phénomènes sociaux, environnementaux, ou économiques. Nous soulignons que le paramétrage d\u0027une telle analyse peut s\u0027avérer délicat, et nécessite donc de développer un outil adapté afin de rendre les choix plus compréhensibles pour un utilisateur néophyte.\nLa réalisation d\u0027un tel prototype nécessite une réflexion avancée et coordonnée sur l\u0027architecture, les modalités d\u0027optimisation des calculs, et de paramétrage de l\u0027analyse pour offrir une visualisation interactive. En effet, le coût du calcul, et les contraintes de confidentialité sur les données nous placent face à un défi technique.\nLe bilan de notre réalisation s\u0027avère positif quant au choix de l\u0027architecture distribuée : répartition des calculs sur un serveur parallèle d\u0027un coté, visualisation et paramétrage de l\u0027analyse sur un client Java intelligent de l\u0027autre, les deux parties étant connectées via un protocole de plus en plus répandu et présentant une accessibilité et sécurité maximale : SOAP, couplé avec un cryptage SSL. De plus, les calculs sont accélérés grâce à l\u0027utilisation d\u0027une méthode de cut-off algébrique, et une tabulation des distances orthodromiques. L\u0027ensemble du code et un manuel du serveur sont accessibles librement sur Internet à l\u0027adresse suivante : http://hyantes.gforge.inria.fr/ Une question d\u0027ordre théorique et algorithmique porte sur l\u0027introduction d\u0027autres distances, et sur l\u0027extension de la méthode de façon à illustrer les propriétés anisotropiques de l\u0027espace géographique réel. Comme l\u0027a montré une étude des phénomènes de tempêtes, aux flux très orientés (Boulier, 2003), il est intéressant de développer sur la base de la méthode générale une vision anisotropique de l\u0027espace. Pour quantifier le potentiel en un lieu, il s\u0027agit de tenir compte à la fois de la distance de la source d\u0027information au point à estimer, mais aussi de l\u0027orientation du flux d\u0027information qui relie les deux points. Dans le premier exemple, appliqué à la forêt, les corrélations entre les potentiels ainsi déterminés et les dégâts constatés sont très nettes. Cette modification de la méthode générale permet d\u0027intégrer de nouvelles formalisations des mouvements dans l\u0027espace (vents dominants, courant marin, etc.) complétant celles qui sont associées à la fonction d\u0027interaction spatiale. Cette extension du modèle peut également alors rendre compte des barrières géographiques naturelles comme la mer ou bien les montagnes pour l\u0027étude de la dispersion de phénomènes dans l\u0027espace, ou de phénomènes de polarisation.\n"
  },
  {
    "id": "841",
    "text": "Introduction\nLa prise en compte des connaissances additionnelles constitue un problème essentiel et un vrai défi pour la recherche actuelle dans le domaine de la classification automatique. Il s\u0027agit à la fois de l\u0027expression, de la structuration et de la formalisation des connaissances (appelées aussi connaissances a priori) pour les intégrer dans le processus de la classification automatique. Les premiers travaux dans ce domaine ont été réalisés par (Wagstaff et Cardie, 2000) en modifiant l\u0027algorithme COBWEB proposé par (Fisher, 1987). Les auteurs ont montré, à partir de résultats expérimentaux, une amélioration claire de la précision de la classification. Les mêmes auteurs ont proposé une autre approche qui intègre les contraintes dans l\u0027algorithme K-means (MacQueen, 1967). L\u0027algorithme proposée est appelé COP-Kmeans (Wagstaff et al., 2001). Son principe consiste à contrôler la violation des contraintes dans la phase de mise à jour des classes. Les auteurs arrivent à démontrer qu\u0027il est possible d\u0027améliorer sensiblement la précision du partitionnement même avec un nombre réduit de contraintes. Les auteurs dans (Davidson et Ravi, 2005) ont étudié le problème de la faisabilité de la classification en présence de plusieurs combinaisons de contraintes dans une approche de type K-means. Récemment, nous avons proposé dans (Elghazel et al., 2007)  Les contraintes sur les observations définissent une relation binaire, positive et transitive entre les observations. Ce qui génere des contraintes sous-entendues apportées par le calcul de la fermeture transitive. Ces contraintes sont présentées sous la forme suivante :\nL\u0027algorithme proposé\nNotre contribution se situe dans les deux phases de l\u0027étape itérative de l\u0027algorithme de Kohonen (Kohonen, 1994) : la phase d\u0027affectation (compétition) au cours de laquelle une forme z i est affectée au neurone de la carte le plus proche au sens de la distance euclidienne et qui n\u0027entraîne pas une violation de contraintes (L\u0027ensemble de ces neurones est noté C Cond ), et la phase de minimisation (adaptation) qui consiste à mettre à jour les poids des neurones du voisinage du neurone vainqueur dans l\u0027étape de compétition. Ces neurones ne doivent pas entraîner une violation de contraintes. La modification majeure apportée à l\u0027algorithme de Kohonen est essentiellement dans la phase de compétition par l\u0027incorporation de la fonction ViolateCon(.) (Algorithme 2). Cette fonction prend en entrée le neurone courant c, l\u0027observation z i présentée à l\u0027apprentissage, l\u0027ensemble des contraintes positives (Con \u003d ) et l\u0027ensemble des contraintes négatives (Con \u003d ). Elle retourne en sortie l\u0027ensemble des neurones C viol qui entraînent une violation de contraintes avec l\u0027observation z i . D\u0027une part, CrTM (Algorithme 1)parcourt les paires (z i , z j ) satisfaisant les contraintes positives Con \u003d et vérifie si z j est affecté à un neurone différent de c, dans le cas échéant, il y a violation de contraintes et donc c ? C viol qui n\u0027entrera pas en jeu dans le choix du neurone vainqueur. D\u0027autre part, l\u0027algorithme parcourt les paires (z i , z k ) qui satisfont les contraintes négatives(Con \u003d ) et vérifie si z k est affecté au neurone courant c, si oui, il y aura une violation de contraintes et donc C viol \u003d {c, V (c)} tel que V (c) représente les voisins de c. Cette vérification est nécessaire pour garantir que z i sera affecté loin du neurone violant c et aussi pour que l\u0027ensemble des neurones C viol ne se rapprochent pas de z i et donc ils ne l\u0027apprennent pas en conséquence. \nAdaptation :mise à jour des poids des neurones C Cond \nun paramètre d\u0027adaptation , appelé pas d\u0027apprentissage et T \u003d T (t) représente le rayon de voisinage. Ces deux paramètres décroissent en fonction du temps t. K T (?(c, r)) \u003d exp( ?0.5?(c,r) T (t)\n).\nAlgorithme 2 ViolateCon(c, z i , Con \u003d , Con \u003d) ENTRÉES: c : un neurone donné, z i : une observation donnée, Con \u003d , Con \u003d : Ensembles des contraintes positives et négatives, respectivement. SORTIES: C viol : Ensemble des neurones entraînant une violation avec l\u0027observation présen-tée z i . 1: pour tout z j tel que Con \u003d (z i , z j ) faire 2:\nC viol \u003d {c} 4:\nsi z k ? c alors  (Rand, 1971). Cet indice représente une mesure d\u0027accord entre deux partitions P 1 , P 2 d\u0027un même ensemble de données A. P 1 représente la partition correcte produite par les étiquettes des classes prédéfinies. P 2 représente la partition produite par l\u0027algorithme CrTM. Chaque partition est vue comme un ensemble de N (N ? 1)/2 paires de décisions où N est la taille de A. Pour chaque paire d\u0027observations z i , z j dans A, P 1 et P 2 les assignent à la même classe ou à deux classes différentes. Nous montrons aussi pour tester notre proposition que les informations apportées par les contraintes peuvent améliorer la performance de la classification même sur les observations qui n\u0027ont pas été concernées par les contraintes (\u0027Held-Out\u0027). Cette dernière mesure représente le taux de bonne classification en ne considérant que les observations qui ne sont pas directement (ou par transitivité) concernées par les contraintes.\nRésultats utilisant les contraintes artificielles\nLa validation expérimentale est réalisée par la génération des contraintes artificielles, c\u0027est-à-dire pour générer une contrainte, nous prélevons aléatoirement deux observations de la base étiquetée et nous comparons leurs étiquettes : si elles ont la même étiquette, une contrainte positive est générée sinon une contrainte négative. Nous avons choisi trois bases de données de la banque UCI (Blake et Merz., 1998) : \"Soybean\", \"Tic-tac-toe\" and \"Heart Disease\". L\u0027application de CrTM sur la première base a fournit une performance de 100% sans aucune contrainte. Ce résultat parfait est dû à la séparation linéaire des 4 classes dans l\u0027espace des données, ce qui n\u0027a pas été un souci pour l\u0027apprentissage de la carte. Nous rappelons que COB-COBWEB et COP-Kmeans atteignent, respectivement, 96% et 98% après l\u0027intégration de 100 contraintes artificielles. COP-b-coloring atteint la performance de 100% avec 30 contraintes. Vue la nature symbolique des variables de la deuxième base, nous avons procédé par codage disjonctif pour générer une base d\u0027apprentissage numérique. CrTM commence avec un taux de 66.6 % sans contraintes. Il atteint une performance totale de 96,70% (\"Overall Accuracy\") et 91.40% sur le \"Held-Out\" avec 500 contraintes aléatoires (les résultats du \"Held-Out\" sont : 49% par COP-COBWEB, 56% par COP-Kmeans et 82% par COP-b-coloring avec le même nombre de contraintes). CrTM a donc permis une amélioration de 24% (Figure 1.b) En l\u0027absence de contraintes, l\u0027algorithme de Kohonen atteint une performance de 78% sur la troisième base (Figure 1.a). Après l\u0027intégration de 240 contraintes aléatoires, notre algorithme améliore de 13% la performance totale, avec un \"Overall Accuraccy\" de 91%. Le \"HeldOut\" quant à lui atteint les 85% avec seulement 150 contraintes intégrées, soit 7% d\u0027améliora-tion avec seulement 5% d\u0027information a priori. L\u0027algorithme COP-b-coloring atteint une performance de 50% sans contraintes puis de 89% avec 500 contraintes et le \"Held-Out\" avoisine les 66%.\nApplication aux données de mélanomes chez l\u0027humain\nNous avons testé l\u0027algorithme CrTM sur une base médicale contenant 226 images de grain de beauté. Chaque image est caractérisée selon la règle ABCD (Stolz, 1994). l\u0027Asymétrie de formes, de textures et de couleurs, les Bords abrupts des structures pigmentées, la diversité des Couleurs, et différentes structures Dermatoscopiques. Les contraintes sont représentées par l\u0027information a priori apportée par les diagnostics des dermatologues sous forme d\u0027étiquettes associées aux images. Nous avons constaté que CrTM atteint une performance de 78,2% sans contraintes. \"Overall Accuracy\" augmente jusqu\u0027à 84%, après l\u0027intégration de 210 contraintes, soit un total de 3200 paires de décisions (y compris celles obtenues par transitivité). Ces 17% d\u0027information a priori nous ont amélioré la performance de la classification, calculée par \"Held Out\", pour atteindre 81,2% (voir Figure 2).\nFIG. 2 -Evaluation de la performance de CrTM sur la base des mélanomes\nConclusion\nNous avons proposé dans cet article une extension de l\u0027algorithme des cartes topologiques pour intégrer les contraintes liées aux données. Ces contraintes sont binaires (entre chaque\n"
  },
  {
    "id": "842",
    "text": "Introduction\nEn fouille de texte comme en recherche d\u0027information (RI), plusieurs modèles sont utilisés pour représenter un document. Ces modèles, de type probabiliste, booléen ou vectoriel, se sont révélés bien adaptés pour représenter des documents textuels. Cependant, ils présentent l\u0027inconvénient de ne pas tenir compte de la structure du document. Or, la plupart des informations disponibles aujourd\u0027hui sur Internet ou dans des bases documentaires sont fortement structurées. C\u0027est la raison pour laquelle des travaux récents, en RI comme en fouille de données se sont intéressés à la structure des documents. Ceci a notamment conduit à l\u0027émergence de la recherche d\u0027information XML orientée contenu dont l\u0027objectif est justement d\u0027exploiter l\u0027information structurelle contenue dans les documents pour concevoir des systèmes de RI plus efficaces. La compétition INEX 2 (INitiative for Evaluation of XML Retrieval) produit d\u0027ailleurs depuis 2002 de larges collections de documents utilisables pour l\u0027évaluation de tels systèmes. L\u0027exploitation de la structure a aussi été étudiée dans des tâches de classement, supervisé ou non, de documents . Dans ce contexte, plusieurs voies ont été envisagées, parmi lesquelles on citera l\u0027extension des modèles usuels de représentation de documents textuels [Doucet et Ahonen-Myka (2002)] ou l\u0027exploitation de la structure arborescente des documents XML [Yi et Sundaresan (2000); Marteau et al. (2005); Vercoustre et al. (2006)]. Enfin, dans le contexte de la détection d\u0027information nouvelle (Novelty Detection), d\u0027autres travaux ont pris en compte la structure logique des documents en estimant le poids à accorder chacune des parties qui le composent [Jacquenet et Largeron (2006)].\nDans cet article, nous proposons d\u0027étendre le modèle probabiliste de façon à tenir compte du rôle joué par les éléments de structure et de mise en forme pour mettre en évidence des informations importantes. Notre approche nécessite une phase d\u0027apprentissage sur une partie de la collection considérée. Au cours de cet apprentissage, un poids est calculé pour chacune des balises, basé sur la probabilité pour que cette balise distingue les termes pertinents. Dans une seconde phase, le modèle que nous avons développé permet d\u0027estimer, en tenant compte de ce poids, la probabilité qu\u0027un document de la collection soit pertinent pour une requête donnée. Ce modèle est décrit dans la prochaine section tandis que les résultats d\u0027expérimentations obtenus sur la collection INEX 2006 sont présentés dans la troisième section.\n2 Un modèle probabiliste de représentation de documents structurés\nPrincipe d\u0027intégration de la structure dans un modèle probabiliste de documents\nEn Recherche d\u0027Information, le modèle probabiliste de documents [Robertson et Jones (1976)] aspire à estimer la pertinence d\u0027un document pour une requête à partir de deux probabilités : celle de trouver une information pertinente et celle de trouver une information non pertinente. Ces estimations sont basées sur la probabilité de chacun des termes contenus dans le document d\u0027apparaître dans un document pertinent ou dans un document non pertinent de la collection. Pour ce faire, on utilise une collection de test, composée de documents, de requêtes et de la connaissance des documents pertinents pour chaque requête. Cette collection permet, dans une phase d\u0027apprentissage, d\u0027estimer la probabilité de pertinence de chaque terme en fonction de ses distributions respectivement dans les documents pertinents et les documents non pertinents.\nNotre objectif est d\u0027intégrer la structure des documents dans ce modèle afin de parvenir à une recherche d\u0027information structurée. Dans notre modèle, seront considérés des éléments de structure logiques (titre, section, paragraphe, etc.) et de mise en forme (souligné, en gras, centré, etc.). L\u0027intégration de la structure dans le modèle probabiliste s\u0027effectue ensuite à deux niveaux. Dans le premier, la structure logique est utilisée pour identifier les éléments XML qui seront susceptibles d\u0027être indexés par notre système : les sections, paragraphes, tableaux, etc. Dans le second, les balises de structure logique et de mise en forme sont intégrées au modèle probabiliste classique. Cette intégration nécessite une étape préliminaire qui consiste à estimer un poids pour chacune des balises. Ce poids est basé sur la probabilité pour qu\u0027une balise distingue les termes pertinents. Dans la seconde étape d\u0027intégration des balises, le modèle que nous avons développé permet de déterminer la probabilité qu\u0027un document de la collection soit pertinent pour une requête donnée en tenant compte non seulement de la pondération classique des termes du modèle probabiliste, mais aussi de la pondération de chacune des balises qui englobent ces termes. La section suivante présente plus formellement ce modèle probabiliste de représentation de documents structurés.\nNotations\nOn dispose d\u0027un ensemble D de documents structurés. En pratique, il s\u0027agira le plus souvent de documents XML. Chaque élément logique (i.e. section, paragraphe,...) e j d\u0027un document XML représente donc un ensemble de termes, délimité par une balise structurelle logique, qui sera utilisée pour indexer l\u0027élément.\nOn note : -E \u003d {e j , j \u003d 1, ..., l}, l\u0027ensemble des éléments structurés considérés dans la collection, par exemple des sections, des paragraphes, etc. -T \u003d (t 1 , ..., t i , ...t n ), un index de termes construit sur E. -B \u003d {b 1 , ..., b k , ..., b m }, l\u0027ensemble des balises logiques et de mise en forme considé-rées. Soit E j , un vecteur de variables aléatoires T ij à valeur dans {0, 1} :\nT ik \u003d 1 si le terme t i apparaît étiqueté par la balise b k T ik \u003d 0 sinon T i0 \u003d 1 si le terme t i apparaît sans être étiqueté par une des balises de mise en évidence de B T i0 \u003d 0 si le terme t i n\u0027apparaît pas sans étiquette On notera e j \u003d (t 10 , ..., t 1k , ..., t 1m , t i0 , ..., t ik , .., t im , t n0 , ..., t nk , .., t nm ) une réalisation de la variable aléatoire E j . À partir de cette représentation, l\u0027objectif est maintenant d\u0027étendre le modèle probabiliste pour prendre en compte la structure de mise en forme des documents.\nProbabilité de pertinence d\u0027un élément XML, basée sur les balises\nLa fonction de pondération BM25, introduite par [Robertson et Jones (1976)] auquel nous renvoyons le lecteur pour plus de précision, est très largement utilisée dans les systèmes de recherche d\u0027information probabilistes pour estimer le poids d\u0027un terme t i dans un élément XML e j . Dans notre modèle, cette première pondération, notée w ij , est enrichie de manière à prendre en compte la structure logique et de mise en forme des documents. Dans un contexte de recherche d\u0027information, on désire en effet estimer la pertinence d\u0027un élément XML e j relativement à une requête. Ce qui revient à estimer P (R|e j ) (respectivement P (N R|e j )), la probabilité de trouver une information pertinente (respectivement non pertinente) quand on observe l\u0027élément e j pour une requête donnée.\nOn introduit une fonction de classement f c 1 (e j ) qui permettra, en comparant ces deux probabilités, d\u0027ordonner les documents en fonction de leur pertinence par rapport à la requête :\nPlus f c1(e j ) est élevée, plus pertinentes sont les informations contenues dans e j . Par la formule de Bayes et en éliminant le terme P (R) P (N R) constant sur la collection pour une requête donnée qui n\u0027interviendra donc pas dans le classement des documents, on obtient f c 2 , proportionnelle à f c 1 : f c 2 (e j ) \u003d P (e j |R) P (e j |N R)\nMoyennant l\u0027hypothèse d\u0027indépendance des termes (Binary Independance Model) :\nPour simplifier les notations, on pose :\nprobabilité de ne pas avoir t i sachant que l\u0027élément est pertinent\nprobabilité de ne pas avoir t i sachant que l\u0027élément est non pertinent\nEn reportant dans la fonction de classement f c 2 (e j ) :\nLa fonction logarithmique étant croissante, en prenant le logarithme de f c 2 (e j ) le classement produit par la fonction f c 3 (e j ) \u003d log(f c 2 ) sera le même que celui produit par f c 2 :\nLe terme t ik ?ej log(\n) est une constante relativement à la collection (i.e. indépen-dant de t ik ), ne pas le considérer ne change pas le classement produit par la fonction. On en déduit la fonction de pertinence, tirée de f c 3 :\nAinsi, dans ce modèle probabiliste tenant compte de la structure du document, la pertinence d\u0027un élément e j par rapport aux balises est mesurée par le score f c balises (e j ) :\nEn pratique, pour mesurer cette pertinence, il convient d\u0027estimer les probabilités p ik et q ik , i ? {1, .., n} et k ? {0, .., m} à partir d\u0027un échantillon d\u0027apprentissage EA constitué d\u0027élé-ments déjà jugés pour une requête. À partir des ensembles R et NR contenant respectivement les éléments pertinents et non pertinents, on obtient :\n-r ik : nombre de termes t i étiquetés par b k parmi les éléments pertinents de EA ; -n ik : nombre de termes t i étiquetés par b k parmi les éléments de EA ; -r ? ik \u003d n ik ? r ik : nombre de termes t i étiquetés par b k parmi les éléments non pertinents de EA ; -R \u003d ik r ik : somme des occurrences des termes figurant parmi les éléments pertinents de EA ; -N ? R \u003d ik r ? ik : somme des occurrences des termes figurant parmi les éléments non pertinents de EA.\nAyant construit des estimateurs sans biais de pik et de q ik , les probabilités d\u0027avoir le terme t i étiqueté par b k sachant que l\u0027élément est respectivement pertinent et non pertinent, on en déduit p.k, la probabilité d\u0027avoir la balise b k sachant que l\u0027élément est pertinent et q.k, la probabilité d\u0027avoir la balise b k sachant que l\u0027élément n\u0027est pas pertinent :\nCombinaison des pertinences basées sur les termes et sur les balises\nPour obtenir un score global de classement f c(e j ) d\u0027un élément e j , en fonction des termes et des balises, qui permette d\u0027estimer sa pertinence par rapport à une requête, nous avons proposé une une première approche qui consiste à multiplier le poids w ij de chaque terme de e j par la moyenne des poids w ? ik correspondant à toutes les balises qui englobent le terme. Ainsi, nous calculons f c(e j ) \u003d ti?ej w ij * k/t ik \u003d1 w ? ik 3 Expérimentation sur la collection INEX\nPrésentation de la collection\nNous avons évalué notre modèle sur la collection d\u0027INEX 2006 (Initiative for Evaluation of XML Retrieval) composée de 659.388 articles en anglais issus de l\u0027encyclopédie Wikipedia. Le modèle vectoriel basé sur la fonction de pondération BM25, a été utilisé comme modèle de référence et comparé au modèle probabiliste structurel décrit précédemment, qui utilise lui aussi la fonction de pondération BM25, mais en intégrant les balises. Les résultats ont été évalués en utilisant les taux de précision et de rappel ainsi que la mesure de performance globale interpolated mean average precision (iMAP) permettant de les combiner et définie dans [Pehcevski et al. (2007)] Sur les 114 requêtes de la collection, l\u0027indice iMAP est égal à 2,34% dans le cas du modèle de référence sans utilisation des balises. Il est égal à 1,80% quand toutes les balises sont considérées. Cette tendance est confirmée lorsqu\u0027on considère le rappel et la précision indé-pendamment l\u0027un de l\u0027autre [Géry et al. (2007)]. Ces résultats préliminaires peu convaincants, ne remettent pas nécessairement en cause l\u0027intérêt d\u0027exploiter l\u0027information structurelle en plus de l\u0027information textuelle mais plutôt les modalités de combinaison des poids des termes avec ceux des éléments structurels.\nConclusion\nDans cet article, nous avons proposé d\u0027étendre le modèle probabiliste de représentation des documents structurés de façon à tenir compte du poids des balises représentant la structure logique et la structure de mise en forme. Ces poids sont estimés par apprentissage puis inté-grés dans le calcul de la probabilité qu\u0027un document de la collection soit pertinent pour une requête donnée. Bien que les résultats préliminaires obtenus sur la collection test de la campagne d\u0027évaluation INEX 2006 soient peu convaincants, nous pensons qu\u0027ils ne remettent pas en cause l\u0027intérêt d\u0027exploiter l\u0027information structurelle en plus de l\u0027information textuelle, mais que la combinaison des poids des termes avec ceux des balises doit être étudiée de manière plus approfondie.\n"
  },
  {
    "id": "843",
    "text": "Introduction\nL\u0027ensemble des propositions présentées dans cet article est élaboré dans le cadre du projet ?R 1 , un des projets clefs d\u0027une action de recherche appliquée initiée depuis trois ans 2 .\nDu point de vue approche adoptée, elle est résolument descendante (top-down). Par l\u0027étude et la compréhension des usages cibles, elle débouche sur la formalisation du fonctionnement de l\u0027information nécessaire à cet usage. Cette information étant contenue dans un corpus documentaire source. La formalisation obtenue permet alors de concevoir un modèle de l\u0027information attendue, puis d\u0027opérationaliser des outils de repérage et de représentations symboliques afin d\u0027alimenter de manière automatique ce modèle.\nComme annoncé dans Casenave et al. (2004) et formalisé dans Nodenot et al. (2006) et Loustau et al. (2008), de plus en plus d\u0027activités pédagogiques sont construites autour des documents patrimoniaux. C\u0027est notamment le cas en géographie : localisation des principaux lieux constitutifs d\u0027un itinéraire de voyage, positionnement sur une carte des lieux visités, lecture d\u0027un itinéraire à différentes échelles, etc. L\u0027enjeu ici est de se servir des documents analysés automatiquement pour ne concevoir que les activités que l\u0027on va pouvoir encadrer de manière automatisée (ou assistée) par un tuteur informatique. Des expérimentations avec des enseignants et des apprenants sont menées depuis 2 ans pour définir les contextes d\u0027utilisation opérationnels qui ont du sens pour de vrais enseignants.\nConcernant le traitement automatique, la réponse à l\u0027usage cible retenu nécessite une phase de marquage de l\u0027information mettant en oeuvre des méthodes s\u0027apparentant à de l\u0027extraction d\u0027information (IE) et une phase de Recherche d\u0027Information Géographique (GIR), concept proposé en 2004 et précisé en 2005 par Jones et Purves (2006) les co-fondateurs du premier workshop de ce nom. Selon le niveau de la représentation symbolique souhaité par la suite, l\u0027interprétation à réaliser dans la phase de GIR peut s\u0027appuyer sur une granularité plus ou moins locale de l\u0027information géographique. Compte tenu de l\u0027approche adoptée, cette granularité variera du niveau syntagmatique 3 (souvent considéré comme atomique pour l\u0027information géographique) à des agglomérats de ce premier niveau, de plus en plus importants (portés par une proposition, une phrase, un paragraphe ou plus) et ceci selon les besoins exprimés par l\u0027usage.\nDès que l\u0027on veut obtenir une représentation symbolique d\u0027une information, les traitements en GIR doivent se baser sur des méthodes de calcul du sens. Dans ce cas il faut alors se questionner sur le type de représentation sémantique le plus adapté pour capturer automatiquement le sens de telle ou telle granularité textuelle. Comme le précise Blackburn et Bos (2003), il n\u0027y a actuellement pas de réponse unique à ce type de questionnement : cela dépend essentiellement du niveau de finesse attendu et du type de « phénomène »linguistique auquel on s\u0027intéresse.\n1 ?R : Prototype pour l\u0027Interprétation d\u0027Itinéraires dans des Récits de voyage 2 Cette action de recherche est portée par diverses formes de collaborations réalisées entre chercheurs du LIUPPA (équipe-projet DeSI), chercheurs de laboratoires tels que le LRI, le COGIT (IGN), L\u0027IRIT (grâce au projet ANR-GEONTO) et différents partenaires d\u0027organisations privées ou publiques : les entreprises DIS et Géocime, la média-thèque de Pau (MIDR), la communauté d\u0027agglo. (CDAPP), le conseil général (CG64). Cette action a comme objectif de créer un ensemble de ressources et de traitements afin de raisonner, dans le cadre d\u0027usages ciblés, sur de l\u0027information textuelle à contenu géographique, après l\u0027avoir recherchée, marquée et annotée.\n3 c\u0027est-à-dire composé de syntagme(s). Le syntagme est un groupe de termes dont la succession a un sens qui forme une unité fonctionnelle.\nÉtat de l\u0027art\nDifférentes actions de recherche ont été menées autour de la problématique de l\u0027itinéraire, elles ont donné naissance à des plateformes comme nous allons le constater au travers des projets comme Egges et al. (2001), Coyne et Sproat (2001) ou encore Maaßet al. (1993). Certains de ces projets s\u0027intéressent comme nous au lien entre expression textuelle en langue naturelle et concepts géographiques liés aux itinéraires.\nDe nombreux travaux ont d\u0027autre part été effectués autour de la problématique de modé-lisation du concept d\u0027itinéraire du point de vue cognitif. Le modèle des attendus que nous proposerons, bien qu\u0027émanant des attentes pour un usage pédagogique, s\u0027inspire de ces diffé-rents travaux.\nEnfin, dans le domaine du TALN et chez les linguistes, divers travaux vont nous permettre de proposer un modèle d\u0027extraction (c\u0027est-à-dire de repérage pour une interprétation) afin de rendre possible un traitement automatique. Les uns concentrent leurs efforts sur la caractérisa-tion d\u0027expressions dont la granularité est inférieure à la proposition. Les autres s\u0027intéressent à la manière dont l\u0027itinéraire est exprimé dans la langue (orale ou écrite) et enfin d\u0027autres encore focalisent leurs recherches sur les verbes de déplacements. Ces verbes occupent en effet une place prépondérante dans une description d\u0027itinéraire.\n-179 -RNTI-E-13\nDes projets autour de l\u0027itinéraire\nPrès de nos préoccupations, à savoir établir un lien entre le langage écrit et un concept géographique, de nombreux auteurs se sont intéressés à des systèmes permettant de passer automatiquement d\u0027un texte à une scène (text-to-scene conversion). Coyne et Sproat (2001) avec WordsEye en est un exemple : ce système permet de visualiser en trois dimensions des descriptions du type : The cat is on the large chair. A dog is facing the chair. WordsEye utilise une base lexicale de plus de 10000 objets (et leur représentation associée), l\u0027ontologie Wordnet, un analyseur de dépendances et des grammaires de cas pour interpréter les textes soumis. Cependant, WordsEye ne semble pas prendre en compte des textes relatant des histoires réelles et les exemples donnés ne sont que des cas d\u0027école relativement courts dans lesquels le système cherche à positionner correctement les objets les uns par rapport aux autres de manière statique.\nCarsim (Egges et al. (2001)) est un projet qui cherche à offrir une visualisation en 3D de descriptions d\u0027accidents de la route à partir de constats textuels. L\u0027approche est quelque peu semblable à celle que nous proposons : les auteurs cherchent à décrire formellement l\u0027accident à partir d\u0027une analyse linguistique puis, dans un deuxième temps à générer la scène virtuelle associée. Maaßet al. (1993) avec le projet VITRA (VIsual TRAnslator) s\u0027intéresse à la connexion entre langage et perception visuelle. L\u0027élaboration d\u0027un système de représentation des connaissances qui permette un accès en langage naturel aux données visuelles est un objectif de ce projet. VITRA vise plus particulièrement la génération automatique de descriptions d\u0027itinéraires. Nous sommes là en présence de travaux qui adoptent une démarche exactement inverse à la nôtre : à partir d\u0027un modèle de données de l\u0027itinéraire, on cherche à produire une description d\u0027itinéraire. Dans ce projet, l\u0027accent est mis sur l\u0027apport de la bi-modalité dans la description de l\u0027itinéraire (description verbale et description graphique).\nLes spécificités de notre approche par rapport à ces projets sont triples. Tout d\u0027abord dans notre projet nous travaillons sur des documents d\u0027un contenu bien plus volumineux que celui des constats d\u0027accidents de la route ou des descriptions du type the cat is on a large chair. Cela a pour conséquence non seulement un nombre de données spatio-temporelles à manipuler plus important mais également des échelles variables dans les références spatiales. De plus, les formes d\u0027expression de l\u0027espace et du temps sont différentes : si dans les constats d\u0027accidents les objets, leur trajectoire, les chocs sont importants, dans les documents patrimoniaux à composante géographique, les lieux, les époques, et les déplacements le sont plus. D\u0027autre part, nous souhaitons intégrer dans notre approche les usages qui seront faits des informations extraites : dans le cas d\u0027étude que nous proposons, celui de l\u0027extraction des itinéraires, nous souhaitons aller plus loin que la visualisation des informations extraites. L\u0027objectif est en effet à terme de fournir des textes interprétés à des applications à vocation pédagogique.\nDans les années quarante, des théories concernant l\u0027espace et son modèle cognitif font leur apparition. Les travaux de Tolman (1948) sur la notion de carte cognitive chez l\u0027animal et les recherches de Piaget qui s\u0027est intéressé en 1948 au développement de l\u0027apprentissage de l\u0027espace chez les enfants en sont les principaux exemples. B. Kuipers est également à l\u0027origine de travaux largement cités dans ce domaine. Kuipers (1977) propose le modèle TOUR dans lequel il décrit les fonctions de la carte cognitive qui sont d\u0027assimiler les informations concernant l\u0027environnement, de représenter la position actuelle, et de répondre à des questions de localisation et de recherche d\u0027itinéraires. Le modèle TOUR est un modèle computationnel psychologique du sens commun spatial pour la description d\u0027itinéraire. Wunderlich et Reinelt (1982) s\u0027intéressent, dans un dialogue, au processus de la description d\u0027un chemin répondant à la question How to get there from here ? Ils décomposent ce processus en quatre phases : initiation (question initale, confirmation, reconfirmation), description de la route (par l\u0027informateur), confirmation (confirmation et répétition éventuelle) et fermeture (remerciements, etc.). La phase 2 nous intéresse ici plus particulièrement puisqu\u0027il s\u0027agit de décrire un itinéraire. Dans ces travaux, les auteurs décomposent l\u0027itinéraire ainsi : un point de départ, des destinations intermédiaires et un point d\u0027arrivée. Ces destinations intermédiaires sont des landmarks, un landmark a des caractéristiques particulières qui le rendent facilement reconnaissable. Les auteurs introduisent également les extended landmarks. Au contraire des landmarks classiques qui s\u0027apparentent à des points (un monument, un carrefour, etc.), les extended landmarks sont des portions de route que l\u0027on doit suivre (une route, un tunnel, le bord d\u0027un cours d\u0027eau, etc.). Denis (1994) puis Przytula-Machrouh et al. (2004) s\u0027intéressent aux connaissances utilisées par les personnes décrivant un itinéraire, aux modes de représentation de ces connaissances et à leur utilisation. Ces travaux sont à cheval entre le concept et son évocation de manière verbale. Concernant le concept d\u0027itinéraire, les auteurs se placent dans un contexte de description a priori d\u0027un itinéraire en ville, dans le but d\u0027apporter une information, de la même manière que Wunderlich et Reinelt (1982). Dans ces travaux, le concept de scène élémentaire est utilisé comme unité de base de la description d\u0027itinéraire. Une scène élémentaire est fondamentalement constituée de repères et d\u0027actions. Les auteurs se sont également intéressés au moyen de verbaliser ces descriptions d\u0027itinéraire et actent que les repères sont représentés par des noms ou des groupes nominaux alors que les actions sont verbalisées grâce à des verbes de déplacement. Fraczak et Lapalme (1999) se placent également dans le contexte de la description d\u0027itinéraire a priori, ils décrivent la structure conceptuelle de l\u0027itinéraire comme une succession d\u0027entités spatio-temporelles appelées segments et relais. Le segment est un fragment de l\u0027itiné-raire durant lequel une ou plusieurs caractéristique(s) reste(nt) constante(s) tandis qu\u0027un relais marque un changement de caractéristique(s). Ces caractéristiques pouvant être une orientation, une direction, un type de chemin, etc.\nSi la modélisation de l\u0027itinéraire a donné lieu à de nombreux travaux, ces travaux se placent en amont de la réalisation de l\u0027itinéraire. Il s\u0027agit d\u0027une description a priori qui a un objectif bien particulier : conduire celui qui va réaliser l\u0027itinéraire d\u0027un point de départ à un point d\u0027arrivée. Le référentiel de la description est donc celui qui effectue l\u0027itinéraire. Dans nos travaux, l\u0027itinéraire considéré est un itinéraire a posteriori. Le narrateur le décrit une fois qu\u0027il a été réalisé non pas pour qu\u0027un autre le réalise à son tour mais dans un but narratif, pour que la majorité des personnes puisse comprendre son voyage. Cela implique une description parfois moins fine de repères visuels d\u0027orientation et l\u0027utilisation d\u0027un référentiel intrinsèque. Autrement dit un référentiel peu dépendant de l\u0027objet de référence et faisant appel à des relations ternaires impliquant un observateur, l\u0027objet à situer et l\u0027objet de référence. Ce référentiel est le seul qui puisse être à la fois partagé par celui qui a fait l\u0027itinéraire et celui qui le lit. En effet, le lecteur n\u0027est pas physiquement sur les lieux racontés, et, sauf exception, il n\u0027a pas non plus en tête la configuration exacte des lieux traversés et racontés dans le récit.\nD\u0027autre part, nous pensons qu\u0027un modèle conceptuel ne peut s\u0027élaborer de manière purement objective : il adopte forcément un point de vue. Nous verrons (cf section 3.1) que notre point de vue est celui du pédagogue qui souhaite faire intervenir ces descriptions d\u0027itinéraires dans ses activités pédagogiques.\nEnfin, si l\u0027itinéraire global est certes perçu avant les éléments qui le composent, il n\u0027en est pas moins indissociable. En effet, dans un texte, il n\u0027y a pas d\u0027itinéraire sans l\u0027évocation des emplacements successifs et des déplacements du narrateur. C\u0027est le sujet de la section suivante.\nL\u0027itinéraire : son évocation dans la langue par le déplacement\nDans le développement de la méthode que nous proposons pour interpréter les itinéraires, nous nous appuyons sur les travaux de linguistes et du TALN. Ces travaux montrent que le déplacement est primordial dans la compréhension de l\u0027itinéraire, ceci sera confirmé par des observations faites sur un échantillon de notre corpus. Nous donnons ici un aperçu des principaux travaux linguistiques qui ont été menés autour de l\u0027expression du déplacement dans la langue. Boons (1987) constate que les critères pour déterminer les compléments locatifs de la phrase sont insuffisants. Le plus souvent, la question où ? que l\u0027on nous apprend à nous poser à l\u0027école primaire pour trouver le complément locatif ne suffit pas. Boons propose alors de classer les verbes de mouvement selon la phase spatio-temporelle sur laquelle ils sont focalisés. Les verbes de déplacement peuvent donc avoir une polarité aspectuelle initiale (comme pour le verbe sortir), médiane (comme pour le verbe passer) et finale (comme pour le verbe arriver). Laur (1991)  Plus tard, Sablayrolles (1995) reprend ces travaux et introduit le changement d\u0027emplacement. Le troisième critère de Laur montre une large sous-spécification du déplacement. En effet, s\u0027il n\u0027y a effectivement pas de changement de LRV dans la phrase Paul s\u0027approche du mur, il y a tout de même un changement d\u0027état. Paul est effectivement plus près de la cible mur au temps t+1 qu\u0027au temps t. Les lieux sont donc insuffisants pour décrire le déplacement dans la langue. C\u0027est pour palier à ce problème que Sablayrolles introduit les emplacements, en plus des lieux. A la différence d\u0027un lieu, un emplacement est une portion de surface, sans aucune fonctionalité ni élément lexical associé. Il est uniquement défini géométriquement par l\u0027enveloppe pragmatique associée à l\u0027entité concernée. De notre point de vue, l\u0027approche de Sablayrolles nous semble être relativement semblable à celle de Laur, si ce n\u0027est qu\u0027il introduit un grain plus fin que le lieu : l\u0027emplacement. Sarda (2000) s\u0027est penchée sur le cas particulier des verbes de déplacement transitifs directs et en propose une typologie. Elle cherche à raffiner la catégorie des verbes médians qui selon elle est plutôt définie par défaut, c\u0027est-à-dire rassemblant les verbes qui ne sont ni initiaux, ni finaux. Sarda propose de catégoriser les verbes en fonction de la nature des relations de localisation qu\u0027ils impliquent. Elle distingue en premier lieu les verbes relationnels (qui dénotent un déplacement quel que soit l\u0027objet) des verbes référentiels (qui dénotent un déplacement seulement lorsque l\u0027objet est un lieu). Les verbes relationnels sont catégorisés selon des relations de distance (approcher, fuir, etc.), d\u0027orientation (monter, descendre, etc.) ou de passage (traverser, sauter, etc.) et les verbes référentiels sont soit neutres initiaux (quitter, déserter, etc.) soit neutres finaux (atteindre, regagner, etc.) soit de contact (heurter, taper, etc.). Une classe de verbes moins clairement définie subsiste : celle des verbes médians (arpenter, sillonner, parcourir, etc.) qui sont référentiels par rapport à un domaine topologique (un intérieur par exemple).\nLa représentation de la sémantique des verbes de déplacement a donné lieu a de très nombreux travaux, notamment chez les linguistes et dans la communauté TALN. Elle pose encore de nombreux problèmes et est sans cesse remise en question comme dans la thèse de Mathet (2000). Cependant, le type de documents sur lesquels nous travaillons (des récits de voyage) ainsi que les objectifs que nous souhaitons atteindre quant à la finesse de l\u0027interprétation du déplacement dans la langue nous permettent de simplifier le problème. Le sous-ensemble des verbes de déplacement auxquels nous nous intéressons ici sont ceux qui entrent dans une construction verbe,préposition (facultative),entité_spatiale, que nous noterons triplet (V,P ?,E). Cette construction lève une grande partie des problèmes d\u0027ambiguïté que l\u0027on peut trouver dans des propositions comme quitter son mari, traverser une mauvaise période, etc.. De plus, nous souhaitons simplement inférer qu\u0027à un moment donné, le sujet est localisé sur l\u0027objet entité spatiale. Les préoccupations plus fines sont pour le moment écartées de nos attendus. En effet, dans un but de traitement automatique de corpus volumineux à des fins d\u0027interprétation pour le domaine pédagogique, une première approximation du déplacement par la polarité aspectuelle des verbes de déplacement nous semble suffisante. Pour prendre un exemple, dans la compré-hension globale de l\u0027itinéraire décrit dans un récit, que le narrateur s\u0027éloigne de Pau ou quitte Pau est relativement semblable.\nModélisation du concept d\u0027itinéraire et son expression dans des récits de voyage : deux modèles computationnels\nLes enseignants proposent de nombreuses activités pédagogiques à partir de documents à contenu géographique. Les origines de ces documents peuvent être multiples : des extraits de publications universitaires, des sélections obtenues dans des publications plus grand public (par exemple des articles de quotidiens, d\u0027hebdomadaires, de guides touristiques, . . .) ou encore des passages choisis dans des oeuvres littéraires. Pour l\u0027ensemble de ces « documents sources » se posent des problèmes pédagogiques importants : l\u0027adéquation du matériau aux objectifs d\u0027apprentissage visés. En effet, souvent l\u0027exploitation directe du contenu de ce type de document reste une tâche difficile pour les élèves (du secondaire en particulier). Il s\u0027ensuit que le péda-gogue, auteur de manuel ou professeur, adapte le document afin de le rendre plus accessible aux élèves. Il s\u0027agit très fréquemment d\u0027une adaptation dans le sens de la simplification, d\u0027où la mention fréquente dans les manuels : « d\u0027après tel auteur ».\nPour les travaux présentés ici, nous avons retenu comme activité cible : la mobilisation chez l\u0027élève des repères spatio-chronologiques. Elle est une composante incontournable de plusieurs objectifs pédagogiques. Étant données les diverses formes qu\u0027une telle activité peut revêtir et afin de modéliser un processus de complexité maîtrisable, nous nous sommes restreints à l\u0027explicitation de trois tâches :\n1. localisation des principaux lieux constitutifs d\u0027un itinéraire, 2. interprétation des déplacements plausibles entre les différents lieux empruntés ou traversés au sein l\u0027itinéraire, 3. cartographie du parcours interprété. Ces tâches devront être appliquées à une sous catégorie de « documents sources » des documents de type oeuvre littéraire et plus spécifiquement du genre littéraire récit de voyage.\nDans la démarche du pédagogue de mise en adéquation de la source documentaire aux finalités pédagogiques, il lui est nécessaire de passer par trois grandes étapes :\n1. une étape de recherche d\u0027information et d\u0027extraction de passages qui peut lui retourner plusieurs documents, 2. une étape de comparaison du matériau retourné en étape 1 qui lui fait choisir un document, 3. enfin une étape de reconstruction du « document source pédagogique », à partir du document choisi en étape 2. Le corpus de « documents sources » sur lequel nous travaillons est constitué de monographies datant d\u0027une même époque (fin du XIXème siècle) et évoquant des récits de voyage réalisés sur un même territoire : les Pyrénées.\nRemarquons que les textes du corpus sont spécifiques par rapport aux nombreux travaux du domaine de la description d\u0027itinéraire car leur but n\u0027est pas de décrire uniquement un itinéraire. L\u0027objectif premier des auteurs de ces documents est de relater l\u0027expérience personnelle de l\u0027auteur. Néanmoins, on peut observer que le genre descriptif est d\u0027avantage utilisé que le genre narratif et que la description de l\u0027itinéraire y occupe une place prépondérante. On peut constater, d\u0027autre part, qu\u0027il s\u0027agit dans notre cas d\u0027une description a posteriori alors que la plupart des travaux existants qui traitent de la description d\u0027itinéraire se placent a priori. Le récit de voyage ne contient donc pas des énoncés tels que ceux qui apparaissent dans les descriptions a priori comme continuer tout droit, tourner à droite, etc.. On peut remarquer également une certaine régularité dans l\u0027évocation du territoire et des déplacements au sein de l\u0027itinéraire, comme évoqué par Boons (1987), Laur (1991), Mathet (2000). Il est le plus souvent évoqué par des verbes de déplacement (j\u0027ai quitté Bordeaux à 8h00, je suis arrivé à Pau à 11h, j\u0027ai gravi le Pic du Midi d\u0027Ossau le lendemain, etc.), et ce quel que soit l\u0027auteur. Enfin, le récit de voyage obéit à des règles particulières concernant la chronologie des évènements relatés : il y a dans ce genre de document une synchronisation quasi parfaite entre la chronologie du texte et la chronologie du voyage.\nCompte tenu de l\u0027état actuel de la formalisation des connaissances dans le domaine de la compréhension automatique de texte concernant les concepts spatiaux, temporels et spatiotemporels, il n\u0027est pas encore possible de concevoir un système permettant d\u0027assurer de manière autonome les trois grandes étapes de préparation d\u0027un tel matériau pédagogique. Par contre, comme la suite de ce papier tente de le montrer, il est possible de concevoir un outil accompagnant le pédagogue dans ces tâches.\nDans un premier temps, nous présenterons une formalisation de l\u0027information attendue, puis après avoir discuté les résultats d\u0027une étude réalisée sur un échantillon du corpus, nous détaillerons le modèle d\u0027extraction. Sera enfin relatée la phase d\u0027interprétation, autrement dit la transformation de l\u0027information exprimée dans le modèle d\u0027extraction pour sa ré-expression dans le modèle des attendus.\nLe modèle des attendus\nDans le cas particulier de l\u0027exemple de récit de voyage donné en figure 1, les éléments constituant une description d\u0027itinéraire sont :\n1. des lieux et des « temps calendaires » 4 , 2. des déplacements, 3. des faits (activités ou situations) qui modifient la dynamique par défaut du déplacement.\nLes lieux sont mentionnés lorsque l\u0027auteur les a simplement traversés ou qu\u0027il y situe des faits marquants. Tous les lieux traversés lors d\u0027un itinéraire ne sont pas relatés ce qui nous rappelle les propriétés de saillance de Przytula-Machrouh et al. (2004); Denis (1994) ;Wunderlich et Reinelt (1982).\nD\u0027autre part, seuls les lieux associés à une description de faits (arrêt pour visiter, pour manger, pour changer de moyen de transport, etc.) peuvent éventuellement donner naissance à un itinéraire d\u0027échelle inférieure. Nous les apparentons à la notion d\u0027étape intermédiaire de Fraczak et Lapalme (1999). Nous proposons également de reprendre les notions de relais et de segments (figure 2) telles que décrites par Fraczak et Lapalme (1999) Pour donner une représentation semi-formelle du modèle obtenu, nous avons utilisé le langage MADS de Parent et al. (2006). MADS est un modèle de données qui permet d\u0027ajouter des 5 j u i l l e t . Dimanche . J e me s u i s b a i g n é . J e s u i s p a r t i à 11 h1 / 4 a v e c un g r o u p e de g e n s p o u r R o c h e f o r t d a n s une p i n a s s e . Vent c o n t r a i r e . En l o u v o y a n t , en r a m a n t e t a v e c l \u0027 a i d e de l a marée , n o u s a r r i v â m e s à R o c h e f o r t v e r s t r o i s h e u r e s ; c e p e n d a n t l a j o u r n é e f u t b e l l e e t l a c o m p a g n i e p l u t ô t a g r é a b l e . J \u0027 a i vu t o u t e s l e s p a r t i e s du p h a r e ; l a b a s e f u t c o n s t r u i t e au t e m p s de L o u i s XIV , l a p a r t i e s u p é r i e u r e en 1 7 8 9 . Le d i s p o s i t i f l e n t i c u l a i r e a c t u e l s e r t d e p u i s d o u z e a n s . Le g a r d i e n s e p l a i n t s e u l e m e n t de l a lampe q u i l u i c a u s e d e s e n n u i s . Nous a v o n s q u i t t é R o c h e f o r t v e r s 5 h e u r e s . P r e s q u e c a l m e ; n o u s a v o n s mis deux h e u r e s p o u r a r r i v e r à Royan . J \u0027 y a i p a s s é l a n u i t .\n6 j u i l l e t . J e s u i s p a r t i p a r l e b a t e a u à v a p e u r à 6 h e u r e s du m a t i n e t j \u0027 a i a t t e i n t B o r d e a u x à m i d i a p r è s une b r è v e t r a v e r s é e . Après?m i d i p l u v i e u s e . J \u0027 a i d î n é a v e c M. G u e s t i e r l . J \u0027 a i é c r i t à E l i s a .\n7 j u i l l e t . J \u0027 a i q u i t t é B o r d e a u x à 7 h e u r e s en d i l i g e n c e p o u r Pau . J \u0027 a i é t é a g r é a b l e m e n t s u r p r i s p a r l a b e a u t é de l a campagne [ . . . ] Nous a v o n s t r a v e r s é Langon .\nFIG. 1 -Extrait d\u0027un récit de voyage.\ntypes de données propres aux données spatio-temporelles mais aussi d\u0027ajouter des relations spécifiques entre ces données. MADS se veut également un modèle de données résolument tourné vers une modélisation des concepts lors de la phase de modélisation. Les concepteurs doivent en effet s\u0027abstraire totalement des contraintes d\u0027implémentation pour être le plus près possible du monde réel et de ses représentations. Cela n\u0027était pas le cas dans la gestion des données géographiques où les concepteurs ont longtemps été influencés par des préoccupa-tions d\u0027implémentation internes aux outils qui gèrent ces données. Avant de décrire un à un les objets qui composent le modèle que nous proposons, il est bon de préciser que les quatre principaux objets sont tous des Entités Géographiques (EG). De manière abstraite une EG est définie par trois composantes : une spatiale, une temporelle et une thématique ou phénomène. Cette définition explicitée dans Usery (2003)  Le segment (ou fragment d\u0027itinéraire) : il est le chemin qui relie deux relais consé-cutifs. Il est du type spatial MADS OrientedLine ( ) et du type temporel MADS Interval ( ). Il est important de noter que ce chemin n\u0027est qu\u0027un chemin virtuel, c\u0027est-à-dire une des représentations possibles de l\u0027espace. Dans la carte cognitive -définie par Kuipers (1977) que se construit le lecteur, le chemin qui relie deux relais n\u0027est pas forcément celui qui a été véritablement emprunté par l\u0027acteur de l\u0027itinéraire. Ce chemin virtuel approche cependant le chemin véritable de manière plus ou moins juste selon les indications dont dispose le lecteur. Ces indications peuvent être aussi nombreuses que variées : la modalité du transport, la vitesse de parcours, la topologie du terrain qui sépare les deux relais, etc. Ces indices, qui aident le lecteur à approcher le chemin véritablement parcouru par l\u0027acteur, peuvent cependant être classés en deux catégories. D\u0027une part, les précisions données par l\u0027acteur de l\u0027itinéraire dans son récit (modalité du transport, vitesse de parcours, etc.), d\u0027autre part, des connaissances dont dispose le lecteur sur la région traversée (topologie du terrain, difficulté de parcours, existence de telle voie de communication, etc.).\nTout segment est associé à un relais de départ et àun relais d\u0027arrivée par les associations depart et arrivee. Elles sont du type MADS TopoTouch (( )), c\u0027est-à-dire que le segment est adjacent du point de vue topologique aux relais (de départ comme d\u0027arrivée). L\u0027activité : l\u0027activité correspond aux occupations du narrateur lors de ses étapes (visites, repas, etc.).\nNous avons ajouté au modèle des attendus une contrainte OCL qui spécifie que dans le cas où un itinéraire I ? naît à une étape de départ (respectivement d\u0027arrivée), cet itinéraire I ? est différent de l\u0027itinéraire I pour lequel cette étape joue le rôle d\u0027étape de départ (respectivement d\u0027arrivée). Ceci pourrait être le cas dans des configurations telles que celles de la figure 3.\nFIG. 3 -Des exemples de configurations où plusieurs itinéraires émergent.\nLe modèle des attendus que nous proposons se veut compact de manière à pouvoir être manipulé dans une chaîne de traitement informatique permettant de reconstruire un itinéraire à partir de son évocation dans un texte. Le second modèle, le modèle d\u0027entrée ou d\u0027extraction, a pour but de modéliser la manière dont les auteurs expriment leur itinéraire dans leur récit de voyage. Pour sa mise au point, nous avons observé sur un échantillon du corpus, les différentes formes des expressions supportant l\u0027information sur les itinéraires et leur organisation dans le discours.\nUn modèle d\u0027extraction\nObservations sur un échantillon du corpus\nParmi les documents mis à notre disposition par la médiathèque, trois ont été retenus 5 afin d\u0027y entreprendre plusieurs études pour valider plus finement les premières observations (description a posteriori, régularité dans l\u0027évocation du territoire et des déplacements au sein de l\u0027itinéraire, synchronisation entre la description et le déroulement du voyage).\nCes trois textes racontent le voyage d\u0027un explorateur qui part d\u0027une grande ville (Bordeaux, Paris) à la découverte des Pyrénées sur plusieurs jours.\nÉtude 1, importance des Entités Spatiales : cette première étude consiste à relever les propositions qui évoquent la position (PP) de l\u0027auteur dans son voyage et à comptabiliser combien parmi elles utilisent des Entités Spatiales (ES). Ces PP qui utilisent des ES seront appelées -188 -RNTI-E-13 P. Loustau et al.\nPPES. Le concept d\u0027ES a fait l\u0027objet de travaux antérieurs\n6 , succinctement, une ES est une zone géolocalisable. On définit une ES par rapport à un point d\u0027ancrage dans l\u0027espace : l\u0027Entité Géographique Nommée (EGN). L\u0027EGN est un objet dont on peut obtenir la géolocalisation à l\u0027aide d\u0027une ressource, grâce à son nom. Les résultats sont donnés dans la figure 4. L\u0027emploi des ES pour évoquer le déplacement y apparaît assez nettement. On atteint des taux de l\u0027ordre de 80% selon les auteurs, mais aussi selon la nature du voyage qui est relaté. En effet, le déficit d\u0027emploi d\u0027ES dans l\u0027évocation du déplacement apparaît le plus souvent lorsque l\u0027auteur évoque des déplacements de plus petites tailles (je suis allé au parc, j\u0027ai fait une promenade sur le port, j\u0027ai quitté l\u0027hôtel, etc.), le plus souvent lorsqu\u0027il est à une étape intermédiaire. C\u0027est le cas pour le texte de Ann Lister, dans lequel l\u0027auteur évoque largement ses occupations lors des étapes le long de son voyage.\nÉtude 2, importance des formes verbales : cette seconde étude a pour but d\u0027évaluer le poids des verbes. Elle consiste à comptabiliser dans les propositions (PP) qui évoquent le déplacement le nombre de propositions qui utilisent des formes verbales à cette fin (PPV). Avec une moyenne de 87,7% (les résultats par document sont donnés en figure 4) cette étude montre leur prédominance.\nSi ces deux études doivent encore être approfondies en augmentant le nombre de récits étudiés, cela donne d\u0027ores et déjà une bonne idée du poids qu\u0027occupent les ES et les verbes de déplacement dans notre corpus. Ces observations corroborent également les travaux autour de l\u0027évocation du déplacement dans la langue tels que ceux de Laur (1991); Sarda (1992); Muller et Sarda (1999 \nFIG. 4 -Études 1 et 2 : le poids des propositions utilisant les ES (PPES) et des verbes (PPV) dans les propositions évoquant la position (PP)\nÉtude 3, synchronisation entre le récit et le voyage : cette étude a pour but de montrer le parallèle qui existe entre la chronologie du voyage et celle du texte le décrivant. Nous avons relevé les dates et heures qui peuvent être attachées aux déplacements mentionnés dans les textes utilisés dans les études précédentes.\nLes résultats sont donnés en figure 5. Ils montrent clairement la synchronisation entre le voyage et le récit qui en est fait. Les rares exceptions qui font que des déplacements ne sont pas évoqués dans l\u0027ordre chronologique sont issus d\u0027exemples du type : avant d\u0027arriver à ES x , j\u0027ai traversé ES y . 6 Dans nos précédents travaux (Lesbeguerries et Loustau (2006) \nLe modèle\nLes études réalisées sur le corpus documentaire ont montré l\u0027importance des ES et des verbes de déplacement dans l\u0027évocation du déplacement des acteurs lors du récit de leur voyage (cf figure 4). Ces évocations de déplacements permettent au lecteur de se construire une repré-sentation mentale de l\u0027itinéraire parcouru au sein de sa carte cognitive, concept introduit par Kuipers (1977). Nous présentons dans ce paragraphe la manière dont les déplacements apparaissent dans la langue.\nFIG. 6 -Un modèle pour l\u0027extraction des déplacements.\nNous reprenons ici principalement le critère de polarité aspectuelle des verbes introduits par Boons (1987) et repris par Laur (1991). Nous modélisons donc les déplacements dans la langue en prenant en compte les verbes de déplacement obligatoirement associés à un acteur, des ES, et une ET (entité temporelle). De cette manière, nous résolvons en partie le difficile problème de la polysémie de certains verbes (cf. section 2.3 pour quelques exemples) qui renferment ou non, selon le contexte, un sens spatial.\n-190 -RNTI-E-13 Ainsi, conformément à la notion de polarité aspectuelle, les déplacements extraits seront initiaux (quitter), médians (traverser) ou finaux (arriver).\nLes notions d\u0027origine, de destination et de position intermédiaire interviennent également. Les constructions verbales du déplacement font en effet émerger une (quitter Pau), deux (quitter Pau pour Bordeaux) ou trois (quitter Pau pour Bordeaux par la RN 134) de ces propriétés. De même, les notions temporelles qui permettent de situer le déplacement dans le temps jouent également un rôle important. Cependant, le lien entre le verbe de déplacement et l\u0027entité temporelle est moins fort syntaxiquement parlant que celui entre le verbe et la ou les entité(s) spatiale(s), notamment dans les textes que nous considérons.\nPour résumer (cf figure 6), un verbe de déplacement est spécialisé en verbe initial, médian ou final. Dans la langue, il est associé à un Acteur et à une Entite_Spatiale au moins (qu\u0027elle soit d\u0027origine, intermédiaire ou de destination). Il est également associé à une Entite_Temporelle, ce qui permet d\u0027horodater le déplacement.\nLa phase d\u0027interprétation : transformation du modèle d\u0027extraction vers le modèle des attendus\nNous donnons ici, de manière abstraite, le parallèle qui peut être fait entre le modèle d\u0027extraction et le modèle d\u0027interprétation. Nous reviendrons par la suite sur la méthode, les outils et les ressources nécessaires au passage de l\u0027un à l\u0027autre. Attardons-nous donc exclusivement sur la manière dont les principaux objets du modèle des attendus naissent à partir du modèle d\u0027extraction. Naissance du relais : un relais naît lorsqu\u0027un déplacement est évoqué par un verbe de polarité aspectuelle donnée associé à une entité spatiale compatible avec cette polarité (entité origine compatible avec verbe initial, entité indermédiaire avec verbe médian, entité destination avec verbe final).\nNaissance du segment : un segment naît lorsque deux relais consécutifs sont identifiés (cela pré-suppose que les relais ont été ordonnés dans le temps selon leur propriété temporelle).\nNaissance d\u0027une étape : par défaut, le premier et le dernier relais deviennent étapes. D\u0027autres relais peuvent devenir étapes : lorsqu\u0027une activité peut être identifiée et rattachée à un relais (qui devient étape de ce fait).\nNaissance d\u0027une activité : les activités peuvent être variées (nuit dans un hôtel, repas, changement de modalité, etc.). Nous verrons plus loin comment procéder afin de détecter de manière automatique ces activités grâce à la notion de rupture.\nNaissance d\u0027un itinéraire : la naissance de l\u0027itinéraire est directement liée à la naissance des étapes. Dès que deux étapes sont identifiées, un itinéraire est identifié.\nNous donnons dans la figure 7 un exemple de ce passage dans un cas simple : celui dans lequel un seul itinéraire est décrit dans un document. Les exemples d\u0027instances du modèle d\u0027extraction (du haut vers le bas) sont les résultats de l\u0027interprétation des déplacements de l\u0027extrait de document (Le journal de James David Forbes) donné en figure 1 :\n-  Lesbegueries et al. (2006). Extraction des déplacements : nous proposons ici de rendre opératoire la modélisation des déplacements donnée en section 3.2.2 grâce à des transducteurs. Nous rappelons qu\u0027un transducteur est un dispositif qui transforme un langage donné en un autre. Les transducteurs sont basés sur des machines à états finis mettant en correspondance deux langages réguliers. Compte tenu des observations faites sur notre corpus documentaire, la construction des verbes de déplacements peut être apparentée à un langage régulier, modélisé par des machines à états finis, tel que présenté dans la figure 8. Cette modélisation du déplacement sous forme de transducteurs est générique aux documents du type récits de voyage et aux documents dans lesquels le déplacement est exprimé principalement sous forme verbale. Les transducteurs sont traduits en règles de grammaire dans lesquelles nous retrouvons les principaux objets du modèle : le verbe, la préposition et l\u0027ES. Cette analyse à base de règles s\u0027appuie sur les résultats d\u0027analyses plus en amont. La première est une analyse morphosyntaxique capable de retourner deux étiquettes concernant la forme (@tag et @stag dans la figure 8) et le lemme 8 de chaque unité lexicale (@lemme dans la figure 8). Elle permet de s\u0027abstraire des formes fléchies des mots, notamment celles des verbes conjugués. La deuxième analyse est l\u0027extraction des ES (@sem dans la figure 8).\nPrenons par exemple l\u0027interprétation du déplacement précédemment étudié : nous arrivâmes à Rochefort. Nous considérons à ce stade que des analyses en amont ont déjà été réa-lisées. De ce fait, nous avons accès à diverses informations pour chaque mot de cette phrase. Ces informations sont les suivantes.\n-nous : @texte\u003dnous / @lemme\u003dnous / @tag\u003dpro / @stag\u003dnull / @sem\u003dnull -arrivâmes : @texte\u003darrivâmes / @lemme\u003darriver / @tag\u003dver / @stag\u003dppa / @sem\u003dnull -à : @texte\u003dsuis / @lemme\u003dêtre / @tag\u003dpre / @stag\u003dnull / @sem\u003dnull -Rochefort : @texte\u003dRochefort / @lemme\u003dRochefort / @tag\u003dnom / @stag\u003dprp / @sem\u003des Nous pouvons alors analyser la phrase à l\u0027aide du transducteur de la figure 8 comme ceci : -en début d\u0027analyse, le transducteur est en état 0 et le pointeur d\u0027analyse positionné sur le mot nous. -il n\u0027y a pas de transition par nous (ou une de ses propriétés), on avance donc le pointeur sur le deuxième mot arrivâmes, le transducteur reste en état 0. -il existe une transition par arrivâmes (concernant son lemme, @lemme\u003darriver), le transducteur passe en état 7 et nous avons détecté un déplacement final. Le pointeur est avancé sur le mot à. -il existe une transition par à (concernant son texte, @texte\u003dà), le transducteur passe en état 8. Le pointeur est avancé sur le mot Rochefort.\nFIG. 8 -Extrait simplifié de la base des transducteurs spatiaux. Le symbole % représente le « joker ». Les transitions en pointillé montrent que d\u0027autres états existent.\n-il existe une transition par Rochefort (concernant sa propriété sem, @sem\u003des), le transducteur passe en état 9 et nous avons détecté un déplacement final qui a pour destination Rochefort. Le pointeur est en fin de phrase et le transducteur sur un état final, l\u0027analyse est validée : nous avons reconnu un déplacement de polarité aspectuelle finale ayant pour destination Rochefort. Dans la section suivante nous chercherons à répondre à cette question : comment s\u0027élever au niveau du discours à partir de ces éléments extraits localement ?\nDes déplacements à l\u0027itinéraire\nTout comme l\u0027humain doit avoir une connaissance géographique du monde pour pouvoir raisonner sur un itinéraire Kuipers (1977), un système doit également avoir des capacités équi-valentes. Les principales ressources envisagées sont des ressources de type géographique et elles peuvent être classées en deux catégories : les données et les raisonnements.\nDonnées factuelles : ce sont des données brutes, que l\u0027on pourrait qualifier de données universelles. Elles s\u0027apparentent aux connaissances du monde que peut avoir l\u0027humain. Elles représentent des faits, au sens logique du terme, c\u0027est-à-dire qu\u0027elles sont considérées comme vraies. Les couches de données SIG 9 (comme celles des communes de France, du réseau rou-\nFIG. 9 -Quitter Saint-Jean-de-Luz : les zones probables selon la modalité du transport. (1) en bateau,(2) en voiture\ntier, ou du réseau fluvial), les Gazeteers 10 , les bases toponymiques, sont des données factuelles envisageables.\nRaisonnements : ils sont de deux types. Les plus génériques, s\u0027effectuent directement sur les données factuelles et peuvent être ainsi qualifiés de raisonnement bas niveau. Ces raisonnements sont très souvent offerts par les outils traitant l\u0027Information Géographique, ils permettent notamment d\u0027utiliser des opérateurs et des fonctions afin de déduire de nouvelles informations à partir des données factuelles. Prenons un exemple simple et considérons que dans nos données factuelles nous ayons les coordonnées géographiques des villes de Pau et Bordeaux. Une fonction élémentaire d\u0027un SIG nous permettra de connaître la distance entre Pau et Bordeaux ou de construire la ligne correspondant au segment reliant Pau et Bordeaux. Nous considérons ces fonctions comme prédéfinies dans notre système. Des raisonnements plus spécifiques sur ces données tentent d\u0027apporter au système des règles de bon sens ou règles de sens commun dans le domaine des itinéraires.\n-Règles situant l\u0027acteur par rapport à l\u0027entité et mettant en relation la polarité aspectuelle du verbe avec les ES d\u0027origine, intermédiaires et de destination. Un déplacement de polarité aspectuelle initiale P i ayant pour origine ES o situe l\u0027acteur du déplacement en ES o . Un déplacement de polarité aspectuelle médiane P m ayant pour entité intermé-diaire ES i situe l\u0027acteur en ES i . Un déplacement de polarité aspectuelle finale P f ayant pour destination ES d situe l\u0027acteur en ES d . -Règles concernant l\u0027étendue des zones probables de localisation de l\u0027acteur en fonction de la polarité aspectuelle des verbes et de la modalité de déplacement. Un déplacement évoqué avec un verbe initial dont l\u0027origine est l\u0027ES ES o situe l\u0027acteur dans une région limitrophe de la frontière de l\u0027entité ES o , la largeur de cette zone est différente selon la modalité de déplacement : elle sera plus large sur un déplacement en voiture que sur un déplacement à pied et cela est principalement dû à la vitesse du déplacement. -Règles concernant les modalités de transport et leur localisation probable (ex : voiture sur route, bateau sur mer/océan/fleuve, vélo sur route mais pas autoroute, etc.).\nCes deux dernières règles concernant les modalités du transport sont illustrées sur la figure 9. L\u0027utilisation de fonctions comme Boundary, Intersection et Buffer proposées dans les spé-cifications Open GIS 11 et implémentées dans la plupart des SIG permettent de construire ces zones. Par exemple, on obtient les zones hachurées de la figure 9.2 en appliquant une extension (Buffer) d\u0027un facteur ?, dépendant de la vitesse de déplacement sur l\u0027intersection (Intersection) entre la frontière du polygone de St-Jean-De-Luz (Boundary(Geom stjean )) et les géométries des routes du département des Pyrénées Atlantiques (Geom routes64 ). Une dernière intersection du résultat obtenu avec les routes donne les zones hachurées. Ceci se résume par une requête de la forme :\nEnfin des règles de détection de ruptures permettent de différencier les étapes des relais, avec comme ruptures possibles (notons que c\u0027est la combinaison de ces ruptures qui permet de déceler une activité) :\n-rupture dans la modalité : détection simple dès lors que les modalités peuvent être repé-rées ; -rupture dans l\u0027amplitude dans le déplacement et/ou le temps (ie changement d\u0027échelle) : détection possible par inclusion spatiale et/ou temporelle ; -rupture dans la structure logique : dépend de la qualité de la phase ROC 12 (détection des changements de paragraphe possible avec une ROC basique, détection de titre de chapitre, de section voire plus avec des ROC de meilleure qualité) ; -rupture dans la linéarité spatiale de l\u0027itinéraire : apparition de boucles ; -rupture dans la continuité spatiale de l\u0027itinéraire : apparition de sauts spatiaux ; -rupture dans la continuité temporelle de l\u0027itinéraire : apparition de sauts temporels.\n?R : un Prototype pour l\u0027Interprétation d\u0027Itinéraires dans des Récits\nLa démarche générale consiste à construire une chaîne de traitement linguistico-géographique (figure 10), capable d\u0027extraire les déplacements de manière locale au niveau phrastique dans les textes puis de reconstruire l\u0027itinéraire en utilisant des ressources comme nous le décrivions dans la section précédente. Cette chaîne de traitement utilise le langage XML qui permet de facilement enchaîner différents traitements en ajoutant à l\u0027information extraite dans une phase n l\u0027information extraite à une phase n+1. Nous décrivons ici les outils utilisés par chaque phase du traitement.\nExtraction des déplacements : correspond à la partie supérieure de la figure 10. Comme précédement évoqué, l\u0027extraction des ES est effectuée par le prototype PIV de Lesbegueries et al. (2006). L\u0027extraction des déplacements est une chaîne de traitement linguistique à part entière. Elle est constituée des grandes phases que nous décrivions dans la section précédente et a été implé-mentée grâce à la plate-forme de traitement linguistique  (2005). Au sein de celle-ci, l\u0027analyse morpho-syntaxique est effectuée par Tree-Tagger, l\u0027analyseur morphosyntaxique éprouvé de Schmid (1994). Les transducteurs des verbes de déplace-ments sont traduits en grammaires DCG 13 et l\u0027analyse basée sur ces grammaires est confiée à Prolog afin de profiter des mécanismes de déduction et d\u0027unification de ce langage.\nOn obtient ainsi la première partie de notre chaîne de traitement, celle qui est capable d\u0027extraire les déplacements des textes (cf figure 10).\nReconstruction de l\u0027itinéraire : ce module nous permet de passer des déplacements extraits à un itinéraire. Il prend en entrée un fichier XML dans lequel les déplacements sont représentés selon les critères que nous avons évoqués dans la section 3.2.2. Il utilise le SIG PostGIS 14 afin de mettre en application les règles de raisonnement spatial précédemment évo-quées. Diverses ressources sont également nécessaires afin de reconstruire l\u0027itinéraire. Dans ?R, nous utilisons différentes couches de données pour la géolocalisation des relais (communes de France, toponymes, etc.) et des segments (réseau autoroutier, routier, sentier, etc.). Ces couches de données sont également rendues disponibles dans PostGIS.\nPour chaque déplacement extrait, on cherche tout d\u0027abord à produire une zone probable de localisation de l\u0027acteur. Pour cela nous faisons appel aux règles concernant la polarité aspectuelle et la modalité du transport. Les fonctions boundary, buffer, intersection du SIG PostGIS ont été utilisées à ces fins. Considérons le déplacement de l\u0027exemple : « Nous avons quitté Bordeaux pour Langon en voiture. ». Celui-ci a pour origine Bordeaux et pour destination Langon. Il fait donc émerger un segment reliant les deux relais que sont Bordeaux et Langon. La modalité du déplacement étant voiture, on va faire appel à un algorithme de calcul de trajet dans le réseau routier pour construire la route probablement empruntée par l\u0027acteur du déplacement. Cette fonctionnalité est apportée par le module pgRouting 15 . Rappelons qu\u0027il ne s\u0027agit pas forcément de la réalité ; l\u0027acteur a peut-être pris un autre chemin. Par cette méthode, on cherche à 13 DCG : Definite Clause grammar 14 PostGIS : extension GIS du système de gestion de base de donnée libre PostgreSQL 15 pgRouting est un module qui donne au SIG PostGIS des fonctions de calcul de plus court chemin (algorithme de Dijkstra, A-étoile, etc.) FIG. 11 -Sortie simplifiée du prototype ?R : un fichier XML contenant l\u0027interprétation de l\u0027itinéraire. Visualisation effectuée avec MapServer approcher la représentation mentale de l\u0027itinéraire que pourrait se faire le lecteur dans sa carte cognitive.\nUne fois que chaque segment de l\u0027itinéraire est ainsi construit, on stocke les segments dans le SIG PostGIS.\nLes détections de ruptures qui donnent naissance à des activités ne sont pas encore implé-mentées. De ce fait, nous appliquons la règle par défaut pour la détection des étapes de départ et d\u0027arrivée : le premier relais devient étape de départ, le dernier relais devient étape d\u0027arrivée.\nEn sortie du prototype ?R, nous obtenons donc une instance du modèle des attendus au format XML. Ce fichier XML contient l\u0027ensemble des objets qui permettent de décrire un itinéraire : des segments, des relais et des étapes. Tous ces objets sont des EG, ils contiennent donc un géocodage au format GML, et une marque temporelle. Ces interprétations d\u0027itinéraires peuvent être alors visualisées par des outils de cartographie tels que MapServer 16 comme montré dans la figure 11. Ils peuvent également intervenir dans la phase de conception d\u0027activités pédagogiques : c\u0027est ce que nous montrons dans la section suivante par des exemples de requêtes multi-niveaux de l\u0027information géographique.\nFIG. 12 -Prise en compte de la complexité croissante des informations géographiques sur lesquelles portent les requêtes.\n-199 -RNTI-E-13 figure 12 présente quatre exemples de requêtes de complexité croissante ; la première colonne montre un exemple de texte brut pour lequel l\u0027information ciblée est soulignée. La deuxième colonne décrit la structure sémantique de l\u0027information ciblée alors que la troisième colonne propose une projection de cette information afin de pouvoir la traiter avec un SIG par exemple.\nDe nombreux SIG permettent de résoudre la requête 0 (et même des systèmes d\u0027information classiques se basant sur des appariements de type fulltext). Les autres requêtes sont beaucoup plus spécifiques à la problématique étudiée dans cet article puisqu\u0027elles prennent en compte les entités géographiques associées aux verbes de déplacement qui apparaissent dans le texte (si nous avions proposé des modèles computationnels prenant en compte les descriptions de lieux associées à des verbes de perception, le système aurait également dû s\u0027appuyer sur les requêtes de niveau 0).\nPour chacune des requêtes 0 à 3, nous présentons dans les paragraphes qui suivent les traitements effectués (niveau de granularité des informations recherchées) et nous indiquons quels modèles computationnels sont les plus appropriés pour répondre à ces requêtes.\nRequête 0 : Trouver les documents qui parlent de Pau et de Nay Ici, il s\u0027agit de traiter une information géographique de base, donc de faire une simple comparaison entre les entités géographiques nommées (EG) du corpus documentaire et de la requête. Traitements sur le corpus : extraction des EG, géocodage des EG, indexation des EG. Traitements sur la requête : si elle est exprimée en langage naturel, des traitements similaires à ceux effectués sur le corpus sont nécessaires c\u0027est-à-dire extraction des EG, géocodage des EG, indexation des EG. Si la requête est formulée via une interface-usager (via une carte par exemple), le géocodage est obtenu directement. Croisement de la requête et du corpus : il s\u0027agit d\u0027une comparaison géométrique au sens strict entre le géocodage de la requête et le géocodage des EG des documents.\nRequête 1 : Trouver les documents qui parlent de la banlieue de Pau et de Nay Dans cette requête, le niveau d\u0027agrégation des informations géographiques à exploiter est plus élevé car une simple utilisation des EG ne permet pas de traiter la requête. Le système doit être en mesure d\u0027interpréter les EG Pau et Nay mais aussi le concept de banlieue c\u0027est-à-dire d\u0027interpréter des Entités spatiales (ES). Traitements sur le corpus : extraction des ES, géocodage des ES, indexation des ES. Traitements sur la requête : si elle est exprimée en langage naturel, des traitements similaires à ceux effectués sur le corpus sont nécessaires c\u0027est-à-dire extraction des ES, géocodage des ES, indexation des ES. Si la requête est formulée via une interface-usager (via une carte par exemple), le géocodage est obtenu directement. Croisement de la requête et du corpus : il s\u0027agit d\u0027une comparaison géométrique au sens strict entre le géocodage de la requête et le géocodage des ES issues des documents.\nRequête 2 : Trouver les documents dans lesquels le narrateur part de Pau Ici encore, le niveau d\u0027agrégation de l\u0027information géographique est augmenté. Le système doit non seulement interpréter les ES mais aussi les relations spatiales entretenues par le narrateur avec ces ES. En d\u0027autres mots, le système doit être capable de déterminer si l\u0027ES Nay apparaît dans un déplacement du narrateur de polarité finale. Traitements sur le corpus : extraction, interprétation, géocodage et indexation des ES, extraction, interprétation, géocodage et indexation des déplacements. Traitements sur la requête : si elle est exprimée en langage naturel, des traitements similaires à ceux effectués sur le corpus sont nécessaires. Si la requête est formulée via une interface-usager (via une carte par exemple), le géocodage est obtenu directement. Croisement de la requête et du corpus : il ne s\u0027agit plus de faire une simple comparaison géométrique comme dans les cas précédents. Le système doit également interroger la structure sémantique (codée dans un arbre XML) pour déterminer la sémantique du déplacement effectué pour une ES donnée. Dans notre exemple, l\u0027ES Pau est associée à un déplacement de polarité initiale.\nRequête 3 : Trouver les documents dans lesquels le narrateur part de la banlieue de Pau, traverse Bizanos et arrive à Nay Dans cette requête, le niveau d\u0027agrégation est encore augmenté puisque le système doit être en mesure de capter l\u0027itinéraire complet effectué entre la banlieue de Pau et Nay. Traitements sur le corpus : extraction des ES, géocodage des ES, indexation des ES, extraction et interprétation des déplacements, reconstruction et indexation de l\u0027itinéraire. Traitements sur la requête : si elle est exprimée en langage naturel, des traitements similaires à ceux effectués sur le corpus sont nécessaires. Si la requête est formulée via une interface-usager (via une carte par exemple), le géocodage est obtenu directement. Croisement de la requête et du corpus : il s\u0027agit d\u0027effectuer une comparaison géométrique entre le géocodage de la requête et le géocodage des itinéraires extraits des documents du corpus.\nBilan et perspectives\nDans cet article, nous avons présenté une approche computationnelle et un outillage pour capter et exploiter, à différents niveaux de granularité, la sémantique des informations géogra-phiques contenues dans un corpus de récits de voyage. Les informations particulières visées par ces travaux sont (des plus simples aux plus agrégées) des entités géographiques nommées, des entités spatiales, des déplacements, des relais, des étapes et des segments constitutifs des itinéraires relatés par les auteurs.\nNous nous situons ici dans un courant très profond qui est celui du passage du web actuel (pour lequel on affiche des documents avec quelques fonctionnalités de recherche, y compris dans des projets comme Google Library) à un web de la connaissance et des services (le web 2.0).\nDes niches très diverses se constituent des outillages qui sont capables d\u0027exploiter les contenus textuels que l\u0027on peut récupérer via les bibliothèques numériques en ligne. Cela justifie une démarche de traitements totalement automatisés en amont des usages (notre approche consiste à fournir des services d\u0027interrogation / visualisation sur une clé d\u0027accès au document très particulière qui est celle de l\u0027itinéraire). Les traitements automatisés que nous proposons dans ce papier sont transparents pour les utilisateurs puisque les documents sont traités avant que l\u0027utilisateur ne s\u0027y intéresse via un traitement back-office (comme lors d\u0027une indexation classique d\u0027un moteur de recherche). Cela justifie également que l\u0027on propose des services à valeur ajoutée utilisant le potentiel des documents ainsi sélectionnés dans des applications finalisées (la sélection de documents sur des critères géo-spatiaux n\u0027étant pas une fin en soi). D\u0027où la conception d\u0027applications pédagogiques et les usages faits de ces applications par des apprenants.\nL\u0027intérêt des usages pédagogiques et l\u0027utilisabilité des documents analysés par notre outillage est de se servir de leur sémantique pour guider l\u0027activité de conception. D\u0027une part pour définir les types d\u0027interaction avec l\u0027apprenant qui pourront être évalués par le système tuteur (diagnostic des connaissances de l\u0027apprenant, diagnostic des erreurs de compréhension sur la base de la connaissance qu\u0027a ce tuteur du texte mis à disposition de l\u0027apprenant). D\u0027autre part pour éviter au concepteur de définir des activités que la machine ne pourrait pas interpréter parce que ce qui a été capté dans le texte de manière automatisée est trop pauvre par rapport à la compréhension que peut avoir l\u0027apprenant de cette activité par sa lecture \"humaine\" du document. L\u0027enjeu ici est de se servir des documents analysés automatiquement pour ne concevoir que les activités que l\u0027on va pouvoir encadrer de manière automatisée (ou assistée) par un tuteur informatique.\nCompte tenu de la taille du corpus documentaire mis à notre disposition, nous avons proposé une approche totalement automatisée d\u0027extraction des itinéraires de ce corpus. Cette approche se base sur deux modèles computationnels et un processus automatisé de mise en relation de ces deux modèles. Le premier modèle est un modèle des attendus (cf. section 3.1). Il a pour but de donner une représentation au concept d\u0027itinéraire et a été piloté par les usages que nous souhaitons en faire (à des fins de RI et de conception d\u0027activités pédagogiques). Le modèle des attendus proposé s\u0027inspire de travaux des nombreux auteurs (cf. section 2.2) ayant nourri les recherches dans ce domaine. Rappelons enfin que la formalisation MADS que nous avons proposée pour ce modèle des attendus est purement conceptuelle (cf figure 2) et ne pré-juge pas de la façon dont le modèle est ensuite formalisé pour son exploitation par un Système d\u0027Information de type Base de Données. Le second modèle (cf. section 3.2.2) est un modèle d\u0027entrée ou d\u0027extraction, il suit une logique du domaine de l\u0027Extraction d\u0027Information. Il permet par sa mise en application au sein de notre chaîne de traitement de capter environ 75% (cf figure 13) des déplacements importants d\u0027un récit de voyage. Cette capacité varie en fonction de la précision avec laquelle l\u0027auteur évoque ses déplacements : les déplacements de petite ampleur (le plus souvent ceux qui ne font pas intervenir d\u0027ES) sont plus difficiles à capter. Comme dans la plupart des travaux, les capacités d\u0027extraction dépendent fortement de la base lexicale utilisée. Ici, elles s\u0027appuient sur l\u0027existence de transducteurs de tous les verbes susceptibles d\u0027évoquer un déplacement (cf. section 4.1). Des expérimentations en cours semblent toutefois montrer qu\u0027il est possible d\u0027augmenter automatiquement ces ressources lexicales par synonymie, par similarité de construction, etc. Le processus automatisé que nous avons proposé permet de reconstruire l\u0027itinéraire (conformément au modèle des attendus) à partir des déplacements extraits (conformément au modèle d\u0027entrée). La mise en oeuvre de ce processus repose sur une hypothèse relativement forte concernant la chronologie des déplacements. Cette hypothèse est acceptable car les récits de voyage constituent un domaine à part entière : ce qui est dit doit être fidèle à ce qui a été vu, le locuteur rendant compte de ses découvertes avec la plus grande exactitude (ce qui est visé, c\u0027est la parfaite concordance des mots et des choses vues). Cependant, nous tentons actuellement de lever une partie de cette hypothèse en prenant en compte les aspects temporels et thématiques des relais mais aussi en tenant compte du temps (conjugaison) des verbes.\nDu point de vue qualitatif, les résultats actuels sont très encourageants. Nous avons évalué les capacités de notre outillage en comparant les itinéraires annotés automatiquement à une annotation manuelle effectuée sur trois textes. Nous donnons en figure 13 les résultats de cette évaluation : Dans l\u0027évaluation 1, nous avons comparé les déplacements captés automatiquement à tous les déplacements annotés manuellement. Les résultats sont mitigés car certains dé-placements ne sont pas évoqués sous la forme attendu, c\u0027est-à-dire avec le triplet (V,P ?,E). En comparant l\u0027extraction automatique avec les déplacements évoqués grâce au triplet (V, P ?,E), nous avons obtenu de bien meilleurs résultats (évaluation 2). Dans une grande majorité des cas, c\u0027est l\u0027absence de l\u0027ES qui met en échec notre processus d\u0027extraction des déplacements. \n"
  },
  {
    "id": "844",
    "text": "Introduction\nL\u0027interprétation automatique d\u0027images devient un processus de fouille de données de plus en plus complexe. Pour les images à très haute résolution, l\u0027utilisation de l\u0027approche dite orientée objet consiste à identifier dans l\u0027image, souvent à l\u0027aide d\u0027une segmentation de l\u0027image, des objets composés de plusieurs pixels connexes et ayant un intérêt pour l\u0027expert du domaine.\nIl existe de nombreux algorithmes de segmentation. Néanmoins, ces techniques nécessi-tent souvent une paramétrisation complexe telle que le choix de seuils ou de pondérations. Le nombre de paramètres augmente bien souvent avec la complexité des algorithmes. Ainsi, l\u0027utilisateur amené à définir ces paramètres a souvent du mal à faire le lien entre sa connaissance sur les objets présents dans l\u0027image et les paramètres adéquats pour les construire et les identifier dans une segmentation.\nL\u0027utilisation des algorithmes génétiques (Goldberg, 1989) est une solution à ce problème de recherche des paramètres optimaux. Ils peuvent être utilisés pour optimiser un ensemble d\u0027attributs si une fonction d\u0027évaluation des paramètres est disponible. Les méthodes existantes d\u0027optimisation de segmentation par approche génétique (Pignalberi et al., 2003;Bhanu et al., 1995;Song et Ciesielski, 2003;Feitosa et al., 2006) se basent sur des fonctions d\u0027évalua-tions demandant des exemples d\u0027objets segmentés fournis par l\u0027expert. Si aucun exemple n\u0027est disponible, il est possible d\u0027utiliser des critères non supervisés (Bhanu et al., 1995;Feitosa et al., 2006), c\u0027est à dire jugeant la qualité intrinsèque que doit avoir une segmentation (e.g. homogénéité des régions). Néanmoins ces critères non supervisés sont souvent insuffisants pour obtenir une segmentation de bonne qualité notamment pour l\u0027analyse d\u0027images complexes.\nDans cet article, nous proposons d\u0027utiliser des connaissances du domaine afin d\u0027évaluer la qualité d\u0027une segmentation. En effet, l\u0027approche orientée objet permet à l\u0027expert d\u0027exprimer ses connaissances sur les objets de l\u0027image. Ce nouveau cadre de discernement permet de raisonner sur des régions et non sur des pixels ce qui permet une description intuitive et naturelle des objets pouvant être présents dans une image. Une ontologie, ou base de connaissance, peut alors être utilisée pour définir les différents types d\u0027objets du domaine (i.e. concepts) de l\u0027image ainsi que leurs caractéristiques. Il devient alors possible d\u0027évaluer la cohérence d\u0027une segmentation par rapport aux concepts définis dans cette ontologie. Cette approche a l\u0027avantage de ne pas nécessiter d\u0027exemples et utilise la connaissance définie dans l\u0027ontologie.\nLe plan de cet article est le suivant. Tout d\u0027abord nous introduisons l\u0027algorithme de segmentation ainsi que ses paramètres. Nous présentons ensuite la modélisation de la connaissance sous la forme d\u0027une ontologie. Nous étudions ensuite comment un algorithme génétique est utilisé pour choisir les paramètres de la segmentation grâce à une évaluation utilisant l\u0027ontologie. Finalement nous présentons des expérimentations dans le cadre de l\u0027interprétation d\u0027images pour l\u0027observation de la Terre.\nSegmentation d\u0027image\nDans cet article, nous utilisons l\u0027algorithme de segmentation par ligne de partage des eaux (Vincent et Soille, 1991) ainsi que les méthodes de seuillage du gradient, de la réduction de dynamique et de fusion de régions. Ces différentes techniques sont utilisées simultanément pour réduire la sur-segmentation provoquée par la ligne de partage des eaux et nécessitent donc de définir 3 paramètres (le seuil du gradient, le seuil de dynamique et le seuil de fusion). Les valeurs optimales de ces paramètres sont difficiles à trouver car la valeur pour un paramètre donné dépend des valeurs choisies pour les autres paramètres. De plus, il existe de nombreux optima locaux, ce qui accroît la difficulté de trouver la meilleure solution.\nOntologie d\u0027objets géographiques\nNous présentons ici les principes de l\u0027ontologie d\u0027objets géographiques définie dans (Brisson et al., 2007) puis étendue dans . Cette ontologie se présente comme une hiérarchie de concepts ainsi que des relations entre ces concepts. Un mécanisme d\u0027appariement permet de comparer une région construite lors d\u0027une segmentation et les différents concepts définis dans l\u0027ontologie.\nL\u0027ontologie est formée d\u0027une hiérarchie de concepts dont la figure 1 présente un extrait. Dans cette hiérarchie chaque noeud correspond à un concept. Chaque concept a une éti-quette (e.g. pavillon) et est défini par des attributs. Chaque attribut est associé à un intervalle de valeurs acceptées pour cet attribut (e.g. [50; 60]) ainsi qu\u0027une pondération (dans [0; 1]) représentant son importance pour reconnaître l\u0027objet géographique correspondant à ce concept (1 indiquant que cet attribut est très pertinent). Les valeurs de ces concepts ont été renseignées par les experts géographes grâce à leur connaissance de la morphologie des objets urbains. Certaines informations ont également été extraites de bases de données topographiques ou de connaissances sur les réponses spectrales de certains types de matériaux (tuile, bitume . . .).\nUn mécanisme d\u0027appariement de région permet d\u0027évaluer la similarité entre une région construite lors d\u0027une segmentation et les concepts définis dans la hiérarchie de l\u0027ontologie. Ce mécanisme permet d\u0027obtenir la signification sémantique d\u0027une région si les caractéristiques de celle-ci se rapprochent de la description d\u0027un des concepts définis dans l\u0027ontologie. L\u0027appariement d\u0027une région consiste à vérifier la validité des caractéristiques extraites de celle-ci (réponse spectrale, taille, élongation . . .) en fonction des propriétés et des contraintes définies dans les concepts de l\u0027ontologie.\nAlgorithme génétique\nLes algorithmes génétiques (Goldberg, 1989) font partie des méthodes d\u0027optimisations. Ils sont reconnus pour être efficaces même lorsque l\u0027espace de recherche est vaste et contient de nombreux maxima locaux. Ces algorithmes sont d\u0027inspiration biologique avec des notions d\u0027individus (i.e solutions) et de population d\u0027individus qui évoluent pour s\u0027améliorer à travers des opérations de sélection, croisement et mutation. Dans cet article, un individu représente le vecteur des paramètres de la méthode de segmentation. On considère que les valeurs de ces paramètres sont définies entre zéro et un, on obtient donc un espace de recherche à 3 dimensions pour la ligne de partage des eaux (trois seuils). Nous utilisons un taux de mutation de 1% et un nombre de générations de 14, des tests ayant montré que plus de générations n\u0027amélioraient pas les résultats.\nLa définition de la fonction d\u0027évaluation est une des étapes les plus importantes dans un système d\u0027évolution génétique. Nous cherchons ici à utiliser les connaissances de l\u0027ontologie pour guider le processus évolutif et trouver les paramètres qui permettent de maximiser la découverte d\u0027objets dans l\u0027image. Nous allons donc utiliser comme fonction d\u0027évaluation, le pourcentage de la surface de l\u0027image qui est identifié par l\u0027ontologie. Chaque individu i va donc produire un vecteur de paramètres produisant une segmentation de l\u0027image. Soit R i les régions de la segmentation issue de l\u0027individu i et R avec Aire(r) une fonction renvoyant la surface en pixel de la région r. La surface des régions identifiées a été préférée à leur nombre pour évaluer le résultat comme nous cherchons ici à maximiser la surface de l\u0027image reconnue par l\u0027ontologie.\nExpérimentations\nLa méthode proposée a été évaluée sur une image de Strasbourg prise par le satellite Quickbird. La figure 2 présente des extraits de segmentations avec les paramètres obtenus au cours d\u0027une évolution génétique aux générations 1, 3, 5 et 11. On observe une réduction de la sous--segmentation (trop peu de régions) au cours des générations et une amélioration de la construction des objets, l\u0027image étant de mieux en mieux interprétée par l\u0027ontologie. Pour valider ces résultats nous avons évalué la qualité des segmentations obtenues avec des vérités terrains de trois classes, pavillon, végétation et route. Les évaluations sont effectuées sur des objets géographiques construits et labellisés manuellement par un expert. Trois indices de qualité ont été utilisés pour évaluer la qualité des segmentations : le rappel (en considérant la reconnaissance de l\u0027ontologie comme une classification pixel), l\u0027indice de Janssen (Janssen et Molenaar, 1995) et l\u0027indice de Feitosa (Feitosa et al., 2006) qui évaluent la qualité des régions construites.\nNous avons évalué notre critère d\u0027évaluation (surface reconnue par l\u0027ontologie) par rapport à ces trois critères. Pour chacun des critères une moyenne est calculée sur l\u0027ensemble des objets  TAB. 1 -Résultat de l\u0027évaluation de la méthode sur les objets experts pour 4 générations.\nfournis par l\u0027expert. Le but de cette analyse est de vérifier que l\u0027amélioration de notre critère conduit bien à une amélioration de la segmentation. Au cours d\u0027une évolution génétique nous avons évalué chaque individu en fonction de ces différents critères. Ainsi, 200 paramétrages possibles ont été évalués. Ils ont ensuite été ordonnés en fonction de notre critère d\u0027évaluation. La figure 3 présente les courbes pour les différents indices. Nous pouvons constater que dans les trois cas, les deux courbes semblent avoir même comportement et être corrélées. Ces résul-tats montrent qu\u0027optimiser notre critère est pertinent et permet d\u0027effectuer des segmentations de qualité sans contraindre l\u0027expert à fournir des exemples. Enfin, le tableau 1 présente les valeurs de ces indices avec les paramètres trouvés au cours des générations 1, 3, 5 et 11 (la qualité n\u0027évoluant plus après). On constate que les différentes mesures d\u0027évaluation sont optimisées au cours de l\u0027évolution (sauf une légère augmentation de l\u0027indice Feitosa sur la dernière génération due à une sur-segmentation de la végétation mais qui n\u0027influe pas sur la reconnaissance de l\u0027ontologie, la taille d\u0027une région n\u0027étant pas discriminante pour caractériser la végétation).\nConclusion\nDans cet article nous avons présenté un mécanisme permettant de guider un processus automatique d\u0027interprétation d\u0027images grâce à une ontologie. Cette ontologie représente la connaissance des experts sous la forme d\u0027une hiérarchie de concepts, d\u0027une caractérisation de ces concepts et des relations entre ces concepts. Un processus évolutif va alors chercher les paramètres de segmentation produisant des régions qui seront bien identifiées par l\u0027ontologie. Ainsi nous disposons d\u0027une méthode d\u0027interprétation automatique d\u0027images de télédétection basée sur la connaissance des experts à propos des objets à identifier.\nDes résultats intéressants ont montré la validité du système. Il est à noter que n\u0027importe quel algorithme de segmentation nécessitant des paramètres peut remplacer celui présenté dans cet article. Dans le futur nous souhaitons intégrer des connaissances contextuelles (position des objets entre eux) nécessaire à l\u0027identification de certains objets.\n"
  },
  {
    "id": "846",
    "text": "Introduction\nLa gestion et la maîtrise du développement territorial imposent le recours à une vision globale et objective des caractéristiques et des dynamiques d\u0027un espace. C\u0027est pourquoi, avant d\u0027engager des politiques d\u0027action, il est nécessaire de fonder la réflexion sur une dé-marche de diagnostic de territoire. Ce processus de diagnostic territorial permet non seulement d\u0027effectuer une sorte « d\u0027état des lieux » du territoire -considéré comme une construction sociale résultant des interactions entre les acteurs et les activités et s\u0027analysant en tant que réseau de relations (Lardon et al., 2001) -, des relations qui le construisent, mais aussi de -43 -RNTI-E-13\nGEOdoc proposer des solutions aux éventuelles difficultés soulevées par le diagnostic. Il est bien souvent le siège d\u0027une concertation entre acteurs et experts, au sein de laquelle des opinions parfois divergentes doivent pouvoir s\u0027exprimer. La solution retenue consiste alors en l\u0027acceptation d\u0027un consensus par les parties en présence.\nLa mise en oeuvre d\u0027un diagnostic de territoire est un processus complexe. Cette complexité est en partie du à la grande variété d\u0027acteurs impliqués et à la mobilisation importante de représentations spatiales. C\u0027est ce qui explique qu\u0027un certain nombre de méthodes aient été développées pour faciliter le dialogue entre acteurs et experts et favoriser l\u0027usage des représentations spatiales comme base de réflexion et de travail. Citons par exemple l\u0027approche de diagnostic Structure-Dynamique-Projet développée par le laboratoire POP\u0027TER (Politiques publiques et développement des territoires) de l\u0027ENGREF ou encore la méthode du Zonage À Dire d\u0027Acteurs (ZADA) développée par CIRAD-TERA (Brau et al. 2005). Ces deux méthodes, bien que fondées sur des approches différentes, possèdent des similarités en ce qui concerne l\u0027organisation des étapes du diagnostic et le positionnement des représentations spatiales.\nCette démarche suppose donc que les acteurs impliqués cheminent dans le processus vers une conception « commune » du territoire et des objectifs « individuels » compatibles avec les objectifs « communs » (Lardon et Mainguenaud 2006). L\u0027exercice de diagnostic requiert des outils, une méthodologie et des représentations spatiales adaptées, de manière à prendre en compte tous les avis et à aboutir à des résultats acceptés par le plus grand nombre. Certains outils comme les tableaux de bord, les systèmes d\u0027aide à la décision ou les Spatial OLAP, permettent déjà d\u0027aider aux décisions à partir de règles et de paramètres prédéfinis. Mais bien souvent, les décideurs de niveau stratégique (élus locaux par exemple) n\u0027ont pas les connaissances nécessaires indispensables à la manipulation de ces outils même les plus simples. Ils n\u0027ont pas non plus toujours à leur disposition les documents de référence pertinents sur lesquels ils pourraient baser leur jugement (Ciobanu et al. 2006). Il devient alors très difficile dans ces conditions de trouver les informations pertinentes, de les mettre en relation les unes avec les autres, de les contextualiser et finalement de les utiliser à bon escient, notamment à cause de la masse toujours plus importante de documents diffusés.\nPartant de ce constat, nous nous sommes engagés dans la conception d\u0027un outil de visualisation qui permettrait de naviguer de manière simple à travers des documents géographi-ques mis en réseaux. En effet, dans la pratique, le diagnostic de territoire nécessite la conception, la mobilisation et l\u0027étude de nombreux documents géographiques ; des représentations spatiales variées, mais aussi d\u0027autres documents multiformes comme les articles de journaux, les photos, les tableaux de statistiques, etc. Un document géographique, dans la suite de cet article, est défini comme un « support physique ou numérique incluant des données géomé-triques, descriptives ou graphiques, porteurs de représentations, d\u0027informations et de connaissances, en lien direct avec la description et la caractérisation d\u0027un territoire ou d\u0027un espace géographique donné ».\nNous débutons cet article par une synthèse rapide des caractéristiques des processus de diagnostic de territoire, de manière à poser les contraintes auxquelles la solution GEOdoc devra répondre. Nous proposons dans la foulée un bref état de l\u0027art des solutions et outils déjà existants. Nous passons ensuite en revue les techniques et approches existantes pour -44 -RNTI-E-13 répertorier, organiser et visualiser des documents géographiques. Enfin, nous proposons une première conceptualisation de cet outil de visualisation appuyée sur un cas d\u0027usage type, et présentons brièvement quelques perspectives d\u0027évolutions possibles.\n2 Des outils pour appuyer le diagnostic de territoire\nDocuments géographiques et diagnostic de territoire\nLa dimension spatiale du diagnostic de territoire est à l\u0027origine d\u0027une importante complexité. La difficulté est liée au fait que chaque acteur impliqué possède « sa » représentation de l\u0027espace et des enjeux associés. C\u0027est l\u0027une des raisons qui expliquent pourquoi l\u0027intégration sociale des technologies géospatiales dans ces processus de diagnostic est encore faible. Pourtant dans le contexte de la société de l\u0027information, il apparaît plus que jamais nécessaire de concevoir de nouveaux outils qui permettraient de favoriser l\u0027usage de ces technologies au sein des systèmes sociaux de décision et de concertation (Roche et Hodel 2004). C\u0027est un peu ce que prône H. Campbell (1999) dans son approche « d\u0027interactionnisme social ». Elle propose d\u0027intégrer les acteurs sociaux dans le développe-ment des solutions géomatiques ; de ne pas les reléguer au rang de simples spectateurs ou utilisateurs passifs. Elle met ainsi l\u0027accent sur le lien très fort qui associe les avancées technologiques dans un domaine et leurs impacts sociétaux. Les deux notions interagissent et s\u0027influencent à des degrés différents suivant les époques. Ainsi, comme le rappelle C. Bucher (2002), la représentation de l\u0027espace géographique dépend du contexte social de la personne qui en fait la retranscription.\nL\u0027un des fondamentaux du diagnostic de territoire repose précisément sur l\u0027élaboration de représentations spatiales traduisant les problèmes, les forces, les faiblesses, voir les élé-ments de solution. Ceci conduit à produire des formes de représentations différentes dont l\u0027interprétation peut varier en fonction des objectifs visés. Ainsi, outre le format de la repré-sentation (cartes, photos, schémas, chorèmes...), les objets, les phénomènes ou encore les dynamiques représentés sont dépendants de la volonté de leurs auteurs et des priorités qu\u0027ils ont souhaité mettre de l\u0027avant. Ces acteurs concepteurs de représentations ne sont pourtant pas les seuls à fournir des renseignements sur un territoire. D\u0027autres documents viennent préciser le contexte d\u0027analyse, apporter une information chiffrée ou encore synthétiser des points de vue. Ils sont bien souvent présentés sous des formes différentes (textes, tableaux, graphiques, vidéos…) et ne sont pas toujours pris en compte lors d\u0027un diagnostic de territoire, par méconnaissance ou inaccessibilité. Pourtant, ces documents doivent, au même titre que les représentations spatiales, être inclus dans les analyses et les réflexions sur lesquelles se construit le diagnostic. De notre point de vue, ces documents sont fondamentaux dans la mesure où ils offrent bien souvent aux acteurs des clés de lecture complémentaires pour la compréhension des enjeux. C\u0027est précisément l\u0027un des objectifs de GEOdoc que de permettre une visualisation de l\u0027ensemble des documents pertinents pour un diagnostic de territoire, et de rendre explicite le réseau de relations qui lient les documents entre eux, les documents et les acteurs, liens géo-graphiquement contextualisés.\n-45 -RNTI-E-13\nGEOdoc\nLes limites des outils et solutions existants\nLes technologies géospatiales offrent des solutions à géométrie variable pour appuyer les diagnostics de territoire. Des SIG bâtis pour l\u0027analyse, aux SIG participatifs -PPGISconçus et développés pour supporter la participation du public, la panoplie est large. Pour autant, ils ne sont pas particulièrement efficaces dès lors que les données à mobiliser sont de nature hétérogène, ou pour gérer et diffuser des documents multimédia à forme variable (cartes, graphiques, film, vidéo, etc.). D\u0027autres solutions technologiques existent, dont l\u0027objet consiste précisément à gérer des documents sous forme numérique (GED -gestion électro-nique des documents). Mais dans ce cas, la gestion de la composante géospatiale des documents est rarement considérée. Aussi pour prendre la pleine mesure de la solution proposée ici, nous avons jugé pertinent et utile de dresser un rapide portrait des solutions existantes dans le domaine de la gestion documentaire, de la géomatique décisionnelle et de l\u0027intelligence territoriale. Il s\u0027agit là de secteurs fortement marqués par des phénomènes de convergences technologiques (Clemens 2004, Plante et al. 2007, Leblond 2007. GEOdoc vise spécifiquement à appuyer la prise de connaissance, la concertation, puis la prise de déci-sion tactique et stratégique ancrée sur le territoire, objet d\u0027un diagnostic. Bien qu\u0027il s\u0027agisse là d\u0027un type de système décisionnel novateur, GEOdoc possède des affinités fonctionnelles et de contenu avec certains types de systèmes d\u0027information actuels: (1) tableaux de bord, (2) outils de GED et (3) systèmes d\u0027information géographique (Figure 1).\nFIG. 1 -Position du GEOdoc par rapport aux autres solutions existantes\nGEOdoc possède des similitudes avec les outils utilisés en intelligence d\u0027affaires (« Business Intelligence »), domaine décisionnel en pleine effervescence (Dresner et al. 2003  (EMC 2008-b). À l\u0027instar de ces solutions, GEOdoc vise à faciliter l\u0027accès à l\u0027information en constituant plus spécifiquement un \"guichet\" intégré numérique pour accéder à de multiples informations gérées par les parties prenantes. Néanmoins, il est important de comprendre que le GEOdoc ne vise pas à se substituer dans sa finalité aux logiciels de gestion documentaire pouvant déjà être en place dans les organisations, mais vise plutôt à venir les compléter sur le plan géospatial en proposant une interface cartographique novatrice, simple d\u0027utilisation, favorisant (accélérant) ainsi la réflexion et la prise de décision territoriale.\nFinalement, GEOdoc emprunte certaines fonctionnalités aux bases de données géospatia-les, aux systèmes d\u0027information géographique (SIG) et aux logiciels de dessin assisté par ordinateur (DAO). Il repose en effet sur une interface à l\u0027utilisateur, basée sur la localisation et la représentation cartographique. La différence principale réside dans la forte simplification de l\u0027interface de l\u0027outil par rapport à des SIG offrant habituellement des fonctionnalités complexes de navigation géographique et d\u0027analyse spatiale, tels que ArcGIS (ESRI 2008) et MapInfo (Mapinfo 2008). Sur le plan du contenu, GEOdoc diffère aussi des SIG, tout en se rapprochant davantage des géorépertoires ou des catalogues cartographiques de métadon-nées. Il offre ainsi moins un accès aux données de base qu\u0027aux représentations spatialesplus ou moins fermées -déjà conçues à partir des données. Pourtant, certaines déclinaisons récentes particulières des SIG vont dans le même sens que GEOdoc à certains égards. Ainsi, Google Earth et Google Map (Google Earth 2008) proposent une interface cartographique simplifiée pour des non-spécialistes de la géomatique, facilitant ainsi grandement l\u0027accès à l\u0027information géospatiale. D\u0027autre part, le SOLAP (SOLAP 2008) constitue un système inté-ressant, combinant à la fois des fonctions de forage de données géographiques, une structure multidimensionnelle de données, ainsi qu\u0027une interface géographique. Finalement, soulignons le logiciel MetaCarta (MetaCarta 2008), très intéressant dans sa capacité d\u0027identifier une localisation générale attribuable à de multiples textes (documents descriptifs) et de les localiser visuellement dans une interface géographique.\nCette analyse de l\u0027existant nous a permis de mettre en évidence le caractère novateur de GEOdoc. En effet, aucun outil existant ne permet de rendre explicite, navigable et interrogeable le réseau de relations caractéristiques d\u0027un processus de réflexion et de décision collectives spatialisées (diagnostic de territoires) : relations entre les documents géographiques eux-mêmes, relations entre les documents et les acteurs ; l\u0027ensemble de ces relations étant géographiquement contextualisées.\n-47 -RNTI-E-13\nGEOdoc 3 Les concepts à la base de GEOdoc\n« médium humain » et « objets frontières »\nL\u0027un des objectifs principaux du diagnostic territorial consiste à favoriser l\u0027appropriation des représentations spatiales par les décideurs locaux, de manière à faciliter la compréhen-sion des concepts spatiaux qui sont développés. Dans ce sens, deux concepts fondamentaux permettent de renouveler l\u0027approche des systèmes décisionnels géospatiaux dans le diagnostic territorial : le « médium-humain » et les « objets frontières ».\nLe « médium-humain » est en quelque sorte la personne qui assure le lien entre les données produites par les experts et professionnels en particulier (les représentations spatiales par exemple) et les usagers finaux (les décideurs, le public…). Plus exactement, ce médium favorise l\u0027accessibilité et l\u0027assimilation de l\u0027information géographique par des utilisateurs indirects (Roche 2000, Roche et Hodel 2004. Il est le garant de l\u0027interactivité entre des domaines variés comme la géomatique et l\u0027aménagement. Il permet d\u0027éviter certains blocages dus à la difficulté de compréhension des données géographiques par des non spécialistes et ainsi contribue à enrichir le débat en favorisant la communication.\nLes « objets frontières » constituent des éléments fondamentaux dans le partage des connaissances. Ce sont des « objets marquant une frontière, tout en facilitant le dialogue entre des spécialistes dans la réalisation d\u0027un projet commun pluridisciplinaire » (définition issue du Réseau d\u0027Activité à Distance : http://rad2000.free.fr). Il peut s\u0027agir de concepts définis dans un schéma d\u0027aménagement, de règles imposées sur un espace, de cartes ou de modèles… De manière générale, les individus issus d\u0027un même « groupe social » s\u0027organisent en réseau pour transmettre de l\u0027information. Au sein d\u0027un même groupe, la compréhension des informations qui circulent est favorisée par la proximité des profils. Lorsque la communication et les échanges doivent s\u0027opérer entre des groupes différents, c\u0027est presque systématiquement le cas dans un diagnostic de territoire, la compréhension est beaucoup moins évidente. Et c\u0027est là précisément que les objets frontières jouent un rôle central. Dans le domaine des sciences géomatiques par exemple, les « objets frontières » participent, tout comme le concept de « médium-humain », de l\u0027amélioration des interactions entre les TIG -technologies de l\u0027information géographique -et le contexte social d\u0027un projet. Harvey et Chrisman (1998) apportent un éclairage intéressant, qui montre la pertinence des « objets frontières » pour comprendre la problématique d\u0027adoption des TIG dans les contextes de diagnostic de territoire : « The part of social-constructivist thinking we specifically mobilize to examine GIS technology is a concept known as boundary objects. This concept articulates the process through which technology becomes part of different social groups, and how technology successfully connects multiple, even opposing, perspectives. Boundary objects provide coherence by linking multiple social groups through the stabilizations of facts and artefacts ».\nCes deux concepts se situent donc à la croisée des exigences qu\u0027impose la production d\u0027informations sur un territoire donné, pour appuyer une opération de diagnostic d\u0027une part, et l\u0027assimilation de ces connaissances par les acteurs impliqués dans le diagnostic d\u0027autre part. Ces deux concepts posent les bases de GEOdoc : faciliter l\u0027accès à des documents géo--48 -RNTI-E-13 graphiques multiformes par des acteurs locaux aux profils variés, mais aussi organiser et situer ces documents à l\u0027intérieur d\u0027un réseau complexe d\u0027acteurs. En effet, un document géographique n\u0027est assimilable que s\u0027il est « positionné » dans son contexte et mis en perspective par rapport aux autres documents produits. Ainsi, GEOdoc s\u0027inscrit dans une démar-che globale. Il peut être consulté à toutes les étapes d\u0027un diagnostic, de manière à trouver des indications sur le contexte d\u0027élaboration d\u0027un diagnostic de territoire.\nDocuments géographiques et réseaux d\u0027acteurs\nDifficile d\u0027envisager un diagnostic de territoire sans porter une attention particulière au rôle des réseaux d\u0027acteurs. Une démarche de diagnostic de territoire doit se fonder sur une approche systémique du territoire, de manière à ce que soit pris en compte l\u0027ensemble des relations et des interactions. Il est capital d\u0027identifier les stratégies de chaque acteur, ainsi que les contraintes (règles et normes) auxquelles ils sont soumis (Bion 2003). Cette approche globale permet de prendre en compte le système de jeux de pouvoirs qui s\u0027exerce au sein d\u0027un territoire et qui modifie soit directement, par des actions politiques, soit indirectement par une communication des acteurs, le point de vue de la population. Si les interactions entre acteurs sont en quelque sorte le carburant d\u0027un diagnostic, elles constituent aussi une source de blocage dans les négociations et d\u0027échec dans l\u0027aboutissement du diagnostic. Cet ensemble complexe d\u0027interactions a fait l\u0027objet de nombreuses études en sociologie. Dans cet article, nous nous intéressons plus particulièrement à l\u0027approche, connue sous le nom de « Actor Network Theory -ANT », développée en particulier par Bruno Latour. Martin (2000) décrit l\u0027ANT comme: « a dynamic and unusual approach towards technology and the varied roles it plays in influencing society ». Cette approche s\u0027appuie sur des notions comme les « objets frontières » et propose de traduire l\u0027impact des technologies sur des groupes sociaux. Cette théorie renouvelle la conception des réseaux d\u0027acteurs, en présentant les acteurs et les ré-seaux comme deux entités indissociables (Latour 1999). Elle introduit les notions d\u0027acteurs humains et d\u0027acteurs non-humains. En effet, les interactions ne se font pas seulement entre des personnes. Elles sont reliées également à des objets dans un système complexe de dépen-dance, c\u0027est la notion « d\u0027hétérogénéité » des réseaux. Dans le domaine du diagnostic de territoire, on imagine immédiatement une transposition qui permettrait ainsi de repositionner les documents géographiques (les représentations spatiales par exemple) produits et mobilisés dans le réseau d\u0027acteurs impliqués. Dans l\u0027approche ANT, tous les acteurs (humains et non-humains) sont équivalents, du point de vue fonctionnel. D\u0027ailleurs, ce n\u0027est pas tant la structure et la forme des relations qui est étudiée, mais leurs natures -les interactions et les effets entre acteurs (Martin 2000). L\u0027approche ANT permet donc une certaine liberté dans la conception de réseaux et n\u0027impose pas une conceptualisation de référence.\nNous l\u0027avons précisé plus haut, les documents géographiques sont le carburant essentiel d\u0027un diagnostic de territoire, en même temps qu\u0027ils en constituent des éléments matériels de production. La variété et la diversité des documents mobilisés et produits au cours d\u0027un diagnostic imposent, dans la perspective de concevoir un outil comme GEOdoc, de les structurer mais aussi d\u0027en garantir un accès facilité par la mise à disposition des renseignements pertinents à leur évaluation (métadonnées). Ces deux conditions constituent deux autres contraintes de spécification auquel GEOdoc doit répondre.\n-49 -RNTI-E-13 GEOdoc Une autre contrainte de spécification est imposée par la prise en compte de l\u0027approche ANT. En effet, les interrelations étroites qui lient différents documents géographiques porteurs de connaissances sur un territoire donné (les non-humains), mais aussi les liens qui existent entre ces documents et les acteurs impliqués (humains) dans les dynamiques territoriales forment le réseau d\u0027acteurs. C\u0027est précisément ce réseau complexe que GEOdoc doit donner à voir et à comprendre. C\u0027est dans ce réseau qu\u0027il doit permettre de naviguer. C\u0027est en référence à ce réseau, qu\u0027il doit permettre de consulter et de visualiser les documents pertinents. La consultation et la visualisation sont en effet deux dimensions fondamentales de GEOdoc, mais sans une mise en perspective des documents, sans leur référencement au ré-seau d\u0027acteurs (ANT), ces fonctions se révèlent insuffisantes pour répondre aux exigences des processus de diagnostic de territoire. Car c\u0027est bien la compréhension des relations qui lient les documents géographiques aux acteurs moteurs d\u0027un territoire, qui permet de caracté-riser et d\u0027identifier le territoire et ses dynamiques.\nCette exigence impose en particulier que les documents géographiques soient caractérisés aussi bien par des données (géométriques, descriptives et graphiques) que par des métadon-nées. Compte tenu de l\u0027importance des métadonnées pour formaliser le réseau d\u0027acteurs d\u0027un territoire selon l\u0027approche ANT, c\u0027est-à-dire de pouvoir y inclure les acteurs non-humains (les documents géographiques en particulier), il nous a paru fondamental que le prototype de GEOdoc prenne en compte les normes de métadonnées qui existent dans les domaines de l\u0027information géographique et de la gestion des connaissances. Nous avons ainsi étudié les normes Dublin Core, le Content Standard for Digital Geospatial Metadata, la norme Global Information Locator Service ou encore la norme de l\u0027International Standard Organization TC/211. Toutes ces normes fournissent de nombreuses informations sur les données géogra-phiques qui peuvent être échangées ou consultées. Une partie seulement de tous ces renseignements a été retenue pour GEOdoc. En effet dans le contexte du support au diagnostic de territoire, les métadonnées doivent principalement aider les acteurs locaux à identifier la pertinence d\u0027un document et les liens qui le caractérisent (liens avec les autres documents, mais avec les acteurs également). Les normes précédemment citées ont donc servi de réfé-rence pour la conception du modèle de base de données (qui structure les documents géogra-phiques) sur lequel s\u0027appuie GEOdoc (cf. la section suivante).\nUne autre composante fondamentale de GEOdoc est la définition d\u0027un thésaurus -une ontologie très simplifiée -permettant de classer les documents par thèmes appropriés et d\u0027en définir les modalités de manipulation. Sur le plan pratique on peut considérer qu\u0027une ontologie est un ensemble structuré de concepts. Les concepts sont organisés dans un graphe dont les relations peuvent être : des relations sémantiques ou des relations de composition et d\u0027héritage (au sens objet) ». Il s\u0027agit donc d\u0027une manière élaborée de manipuler un vocabulaire défini par avance. Dans le domaine géospatial, « An ontology of geographic kinds is designed to yield a better understanding of the structure of the geographic world, and to support the development of geographic information systems that are conceptually sound » (Smith and Mark 1998). Un thésaurus fait référence quant à lui à un classement en concepts définis dans lequel les relations entre concepts ne sont pas nécessairement spécifiées. Ontologie et thesaurus constituent deux moyens très performants de tri des connaissances et permettent par ailleurs de fournir à un utilisateur un cadre de référence lors de la recherche ou consultation d\u0027un document.\n-50 -RNTI-E-13\nIntelligence collective et réseau ouvert\nLa navigation dans le réseau d\u0027acteurs (ANT) et la visualisation des documents et des relations, ainsi que leur mise à jour dans la base de données constitue un autre enjeu pour GEOdoc. Plusieurs concepts intéressants dans le contexte du diagnostic de territoire, ont été développés au cours des dernières années, même s\u0027ils sont encore peu utilisés dans les outils de recherche ou de consultation de l\u0027information.  \nConception de la maquette de GEOdoc\nLes concepts et notions développés plus haut constituent les bases de fonctionnement de GEOdoc. Nous présentons ici les caractéristiques principales. Nous avons en effet réalisé une première maquette (comme preuve de concept) de manière à illustrer ce que pourrait être un outil intégrant ces concepts novateurs comme support du diagnostic de territoire.\nLe travail réalisé à propos des métadonnées et des ontologies constitue les guides à partir desquels la structure de la base de données de GEOdoc est conçue. Cette structure vise principalement à faciliter la recherche d\u0027un document ou d\u0027un acteur. Des options de recherche sont proposées de façon à pouvoir accéder le plus rapidement possible à l\u0027information recherchée. Le modèle de base de données proposé permet également de structurer les documents et les acteurs en un réseau d\u0027objets (au sens de l\u0027ANT) dans lequel l\u0027utilisateur peut aisément naviguer. Il favorise ainsi la compréhension des interactions entre les différentes composantes des dynamiques du territoire sur lequel porte le diagnostic. Enfin, avant de consulter un document, il est possible d\u0027accéder à certaines métadonnées du document (date de création, format, description, utilisateur…).\nIl est important de rappeler que GEODoc est destiné à des utilisateurs peu habitués à manipuler de l\u0027information géographique. La conception a donc été guidée par un souci constant de ne pas surcharger le modèle avec des données trop complexes à interpréter. La majorité des attributs de la base de données sont par exemple inspirés de la norme Dublin Core car ils sont universels et simples d\u0027utilisation. Pour les documents spécifiquement géogra-phiques (représentations spatiales), les attributs (description spatiale des données) sont issus des normes spécifiques du domaine (ISO en particulier). L\u0027utilisateur peut par exemple obtenir la localisation d\u0027un document par ses coordonnées géographiques ou par un index géo-graphique (« gazetteer »). Des informations génériques sur la projection cartographique sont également disponibles (pas de donnée détaillée sur l\u0027ellipsoïde de référence par exemple).\nConcernant le design d\u0027interaction, les documents et les concepts ont été organisés selon un réseau favorisant la navigation. Les liens qui relient par exemple les entités documents, concepts et acteurs doivent être générés à la volée lors de l\u0027affichage du réseau, ce qui suppose qu\u0027ils soient déjà implémentés dans la base de données. La conception de la base de données s\u0027appuie également sur la définition d\u0027un thésaurus qui permet de classer les docu--52 -RNTI-E-13 ments et de caractériser les relations entre les objets. Ainsi, l\u0027architecture de la base de données facilite l\u0027organisation et la recherche des documents ou des acteurs. Dans une perspective opérationnelle, il serait souhaitable que la création du thésaurus permettant de répertorier les documents soit réalisée conjointement par des experts et utilisateurs (décideurs locaux) afin d\u0027être adaptée aux besoins et au contexte spécifique.\nLe modèle de données est conçu en UML (cf. les extraits du diagramme de classe de la Figure 2), il a été conçu à l\u0027aide de Perceptory (http://sirs.scg.ulaval.ca/perceptory/). La classe « Document géographique » est la classe centrale du modèle conceptuel. Elle regroupe les informations générales sur les documents comme le titre, le sous-titre, la description, le ré-sumé… mais aussi l\u0027index géographique. Toutes les autres classes s\u0027articulent autour de celle-là. Par exemple, la classe « Acteur » lui est reliée avec une classe d\u0027association afin de caractériser plus précisément la relation entre le document et l\u0027acteur.\nIl est ainsi possible de savoir si un acteur est producteur, utilisateur ou éditeur de la donnée (ou plusieurs choses à la fois). La liste n\u0027est d\u0027ailleurs pas exhaustive et peut être complétée selon les besoins. On remarque également que les deux classes précédemment citées sont reliées à elles-mêmes afin de densifier le réseau et de faciliter la navigation entre les documents. Il est donc possible de consulter tous les documents qui ont un lien avec le document visualisé ou encore de voir tous les acteurs qui interagissent avec un acteur donné. Il est également possible de consulter le lien qui unit les acteurs et de savoir ainsi quels sont les acteurs qui travaillent ensemble sur un projet, quels sont ceux qui sont localisés dans la mê-me région, quels sont ceux qui entretiennent des relations privilégiées (figure 2).\nAinsi l\u0027organisation graphique du réseau d\u0027acteurs (ANT) est supportée par l\u0027architecture de la base de données. Les documents sont classés selon trois classes : « Domaine », « Thème » et « Sous-thèmes ». Avec les deux classes « Document géographique » et « Acteur », ces trois classes constituent le squelette de l\u0027organisation du réseau. À travers ces trois concepts, il est ainsi possible de naviguer dans l\u0027espace des documents. Enfin de manière à optimiser la recherche et la navigation dans les documents, nous avons procédé à la spéciali-sation de la classe « Document géographique ». Cette classe regroupe des documents qui se différencient à la fois par leur forme et par leur fond. Nous avons choisi de la spécialiser en créant sept « sous-classes » qui décrivent chacune un type particulier de document géogra-phique. On peut ainsi associer des attributs spécifiques aux sous-classes qui caractérisent plus précisément certains types de documents. L\u0027un des avantages de GEOdoc repose sur la structure évolutive de sa base de données. À l\u0027image des réseaux ouverts, les utilisateurs pourront proposer les informations supplémentaires qu\u0027ils souhaitent voir apparaître lors de la navigation dans le réseau. Un premier pas vers une forme d\u0027intelligence collective au service du diagnostic de territoire.\n-53 -RNTI-E-13\nGEOdoc\nFIG. 2 -Extraits du modèle de données de GEOdoc\nFonctionnalités de GEOdoc : cas d\u0027usage type\nLes fonctionnalités de GEOdoc se déclinent ainsi selon trois catégories principales : (1) la recherche de documents ou d\u0027acteurs (les objets) à l\u0027intérieur du réseau (ouvert), (2) la navigation dans ce réseau, et (3) l\u0027enrichissement du réseau sur le principe de l\u0027intelligence collective. De manière à illustrer le fonctionnement de GEOdoc, nous proposons dans les sections suivantes d\u0027explorer un cas d\u0027utilisation représentatif et de l\u0027expliciter en parcourant ces familles de fonctionnalités. À ce stade du projet, ce cas n\u0027a pas fait l\u0027objet d\u0027un test formel. Ces étapes de tests fonctionnels fera l\u0027objet d\u0027une deuxième phase du projet. \nPlaçons nous dans\nLa recherche spatialisée de documents\nFIG. 3 -Ecran de recherche de la maquette de GEOdoc\nLa première fonctionnalité dépend principalement de l\u0027organisation de la base de données. Elle doit néanmoins permettre à l\u0027utilisateur d\u0027effectuer une recherche sur des critères géographiques et/ou sémantiques (figure 3). La recherche sur des critères géographiques permet de sélectionner le territoire d\u0027intérêt grâce à des outils simples de navigation cartographique ou bien à l\u0027aide d\u0027une requête tout aussi simple sur un localisant (un nom de lieu ou les coordonnées d\u0027une zone) -dans notre cas, le chargé d\u0027étude choisi de sélectionner la ville de Québec et plus spécifiquement la basse ville de Québec. Lors de la première utilisa--55 -RNTI-E-13 GEOdoc tion, nous faisons l\u0027hypothèse que la base de données a été peuplée, dans une version minimale, par le chargé d\u0027études impliqué dans le projet. Il est intéressant en effet de pouvoir restreindre le réseau aux documents ou aux acteurs pertinents pour le diagnostic à réaliser. L\u0027utilisateur accède alors à une information ciblée, ce qui limite les pertes de temps liées à la lecture de résultats non pertinents. Les utilisateurs ont le choix entre différents types de recherche et peuvent, au besoin, accéder à un document ou un acteur en particulier.\nLa navigation dans le réseau « géo-localisé » (documents et acteurs)\nLa navigation dans le réseau repose sur les concepts étudiés plus haut dans l\u0027article. Elle est accessible après avoir réalisé une recherche sur des documents ou des acteurs -dans notre cas d\u0027utilisation, le chargé d\u0027études a lancé une recherche de documents cartographiques relatifs à la répartition spatiale de la population par âge (figure 4). L\u0027affichage du réseau dépend de l\u0027option de recherche choisie par l\u0027usager. On peut décomposer le réseau en trois composantes (figures 4 et 5) :\n-la première composante renvoie à une formalisation de l\u0027organisation du territoire sur lequel porte le diagnostic autour de thèmes et de sous-thèmes, permettant ainsi de classer les documents (basé sur un principe d\u0027ontologie), -la deuxième composante renvoie aux documents géographiques qui sont rattachés aux différents sous-thèmes, -la troisième composante représente les acteurs qui sont en interaction avec le territoire d\u0027étude par l\u0027intermédiaire des documents géographiques.\nFIG. 4 -Ecran de navigation (1) de GEOdoc\n-56 -RNTI-E-13\nIl n\u0027y a aucune discontinuité entre chacune des composantes, mais une symbolisation graphique adaptée permet de distinguer les objets. Ces objets peuvent être des concepts, des documents géographiques ou bien des acteurs. Les concepts sont rattachés à la notion de thésaurus ou d\u0027ontologie. Ils permettent à un utilisateur d\u0027accéder à une classification des documents selon une structure du type : Domaines Thèmes Sous-thèmes. Il s\u0027agit là des balises qui forment le cadre de référence et aident un utilisateur à se situer dans le réseau (qui peut être complexe selon le nombre de documents). Cette classification a été choisie de façon à fournir une première organisation. Pourtant, rien ne dit que c\u0027est la plus adaptée à l\u0027utilisation que souhaitent en faire les acteurs. Il conviendrait sans doute d\u0027établir une structure en accord avec les acteurs et décideurs locaux pour construire une ontologie plus adaptée à des projets de territoire qui regroupent de plus en plus de domaines.\nChaque fois que l\u0027utilisateur se déplace dans le réseau, ses actions sont sauvegardées dans un historique. Ainsi, il lui est possible de reconstruire son cheminement (un peu à l\u0027idée des itinéraires méthodologiques développés par Lardon et al. (2001)), et de revenir sur un document ou un acteur qui aurait attiré son attention. L\u0027objectif consiste ici à fournir à l\u0027utilisateur les conditions qui lui permettront de se construire une vision claire du territoire sur lequel porte le diagnostic. L\u0027affichage des relations entre les concepts, les documents et les acteurs permet de mettre en perspective les connaissances disponibles.\nL\u0027intérêt fondamental de la navigation dans le réseau repose sur la possibilité de consulter les informations sur les documents et les acteurs -ainsi dans notre cas, le chargé d\u0027études peut immédiatement basculer depuis le document issu de la recherche initiale vers les acteurs reliés à ce document. Ces renseignements sont affichés sur la fenêtre de droite (figures 4 et 5), dès que l\u0027utilisateur clique sur un document ou un acteur dans la fenêtre centrale. Il peut alors avoir une idée du contenu du document avant même d\u0027y accéder physiquement. L\u0027utilisateur peut également choisir les données qu\u0027il souhaite voir présentées ; par exemple s\u0027il souhaite savoir avec quel logiciel il peut lire un document donné, il choisit d\u0027afficher le champ dans le cadre texte de droite. Il en est de même avec les informations sur les acteurs. Cette option permet de personnaliser l\u0027interface et d\u0027adapter GEOdoc aux besoins de l\u0027utilisateur.\nLors de l\u0027affichage des noms des documents, chaque objet est caractérisé par un pictogramme qui indique le type auquel il est rattaché (multimédia, image, représentation spatiale géoréférencée, etc.). Cette option visuelle permet de connaître rapidement les caractéristiques du document (figure 5). Lors de l\u0027affichage des relations entre un document et les acteurs ou des relations entre les acteurs eux-mêmes, un symbole graphique (points de couleurs) indique le type de relation. L\u0027usager doit pour cela cliquer sur le symbole. Ces symboles permettent non seulement d\u0027alléger l\u0027affichage graphique mais aussi de renseigner sur l\u0027importance de la relation. Ainsi, plus un acteur entretient de relations avec un autre acteur, plus le symbole est foncé. Il est alors facile de voir, avec cette pondération, comment s\u0027organise et se structure le réseau d\u0027acteurs. Il est aussi possible pour un même acteur de visualiser soit les acteurs qui interagissent avec lui, soit les documents géographiques qui lui sont liés (figure 5).\n-57 -RNTI-E-13\nGEOdoc\nFIG. 5 -Ecran de navigation (2) de GEOdoc\nEnrichissement du réseau par l\u0027utilisateur\nOutre l\u0027attention accordée à la simplicité et à l\u0027ergonomie de l\u0027interface de GEOdoc, les recherches sur les nouveaux concepts d\u0027intelligence collective et de réseaux ouverts nous ont poussés à réfléchir à la problématique d\u0027appropriation de cet outil par les utilisateurs potentiels. Ainsi, des possibilités sont offertes aux usagers de concevoir éventuellement l\u0027organisation même de l\u0027outil, ou pour le moins de contribuer explicitement à son évolution et à la spécification de certaines fonctionnalités. Il est possible par exemple d\u0027ajouter des commentaires à une donnée afin d\u0027apporter des renseignements qui ne figureraient pas dans la base de données. Chaque utilisateur peut ainsi faire un commentaire qui sera ensuite réfé-rencé et consultable par l\u0027ensemble de la communauté d\u0027usagers -dans notre cas, après exploration, le chargé d\u0027étude se rend compte que les documents qu\u0027il a trouvé relativement à la répartition de la population ne sont pas compatibles avec le niveau de granularité des questions soulevées par l\u0027assemblée de quartier, il peut ainsi le noter directement par le biais de l\u0027interface. Cette option peut constituer une forme de forum de discussion qui permettrait à chacun de s\u0027exprimer sur les éléments de diagnostic.\nLa possibilité de faire évoluer la base de données est aussi une caractéristique fondamentale de GEOdoc. Les informations qu\u0027elle contient sur les documents et les acteurs ne sont pas exhaustives et d\u0027autres attributs de classes peuvent sans doute être proposés. Cela permettrait d\u0027enrichir, grâce à l\u0027expérience de chacun, une base de données collective et d\u0027en améliorer à la fois le contenu et la structure. Il serait alors intéressant que des utilisateurs -58 -RNTI-E-13 S. Roche et al. puissent eux-mêmes ajouter, modifier ou supprimer des documents. Cette possibilité pose la question de la gestion de ces droits, un problème technique, en particulier, sur lequel nous n\u0027avons pas travaillé dans le cadre de cette recherche. Sur ce point en particulier, l\u0027implantation d\u0027un moteur du type WikiSIG tel que proposé par Ciobanu et al. (2007) est la solution que nous envisageons de tester dans la phase 2.\nGEOdoc sur ce point en particulier, se situe en marge des outils existants. Il n\u0027a pas pour objectif de proposer une aide directe à la réalisation de nouvelles analyses, ni même de proposer des solutions à un problème territorial à partir de critères prédéfinis. Il vise plutôt à faciliter deux aspects très importants du diagnostic de territoire : l\u0027accès à des documents géographiques de formes très diverses d\u0027une part, la consultation de ces documents et leur mise en perspective dans le réseau qui forme le territoire d\u0027autre part. Le simple fait de visualiser une liste de documents n\u0027est pas suffisant pour comprendre les caractéristiques et les dynamiques d\u0027un territoire. Il faut mettre les documents en relation et les inscrire dans un réseau d\u0027acteurs complexe. C\u0027est donc à partir de la structure de la base de données et des fonctionnalités de l\u0027outil que ces buts peuvent être atteints. Il ne faut pas perdre de vue ici que l\u0027objectif principal de GEOdoc est de faire participer les utilisateurs. Il s\u0027agit de leur permettre d\u0027ajouter, de modifier ou de supprimer un document ; de les faire acteurs de la construction d\u0027une intelligence territoriale collective. Cette solution est assimilable aux concepts de Peer to Peer utilisés par d\u0027autres logiciels. Elle a l\u0027énorme avantage de fonctionner sur une base de données mise à jour constamment et une richesse de documents très importante. Cette solution pour être parfaitement efficace, suppose que GEOdoc fonctionne sur Internet -dans notre cas, le chargé d\u0027étude peut informer en temps réel le conseiller de ce qu\u0027il a trouvé en l\u0027invitant à se connecter. Tous les acteurs et plus généralement toutes les personnes intéressées pourraient alors avoir accès à des informations diverses et variées sur un territoire, disposant ainsi des supports nécessaires pour contribuer au diagnostic.\nConclusion\nLe diagnostic de territoire repose sur un processus de concertation entre acteurs rattachés à des groupes sociaux divers et variés. Ce processus permet de préparer une action de déve-loppement. Il met en jeu de nombreux documents géographiques dont l\u0027analyse et la synthèse débouchent sur des prises de décisions. Il nécessite également une méthodologie éprouvée qui associe des concepts d\u0027analyse à l\u0027expérience pratique. La difficulté de la dé-marche réside dans la nécessité d\u0027aboutir à un consensus. Il faut en effet concilier d\u0027une part les visions de chaque acteur sur le territoire, et d\u0027autre part leurs objectifs dans le projet de territoire. Une discussion sur les atouts et les faiblesses d\u0027un territoire nécessite en outre une connaissance approfondie de ce dernier. Or, c\u0027est à ce niveau-là que les acteurs impliqués connaissent parfois quelques difficultés : méconnaissance ou inaccessibilité des documents pertinents produits sur un (ou à propos d\u0027un) un territoire donné.\nEn réponse à cette problématique, nous avons conçu une maquette d\u0027outil de navigation et de visualisation de documents géographiques d\u0027un nouveau type. GEOdoc n\u0027a pas pour but de proposer des outils d\u0027analyses de données géographiques ou un support direct à la prise de décision. Son objectif central est de rendre accessible et lisible aux acteurs impliqués dans un diagnostic de territoire le réseau dont ils font partie. GEOdoc offre des fonctionnali--59 -RNTI-E-13 6 Références\n"
  },
  {
    "id": "847",
    "text": "Introduction\nL\u0027étude de la migration des termes, en particulier de l\u0027évolution des données relationnelles issues de la synthèse de grands corpus d\u0027information est un aspect majeur dans l\u0027ingénierie de la connaissance et en particulier dans le cadre de la veille. Dans ce contexte, le recours à la visualisation de données par des graphes apporte un réel confort aux utilisateurs, qui, de façon intuitive, peuvent s\u0027approprier une forme de connaissance difficile à décrire autrement. Bien souvent, ces graphes sont trop complexes pour être étudiés dans leur globalité, il faut alors les décomposer de manière à faciliter la lecture et l\u0027analyse des données. Une première simplification du graphe est réalisé par le biais de la classification en un graphe réduit dont les sommets représentent chacun un groupe distinct d\u0027acteurs ou de termes du domaine. D\u0027autre part, la décomposition en graphes de périodes simplifie la structure de la représentation, en prenant en compte la dimension temporelle.\nDans ce contexte, le prototype VisuGraph, module de la plate-forme de veille stratégique Tétralogie, permet déjà la visualisation de données évolutives. Basée sur une visualisation globale, toutes périodes confondues, puis individuelle, les données sont représentées sous forme de sommet dont les coordonnées traduisent les caractéristiques temporelles.\nCe prototype est aussi doté de la classification interactive de données relationnelles, basée sur une technique de Markov Clustering, qui permet à la fois d\u0027obtenir des classes homogènes et le graphe réduit dont les sommets sont les classes obtenues. Nous l\u0027avons aménagée pour pouvoir intervenir sur le nombre de classes (augmentation, diminution), mais celui-ci reste assez aléatoire. Nous proposons alors de prendre en compte la dimension temporelle afin d\u0027analyser l\u0027évolution des différentes classes de données obtenues par l\u0027algorithme MCL au cours du temps. Cette technique prend en compte la topologie du graphe (diamètre, centralité, adjacence, flux, …) dans un contexte évolutif, tout en s\u0027appliquant à une métrique algébrique classique : la distance.\nDans la première partie de cet article, la plate forme de veille Tétralogie est présentée, en mettant l\u0027accent sur l\u0027extraction et le traitement des données. Puis, le module de représenta-tion graphique VisuGraph est proposé. Dans la section 3, nous détaillons notre proposition, en développant l\u0027analyse évolutive des classes de données, que nous expérimentons. Enfin, dans la dernière section, nous concluons sur nos travaux et présentons nos perspectives.\nPrésentation de Tétralogie\nTétralogie (Dousset et al., 1988) est un logiciel de macro-analyse de données textuelles semi-structurées intégrant la dimension temporelle. Les données analysées par la plateforme Tétralogie sont issues de bases de données, de revues, de journaux, de périodiques, de revues de veille technologique, des thèses et de brevets ou encore de CDROMS. Les informations extraites de ces sources sont synthétisées sous forme de matrices de cooccurrence, exploitables dans les différents modules proposés par Tétralogie.\nLes unités de base de toute analyse sont le terme, le champ (auteur, mots-clefs, adresse, date, ...) et le document. Un champ est une balise prédéfinie de la base de donnée semistructurée, par exemple auteur, date, adresse, organisme. Un champ, peut être mono-valué (journal) ou multi-valué (auteur, mot-clef,…). Un terme est une unité textuelle correspondant au contenu d\u0027un champ mono-valué ou une partie d\u0027un champ multi-valué délimité par des séparateurs. Les données analysées peuvent être croisées entre deux champs, sous champs ou groupes de champs afin d\u0027obtenir des matrices de fréquence, de présence/absence ou encore de co-occurrence (une des variables peut contenir plusieurs champs : auteurs, mots, clés…) sur lesquelles porteront ensuite les analyses. Dans le cas de données évolutives, un champ relatif au temps est pris en compte. Dans un contexte temporel, on aura autant de matrices de croisement que de périodes (appelées aussi « instances ») étudiées. Chaque croisement au sein d\u0027une matrice est appelé « valeur de métrique » d\u0027un ou plusieurs champs. Certaines informations sont sémantiquement équivalentes ou hiérarchisées, aussi est-il très utile de disposer de la fonctionnalité de Tétralogie permettant de radicaliser les données en les transformant en données simples, de les nettoyer mais aussi de normaliser ou d\u0027homogénéiser les termes (adresse, organisme, date), tout en contrôlant qu\u0027il n\u0027y ait pas d\u0027ajout ou encore d\u0027oubli de caractère au sein de chaque terme .\n3 La visualisation évolutive de données relationnelles 3.1 La prise en compte de la dimension temporelle Le prototype VisuGraph est un module de visualisation, offrant à la plateforme Tétralogie la possibilité de représenter les données matricielles sous forme de graphe. Dans cet article, nous ne traitons que des graphes non orientés. Considérons le graphe non orienté G \u003d (S,A) où S est l\u0027ensemble fini des éléments appelés sommets ou encore noeuds. A est l\u0027ensemble fini des liens appelés arêtes, liant les sommets. A ? S x S \u003d ?(s,t) ? s,t ? S? Chaque sommet est assimilé à la valeur de la métrique d\u0027un seul champ alors que chaque lien correspond à la valeur du croisement de deux champs. Afin d\u0027obtenir un graphe planaire, sans qu\u0027aucune arête n\u0027en croise une autre, nous avons recours à l\u0027algorithme « force-directed placement » (FDP) (Eades, 1984), assimilant les sommets à des masses et chaque arête d\u0027un graphe à un ressort reliant les sommets. Un tel système engendre des forces entre les sommets, ce qui entraine des déplacements respectifs. Après une phase de transition le système se stabilise. La condition d\u0027arrêt est un nombre maximum d\u0027itérations.\nDans ce cas d\u0027analyse temporelle, un graphe global représente toutes les données, toutes périodes comprises, puis chaque graphe de période est visualisé individuellement, réalisant ainsi une animation. Dans ce contexte évolutif, nous attribuons un sommet virtuel (non visible dans le dessin mais dont la présence est prise en compte dans le graphe) qui servira de repère pour chaque période considérée. Ces sommets virtuels sont fixés dans un ordre chronologique et de façon équidistante sur le contour de la fenêtre de visualisation (comme les heures sur un cadran) (Loubier2, 2007). Le dessin de graphe est influencé par l\u0027attribution de nouveaux arcs reliant chacun des sommets aux repères temporels, qui le concernent, en leur attribuant un poids plus important que la valeur de la métrique d\u0027arête la plus grande. Ceci engendre un déplacement, vers certains repères, en fonction de la plus ou moins forte pré-sence d\u0027un sommet dans chaque période.\nSimplification sous forme de graphe réduit\nAfin de faciliter l\u0027analyse, les données les plus fortement liées doivent être regroupées en classes homogènes. Parmi les travaux effectués sur le partitionnement de graphe, (Alpert et al. 1995, Kuntz et al. 2000, Jouve et al. 2001) se basent sur des approches spectrales alors que les algorithmes de la famille METIS (Karypis et al. 1998) se basent sur le partitionnement multi niveaux. La méthode de partitionnement utilisée dans VisuGraph est inspirée du Markov Clustering (Van Dongen 2000) que nous avons aménagée pour pouvoir influencer le nombre de classes proposées (Karouach, 2003). Cette approche calcule des probabilités de transition entre tous les sommets du graphe en partant de la matrice de transition des marches aléatoires. Deux simples opérations matricielles sont successivement itérées. La première calcule les probabilités de transition par des marches aléatoires de longueur fixée r et correspond à une élévation de la matrice à la puissance r. La seconde consiste à amplifier les diffé-rences en augmentant les transitions les plus probables et en diminuant les transitions les moins probables. Les transitions entre sommets d\u0027une même communauté sont alors favorisées et les itérations successives des deux opérations conduisent à une situation limite dans laquelle seules les transitions entre sommets d\u0027une même communauté sont possibles.\nSoient un graphe G \u003d (V,w) où w : V x V AE R + et M G la matrice associée à G. T G est la matrice normalisée (somme des poids des arcs sortants \u003d 1). Soit T pq la probabilité d\u0027effectuer la transition p AE q(T \u003d T G ). L\u0027expansion s\u0027effectue par multiplication des matrices, visant à élargir la capacité de l\u0027arc entre deux noeuds. L\u0027inflation d\u0027une colonne vise la promotion des voisins favoris au détriment de ceux moins favoris.\nL\u0027évaluation de la méthode MCL a montré la rapidité et la qualité de ses résultats (Enright et al. 2002). Le graphe obtenu est alors un graphe de classe, pour lequel chaque sommet représente une classe. Les liens entre les sommets sont assimilés aux liaisons interclasses. Dans un second temps, l\u0027attribution d\u0027une couleur spécifique à chaque classe permet de visualiser le graphe complet, en figeant un représentant par classe et en distribuant les autres sommets sur une couronne centrée sur ce dernier, permettant ainsi une vue intra classe.\nFIG. 1 : Visualisation du graphe global des classes, puis visualisation par animation des graphes de classe par période (« Rep n » ÅAE repère temporel de la nième période).\nConclusion\nDavantage qu\u0027une simple recherche, la veille consiste à recueillir l\u0027information, à la synthétiser et à tirer des conclusions aidant à la prise de décision en anticipant les tendances. L\u0027outil VisuGraph permet le prétraitement des données, en particulier dans la gestion de la synonymie, et la synthèse des données relationnelles sous forme de matrices de cooccurrences décomposées en périodes homogènes. En se basant sur ces matrices, les données peuvent être représentées dans un premier temps, sous forme de graphe de classes global, toutes tranches de temps confondues, puis dans un second temps successivement sous forme de graphe de période. De par la prise en compte de la dimension temporelle, la position des sommets du graphe est stratégique et spécifiques aux caractéristiques liées au temps, telles que l\u0027appartenance à certaines périodes et non à d\u0027autres. Ainsi, chaque portion de la fenêtre de représentation correspond à une typologie relative au temps particulière. Cependant les limites de cette proposition concerne la méthode de classification choisie, le Markov Clustering MCL, qui ne prend pas en compte le point de vue de l\u0027utilisateur, et, dans certains cas, le RNTI -X -partitionnement est soit trop fin soit trop grossier (à la limite une seule classe est trouvée). Il conviendrait d\u0027offrir à l\u0027utilisateur la possibilité d\u0027intervenir dans la classification par une analyse visuelle des regroupements qui s\u0027opère lorsqu\u0027on joue sur les paramètres permettant de dessiner au mieux le graphe.\nRéférences\n"
  },
  {
    "id": "848",
    "text": "Introduction\nL\u0027extraction d\u0027itemsets fréquents est une problématique de recherche qui intéresse la communauté fouille de données depuis plus d\u0027une dizaine d\u0027années et intervient pour la recherche de règles d\u0027association, de motifs séquentiels ou encore d\u0027itemsets maximaux. Les premiers à traiter cette question furent Agrawal et Srikant (1994), ils ont été suivis en ce sens par Han et al. (2000). Traditionnellement, les différents algorithmes proposés dans la littérature reposent sur des structures de données de type arbre ou encore treillis (e.g. : A-priori (Agrawal et Srikant, 1994), F P-growth (Han et al., 2000), . . . ). La problématique de recherche de motifs (i.e., une généralisation des itemsets) apparaît dans des domaines aussi variés que la bioinformatique ou la fouille de textes. En ce qui concerne ce dernier, de nouvelles structures de données, basées sur des automates sont apparues afin d\u0027extraire les sous-séquences communes à une ensemble de textes (Troní?ek, 2002). Par exemple, Hoshino et al. (2000) ont introduit, un nouvel automate déterministe et acyclique : le SA (Subsequence Automaton) qui permet de reconnaître toutes les sous-séquences d\u0027un ensemble de textes. L\u0027un des problèmes principaux auxquels doit faire face une approche d\u0027extraction de motifs est de disposer de structures qui soient suffisamment compactes et informatives afin de minimiser l\u0027explosion combinatoire liée à d\u0027importants espaces de recherche. En effet, l\u0027applicabilité des algorithmes proposés peut être remise en question en raison des coûts trop prohibitifs en temps de calcul et en espace mémoire utilisé. Le premier objectif de cet article est d\u0027apporter une réponse à la question suivante : est-il possible de trouver de nouvelles structures de données, suffisamment informatives et compactes, pour extraire de façon efficace les itemsets fréquents ?\nRécemment, pour faire face au fait que les données peuvent être disponibles sous la forme de flots qui arrivent de manière continue et sont éventuellement en quantités infinies, les chercheurs de la communauté fouille de données se sont intéressés à l\u0027extraction de connaissance dans de telles conditions. De nombreuses applications (e.g. transactions financières, navigation sur le Web, téléphonie mobile, . . . ) rentrent dans ce cadre et nécessitent d\u0027obtenir des résul-tats rapidement. Dans le cas de la problématique d\u0027extraction d\u0027itemsets fréquents, la prise en compte de données disponibles sous la forme de flots engendre de nouvelles problématiques. En premier lieu, il est indispensable de considérer les aspects liés à la mise à jour. En effet, étant donné que les données arrivent de manière continue, la base de données est sujette à des mises à jour régulières et fréquentes. Ainsi, la connaissance obtenue pour une base à un moment donné n\u0027est plus forcément valable lorsque de nouvelles données arrivent et il n\u0027est pas envisageable de relancer l\u0027algorithme sur toute la base mise à jour. La maintenance de connaissance incré-mentale a, par exemple, été étudiée dans Masséglia et al. (2003) où les auteurs proposent de maintenir la connaissance au fur et à mesure des mises à jour successives. Malheureusement, tous les travaux de recherche basés sur une approche incrémentale ne sont pas adaptés aux flots dans la mesure où nous ne pouvons pas disposer de la base dans son intégralité. Il est donc né-cessaire, en second lieu, de disposer de nouveaux algorithmes qui effectuent une seule passe sur la base (i.e., des algorithmes une-passe). Les travaux récents sur les flots ont montré qu\u0027il n\u0027était plus envisageable d\u0027obtenir une réponse exacte (i.e., les itemsets réellement fréquents sur le flot) et qu\u0027il fallait accepter une approximation quant à l\u0027estimation de la fréquence des motifs : les itemsets obtenus ne sont en fait que des itemsets fréquents observés. Il faut donc prendre en compte l\u0027incertitude engendrée par la connaissance toujours incomplète du flot de données. Il est important de noter que dans les flots, des itemsets classés comme non fréquents peuvent le devenir sur une plus longue période d\u0027observation et inversement, des itemsets observés fréquents peuvent ne plus l\u0027être après un certain temps. Dans Vapnik (1998), l\u0027auteur a montré qu\u0027il est statistiquement impossible de s\u0027affranchir de ces deux sources d\u0027erreurs à partir de la connaissance d\u0027une partie (même très grande) du flot. On peut toutefois chercher à minimiser l\u0027une des sources d\u0027erreurs tout en maintenant l\u0027autre en dessous d\u0027un seuil raisonnable. La seconde question à laquelle nous nous intéressons dans cet article est la suivante : est-il possible de trouver une structure de données ayant des propriétés incrémentales satisfaisantes et qui permette de construire et de la maintenir de façon efficace l\u0027ensemble des itemsets fréquents du flot de données tout en minimisant l\u0027une ou l\u0027autre des sources d\u0027erreurs ?\nLa suite de l\u0027article est organisée de la manière suivante. La Section 2 présente plus formellement la problématique. Dans la Section 3, nous proposons un aperçu d\u0027autres travaux abordant cette problématique. La Section 4 présente notre approche et nos solutions. Les expé-rimentations sont décrites dans la Section 5 et une conclusion est proposée dans la Section 6.\nProblématique\nSoit I \u003d {i 1 , i 2 , . . . , i m } un ensemble d\u0027items muni d\u0027une relation d\u0027ordre utilisés dans une base de données DB de transactions, où chaque transaction t r identifiée de manière unique est associée à un ensemble d\u0027items de I. Un ensemble X ? I est appelé un p-itemset et est RNTI -E -2 représenté par {x 1 , x 2 , . . . , x p }. L\u0027entier p \u003d |X |, est la longueur de X et Sub(X ) l\u0027ensemble de tous les sous-itemsets de X , c\u0027est à dire les itemsets obtenus en supprimant zéro ou plusieurs items de X . Le support d\u0027un itemset X , noté supp(X ), correspond au nombre de transactions dans lesquelles l\u0027itemset apparaît. Un itemset est dit ?-fréquent si supp(X ) ? ?, où ? \u003d ?? × |DB|? correspond au support minimal (généralement spécifié par l\u0027utilisateur) avec ? ?]0; 1] et |DB| la taille de la base de données. Le problème de la recherche des itemsets fréquents consiste à rechercher tous les itemsets dont le support est supérieur ou égal à ? dans DB. Cette problématique, étendue au cas des flots de données, peut s\u0027exprimer comme suit. Soit un flot de données DS \u003d B La fréquence d\u0027un itemset X à un instant donné t est défini comme étant le ratio du nombre de transactions qui contiennent X dans les différents batches sur le nombre total de transactions connu à l\u0027instant t. Ainsi, pour un support minimal fixé par l\u0027utilisateur, le problème de la recherche des itemsets fréquents dans un flot de données consiste à rechercher tous les itemsets X qui vérifient bi ai supp(X ) ? ?? × k? dans le flot. Dans l\u0027exemple de la Table 1  \nTravaux antérieurs\nLes différents travaux portant sur la problématique d\u0027extraction d\u0027itemsets fréquents dans les flots de données se déclinent selon trois axes en fonction du modèle de traitement des itemsets du flot. Le premier utilise des fenêtres à point fixe où sont conservés tous les itemsets acquis du flot (cf. Manku et Motwani, 2002;Li et al., 2004). Le second axe est différent du pré-cédent simplement par le fait que l\u0027on introduit une distinction entre les itemsets récemment et moins récemment acquis. Chang et Lee (2004b) attribuent un poids décroissant aux transactions en fonction de l\u0027ancienneté de leur acquisition. Autrement dit, les anciennes transactions contribuent moins que les nouvelles au calcul de la fréquence des itemsets. Par exemple, Giannella et al. (2004) utilisent une structure de type FP-tree pour rechercher des itemsets fré-quents à différents niveaux de granularité temporelle. Le dernier axe concerne l\u0027extraction à partir de fenêtres glissantes où l\u0027on ne considère plus seulement l\u0027acquisition mais aussi le retrait d\u0027itemsets (cf. travaux de Chang et Lee, 2004a). L\u0027approche que nous développons dans cet article, s\u0027inscrit dans le premier axe. Aussi, nous préciserons dans la suite de ce para-RNTI -E -3 graphe, les caractéristiques des algorithmes ainsi que l\u0027erreur et les types d\u0027approximation sur les résultats relatifs à cet axe. Manku et Motwani (2002) ont développé un algorithme : Lossy counting, basé sur la propriété d\u0027antimonotonie du support. Cet algorithme effectue un seul passage sur les données et utilise une structure à base d\u0027arbres pour représenter les itemsets. Les auteurs introduisent un paramètre d\u0027erreur fixé par l\u0027utilisateur et voulu très inférieur au support afin de minimiser le nombre de résultats faux positifs et afin d\u0027améliorer la valeur de la fréquence obtenue des itemsets. Ils donnent les garanties suivantes sur leurs résultats : tous les itemsets réellement fréquents sont trouvés ; il n\u0027y a pas de faux négatifs ; tous les itemsets considérés fréquents à tort (i.e., les faux positifs) ont une fréquence proche de la fréquence voulue ; l\u0027incertitude sur la fréquence des itemsets est fonction du paramètre d\u0027erreur. Li et al. (2004) proposent d\u0027extraire les itemsets fréquents en partant des plus grands aux plus petits et utilisent une structure très compacte qui résulte d\u0027une extension d\u0027une représentation basée sur des arbres préfixés : le C F I-tree (Candidate Frequent Itemset tree). Toutefois, l\u0027algorithme développé : D S M-F I, bien qu\u0027effectuant un seul passage sur les données, comprend une phase d\u0027élagage du C F I-tree et nécessite plusieurs parcours de la structure pour obtenir l\u0027information sur la fréquence des itemsets. Les garanties apportées, quant aux résultats, indiquent qu\u0027il n\u0027y a pas de faux négatifs et que l\u0027erreur sur la fréquence des itemsets est bornée.\nNotre Approche\nDans un premier temps, nous nous intéressons à l\u0027extraction des itemsets fréquents dans une base de données. Nous introduisons un nouvel automate : le FIA (Frequent Itemset Automaton), qui constitue une structure de données très compacte et informative permettant d\u0027extraire de façon efficace tous les itemsets fréquents d\u0027une base de données. Dans un second temps, nous étendons cette approche à la prise en compte des flots de données et nous montrons comment mettre à jour incrémentalement le FIA lors de l\u0027ajout de nouveaux batches issus du flot. Cependant, pour tenir compte de l\u0027incertitude engendrée par la connaissance toujours incomplète du flot, nous étudions la représentation de la bordure statistique à l\u0027aide du FIA afin de développer une approche prédictive (Laur et al., 2007). En effet, plutôt que d\u0027extraire des itemsets observés fréquents sur la partie connue du flot, nous considérons qu\u0027il est préférable de prédire les itemsets véritablement fréquents sur tout le flot à partir des itemsets connus.\nRappels sur la théorie des automates\nNous présentons dans cette section, les principes fondamentaux de la théorie sur les automates finis (cf. Hopcroft et Ullman, 1990) qui seront utilisés dans la suite.\nDéfinition 1. Un automate à états finis A est un quintuple tel que\nUn mot est reconnu s\u0027il est l\u0027étiquette d\u0027un chemin réussi. Le langage reconnu par l\u0027automate A est l\u0027ensemble des mots reconnus par A, soit L(A) \u003d {w ? ?|?c : i w ? ? f, i ? I, l ? F }. Un état q ? Q d\u0027un automate RNTI -E -4 A \u003d (Q, ?, ?, I, F ) est accessible s\u0027il existe un chemin c : i ? q avec i ? I. De même, l\u0027état q est coaccessible s\u0027il existe un chemin c : q ? f avec f ? F. Un automate est émondé si tous ses états sont accessibles et coaccessibles. Soit P l\u0027ensemble des états accessibles et coaccessibles et soit A 0 \u003d (P, ?, ? ? (P × ? × P), I ? P, F ? P), l\u0027automate A 0 est émondé par construction. Comme tout chemin réussi de A ne passe que par des états accessibles et coaccessibles, on a L(A 0 ) \u003d L(A). Les automates A 0 et A sont dits équivalents.\nDéfinition 2. Un automate à états finis\nNous adaptons les Définitions 3 et 4 proposées initialement par Hoshino et al. (2000) pour l\u0027automate des sous-séquences (SA), au cas des itemsets. \nDéfinition 3. Étant donnés un ensemble S d\u0027itemsets tel que\nL\u0027automate des itemsets fréquents : le FIA\nDans cette section, nous introduisons la définition d\u0027un nouvel automate : le FIA ? qui intègre la notion de fréquence et qui permet de reconnaître l\u0027ensemble des itemsets fréquents d\u0027une base de données.  \nDéfinition 5. Étant donnés un ensemble\nIl est aisé de constater que les états qui ne sont pas ?-satisfaisants ne sont pas coaccessibles (le contraire signifierait qu\u0027un itemset non fréquent est inclus dans un itemset fréquent). Ainsi, le FIA ? émondé s\u0027obtient en ne construisant que les états ?-satisfaisants accessibles. Un simple algorithme glouton permet de le construire et ne requiert en aucun cas une phase d\u0027élagage. Le support d\u0027un itemset étiquetant un chemin de l\u0027état initial à un état q donné du FIA ? (S) s\u0027obtient en calculant le nombre de valeurs qui ne sont pas égales à ? dans le ppos associé à q. Ainsi, un itemset étiquetant q \nS) correspondant, pour tout état q tel qu\u0027il n\u0027existe pas de transition vers un état q ? de même support, le plus long itemset reconnu en q est un itemset fermé (il est unique). Réciproquement, tous les itemsets fermés sont reconnus dans des états q tels qu\u0027il n\u0027existe pas de transition menant à un état q ? de même support.\nEn considérant le batch B 1 0 de la Table 1, nous montrons, sur la Figure 1, le FIA ? émondé pour ? \u003d 0, 4 et donc ? \u003d 2. Les états finaux sont repérés par un double cercle et l\u0027état initial est représenté avec une flèche entrante sans label. L\u0027itemset vide est reconnu à l\u0027état initial (avec le support 5). Par ailleurs, nous observons que les itemsets abd, ad et bd avec une même valeur de support à 2, sont reconnus à l\u0027état q 7 . Il en est de même pour les itemsets bce et ce de support 2 reconnus en q 10 et pour be et e de support 3 en q 5 . Le FIA ? , du fait de la Propriété 2 est une structure très compacte. En effet, seuls les états q 1 , q 6 et q 8 n\u0027identifient pas un itemset fermé (la Propriété 2 donne les équivalences suivantes pour les itemsets fermés : q 0 ? ?, q 2 ? b, q 3 ? c, q 4 ? d, q 5 ? be, q 7 ? abd, q 9 ? cd et q 10 ? bce). Enfin, si l\u0027on choisit la fréquence décroissante comme relation d\u0027ordre ? sur les items, à l\u0027instar de l\u0027algorithme F P-growth, le F P-tree résultant compte 11 noeuds mais 10 états pour le FIA dans ce cas. \nIntégration des bordures statistiques dans le FIA\nAppliquée au cas des flots de données la mise à jour du FIA requiert de connaître l\u0027ensemble des états accessibles, y compris des états non ?-satisfaisants. En effet, mettre à jour le FIA émondé reviendrait à considérer que les itemsets non reconnus par l\u0027automate ont tous un support à 0 ; ce qui engendrerait nécessairement un grand nombre de faux négatifs sur la totalité du flot. À l\u0027inverse, en considérant les états non ?-satisfaisants, un grand nombre d\u0027itemsets vrais négatifs seraient analysés inutilement. Afin d\u0027illustrer cet aspect, considérons d\u0027une part la représentation du FIA 40% émondé de la Figure 1 et d\u0027autre part la représentation de la Figure 2 du FIA 40% non émondé avec tous ses états accessibles. Les états q 11 , q 12 , q 13 , q 14 ne sont pas 2-satisfaisants mais 1-satisfaisants et ne sont donc pas finaux. La question revient à savoir quel est l\u0027automate qu\u0027il convient de considérer pour effectuer la mise à jour du FIA. Il est donc nécessaire de trouver un compromis entre ne conserver aucun état non ?-satisfaisant 1 En pratique, les ppos ne sont pas construits, seul le support est calculé et mis à jour.\nRNTI -E -7\nAutomate des Itemsets Fréquents: le FIA et tous les états accessibles. Idéalement, seuls les états qui correspondent à des itemsets vrais ?-fréquents du flot devraient être construits, quand bien même ils ne satisfont pas la contrainte de support à un instant donné. La solution que nous avons adoptée est d\u0027utiliser la bordure statistique supérieure présentée par Symphor et Laur (2006), dans laquelle est maximisé le rappel (cf. Définition 7 et Théorème 1 ci-après). Le Théorème 1 ci-dessous permet d\u0027établir la valeur du support statistique qui permet la construction de la bordure statistique supérieure.\nThéorème 1. Étant donné un flot de données observé DS\nles supports statistiques ?\nLes fréquences obtenues (? ? \u003d ?±?) sont statistiquement presque optimales (cf. Laur et al., 2007). Celle-ci repose sur l\u0027utilisation d\u0027inégalités de concentration de variables aléatoires, qui, dans ce cas précis, permettent d\u0027obtenir un résultat statistiquement presque optimal. Par optimalité, nous entendons que toute technique d\u0027estimation obtenant de meilleures bornes RNTI -E -8 est condamnée à se tromper (le critère à maximiser n\u0027est plus égal à un) quel que soit son temps de calcul. Dans le cas de la bordure statistique supérieure correspondant à ? ? \u003d ? ? ?, il s\u0027agit de réduire autant que possible le nombre de faux négatifs à savoir les itemsets véritablement fréquents du flot et qui ne sont pas retenus comme tels pour la partie observée du flot. Lorsque seuls les états de la bordure statistique supérieure ont été conservés (i.e., les états ?(? ? ?) × k?-satisfaisants), l\u0027automate obtenu après une mise à jour incrémentale est une approximation du FIA ? pour le flot observé (nous le noterons  \nL\u0027intérêt du calcul des P -valeurs est clairement illustré dans Denise et al. (2001). Ici, la P -valeur traduit littéralement que la probabilité qu\u0027il existe un faux positif (i.e., un itemset de fréquence observée ? ? qui ne soit pas ?-fréquent sur tout le flot), est inférieure à ?.\nRNTI -E -9\nAutomate Notre algorithme de construction du FIA est en une passe, mais pour la comparaison avec notamment F P-growth, nous avons utilisé une version deux-passes, le premier passage permettant uniquement de réordon-ner les items par ordre de fréquences décroissantes. Les résultats obtenus avec le FIA sont meilleurs sur une large plage de fréquence tant pour le temps pris que pour la mémoire requise. On observe effectivement, au-delà des valeurs de fréquence à 5%, un écart en temps de l\u0027ordre de 3 sec. et de mémoire de 1M o en faveur du FIA, qui prend un temps total de 4 sec. et consomme 3M o par rapport à F P-growth. L\u0027algorithme A-priori, qui effectue plusieurs passages sur les données, donne de meilleurs résultats seulement à partir des fréquences dépas-sant 45%. Toutefois, les performances du FIA sont dépendantes des données indexées. En effet, cette structure est d\u0027autant plus avantageuse que les itemsets fréquents sont grands. En contrepartie, lorsque les itemsets fréquents sont, majoritairement des singletons, ce qui est le cas pour de très faibles fréquences, le FIA tend à ressembler à un arbre lexicographique et l\u0027algorithme de construction devient inadapté au regard des méthodes classiques de construction de tels arbres. Le FIA est une structure d\u0027autant plus efficace que les données sont denses en informations à extraire. Sur les Figures 6 et 7, nous illustrons les résultats obtenus avec l\u0027algorithme du FIA incrémental en représentant le temps pris et la mémoire requise en fonction de l\u0027insertion de nouveaux batches (nombre constant de transactions) pour T10I4D100K. Le temps pris et la mémoire consommée demeurent stables. Cela montre l\u0027applicabilité de l\u0027algorithme du FIA incrémental dans le cas des flots de données.\nConclusion\nDans cet article, nous apportons une contribution originale en élaborant un nouvel automate : le FIA qui permet de traiter de façon efficace la problématique de l\u0027extraction des itemsets fréquents dans les flots de données. À notre connaissance, les automates en tant que structure de données n\u0027ont pas du tout été utilisés pour aborder cette question. Nous montrons que le FIA est une structure très compacte et informative car plusieurs itemsets fréquents ayant la même valeur de support sont reconnus à un même état. Par ailleurs, la structure indexe directement tous les itemsets fréquents sans qu\u0027il soit nécessaire de lui associer un tableau pour finalement obtenir les résultats. Le FIA présente également des propriétés incrémentales qui facilitent grandement la mise à jour dans le cas des flots de données avec une granularité très fine par batch. Utilisé dans le cadre d\u0027une approche prédictive, le FIA permet d\u0027indexer les itemsets véritablement ?-fréquents du flot en maximisant le rappel et en fournissant à tout moment une information sur leur pertinence statistique avec la P -valeur. Ces deux avantages permettent notamment de construire le FIA par incrément à partir d\u0027une base initiale vide. L\u0027algorithme développé, pour mettre à jour le FIA, ne requiert qu\u0027un seul passage sur les données qui sont prises en compte par batch, itemset par itemset et pour chaque itemset, item par item. Lors de l\u0027acquisition de nouveaux batches, la connaissance du flot augmentant, il est possible de mettre à jour la bordure supérieure. Celle-ci tend à diminuer (la valeur de ? tends vers 0, donc ? ? tends vers ?) au fur et à mesure des mises à jour. Il devient donc possible de construire le FIA ? à partir de l\u0027automate vide, est les premiers résultats obtenus (non présentés dans cet article, faute de place essentiellement) montrent clairement la robustesse de notre approche. Ceci est également étayé par les expérimentations présentées, avec une analyse en temps de calcul et en mémoire consommée, qui donnent des résultats satisfaisants et qui prouvent l\u0027applicabilité et le passage à l\u0027échelle de l\u0027algorithme. Notre contribution ouvre donc une voie prometteuse avec le FIA, quant à l\u0027utilisation de nouvelles structures de données de type automate, dans les problématiques d\u0027extraction de motifs fréquents dans les flots de données.\nRéférences Agrawal, R. et R. Srikant (1994 \n"
  },
  {
    "id": "850",
    "text": "FIG. 1 -Variation des histogrammes de la variable « DIVERS »\nFIG. 2 -Caractérisation de la classe 7 FIG. 3 -Représentation croisée des variables histogramme tarification et vague\nFIG. 4 -Une nouvelle visualisation 2D et 3D pour les pyramides et hiérarchies\n"
  },
  {
    "id": "851",
    "text": "Introduction\nUne base de données de grande taille est difficile à appréhender dans sa totalité. Pour palier ce problème, diverses techniques ont été créées afin de fournir des vues partielles ou d\u0027effectuer des regroupements de données par thèmes. De façon similaire il est difficile de comprendre une base de connaissances. Plus une base de connaissance est grande, plus le nombre de connaissances utilisables afin d\u0027effectuer une déduction est important. A partir d\u0027un certain nombre l\u0027humain ne peut plus évaluer toutes les connaissances mises en jeu dans une déduction. Il est donc nécessaire de diviser l\u0027ensemble des étapes d\u0027une déduction par paquets et de fournir à l\u0027humain une évaluation de chaque paquet. Cette évaluation peut être imprécise mais facilite la compréhension en donnant l\u0027idée générale. Pour notre étude nous nous intéressons à un modèle graphique de gestion de connaissances appelé cartes cognitives (Tolman, 1948).\nUne carte cognitive représente un réseau d\u0027influences entre concepts. Une influence est une relation de causalité entre deux concepts. L\u0027effet de l\u0027influence d\u0027un concept sur un autre peut être représenté de manière numérique ou symbolique. Ce type de représentation fournit un bon support à la communication entre humains dans le but d\u0027effectuer une analyse d\u0027un système complexe. Les cartes cognitives ont été utilisées dans de nombreux domaines tels que la biologie (Tolman, 1948) (Touretzky et Redish, 1995), l\u0027écologie (Celik et al., 2005) (Poignonec, 2006), la sociologie (Poignonec, 2006). Un mécanisme d\u0027inférence des influences dans une carte cognitive peut être défini, ce qui en fait un outil d\u0027aide à la décision. Ce type d\u0027outils a été utilisé par exemple en politique et en économie (Axelrod, 1976) (Cossette, 1994). La repré-sentation informatique d\u0027une carte cognitive et la mise en oeuvre d\u0027un calcul automatique de l\u0027inférence est relativement simple. L\u0027objectif de ce travail est de faciliter la compréhension et l\u0027exploitation de cartes cognitives de grandes tailles. Pour cela nous présentons un modèle de cartes cognitives permettant à l\u0027utilisateur d\u0027obtenir des vues partielles et synthétiques d\u0027une carte.\nLes vues partielles sont calculées à l\u0027aide de regroupements de concepts préalablement défi-nis par le concepteur de la carte sous la forme d\u0027une hiérarchie. Un regroupement de concepts est considéré comme une nouveau concept. Ce concept peut être choisi par l\u0027utilisateur afin qu\u0027il soit présent dans la vue partielle de la carte et ainsi remplacer l\u0027ensemble des concepts qu\u0027il regroupe. Lorsqu\u0027il existe une influence entre deux concepts appartenant à deux regroupements différents et que les regroupements sont remplacés par leurs concepts correspondants, il est nécessaire d\u0027ajouter une influence entre ces derniers dans la vue partielle. La représen-tation symbolique ou numérique de l\u0027effet de cette influence doit fournir à l\u0027utilisateur une idée de l\u0027effet des influences qu\u0027il existe entre des concepts de ces regroupements. Pour dé-terminer cet effet il est nécessaire de préalablement définir un mécanisme d\u0027inférence entre deux regroupements de concepts. Ce mécanisme est inspiré d\u0027un travail précédemment effectué (Genest et Loiseau, 2007) ainsi que de travaux ayant pour objectif de fusionner plusieurs cartes cognitives en utilisant des mécanismes de fusion de plusieurs influences en une seule (Axelrod, 1976)(Chaib-draa, 2002 (Fabiola Mata Avila, 2002) (Jung et al., 2003).\nLa première section de cet article présente un modèle standard de carte cognitive, dit modèle de carte cognitive simple et son mécanisme d\u0027inférence permettant de l\u0027exploiter. La seconde partie présente un mécanisme permettant à l\u0027utilisateur d\u0027interroger la carte afin de connaître l\u0027influence d\u0027un regroupement de concepts sur un autre. La troisième section pré-sente notre modèle de carte cognitive hiérarchique s\u0027appuyant sur une hiérarchie de concepts qui détermine des regroupements de concepts. La quatrième section fournit le mécanisme d\u0027inférence adapté aux cartes cognitives hiérarchiques. La dernière section décrit un mécanisme de visualisation partielle d\u0027une carte cognitive hiérarchique.\nUn modèle simple de cartes cognitives\nUne carte cognitive simple est un graphe orienté dont les noeuds sont étiquetés par des concepts. Une concept est représenté par un texte. Un arc du graphe représente une influence, c\u0027est à dire une relation de causalité possible entre deux concepts. Un arc porte un symbole caractérisant l\u0027effet de l\u0027influence qu\u0027il représente. Typiquement une influence est positive ou négative.\nDéfinition (Carte cognitive simple): Soit S un ensemble de symboles. Soit C un ensemble de concepts. Une carte cognitive simple définie sur C et S, est un multigraphe orienté étiqueté Un mécanisme de propagation de l\u0027influence dans une carte cognitive permet de détermi-ner l\u0027effet d\u0027un concept sur un autre (Axelrod, 1976). L\u0027influence propagée d\u0027un concept sur un autre est calculée en fonction des chemins qu\u0027il existe entre les noeuds étiquetés par ces concepts. On appelle ces chemins des chemins d\u0027influence.\n, une carte cognitive simple définie sur un ensemble de concepts C et un ensemble de symboles S. Soit c 1 , c 2 deux concepts de C. On appelle un chemin d\u0027influence P entre c 1 et c 2 une liste de\nExemple: Sur la carte cognitive simple de la figure 1, ((´ etiq\nAfin de connaître l\u0027effet d\u0027un concept sur un autre il est nécessaire de déterminer l\u0027ensemble des chemins d\u0027influences entre ces concepts. Il faut ensuite évaluer l\u0027influence propagée par chaque chemin d\u0027influence. Pour cela il est nécessaire de cumuler les symboles de chaque influence présente dans un chemin d\u0027influence. Classiquement l\u0027ensemble des symboles utilisés pour décrire l\u0027effet d\u0027une influence est {+, ?}. Nous nous limitons dorénavant à ces symboles afin de pouvoir fournir des mécanismes de calculs qui les utilisent.\nDéfinition (Influence propagée dans un chemin d\u0027influence):\n, une carte cognitive simple définie sur un ensemble de concepts C et l\u0027ensemble de symboles {+, ?}. L\u0027influence propagée dans un chemin d\u0027influence P est définie de la façon suivante:\navec est une application définie sur {+, ?} × {+, ?} ? {+, ?} représentée par la matrice:\nL\u0027influence propagée dans le seul chemin d\u0027influence entre Autoroute et Erreur de trajectoire est négative.\nOn remarque que l\u0027ensemble des chemins d\u0027influence entre deux notions peut être de dimension infinie car la carte cognitive peut contenir des cycles. Nous definirons donc l\u0027influence propagée entre deux concepts à l\u0027aide d\u0027un sous-ensemble fini de cet ensemble. Ce sous-ensemble est appelé l\u0027ensemble des chemins d\u0027influence minimaux.\n, une carte cognitive simple définie sur un ensemble de concepts C et un ensemble de symboles S. Soit P un chemin d\u0027influence entre deux concepts c 1 et c 2 de C. P est un chemin d\u0027influence minimal si et seulement si il n\u0027existe pas de chemin d\u0027influence P entre c 1 et c 2 tel que P est une sous-liste de P . On note P c1,c2 l\u0027ensemble des chemins d\u0027influence minimaux entre c 1 et c 2 .\nLe mécanisme de propagation de l\u0027influence entre deux concepts effectue des comparaisons des influences propagées dans les différents chemins d\u0027influence minimaux existants entre ces deux concepts. La valeur retournée par ce mécanisme peut être positive (notée +), négative (-), nulle (0) ou ambiguë ( ?). Cette valeur est positive (respectivement négative) quand tous les chemins d\u0027influence minimaux entre ces concepts ont une influence propagée positive (respectivement négative). Cette valeur est nulle quand il n\u0027y a pas de chemin d\u0027influence minimal entre ces concepts. Cette valeur est ambiguë lorsque deux chemins d\u0027influences minimaux ont une influence propagée de symboles différents.\nDéfinition (Propagation de l\u0027influence entre deux concepts):\n, une carte cognitive simple définie sur un ensemble de concepts C et l\u0027ensemble de symboles {+, ?}. Soit c u , c v deux concepts de C. L\u0027influence propagée entre c u et c v est définie de la façon suivante: \nDe nombreux travaux formalisent des mécanismes de propagation dans des cartes cognitives. Certains travaux représentent une carte cognitive sous la forme d\u0027une matrice d\u0027adjacence (Fabiola Mata Avila, 2002). Le mécanisme de propagation étant effectué à l\u0027aide d\u0027opé-rations matricielles. Définir ainsi le mécanisme de propagation a l\u0027avantage de fournir une évolution du système au cours du temps. D\u0027autres travaux quantifient de manière différente les influences. Leur principal objectif étant de lever automatiquement les ambiguités obtenues au cours de la propagation de l\u0027influence. Ces travaux s\u0027appuient souvent sur la logique floue (Zadeh, 1965) et définissent des cartes cognitives floues (Kosko, 1992) (Taber, 1991) (Perusich, 1996) où les influences sont pondérées par des valeurs numériques allant de ?1 à 1. Des modèles de cartes cognitives floues (Carvalho et Tomé, 1999) plus complexes associent des règles de logique floue pour chaque concept de la carte. Ces travaux sont intéressants pour la conception d\u0027outils capables d\u0027effectuer des raisonnements de façon autonome mais les ré-sultats fournis étant plus difficiles à interpréter par l\u0027utilisateur, nous avons choisi de garder un formalisme plus simple.\nMécanisme d\u0027inférence entre deux regroupements de concepts\nOn souhaite pouvoir interroger une carte cognitive simple afin de connaître l\u0027influence d\u0027un ensemble de concepts sur un autre ensemble de concepts. Nous définissons donc un regroupement de concepts comme étant un sous-ensemble des concepts.\nDéfinition (Regroupement de concepts):\nSoit C un ensemble de concepts. Un regroupement de concepts est un sous-ensemble de concepts de C.\nExemple:\nLes concepts Brouillard, Pluie, Neige, Givre forment un regroupement de concepts (figure 1) appelé Mauvais temps.\nDéfinissons un mécanisme d\u0027influence entre deux regroupements de concepts C 1 et C 2 . L\u0027influence entre C 1 et C 2 est nulle (notée 0) lorsqu\u0027il n\u0027y a pas de chemins d\u0027influence entre un concept de C 1 et un concept de C 2 . Elle est positive, notée + (resp. négative, notée ?), lorsque chaque concept de C 1 influence positivement (resp. négativement) tous les concepts de C 2 . Elle est \"positive ou nulle\", noté ? (resp. \"négative ou nulle\", notée lorsque un concept de C 1 n\u0027influence pas un concept de C 2 et qu\u0027un concept de C 1 influence positivement (resp. négativement) un concept de C 2 mais aucun concept de C 1 influence négativement (resp. positivement) un concept de C 2 . Elle est ambiguë, notée ?, lorsqu\u0027un concept de C 1 influence positivement (ou de manière ambiguë) un concept de C 2 et un concept de C 1 influence négativement (ou de manière ambiguë) un concept de C 2 .\nDéfinition (Propagation de l\u0027influence entre deux regroupements de concepts):\nSoit M \u003d (V, ´ etiq V , I, ´ etiq I ), une carte cognitive simple définie sur un ensemble de concepts C et l\u0027ensemble de symboles {+, ?}. Soit C 1 ,C 2 deux regroupements de concepts. La propagation de l\u0027influence entre deux regroupements est l\u0027application I C définie sur C × C ? {0, +, ?, ?, ?} telle que: \nHiérarchie et carte cognitive\nOn souhaite permettre à l\u0027expert qui conçoit la carte de prédéfinir des regroupements de concepts qui ont un sens à être ensemble. Ces regroupements prédéfinis facilitent l\u0027emploi par l\u0027utilisateur du mécanisme de propagation entre deux regroupements de concepts. Nous considérons ici que les seuls concepts qui peuvent être regroupés ensemble sont ceux qui sont une spécialisation de la même chose. Nous proposons de définir un regroupement de concepts comme un nouveau concept appelé par le nom choisi. Ce concept peut être à son tour groupé avec d\u0027autres concepts. Nous considérons ici qu\u0027un concept est regroupé par un autre concept signifie que le premier est une spécialisation du second. L\u0027ensemble des concepts forme alors un ensemble partiellement ordonné par une relation de spécialisation. Nous appelons hiérar-chie cet ensemble partiellement ordonné de concepts.\nDéfinition (Hiérarchie):\nUne hiérarchie est un ensemble de concepts C muni d\u0027une relation d\u0027ordre partiel notée Cette relation d\u0027ordre est une relation de spécialisation.\nExemple:\nLa figure 2 est un diagramme de Hasse (Skiena, 1990) représentant l\u0027ensemble partiellement ordonné des concepts. Un diagramme de Hasse est un graphe acyclique orienté qui ne contient pas d\u0027arc de transitivité. Les arcs dans la hiérarchie symbolisent des relations de spécialisation. Par exemple, Ville et Lieu sont reliés car une Ville est une sorte de Lieu.\nUn ensemble partiellement ordonné par une relation contient des éléments minimum, c\u0027est à dire des éléments pour lesquels il n\u0027existe pas d\u0027éléments inférieurs. Nous appelons les élé-ments minimum d\u0027une hiérarchie des concepts-minimum.\nDéfinition (Ensemble des concepts-minimum d\u0027une hiérarchie):\nSoit (C, une hiérarchie de concepts. On appelle l\u0027ensemble des concepts-minimum le sous ensemble des concepts de C défini de la façon suivante:\nExemple: Les concepts-minimum de la hiérarchie de la figure 2 sont ceux utilisés dans la carte cognitive simple de la figure 1.\nUne carte cognitive hiérarchique est une carte cognitive simple pour laquelle des regroupements de concepts ont été prédéfinis par le concepteur.\nFIG. 2 -Hiérarchie de concepts\nDéfinition (Carte cognitive hiérarchique): Soit (C, une hiérarchie de concepts. Soit S un ensemble de symboles. Une carte cognitive simple définie sur M in(C), S est une carte cognitive hiérarchique définie sur (C, S.\nExemple:\nLa carte cognitive simple de la figure 1 devient une carte cognitive hiérarchique définie sur la hiérarchie de la figure 2. Les noeuds de cette carte cognitive hiérarchique sont étiquetés par les concepts-minimum de cette hiérarchie.\nMécanisme d\u0027inférence dans une carte cognitive hiérar-chique\nDéfinissons un mécanisme d\u0027inférence permettant à l\u0027utilisateur d\u0027interroger une carte cognitive hiérarchique afin de déterminer l\u0027influence entre deux concepts de n\u0027importe quel type, que ce soit des concepts-minimum ou non. Ce mécanisme d\u0027inférence entre deux concepts c et c détermine dans un premier temps deux regroupements de concepts-minimum : le regroupement de concepts-minimum qui sont inférieurs ou égaux à c et le regroupement de concepts-minimum qui sont inférieurs ou égaux à c .\nDéfinition (Ensemble des concepts-minimum inférieurs ou égaux à un concept):\nSoit (C, une hiérarchie de concepts. L\u0027ensemble des concepts-minimum inférieurs ou égaux à un concept c de C est {c ? M in(C)|c c}. On note cet ensemble M inInf (c).\nExemple:\nDéterminons l\u0027ensemble des concepts-minimum inférieurs ou égaux à Brouillard : M inInf (Brouillard) \u003d {Brouillard} et l\u0027ensemble des concepts-minimum inférieurs ou égaux à Mauvaises conditions de circulation : M inInf (Mauvaises conditions de circulation) \u003d {Mauvaise visibilité, Densité de circulation, Route glissante, Route sinueuse}\nUne fois que les deux regroupements de concepts-minimum sont déterminés, le mécanisme d\u0027inférence dans une carte cognitive hiérarchique calcule l\u0027influence entre ces deux regroupements de concepts à l\u0027aide du mécanisme d\u0027inférence entre deux regroupements vu précédem-ment.\nDéfinition (Mécanisme d\u0027inférence dans une carte cognitive hiérarchique):\nSoit H \u003d (V, ´ etiq V , I, ´ etiq I ) une carte cognitive hiérarchique définie sur une hiérarchie (C, ) et l\u0027ensemble de symboles {+, ?}. L\u0027inférence dans une carte cognitive hiérarchique H est une application définie sur C × C ? {0, +, ?, ?, ?} telle que : \nMécanisme de visualisation partielle d\u0027une carte cognitive hiérarchique\nCette section présente un mécanisme qui, à partir d\u0027une carte cognitive hiérarchique et d\u0027un ensemble de concepts sélectionnés par l\u0027utilisateur dans la hiérarchie, calcule une carte cognitive simple représentant une vue synthétique du système. On appelle vue partielle la carte cognitive simple obtenue par ce mécanisme. La vue partielle est définie sur un sous-ensemble de concepts de la hiérarchie (pas seulement les concepts-minimum). Ces concepts sont reliés par des influences dont le symbole est déterminé utilisant le mécanisme d\u0027inférence des cartes cognitives hiérarchiques. La vue partielle est une représentation synthétique du système, les influences qui la composent portent donc des symboles moins précis que ceux employés dans une carte cognitive classique. Les symboles que nous employons sont +, ?, ?, et ? (Axelrod, 1976)(Chaib-draa, 2002 (Fabiola Mata Avila, 2002).\nNotation (Ensemble de concepts sélectionnés dans la hiérarchie):\nSoit H \u003d (V, ´ etiq V , I, ´ etiq I ) une carte cognitive hiérarchique définie sur (C, et S. Notons C s le sous ensemble des concepts de C sélectionnés par l\u0027utilisateur.\nL\u0027ensemble des concepts de la vue partielle contient les concepts sélectionnés par l\u0027utilisateur, les concepts-minimum qui ne sont pas strictement inférieurs aux concepts sélectionnés. Si un concept sélectionné est inférieur à une autre concept sélectionné, il ne fait pas parti de l\u0027ensemble des concepts de la vue partielle.\nDéfinition (Ensemble de concepts d\u0027une vue partielle):\nSoit (C, une hiérarchie de concepts. Soit C s l\u0027ensemble des concepts sélectionnés par l\u0027utilisateur. Soit StrictInf (c) \u003d {c ? C|c c ? c \u003d c} l\u0027ensemble des concepts strictement inférieurs à un concept. L\u0027ensemble des concepts de la vue partielle est défini tel que:\nExemple: Les concepts présentent dans la figure 3 sont les concepts obtenus lorsque l\u0027utilisateur sélec-tionne les concepts : Erreurs, Accident, Lieu, Puissance, Mauvaises conditions de circulation et Type de véhicule Une vue partielle d\u0027une carte cognitive hiérarchique est une carte cognitive simple. L\u0027ensemble des noeuds de la vue partielle sont étiquetés par des concepts de la hiérarchie déterminés en fonction des concepts sélectionnés dans la hiérarchie par l\u0027utilisateur. Deux noeuds de la vue partielle v 1 et v 2 associés à deux concepts c 1 et c 2 sont reliés par une influence si dans la carte cognitive hiérarchique il existe une influence entre un noeud associé à un concept inférieur à c 1 et un noeud associé à un concept inférieur à c 2 . Le symbole de cette influence est déterminé à l\u0027aide du mécanisme d\u0027inférence dans une carte cognitive hiérarchique appliqué aux concepts c 1 et c 2 .\nDéfinition (Vue partielle sur une carte cognitive hiérarchique): Soit H \u003d (V, ´ etiq V , I, ´ etiq I ) une carte cognitive hiérarchique définie sur une hiérarchie (C, ) et l\u0027ensemble de symboles {+, ?}. Soit C s l\u0027ensemble des concepts sélectionnés par l\u0027utilisateur. Une vue partielle de H en fonction de C s est la carte cognitive simple \nConclusion\nCet article présente un nouveau modèle de carte cognitive, dite hiérarchique, qui permet à un utilisateur d\u0027obtenir une vue partielle et synthétique de celle-ci. Cet article présente d\u0027abord un modèle simple de carte cognitive qui formalise les nombreux travaux portant sur les cartes cognitives. Un mécanisme d\u0027inférence de l\u0027influence entre deux concepts et un mécanisme d\u0027inférence entre deux regroupements de concepts sont définis. Ensuite ce modèle est étendu en lui associant une hiérarchie. Une hiérarchie déterminée par le concepteur de la carte représente des regroupements de concepts dans une carte cognitive simple. Le mécanisme d\u0027inférence de l\u0027influence entre deux regroupements est utilisé afin de définir l\u0027influence de n\u0027importe quel concept de la hiérarchie sur un autre. Ces mécanismes ont permis de fournir à l\u0027utilisateur une visualisation partielle d\u0027une carte cognitive hiérarchique. Nous avons développé un prototype 1 en java qui permet de construire et d\u0027utiliser des cartes cognitives hiérarchiques. Les différents composants nécessaires à la visualisation des cartes sont implémentés en utilisant JGraph 2 , une bibliothèque de visualisation de graphes. Les figures de cartes cognitives présentées dans cet article ont été obtenues à l\u0027aide de ce prototype.\n"
  },
  {
    "id": "852",
    "text": "Introduction\nAujourd\u0027hui de nombreux génomes séquencés sont disponibles du fait du développement continu des technologies à haut débit et des procédures expérimentales 1 . Les experts biologistes jouent un rôle central dans l\u0027analyse et l\u0027annotation de cette quantité massive de données brutes. Pour annoter un nouveau génome, ils doivent intégrer plusieurs types d\u0027informations en provenance de sources variées, ce qui prend entre 12 et 18 mois à une équipe de 2 à 4 personnes pour un petit génome bactérien contenant environ 2000 gènes. Pour faire face au déluge des nouvelles données génomiques, le processus d\u0027annotation doit être le plus automatisé possible. Dans le contexte du projet RAFALE 2 , nous proposons aux biologistes utilisant la plate-forme AGMIAL 3 , un système semi-automatique d\u0027annotation fonctionnelle de protéines. Nous proposons un système semi-automatique car le processus est collaboratif : pour chaque protéine, une annotation est suggérée par le système et les biologistes décident de l\u0027annotation finale.\nLes annotations sont issues d\u0027arbres de décision obtenus par deux approches différentes : TILDE (Blockeel et Raedt, 1998) et Clus-HMC (Blockeel et al., 2006). Ces arbres sont appris sur les données d\u0027un génome puis testés sur un autre génome (tous deux disponibles dans AGMIAL). Les classes fonctionelles que l\u0027on propose (appelées prédictions) et les classes fonctionnelles données par l\u0027expert aux protéines (appelées annotations) sont classées dans une hiérarchie fonctionnelle dérivée de celle de Subtilist (Moszer et al., 2002).\nMesures d\u0027évaluation hiérarchiques\nLes prédictions obtenues par les arbres de décision sont pondérées par un indice de confiance qui est égal au pourcentage d\u0027exemples arrivant dans la feuille de l\u0027arbre permettant d\u0027effectuer la prédiction, arbre appris sur l\u0027ensemble d\u0027apprentissage.\nCet indice de confiance est utilisé pour contrôler la qualité des prédictions via un seuil d\u0027élagage défini par l\u0027expert.\nAfin de mieux évaluer la pertinence des prédictions effectuées, nous avons exhaustivement recensé les types de couples annotation/prédiction possibles, en fonction des différents niveaux de la hiérarchie. Ce classement peut être comparé à celui de TABS (Iliopoulos et al., 2003) qui associe un score à chaque type de différences observées entre deux annotations d\u0027un même génome. Comme nous ne travaillons pas dans le même contexte d\u0027étude, certains types de différences de TABS ne s\u0027appliquent pas à notre système (erreurs de typographies, annotations non répertoriées, erreurs de domaines, prédictions sans annotations) 4 . Nous définissons les types de différences, entre une annotation et une prédiction, réperto-riés dans TABS et utilisés dans notre système à l\u0027aide des notations suivantes :\nNotons E l\u0027ensemble des exemples étudiés (ici les protéines). Soit x ? E une protéine et f (x, i) (resp. f (x, i)) l\u0027annotation (resp. la prédiction) de x, si elle existe, au niveau i de la hié-rarchie utilisée. Soit\nensemble des protéines pour lesquelles annotation et prédic-tion sont en adéquation jusqu\u0027au niveau k de la hiérarchie mais pas au niveau k + 1, et il existe une annotation (resp. une prédiction) jusqu\u0027au niveau i (resp. j).\nLes types de différences de TABS pertinents pour notre approche sont les suivants : l\u0027annotation et la prédiction existent et sont identiques jusqu\u0027au niveau i (cas d\u0027une protéine dans A i i,i ) ; l\u0027annotation est plus précise que la prédiction : elles sont identiques jusqu\u0027au niveau j (cas d\u0027une protéine dans A j i,j avec i \u003e j) et l\u0027annotation est moins précise que la prédiction : elles sont identiques jusqu\u0027au niveau i (cas d\u0027une protéine dans A i i,j avec i \u003c j). De plus, nous ajoutons pour notre système le cas où i est le premier niveau où la prédiction diffère de l\u0027annotation (cas d\u0027une protéine dans A i?1 i,i ). L\u0027évaluation de la qualité des prédictions est effectuée avec différentes mesures hiérar-chiques telles que : la précision, le rappel, la spécificité ou le Fscore.\nEtude des prédictions plus précises que l\u0027annotation\nLe cas d\u0027une protéine annotée 3.5.2 et prédite 3.5.1 (incluse dans l\u0027ensemble A Or, si le premier cas correspond clairement à une erreur de pr édiction, le second cas correspond à une sous-prédiction supplémentaire par rapport à l\u0027existant qui pourrait permettre de raffiner une annotation, ce qui est potentiellement intéressant pour le biologiste.\nLa figure 1 présente différentes configurations d\u0027intérêt pour notre étude.\nFIG. 1 -Exemples de différences entre annotations (encadrées) et prédictions (grisées) dans le cadre de mesures hiérarchiques.\n-(e 1 ) correspond à l\u0027adéquation parfaite entre l\u0027annotation et la prédiction (e 1 ? A 3 3,3 ).\n-(e 2 ) correspond au cas où l\u0027annotation et la prédiction sont connues jusqu\u0027au niveau 3 de la hiérarchie et ne sont en accord que jusqu\u0027au niveau 2. Les prédictions de niveau 1 et 2 sont considérées comme justes, la prédiction de niveau 3 est considérée comme erronée par rapport à l\u0027annotation (e 2 ? A 2 3,3 ). -(e 3 ) correspond au cas où l\u0027annotation est plus précise que la prédiction. Le système, bien que n\u0027ayant pas commis d\u0027erreur de prédiction, n\u0027a pas su prédire suffisament finement la classe fonctionnelle (e 3 ? A 2 3,2 ). -Enfin, le cas (e 4 ) illustre une prédiction plus précise que l\u0027annotation courante. Le système a prédit la classe fonctionnelle 3.5.2 alors que l\u0027annotation est 3.5. Ce cas est traditionnellement considéré comme érroné (e 4 ? A 2 2,3 ). Les protéines possédant une prédiction plus précise ( comme e 4 )peuvent être classées de deux manières selon le sens que l\u0027on décide de leur attribuer : -elles interviennent dans le calcul du décompte des sous-prédictions erronées quand une sous-prédiction supplémentaire est comptée comme une erreur. Cette signification est couramment utilisée afin d\u0027éviter de propager des erreurs en cas d\u0027annotation d\u0027un gé-nome par rapport à un autre. -elles interviennent à la fois dans le calcul du décompte des sous-prédictions justes et erronées pour moitié, en considérant qu\u0027une sous-prédiction supplémentaire peut être aussi bien une erreur qu\u0027une spécialisation justifiée (par exemple, dans le cas où de nouvelles informations sont disponibles depuis l\u0027annotation).\nNous nous sommes focalisés sur l\u0027analyse des protéines correspondant au cas (e 4 ). Les résultats des expérimentations que nous avons réalisées sur les génomes L.bacillus et L.sakei annotés avec la plateforme AGMIAL sont présentés dans la section suivante.\nDans la suite de cet article, nous choisissons la deuxième signification et considérons qu\u0027une sous-prédiction supplémentaire est à la fois une erreur et une spécification justifiée.\nRNTI -X -3\nRésultats expérimentaux\nLes résultats sont analysés en fonction du seuil d\u0027élagage appliqué aux arbres de décision (évoqués à la section 2).\nLes courbes de la figure 2 présentent la proportion de prédictions plus précises entre les niveaux 1 et 2 (courbe A) et les niveaux 2 et 3 (courbe B). Dans l\u0027approche usuelle, ces protéines sont indifférenciables des protéines ayant une prédiction réellement erronée.\nD\u0027après ces courbes, nous pouvons remarquer que nous identifions des prédictions plus précises que l\u0027annotation, essentiellement au troisième niveau de la hiérarchie (jusqu\u0027à 100% pour L.sakei à partir d\u0027un seuil d\u0027élagage 0.90, courbe B, correspondant aux cas des différences entre les niveaux 2 et 3). De manière similaire, nous observons (courbe A) pour le même seuil d\u0027élagage, uniquement 4 sous-prédictions plus fines que les annotations au niveau 2. Cela est potentiellement dû à la qualité des annotations. En effet, la plupart des protéines des génomes étudiés sont annotées aux niveaux 2 ou 3 de la hiérarchie et rares sont celles qui sont uniquement annotées au niveau 1.\nVoici par exemple, les règles concluant à la prédiction de la classe 1.2.5 de la protéine 157 de L.sakei qui est annotée en 1.2 : Ces règles sont des chemins issus de plusieurs arbres appris par TILDE à partir d\u0027informations utilisées sur les protéines de L.bulgaricus. La hiérarchie que nous utilisons est organisée en trois niveaux, nous prédisons donc une classe fonctionnelle en trois étapes correspondant aux niveaux successifs.\n1. Au premier niveau, comme plus de 60% des protéines qui ressemblent à la protéine considérée (sous certaines conditions), sont annotées avec le terme GO :0006810 transport, la protéine esa157 est classée dans la classe 1 (Cell envelope and cellular processes) avec un indice de confiance de 97%. L\u0027indice de confiance, (acc97), correspond au nombre de protéines dont l\u0027annotation et la prédiction sont en adéquation lors de la phase d\u0027apprentissage des arbres.\n2. Au second niveau, comme il existe des protéines qui ressemblent à la protéine considérée, annotées avec le terme GO :0006810 transport et comme il n\u0027y a pas plus de 62.5% des protéines qui ressemblent à la protéine esa157, et qui sont annotées avec le terme GO :0016469 proton-transporting two-sector ATPase complex, la protéine esa157 est classée en 1.2 Transport/binding proteins and lipoproteins (indice de confiance de 95%).\n3. Au troisième niveau comme il n\u0027existe pas de protéine qui ressemble à la protéine considérée et qui soit annotée avec le terme GO :0009401 phosphoenolpyruvate-dependent sugar phosphotransferase system ou GO :0003824 catalytic activity ou le mot clé SwissProt Lipoprotein, et comme il existe des protéines qui ressemblent à la protéine considé-rée, qui sont annotées avec le terme GO :0006865 amino acid transport alors la protéine esa157 est classée en 1.2.5 Transport/binding of amino-acids (indice de confiance de 91%).\nL\u0027identification explicite des protéines ayant des prédictions plus précises que l\u0027annotation permettra à l\u0027expert de les traiter séparément, surtout dans le cadre de réannotation de génomes où les informations complémentaires apportées par l\u0027annotation automatique sur ces protéines pourront être analysées plus rapidement. Ici, pour la protéine esa157, l\u0027annotation initiale 1.2 peut être corrigée en 1.2.5, comme prédit (après consultation de l\u0027annotateur).\nConclusion et perspectives\nLes mesures hiérarchiques usuelles prennent en compte la hiérarchie mais elles ne traduisent pas bien la statégie d\u0027annotation des experts biologistes qui évitent de propager des erreurs.\nDe plus le cas de prédiction plus fine que l\u0027annotation existante est traité comme une erreur par les mesures usuelles alors que dans un cadre d\u0027annotation fonctionnelle, cette information doit impérativement être différenciée.\nNous avons répertorié les types de différences qui peuvent exister entre une annotation et une prédiction dans notre problème d\u0027annotation fonctionnelle, et proposé une formalisation pour les représenter, valable quelque soit le niveau de profondeur de la hiérarchie.\nNous travaillons actuellement sur de nouvelles mesures hiérarchiques permettant de traiter ces cas à l\u0027aide de cette formalisation. Nous pourrons ainsi traiter différemment les protéines ayant des prédictions plus précises et les protéines ayant une prédiction erronée.\nRNTI -X -5\n"
  },
  {
    "id": "853",
    "text": "Introduction\nL\u0027évaluation d\u0027une ontologie est une tâche difficile. Ceci explique l\u0027absence de méthodes d\u0027évaluation standard ou de mesures d\u0027évaluation servant à valider l\u0027ontologie. Dans cet article, nous focalisons notre intérêt sur l\u0027évaluation des concepts de l\u0027ontologie de domaine (appelés concepts ontologiques) qui sont extraits des pages Web. Nous travaillons sur les documents HTML écrits en français, dans le domaine du tourisme. Dans un travail précé-dent, nous avons défini un contexte structurel qui tient compte du document HTML et déve-loppé un algorithme de clustering afin de bien rassembler les mots sémantiquement proches (Karoui et al, 2006). Le résultat de ce travail était constitué de classes de mots pour lesquelles les experts ont du réaliser une lourde tâche d\u0027évaluation et d\u0027étiquetage. Pour aider ces derniers et faciliter l\u0027interprétation sémantique de ces classes (concepts), nous avons défini une méthode d\u0027évaluation basée sur trois critères révélateurs. Ces derniers sont le degré de crédibilité, le degré de cohésion et le degré d\u0027éligibilité. Le degré de crédibilité exploite deux types de contextes : un contexte linguistique et un contexte documentaire. En se basant sur ces deux types de contextes, nous calculons le degré de crédibilité associé à chaque classe de mots et à chaque contexte. Le degré de cohésion calcule le degré de rapprochement des mots d\u0027une classe en utilisant les documents du web. Le degré d\u0027éligibilité sélectionne ou suggère le mot de la classe qui peut être son concept ou qui peut orienter le raisonnement vers le futur concept approprié. Dans ce qui suit, nous détaillerons ces critères ainsi que leur rôle.\nLes critères révélateurs de l\u0027évaluation intelligente des concepts\nLes critères révélateurs assistent l\u0027expert du domaine durant la tâche d\u0027évaluation. Ces critères sont : le degré de crédibilité: le caractère de ce qu\u0027on croit ; le degré de cohésion: le caractère d\u0027une chose dont toutes ses parties sont réunies avec une relation logique entre ses éléments et sans aucune contradiction ; le degré d\u0027éligibilité : le caractère d\u0027un mot qui com-bine les conditions nécessaires pour être élu comme concept puisqu\u0027il est le mot le plus représentatif de la classe ou qu\u0027il peut orienter le raisonnement, l\u0027interprétation et la tâche d\u0027étiquetage.\nDegré de crédibilité. Notre objectif est de pouvoir modifier les classes en cas de nécessi-té, de les étiqueter et d\u0027estimer le rapprochement sémantique de leurs mots. Pour cela, nous effectuons une « contextualisation progressive » lors du processus d\u0027évaluation, qui exprime la diversité des contextes et se concentre sur comment différents objets (dans notre cas les mots) opèrent et s\u0027adaptent à leurs contextes à travers différentes structures de documents, d\u0027organisations de mots, de méthodes de conception, d\u0027intensions de concepteur, etc. La contextualisation progressive rejette l\u0027idée de l\u0027utilisation d\u0027un unique contexte pour comprendre un objet. Au contraire, elle affirme qu\u0027un discours ou une rédaction ordinaire révè-lent plusieurs contextes pour chaque objet étudié. L\u0027interaction de ces contextes permet une interprétation sémantique correcte et écarte les mauvaises évaluations. Dans notre recherche, les différents contextes déduits à partir des documents sont donnés par deux analyses diffé-rentes. La première est une analyse linguistique qui permet de fournir les groupes nominaux, groupes verbaux, les associations de mots par une préposition (de, à, etc.) et celles par une conjonction (et, ou, etc. randonneur, gorge TAB. 1 -Exemples de classes de mots En fournissant tous ces contextes à l\u0027expert, il lui sera difficile d\u0027exploiter les associations entre les mots pour évaluer et étiqueter les classes (au vu de la quantité d\u0027information importante qu\u0027il devra analyser). Afin de lui faciliter la tâche, nous définissons un indice sémanti-que qui représente la crédibilité de l\u0027association des mots étudiés en considérant les diffé-rents contextes. Cet indice est appelé « degré de crédibilité ». Il est calculé automatiquement pour chaque classe de mot et pour chaque contexte.\nNotre algorithme, intitulé « Calcul de Degré de Crédibilité » et noté CDC, est exécuté sur un ensemble de classes de mots afin de calculer leurs degrés de crédibilité (DC). Ces derniers sont de la forme suivante : DC\u003d {nombre de mots} nombre d\u0027associations /Exemple : 3 2 (deux associations de trois mots).\nPar exemple, en prenant la classe 1 (TAB.1) et en choisissant la phrase comme contexte, l\u0027algorithme génère toutes les combinaisons possibles dans ce contexte c.-à-d. il essaye de trouver les 4 mots {académie, golf, golfeur, club}, puis les associations possibles de combinaisons de trois mots, etc. Pour chaque association trouvée, il présente les mots associés ainsi que le degré de crédibilité qui représente le nombre de fois que cette association a été trouvée. Par exemple, l\u0027algorithme détecte deux associations de trois mots qui sont {académie, golf, golfeur} et {golf, golfeur, club}. Dans ce cas, le degré de crédibilité est 3 2 c.-à-d. deux associations de trois mots. Cet algorithme possède plusieurs fonctionnalités à savoir :\n-Trouver les associations disponibles dans les contextes ainsi que le concept de la classe de mots. Par exemple, pour la Classe 2 (TAB.1), le concept est « civilisation » car ce terme de la classe est le plus fréquent dans les associations.\n-Détecter les éléments générant du bruit dans une classe pour les supprimer ou les transférer dans leurs classes appropriées. Par exemple pour la classe 6 (TAB.1), le mot \u0027randonneur\u0027 a été trouvé dans de nombreuses associations contextuelles relatives à la classe 5 et pas dans celles de la classe 6. Dans ce cas, l\u0027algorithme peut proposer de changer la place de ce mot de la classe 6 vers la classe 5. Lorsqu\u0027un mot d\u0027une classe apparaît souvent dans les associations de deux classes différentes (par exemple sa classe et une autre), l\u0027algorithme écrit le mot en rouge pour souligner une situation ambiguë.\n-Enrichir une classe par d\u0027autres mots provenant des associations produites par les contextes. Pour la classe 3, nous pouvons enrichir la classe par les mots \u0027naturel\u0027 et \u0027espace\u0027 et lui associer le nom du concept suivant : \u0027espace naturel\u0027.\nCette méthode d\u0027évaluation permet à un utilisateur (expert, ingénieur de connaissances, étudiant, etc.) de commencer par analyser les associations produites par les contextes linguistiques. A défaut de ne pas retrouver l\u0027information utile, il pourra étendre son analyse aux résultats des contextes documentaires. Grâce au degré de crédibilité calculé pour chaque classe et pour chaque contexte, l\u0027utilisateur peut obtenir des informations utiles et dans certains cas suffisantes pour manipuler les classes de mots (supprimer, ajouter, etc.), les évaluer et les étiqueter. Pour une classe, si l\u0027utilisateur trouve les degrés (5 1 , 4 3 , 3 8 , 2 15 ) qui sont des indications quantitatives, il devrait commencer par analyser l\u0027association de 5 mots. Si ceci s\u0027avère insuffisant pour lui, il pourra poursuivre avec les associations suivantes (de 4 mots, etc.). Autrement dit, l\u0027algorithme génère un nombre d\u0027informations complémentaires, mais l\u0027utilisateur ne sera pas amené à toutes les utiliser mais plutôt à suivre l\u0027ordre (des contextes linguistiques vers les contextes documentaires et au sein d\u0027un même contexte des associations les plus larges (association de cinq mots 5 1 ) vers les plus réduites (les associations de deux mots 2 15 )) pour décider de s\u0027arrêter dès qu\u0027il atteindra son objectif. Degré de cohésion. Notre idée est basée sur le fait qu\u0027il existe une grande quantité de pages web indexées par Google (8 058 044 651). Le web représente des connaissances fournies par différentes personnes ou organismes qui ont des expériences différentes mais certainement complémentaires dans le domaine. Nous sommes intéressées par le calcul du degré de cohé-sion de chaque classe de mots. Pour cela, nous définissons une distance sémantique entre les mots d\u0027une classe en se basant sur les documents indexés par Google. Notre mesure intitulée \u0027degré de cohésion\u0027, notée Coh-D, utilise le nombre de documents dans lesquels les mots de la classe apparaissent ensemble. Ce critère est défini après une adaptation de la distance RNTI -X -normalisée de Google (Cilibrasi etVitanyi, 2006) de manière à pouvoir calculer le rapprochement sémantique pour un nombre de mots supérieur ou égal à deux (la distance de Google peut estimer le rapprochement sémantique entre uniquement deux mots). Notre formule est la suivante:  (karoui et al, 2006). Une classe acceptable est une classe que l\u0027expert est capable de labelliser et dans laquelle les termes appartenant au même groupe sont proches sémantiquement. Une classe incorrecte est une classe qui présente l\u0027un des deux cas suivants : soit elle contient certains termes qui n\u0027ont pas de relations avec le concept extrait de cette classe, soit elle contient plusieurs concepts clairement identifiés par l\u0027expert. Une classe inconnue est une classe dont les termes n\u0027ont aucune relation sémantique ; l\u0027expert ne peut pas en donner une interprétation sémantique.\nPar exemple, les classes 1 et 6 (TAB.2) sont deux classes acceptables puisque tous les mots sont associés au même concept. Les classes 3 et 5 (TAB.2) contiennent des éléments de bruit (mots qui ne doivent pas appartenir à la classe en question). C\u0027est pour cela, que les experts du domaine suppriment plusieurs mots afin de trouver le concept ou ne peuvent pas définir de concept pour la classe. Dans les autres cas, la classe peut contenir des mots de bruit mais qui sont facilement repérables (\u0027feuille\u0027 dans la classe 8). La question qui se pose est comment peut-on détecter le mot qui doit être supprimé de la classe. En utilisant le moteur de recherche Google et sa base de données, nous calculons le nombre de réponses de chaque mot de la classe en l\u0027associant au mot représentant le domaine (dans notre cas \"tourisme\"). Le mot qui apparaît le moins est supprimé et nous maintenons cette modification au niveau du calcul du degré de cohésion pour observer son effet. D\u0027autres exemples sont pré-sents dans la TAB.2. Les mots en gras sont ceux qu\u0027une fois supprimés, améliorent le degré de cohésion.\nPar exemple, dans la classe 7 (TAB.2), en supprimant le mot \"cervidés\", nous obtenons un degré de cohésion plus représentatif (un impact réel puisque le degré de cohésion est passé de 223.52 à 81433.26) et par conséquence une tâche d\u0027évaluation plus facile. D\u0027après nos expérimentations et l\u0027avis des experts, quand le degré de cohésion est faible (\u003c1000), cela indique que le contenu de la classe devra être modifié.\nLe degré de cohésion est un critère quantitatif qui aide l\u0027expert du domaine ou un utilisateur novice à voir si la classe est sémantiquement cohérente ou non. Mais l\u0027évaluateur ne peut pas établir un jugement uniquement en se basant sur ce critère. Ce dernier est plus inté-ressant quand il est considéré avec les autres critères (crédibilité et éligibilité). Degré d\u0027éligibilité. Le degré d\u0027éligibilité représente le nombre de voix qui indique à l\u0027utilisateur quel est le mot candidat qui peut probablement représenter la classe ou du moins initier le processus de raisonnement qui mène à la définition du concept approprié de la classe. Il est calculé en appliquant la formule suivante :\nNT(x) est le nombre d\u0027occurrences de x ; n est le nombre de mots dans la classe. Le mot ayant le degré d\u0027éligibilité le plus proche de la moyenne de la classe désigne le candidat représentant la classe. Il est noté Mot-E-D. Ce degré est calculé uniquement pour les classes acceptables. Vu que nous opérons dans le domaine de la sémantique, ces critères révélateurs ainsi que les constatations résultantes restent des propositions à approuver ou à rejeter par l\u0027expert. Dans les deux cas, nous en tirons profit car même en cas de proposition incorrecte, l\u0027expert raisonnera plus rapidement pour définir le \u0027bon\u0027 concept approprié à la classe. \nN°\nTAB. 3 -Des résultats\nPour la classe 1 (TAB.3), notre critère trouve le bon terme \"golf\" qui représente la classe. Dans le cas ou le mot désigné par le degré d\u0027éligibilité et celui mentionné par le degré de crédibilité en tant que mot de référence sont identiques, l\u0027algorithme présente le mot en question en un style gras afin d\u0027exprimer une forte suggestion. La classe 1 est une classe acceptable. La classe 3 (TAB.3) contient un mot de bruit qui est \"mètre\". En le supprimant, l\u0027algorithme détecte le bon terme représentatif de la classe qui est \"parc\". Pour la classe 2, l\u0027algorithme propose \"caravane\" en tant que concept. Mais le mot de référence (le mot qui est donné par l\u0027algorithme de calcul de degré de crédibilité et qui est le plus présent dans tous les contextes) de cette classe est \"camping\". Puisque les deux mots sont différents, l\u0027algorithme les mentionne avec une couleur rouge pour signaler une situation d\u0027ambigüité. Dans ce cas, l\u0027évaluateur doit faire le choix entre les deux mots ou définir un autre concept soit sur la base des deux ou complètement nouveau. Mais dans les deux premiers cas (les classes 1 and 3 (TAB.3)), l\u0027évaluateur peut décider facilement.\nDiscussion. Notre méthode d\u0027évaluation des concepts ontologiques, que nous avons caracté-risée comme étant \u0027intelligente\u0027, du fait qu\u0027elle pourra à terme s\u0027appliquer automatiquement, donne deux types d\u0027informations révélatrices à savoir :: une indication qualitative basée sur les associations des mots déduites à partir de la projection des mots d\u0027une classe dans un contexte défini ; une indication quantitative représentée par le degré de crédibilité calculé pour chaque classe et pour chaque contexte, le degré de cohésion et celui d\u0027éligibilité. L\u0027évaluation qualitative permet d\u0027aider l\u0027utilisateur au niveau de l\u0027interprétation sémantique. Notre méthode permet à un utilisateur ordinaire d\u0027aider l\u0027expert en manipulant en amont les classes de mots et en suggérant des étiquettes sémantiques. Par conséquent, l\u0027expert peut décider rapidement si le label est convenable ou si la classe est sémantiquement correcte. Ce type d\u0027évaluation permet, également, la réutilisation et l\u0027évolution de l\u0027ontologie puisqu\u0027elle est faite à partir des pages web et tiendra compte de leurs changements possibles. Ainsi, si les documents du web changent, les contextes extraits changent et les résultats de l\u0027algorithme CDC seront différents. D\u0027où l\u0027effet de mise à jour qui sera appliqué. Ces informations présentées à l\u0027expert serviront à l\u0027évaluation en ligne et seront stockées pour une éventuelle utilisation ultérieure. Notre méthode a été appliquée après (pourra être pendant) la construction de la hiérarchie de concepts (pas de l\u0027ontologie complète). Dans ce travail, le choix des contextes a été défini manuellement, par contre l\u0027application de ce choix sur le corpus, l\u0027extraction des contextes et des associations ainsi que les calculs des degrés d\u0027évaluation sont faits automatiquement.\nConclusion\nDans cet article, nous avons proposé une méthode d\u0027évaluation conduite par le web qui guide l\u0027interprétation, évite les cas ambigus et aide l\u0027expert ou l\u0027utilisateur. Afin d\u0027atteindre cet objectif, notre méthode se base sur trois critères révélateurs qui sont le degré de crédibili-té, le degré de cohésion et le degré d\u0027éligibilité. Chaque critère est calculé en appliquant un algorithme séparé. L\u0027algorithme de crédibilité essaye d\u0027éliminer ou de déplacer les éléments bruits, propose des labels sémantiques et produit des associations de mots par type de contexte et pour chaque classe. Les degrés de cohésion et d\u0027éligibilité informent l\u0027utilisateur de la relation entre les mots de chaque classe et quel serait le possible candidat représentant le concept. Comme perspective, nous travaillons sur la visualisation de ces critères.\n"
  },
  {
    "id": "856",
    "text": "Introduction\nLe monde bouge et bien des phénomènes ne peuvent être compris sans étudier leurs mouvements. Heureusement, de nouvelles technologies (GPS et autres transmetteurs, capteurs et réseaux de capteurs, marqueurs de type RFID) permettent désormais de saisir des positions spatio-temporelles et ainsi de suivre un grand nombre d\u0027objets en mouvement. Cette voie correspond aux besoins réels d\u0027applications dans de nombreux domaines, parmi lesquels l\u0027analyse des déplacements des habitants d\u0027une ville ou d\u0027un état afin d\u0027optimiser les infrastructures de transport, le suivi d\u0027une flotte de véhicules de livraison, ou encore les études comportementales d\u0027animaux migratoires.\nMalheureusement les systèmes de gestion de base de données (SGBD) actuels n\u0027offrent au mieux que la possibilité de traiter de positions fixes. Toute la logique de modélisation et de traitement du mouvement est à la charge des programmes d\u0027applications. Le monde de la recherche a été plus loin, en concrétisant au niveau du prototypage les concepts et les opérations liés à la gestion d\u0027objets en mouvement, dits objets mobiles (Almeida et al., 2006, Pelekis et al., 2006. Toutefois, ces travaux n\u0027ont abordé que la dimension spatio-temporelle du mouvement, à savoir la trace géométrique du parcours de l\u0027objet dans l\u0027espace au cours du temps.\nCela est insuffisant pour les applications qui utilisent une vision plus structurée, plus sémantique, du mouvement. Par exemple, de nombreuses applications perçoivent le mouvement d\u0027un objet non pas comme un déplacement unique qui commencerait au début de la vie de l\u0027objet et ne se terminerait qu\u0027avec elle, mais comme une séquence de déplacements, chaque déplacement ayant son propre but, un lieu et un instant de départ bien définis, et un lieu et un instant d\u0027arrivée bien définis. C\u0027est, entre autres, le cas des applications qui analysent les déplacements quotidiens des employés entre leur domicile et leur lieu de travail, les tournées hebdomadaires de camions qui livrent des marchandises dans une région donnée, ou les migrations annuelles d\u0027oiseaux à la recherche d\u0027une nourriture plus abondante. Dans ces cas, la perception d\u0027un mouvement unique pour chaque objet n\u0027est pas adéquate, et une perception sémantique verra plutôt une succession de déplacements nettement différenciés les uns des autres. C\u0027est chacun de ces déplacements successifs, effectués au cours de la vie d\u0027un objet mobile, que nous appelons trajectoire. L\u0027analyse de ces trajectoires permet de construire des modèles de mobilité servant soit à une prise de décision (par exemple récolter des trajectoires urbaines afin d\u0027en dériver des connaissances utiles pour optimiser le trafic), soit à l\u0027acquisition de connaissances concernant l\u0027objet mobile (par exemple analyser les trajectoires des oiseaux pour mieux comprendre leur comportement), soit encore au contrôle de stratégies, notamment en logistique (par exemple surveiller le bon fonctionnement et l\u0027efficacité du système de livraison de colis par une entreprise postale).\nDu point de vue des applications, l\u0027étude de ces trajectoires demande de récolter de nombreuses autres informations en plus du chemin parcouru. Par exemple pour une application étudiant les déplacements quotidiens pour aller au travail, le moyen de transport est une information essentielle. Pour l\u0027étude des migrations d\u0027oiseaux, ce sont les conditions météorologiques et les arrêts des oiseaux en cours de parcours (quand, dans quel type de lieu, pourquoi et pendant combien de temps). Il est aussi important de pouvoir décrire les contraintes liées à l\u0027application, comme le fait que certains oiseaux ne volent jamais durant la nuit, tandis que d\u0027autres ne volent que de nuit. Ces applications ont besoin de nouvelles structures de représentation qui aillent au-delà de la simple description du mouvement. Ces nouvelles structures doivent comprendre une partie fixe pour les caractéristiques communes à toutes les trajectoires, et une partie variable pour les caractéristiques spécifiques aux trajectoires de chaque application.\nCet article étudie le problème de modélisation des trajectoires au niveau conceptuel, ce qui nous permet d\u0027établir les bases de modélisation sans être influencés par des considérations liées à des implémentations particulières. Nous proposons un patron de modélisation des trajectoires qui peut être adapté à toute application qui utilise des trajectoires. Cette approche par patron semble particulièrement adaptée parce qu\u0027elle offre la flexibilité indispensable pour prendre en compte la diversité de la sémantique des\nÉtat de l\u0027art\nDepuis longtemps différents types de mouvements ont fait l\u0027objet d\u0027études dans des domaines divers. C\u0027est le cas en sciences sociales pour l\u0027étude des migrations, des déplacements quotidiens ou des comportements humains en voyage (Kwan et Lee 2003, Thériault et al. 2002. C\u0027est le cas en biologie pour l\u0027étude du déplacement des cellules ou en médecine pour l\u0027étude de la propagation de maladies... Par ailleurs en géomatique, le mouvement effectué par un objet mobile a été décrit de façon générique. Il est appelé \"ligne de vie geospatiale\" par Mark et al. (1999) et est décrit comme une ligne dans un espace spatio-temporel à trois dimensions (x,y,t), les coordonnées géographiques (x,y) et le temps, t. Hornsby et Egenhofer (2002) en ont proposé une définition formelle qui permet d\u0027appréhender les problèmes des granularités multiples. Laube et al. (2005) ont fait appel aux techniques de fouille des données pour analyser les lignes de vie géospatiales afin d\u0027en extraire une typologie des déplacements. Suivant une approche base de données, Güting et al. (Güting et al. 2000, Güting et Schneider 2005 ont apporté une contribution essentielle en proposant une théorie formelle pour les objets mobiles (points et surfaces) et un prototype qui implémente cette théorie en utilisant un SGBD propriétaire extensible, Secondo (Güting et al. 2004). Le coeur de cette théorie est un ensemble de types de données qui comprend des types spatiaux, des types temporels et des types \"mobiles\" (des types qui représentent une valeur qui varie dans le temps). Plus récemment, les auteurs ont étendu leur approche à la modélisation et manipulation de trajectoires contraintes par un réseau prédéfini, par exemple, un réseau ferré ou routier (Güting et al. 2006). La plupart des -153 -RNTI-E-13 travaux sur le mouvement contraint par un réseau décrivent, d\u0027un côté, le réseau avec sa structure et sa localisation et, d\u0027un autre côté, l\u0027objet mobile avec ses coordonnées. Puis ils contraignent les coordonnées de l\u0027objet à se situer sur le réseau. L\u0027approche de Güting et al. (2006) est différente : ils décrivent la position de l\u0027objet via une référence aux routes du réseau, par exemple \"sur l\u0027autoroute A9, 10 km après Montreux en direction du Valais\". Cette technique de spécification implique que l\u0027objet mobile est nécessairement positionné à l\u0027intérieur du réseau, ce qui rend inutile le recours à des contraintes spatiales. Wolfson et al. (1998Wolfson et al. ( , 1999 ont élaboré une autre approche des objets mobiles dans l\u0027objectif de réduire le coût des mises à jour. Ceci les conduit à stocker des vecteurs de mouvement (direction, vitesse, instant) plutôt que des positions spatio-temporelles (point, instant), ce qui a l\u0027avantage de permettre de prédire le mouvement futur en fonction du dernier vecteur de mouvement. Le vecteur courant reste actif tant que la différence entre la position prédite et la position relevée est inférieure à un seuil donné. Quand le seuil est dépassé un nouveau vecteur est calculé et stocké. Ce travail ne considère que les points mobiles, pas les régions mobiles. Les mêmes auteurs se sont aussi intéressés aux déplacements contraints par un réseau. Notons que ce thème des déplacements contraints a aussi été étudié par Speicys et al. (2003). Noyon et al. (2005) ont étudié comment les déplacements de deux objets mobiles ponctuels ou surfaciques peuvent être corrélés via des mesures de distance, ou comment analyser l\u0027évolution de la position relative d\u0027un objet par rapport à celle d\u0027un autre objet.\nL\u0027étude des mouvements périodiques (se répétant régulièrement, par exemple le mouvement de planètes, de trains ou d\u0027animaux migrateurs) a intéressé plusieurs équipes. Behr et al. (2006) ont proposé un modèle formel pour représenter les mouvements périodiques qui peuvent contenir des répétitions imbriquées comme, par exemple, le mouvement d\u0027un métro qui réalise la même trajectoire chaque jour de la semaine. Suivant l\u0027approche initiale à base de types abstraits de données proposée par Güting et al. (2000), les auteurs ont défini et implémenté sur Secondo un ensemble de types de données apte à décrire les mouvements périodiques, en particulier, le type pmpoint (periodic moving point).\nDu coté des implémentations, on trouve le système Hermès (Pelekis et al. 2006), dont les spécifications ont été construites en se basant sur les approches de Güting et de Wolfson. Hermès est une nouvelle cartouche de données spatio-temporelles pour le SGBD relationnelobjet Oracle 10. La cartouche utilise les types spatiaux statiques d\u0027Oracle (Oracle 2007) et les types temporels proposés par la cartouche temporelle de TAU-TLL de Pelekis et Theodoulidis (2002). Hermès offre aussi différents types de fonctions d\u0027interpolation pour reconstituer d\u0027éventuelles parties manquantes du mouvement. Par ailleurs, certains travaux ont élaboré des techniques d\u0027indexation spécifiques aux objets mobiles pour améliorer les performances du système (voir Chakka et al. 2003et Pelanis et al. 2006.\nA contrario, la prise en compte d\u0027une vision applicative des objets mobiles, c\u0027est-à-dire une vision qui tienne compte des aspects sémantiques du mouvement, ne semble pas avoir motivé les chercheurs. Ainsi, nous n\u0027avons pas trouvé d\u0027approche conceptuelle du mouvement qui soit basée sur les besoins des applications. Les modèles conceptuels pour bases de données spatio-temporelles proposent bien le concept de point mobile (voir par exemple Khatri et al. 2004ou Parent et al. 2006a, mais elles ne vont pas au-delà. À notre connaissance un seul travail, Brakatsoulas et al. (2004), s\u0027est donné pour objectif d\u0027enrichir la sémantique d\u0027un modèle pour objets mobiles. Malheureusement les trajectoires de ces auteurs restent au niveau géométrique. Ce sont des polylignes qui connectent les différentes positions spatio-temporelles relevées; elles représentent le mouvement par une séquence de -154 -RNTI-E-13 segments spatio-temporels plutôt que par une séquence d\u0027étapes sémantiques. De même, les propriétés des trajectoires sont uniquement celles qui sont dérivables des positions spatiotemporelles, comme la vitesse ou le chemin parcouru, et non d\u0027autres propriétés sémantiques qu\u0027une application pourrait souhaiter conserver à propos de ses trajectoires, comme, par exemple, le but de la trajectoire.\nSe démarquant des approches purement géométriques, cet article perçoit les trajectoires comme des déplacements qui ont une signification sémantique (trajectoires de personnes allant au travail, d\u0027animaux en migration, de véhicules en route pour une destination, de phénomènes naturels régis par les lois de la nature, …). La liste des positions spatiotemporelles d\u0027un objet tout au long de sa vie ne fournit qu\u0027un des éléments de la trajectoire. D\u0027autres éléments décrivent, entre autres, quand la trajectoire débute, se termine ou quand elle est temporairement suspendue. Ces éléments sont déterminés par l\u0027application qui, seule, peut définir la sémantique de la trajectoire. Notre approche est basée sur ce principe.\nLes applications ont aussi souvent besoin de pouvoir décrire des trajectoires selon différentes granularités spatiales et temporelles. Quelques travaux se sont intéressés à la multi-granularité dans le cadre des objets mobiles (Friis-Christensen et al. 2005, Camossi et al. 2003. Nous prévoyons de réutiliser un travail précédent sur la multi-représentation dans les bases de données spatio-temporelles (Balley et al. 2004), Parent et al. 2006b) et de proposer une approche similaire pour la multi-représentation des trajectoires. Cet aspect n\u0027est cependant pas présenté dans cet article.\nUne application\nCe paragraphe présente un exemple typique d\u0027application utilisant des trajectoires. Ce type d\u0027application a pour objectif d\u0027analyser les déplacements d\u0027animaux migrateurs. Les données sur leurs mouvements sont soit obtenues automatiquement grâce à des émetteurs portés par les animaux et qui envoient régulièrement leur position, soit obtenues par observation directe en capturant, temporairement, les animaux.\nL\u0027exemple qui nous sert d\u0027illustration concerne les migrations annuelles des cigognes blanches (Ciconia ciconia), auxquelles s\u0027intéressent plusieurs groupes de recherche 1 . Comme plusieurs espèces d\u0027oiseaux migrateurs, les cigognes migrent tous les ans. Chaque automne, elles quittent l\u0027hémisphère nord, notamment l\u0027Europe, pauvre en nourriture en hiver, et migrent vers l\u0027hémisphère sud, où la nourriture est abondante tout au long de l\u0027année. Mais pour se reproduire et nourrir leurs petits, les cigognes ont besoin des longues journées qu\u0027offre l\u0027hémisphère nord en été. Elles reviennent donc vers le nord au printemps.\nL\u0027analyse des migrations animales a pour objectif général d\u0027améliorer les connaissances concernant le comportement animal. Est-ce que la migration est innée et en quelque sorte programmée génétiquement ? Comment les oiseaux s\u0027orientent-ils lors de la migration ? En effet, ils reviennent souvent dans les mêmes endroits pour se reproduire. Comment font-ils pour voler sur de si longues distances ? Volent-ils en groupe ? Où et quand s\u0027arrêtent-ils pour se reposer pendant la migration ? Quels sont les facteurs environnementaux qui influencent leur migration (topographie, conditions météorologiques, prédateurs, disponibilité de nourriture) ? Quels sont les dangers rencontrés ? Pourquoi certains oiseaux ne survivent-ils pas à la migration ? … Pratiquement les données que nous avons utilisées proviennent d\u0027un ensemble de cigognes blanches. Elles ont été équipées chacune d\u0027un petit émetteur qui émet régulièrement des signaux qui sont captés par des satellites. Les cigognes blanches volent uniquement durant la journée parce qu\u0027elles utilisent les courants thermiques créés par le soleil quand il réchauffe la terre. Elles interrompent donc leur vol la nuit et en profitent pour se reposer et se nourrir. Elles peuvent parcourir plusieurs centaines de kilomètres par jour selon les conditions météorologiques. Elles migrent en groupes dont la composition peut changer lors des arrêts. Pour détecter les groupes de cigognes volant ensemble on analyse les positions spatio-temporelles des cigognes. Cependant, comme seul un petit pourcentage des cigognes sont équipées d\u0027un émetteur, le groupe dans lequel vole un oiseau peut rester inconnu.\nAfin d\u0027analyser le comportement des cigognes blanches durant leur migration, les chercheurs ont besoin de connaître deux types d\u0027information :\n(1) Informations propres aux trajectoires des oiseaux : Durant le vol des cigognes, les informations suivantes sont enregistrées à intervalles réguliers :\n-leur position spatio-temporelle.\n-l\u0027altitude à laquelle la cigogne vole. Cette information, de même que les conditions météorologiques et les objets naturels ou artificiels rencontrés, change le long de la trajectoire de l\u0027oiseau. -le groupe avec lequel la cigogne vole. Lorsque les cigognes blanches sont à l\u0027arrêt, les informations suivantes sont enregistrées :\n-le type d\u0027arrêt : arrêt pour la nuit ou arrêt plus long (généralement quelques jours ou semaines) pour se reposer et se nourrir. -si possible, les activités de l\u0027oiseau durant l\u0027arrêt : alimentation, repos, … -si la cigogne a pu être attrapée durant l\u0027arrêt, son poids, le pourcentage de graisse, la température de son corps et sa condition générale.\n(2) Informations sur les conditions environnementales des trajectoires : Ici sont principalement enregistrés : -Les conditions météorologiques : vent (direction, force), température, pression, état du ciel (soleil, pluie, nuages, brouillard, ciel clair). -Les objets naturels (montagnes, grandes étendues d\u0027eau, déserts…), et les objets artificiels (lignes électriques, antennes, gratte-ciels, éoliennes…) qui peuvent constituer des obstacles pour les cigognes et influencent donc leur vol, notamment leur direction de vol. La description de ces obstacles est particulièrement importante parce qu\u0027ils constituent une menace qui peut entraîner la mort des migrateurs.\nTrajectoires : Définitions de base\nNous présentons maintenant les définitions de base sur lesquelles repose notre travail, et l\u0027approche de modélisation conceptuelle des trajectoires que nous avons élaborée à partir de l\u0027étude des besoins. Notre objectif est de permettre aux applications gérant des objets mobiles de décrire leur vision du mouvement d\u0027un objet mobile (c\u0027est-à-dire l\u0027évolution de sa position au cours de sa vie) par un ensemble de trajectoires identifiables et significatives pour l\u0027application. Par exemple, une entreprise gérant une flotte de camions de livraison pourra décrire les déplacements des camions par un ensemble de trajectoires, chacune correspondant à une tournée de livraison qui part du siège de l\u0027entreprise et qui y retourne. À une autre granularité temporelle, le déplacement d\u0027un oiseau migrateur pourra être décrit par deux trajectoires annuelles : l\u0027une en automne de l\u0027Europe vers l\u0027Afrique, l\u0027autre au printemps suivant de l\u0027Afrique vers l\u0027Europe. La figure 1 illustre l\u0027idée de la désignation d\u0027un ensemble de trajectoires dans le mouvement d\u0027un objet mobile. La figure montre aussi que certains segments du mouvement peuvent n\u0027appartenir à aucune trajectoire (par exemple, le segment entre t 1 et t 2 ). Cette non-appartenance traduit le fait que ces segments correspondent à des déplacements qui n\u0027intéressent pas l\u0027application.\nFIG. 1 -Parcours spatio-temporel d\u0027un objet en mouvement, support de trajectoires définies par segmentation sémantique\nÉtant donné qu\u0027un objet ne peut être qu\u0027à un seul endroit à un instant donné, chaque trajectoire d\u0027un objet est repérable par l\u0027intervalle de temps qui la délimite, depuis l\u0027instant (appelé tdébut) où l\u0027objet commence un déplacement dans un but donné jusqu\u0027à l\u0027instant (appelé tfin) où le déplacement pour ce but se termine. Identifier (soit directement soit par une règle de calcul) le début et la fin de chaque trajectoire est de la responsabilité de l\u0027application, en fonction de ses propres règles de gestion.\nNous pouvons maintenant définir, au niveau conceptuel, la notion de trajectoire pour un objet ponctuel ou indéformable.\nDéfinition 1 (Trajectoire) : Une trajectoire est la description, du point de vue utilisateur, de l\u0027évolution de la position (perçue comme un point) d\u0027un objet qui se déplace pendant un intervalle de temps donné pour un but particulier.\nTrajectoire : [t début , t fin ] ? espace\nLes trajectoires sont ainsi introduites comme un concept sémantique, qui interprète un segment du parcours d\u0027un objet mobile, identifié par le choix de l\u0027intervalle de temps [t début , t fin ], comme une unité significative par rapport à l\u0027application. Par exemple, dans l\u0027application de suivi des oiseaux, les ornithologistes doivent définir s\u0027ils voient l\u0027aller-retour annuel d\u0027un oiseaux comme une seule trajectoire ou comme deux trajectoires différentes. Dans certains domaines d\u0027application, les trajectoires sont facilement identifiables: par exemple, le parcours des camions de livraison peut être aisément segmenté en trajectoires journalières. Dans d\u0027autres domaines d\u0027application, par exemple le suivi de chimpanzés, il n\u0027est pas toujours évident de déterminer quand l\u0027objet (le chimpanzé) commence une nouvelle trajectoire et quand il continue simplement la trajectoire précédente. Dans ce cas, la segmentation peut être induite par l\u0027observateur plutôt que par l\u0027observé : par exemple, en définissant une trajectoire par période d\u0027observation. Quel que soit le critère choisi par l\u0027application, l\u0027intervalle de temps qui définit la trajectoire, [t début ,t fin ], est nécessairement inclus dans le cycle de vie de l\u0027objet et est nécessairement disjoint (ou adjacent) aux intervalles de temps des autres trajectoires du même objet. Si la segmentation en trajectoires couvre l\u0027intégralité du parcours, les intervalles de temps définissant deux trajectoires successives se touchent toujours. De nouveau, ce choix dépend de l\u0027application.\nLa trajectoire ainsi définie ne constitue que la trace matérielle d\u0027un voyage de l\u0027objet. Cette description minimale peut être enrichie, par exemple en décrivant également, avec plus ou moins de richesse, l\u0027objectif assigné à la trajectoire, les conditions dans lesquelles elle a été réalisée, ou les résultats auxquels elle a abouti.\nLa fonction temps ? espace qui définit une trajectoire n\u0027est pas nécessairement identique à celle produite par le processus d\u0027acquisition des données. Ce dernier fournit les données brutes du déplacement sous la forme d\u0027une suite de couples (point d\u0027échantillonnage, instant). Ces données brutes sont souvent d\u0027abord modifiées (nettoyées), afin de corriger les erreurs de saisie et les approximations réalisées lors de l\u0027acquisition, puis retravaillées pour constituer une fonction qui produise une image du déplacement ayant la qualité souhaitée. Enfin, les besoins applicatifs déterminent comment la fonction-trajectoire doit être définie. L\u0027application pourrait n\u0027être intéressée que par un sous-ensemble des points connus : par exemple, elle peut supprimer les points acquis durant la nuit et ne conserver que les déplacements réalisés durant la journée. Elle peut aussi avoir ses propres règles pour simplifier la fonction en remplaçant certaines séquences de points par un point unique synthétisant la séquence.\nLes objets ne se déplacent pas nécessairement tout le temps pendant une trajectoire (c\u0027est le cas dans l\u0027exemple des oiseaux). Ils peuvent marquer des temps d\u0027arrêt. Les trajectoires peuvent donc être elles-mêmes segmentées en périodes de déplacement et périodes d\u0027absence de déplacement (où l\u0027objet est vu comme immobile). Nous appelons ces premières des déplacements et ces dernières des arrêts. Une trajectoire peut donc être perçue comme une séquence de déplacements allant d\u0027un arrêt à l\u0027arrêt suivant. Par exemple, un oiseau en migration va faire un arrêt quelque part pour un intervalle de temps donné pour se nourrir, un autre arrêt pour se reposer et ainsi de suite jusqu\u0027à ce qu\u0027il atteigne sa destination finale, la fin de sa trajectoire. Des représentants de commerce en déplacement vont faire des arrêts à tous les endroits où ils doivent rencontrer un client.\nComme pour la détermination du début et de la fin des trajectoires, l\u0027identification des arrêts et des déplacements dans une trajectoire est de la responsabilité de l\u0027application. Les arrêts physiques (c\u0027est-à-dire le fait que la position de l\u0027objet est la même durant au moins deux instants consécutifs) ne sont pas systématiquement des arrêts conceptuels, parce qu\u0027ils peuvent être la conséquence d\u0027évènements qui ne sont pas pertinents pour l\u0027application. Par exemple, l\u0027arrêt fait par le représentant de commerce pour boire un café n\u0027est pas pertinent pour l\u0027application. Par contre, l\u0027arrêt réalisé pour rencontrer un client l\u0027est. L\u0027application peut vouloir compter le nombre d\u0027arrêts par trajectoire et ici seuls les arrêts significatifs doivent être comptés. Dans la suite, nous supposons que les arrêts et les déplacements couvrent complètement la trajectoire (c\u0027est-à-dire qu\u0027il n\u0027existe pas d\u0027instant inclus dans Nous ne considérons pas le début et la fin de la trajectoire comme des arrêts. Leur emprise temporelle est en effet un instant, et non pas un intervalle non vide.\nÉtude des besoins pour la modélisation de trajectoires\nLes définitions précédentes sont valables quel que soit le type de trajectoire. Cependant, les besoins en termes de modélisation peuvent varier suivant le type de trajectoire considéré. Nous différencions dans cet article trois types de trajectoires : les trajectoires métaphoriques, les trajectoires géographiques et les trajectoires spatio-temporelles.\nLes trajectoires métaphoriques\nLe terme de trajectoire est quelquefois utilisé dans un sens métaphorique afin de décrire une évolution qui n\u0027est pas un mouvement physique. Par exemple, il n\u0027est pas rare de parler de la trajectoire professionnelle d\u0027une personne pour décrire une succession d\u0027états ou de changements, comme dans \"Je suis passée du milieu académique à l\u0027industrie où j\u0027ai travaillé dans une grande compagnie, puis je suis retournée dans le milieu académique\". Cette utilisation métaphorique de la notion de trajectoire repose sur l\u0027image d\u0027un objet (ici une personne) se déplaçant dans un espace abstrait dont les points sont les différentes valeurs d\u0027un attribut thématique (ici un attribut décrivant le secteur professionnel).\nDu point de vue de la modélisation, ce type de trajectoire peut être décrit en définissant l\u0027attribut thématique comme variable dans le temps, c\u0027est-à-dire un attribut dont la valeur est définie par une fonction du temps vers le domaine de valeurs de l\u0027attribut. Réciproquement, tout attribut thématique variable dans le temps peut être vu comme définissant une trajectoire métaphorique pour les objets qui ont cet attribut. Les modèles de base de données spatiotemporelles existants, tels ceux de Khatri et al. (2004) ou Parent et al. (2006a), proposent ce concept d\u0027attribut variable dans le temps. La variabilité peut être :\n-continue : les valeurs de l\u0027attribut changent de façon continue. C\u0027est le cas, par exemple, de la température mesurée en un point particulier ou de la valeur d\u0027une action en bourse. -par paliers : les changements de valeur sont instantanés et chaque valeur est valable durant un intervalle de temps. C\u0027est le cas, par exemple, du salaire d\u0027un employé ou du secteur professionnel de la personne dans l\u0027exemple ci-dessus. -discrète : les valeurs de l\u0027attribut existent uniquement à certains instants. C\u0027est le cas, par exemple, de l\u0027attribut prime d\u0027un employé qui est un évènement ponctuel. Un attribut variable en continu a nécessairement un domaine de valeurs continu à la granularité près. Par contre, un attribut à domaine de valeurs continu peut être variable selon l\u0027un quelconque des trois types. Le choix entre ces trois types dépend de comment change la valeur de l\u0027attribut dans le temps : évolution continue, remplacement d\u0027une valeur par une autre au bout d\u0027un certain temps, valeur significative seulement à certains instants.\nTant que l\u0027application n\u0027a besoin que de mémoriser l\u0027évolution de la valeur des attributs, modéliser ce type de trajectoire métaphorique ne requiert pas de nouveaux concepts. Cependant, si la description d\u0027une trajectoire métaphorique implique des liens entre la trajectoire et d\u0027autres objets de l\u0027application, la trajectoire doit alors être modélisée comme un objet et non comme un attribut. En conséquence, ces trajectoires, dont la sémantique est plus riche, doivent être modélisées de manière similaire aux trajectoires spatio-temporelles (voir le paragraphe 5.3). Dans ce cas, les relations topologiques et de synchronisation fréquemment utilisées pour modéliser les trajectoires spatio-temporelles seront de simples relations thématiques.\nLes trajectoires à connotation géographique naïve\nLes voyages sont fréquemment décrits comme un déplacement d\u0027un endroit à un autre, par exemple d\u0027une ville à une autre ville: \"Je suis allée à Paris, puis à Bruxelles, puis je me suis rendue à Amsterdam et Berlin avant de revenir à Lausanne\". De façon similaire, les déplacements d\u0027une trajectoire peuvent être définis par référence à des objets à connotation géographique de nature linéaire mais dont les coordonnées spatiales ne sont pas définies ou pas utiles pour l\u0027application. Par exemple : \"J\u0027ai pris le TGV de Lausanne à Paris, puis le Thalys de Paris à Bruxelles...\". Le voyage a dans ces exemples une connotation géographique forte -ce pourquoi nous qualifions ces trajectoires de géographiques -mais ils n\u0027est pas défini en termes de coordonnées spatiales: il est défini par référence à des objets géographiques (comme dans les approches dites \"Géographie Naïve\" Egenhofer et al. (1995)). De fait, les trajectoires géographiques naïves sont un cas particulier de trajectoires métaphoriques. Comme ces dernières, elles peuvent être décrites en utilisant des attributs thématiques variables dans le temps (dans les exemples ci-dessus villeVisitée et trainUtilisé) ou d\u0027une manière similaire aux trajectoires spatio-temporelles.\nLes trajectoires spatio-temporelles\nUne trajectoire est dite spatio-temporelle si des cordonnées spatiales -dans l\u0027espace vu comme ayant deux ou trois dimensions -sont utilisées pour décrire la position de l\u0027objet qui se déplace. Une trajectoire spatio-temporelle comprend deux composantes :\nUne composante géo-temporelle qui décrit physiquement le déplacement de l\u0027objet. Une composante sémantique qui décrit les informations de la trajectoire liées à l\u0027application telles que des annotations et des liens vers les objets décrits dans la base de données. Cette composante sémantique est analysée dans le paragraphe suivant.\nLa sémantique des trajectoires\nLes caractéristiques sémantiques pertinentes des trajectoires varient d\u0027une application à une autre. Cependant, on peut identifier des constantes qui peuvent faire l\u0027objet d\u0027une approche de modélisation générique. Ce sont ces constantes qui nous intéressent ici et qui nous conduisent à proposer un patron de modélisation.\nComme nous l\u0027avons déjà dit, une première préoccupation consiste à définir, s\u0027il en existe, les arrêts qui décomposent la trajectoire en une séquence de déplacements.\nLes arrêts peuvent être définis directement par l\u0027objet mobile. C\u0027est le cas, par exemple, de la trajectoire d\u0027un représentant de commerce qui définira lui-même quand il est en arrêt pour visiter des clients, de même qu\u0027il définira lui-même quand il commence et termine sa tournée, c\u0027est-à-dire une trajectoire. Par contre, d\u0027autres applications ne peuvent pas compter sur des interactions avec l\u0027objet mobile. Elles doivent elles-mêmes repérer et définir les segments du parcours spatio-temporel qui peuvent constituer des arrêts, ainsi que le début et la fin de chaque trajectoire. C\u0027est le cas du suivi des cigognes. Dans ce cas, c\u0027est la connaissance que l\u0027on a du comportement des animaux qui permet de fixer des règles qui, lors de l\u0027analyse de leur déplacement, vont déterminer les composants : début, fin, arrêts et déplacements. Les règles ci-dessous en sont des exemples.\n-Les cigognes ne volent jamais de nuit. -Les cigognes s\u0027arrêtent pour manger et pour se reposer.\n-Le début d\u0027une nouvelle trajectoire est détecté en observant dans la suite des positions spatio-temporelles de l\u0027oiseau un changement entre une période de résidence stable de l\u0027oiseau (plusieurs mois pendant lesquels sa position nocturne reste dans une même zone) et une période de déplacement (une suite de journées où l\u0027oiseau se déplace en s\u0027éloignant de sa zone de résidence). Le déplacement constaté doit se diriger approximativement vers le Sud s\u0027il a lieu en automne, vers le Nord s\u0027il a lieu en fin d\u0027hiver. Les paramètres exacts de ce calcul (taille de la zone de résidence, durée du séjour, durée minimale permettant de conclure à un début de déplacement) sont à fixer en fonction des caractéristiques du comportement des cigognes. -La fin d\u0027une trajectoire est détectée de façon similaire mais inverse, en observant un changement entre une période de déplacement et une période de résidence stable de l\u0027oiseau. -Une suite de positions spatio-temporelles situées entre le début et la fin d\u0027une trajectoire constituent un arrêt si elle satisfait les trois conditions suivantes. 1/ Les positions forment un nuage de points dont le rayon est inférieur à un certain seuil (par exemple 10 Km). 2/ L\u0027intervalle de temps défini par l\u0027instant de la première position et celui de la dernière position a une durée supérieure à un certain seuil (par exemple 30 minutes). 3/ Cette suite est maximale, c\u0027est-à-dire que la position spatiotemporelle précédant la première position de la suite et celle suivant la dernière position ne satisfont pas les conditions. Pour l\u0027arrêt ainsi défini, son point sera le centre du nuage de points et son intervalle sera formé des instants de la première à la dernière position de la suite. -On appellera arrêt de nuit un arrêt d\u0027une durée supérieure à 6h, ayant lieu approximativement du coucher du soleil au lever du soleil le lendemain. -On appellera arrêt de repos tout autre arrêt.\nUne autre caractéristique des trajectoires est qu\u0027une partie, importante, de leur sémantique est traduite par des liens entre la trajectoire et les autres objets de la base de données (en particulier, les objets géographiques comme les rues, bâtiments, villes, régions, pays, lacs...). Lorsque l\u0027objet lié est un objet spatial, doté de sa géométrie, ces liens sont habituellement porteurs de contraintes topologiques (par exemple, pour exprimer que les arrêts de la trajectoire se situent nécessairement dans une ville). Lorsque ce n\u0027est pas le cas, le lien est une simple relation classique d\u0027association. Certains liens sont au niveau de la trajectoire entière, d\u0027autres sont au niveau d\u0027une partie de trajectoire ou de ses composants. Un lien à contrainte spatiale au niveau trajectoire exprime que chaque point de la trajectoire doit satisfaire la contrainte spatiale. Par exemple, telle tournée d\u0027un camion de livraison (c\u0027est-à-dire telle trajectoire du camion) est tout entière incluse dans telle région. Comme exemple de lien à contrainte spatiale au niveau composant, citons de nouveau le cas des trajectoires de migration des cigognes: les ornithologues peuvent lier chacun des arrêts à l\u0027objet géographique surfacique qui est situé à cet endroit et qui présente un intérêt du point de vue ornithologique, tel qu\u0027un marais ou une roselière au bord d\u0027un cours d\u0027eau. La relation sera une relation topologique de type inclusion, ce qui signifie que le point représentant la cigogne doit être dans la surface associée à l\u0027objet géographique. Les ornithologues peuvent aussi vouloir mémoriser quels obstacles la cigogne a survolé, par exemple une ligne à haute tension. Ils pourront le faire en reliant à l\u0027aide d\u0027une relation topologique chaque déplacement de la trajectoire aux objets qui décrivent les obstacles surmontés.\nEn plus de liens vers des objets de la base de données, la description sémantique d\u0027une trajectoire peut conduire à associer à la trajectoire ou à ses composants des attributs et des contraintes d\u0027intégrité, et plus généralement toute caractéristique offerte par un modèle conceptuel. Le processus qui consiste à associer une information thématique aux instances de trajectoire est généralement appelé annotation sémantique. Des exemples d\u0027annotation sont: le but de la trajectoire, le moyen de locomotion, le nom du lieu où se trouve l\u0027objet mobile à chaque instant, le type d\u0027activité de l\u0027objet mobile durant ses déplacements ou durant ses arrêts, les rencontres effectuées lors des arrêts, et pour les migrations d\u0027oiseaux l\u0027altitude et -162 -RNTI-E-13 les conditions météorologiques lors des vols. Suivant le cas, ces annotations prennent la forme de valeurs d\u0027attribut (par exemple, pour le but de la trajectoire) ou d\u0027instanciation d\u0027un lien vers un objet particulier de la base de données (par exemple, pour noter le nom du lieu où se trouve un point de la trajectoire, en supposant que ce lieu est représenté comme un objet dans la base de données).\nCes annotations peuvent avoir la même valeur pour toute la trajectoire (par exemple le but de la trajectoire) ou une valeur différente pour chaque composant de la trajectoire (par exemple la vitesse moyenne par déplacement, l\u0027activité principale pendant chaque arrêt). Ainsi, selon le cas, l\u0027attribut sera un attribut de la trajectoire, du déplacement, ou de l\u0027arrêt. De plus, la valeur de l\u0027attribut peut varier tout au long de la trajectoire ou du déplacement ou de l\u0027arrêt. Dans ce dernier cas l\u0027attribut sera variable dans le temps. Par exemple pour les cigognes, la valeur de l\u0027attribut altitude varie pendant chaque déplacement. Quand il y a plusieurs annotations variables dans le temps, il faut déterminer quelles sont les règles d\u0027échantillonnage utilisées par l\u0027application. Si plusieurs annotations variables sont mesurées aux mêmes instants, alors elles seront décrites par un seul attribut complexe qui regroupera ces différentes valeurs et qui sera variable dans le temps. Il y aura ainsi une seule et même suite de points d\u0027échantillon pour toutes les valeurs. Par contre si chaque annotation variable est mesurée selon une séquence d\u0027instants qui est indépendante de la séquence des autres, alors chaque annotation sera décrite par un attribut simple variable dans le temps.\nDes contraintes d\u0027intégrité thématique, spatiale et temporelle peuvent aussi être associées aux trajectoires. Par exemple, un avion ne peut pas voler s\u0027il n\u0027y a pas une équipe de pilotes et d\u0027hôtesses, les cigognes ne volent pas durant la nuit, les arrêts des représentants doivent inclure au moins un rendez-vous avec un client, un nombre limité d\u0027arrêts est autorisé, ou encore la distance ou la durée entre deux arrêts ne doit pas être inférieure ou supérieure à un certain seuil. Très fréquemment les trajectoires des humains sont contraintes de suivre un réseau spécifique, par exemple les voitures et les camions ne peuvent se déplacer que sur le réseau routier. Une trajectoire contrainte par un réseau doit respecter deux types de contraintes : une contrainte topologique d\u0027inclusion (le point mobile doit toujours être dans la géométrie du réseau), et les contraintes associées aux noeuds du réseau. En effet, le réseau routier décrit non seulement les voies, mais aussi les intersections avec les changements de direction autorisés (tourner à droite ou à gauche, faire demi-tour...). La position des objets qui se déplacent dans un réseau peut être décrite par rapport à la géométrie du réseau : chaque point mobile est alors défini par l\u0027identifiant de la voie et sa position relative sur cette voie (par exemple au 12 ème km), Güting et al. (2006). Un type particulier de trajectoire contrainte est celui des trajectoires fixes et récurrentes, comme celle des trains, métros et bus, qui font toujours le même trajet avec les mêmes arrêts, avec des horaires prédéfinis. D\u0027autres trajectoires ont des contraintes spatiales ou temporelles particulières, qui doivent être calculées dynamiquement. C\u0027est le cas, par exemple, des trajectoires des oiseaux migrateurs qui font du vol plané grâce aux thermiques, qui ne peuvent pas traverser de grandes étendues d\u0027eaux (les cigognes contournent la Méditerranée) ou s\u0027effectuer de nuit à cause de l\u0027absence de thermiques.\nLes applications peuvent aussi souhaiter conserver des caractéristiques dérivées du mouvement, comme la direction, la vitesse, l\u0027accélération (instantanée ou moyenne). Les informations de ce type sont calculables à partir de la fonction qui définit la trajectoire ou de la séquence de points d\u0027échantillon et sont généralement obtenues en appelant des méthodes du type de données point mobile.\nEnfin, les applications doivent avoir la possibilité de spécifier plusieurs représentations de la même trajectoire selon le point de vue, la résolution et les objectifs de l\u0027application. Tous les composants et toutes les informations sémantiques de la trajectoire peuvent avoir plusieurs représentations. Par exemple, si on considère la trajectoire d\u0027une cigogne, une application aura besoin de différencier les différents types d\u0027arrêts le long de la trajectoire (de repos, de nuit, etc...) et décrira chaque type avec des informations spécifiques. Une autre application aura simplement besoin de savoir que l\u0027oiseau s\u0027est arrêté et à tel endroit pendant tant de temps, sans s\u0027intéresser à d\u0027autres informations relatives à cet arrêt. Enfin, une troisième application ne prendra en compte que les longs arrêts de repos et ignorera les arrêts qui ne durent qu\u0027une nuit. Un mécanisme qui permet cette représentation multiple pour les objets spatio-temporels a été présenté dans MADS, Parent et al. (2006b).\nEn résumé, une trajectoire est composée d\u0027un début, d\u0027une fin, de déplacements et d\u0027arrêts. Un modèle conceptuel pour trajectoires doit permettre la définition de trajectoires et de leurs composants avec des attributs, des liens vers les objets de la base de données, des contraintes thématiques, spatiales et temporelles. Chaque élément a une géométrie, qui est ponctuelle pour le début, la fin et les arrêts, et qui est linéaire pour les déplacements. Chaque élément a une extension temporelle qui est un instant pour le début et la fin, et un intervalle de temps pour les arrêts et les déplacements. Ces derniers peuvent donc être annotés avec des attributs variables ou non dans le temps.\nModélisation des trajectoires\nCertains modèles conceptuels pour bases de données incluent la description des aspects spatiaux et temporels, par exemple Parent et al. (2006a), Tryfona et al. (2003), Bédard et al. (2004). Ces modèles permettent de décrire des objets en mouvement et répondent donc en partie aux besoins de modélisation des trajectoires : la description de la composante géo-temporelle. Mais ils n\u0027offrent pas de constructeurs spécifiques aux trajectoires; les notions de trajectoire, de début, de fin, d\u0027arrêt et de déplacement ne font pas partie du modèle et seront donc ignorés du SGBD sur lequel la base de données sera implantée. Ces notions restent alors du domaine exclusif des utilisateurs et des programmes d\u0027application. Nous avons donc étudié différentes solutions pour compléter les services offerts par les SGBD spatiotemporels, et en avons développé deux. La première est basée sur la définition d\u0027un patron de modélisation, la seconde sur celle de nouveaux types de données. Dans cet article, nous présentons la première solution, la seconde est décrite dans Spaccapietra et al. (2008).\nLe concept de patron de modélisation -de l\u0027anglais \"design pattern\" Gamma et al. (1995) -vient du génie logiciel. Appliqué aux bases de données, il dénote un schéma prédéfini qui décrit une solution simple à un problème de modélisation récurrent. Il peut être utilisé par le concepteur de bases de données dans l\u0027étape de modélisation. Le patron est en effet prévu pour être facilement intégré dans un schéma de bases de données. Notre patron de modélisation pour trajectoires vise à représenter explicitement les trajectoires et leurs composants, début, fin, arrêts et déplacements. Cette solution utilise un modèle conceptuel spatio-temporel sans le modifier, ce qui permet de l\u0027implémenter facilement sur tout SGBD qui offre un support pour le spatial (par exemple Oracle, DB2 ou SQL Server) ou sur un système d\u0027information géographique. Cette solution réifie les composants des trajectoires : le début, la fin, les arrêts et les déplacements sont représentés par des objets. Cette réification a pour but de satisfaire un des besoins vus aux paragraphes précédents : le début, la fin, les arrêts et les déplacements doivent pouvoir être reliés à des objets de la base de données. Pour cela, ils doivent être représentés eux aussi comme des objets. En effet, les modèles de données permettent de relier entre eux des éléments de premier rang (les entités en entité-relation, les objets en orienté-objets, les tables en relationnel), mais pas les attributs. Cette solution a l\u0027avantage de donner des schémas clairs, lisibles et qui permettent facilement de compléter la description des trajectoires avec l\u0027information sémantique spécifique à l\u0027application. De plus, ces schémas peuvent évoluer facilement.\nLe paragraphe 6.1 décrit dans ses grandes lignes le modèle conceptuel spatio-temporel, MADS, dans lequel est décrit le patron pour trajectoires. Le patron est présenté en détail au paragraphe 6.2, et un exemple de schéma utilisant le patron est décrit au paragraphe 6.3.\nLe modèle conceptuel spatio-temporel MADS\nLe modèle MADS est un modèle de données de type entité-relation étendu au spatial, au temporel et à la multi-représentation (possibilité d\u0027avoir plusieurs représentations différentes pour une même information). Pour la dimension thématique, on notera simplement que MADS permet de décrire des attributs simples et des attributs complexes (composés d\u0027autres attributs), des attributs monovalués (qui prennent une valeur) et des attributs multivalués (qui peuvent prendre plusieurs valeurs).\nPour la dimension spatiale, MADS offre un jeu de types de données spatiales, organisé en une hiérarchie. Les types les plus courants sont Point, Line, Surface, et le type générique Geo qui contient toutes les valeurs spatiales qu\u0027elles soient simples ou complexes. De même il y a une hiérarchie de types de données temporels dont Time est la racine et Instant, TimeInterval, InstantSet sont les plus courants. En MADS, les types d\u0027objet, tout comme les types de relation, peuvent être spatiaux et/ou temporels. Un type d\u0027objet (ou de relation) spatial possède un attribut particulier, appelé geometry, qui décrit son emprise spatiale et dont le domaine de valeurs est l\u0027un des types spatiaux. Un type d\u0027objet (ou de relation) temporel possède un attribut particulier, appelé lifecycle, qui décrit sa durée de vie et les états par lesquels il passe au cours de sa vie : actif, suspendu, invalidé. Certains types de relations contraignent les objets qu\u0027ils lient. Ce sont d\u0027une part les relations topologiques qui relient deux objets spatiaux et contraignent leurs attributs geometry à satisfaire un prédicat topologique, par exemple adjacence, intersection ou inclusion, et d\u0027autre part les relations de synchronisation qui relient deux objets temporels et contraignent leurs attributs lifecycle à satisfaire un prédicat temporel, par exemple l\u0027un doit succéder à l\u0027autre. Les attributs, quelles que soient leurs caractéristiques, peuvent varier dans le temps et/ou dans l\u0027espace. Dans ce cas, leur valeur est définie au niveau conceptuel par une fonction qui associe à chaque instant et/ou chaque point un élément du domaine de définition de l\u0027attribut. Si l\u0027attribut est spatial (c\u0027est-à-dire qu\u0027il a pour domaine un des types de données spatiales) et variable dans le temps, alors cet attribut décrit un déplacement et/ou une déformation. Par exemple, un attribut de domaine Point et variable dans le temps a pour valeur un point mobile. Il existe trois catégories d\u0027attribut variable : attribut variable en continu, par paliers ou attribut variable discret. Chacune correspond à une façon de varier des phénomènes décrits. Par exemple, la température varie de façon continue, le salaire d\u0027un employé varie par paliers. Selon la catégorie d\u0027attribut variable, les valeurs sont relevées de façon différente. Les valeurs d\u0027un attribut variable en continu ou discret sont fournies sous la forme d\u0027une suite de couples (valeur-instantanée, instant), et celles d\u0027un attribut variable par paliers sous la forme d\u0027une suite de couples (valeur-instantanée, intervalle-de-temps).\nLe lecteur qui voudrait avoir plus de détails sur le modèle de données MADS est invité à consulter Parent et al. (2006a).\nPatron de conception pour les trajectoires\nComme nous l\u0027avons vu, un patron de conception pour les trajectoires doit comprendre un type d\u0027objet pour représenter chacun des concepts : trajectoire, début, fin, arrêt, et déplacement. La figure 2  Les deux types d\u0027objet Noeud et Déplacement sont reliés par deux types de relation, De et Vers, représentant le fait que chaque déplacement commence et finit par un arrêt. Ces deux relations sont à la fois topologiques et de synchronisation (à contrainte de type adjacence temporelle avant/après). Les deux contraignent chaque déplacement à être lié à des arrêts qui leur sont adjacents dans l\u0027espace et dans le temps. Dans les modèles de données spatiotemporels, la sémantique des relations spatiales qui contraignent les géométries de deux objets doit être étendue au cas où l\u0027un des deux objets (voire les deux) bouge ou se déforme. À cette fin en MADS, les relations topologiques qui lient des objets qui bougent ont été étendues en demandant au concepteur de définir la portée temporelle de la contrainte : la contrainte doit être satisfaite soit au moins pendant un instant, soit pendant toute la durée de vie de l\u0027objet, ce qui dans notre cas revient à toute la durée de la trajectoire ou toute celle d\u0027un arrêt ou d\u0027un déplacement. Par exemple, le fait qu\u0027une cigogne passe au-dessus d\u0027une ligne à haute tension pendant un certain déplacement, sera représenté en deux dimensions par une relation topologique d\u0027inclusion du point mobile (la cigogne) dans la ligne (haute tension) pendant au moins un instant du déplacement. Dans le patron de la figure 2, la contrainte topologique de la relation De (Vers), qui lie un déplacement à son noeud de départ (fin), est une contrainte d\u0027égalité qui doit être satisfaite au premier (dernier) instant du déplacement.\nFIG. 2 -Un patron de modélisation de trajectoires\nEn plus de la description de la structure interne de la trajectoire, le patron inclut des relations-crochets qui seront utilisées par le concepteur pour relier la trajectoire aux objets de l\u0027application. Dans la figure 2, les noms des crochets sont écrits en italiques. Le type d\u0027objet Trajectoire est relié à un type d\u0027objet-crochet ObjetATrajectoires qui représente l\u0027objet du monde réel qui effectue les trajectoires. Début, fin et arrêt (Noeud) peuvent être reliés à des types d\u0027objets spatiaux qui sont représentés par le type d\u0027objet-crochet spatial générique, ObjetGéographique1. La relation EstDans impose une contrainte topologique d\u0027inclusion : le noeud doit être situé dans l\u0027objet géographique. Comme ce crochet est optionnel, il est dessiné en pointillés. De façon similaire, les déplacements peuvent être reliés à un type d\u0027objet-crochet spatial ObjetGéographique2 par une autre relation topologique d\u0027inclusion, EstSur. Cette dernière peut être utilisée pour modéliser les trajectoires contraintes par un réseau.\n-167 -RNTI-E-13\nModélisation conceptuelle des trajectoires\nFIG. 3 -Le patron de trajectoires utilisé pour l\u0027application des Cigognes\nUn exemple d\u0027emploi du patron de trajectoires\nQuand un patron est utilisé, les concepteurs doivent l\u0027adapter à leur application. Ils peuvent, par exemple, supprimer des éléments sans intérêt pour l\u0027application, ajouter de nouveaux éléments répondant à des besoins supplémentaires ou modifier la structure du patron pour l\u0027adapter à leurs besoins. Par exemple, des types d\u0027objet Début, Fin et Arrêt peuvent être ajoutés comme sous-types de Noeud. Afin de connecter le patron au reste du schéma, les concepteurs doivent remplacer les types d\u0027objet-crochet par des types d\u0027objet de l\u0027application ou rajouter d\u0027autres relations-crochets spécifiques à l\u0027application.\nLa figure 3 montre un exemple d\u0027utilisation du patron de trajectoires pour l\u0027application de migration des cigognes. Les concepteurs ont personnalisé le patron de la façon suivante. Le type d\u0027objet-crochet ObjetATrajectoires a été mis en correspondance avec le type d\u0027objet Cigogne. Le type d\u0027objet-crochet ObjetGéographique1 et sa relation ont été dédoublés et sont devenus Pays et Zone (ce dernier avec sa hiérarchie de sous-types). Zone et ses sous-types décrivent les zones d\u0027intérêt pour les oiseaux. De la même façon, le type d\u0027objet-crochet ObjetGéographique2 et sa relation ont été dédoublés. La relation PasseSur, qui décrit le fait que l\u0027oiseau est passé au-dessus d\u0027une zone dangereuse pour lui, est une relation topologique de type inclusion pour un instant au moins : Pour qu\u0027une instance de Déplacement puisse être liée à une instance de DangerNaturel le point mobile du déplacement doit être au moins un instant dans la zone du danger naturel. Quant à la relation PassePrès son type a été modifié. C\u0027est une relation métrique qui contraint la distance entre les objets liés. Un nouveau lien, non spatial, Avec, a été ajouté au type d\u0027objet Déplacement; il décrit le groupe -s\u0027il est connu -avec lequel l\u0027oiseau a fait ce vol (ce déplacement).\nFinalement, des attributs spécifiques à l\u0027application ont été ajoutés aux types d\u0027objets du patron. Par exemple, la direction (Nord/Sud), l\u0027année de la trajectoire, la météo durant la trajectoire ont été ajoutés à Trajectoire; l\u0027altitude est enregistrée le long des déplacements; pour chaque arrêt où l\u0027oiseau a été observé, le type de l\u0027arrêt, les activités de l\u0027oiseau, son poids et son pourcentage de graisse sont enregistrés. Les attributs météo et altitude sont des attributs variables dans le temps, c\u0027est-à-dire que l\u0027historique de la météo est enregistré durant la totalité de la trajectoire et l\u0027historique de l\u0027altitude de l\u0027oiseau est enregistré durant chaque déplacement. En pratique, la fonction spécifiant un attribut continu variable dans le temps est définie par une liste de couples (valeur, instant), appelées valeurs d\u0027échantillon (ou points d\u0027échantillon dans le cas de points variables dans le temps) et par des fonctions d\u0027interpolation entre les points de l\u0027échantillon. Quand un objet comporte plusieurs attributs variables dans le temps, a priori leurs fonctions sont indépendantes, c\u0027est-à-dire que leurs valeurs d\u0027échantillon sont définies pour différents instants. Cependant, si les valeurs de plusieurs attributs variables sont toujours enregistrées en même temps, ces attributs sont temporellement liés. Et cela doit être explicitement spécifié par une contrainte d\u0027intégrité ou en regroupant les attributs dans un attribut complexe qui sera, lui, variable dans le temps. Par exemple, dans la figure 3, météo est un attribut complexe variable dans le temps dont les valeurs d\u0027échantillon possèdent le format suivant: (instant, vent, température, pression, ciel).\nD\u0027un autre côté, le type d\u0027objet Déplacement possède deux attributs variables dans le temps, geometry et altitude qui ne peuvent pas être regroupés dans un attribut complexe parce que geometry est un attribut prédéfini par le modèle de données MADS. Une contrainte d\u0027intégrité est donc nécessaire afin d\u0027exprimer la contrainte suivante : \"Les listes de points d\u0027échantillon des attributs geometry et altitude sont basées sur la même liste d\u0027instants\". Enfin, afin de faciliter l\u0027accès à l\u0027information pour les utilisateurs, les concepteurs peuvent choisir de stocker de l\u0027information redondante et de gérer la cohérence de cette redondance via des contraintes d\u0027intégrité. Par exemple, l\u0027attribut naissance.année du type d\u0027objet Cigogne est défini comme dérivé de l\u0027instant de début du cycle de vie de l\u0027objet et l\u0027attribut Nord/Sud de Trajectoire comme dérivé des positions spatiales du début et de la fin de la trajectoire (c\u0027est-à-dire les positions des premier et dernier objets Noeud).\nImplémentation de l\u0027approche\nAfin de valider nos approches de modélisation des trajectoires, nous sommes en train de recueillir les descriptions et les données de plusieurs applications portant sur des trajectoires. Nous modélisons les bases de données des applications, puis les implémentons sur uun SGBD relationnel étendu au spatial (Oracle 10). Nous utilisons ces bases de données de plusieurs façons : pour répondre à des requêtes ponctuelles de type classique (SQL), pour faire de la fouille des trajectoires à l\u0027aide de programmes spécifiques, et enfin nous montons un entrepôt spécialisé pour les trajectoires.\nDans ce paragraphe, nous présentons pour l\u0027application des cigognes le schéma relationnel de la base de données qui correspond au schéma conceptuel de la figure 3 et, à titre d\u0027exemples, quelques requêtes SQL. Le schéma relationnel (voir la figure 4)  \nFIG. 4 -Schéma relationnel correspondant à la figure 3\nUne autre spécificité des schémas MADS est la présence de types de relation topologique ou de synchronisation temporelle qui contraignent les géométries ou les cycles de vie des objets liés. Ces types de relation à contraintes sont traduits de la même façon que des relations classiques, mais avec en plus un trigger qui vérifie lors des insertions et des mises à jour des objets liés que leurs géométries ou leurs cycles de vie satisfont bien la contrainte. Par exemple, le type de relation topologique EstDans, qui lie Noeud à Pays dans la figure 3, est traduit, dans la À titre d\u0027exemple, nous donnons deux requêtes SQL qui peuvent être posées à la base de données. La requête \"Combien de fois la cigogne Max s\u0027est-elle arrêtée durant chacune de ses trajectoires ?\" peut être exprimée comme suit : SELECT t.N°Traj, t.Debut, COUNT(n.N°Noeud) FROM Trajectoire AS t, Noeud AS n WHERE t.N°Traj\u003dn.N°Traj AND t.nomCigogne\u003d\"Max\" GROUP BY t.N°Traj, t.Debut Quant à la requête : \"Quelles activités a eu la cigogne pendant chaque arrêt de la trajectoire 133 ?\", elle s\u0027écrit très simplement :\nSELECT N°Noeud, activité FROM NoeudActivité WHERE N°Traj\u003d133\nConclusion\nDans les applications qui s\u0027intéressent à des objets mobiles, une connaissance approfondie des trajectoires spatio-temporelles de ces objets est très souvent indispensable à la réalisation des objectifs de l\u0027application. Par exemple, l\u0027optimisation d\u0027un réseau de transport demande que l\u0027on recueille les données sur les déplacements de la population concernée et que l\u0027on modélise ces déplacements. Le plus souvent la connaissance recherchée ne se limite pas au tracé spatio-temporel des déplacements. Elle peut inclure d\u0027autres informations, par exemple le moyen de transport utilisé, la classe d\u0027âge de la personne ou sa capacité de mouvement (invalides, handicapés), ou encore des informations sur les lieux visités. La description des informations pertinentes relatives à une trajectoire peut donc conduire à des constructions complexes qui combinent des caractéristiques relatives au mouvement brut (où est l\u0027objet et quand) avec une variété d\u0027annotations sémantiques décrivant les connaissances spécifiques nécessaires à chaque application.\nLa contribution de cet article est essentiellement de définir précisément ce concept de trajectoire et d\u0027en donner une caractérisation qui, tout en s\u0027appuyant sur une étude des besoins, est indépendante de toute application particulière. En particulier, nous proposons de structurer les trajectoires grâce aux concepts complémentaires d\u0027arrêt et de déplacement, ce qui permet d\u0027enrichir le contenu sémantique d\u0027une trajectoire. Ensuite, l\u0027article propose une approche de modélisation des trajectoires à l\u0027aide d\u0027un patron de conception (design pattern), autrement dit une construction standard (ici exprimée dans les termes d\u0027un modèle entité-relation) que les concepteurs peuvent réutiliser quelle que soit la nature de leur application. L\u0027article détaille les composantes du patron proposé et la manière de l\u0027intégrer dans le schéma de la base de données de l\u0027application. Enfin, l\u0027implémentation du patron sur un SGBD relationnel-objet est montrée.\nÀ notre connaissance, ce travail est le premier à proposer une approche conceptuelle pour modéliser la sémantique des objets mobiles. Il ajoute ainsi une couche sémantique par rapport aux approches habituelles de modélisation qui représentent uniquement les déplacements bruts des objets mobiles.\nLe travail présenté peut être étendu dans plusieurs directions. Par exemple, il conviendrait d\u0027explorer les contraintes particulières des trajectoires liées à un réseau (par exemple, les voitures circulant sur un réseau routier) afin de déterminer l\u0027influence de ces contraintes en termes de modélisation. Dans le cadre du projet GeoPKDD (http:/geopkdd.it) nous explorons une stratégie similaire de modélisation pour un entrepôt de trajectoires, première étape pour réaliser des fouilles de données destinées à extraire des trajectoires des connaissances plus synthétiques pour les décideurs du domaine étudié (voir, par exemple, Alvarez et al. 2007et Giannotti et Nanni 2006.\nLe travail pour le développement d\u0027un entrepôt de trajectoires doit notamment conduire à la spécification d\u0027opérateurs d\u0027agrégation de trajectoires (voir Braz et al. 2007). Différents types d\u0027agrégation sont possibles et peuvent être réalisés à différents niveaux en distinguant par exemple les opérateurs appliqués aux composants d\u0027une trajectoire des opérateurs appliqués sur la trajectoire entière. Un exemple d\u0027opération sur les composants est l\u0027agrégation qui remplace un ensemble d\u0027arrêts dans une région par un arrêt unique représentant l\u0027intégralité du temps passé dans cette région. Un exemple d\u0027opération réalisée sur la trajectoire entière est l\u0027agrégation qui remplace un ensemble de trajectoires par la trajectoire moyenne calculée à partir de l\u0027ensemble.\n"
  },
  {
    "id": "857",
    "text": "Introduction\nL\u0027utilisation des descripteurs locaux permet d\u0027obtenir de bons résultats pour la reconnaissance d\u0027images, la classification d\u0027images et la recherche d\u0027images par le contenu. Ces descripteurs sont robustes aux changements de contenu. Cette méthode a été proposée en 1997 par C. Schmid dans (Schmid et Mohr, 1997). Récemment, les méthodes développées originellement pour l\u0027analyse des données textuelles (ADT) comme pLSA (probabilistic Latent Semantic Analysis) (Hofmann, 1999a), LDA (Latent Dirichlet Allocation) (Blei, 2003) sont appliquées en analyse d\u0027images, par exemple pour la classification des images (Willamowski, 2004), la découverte des thèmes dans l\u0027image (Sivic et al., 2005), la classifications des scènes (Bosch et al., 2006), et la recherche d\u0027images (Lienhart et Slaney (2007)).\nDans ce travail, nous utilisons l\u0027analyse factorielle des correspondances (AFC) pour la recherche d\u0027images. Etant donné une image requête, le système doit retourner les images (dans la collection des images) les plus similaires à la requête. L\u0027AFC permet de réduire l\u0027espace pour représenter les images et calculer la similarité entre les images dans cet espace réduit. Les deux contributions principales de cet article sont l\u0027utilisation de l\u0027AFC pour la recherche d\u0027images, et la proposition d\u0027un prototype de recherche d\u0027images utilisant des fichiers inversés basés sur la qualité de représentation des facteurs de l\u0027AFC.\nL\u0027article est organisé de la façon suivante : nous décrivons brièvement les méthodes pLSA et l\u0027AFC dans la section 2. La section 3 présente la recherche d\u0027images par l\u0027AFC. La section 4 est consacrée aux résultats expérimentaux. Dans la conclusion, nous présentons les perspectives de ce travail.\nMéthodes\nReprésentation des images\nQu\u0027il s\u0027agisse des méthodes comme le pLSA (probabilistic semantic latent analysis) ou l\u0027AFC, il faut d\u0027abord représenter le corpus sous forme d\u0027une matrice d\u0027occurrences F (un tableau de contingence) de dimension M x N où M désigne le nombre de documents et N indique le nombre de différents mots apparaissant dans le corpus. Chaque case de la matrice F ij décrit le nombre de fois où le mot j (indice de colonne) est observé dans le document i (indice de ligne). Une telle représentation ignore l\u0027ordre des mots dans un document et est appelée modèle sac-de-mots (bag-of-words).\nIl n\u0027y pas de mots au sens littéral du terme dans les images. Il faut donc les construire.\nConstruction des mots visuels\nLes mots dans les images, appelés mots visuels, doivent être calculés pour constituer un vocabulaire de N mots. Chaque image sera donc représentée enfin par un histogramme de mots. La construction des mots visuels se fait en deux étapes : (i) calcul des descripteurs locaux pour un ensemble d\u0027images, (ii) classification (clustering) des descripteurs obtenus. Chaque cluster correspondra à un mot visuel. Il y aura donc autant de mots que de clusters obtenus à l\u0027issue de l\u0027étape (ii).\nLe calcul des descripteurs locaux dans une image se fait aussi en deux étapes : il faut d\u0027abord détecter des points d\u0027intérêt dans l\u0027image. Ces points d\u0027intérêt sont, soit des maximums du Laplacien de Gaussien (Lindeberg, 1998), soit des extremums locaux 3D de la différence de Gaussien (Lowe, 1999), soit des points extraits par un détecteur Hessian-affine (Mikolajczyk, 2004). Ensuite, le descripteur de ce point d\u0027intérêt est calculé sur le gradient des niveaux de gris dans la région autour du point. On a sélectionné des descripteurs invariants à la rotation et au changement d\u0027échelle, les descripteurs SIFT (Lowe, 2004). Chaque descripteur SIFT est un vecteur à 128 dimensions. La seconde étape consiste à former des mots visuels à partir des descripteurs locaux calculés à l\u0027étape précédente. La plupart des travaux effectue un k-means sur les descripteurs locaux et prend les moyennes de chaque cluster comme mots visuels (Willamowski, 2004, Sivic, 2005, Bosch et al., 2006.\nAprès avoir construit le vocabulaire visuel, chaque descripteur est affecté au cluster le plus proche. Pour cela, on calcule dans R 128 les distances de chaque descripteur aux représen-tants des clusters définis précédemment. Une image est ensuite caractérisée par la fréquence de ses descripteurs dans chaque cluster. On obtient ainsi un tableau de contingence croisant les images et les clusters.\nDans nos expérimentations, nous avons utilisé la méthode décrite dans (Mikolajczyk, 2004) pour détecter des points d\u0027intérêt. Le vocabulaire est construit en appliquant un kmeans sur environ 300000 descripteurs tirés aléatoirement (un tiers pour chaque catégorie : faces, motorbikes, airplanes, background et cars). Le vocabulaire obtenu est de 2224 mots pour 4090 images. Ce choix de 2224 mots a été effectué par Sivic (Sivic et al., 2005).\nPLSA\nIntroduite par Thomas Hofmann (1999a), le pLSA est une technique statistique qui s\u0027inspire du LSA (Latent Semantic Analysis) (Deerwester, 1990) pour l\u0027analyse des tableaux de contingence. LSA est une méthode purement géométrique qui ressemble beaucoup aux méthodes factorielles ; dans pLSA, on introduit un modèle probabiliste : la distribution des mots dans une image est considérée comme multinomiale. La méthode se base sur une dé-composition des mélanges dérivée d\u0027un modèle de variables latentes.\npLSA introduit une variable latente z ? Z \u003d {z 1 , z 2 , …, z K } et modélise la probabilité jointe par :\nLe logarithme de la vraisemblance du corpus est défini par : \nAnalyse factorielle des correspondances\nL\u0027AFC est une méthode exploratoire classique pour l\u0027analyse des tableaux de contingence. Elle a été proposée par J. P. Benzécri (1973) dans le contexte de la linguistique, c\u0027est-à-dire pour l\u0027analyse de données textuelles. La première étude a été réalisée sur les tragédies de Racine. L\u0027AFC sur un tableau croisant des mots et des documents permet de répondre aux questions suivantes : y a-t-il des proximités entre certains mots ? Y a-t-il des proximités entre certains documents ? Y a-t-il des liens entre certains mots et certains documents ? L\u0027AFC comme la plupart des méthodes factorielles utilise une décomposition en valeurs singulières d\u0027une matrice particulière et permet la visualisation des mots, et des documents dans un espace de dimension réduit. Cet espace de dimension réduit a la particularité d\u0027avoir un nuage de points projetés (mots et/ou documents) d\u0027inertie maximale. Par ailleurs, l\u0027AFC fournit des indicateurs pertinents pour l\u0027interprétation des axes comme la contribution d\u0027un mot ou d\u0027un document à l\u0027inertie de l\u0027axe ou la qualité d\u0027un mot et/ou d\u0027un document sur un axe (Morin, 2004).\nPour déterminer le meilleur sous-espace de projection des données, on calcule les valeurs propres et les vecteurs propres de la matrice de taille N x N, où\nX est la transposée de X . On obtient alors les valeurs propres ? et les vecteurs propres u :\nOn ne garde que les K (K \u003c N) premières valeurs propres les plus grandes et les vecteurs propres associés. Ces K vecteurs propres constituent une base orthonormée de l\u0027espace réduit (appelé aussi espace des facteurs). Le nombre de dimensions du problème passe de N à K. Les documents (images) sont projetés dans le nouvel espace réduit :\nDans cette formule, X P 1 ? représente les profils lignes et A est la matrice de transition associée à l\u0027AFC. La projection des mots dans le sous-espace de dimension K est fournie par la formule suivante:\nUn nouveau document (i.e. \nMesure de similarité des images\nAprès avoir calculé les coordonnées des images dans le nouvel espace (i.e. représentation probabiliste dans le cas de pLSA, sous-espace pour l\u0027AFC), nous devons définir la similarité entre une requête et les images. Plusieurs métriques pour cette mesure de similarité sont disponibles, parmi les quelles les 4 mesures suivantes :\nDistance Manhattan (norme 1):\nDivergence de Jensen-Shannon :\nDans les expérimentations, nous avons utilisé la distance euclidienne et celle de cosinus.\nRecherche d\u0027images par l\u0027AFC\nAFC pour réduction de dimension\nUn des avantages de l\u0027AFC est de réduire la dimension du problème. Pour tenir compte du nombre d\u0027images, il est préférable d\u0027utiliser une structure d\u0027indexation sous forme d\u0027arbre comme un arbre k-d (Bentley, 1975). Cependant, une telle structure deviendra inefficace quand le nombre de dimensions est supérieur à 16 à cause de la malédiction de la dimension (Bellman, 1961). Enfin, l\u0027indexation par l\u0027arbre utilise la distance euclidienne la plupart du temps car on rencontre des difficultés lorsqu\u0027on travaille avec d\u0027autres distances.\nPassage à l\u0027échelle\nIl y a deux indicateurs importants pour l\u0027interprétation et l\u0027évaluation en AFC. Ce sont la contribution des images à l\u0027inertie d\u0027un axe (facteur) d\u0027une part et la qualité de représenta-tion des images sur un axe d\u0027autre part. Dans les expérimentations, nous avons constaté que la distance cosinus donnait de meilleurs résultats que la distance euclidienne. La distance cosinus est directement liée à la qualité de représentation des images sur les axes. Pour cette raison, nous avons proposé un nouveau prototype de recherche d\u0027images utilisant des fichiers inversés basés sur la qualité de représentation. Cela permettra de réduire le nombre d\u0027images à considérer lors du calcul de leur similarité avec la requête. \nRNTI -X -\nAprès avoir réduit la dimension des données à K, on construit, pour chaque axe, 2 fichiers inversés (un pour la partie positive et un autre pour la partie négative)\n1 . Chaque fichier contient des images ayant une bonne qualité de représentation sur la partie (positive, néga-tive) de l\u0027axe associé. Le seuil ? est un paramètre qui contrôle la qualité de résultat et le temps de recherche. Dans les expérimentations, nous avons choisi un seuil égal à la moyenne de la qualité de représentation, à la moitié, et à un quart de la moyenne. Plus le seuil est grand, plus le nombre d\u0027images dans un fichier inversé est petit, moins il faut de temps de recherche mais la qualité de résultat diminue.\nPrototype de recherche\nAlgorithme pour rechercher une image requête q est donné dans le tableau 1.\nEntrée : un vecteur q représentant l\u0027image requête 1. Projeter q dans l\u0027espace des facteurs suivant la formule [3] et trier les facteurs par leur qualité de représentation. 2. Prendre NF fichiers inversés associés à NF premiers facteurs. 3. Faire l\u0027union des fichiers inversés et filtrer les images par ces fichiers inversés : une image sera conservée si elle apparaît dans au moins NF/2 fichiers. 4. Calculer la distance entre la requête et les images conservées dans l\u0027étape 3. Sortie : la liste des images les plus similaires à l\u0027image requête TAB. 1 -Algorithme de recherche utilisant des fichiers inversés.\nLe nombre d\u0027images retrouvées après le filtrage sera beaucoup plus petit que le nombre d\u0027images dans la base. Donc, le temps de recherche sera considérablement réduit.\nLe nombre NF est choisi empiriquement. Cependant, il est préférable que NF soit impair et inférieur à K/2 (pour le vote majoritaire dans l\u0027étape de filtrage). Nous proposons une heuristique qui peut être utilisée pour déterminer NF en fonction de l\u0027image requête. Cette heuristique se base sur l\u0027observation suivante : si un point est bien représenté sur quelques axes, le cosinus carré sur ces axes sera grand et le cosinus carré des autres axes sera petit car la somme des cosinus carrés est égale à 1. Donc, on peut prendre NF axes tels que la somme des cosinus carrés sur ces axes est supérieure à ? (ex : ? \u003d 0.75).\nRésultats\nNous avons implémenté l\u0027AFC en GNU Octave (Eaton, 1995). Sur un PC Pentium 4, 512MO avec système d\u0027exploitation Linux, le temps d\u0027exécution pour l\u0027AFC sur un tableau de contingence de 4090 x 2224 est d\u0027environ 3 minutes.\nEnsemble de tests\nLes tests ont été réalisés sur la base Caltech4 (Sivic et al., 2005) tirée de la base Caltech101 (Fergus et al., 2003 La base est divisée en 10 sous-ensembles, et l\u0027évaluation est faite par validation croisée.\nRNTI -X -\nAFC vs d\u0027autres méthodes\nNous avons fait une AFC sur les données de la base Caltech4 et avons gardé les 7 premiers axes (pour la comparaison au pLSA, avec 7 modalités). Lorsqu\u0027on augmente le nombre d\u0027axes conservés, on constate que les premières images retournées (i.e. les 10 premières images) sont bonnes mais que la qualité se dégrade lorsqu\u0027on retient un plus grand nombre d\u0027images (voir le tableau 3). La distance euclidienne et la distance cosinus sont utilisées pour calculer les similarités dans les espaces de dimension réduite. Nous avons utilisé la courbe de précision -rappel pour comparer la performance des différentes méthodes.\nFIG. 2 -Projection de la base Caltech4 sur les axes : à gauche, projection des images (sans background) sur les axes 1 et 2 ; à droite, projection des images (avec catégorie background) sur les axes 1, 2 et 3.\nTF*IDF\nAvec cette méthode, chaque élément F(i,j) dans le tableau de contingence est normalisé à TF(i,j) et pondéré par IDF(j) où TF(i, j) est le nombre de mots j qui apparaît dans l\u0027images i divisé par le nombre de mots dans l\u0027image i et IDF(j) \u003d ln(N/N j ) où N i est le nombre d\u0027images qui contiennent le mot j et N le nombre d\u0027images dans la base. La distance euclidienne et la distance cosinus sont calculées sur les données pondérées.\npLSA\nNous avons appliqué un modèle pLSA avec 7 modalités (ce modèle donne les meilleurs résultats sur cette base (cf. Sivic et al., 2005)). Chaque image dans la base est représentée par sa distribution P(z|d). La dimension du problème est réduite à 7. Les nouvelles coordonnées servent à calculer la similarité entre la requête et les images dans la base (Hofmann, 1999b).\nDiscussion\nLe nombre de thèmes (modalités de la multinomiale) dans pLSA et le nombre d\u0027axes conservés en AFC sont des paramètres à régler. Le nombre d\u0027axes conservés en AFC est difficile à choisir car les valeurs propres en AFC décroissent très lentement. Nous avons pris 7 axes en AFC pour la comparaison avec pLSA. La figure 3 montre les résultats quand on fait des tests sur 4 catégories (on ne tient pas compte de la catégorie « background ») et sur 5 catégories (avec la catégorie « background »). Le meilleur résultat est obtenu avec l\u0027AFC quelle que soit la distance, euclidienne ou cosinus. Nous avons également testé TF*IDF avec la distance de Manhattan. Dans ce cas, on améliore le résultat mais ce n\u0027est pas significativement supérieur aux résultats obtenus avec d\u0027autres distances, ni avec pLSA et l\u0027AFC.\nFIG. 3 -Courbe de précision -rappel : à gauche, résultat pour 4 catégories (sans background) et à droite, résultat pour 5 catégories (avec background).\nAFC avec fichiers inversés\nPour comparer la performance de la nouvelle méthode de recherche avec la méthode exhaustive, nous avons calculé la précision sur les 5, 10, 50 et 100 premières images retournées. Le paramètre K est pris égal à 7, 15 et à 30 ; le seuil pour des fichiers inversés est mis à 1/(4*K); NF est calé à 3. Les résultats figurent dans le tableau 3. La nouvelle méthode limite le nombre d\u0027images pour lesquelles on calcule la similarité avec la requête. Cela diminue le temps de réponse. En général, la nouvelle méthode est 4 fois plus rapide que la méthode exhaustive (on gagne 75% du temps) avec une perte inférieure à 1% du taux de précision. Par ailleurs, dans certains cas (ex. K \u003d 15, et avec 5 premières images retournées), elle fait mieux que la méthode exhaustive. Enfin, dans tous les cas, la nou-RNTI -X -velle méthode est meilleure que TF*IDF en qualité de résultat (plus de 10%) ainsi que le temps de recherche (13 fois plus rapide).\nLes Quand NF est petit (ex. NF \u003d 1), un seul axe ne suffit pas en terme d\u0027information récupé-ré et donc le résultat n\u0027est pas bon. Quand on prend plus d\u0027axes (ex. NF \u003d 3) la qualité augmente et le filtre devient également trop contraignant (dans le cas NF \u003d 7, une image sera gardée si elle apparaît au moins 4 fois dans les fichiers inversés). Par conséquent, cela dé-grade la qualité du résultat. C\u0027est pour cela qu\u0027il faut choisir le paramètre NF en se basant sur la somme des cosinus carrés des NF premiers axes (cf. Section 3.1.2). Sur la base Caltech4, NF moyen est égale à 4.6 et notre proposition est justifiée a posteriori.\nConclusion et perspectives\nNous avons présenté dans cet article une nouvelle approche pour la recherche d\u0027images par le contenu en utilisant l\u0027AFC. Cette méthode est testée sur la base Caltech4 et comparée aux méthodes classiques comme : TF*IDF et pLSA. Les expérimentations ont montré que dans tous les cas, l\u0027AFC donne le meilleur résultat. Nous avons aussi proposé une nouvelle approche utilisant des fichiers inversés basés sur la qualité de représentation des images sur les axes. La nouvelle méthode réduit le temps d\u0027exécution et dans certains cas, améliore le taux de précision par rapport à la méthode exhaustive. Comme la plupart des méthodes de réduction de dimensions, une question concerne le nombre de dimensions à conserver. Dans la littérature, on conserve souvent 100 dimensions pour LSA et 30 pour l\u0027AFC. On peut suggérer l\u0027utilisation de la méthode Bayésienne de (Teh et al., 2004) pour déterminer ce nombre.\nPour le passage à l\u0027échelle, les méthodes basées sur la décomposition de la base d\u0027images en sous-bases sont prometteuses. La base peut être décomposée en clusters et l\u0027AFC est appliquée sur les clusters. Un algorithme heuristique sera utilisé pour sélectionner les clusters dans lesquels il faudra chercher la réponse à la requête. Une autre amélioration des résultats est de combiner l\u0027AFC avec d\u0027autres méthodes comme CDM (Contextual Dissimilarity Measure) de (Jegou et al., 2007) et/ou des méthodes d\u0027apprentissage pour les rangs (Chu, 2005 ;Burges, 2005).\n"
  },
  {
    "id": "859",
    "text": "Introduction\nLes Machines à Vecteurs de Support (SVM) sont une méthode très populaire d\u0027apprentissage supervisé pour la classification et la régression. Dans sa forme la plus simple pour la classification bi-classes, cette méthode est basée sur un classificateur linéaire séparant deux ensembles de points par un hyperplan. L\u0027idée originale est de trouver un hyperplan séparant \"au mieux\" les points par la maximisation de la marge entre l\u0027hyperplan séparateur et les points dans la base d\u0027apprentissage. Cette formulation conduit à un problème d\u0027optimisation d\u0027une fonction convexe sous des contraintes linéaires. Récemment des extensions de cette technique de base et de l\u0027approche de maximisation de la marge ont été proposées pour le traitement de données structurées comme les séquences, les arbres etc (Tsochantaridis et al., 2004).\nLa méthode originale de Vladimir Vapnik pour résoudre le problème d\u0027optimisation avec contraintes des SVMs consiste à introduire des multiplicateurs de Lagrange pour chaque contrainte, et d\u0027optimiser le problème dual équivalent. Cet algorithme est coûteux en temps et en mémoire. Par exemple, l\u0027espace mémoire nécessaire (la matrice noyau est de taille N au carré, si N est le nombre d\u0027exemples). Ces caractéristiques de complexité rendent difficile l\u0027emploi de machines à vecteurs support et plus généralement de méthodes de maximisation de la marge dans certaines situations, lorsque l\u0027on traite des données structurées ou bien lorsque l\u0027on dispose de très grandes quantités de données d\u0027apprentissage. Plusieurs voies ont été suivies pour dépasser les problèmes posés par l\u0027optimisation dans ce cadre.\nCertains travaux ont porté sur l\u0027optimisation efficace du dual, par le contrôle du nombre de contraintes actives (Joachims, 2006), ou par la décomposition du problème d\u0027apprentissage (Osuna et al., 1997). Dans ce dernier cas, l\u0027algorithme SMO ou SVMLight par exemple, on ne s\u0027intéresse à une itération donnée qu\u0027à un nombre limité de variables actives.\nDes travaux plus récents ont porté sur l\u0027optimisation directe de la forme primale par l\u0027usage de la fonction hinge(z)\u003dmax(0,z). Cela permet de se ramener à un problème d\u0027optimisation sans contraintes où la fonction objectif est convexe. La difficulté de ces dernières approches vient du fait que la fonction hinge n\u0027est pas dérivable en 0. Divers travaux ont alors proposé d\u0027utiliser une version dérivable partout de cette fonction hinge par lissage ou bien en utilisant un coût quadratique car dans ce cas des méthodes d\u0027optimisation standard peuvent être appliquées. Par exemple, (Chapelle, 2007) a montré qu\u0027il était possible d\u0027utiliser la méthode de Newton ou bien du gradient conjugué.\nUne autre approche pour optimiser le primal consiste à utiliser la méthode du sous-gradient directement sur le problème d\u0027optimisation de la fonction objectif non différentiable (Zhang, 2004). L\u0027avantage de cette approche est sa simplicité, mais la vitesse de convergence est très dépendante du réglage du pas de gradient. Une exception est l\u0027algorithme Pegasos (ShalevShwartz et al., 2007) qui est le seul algorithme de ce type ne nécessitant pas le réglage d\u0027un hyperparamètre pour le pas de gradient.\nDans ce travail, nous nous plaçons dans le cadre de l\u0027optimisation de fonction continue non partout dérivable. L\u0027idée principale est que la minimisation du primal peut être attaquée comme un problème minimax sur un ensemble de fonctions quadratiques convexes définies sur des sous-espaces de l\u0027espace des paramètres. Nous montrons que la fonction objectif n\u0027est pas dérivable sur les frontières entre ces sous-espaces de définition et que ces frontières correspondent à des hyperplans associés à chaque exemple d\u0027apprentissage. En analysant ces hyperplans dans l\u0027espace des paramètres, nous décrivons une méthode efficace pour calculer la direction de plus grande pente et estimer le pas optimal. En exploitant ces résultats, nous proposons un nouvel algorithme d\u0027optimisation qui se compare favorablement aux algorithmes de type Pegasos en mode batch et en mode on-line.\nNous décrivons tout d\u0027abord le cadre d\u0027optimisation dans lequel nous nous plaçons et les outils que nous allons utiliser. Ensuite nous décrivons notre algorithme en détails puis le validons expérimentalement en le comparant à des algorithmes de référence.\nPréliminaires\nNous formalisons tout d\u0027abord le problème que nous attaquons ici et qui consiste à optimiser une fonction objectif non partout différentiable. Nous rappelons ensuite quelques résultats sur ce type de fonctions et décrivons brièvement une méthode classique pour leur optimisation.\nFormalisation\nConsidérons un problème de classification de données d\u0027entrée x en 2 classes y ? {?1, +1}. Considérons une base d\u0027apprentissage\nNous nous intéressons à l\u0027apprentissage du classifieur linéaire : h w (x) \u003d sign( w où w ? R d est l\u0027ensemble des paramètres du classifieur à apprendre. Notons que w divise R d en deux sous-espaces, la frontière est un hyperplan d\u0027équation H w : w \u003d 0. L\u0027idée principale de l\u0027apprentissage vaste marge est de trouver un hyperplan séparant \"au mieux\" les points par maximisation de la marge entre l\u0027hyperplan séparateur et les points de la base d\u0027apprentissage. Cette formulation conduit à un problème d\u0027optimisation d\u0027une fonction convexe sous des contraintes linéaires :\noù ? est un hyper-paramètre de l\u0027algorithme qui permet de régler l\u0027importance des deux termes de la fonction objectif. Les ? i sont des variables non-négatives qui représentent des pénalités pour les cas où la contrainte de marge n\u0027est pas respectée pour l\u0027exemple x i .\nEn introduisant la fonction hinge(z) \u003d max(0, z), on obtient un problème équivalent sans contraintes :\nNotons que cette fonction objectif est quadratique par morceau et convexe mais qu\u0027elle n\u0027est pas différentiable (par rapport à w) en certains points, sur les hyperplans H i : 1 ? y i i , w \u003d 0. Il y a un hyperplan par exemple d\u0027apprentissage. En des points qui n\u0027appartiennent à aucun de ces hyperplans la fonction objectif est localement quadratique.\nChaque hyperplan H i divise l\u0027espace des w en deux sous-espaces :\nDans chaque hypercube C k , la fonction objectif a une forme quadratique :\nensemble des indices des exemples qui violent la contrainte de la marge pour w ? C k . Par construction cet ensemble est identique pour tous les w d\u0027un même hypercube C k , c\u0027est pourquoi nous ne marquons pas la dépendance de I k à w. Avant de présenter notre algorithme en détail, nous commençons par quelques résultats théoriques sur l\u0027optimisation de fonctions non-différentiables.\nGénéralités sur l\u0027optimisation de fonctions non différentiables\nDans cette section, nous présentons quelque résultats concernant l\u0027optimisation et l\u0027analyse des fonctions convexes non-différentiables. Ces résultats, et leurs dérivations, peuvent être trouvés dans (Bertsekas et al., 2003;Demyanov et Vasilev, 1985). Nous introduisons tout d\u0027abord la définition du sous-gradient et de la sous-différentielle d\u0027une fonction convexe.\nSoit f :\nL\u0027ensemble de tous les sous-gradients en x 0 est appelé la sous-différentielle en x 0 , et est noté par ?f (x 0 ). La sous-différentielle est un ensemble compact, non vide et convexe.\nThéorème 1 (Demyanov and Vasilev) : Une condition nécessaire pour une fonction continue non partout dérivable, éventuellement non convexe, f (x) :\n. Pour une fonction convexe, la condition est également suffisante. Si 0 / ? ?f (x * ) alors la direction ? \u003d ?arg min d??f (x) est la direction du sous-gradient de plus grande pente.\n.., m} un ensemble de fonctions convexes différentiables, alors la sous\nMéthode du sous-gradient\nUn façon d\u0027optimiser le problème de l\u0027équation (2) est d\u0027utiliser la méthode du sousgradient, qui converge vers le minimum global (Bertsekas et al., 2003). Nous comparerons notre algorithme à une méthode de référence de ce type (Shalev-Shwartz et al., 2007). A chaque itération, le nouveau vecteur de paramètres w t est calculé par :\noù ? t est le pas de sous-gradient et ? t ? ?f (w t ) est un sous-gradient quelconque de la fonction f à w t .\n3 Descente de sous-gradient pour l\u0027optimisation du primal\nDans ce travail, nous proposons d\u0027appliquer un algorithme du type descente de sousgradient. La différence avec la méthode du sous-gradient est que nous choisissons la direction du sous-gradient selon la plus grande pente (plutôt que de prendre un sous-gradient quelconque dans la sous-différentielle), et que nous proposons une méthode optimale pour déterminer le pas de gradient. L\u0027algorithme est résumé par le pseudo code décrit ce-dessous.\nCet algorithme a comme nous le verrons une complexité linéaire dans le nombre d\u0027exemples d\u0027apprentissage à chaque itération. En effet, il passe en revue tous les exemples d\u0027apprentissage pour déterminer la direction de descente de plus grande pente et pour ensuite calculer le pas de gradient optimal dans cette direction. Nous décrivons maintenant ces étapes de calcul de la direction de plus grande pente du gradient et de calcul du pas de gradient optimal.\nEntrées : BA \u003d {(x 1 , y 1 ), ..., (x N , y N )} Sorties : w Initialization : w 1 \u003d 0; t \u003d 1 ; tant que vrai faire\nCalculer la direction du sous-gradient de plus grande pente ? t ; si t \u003d 0 alors return w t ; ; Calculer le pas de gradient optimal ? t ; Mise à jour : w t+1 \u003d w t + ? t ? t ; t \u003d t + 1; fin 3.1 Direction du sous-gradient de plus grande pente Rappelons que nous souhaitons minimiser la fonction objectif sous sa forme primale donnée dans l\u0027Eq. (2). Nous cherchons dans un premier temps à déterminer la sous-différentielle de f (w) en un \"point\" w. Pour cela nous commençons par nous intéresser à la sous-différentielle de la fonction élémentaire max(0,\n1?yi N\n). Cette fonction est différentiable sauf pour les points w tel que y i i , w \u003d 1. Pour un point de ce type on obtient en appliquant le théorème 2 :\nPar ailleurs en un point w tel que y i i , w \u003d 1 la fonction est dérivable et sa sousdifférentielle est réduite à sa dérivée. La sous-différentielle de la fonction élémentaire s\u0027exprime donc suivant les cas par :\nFinalement, en utilisant la proposition 1 de la section précédente, on obtient :\nIl est intéressant de noter que cette formulation est proche de celle utilisée dans (Eizinger et Plach, 2003). Cela vient de la présence de fonctions hinge dans la formalisation de l\u0027apprentisage du perceptron, qui peut s\u0027écrire comme la minimisation d\u0027une fonction linéaire par morceaux, en fait la somme des fonctions hinge(?y i i , w A noter que dans ce cas la fonction objectif est non strictement convexe et qu\u0027il peut exister, dans le cas séparable, une infinité de solutions. Dans notre formalisation la fonction objectif comporte un terme quadratique 2 et des termes hinge(1 ? y i i , w ce qui rend le problème strictement convexe. Par ailleurs, notre fonction objectif vise à maximiser la marge ce qui doit conduire à de meilleures propriétés de généralisation.\nLa recherche de la direction du sous-gradient de plus grande pente peut être vu comme le problème des moindres carrés suivant :\nCe problème peut être résolu par des procédures standard (voir (Boyd et Vandenberghe, 2004)). Une fois ? * calculé on obtient la direction du gradient de plus grande pente par la direction donnée dans le Théorème 1 avec les valeurs de ? * données dans l\u0027Eq. (11). Notons pour terminer que si K \u003d 0 alors la fonction est différentiable en w et la direction du gradient de plus grande pente est simplement ? t \u003d ?d 0 .\nPas de gradient optimal\nFIG. 1 -L\u0027espace des w.\nUne fois la direction de recherche ? t déterminée comme décrit à la section précédente, nous proposons ici une méthode pour déterminer le pas de gradient de façon optimale. Cela revient à résoudre le problème d\u0027optimisation unidimensionnel suivant :\nNous nous intéressons à la droite D t \u003d {w \u003d w t + ?? t |? ? R} et au comportement de f (w) sur cette droite. Si l\u0027on \"avance\" sur la droite D t dans la direction de ? t , c\u0027est à dire qu\u0027on examine les w \u003d w t + ?? t pour ? croissant, alors w va successivement croiser des hyperplans H i frontières entre hypercubes et traverser d\u0027autres hypercubes. Au passage, la fonction f (w) change d\u0027une fonction quadratique d\u0027un hypercube à une autre (voir figure 1). La fonction de ? que nous cherchons à optimiser, g(?) \u003d f (w t + ?? t ) est une fonction quadratique par morceaux (voir figure 2). Les points non-différentiables correspondent aux intersections entre w t + ?? et les hyperplan H i séparateurs. On peut caractériser l\u0027intersection, si elle existe, entre la droite D t et un hyperplan H i : 1 ? y i i , w \u003d 0 par une valeur particulière de ?, que nous notons ? i . Elle est déterminée par :\nUne valeur positive de ? i signifie que l\u0027hyperplan H i est \"devant\" w t (dans la direction ? t ) et une valeur négative de ? i signifie le contraire. Imaginons, sans perte de généralité, que la droite décrite par w \u003d w t + ?? t en faisant croître ? de zéro vers l\u0027infini traverse successivement les hyperplans H 1 , H 2 , H 3 ... (voir Figure  2a). Sur le segment ? n , ? n+1 , g(?) vaut :\ni?I k(n) où k(n) est l\u0027indice de l\u0027hypercube correspondant au n ieme segment. Sa dérivée par rapport à la variable ? est :\nA l\u0027optimum cette dérivée, si elle existe, est nulle. Dans ce cas :\nSi ? n opt appartient effectivement au n ieme segment, et que g(?) est dérivable en cette valeur alors le pas optimal vaut ? n opt . Cependant, g(?) peut ne pas être dérivable à l\u0027optimum (cf. figure 2b), cela signifie que l\u0027optimum correspond à un ? sur une frontière entre segments (i.e. le w optimal est sur une frontière, un hyperplan séparateur, entre hypercubes). Dans ce cas, pour tout n l\u0027optimum de g n (?) est \"en dehors\" du n ieme segment. Une méthode simple pour vérifier si le n ieme point d\u0027intersection ? n est la solution est de calculer ?\nopt alors ? n est la solution, sinon ? n n\u0027est pas la solution. Au final, notre algorithme pour trouver le pas de gradient optimal est décrit par le pseudo code ci-dessous. Dans ce code, nous notons L le nombre d\u0027hyperplans à envisager (a priori inconnu mais nécessairement L ? N ), c\u0027est à dire ceux correspondant à des exemples pour lesquels il existe une intersection.\nEntrées : BA, w t , ? t Sorties : ? t Initialization; Estimer I 0 ? {i|y i i , w ? 1?w ? [w t , w t + ? 1 ?]}; Estimer les ? corespondant aux intersections de D t avec les hyperplans H i , sélectioner des ? non-negatives et les trier dans l\u0027ordre croissant :\n.., i L les indices des hyperplans (i.e. exemples d\u0027apprentissage) correspondents; n \u003d 0; ? 0 \u003d 0; tant que vrai faire\nShrinking et Complexité\nL\u0027algorithme itératif présenté dans la section précédente se compare avantageusement à d\u0027autres algorithmes d\u0027optimisation batch proposés récemment, en termes de performance et de complexité algorithmique. Nous proposons ici une variante beaucoup plus rapide de cet algorithme. Rappelons que l\u0027algorithme passe en revue tous les exemples d\u0027apprentissage (ou plutôt les hyperplans correspondants) pour déterminer la direction de descente de plus grande pente et pour ensuite calculer le pas de gradient optimal dans cette direction. Or en pratique, il y a le plus souvent très peu d\u0027exemples qui contribuent à l\u0027estimation de la direction de recherche et à l\u0027estimation du pas de gradient optimal. Ce sont des exemples que nous appelons des exemples actifs, les autres étant passifs. On peut donc espérer casser la complexité de l\u0027algorithme en réduisant le problème d\u0027estimation à chaque itération en ne considérant que les exemples actifs. Nous proposons ici d\u0027utiliser une méthode de shrinking similaire à ce qui est utilisé dans les techniques d\u0027optimisation du dual pour sélectionner à une itération donnée un nombre restreint de variables actives (Joachims, 1999).\nPour minimiser la fonction objectif de l\u0027Eq. (2), nous n\u0027allons considérer à chaque itération qu\u0027un nombre limité d\u0027hyperplans actifs, en cherchant à optimiser la fonction :\noù L A représente la liste des exemples actifs et L I représente la liste des exemples inactifs violant la marge. L\u0027itération d\u0027optimisation est réalisée comme décrit à la Section 3, la seule différence venant du fait que certains hyperplans ne sont pas considérés (les termes correspondants sont rajoutés à d 0 , cf. Eq. (11)) et que l\u0027on cherche une solution w t+1 dans l\u0027espace délimité par les hyperplans actifs. La procédure de sélection des exemples/hyperplans est heuristique. A l\u0027initialisation tous les exemples sont actifs. A une itération t on considère comme actifs les K t hyperplans les plus proches de la solution courante. Afin d\u0027obtenir une optimisation très rapide, on réduit de moitié le nombre K t d\u0027hyperplans actifs à chaque itération.\nPour garantir que la solution est correcte vis à vis des hyperplans sélectionnés, on restreint de plus l\u0027espace de recherche à une boule B(w t , R t ) centrée autour de la solution courante et de rayon R t , qui est calculée comme la distance maximale entre la solution courante et les hyperplans actifs sélectionnés. Il s\u0027agit d\u0027une heuristique qui permet d\u0027éviter la plupart du temps de traverser un hyperplan inactif lors de la mise à jour de w. Si c\u0027est le cas la solution trouvée est identique à celle que l\u0027on trouverait avec tous les hyperplans actifs. Si ce n\u0027est pas le cas on n\u0027a plus cette garantie. C\u0027est la raison pour laquelle nous avons choisi, régulièrement, de tout réinitialiser en réactivant tous les hyperplans, soit lorsque le nombre K t est inférieur à un seuil (e.g. 10) soit lorsque le rayon R t tombe à zéro. Cette procédure permet de garantir la convergence comme pour l\u0027algorithme originel de la Section 3.\n.N }; L I \u003d ?; R t \u003d inf tant que vrai faire 1. Estimer la direction de recherche optimale ? t en considérant la fonction objectif de l\u0027Eq. (11), pour les hyperplans actifs de L A 2. si |? t | \u003d 0 alors Arrêt fin 3. Estimer le pas optimal ? t pour la fonction objectif de l\u0027Eq. (12), dans la direction ? t et pour les hyperplans actifs de L A 4. Contraindre la nouvelle solution à appartenir à B(w t , R t ) : ? t \u003d min(? t , Rt?1 ) 5. Mise à jour de w : \nExpériences\nNous décrivons ici des résultats expérimentaux obtenus en classification d\u0027images de chiffres manuscrits sur la base MNIST 1 . Elle contient 60000 exemples d\u0027apprentissage et 10000 exemples de test, les images sont en dimension 28x28. Nous avons prétraité les données via une analyse en composantes principales (ACP) afin de réduire la dimension des données à 50 dimensions (on ne garde que les 50 composantes des images sur les 50 axes principaux d\u0027inertie). Il s\u0027agit d\u0027un prétraitement standard sur ces données, décrit par exemple dans (LeCun et al., 1998).\nNous avons comparé notre algorithme noté HyperPass (Hyperplane Passenger) avec l\u0027algorithme Pegasos, basé sur la méthode de sous-gradient. Ce dernier s\u0027est montré expérimentale-ment très efficace par rapport aux méthodes d\u0027optimisation du dual pour les bases de données de grand dimension. Tout d\u0027abord nous comparons la vitesse de convergence de Pegasos en mode batch et de notre algorithme sans la stratégie de shrinking. La figure 3 montre l\u0027évolu-tion du primal en fonction du nombre de passages sur la base de données. On voit que notre algorithme HyperPass converge beaucoup plus rapidement que Pegasos. Notons toutefois que chaque passage de la base de données correspond à une itération de HyperPass avec complexité 2 × O(N d), tandis que la complexité d\u0027une itération de Pegasos est en O(N d). On voit égale-ment que la courbe de Pegasos est large car la valeur primal oscille beaucoup entre itérations successives.\nFIG. 3 -MNIST \u00270\u0027 vs all -Primal objectif.\nNous avons également étudié sur le même problème la version HyperPass avec shrinking et observé que cet algorithme converge après quelques cycles d\u0027itérations (l\u0027algorithme se termine au 7 ieme cycle, ce qui correspond à une complexité de 28 × O(N d)). Nous comparons les solutions de HyperPass Shrinking et de la version on-line de Pegasos (beaucoup plus rapide que la version batch) pour une même complexité algorithmique (tableau 1). Le paramètre K de Pegasos représente le nombre d\u0027exemples utilisés pour faire un pas de sous-gradient, Enfin, nous comparons la solution de HyperPass Shrinking à celles obtenues avec l\u0027algorithme Pegasos en le faisant tourner jusqu\u0027à arriver à une valeur du primal seuil égale à celle obtenue par HyperPass + pour différentes valeurs de Le tableau 2 montre que Pegasos descend très vite au debut lorsque K est petit, mais que la vitesse de convergence est plus faible ensuite. Par exemple, dans le cas K \u003d 10, il faut 51 passages pour arriver à une solution avec \u003d 0.001 tandis qu\u0027il faut 246 passages pour arriver à une solution avec \u003d 0.0001. \nConclusions\nNous avons proposé dans ce travail un nouvel algorithme d\u0027apprentissage vaste marge pour des machines de type SVM. Cet algorithme optimise la fonction objectif sous sa forme primale en combinant la méthode du sous-gradient, des résultats sur l\u0027optimisation de fonctions non\n"
  },
  {
    "id": "860",
    "text": "Introduction\nLes réseaux de neurones supervisés sont d\u0027excellents régresseurs permettant à la fois de classer des données ou de trouver des relations entre des entrées et des sorties (régression). Cependant ils sont bien souvent durs à mettre en oeuvre de part le nombre important de paramètres. L\u0027utilisation d\u0027algorithmes évolutionnaires (en particulier les algorithmes génétiques) permet de faciliter la mise en oeuvre de ces réseaux, en définissant leur structure ou les poids des connexions. Grâce à une inspiration fortement biologique, nous proposons un nouvel algorithme, RBF-Gene, qui permet une optimisation incrémentale d\u0027un réseau RBF (structure et connexions) de manière efficace.\nAlgorithmes évolutionnaires et réseaux de neurones\nDans un réseau de neurones chaque neurone réalise un traitement simple et ce sont le nombre de neurones et leur connectivité qui vont faire toute la puissance du réseau. Dans les réseaux de neurones dits \"en couche\", le réseau est constitué de trois sous-ensembles de neurones : les neurones d\u0027entrée, les neurones cachés (complètement connectés aux neurones d\u0027entrée ou, si le réseau possède plus d\u0027une couche cachée, à la couche précédente) et les neurones de sortie connectés à la dernière couche de neurones cachés.\nLes réseaux RBF (Poggio et Girosi, 1989), pour \"Radial Basis Function\", sont des réseaux à une couche cachée dont les neurones cachés utilisent une fonction de transfert gaussienne tandis que les neurones de sortie réalisent une \"simple\" somme pondérée des réponses des neurones cachés. Ces réseaux sont des approximateurs universels (Park et Sandberg, 1991) et sont très efficaces pour des tâches de classification ou de régression. Leur utilisation passe par le choix d\u0027un grand nombre de paramètres libres (le nombre de neurones cachés, leurs paramètres -moyenne et écart-type -et les poids de leur combinaison linéaire). Or ces paramètres sont tous interdépendants ce qui rend leur détermination difficile.\nLes algorithmes évolutionnaires (AE) ont souvent été employés pour paramétrer des ré-seaux de neurones. Ils recouvrent différentes techniques inspirées de l\u0027évolution biologique : algorithmes génétiques, programmation génétique, stratégies d\u0027évolution, évolution grammaticale, etc. . . Cependant, le principe général est toujours le même : une population de solutions est générée aléatoirement puis va successivement subir une phase de sélection puis de reproduction jusqu\u0027à un critère de terminaison.\nOn peut classer les AEs permettant l\u0027optimisation de réseaux de neurones en trois caté-gories. Les optimiseurs de poids (Blanco et al., 2001) permettent d\u0027optimiser les poids d\u0027un réseau dont la structure a été fixée préalablement. Les optimiseurs de structure (MacLeod et Maxwell, 2001;Barrios et al., 2001) permettent de tester différentes structures pour le réseau de neurones (un second algorithme étant utilisé pour déterminer les poids). Enfin, les optimiseurs de structure et de poids (Arotaritei et Negoita, 2002;Ku¸sçuKu¸sçu et Thornton, 1994) permettent d\u0027optimiser simultanément la structure du réseau et les poids des connexions.\nDans ces derniers algorithmes, la représentation choisie est souvent de taille variable, une partie correspondant à la structure du réseau (nombre de neurones cachés et leur placement) et une autre aux poids des connexions entre les neurones cachés et les sorties. Or la structure d\u0027un réseau a une forte influence sur le nombre de connexions et donc de poids. Toute mutation dans la partie \"structure\" doit entraîner une modification de la taille de la partie \"connexions\". L\u0027évolution va alors se faire en deux temps : tout d\u0027abord l\u0027évolution de la première partie et son décodage, puis l\u0027évolution de la seconde en fonction des informations de la première. Des processus spécifiques doivent être mis en oeuvre pour maintenir la cohérence des génomes, en particulier pour les opérations de croisement (crossover).\nNos individus possèdent donc trois niveaux d\u0027organisation : un génome composé d\u0027un ensemble de gènes détectés localement, un protéome composé de l\u0027ensemble de nos protéines virtuelles (des neurones cachés entièrement définis), et un phénotype qui est le réseau RBF construit à partir du protéome. Le passage du protéome au phénotype est trivial : le réseau construit est simplement l\u0027assemblage des différents neurones cachés. Nous nous intéresserons donc surtout au passage du génome au protéome, et aux différents opérateurs génétiques. Une présentation détaillée de l\u0027algorithme peut être trouvée dans (Lefort, 2007).\nCode génétique et mapping génotype-protéome\nChacun de nos gènes code pour un neurone caché et la séquence de ce gène doit définir totalement ce neurone de la même façon qu\u0027un gène détermine totalement la séquence d\u0027une protéine. Ceci est rendu possible par l\u0027utilisation d\u0027un code génétique qui va transformer le contenu du gène en une suite d\u0027acides aminés, celle-ci étant ensuite décodée pour calculer les paramètres du neurone : la moyenne (qui est un vecteur dont la dimension est celle des entrées), l\u0027écart-type et le vecteur de poids de sortie.\nNous avons choisi de coder chacun de ces paramètres avec un code binaire dont la longueur pourra évoluer, de manière à pouvoir avoir des valeurs grossières en début d\u0027évolution (gènes courts) et, si nécessaire, précises en fin d\u0027évolution (gènes longs).  Notre génome est constitué d\u0027une suite de lettres (ou bases), deux lettres particulières (A pour \"start\" et B pour \"stop\") permettant de délimiter les gènes, les autres étant utilisées par paires pour coder les différents paramètres. Le nombre de lettres nécessaires dépend du problème : outre le \"start\" et \"stop\", il faut deux lettres pour chaque caractéristique du neurone (une pour représenter un 0 binaire, et une pour le 1). Ainsi, sur un problème avec une entrée et une sortie, nous avons besoin de 8 lettres.\nPour reconstituer la valeur d\u0027un paramètre, il suffira de rechercher les deux bases correspondantes dans le gène, de les extraire (les différents paramètre pouvant être mixés) et de les transformer en suite de 0 et de 1. Il suffit ensuite de normaliser la valeur binaire pour obtenir une valeur réelle (figure 1).\nOpérateurs\nNotre algorithme conserve la boucle générationnelle des algorithmes évolutionnaires. Cependant la structure génétique nous autorise à élargir le répertoire des opérateurs de mutation puisque ceux-ci peuvent modifier les gènes et/ou la structure du génome.\nNous avons sept opérateurs de mutations. Trois opérateurs locaux n\u0027agissent que sur une seule base à la fois et vont principalement modifier les valeurs des paramètres : le switch (qui remplace une base par une autre), la délétion ponctuelle (qui supprime une base) et l\u0027insertion (qui en rajoute une). Trois opérateurs globaux agissant sur des segment génétiques (qu\u0027ils contienne des gènes ou non) permettent des remaniements de la structure génétique. Il s\u0027agit de la translocation (qui déplace une zone à un autre endroit du génome), de la duplication (qui insère une copie d\u0027une zone dans le génome) et de la délétion large (qui supprime une zone du génome). Enfin, nous avons un crossover à un point, qui sera effectué après un alignement à gauche des génomes pour permettre un échange de gènes.\nÉtude en régression\nNous avons étudié notre algorithme sur des benchmarks de régression, afin de vérifier tout d\u0027abord la facilité de paramétrage, puis les résultats en convergence et enfin le déroulement de l\u0027évolution. Le benchmark utilisé ici est Boston Housing (Automatic Knowledge Miner (AKM) Server, 2003), qui possède 13 entrées pour une sortie. Il est constitué de 506 points (405 pour l\u0027apprentissage et 101 pour la validation). L\u0027algorithme a été testé sur d\u0027autres benchmarks (1D et 2D Sine Wave, Abalone), les résultats en convergence pouvant être trouvés dans (Lefort, 2007).\nLe paramétrage n\u0027est pas détaillé ici, mais nous utilisons les réglages que l\u0027on peut trouver dans (Lefort, 2007). On notera que notre algorithme ne possède que deux paramètres qui ne soient pas classiques aux algorithmes évolutionnaires (taux des mutations larges et taille initiale des génomes).\nMême si la comparaison de notre algorithme avec d\u0027autres algorithmes de régression n\u0027est pas le coeur de ce travail, notre algorithme a montré qu\u0027il était compétitif avec les principaux algorithmes de régression utilisés dans (Madigan et Ridgeway, 2004). Les détails des comparaisons se trouvent dans (Lefort, 2007).\nNos solutions au terme de l\u0027évolution sont compétitives avec d\u0027autres algorithmes de ré-gression. Notre algorithme possède cependant de nombreux degrés de liberté (nombre de neurones, taille des gènes, taille des génomes. . .) qu\u0027il est intéressant d\u0027étudier. Nous allons nous intéresser à deux indicateurs : la taille du génome et le taille moyenne des gènes. Les évolutions de ces indicateurs sont représentées figure 2.\nOn remarque que dans une toute première phase, la taille du génome augmente de manière exponentielle. Cependant, et malgré l\u0027absence de processus limitant la taille, celle-ci se stabilise à 15000 bases (la valeur de stabilisation dépendant du problème et du taux de mutation). Contrairement à d\u0027autres algorithmes (de type programmation génétique) nous n\u0027avons donc pas une explosion de la taille.\nCette augmentation de la taille est due à deux facteurs. Le premier est l\u0027augmentation du nombre de gènes (de 2 à 80 par génome) ce qui permet de résoudre efficacement le problème Boston Housing. Le deuxième correspond à la taille des gènes qui reste stable dans les toutes premières générations puis augmente fortement pour se stabiliser vers 150 bases. Cette augmentation correspond à une amélioration de la précision du codage : à la convergence, chaque paramètre est codé en moyenne sur dix bits.\nNotre algorithme gère donc de façon dynamique le nombre de gènes comme leur taille (et donc le nombre de neurones cachés et la précision des paramètres), notre précision tout comme notre nombre de neurones restant cohérents avec le problème posé.\nConclusion\nEn nous inspirant de la biologie, nous avons proposé un algorithme évolutionnaire qui permet d\u0027optimiser des réseaux RBF pour la régression. Notre génome est homogène et composé de gènes indépendants les uns des autres et pouvant se déplacer, se dupliquer ou changer de taille. Chacun de ces gènes représente un neurone caché complètement défini, dont les paramètres seront issus uniquement du contenu du gène, transformé via un code génétique artificiel en un code binaire de taille variable.\nL\u0027évolution artificielle va donc pouvoir optimiser les valeurs des différents paramètres, mais aussi la précision de chacun d\u0027entre eux, le nombre de gènes et leur disposition sur le génome. Nos résultats sont compétitifs avec d\u0027autres algorithmes dédiés à la régression, mais surtout notre algorithme permet un choix dynamique de la taille du génome ou de celle des gènes, qui se stabilisent toutes les deux. Il serait maintenant intéressant de l\u0027appliquer à des problèmes réels plus complexes pour tester ses limites.\n"
  },
  {
    "id": "861",
    "text": "Introduction\nLa taille des données peut être mesurée selon deux dimensions, le nombre de variables et le nombre d\u0027observations. Ces deux dimensions peuvent prendre des valeurs très élevées, ce qui peut poser un problème lors de l\u0027exploration et l\u0027analyse de ces données. Pour cela, il est fondamental de mettre en place des outils de traitement de données permettant une meilleure compréhension des données. La réduction des dimensions est l\u0027une des plus vieilles approches permettant d\u0027apporter des éléments de réponse à ce problème. Les méthodes qui nous intéressent dans ce papier sont celles qui permettent de faire à la fois de la réduction de dimension et la classification non supervisée de données en utilisant les cartes autoorganisatrices (SOM : Self-organizing Map). Celles-ci sont souvent utilisées parce qu\u0027elles sont considérées à la fois comme outils de visualisation et de partitionnement non supervisé de différents types de données. Elles permettent de projeter les données sur des espaces discrets qui sont généralement en deux dimensions. Plusieurs extensions des cartes autoorganisées ont été dérivées du premier modèle original proposé par Kohonen (  (Guérif et Bennani, 2007), BeSOM (Lebbah et al., 2007). Ces modè-les sont différents les uns des autres, mais partagent la même idée de présenter les données de différents types de grande dimension en une simple carte à deux dimensions. Un intérêt majeur sera donné à l\u0027algorithme w-SOM qui est une extension de l\u0027algorithme w-kmoyennes. Ce modèle permet en même temps de construire une carte topologique et d\u0027estimer des pondérations globales de chacune des variables constituant la base d\u0027apprentissage.\nLa pondération de variable consiste à associer des valeurs numériques (poids de pondé-ration) à chaque variable. Elle permet de nous donner une information sur l\u0027importance de la variable. Ainsi une variable possédant une pondération forte explicite le fait qu\u0027elle est pertinente et qu\u0027elle a participé activement au processus de classification.\nDans ce papier, nous proposons une méthode de pondération locale des variables basée sur l\u0027approche w-SOM (Guérif et Bennani, 2007). Ces pondérations seront utilisées pour la segmentation de la carte topologique. En effet, contrairement à la méthode de pondération globale w-SOM qui estime un seul vecteur de pondérations pour tout l\u0027ensemble des réfé-rents, la pondération locale associe un vecteur de pondérations des variables à chaque réfé-rent de la carte. Par conséquent nous pouvons utiliser ces pondérations pour regrouper les prototypes qui ont les mêmes variations du vecteur de pondérations.\nLa suite de cet article est organisée comme suit : nous présentons notre approche de pondération locale des variables dans la section 2, après l\u0027introduction de l\u0027algorithme w-SOM. Dans la section 3, nous présentons les différents résultats obtenus. Finalement on termine par la conclusion et les perspectives de la méthode proposée. Dans la version globale de w-SOM, le poids w j associé à la j-eme variable est le même pour toutes les autres j-eme variables associées à l\u0027ensemble des référents Z de la carte. En étendant ce modèle au cas d\u0027une pondération locale, chaque référent k de la carte a alors son propre vecteur de poids w k . En Notant jk w le poids de la j-ème variable pour le référent k, la fonction de coût s\u0027écrit de la manière suivante :\nPondération des variables\noù C est le nombre de référents, N le nombre d\u0027exemples, et n la dimension de l\u0027espace.\nEn notant U la matrice de partitionnement ou d\u0027affectation des exemples x sur la carte topologique, l\u0027optimisation de\nW) Z, P(U,\n, s\u0027effectue en itérant l\u0027optimisation suivante :\nen fixant Z et W ; chaque individu x est affecté au référent dont il est le plus proche au sens de la distance euclidienne pondérée :\nen fixant U et W ; chacun des référents est remplacé par le barycentre des individus qui lui sont affectés et de ceux qui sont affectés à ces voisins détermi-nés par la fonction de voisinage h: \nLe poids jk w de la variable j pour le référent k est défini de la manière suivante :\nRNTI -XPondération locale des variables en apprentissage numérique non-supervisé\nAinsi nous obtenons un vecteur des pondérations des variables pour tous les sous ensembles associés aux référents de la carte. La pertinence des variables dépend de ces pondéra-tions, ainsi, si elles sont plus petites, on pourra les éliminer. Par conséquent, nous pourrons utiliser ces pondérations pour regrouper les référents qui ont des vecteurs de pondération les plus proches. \nFin de boucle\nGénéralement l\u0027utilisation des cartes topologiques est suivie d\u0027une segmentation des réfé-rents de la carte. Souvent ces méthodes de segmentation se résument à l\u0027utilisation de l\u0027algorithme de classification hiérarchique ou des K-moyennes combinés avec un indice de qualité pour déterminer la taille de la partition idéale de la carte. Ainsi dans ce qui suit nous proposons une application de lw-SOM consistant à utiliser ces pondérations locales, pour segmenter la carte en utilisant l\u0027algorithme K-moyennes combiné avec l\u0027indice de qualité interne de Davies-Bouldin, (Vesanto et Alhoniemi (2000) \nSegmentation de la carte topologique\nL\u0027algorithme lw-SOM décrit dans la section précédente permet d\u0027obtenir d\u0027une part, une projection en deux dimensions des données et d\u0027autre part, une pondération des variables spécifiques à chaque région de l\u0027espace. Vesanto et Alhoniemi (2000) ont proposé de segmenter une carte topologique en combinant l\u0027algorithme des k-moyennes à l\u0027indice de Davies-Bouldin qui permet de déterminer automatiquement la taille de la partition après segmentation. Nous avons appliqué cette approche sur les référents et sur les pondérations.\nEn utilisant le jeu de données Iris, nous avons évalué le découpage par l\u0027approche classique et en utilisant les k-moyennes sur les vecteurs de pondérations. La figure 1 indique une segmentation de la carte 6?4 en deux sous ensembles, utilisant seulement les référents de la carte. La figure 2 indique une segmentation de la même carte en utilisant dans ce cas les vecteurs de pondérations qui prennent en considération l\u0027importance locale de chacune des \nFIG. 4 -Segmentation de la carte avec lw-SOM, en appliquant les kmoyennes sur les pondérations des neurones. Valeur de l\u0027indice de Davies\nBouldin \u003d 0,007.\nEn observant les résultats obtenus avec les deux bases, il est clair que l\u0027utilisation des pondérations des variables locales permet de mieux segmenter la carte topologique.\nDiscussion\nLes figures 5 et 6 représentent les différentes pondérations des variables associées aux ré-férents de la carte topologique obtenue avec lw-SOM, sous forme de signal. En observant les cartes, il est clair que visuellement, on peut segmenter la carte par rapport aux différentes pondérations en regroupant les référents qui ont des pondérations proches. Contrairement à w-SOM globale, l\u0027algorithme lw-SOM, permet de caractériser chaque sous ensemble associé à un référent de la carte, par un vecteur de pondérations indiquant la pertinence de chacune des variables. Par conséquent, les variables qui ont des pondérations proches permettent de distinguer les référents proches, ainsi d\u0027obtenir la segmentation.\nEn observant la figure 5 représentant les pondérations locales de la base des Iris, on peut voir un sous ensemble de prototype sur le coin haut à gauche de la carte qui est caractérisé par les deuxièmes et quatrièmes variables associées à des pondération très forte. En observant aussi la figure 6 représentant les pondérations locales de la base waveform, on peut voir trois sous ensembles de référent avec des pondérations variables. On rappelle que les variables du bruit sont représentées dans la partie droite du signal. On distingue par exemple que dans le coin droit en bas de la carte ces pondérations représentent plus le bruit que les variables.\nRNTI -XPondération locale des variables en apprentissage numérique non-supervisé\nFIG. 5 -Les pondérations des variables de la base Iris\nFIG. 6 -Les pondérations de variables de la base Waveform\nConclusions\nDans cette étude, nous avons introduit une nouvelle méthode de pondération des variables. L\u0027algorithme lw-SOM propose d\u0027apprendre les vecteurs de pondération associés à chaque référent durant le processus d\u0027apprentissage en se basant sur l\u0027algorithme batch de Kohonen. Contrairement à w-SOM, notre approche permet de caractériser chaque sous ensemble « cluster » par les variables les plus pertinentes. En obtenant un vecteur des pondérations pour chaque référent de la carte, on peut regrouper ceux qui ont des pondérations similaires et éliminer ainsi les autres. Nous obtenons donc un découpage de la carte plus fin en comparaison aux autres algorithmes classiques. Comme perspectives, nous envisageons poursuivre ce travail pour un objectif de sélection de variables et d\u0027étendre ce modèle aux variables binaires.\nRéférences\n"
  },
  {
    "id": "862",
    "text": "Introduction\nLes chimistes mettent au point de nouveaux procédés de synthèse de molécules en consultant de très grandes bases de données recensant les réactions chimiques disponibles. Les chimistes aimeraient pouvoir fouiller les graphes moléculaires contenus dans ces données pour en extraire des schémas de réactions fréquents qui serviront de candidats privilégiés lors de nouveaux problèmes de synthèse. Deux obstacles s\u0027opposent à cela. D\u0027une part la manière dont les chimistes représentent les réactions par des graphes ne permet pas aux techniques de fouille de graphes d\u0027extraire les schémas de réactions fréquents. Il existe des algorithmes efficaces (Yan et Han, 2002, 2003Nijssen et Kok, 2004) pour extraire d\u0027un ensemble E de graphes étiquetés l\u0027ensemble des sous-graphes G connexes fréquents dont le support, défini comme le nombre de graphes de E qui contiennent au moins un sous-graphe isomorphe à G, est supé-rieur à un certain seuil. Si ces méthodes peuvent s\u0027appliquer avec succès à la fouille de graphes moléculaires (Fischer et Meinl, 2004), leur application directe aux graphes d\u0027une base de ré-actions ne conduirait à aucun résultat pertinent : tout au plus pourrait-on mettre en évidence les fragments de graphes moléculaires qui sont fréquemment détruits ou au contraire fréquem-ment créés lors des réactions sans qu\u0027aucun schéma de réaction, c\u0027est à dire, aucun schéma de transformation entre graphes moléculaires, ne puisse s\u0027en déduire. D\u0027autre part les bases de données contiennent des descriptions de réactions souvent incomplètes, ambiguës ou erronées. Leur fouille sans autre filtrage conduirait à des résultats trop bruités donc inexploitables.\nUne étape de prétraitement s\u0027avère donc indispensable pour améliorer la qualité des données fouillées et pour exprimer les données au sein d\u0027un modèle répondant au problème posé. Considérant qu\u0027un problème d\u0027extraction de connaissance ne peut être résolu efficacement que si les connaissances du domaine d\u0027application sont prises en compte à tous les niveaux, que ce soit lors du prétraitement des données, de leur fouille proprement dite ou de l\u0027analyse des résultats, le présent article décrit comment les connaissances du domaine, c\u0027est à dire certains principes établis de chimie organique, ont aidé à la conception d\u0027un prétraitement original des bases de réactions, conçu spécifiquement pour extraire les schémas de réactions fréquents à l\u0027aide des algorithmes existants de recherche de sous-graphes fréquents. L\u0027article se restreint essentiellement à présenter les détails de ce prétraitement, qui n\u0027est qu\u0027une étape d\u0027un processus d\u0027extraction de connaissances plus vaste, dont les principes généraux ont été introduits dans Pennerath et Napoli (2006) et dont les premiers résultats sont exposés dans Pennerath et Napoli (2008).\nDéfinitions formelles des données et du problème\nLes molécules sont des groupes d\u0027atomes maintenus solidaires par des forces de covalence. Toute molécule se modélise donc naturellement par un graphe moléculaire g étiqueté et connexe dont les ensembles de sommets S(g) et d\u0027arêtes A(g) représentent respectivement les atomes et les liaisons de covalence de la molécule. Chaque sommet s ? S(g) est étiqueté par l\u0027élément chimique e(s) de son atome (H pour l\u0027hydrogène, ..., et par défaut C pour le carbone) et chaque arête a ? A(g) est étiquetée par la multiplicité m(a) de la liaison associée (simple, double, etc). Une réaction chimique modifie la structure de certaines molécules et se modélise en première approximation par une transformation de graphes moléculaires. Les chimistes représentent cette transformation par une équation chimique (cf figure 1) dont les membres de gauche et de droite représentent respectivement l\u0027ensemble des graphes moléculaires des molécules de départ, appelées réactants, et des molécules d\u0027arrivée appelées produits. Les bases FIG. 1 -Exemple d\u0027équation de réaction à deux réactants et deux produits.\nde réactions décrivent principalement chaque réaction par son équation chimique, c\u0027est à dire par la donnée de deux graphes moléculaires R et P représentant respectivement les membres gauche et droit de l\u0027équation. Les composantes connexes de R (resp. P) correspondent aux graphes moléculaires des différents réactants (resp. produits). Deux fonctions d\u0027annotation ? R : D ? R ? S(R) ? N et ? P : D ? P ? S(P) ? N des sommets de R et de P complètent les données des graphes R et P. Un sommet s est apparié si ? R ou ? P lui associe un indice d\u0027appariement (i.e. s ? D ? R ?D ? P ). Une équation (cf figure 1) annote chaque sommet apparié par son indice. Deux sommets s 1 ? R et s 2 ? P sont appariés l\u0027un à l\u0027autre s\u0027ils sont annotés par le même entier. Ils sont alors censés représenter un et un seul même atome. Une équation (R, P) est dite totalement appariée si tout sommet de P est apparié. Une base de réactions est donc équivalente à un ensemble de 4-uplets {(R i , P i , ? Ri , ? Pi }) 1?i?n .\nUn schéma de réaction est une équation de réaction incomplète qui permet de représen-ter un schéma de transformation commun à plusieurs réactions. Le schéma de la figure 2 est un des nombreux schémas généralisant l\u0027équation de la figure 1. Formellement le schéma de réaction S 1 \u003d (R 1 , P 1 , ? R1 , ? P1 ) généralise l\u0027équation ou le schéma de réaction S 2 \u003d (R 2 , P 2 , ? R2 , ? P2 ) (et on note S 1 ? S S 2 ) si les graphes R 1 et P 1 sont isomorphes à des sousgraphes de R 2 et P 2 et si les injections correspondantes de sommets ? R :\nLe support d\u0027un schéma de réaction S relativement à un ensemble E de réactions est le nombre de réactions de E généralisées par S. Le problème de la recherche des schémas de réactions fréquents dans une base de réactions consiste à déterminer le support de tous les schémas de réactions fréquents dont le support est supérieur ou égal à un seuil f min .\nL\u0027axiomatisation des connaissances du domaine\nLes équations présentes dans les bases de réactions comportent souvent des erreurs. Les chimistes prennent par exemple rarement la peine de décrire tous les appariements entre sommets et certains produits jugés inintéressants sont tout simplement omis de l\u0027équation. Ces né-gligences sont tolérées dans la mesure où les chimistes n\u0027ont aucune difficulté à réinterpréter correctement les données l\u0027aide des connaissances qu\u0027ils ont du domaine. Dans le cadre d\u0027un processus automatisé d\u0027extraction de connaissance, il devient nécessaire d\u0027identifier les propriétés particulières que présentent les graphes (R, P) et que le chimiste utilise implicitement. La démarche adoptée consiste à reformuler ces propriétés en axiomes exprimés exclusivement à partir de concepts propres à l\u0027informatique et à la théorie des graphes de manière à ce que les algorithmes de prétraitement puissent s\u0027en déduire indépendamment des connaissances du domaine :\nConservation des sommets Tout atome se conservant au cours d\u0027une réaction, il existe une bijection ? des sommets du graphe P vers ceux de R. Cela implique en particulier que les fonctions\nValence des sommets Le nombre de liaisons simples auxquelles un atome stable participe étant défini par l\u0027élément chimique de l\u0027atome, tout sommet s de R ou P et d\u0027étiquette l(s) a un degré pondéré deg(s) (i.e. la somme des multiplicités m(a) des arêtes a incidentes à s) égal à l\u0027image de l(s) par une fonction appelée valence :\nRéagencement des arêtes Du fait de la propriété de conservation des sommets, les réactions consistent uniquement à briser, créer ou changer la multiplicité des liaisons. Le graphe produit P s\u0027obtient donc du graphe de départ R en ajoutant à la multiplicité m(a) de chaque arête a \u003d {s 1 ;\nLa propriété de valence des sommets implique alors :\nMinimalité de la distance d\u0027édition Une réaction transforme ses réactants en ses produits en suivant statistiquement la séquence (t j ) de transformations élémentaires qui minimise l\u0027énergie thermodynamique nécessaire. Cette énergie est proportionnelle en première approximation à la distance d\u0027édition d(R, P) \u003d c(t j ) pour passer de R à P en supposant que le coût c(t j ) soit l\u0027énergie nécessaire à la transformation t j . D\u0027après l\u0027axiome de réagencement des arêtes, les transformations élémentaires consistent uniquement à diminuer ou augmenter la multiplicité des arêtes. La distance peut donc se réécrire comme d(R, P) \u003d A(R)?A(P) c(a) où le coût c(a) correspond à l\u0027énergie nécessaire pour modifier de r(a) unités, la multiplicité m(a) de l\u0027arête a. Ce coût est nul si r(a) ? 0 puisque la formation d\u0027une liaison libère de l\u0027énergie, et peut être supposé proportionnel au nombre d\u0027arêtes élémentaires brisées lorsque r(a) \u003c 0. Finalement :\n4 Le processus de fouille des schémas de réactions La figure 3 présente les différentes étapes du processus de fouille de schémas de réactions à partir de bases de réactions. Ce processus entièrement opérationnel comprend toutes les étapes du processus d\u0027extraction de connaissances tel que décrit par Fayyad et al. (1996)  réactions à l\u0027aide d\u0027un langage de requêtes spécifique puis sauvegarde la réponse de sa requête dans un fichier Reaction Data File ou RDF 1 . L\u0027étape de prétraitement développée en section 6, filtre et corrige l\u0027ensemble {E i } des équations de départ en un ensemble {E j } d\u0027équations totalement appariées. Cet ensemble peut alors être transformé en l\u0027ensemble des graphes de réactions équivalents {G r (E j )} dont le modèle est développé en section 5. Les descriptions de ces graphes étiquetés sont sauvegardées dans un fichier au format gbdm afin d\u0027être exploités par un algorithme de fouille de graphes, comme Gaston (Nijssen et Kok, 2004) ou gSpan (Yan et Han, 2002) pour la recherche des sous-graphes fréquents ou Forage (Pennerath et Napoli, 2007) pour l\u0027extraction des schémas de réactions les plus informatifs. Dans tous les cas, l\u0027algorithme produit un fichier gbdm contenant un ensemble {(g k , f k , ...)} de motifs g k associés à leur fréquence f k plus éventuellement d\u0027autres propriétés (score, information, etc). L\u0027étape de post-traitement permet de convertir l\u0027ensemble des graphes de réactions partiels g k en l\u0027ensemble {S(g k )} des schémas de réactions équivalents, qui sont ensuite triés selon les valeurs décroissantes d\u0027une des propriétés spécifiée par l\u0027expert (par exemple le score ou la fréquence), avant d\u0027être sauvegardés dans un fichier RDF. L\u0027expert peut alors analyser les schémas obtenus et leurs propriétés à l\u0027aide d\u0027un logiciel de visualisation d\u0027équations de réac-tions.\nLa transformation des données : les graphes de réactions\nTout algorithme de recherche de motifs fréquents exploite la propriété de monotonicité de la relation de subsomption entre motifs qui correspond en l\u0027occurrence à la relation d\u0027inclusion ? S . Les méthodes existantes de fouille de graphes sont incapables de s\u0027adapter à cette relation d\u0027inclusion pour deux raisons essentielles : d\u0027une part ces méthodes ne génèrent, pour des raisons de réduction combinatoire, que des motifs de graphes connexes, ce qui n\u0027est pas le cas des schémas de réactions. D\u0027autre part ces méthodes n\u0027intègrent pas naturellement la relation d\u0027ordre ? S entre schéma de réactions puisque cette relation repose sur la notion étran-gère d\u0027appariement entre sommets (i.e. les fonctions ? R et ? P ). Le modèle des graphes de réactions introduit initialement dans Vladutz (1986) pour produire une représentation connexe d\u0027une réaction, a également l\u0027avantage de résoudre le second problème Pennerath et Napoli (2006) : tirant parti des axiomes de conservation des sommets et de réagencement des arêtes, le graphe de réaction G r (S) associé à un schéma de réaction S \u003d (R, P, ? R , ? P ) totalement apparié revient à confondre le graphe des produits P avec celui des réactants R en fusionnant les sommets appariés afin d\u0027identifier les arêtes inchangées, détruites et créées lors de la réaction (par des étiquettes d\u0027arêtes 0, ? et +). Le graphe de réaction de l\u0027équation de la figure 1 est représenté sur la figure 4. Cette transformation S ? G r (S) est bijective : les FIG. 4 -Graphe de réaction équivalent à l\u0027équation totalement appariée de la figure 1.\ngraphes R et P de S s\u0027obtiennent de G r (S) en supprimant respectivement les arêtes créées (marquées +) et brisées (marquées ?) puis en remplaçant dans R et P tout ensemble A d\u0027arêtes multiples par une seule arête a de multiplicité m(a) \u003d |A|. Un graphe de réaction G r (S) est donc un graphe connexe rigoureusement équivalent à un schéma de réaction S totalement apparié. On démontre que la relation d\u0027ordre ? g de sous-graphe isomorphe définie sur l\u0027ensemble des graphes de réactions est isomorphe à la relation d\u0027inclusion entre schémas de réactions : S 1 ? S S 2 ? G r (S 1 ) ? g G r (S 2 ). La figure 5 illustre l\u0027équivalence des deux relations d\u0027ordre sur l\u0027exemple du schéma de réaction de la figure 2 inclus dans l\u0027équation de la figure 1. Le problème de recherche des schémas de réactions fréquents dans un ensemble (E i ) 1?i?n d\u0027équations de réactions est donc équivalent à celui de la recherche des graphes fré-quents dans l\u0027ensemble des graphes de réactions équivalents (G r (E i )) 1?i?n . Les algorithmes existants de fouille de graphes peuvent donc résoudre le problème de la recherche de schémas de réactions fréquents (du moins si on se restreint aux motifs contenant au moins une arête de type ? ou +).\nFIG. 5 -Equivalence des relations d\u0027ordre\n6 Le prétraitement des données\nLes données du domaine et leurs imperfections\nUne base de réactions décrit une équation chimique par un 4-uplet (R, P, ? R , ? P ). Ce 4-uplet respecte très rarement tous les axiomes de la section 3. Les équations telles que celles des figures 6 et 7 peuvent être qualifiées différemment selon la nature de leurs non conformités (Berasaluce, 2002). Une équation est ainsi :\nNon saturée quand les atomes d\u0027hydrogène ne sont pas explicités et que le principe de la valence n\u0027est pas respecté (i.e. quand {s ? S(R)?S(P)|deg(s) \u003c valence(l(s))} \u003d ?, cf figure 6).\nFIG. 6 -Exemple d\u0027équation non saturée, non déterministe, incomplète et ambiguë.\nNon déterministe quand les produits d\u0027une équation ne sont pas produits simultanément mais concurremment selon des rendements statistiques respectifs (i.e. quand il existe au moins deux sommets s 1 et s 2 de deux composantes connexes distinctes de P portant le même indice d\u0027appariement ? P (s 1 ) \u003d ? P (s 2 ), cf figure 6).\nNon équilibrée quand certains réactants ou produits doivent être dupliqués pour que le principe de conservation des atomes soit respecté (i.e quand il existe une combinaison linéaire H R × C R \u003d H P × C P telle que les vecteurs de pondération C R et C P aient des coefficients strictement positifs non tous égaux et que la matrice H R (resp. H P ) ait pour coefficients h ij les nombres de sommets d\u0027étiquette i dans le j ème réactant R j (resp. j ème produit P j )). Erronée quand l\u0027équation ne peut être équilibrée et que certains éléments chimiques sont plus présents dans les produits que dans les réactants (i.e. quand il existe une étiquette de sommet d\u0027avantage présente dans P que dans R, cf figure 7).\nFIG. 7 -Exemple d\u0027équation erronée.\nIncomplète quand l\u0027équation n\u0027est ni équilibrable ni erronée parce que certains produits secondaires sont omis de l\u0027équation (i.e. quand il existe une étiquette de sommet plus présente dans R que dans P, cf figure 6) Ambiguë quand l\u0027équation n\u0027est pas totalement appariée parce que plusieurs appariements sont envisageables (i.e. quand D ? P \u003d P, cf figure 6).\nLes étapes du prétraitement\nLe prétraitement des données se décompose en une succession d\u0027étapes. Chaque étape peut être perçue comme une fonction qui transforme tout 4-uplet qu\u0027elle reçoit en entrée en un ensemble éventuellement vide de 4-uplets. Tout 4-uplet en sortie de e i devient ensuite un 4-uplet en entrée de e i+1 . L\u0027ordre des étapes est défini de telle sorte qu\u0027une étape résout une catégorie particulière de défauts sans jamais introduire un type de défaut résolu par une étape précédente. La succession des étapes garantit que les graphes de réactions obtenus en sortie de la dernière étape correspondent aux équations des réactions les plus plausibles. Les étapes sont dans l\u0027ordre : La saturation des molécules qui consiste simplement à connecter à chaque sommet s de R et de P, valence(l(s)) ? deg(s) sommets d\u0027hydrogène par des arêtes simples pour satisfaire l\u0027axiome de valence des sommets. La scission des équations non déterministes : si l\u0027équation (R, P, ? R , ? P ) est non détermi-niste, toute composante connexe P k de P dont le rendement associé dépasse un seuil configurable donne lieu à une équation déterministe (R,\nélimination des équations erronées : si il existe une étiquette de sommet (i.e. un élément chimique) qui est présente dans P sans l\u0027être dans R, l\u0027équation candidate est erronée et est éliminée. La pondération des équations non équilibrées est un problème théorique complexe de programmation linéaire en nombres entiers (Sena et al., 2006). En pratique les équations comptent rarement plus de trois réactants et trois produits. L\u0027étape de pondération se contente donc pour toute équation non équilibrée de tester toutes les duplications évi-dentes de réactants et/ou de produits jusqu\u0027à obtenir une équation équilibrée (en testant la condition H R × C R \u003d H P × C P ). Si toutes les tentatives de pondération échouent, on distingue deux cas : si il existe une étiquette de sommet plus représentée dans P que dans R, l\u0027équation candidate est considérée erronée et est éliminée. Dans le cas contraire l\u0027équation est incomplète mais peut passer à l\u0027étape suivante.\nLa complétion des appariements : A ce stade du prétraitement, l\u0027équation obtenue est presque toujours ambiguë. Un rejet systématique des équations ambiguës est donc impossible. La solution adoptée consiste à produire l\u0027ensemble des équations totalement appariées les plus plausibles qui se déduisent de l\u0027équation ambiguë. Cet ensemble contient vraisemblablement l\u0027unique réaction se produisant réellement en plus d\u0027éventuelles équations factices. L\u0027effet néfaste introduit par la présence de ces artefacts est toutefois limité par le filtrage statistique qu\u0027opère la recherche des motifs fréquents en ignorant les schémas de réactions non représentatifs. Pour désambiguïser une équation, il est nécessaire d\u0027apparier tous les sommets non appariés de P à des sommets non appariés de R. Compte tenu du nombre exponentiel d\u0027appariements possibles, il est nécessaire de restreindre la procédure aux appariements les plus plausibles. On introduit à cette fin la notion de compatibilité entre sommets : deux sommets s 1 ? S(P) et s 2 ? S(R) sont compatibles si aucun des deux n\u0027est déjà apparié, si ils ont la même étiquette, si ils sont tous deux adjacents à un sommet apparié et si ils partagent le même ensemble maximal de sommets voisins appariés : s 1 est incompatible avec s 2 si il existe un sommet s 3 ? S(R) compatible avec s 1 qui a plus de voisins appariés avec des voisins de s 1 que s 2 n\u0027en a avec s 1 . En notant V(s) l\u0027ensemble des sommets voisins du sommet s, cette dernière condition équivaut à :\nL\u0027appariement de deux sommets n\u0027est plausible que si ces derniers sont compatibles, selon le principe de minimalité de la distance d\u0027édition. L\u0027élimination des appariements incompatibles permet d\u0027élaguer l\u0027arbre de recherche dans l\u0027espace d\u0027état des appariements possibles. La procédure consiste alors en un algorithme de backtracking qui effectue à chaque étape de sa progression l\u0027appariement d\u0027un sommet de P non encore apparié avec un sommet de R qui lui est compatible puis met à jour les fonctions ? R et ? P . Un retour arrière se produit soit dès qu\u0027un sommet non apparié de P n\u0027est plus compatible avec aucun sommet de R soit dès que tout sommet de P est apparié, l\u0027équation associée étant alors passée à l\u0027étape de traitement suivante.\nLa construction du graphe de réaction : A ce stade du prétraitement, toutes les équations sont totalement appariées. Chaque équation E est donc remplacée par son graphe de réaction G r (E), conformément à la section 5.\nL\u0027élimination des graphes de réactions non réalistes : L\u0027étape de complétion des appariements transforme une équation de départ en un nombre limité d\u0027équations E i totalement appariées qui n\u0027ont pas nécessairement le même degré de plausibilité. De ce fait, le nombre n i d\u0027arêtes brisées (étiquetées ?) de chaque graphe de réaction G r (E i ) est calculé. Tout graphe G r (E i ) dont le nombre n i n\u0027est pas égal à n min \u003d min(n i ) peut alors être éliminé au nom du principe de minimalité de la distance d\u0027édition. Après élimina-tion, toute équation initiale E aboutit à un ensemble d\u0027équations totalement appariées de même n i minimal. Le nombre de ces équations est le plus souvent égal à 1, parfois nul lorsque E s\u0027avère erronée, parfois égal à 2 ou 3 (mais rarement plus) lorsque E peut valablement être interprétée selon différents appariements de sommets. Lorsque le nombre d\u0027appariements possibles est supérieur à un seuil (fixé à 4), on estime que l\u0027appariement de l\u0027équation initiale est insuffisant et que les équations qui en découlent sont trop incertaines et doivent être éliminées.  7 Résultats des tests L\u0027algorithme de prétraitement a été implémenté et testé sur plus de 95000 réactions monoétapes issues de l\u0027intégralité des deux bases de réactions Orgsyn et JSM 2 particulièrement importantes pour la variété des méthodes de synthèse qu\u0027elles proposent. Les résultats des tests résumés dans le tableau de la figure 9 reposent sur la définition de différents taux indicateurs. Soit ainsi I l\u0027ensemble initial des équations de la base de données. Soit E ? I l\u0027ensemble des  une diminution régulière, probablement due à une sélection de plus en plus stricte des données. Le taux d\u0027ambiguïté t a observe une décroissance plus récente, traduisant une qualité croissante des appariements.\nConclusions\nLes résultats obtenus prouvent qu\u0027il est possible d\u0027extraire les schémas de réactions fré-quents des bases de réactions à l\u0027aide des algorithmes existants de fouille de graphes. L\u0027outil Forage a ainsi pu extraire de différents ensembles de réactions les schémas de réactions fré-quents, fermés fréquents et les plus informatifs fréquents (Pennerath et Napoli, 2007). Si ce résultat devrait à terme permettre aux chimistes d\u0027identifier des schémas de réactions intéres-sants, il pourrait également inciter les spécialistes de la fouille de données à s\u0027intéresser de plus près à ces objets d\u0027étude riches et complexes que sont les réactions chimiques.\n"
  },
  {
    "id": "863",
    "text": "Introduction\nOn appelle « concept », une entité qui se définit par un croisement de catégories. L\u0027objet de l\u0027ADS est d\u0027analyser des ensembles de concepts décrits par des variables symboliques. Ces variables sont non seulement à valeur numérique ou qualitative mais aussi à valeur intervalle, histogramme, loi de probabilité, fonction, ensemble de valeurs etc., afin de tenir compte de la variation des valeurs prises par les individus de l\u0027extension de chaque concept. L\u0027ADS et son logiciel SODAS comportent deux étapes : la première consiste à construire la description des concepts à partir de celle des individus, la seconde consiste à analyser le tableau de données symboliques ainsi créé en étendant les méthodes de la Statistique ou du Data Mining aux concepts considérés comme unités statistiques de plus haut niveau. Nous illustrons ces deux étapes en montrant trois avantages de l\u0027ADS : i) on peut étudier les bonnes unités statistiques à un niveau de généralisation voulu par l\u0027utilisateur ; ii) on réduit la taille des données en considérant comme unités d\u0027étude, des classes plutôt que les individus ; iii) on réduit le nombre de variables du fait qu\u0027elles sont à valeur symbolique (par exemple, à valeur « histogramme » plutôt qu\u0027à valeur «fréquence d\u0027une catégorie» ou à valeur intervalle plutôt qu\u0027à valeur « borne d\u0027intervalle »). On utilise pour cela le logiciel SODAS (voir l\u0027ouvrage collectif issu du projet européen ASSO d\u0027 EUROSTAT : Diday, Noirhomme (2007)).\nDescription\nLes données fournies par le LCPC (Laboratoire Central des Ponts et Chaussées) sont constituées d\u0027un ensemble de 14 TGV qui en passant à une température donnée sur un pont déclenchent des signaux de 9 capteurs répartis à différents endroits du pont (voir la figure 1). En entrée, on dispose d\u0027un tableau de données symboliques qui contient dans la case (i, j) le RNTI-E-11 -211 -\n"
  },
  {
    "id": "864",
    "text": "Introduction\nDe nombreux domaines comme la biologie ou la médecine voient naître chaque jour de nouveaux termes et abréviations, notamment des sigles. Un sigle est un ensemble de lettres initiales servant d\u0027abréviation, par exemple \"RATP\" peut être associé à la définition (aussi appelée expansion) \"Régie Autonome des Transports Parisiens\". Nos travaux ont consisté à développer un logiciel afin de faciliter l\u0027acquisition ou l\u0027enrichissement de dictionnaires en extrayant automatiquement, à partir de diverses sources, les sigles et leur(s) définition(s). Une fois ces dictionnaires constitués, l\u0027approche AcroDef que nous avons proposée dans (Roche et Prince (2007)) consiste à établir la définition pertinente d\u0027un sigle présent dans un document. Dans ces documents, la définition n\u0027est pas toujours présente d\u0027où la difficulté du traitement. Dans ce contexte, il est donc essentiel d\u0027avoir à disposition un dictionnaire adapté, ce qui justifie les travaux présentés dans cet article.\nDe nombreuses méthodes pour extraire les sigles et leur(s) définition(s) ont été développées (Larkey et al. (2000); Okazaki et Ananiadou (2006)). La plupart des approches de détection de sigles dans les textes s\u0027appuient sur l\u0027utilisation de marqueurs spécifiques associés à des heuristiques adaptées. Certains travaux récents (Okazaki et Ananiadou (2006)) consistent à associer ces approches à des mesures statistiques spécifiques pour améliorer la qualité des méthodes d\u0027acquisition de dictionnaires. L\u0027approche que nous avons développée se compose de deux étapes successives qui sont détaillées dans la section 2.\nLa seconde étape de notre application utilise les résultats obtenus lors de la première phase afin de filtrer les candidats pertinents. Les résultats sont triés afin de (1) supprimer les paires sigle/définition non pertinentes, (2) extraire précisément les définitions présentes dans les dé-finitions potentielles (ces dernières pouvant être trop longues puisque coupées arbitrairement lors du second cas de la recherche des candidats). Pour permettre un tel filtrage, nous effectuons un alignement des lettres contenues dans le sigle avec les mots de la définition. Cet alignement consiste à vérifier la correspondance entre les lettres des sigles avec les premières lettres de chacun des mots des définitions. Dans notre méthode, si le premier caractère des mots de la définition candidate ne peut être aligné, les caractères qui suivent au sein des mots sont considérés. Par exemple, cette méthode permet de reconnaître \"Extraction Itérative de la Terminologie\" comme la définition du sigle EXIT dans lequel la lettre \"X\" a pu être alignée. Nous présentons ici une évaluation de notre système d\u0027alignement des sigles avec les défi-nitions candidates. Pour cette évaluation, nous nous appuyons sur les données issues du site http ://www.sigles.net/. L\u0027évaluation consiste à extraire aléatoirement de ce site des sigles de 2, 3 et 4 caractères et d\u0027évaluer le taux de réussite de l\u0027alignement (nombre de sigles alignés avec les définitions du site en utilisant la version actuelle de notre logiciel développé en Javaversion 1.0). Le tableau ci-dessous présente les résultats de plus de 800 alignements qui sont globalement très satisfaisants (taux de réussite de 78% à 98%). \nConclusion et perspectives\nL\u0027application présentée dans cet article consiste à acquérir ou enrichir de manière automatique un dictionnaire de sigles/définitions à partir d\u0027un corpus. Notre approche n\u0027utilise aucune connaissance linguistique, elle peut donc s\u0027appliquer à des textes en différentes langues. Dans nos futurs travaux, nous proposons d\u0027associer de manière automatique le domaine de chaque sigle/définition en utilisant des méthodes fondées sur le contexte (Roche et Prince (2007)).\n"
  },
  {
    "id": "865",
    "text": "Introduction\nDans ce papier, nous proposons un nouveau système de détection d\u0027intrusions, visant la diminution de génération de fausses alarmes et l\u0027augmentation de détection de vraies intrusions. Nous montrons que l\u0027utilisation des règles associatives génériques, de taille très compacte, permet d\u0027atteindre ce double objectif. Les expérimentations que nous avons menées, montrent que l\u0027approche proposée permet d\u0027obtenir un SDI robuste avec un taux très élevé de détection de vraies intrusions.\nLe système de détection d\u0027intrusions IDS-GARC\nPeu de travaux ont fait appel au concept des règles associatives dans le cadre de détection d\u0027intrusions. Pour améliorer la qualité de détection d\u0027intrusions, nous proposons un nouveau SDI appelé IDS-GARC (Intrusion Detection System based on Generic Association Rule with Classifier), dont l\u0027objectif est de minimiser la génération de fausses alarmes et surtout l\u0027augmentation de détection de vraies intrusions.\nLe nouveau système IDS-GARC, dont l\u0027architecture du IDS-GARC est décrite par la figure 1, dérive de l\u0027application d\u0027un processus, qui peut être résumé dans les quatre étapes suivantes :\n-Pré-traitement des données : Nous discrétisons automatiquement des données de détec-tion d\u0027intrusions identifiées par des experts en sécurité informatique -Génération de la base générique des règles associatives : En particulier, nous utilisons les règles génériques extraites de la base IGB (Gasmi et al., 2006) -Sélection des règles associatives génériques de détection : Pour se faire, nous avons recours à la classification associative. -Construction d\u0027un classifieur : Pour détecter les nouvelles attaques, nous utilisons un classifieur appelé GARIDC (Generic Association Rule for Intrusion Detection based Classifier).\nEvaluation expérimentale\nAfin d\u0027évaluer les performances du IDS-GARC, nous avons mené une série d\u0027expérimen-tations sur une base de données orientée détection d\u0027intrusions DARPA 98. Le choix de cette base s\u0027explique par le fait puisqu\u0027elle est fréquemment utilisée pour évaluer les performances des SDIs. Le tableau 1 présente les résultats obtenus en termes de taux de détection et de\nFIG. 1 -Architecture du système IDS-GARC\nfausses alarmes. L\u0027analyse du tableau 1 permet de conclure que la technique de règles gé-nériques de classification permet d\u0027obtenir le meilleur taux de détection de vraies intrusions (97, 86%). De plus, elle engendre peu de fausses alarmes, soit 2, 50%. Ainsi, les deux taux TD et TF permettent de montrer la robustesse et l\u0027efficacité du SDI que nous avons proposé dans ce papier.\nApproche\nTaux de détection (TD) Taux de fausses alarmes (TF) Arbre de décision 92,05% 2, 50% Algorithmes génétiques 92, 60% 3, 62% Règles génériques de classification 97, 86% 2, 50%\nTAB. 1 -Comparaison des taux de détection de vraies intrusions et de fausses alarmes\nLe tableau 2 montre les taux de Pourcentage de Classification Correcte (PCC) obtenus avec notre approche par rapport à celles trouvées avec des approches de la littérature de la fouille de données, pour la base d\u0027audit DARPA 98. \nTAB. 2 -Comparaison de la précision de classification correcte\nEn comparant les résultats du tableau 2, pour la catégorie normale, nous constatons que les règles génériques de classification donnent le meilleur taux i.e., 100%. De plus, cette stratégie présente le taux de PCC le plus élevé, pour la catégorie U2R, avec 91,41%.\nSummary\nIntrusion Detection Systems mainly aim to guarantee high immunity to networks from external attacks. In this paper, we introduce a novel approach for intrusion detection based on the use of generic basis of association rules. Preliminary carried out results showed the efficiency of such an approach. Keywords : Intrusion Detection Systems, Association rules, generic basis\n"
  },
  {
    "id": "866",
    "text": "Introduction\nUn entrepôt de données est une structure informatique dans laquelle est centralisé un volume important de données. L\u0027organisation des données permet l\u0027analyse multidimensionnelle qui consiste à explorer tout ou partie des données à un niveau détaillé et/ou agrégé grâce aux outils OLAP (On-Line Analytical Processing). L\u0027information géo-référencée, souvent contenue dans les données, est intégrée sous forme textuelle dans les modèles multidimensionnels classiques. Des modèles plus récents, de type \"OLAP Spatial\" (SOLAP), visent à intégrer la donnée spatiale dans l\u0027OLAP et à enrichir les systèmes OLAP classiques grâce, par exemple, à la visualisation cartographique, permettant ainsi d\u0027expliciter la distribution géographique d\u0027une information et/ou de mettre en relation des informations à diverses granularités géographiques.\nL\u0027information géographique est la représentation d\u0027objets, ou de phénomènes, localisés dans l\u0027espace ; les modèles SOLAP se concentrent généralement sur cette composante spatiale de l\u0027information géographique. Or celle-ci est aussi caractérisée par un ensemble d\u0027aspects sémantiques (ses attributs descriptifs alphanumériques et ses relations avec d\u0027autres objets) qui sont pertinents à la fois dans la modélisation de la donnée géographique multidimensionnelle et de ses hiérarchies, et aussi dans l\u0027analyse. L\u0027analyse spatiale offerte par les Systèmes d\u0027Information Géographique est par nature flexible et itérative : les données géographiques peuvent être modifiées ou remplacées grâce aux méthodes de transformations spatiales tout au long du processus d\u0027analyse. Or les opérateurs spatiaux fournis par les différents systèmes SOLAP sont souvent des opérateurs orthogonaux aux opérateurs et mesures détaillées) et/ou pré-calculées (mesures agrégées). Chaque résultat d\u0027analyse est la conséquence des résultats précédents. Chaque étape du processus d\u0027analyse est effectuée par une navigation dans l\u0027hypercube, ou par une requête multidimensionnelle. Ces requêtes utilisent les opérateurs OLAP (Roll-up, Drill-down, Slice, etc…) permettant de calculer les mesures pour des ensembles de membres à des niveaux de granularité et selon des prédicats sélectionnés par l\u0027utilisateur.\nLes architectures des systèmes OLAP sont des architectures trois tiers. Le premier tiers est un serveur d\u0027entrepôt de données. Les données, sélectionnées pour leur pertinence, sont extraites des bases de données transactionnelles, nettoyées, transformées puis intégrées dans l\u0027entrepôt. Le deuxième tiers est un serveur OLAP qui, pour optimiser le temps de réponse, pré-calcule les différents agrégats en utilisant des fonctions d\u0027agrégation classiques de l\u0027algèbre relationnelle (SUM, MIN, MAX, AVG ou COUNT). Le dernier tiers est un client OLAP qui offre une interface utilisateur avec des outils de reporting, d\u0027analyse interactive, et parfois de fouille de données. Le paradigme de visualisation le plus adopté par les clients OLAP est la table de pivot. Il s\u0027agit d\u0027un tableau multidimensionnel auquel sont associés les totaux et les sous-totaux, et qui offre une vue de données imbriquées sur plusieurs niveaux. Les tables de pivot sont généralement couplées avec des affichages graphiques (histogrammes, courbes, etc.). Les actions de l\u0027utilisateur sur ces différentes composantes (clic de souris, sélection graphique, drag-and-drop) se traduisent par l\u0027appel aux opérateurs OLAP.\nL\u0027uniformisation de données hétérogènes, la présentation de l\u0027information à différents niveaux de détail et sur différents axes d\u0027analyse, la rapidité d\u0027accès aux données et l\u0027interface interactive, simple et intuitive font des entrepôts de données associées aux outils OLAP de véritables SAD.\nSystèmes d\u0027Information Géographique et analyse spatiale\nL\u0027information géographique est la représentation d\u0027un objet ou d\u0027un phénomène réel localisé dans l\u0027espace à un moment donné. L\u0027information géographique est caractérisée par une composante purement spatiale et une composante sémantique (Degréne et Salgé, 1997). La composante spatiale représente la position sur la surface terrestre et la forme d\u0027un objet du monde réel. Une position est décrite dans un système de référence explicite comme par exemple un système de coordonnées. Cette composante permet de représenter la forme de l\u0027objet lui-même et de positionner celui-ci par rapport aux autres phénomènes ou objets du monde réel. La composante sémantique représente l\u0027information relative à la nature, l\u0027aspect et les propriétés descriptives d\u0027un objet ou d\u0027un phénomène du monde terrestre. Cette information peut aussi inclure des relations spatiales, descriptives et de généralisation cartographique avec d\u0027autres objets ou phénomènes. En effet, un des aspects sémantiques qui distingue l\u0027information géographique des données classiques est sa représentation multiple à différentes échelles ou selon différents thèmes secondaires (Weibel et Dutton, 2001). Les cartes généralisées sont obtenues grâce aux opérateurs de généralisation (Regnauld et McMaster, 2007).\nLes Systèmes d\u0027Information Géographique (SIG) permettent de générer, mémoriser et visualiser les données géographiques. Les SIG offrent des fonctions d\u0027analyse spatiale, qui les transforment en véritables outils pour l\u0027analyse spatiale. Ainsi, les utilisateurs de SIG disposent d\u0027un ensemble d\u0027outils statistiques (par exemple des méthodes de Point Pattern Analysis) et non statistiques (par exemple le buffer), qui peuvent créer ou modifier les données géographiques (Longley et al., 2001). Le processus d\u0027analyse spatiale est itératif et comprend les étapes suivantes : (1) identification du problème et des buts de l\u0027analyse, (2) identification des problématiques spatiales et des outils pour les résoudre, (3) identification des données et leur préparation pour les opérations spatiales, (4) création d\u0027un plan d\u0027analyse, (5) exécution du plan et visualisation des résultats (Mitchel, 2005). Chaque étape du processus peut être modifiée, itérée ou éliminée en fonction des résultats obtenus. En d\u0027autres termes, les données, les sujets et les outils du processus d\u0027analyse spatiale ne sont pas fixés a priori. Dans ce processus, la visualisation des résultats joue évidement un rôle très important car elle stimule l\u0027utilisateur dans son processus de découverte de patterns, relations et tendances. La «géo-visualisation» intègre ainsi les techniques de visualisation scientifique, de cartographie, d\u0027analyse des images, d\u0027exploration de données, pour fournir une théorie, des méthodes et des outils pour la représentation et la découverte de la connaissance spatiale (MacEachren et al., 2001).\nL\u0027OLAP Spatial\nYvan Bédard définit l\u0027OLAP Spatial (SOLAP) comme «une plate-forme visuelle spécialement conçue pour supporter l\u0027analyse et l\u0027exploration spatio-temporelles rapides et faciles des données multidimensionnelles composées de plusieurs niveaux d\u0027agrégation à l\u0027aide d\u0027affichages cartographiques aussi bien qu\u0027à l\u0027aide de tableaux et diagrammes statistiques» (Bédard, 1997). Le SOLAP se propose comme un SAD dans lequel les fonctionnalités OLAP sont associées à des fonctionnalités SIG et à des techniques de géo-visualisation (Rivest et al., 2005). La prise en compte de la composante spatiale améliore l\u0027analyse OLAP classique, la représentation cartographique permettant de mettre en évidence des relations spatiales entre différents faits et/ou dimensions qu\u0027une simple étiquette textuelle ou un affichage graphique n\u0027aurait pas, ou mal, montrées. Les modèles SOLAP redéfinissent les concepts principaux de l\u0027OLAP : mesure, dimension et opérateurs de navigation multidimensionnelle.\nConcepts principaux\nLe terme de dimension spatiale désigne la prise en compte de l\u0027information spatiale dans un axe d\u0027analyse d\u0027une application décisionnelle. Plusieurs travaux s\u0027intéressent à la modélisation et la prise en compte des dimensions spatiales, par exemples (Bédard, et al., 2001), (Fidalgo et al., 2004) et (Malinowski et Zimányi, 2005). L\u0027introduction explicite de la composante spatiale dans les niveaux des hiérarchies de dimensions permet de bénéficier d\u0027une représentation cartographique des membres de dimensions, d\u0027utiliser de prédicats spatiaux dans les opérations de coupe, et de prendre en compte les relations topologiques pendant les processus d\u0027agrégation (Jensen et al., 2004), (Malinowski et Zimányi, 2005).\nL\u0027introduction des données spatiales dans les dimensions d\u0027entrepôts de données a mené différents auteurs à la définition d\u0027opérateurs d\u0027analyse spatio-multidimensionnelle. (Rivest et al., 2005), (Sampaio et al., 2006), (Matias et Moura-Pires, 2007), (Scotch et Parmanto, 2005), (Hernandez et al., 2005) reformulent les opérateurs de forage définissant les opérateurs \"spatial-drill down\" et \"spatial roll-up\" qui permettent de naviguer dans une dimension spatiale. (Sampaio, et al., 2006), (Colonnese et al., 2005) et (Matias et MouraPires, 2007) appellent \"spatial slice\" une opération de coupe qui utilise un prédicat spatial.\n-130 -RNTI-E-13 (Scotch et Parmanto, 2005) introduisent deux nouveaux opérateurs de coupe : le \"buffer\" et le \"spatial drill-out\". Le \"buffer\" utilise l\u0027opérateur d\u0027analyse spatiale de buffer, qui crée une zone tampon autours d\u0027un membre, pour sélectionner des membres de la dimension spatiale. Le \"spatial drill-out\" sélectionne tous les membres adjacents au membre sur lequel cet opérateur est appliqué. Ces opérateurs spatio-multidimensionnels permettent de naviguer dans un hypercube spatial en utilisant les attributs géométriques des dimensions spatiales.\nLes outils SOLAP\nUn outil SOLAP repose sur l\u0027intégration des fonctionnalités SIG et OLAP (Kouba, et al., 2000), (Rivest et al., 2005  (Silva et al., 2006) et (Sampaio et al., 2006). (Silva et al., 2006) présentent un outil web SOLAP dont la caractéristique principale est l\u0027usage des services web géographiques pour la définition de GeoMDQL, un langage de requêtes qui étend le langage MDX de Microsoft pour les entrepôts de données spatiales. Le prototype est basé sur le serveur OLAP Mondrian modifié pour gérer les requêtes GeoMDQL et le client OLAP JPivot couplé avec une carte interactive pour la représentation des dimensions spatiales. L\u0027outil utilise la modélisation logique présentée en (Fidalgo et al., 2004) et ne gère pas les mesures spatiales. Dans (Sampaio et al., 2006), les auteurs décrivent un système web SOLAP qui permet d\u0027interroger les entrepôts de données spatiales avec des opérateurs de forage et de sélection sur la dimension spatiale et gérer les mesures spatiales. Dans cette solution, l\u0027interface web est composée d\u0027une interface cartographique, un navigateur pour sélectionner les membres de dimensions, et une zone de texte pour éditer les requêtes multidimensionnelles. Dans (Rivest et al., 2005)  \nLimites des solutions SOLAP\nDimension spatiale\nLes modèles proposés en littérature (Bédard et al. 2001), (Fidalgo et al. 2004) (Malinowski et Zimányi, 2005) qui introduisent l\u0027information géographique comme axe d\u0027analyse, sont caractérisés par la présence de l\u0027attribut géométrique dans les membres des différents niveaux. Les hiérarchies spatiales sont définies en utilisant les attributs spatiaux ou alphanumériques des dimensions. Ces relations définissent, comme pour les hiérarchies classiques, une relation d\u0027inclusion entre les membres de niveaux différents. Les hiérarchies spatiales sont donc des hiérarchies classiques de classification ou de spécialisation (LujianMora et al. 2002) avec des attributs géométriques. Ainsi, entre deux membres spatiaux de -132 -RNTI-E-13 S. Bimonte et al. deux niveaux différents, il existe toujours une relation topologique d\u0027inclusion ou intersection (Malinowsky et Zimányi, 2005). Ces hiérarchies représentent différentes granularités de l\u0027information géographique, chaque niveau géographique représentant une information géographique différente.\nLes dimensions spatiales, basées exclusivement sur les relations spatiales, ne reflètent pas la sémantique des relations entre les membres de différents niveaux. Par exemple, un des aspects sémantiques qui caractérise l\u0027information géographique est sa représentation à différentes échelles ou selon différents thèmes secondaires. Or, les relations hiérarchiques de généralisation cartographique ne représentent pas toujours de relations d\u0027intersection ou inclusion. Grâce à ce type de hiérarchie, le décideur pourrait, par exemple, naviguer à travers différentes représentations à diverses échelles de la même information géographique, et visualiser les mesures à différents degrés de précision spatiale ; les méthodes d\u0027agrégation devraient alors être différentes de celles utilisées pour les hiérarchies spatiales classiques car il n\u0027est pas possible de quantifier l\u0027apport d\u0027un membre par rapport à son ancêtre dans le cas d\u0027une hiérarchie de généralisation.\nSelon nous, la prise en compte de la sémantique des relations entre les différents membres des dimensions spatiales est fondamentale pour le processus décisionnel, car elle peut être utilisée pour caractériser les processus d\u0027agrégation, comme nous le montrons dans la section suivante.\nOpérateurs multidimensionnels\nDans le processus d\u0027analyse multidimensionnelle, l\u0027utilisateur navigue dans l\u0027hypercube à travers les hiérarchies des dimensions, en comparant les mesures, qui sont agrégées à différentes granularités avec des fonctions d\u0027agrégation. Les chemins d\u0027analyse sont donc imprédictibles, mais le contexte d\u0027analyse, i.e. les données et le modèle multidimensionnel, est défini dès la phase de conception. Le processus d\u0027analyse spatiale, lui, est un processus itératif dans lequel les données sont modifiées ou remplacées à chaque itération grâce aux méthodes d\u0027analyse spatiale de transformation (i.e., le buffer, l\u0027overlay, etc.). Ainsi, il apparait souhaitable d\u0027introduire et d\u0027adapter les opérateurs d\u0027analyse spatiale au contexte OLAP afin de profiter pleinement du caractère itératif de l\u0027analyse spatiale. Ainsi les dimensions qui incluent l\u0027information géographique doivent pouvoir être reformulées selon les exigences de l\u0027utilisateur et les mesures doivent alors être recalculées. Dans la majorité des cas, les outils SOLAP offrent d\u0027une part les fonctionnalités OLAP de navigation, et d\u0027autre part les fonctionnalités SIG de visualisation et d\u0027analyse spatiale. Les cartes interactives sont couplées avec un ensemble d\u0027outils d\u0027analyse spatiale (i.e. ajout des cartes en format matriciel ou vectoriel, personnalisation des affichages, opérateurs métriques, requêtes spatiales, opérateurs d\u0027analyse spatiale ou de fouille de données, etc.) qui ne sont pas intégrées au processus de navigation multidimensionnel. En d\u0027autres termes, ces outils ne modifient pas la structure de l\u0027hypercube ce qui limite le caractère itératif du processus d\u0027analyse.\nUn cadre conceptuel pour l\u0027OLAP géographique\nDans cette section, nous introduisons les concepts de dimension géographique et les opérateurs multidimensionnels associés ainsi que les principales définitions de notre modèle formel multidimensionnel.\nPour présenter nos contributions, nous utiliserons comme cas d\u0027étude un projet concernant l\u0027étude de la pollution de la lagune de Venise réalisé dans le cadre d\u0027une collaboration avec l\u0027organisation internationale CORILA (Consorzio per la Gestione del Centro di Coordinamento delle Attività di Ricerca inerenti il Sistema Lagunare di Venezia), qui a comme but la sauvegarde environnementale, architecturale et économique de la lagune de Venise.\nDimension Géographique\nNous appelons objet complexe une entité du monde réel (un patient, un produit, etc.) décrite par un ensemble d\u0027attributs descriptifs alphanumériques (âge, nom, type, etc.). Un objet géographique est un objet complexe (une ville, un bâtiment, etc.) qui présente un attribut spatial (i.e. une géométrie) en plus de ses attributs descriptifs. Un exemple est une unité environnementale de la lagune de Venise décrite par les attributs suivants : son nom (\"Murano\", \"Chioggia\", etc.), la liste de plantes qui la recouvrent (\"Spartima Marittima\", etc.), son type (industrielle, agricole, etc.), un index de salinité (valeur numérique), un attribut spatial décrivant sa géométrie et sa surface.\nDéfinition 1. Une dimension est dite géographique si les membres d\u0027au moins un niveau sont des objets géographiques.\nUne dimension géographique structure l\u0027information à différentes granularités représentées par ses niveaux. Les membres de ces niveaux peuvent être liés par des relations spatiales, et/ou des relations de généralisation cartographique et/ou des relations descriptives. Une dimension géographique peut présenter une ou plusieurs de ces hiérarchies, l\u0027opération d\u0027agrégation appliquée à la mesure dépend alors de la hiérarchie sur laquelle on navigue. On peut ainsi imaginer appliquer une opération additive lorsqu\u0027on navigue sur une hiérarchie spatiale, et aucune agrégation sur une hiérarchie de généralisation cartographique. La modélisation des hiérarchies non-strictes, non-couvrantes et non-onto est une de caractéristiques avancées des modèles multidimensionnels classiques. Une hiérarchie est non-couvrante si une branche de la hiérarchie peut \"sauter\" un ou plusieurs niveaux, nononto si les feuilles ne sont pas toutes au même niveau, et non-stricte si une relation n-n peut exister entre les membres de différentes niveaux. Notre modèle doit permettre de modéliser ces hiérarchies complexes afin de prendre en compte les caractéristiques spécifiques des relations entre objets géographiques. Nous introduisons maintenant les différents types de hiérarchies applicables à une dimension géographique.\nHiérarchie descriptive\nUne hiérarchie descriptive organise l\u0027information géographique à différentes granularités thématiques.\nDéfinition 2. Une hiérarchie descriptive d\u0027une dimension géographique est une hiérarchie OLAP de classification ou de spécialisation. Elle est définie en utilisant les attributs descriptifs des objets géographiques.\nUn exemple de hiérarchie descriptive pour la lagune de Venise est présenté dans la Figure 1. Cette hiérarchie classe les unités (\"Unit\") de la lagune par rapport à leur type (\"Type\").  Les dimensions sont le temps, les polluants et une dimension géographique avec la hiérarchie descriptive. Notons que certains niveaux de la dimension géographique peuvent être alphanumériques (eg. le niveau \"Type\"). Cette application permet d\u0027analyser la pollution des eaux de la lagune en fonction du temps, des polluants et des unités de la lagune. La valeur de pollution est agrégée en utilisant la moyenne. Une requête multidimensionnelle peut être : \"Quelle est la valeur moyenne de la pollution pour les unités de type industriel pour chaque année et pour les polluants de type organique ? \". Ce modèle multidimensionnel utilise la moyenne comme fonction d\u0027agrégation sans contraintes particulières.\nHiérarchie spatiale\nDéfinition 3. Une hiérarchie spatiale d\u0027une dimension géographique est une hiérarchie où les membres de différents niveaux sont liés par des relations topologiques d\u0027inclusion et/ou d\u0027intersection.\nUne hiérarchie spatiale peut éventuellement être calculée grâce à l\u0027attribut géométrique des membres de la dimension. Si les mesures peuvent être redistribuées sur la surface des membres, les relations topologiques qui caractérisent cette hiérarchie peuvent permettre de quantifier l\u0027apport d\u0027un membre par rapport à son ancêtre dans le calcul de l\u0027agrégation. Un exemple de schéma et d\u0027instance d\u0027une hiérarchie spatiale est montré dans Figure 3 où plusieurs unités de la lagune de Venise \"Unit\" sont regroupées de façon topologique en régions \"Zone\". Une relation d\u0027inclusion existe entre les unités et les zones. Le calcul de la mesure, ici la moyenne de la valeur de pollution, peut prendre en compte ces relations topologiques.  \nHiérarchie de généralisation cartographique\nUne hiérarchie de généralisation cartographique représente une information géographique à différentes échelles ou selon différents thèmes secondaires.\nDéfinition 4. Une hiérarchie de généralisation cartographique d\u0027une dimension géographique est une hiérarchie où les membres des niveaux représentent l\u0027information géographique à différentes échelles/ ou selon différents thèmes secondaires et dont les membres d\u0027un niveau sont les résultats de la généralisation des membres du niveau directement inférieur.\nDans la même application multidimensionnelle que celle de la Figure 4, nous substituons à la hiérarchie spatiale une hiérarchie de généralisation cartographique qui représente les unités de lagune de Venise à deux différentes échelles 1:1000 et 1:500 ( Figure 5). Cette application multidimensionnelle permet de répondre aux questions telles que : \"Quelle est la pollution moyenne par polluant, année et par unité de la lagune à l\u0027échelle 1:500 ?\"\nFIG 5. Modèle multidimensionnel comportant une dimension géographique représentant la lagune de Venise décrite par une hiérarchie de généralisation cartographique.\nLa Figure 6 montre un exemple de cette hiérarchie et les deux cartes associées. La carte généralisée est obtenue en utilisant les opérateurs de généralisation, d\u0027agrégation, de simplification et de sélection (Regnauld et McMaster, 2007) : la forme de \"Palude Maggiore\" est simplifiée, \"Campo\" et \"Ruzolo\" sont fusionnées dans une seule grande zone dont la forme est simplifiée et \"Colpo\" est éliminée (\"Colpo\" est associé à la racine de la hiérarchie). Notons que cette relation est multi-valuée.\nLa représentation des mesures à travers des cartes à différentes échelles ou selon différents thèmes secondaires permet à l\u0027utilisateur d\u0027avoir un aperçu visuel global et simplifié du phénomène, en excluant les informations n\u0027étant pas primordiales pour la compréhension de ses caractéristiques générales. Le processus d\u0027agrégation pour les hiérarchies de généralisation cartographique doit tenir compte des relations de multiassociation (Spaccapietra et al., 2007)   \nOpérateurs spatio-multidimensionnels portant sur la dimension géographique\nNous classifions les opérateurs spatio-multidimensionnels qui s\u0027appliquent aux dimensions géographiques en trois catégories :\n• Les opérateurs de forage permettent la navigation dans les dimensions géographiques. Dans le cas où plusieurs hiérarchies co-existent, ils doivent permettre de préciser la hiérarchie de navigation utilisée.\n• Les opérateurs de coupe permettent de sélectionner une partie de l\u0027hypercube en utilisant l\u0027interaction avec la carte et/ou des relations topologiques, métriques et/ou directionnelles entre les membres géographiques.\n• Les opérateurs de modification dynamique de l\u0027hypercube permettent à l\u0027utilisateur de créer de nouveaux membres géographiques à la volée grâce à des opérateurs d\u0027analyse spatiale.\nLes opérateurs de modification dynamique de l\u0027hypercube permettent d\u0027introduire de la dynamique dans la structure de l\u0027hypercube. Au contraire des opérateurs de forage et de coupe, ils représentent une nouvelle approche dans l\u0027analyse spatio-multidimensionnelle. En effet, comme nous l\u0027avons montré dans la section précédente, les opérateurs d\u0027analyse spatiale des solutions SOLAP gardent une définition \"orientée\" SIG. Or, fournir une vision multidimensionnelle de ces opérateurs est indispensable pour que les processus d\u0027analyse multidimensionnelle et d\u0027analyse spatiale s\u0027enrichissent l\u0027un l\u0027autre. L\u0027adaptation des opérateurs d\u0027analyse spatiale au modèle multidimensionnel implique que les dimensions géographiques et les mesures associées puissent être calculées à la volée. La connaissance du phénomène étudié est bien sûr fondamentale pour définir le mode de calcul des mesures après insertion et/ou transformation d\u0027un membre dans la dimension géographique.\nNous détaillons ici un de ces opérateurs. L\u0027overlay est un opérateur d\u0027analyse spatiale de transformation qui permet de mettre en relation des informations de nature différente. A partir de deux cartes, il génère une carte dans laquelle les géométries des objets géographiques sont recalculées grâce à l\u0027opération topologique d\u0027intersection. L\u0027ensemble des objets géographiques de la carte résultat dépend de l\u0027opérateur logique utilisé (AND ou OR). Un exemple d\u0027overlay qui utilise l\u0027opérateur logique AND est montré dans la Figure 7. Dans un contexte spatio-multidimensionnel, il est possible d\u0027appliquer l\u0027overlay entre la carte qui représente un niveau d\u0027une dimension géographique et une autre couche choisie par l\u0027utilisateur. Le résultat de cette opération crée de nouveaux membres de la dimension géographique. Pour ces nouveaux membres, les attributs des mesures doivent être recalculés -138 -RNTI-E-13 en utilisant les parties de membres issues de l\u0027opération d\u0027overlay, les membres originaux des deux couches et les valeurs des attributs des mesures dans la table de faits. De la même façon, les opérateurs spatiaux de buffer, clipping, etc peuvent être utilisés pour modifier la structure et le contenu (i.e. membres et mesures) de l\u0027hypercube.\nLe modèle GeoCube\nLes concepts de l\u0027OLAP Géographique sont formalisés dans le modèle conceptuel GeoCube (Bimonte et al., 2005), (Bimonte, 2007). GeoCube prend en compte les composantes spatiale et sémantique de l\u0027information géographique en dimension et en mesure et introduit la dynamique de l\u0027analyse spatiale dans le processus d\u0027analyse multidimensionnelle.\nAinsi, parallèlement aux concepts de dimension géographique et des opérateurs multidimensionnels géographiques associés, l\u0027OLAP Géographique définit le concept de mesure géographique. Dimension et mesure sont traitées de façon symétrique, ce qui permet d\u0027étendre le type de requêtes effectuées par l\u0027utilisateur. Le concept de mesure géographique étant partie intégrante du modèle et de son algèbre, nous en synthétisons ci-après les grandes lignes. Le lecteur pourra trouver une présentation plus détaillée dans (Bimonte et al., 2006).\nLa mesure géographique est un objet géographique, décrit par un ensemble d\u0027attributs descriptifs et un attribut spatial, et appartenant à une ou plusieurs hiérarchies géographiques, ceci dans une définition parfaitement symétrique avec celle des dimensions. L\u0027information géographique peut donc être utilisée en dimension comme en mesure. L\u0027agrégation d\u0027une mesure géographique (lors d\u0027une opération de forage) consiste en l\u0027agrégation différenciée de ses attributs : par exemple, sur une mesure \"unit\", on appliquera la fusion des géométries, aucune agrégation aux noms, une liste pour les plantes, la moyenne pour la salinité et, pour le type, une moyenne pondérée par la surface. Ces attributs descriptifs pourront être utiles au processus décisionnel, par exemple un index de salinité très élevé peut se révéler toujours associé à une forte pollution en fer. D\u0027autre part, la mesure géographique appartient à une structure hiérarchique et doit donc pouvoir être analysée à différentes granularités qui correspondent aux niveaux des hiérarchies. Ainsi, le passage d\u0027une mesure détaillée à une mesure agrégée s\u0027effectue lors d\u0027une opération de forage ; le passage d\u0027une mesure à une mesure de granularité différente s\u0027effectue lors d\u0027une opération de navigation dans la mesure. Cette vision complètement symétrique entre mesures et dimensions, associée à des opérateurs de navigation dans la mesure augmente encore la dynamique de l\u0027analyse OLAP géographique.\nDans le reste de cette section nous présentons les concepts principaux du modèle de données et de l\u0027algèbre de GeoCube. -Llagoon_spatial \u003d {Szone}, Sunit, Sall_unit sont les niveaux de la hiérarchie, -lagoon_spatial est la relation d\u0027ordre du treillis telle que (Sunit lagoon_spatial Szone) et (Szone lagoon_spatial Sall_unit) Le Schéma du Cube de Base représente le schéma conceptuel de l\u0027application multidimensionnelle et l\u0027Instance de Cube de Base représente le cuboïde de base, i.e. une table de faits où toutes les dimensions sont aux niveaux les plus détaillés. Le Schéma du Cube de Base est défini par un ensemble de Schémas de Hiérarchie et une fonction booléenne. Cette fonction peut être représentée dans un espace multidimensionnel où les instances des membres des niveaux les plus détaillés des dimensions sont projetées sur les axes, et les points sont des valeurs booléennes représentant l\u0027existence du fait. Grâce à cette modélisation de l\u0027espace multidimensionnel, les Schémas de Hiérarchie utilisés dans la définition du Cube de Base jouent tous le rôle de dimensions. La distinction entre les Schémas jouant le rôle de dimensions et le Schéma jouant le rôle de la mesure se fait de façon dynamique, au moment de l\u0027énoncé de la requête multidimensionnelle.\nModèle de données\nLe Cube de Base pour l\u0027application de la figure 4 est BCcorila \u003d ?Hpollutants, Htime, Hlagoon_spatial, Hrate, ?? où :\n-H pollutants est la hiérarchie qui décrit les polluants, -H time est la hiérarchie temporelle, -H rate est la hiérarchie des intervalles de valeurs de pollution, -H lagoon_spatial est la hiérarchie spatiale -? une fonction booléenne définie sur I(S pollutant )×I(S day )×I(S unit )×I(S rate5 ) soit le produit cartésien des instances des niveaux les plus détaillés des hiérarchies du Cube.\nL\u0027algèbre\nLe  Il est aussi possible de définir, à partir du même Cube de Base, une Vue qui utilise les unités de la lagune comme mesures : Vcorila-unit-day \u003d ?BCcorila, ?Spollutant, Sday, Srate?, ?unit-fusion, ?? représentant les unités polluées par polluant, valeur de pollution et jour. où H h est la hiérarchie contenant le membre géographique résultant de l\u0027opérateur de buffer et f est une fonction qui calcule les mesures pour ce nouveau membre géographique. L\u0027opérateur ? utilise les mêmes paramètres. -Deux opérateurs qui permettent de changer la granularité de la mesure. Ces opérateurs permettent de monter dans la hiérarchie de la mesure en remplaçant un ensemble de mesures détaillées par des mesures appartenant à un niveau moins détaillé. L\u0027opérateur Classify, noté ?, nécessite que tous les descendants des nouvelles mesures soient présents dans la   Figure 8A), les graphiques, et la carte avec la GIS Toolbar et la GeWOlap Toolbar. La GIS Toolbar ( Figure 8B) propose des fonctionnalités purement SIG (zoom, pan, etc.). La GeWOlap Toolbar ( Figure 8C) présente les opérateurs tels que définis dans la section précédente, une fonctionnalité (Properties Member) qui affiche les propriétés d\u0027un membre de la dimension géographique et une autre (Measure displays) qui permet de paramétrer le type de diagramme représentant les mesures.\nPrésentation générale de l\u0027interface utilisateur\nNavigation avec GeWOlap\nDans cette section, nous illustrons le processus de navigation dans l\u0027hypercube de la Figure 4. Nous montrons les opérateurs de forage, les opérateurs de coupe et les opérateurs de modification dynamique de l\u0027hypercube. La navigation dans un hypercube avec des mesures géographiques a été décrite en détail dans (Bimonte et al., 2006).\nGIS Toolbar\nGeWOlap Toolbar\nFIG 8. Interface GeWOlap ; A) OLAP Toolbar, B)GIS Toolbar C) GeWOlap Toolbar\nLes opérateurs de forage (GeWOlap Roll-up, GeWOlap Drill-down Position, etc.) sont disponibles dans GeWOlap à travers la table de pivot et la composante cartographique. Lors de ces opérations, GeWOlap synchronise les composantes tabulaire et cartographique, en évitant que des membres cachent (recouvrent) leur fils sur la carte. On peut ainsi obtenir une carte dont les zones géographiques sont de grains différents. Un exemple d\u0027utilisation d\u0027un opérateur de forage est montré dans la Figure 9. La carte initiale (Figure 9.1) montre 3 mesures qui représentent la pollution moyenne pour tous les polluants, les polluants inorganiques et organiques sur la région sud. Après forage sur le membre géographique \"North Lagoon\", la carte de la Figure 9.2 présente la pollution moyenne sur des zones détaillées de la région nord de la lagune (North Swam, South Venice, North Venice, Bocca Lido et Litorale de Cavallino).\nCette opération réalise l\u0027opération ?(Vcorila-rate)[Szone] \u003d Vcorila-rate-zone où Vcorila-rate est la Vue qui représente la valeur de la pollution pour chaque unité, tous les polluants et tous les jours, et Vcorila-rate-zone représente la valeur de la pollution pour chaque zone, tous les polluants et tous les jours. Il est important de souligner le fait que puisque la table de pivot mélange plusieurs granularités d\u0027une même dimension, sa représentation formelle correspond à un ensemble de Vues. Les opérateurs de modification dynamique de l\u0027hypercube (GeWOlap Buffer et GeWOlap Overlay) créent de nouveaux membres de dimensions géographiques et permettent de recalculer les mesures pour ces nouveaux membres. GeWolap Buffer permet d\u0027exprimer des requêtes sur des membres de la dimension géographique définis à la volée par l\u0027utilisateur, grâce à la désignation d\u0027une zone tampon. Pour ces nouveaux membres, les mesures doivent être recalculées grâce à une fonction fournie par l\u0027utilisateur. Le calcul de la mesure prend en compte les parties de membres qui sont recouverts totalement et/ou partialement par la région tampon, les membres originaux et les valeurs de mesures dans la table de faits associés. Supposons ainsi que la concentration des polluants est calculée comme une moyenne pondérée sur les surfaces. La Figure 10.2 montre la pollution moyenne pour \"Bocca Chiogia\", \"Chioggia\", \"Litorale Pellestrino\", \"South Swam\" et une zone tampon de 3 km autour de \"Bocca Malmocco\". Cette opération implémente l\u0027opérateur ?(Vcorila-rate) [Hlagoon_spatial_zoneb,favgbuffer]. Il est appliqué à Vcorila-rate en utilisant comme paramètres une hiérarchie qui contient le résultat de l\u0027opération spatiale de buffer et la fonction pour le re-calcul de la mesure. On note que la modification issue de l\u0027application de cet opérateur se répercute à la fois sur la représentation tabulaire et sur la cartographie. \nFIG 10. GeWOlap Buffer 1) Données initiales 2) Résultat\nL\u0027opérateur GeWOlap Overlay permet de faire des requêtes sur des membres calculés en utilisant une autre information géographique issue d\u0027une autre couche et une fonction qui est fournie par l\u0027utilisateur. Le calcul de la mesure prend en compte les parties de membres issus de l\u0027opération d\u0027overlay, les membres originaux des deux couches et les valeurs de mesures dans la table de faits associés. Supposons que l\u0027utilisateur veuille ajouter aux informations de la pollution une autre source de données, par exemple une carte thématique de la lagune définissant les zones soumises à épuration. Supposons que la valeur de la nouvelle mesure peut être estimée comme une moyenne pondérée sur la surface des zones non épurées. La Figure 11.2 montre les valeurs modifiées de la pollution pour les zones de la lagune \"North Venice\" soumises à épuration. Nous notons que la pollution dans \"Isola delle Tresse\" avant l\u0027overlay était de 2631.4 (Figure 11.1). GeWOlap overlay nous permet de mettre en évidence une zone d\u0027épuration (\"Isola delle Tresse-ZoneRD\") où la valeur de la pollution a pu être ré-estimée. Cette opération implémente l\u0027opérateur ?(Vcorila-rate) [Hlagoon_spatial_zoneov, favgover]. Il utilise comme paramètres une hiérarchie qui contient le résultat de l\u0027opération spatiale d\u0027overlay et la fonction pour le re-calcul de la mesure. \nFIG. 11. 1) Données initiales 2) Résultats du GeWOlap Overlay\nConclusion\nL\u0027OLAP spatial vise à intégrer la donnée spatiale dans les systèmes OLAP pour offrir un processus de décision spatial et multidimensionnel. En partant d\u0027une réflexion sur l\u0027analyse multidimensionnelle et spatiale, nous montrons que les modèles SOLAP ne prennent pas complètement en compte la composante sémantique de l\u0027information géographique et le caractère itératif de l\u0027analyse spatiale. Nous redéfinissons les concepts de dimension spatiale et les opérateurs spatio-multidimensionnels afin de pallier cette lacune. Nous introduisons le concept de dimension géographique et spécifions les différents types de hiérarchies possibles en considérant la sémantique des relations entre les membres. Nous identifions également de nouveaux opérateurs spatio-multidimensionnels qui intègrent l\u0027analyse spatiale dans le paradigme multidimensionnel. Nous présentons notre prototype GeWOlap, une solution web qui intègre des fonctionnalités OLAP géographiques dans un environnement unique et synchronisé en supportant les dimensions et les mesures géographiques. Ce prototype propose des opérateurs qui intègrent l\u0027analyse multidimensionnelle et l\u0027analyse spatiale. L\u0027introduction de ces opérateurs présente une double difficulté : la prise en compte d\u0027une gestion dynamique de la structure de l\u0027hypercube (en termes de membres et de hiérarchie) et le calcul des mesures associées aux nouveaux membres. GeWOlap vise à combiner effectivement l\u0027analyse spatiale et multidimensionnelle dans le même paradigme d\u0027exploration et d\u0027analyse. Actuellement, nous travaillons sur l\u0027intégration des hiérarchies de généralisation cartographique dans GeoCube et sur des politiques d\u0027agrégation ad-hoc pour ce type de hiérarchies. De plus, un cadre global qui détermine automatiquement les variables visuelles à utiliser selon des règles sémiologiques parait nécessaire. En effet, GeWOlap\n"
  },
  {
    "id": "867",
    "text": "Introduction\nUn des principaux objectifs de la biologie moléculaire consiste à comprendre la régulation des gènes d\u0027un organisme vivant dans des contextes biologiques spécifiques. Les facteurs de transcription sont les régulateurs de la transcription qui vont réagir avec les promoteurs de la transcription des gènes cibles. Les techniques récentes d\u0027analyse du transcriptome, telles que les puces à ADN permettent de mesurer simultanément les niveaux d\u0027expression de plusieurs milliers de gènes. Nous avons déjà décrit le système LICORN (Elati et al., 2007a) qui se fonde sur un modèle de régulation locale coopérative : chaque gène peut être régulé par un ensemble des coactivateurs et/ou un ensemble de coinhibiteurs, ces corégulateurs agissent collectivement pour influencer leur(s) gène(s) cible(s). LICORN met en oeuvre une approche originale de Fouille de Données afin d\u0027inférer des relations de régulation coopérative à partir de données d\u0027expression. Cet algorithme a été évalué avec succès sur des données publiques de transcriptome de levure. L\u0027application de LICORN sur des données de transcriptome humaines est plus complexe, car le nombre de régulateurs connus est plus important, et nécessite un temps de calcul considérable. En effet, les gènes de faible support vont avoir un nombre très élevé de régulateurs candidats. Nous proposons dans ce travail d\u0027étendre LICORN pour qu\u0027il puisse traiter une contrainte de sélection de corégulateurs candidats adaptative pour chaque gène, prenant en compte le support du gène cible et bornant le nombre de corégulateurs candidats possibles. La suite de cet article est organisée comme suit. Dans la section 2, nous introduisons briève-ment le principe de LICORN. Dans la section 3, nous détaillons l\u0027extension de LICORN à la recherche adaptative. Nous exposons des résultats préliminaires concernant l\u0027application de cette approche à des données de cancer de vessie (section 4) et enfin, nous concluons sur les perspectives ouvertes par ce travail.\nLICORN : Algorithme d\u0027apprentissage\nÉtant donnés : -un ensemble de régulateurs R et un autre de gènes cibles G -deux matrices d\u0027expression discrétisées (MG, MR) à valeurs dans ? \u003d {?1, 0, 1}, codant les états de R et G pour un échantillon S de n conditions : S \u003d s 1 , . . . , s n -un programme de régulation RP : ? × ? ? ?, qui associe à un état d\u0027un ensemble d\u0027activateurs ? R et à un état d\u0027un ensemble d\u0027inhibiteurs ? R un état du gène cible 1 -une fonction de score local h : ? n × ? n ? R, permettant d\u0027évaluer un réseau local de régulation, notre but est de trouver, pour chaque gène cible, l\u0027ensemble de régulateurs qui explique au mieux son niveau d\u0027expression, i.e. le graphe qui minimise la différence entre la valeur d\u0027expression prédite et la valeur d\u0027expression observée pour un gène cible. Pour le résoudre, nous avons formalisé le problème d\u0027inférence de réseaux de régulation comme un problème de recherche dans un espace d\u0027états muni d\u0027une relation de spécialisation, des motifs satisfaisant un ensemble de contraintes anti-monotones (Agrawal et al., 1993). Ceci nous a permis d\u0027optimiser le parcours de l\u0027espace de recherche et de proposer une méthode originale LICORN, capable d\u0027apprendre des réseaux de régulation coopérative. Cet algorithme décompose la tâche d\u0027apprentissage de structures en trois étapes indépendantes. Nous présentons par la suite ces trois étapes, déjà décrites dans Elati et al. (2007a). Dans la suite, chaque gène ou régulateur g est représenté par deux ensembles supports ? S, son 1-support S 1 (g) et son ?1-support S ?1 (g). LICORN construit d\u0027abord, en utilisant une extension de l\u0027algorithme Apriori (Agrawal et al., 1993) qui manipule en parallèle les deux supports (1 et ?1-supports), le treillis CL des ensembles de corégulateurs fréquents. Un corégulateur est fréquent s\u0027il est fréquent pour au moins une des valeurs d\u0027intérêt, ici 1 ou -1.\nPuis, LICORN calcule pour chaque gène cible g un ensemble de co-régulateurs candidats, de taille réduite par rapport à celle de tous les sous-ensembles possibles de régulateurs. Le critère pour qu\u0027un ensemble de corégulateurs puisse participer au programme de régulation d\u0027un gène cible est d\u0027observer dans les données d\u0027expression une covariation significative de leurs niveaux d\u0027expression. La contrainte de corégulation teste la taille de l\u0027intersection entre les supports du gène et ceux du corégulateur.\nDéfinition 1 (Contrainte de corégulation) Soit une combinaison de co-régulateurs C, un gène cible g, et leurs supports respectifs\nNous distinguons les complexes de régulateurs satisfaisant C coreg pour chaque gène cible selon leur mode de régulation : les complexes d\u0027activateurs candidats A(g) co-varient positivement avec le gène g, en revanche, les complexes d\u0027inhibiteurs candidats I(g) co-varient négative-ment avec le gène cible :\nNous avons décrit dans (Elati et al., 2007a) comment nous exploitons l\u0027anti-monotonie de C coreg pour effectuer une recherche efficace dans le treillis des régulateurs candidats. Ainsi, nous pouvons calculer l\u0027ensemble de tous les graphes de régulation candidats pour chaque gène cible g, à partir des complexes d\u0027activateurs et d\u0027inhibiteurs extraits comme suit :\nUn GRN candidat pour un gène cible g, ou tout simplement un GRN, est un élement de C(g).\nEnfin, à partir de chaque C(g), LICORN extrait le réseau de régulation (GRN) pour chaque gène cible qui explique au mieux les données d\u0027expression, à l\u0027aide d\u0027une fonction de score (l\u0027erreur absolue moyenne) qui mesure la capacité des régulateurs à prédire le niveau d\u0027expression du gène cible g. Ensuite, LICORN sélectionne les GRNs statistiquement significatifs en se fondant sur une méthode (non décrite ici) à base de permutations.\nGénération adaptative de corégulateurs candidats\nL\u0027algorithme précédent de génération de corégulateurs candidats ne fournit pas de méca-nisme pour limiter le nombre de corégulateurs candidats pour un gène cible. Ceci mène parfois à obtenir trop ou trop peu de corégulateurs candidats, et ainsi à un temps excessif de calcul ou à une faible performance. La plupart des algorithmes classiques d\u0027extraction de motifs fréquents ou contraints ne permettent pas d\u0027encadrer a priori le nombre souhaité de résultats. L\u0027extraction des k-meilleurs motifs fréquents a été introduite dans (Fu et al., 2000). Notre objectif ici, est d\u0027extraire pour chaque gène cible, s\u0027il existe, un ensemble de taille modeste de meilleurs corégulateurs au sens de la contrainte de corégulation. Nous proposons, étant donné un intervalle sur le nombre de corégulateurs candidats souhaités, de concevoir un algorithme itératif qui effectue une recherche adaptative des meilleurs corégulateurs candidats.\nCet algorithme, noté AdapSearch (algorithme 1), ajuste le seuil de la contrainte de coré-gulation pendant la phase d\u0027extraction des corégulateurs candidats, afin d\u0027obtenir un nombre approprié de corégulateurs candidats pour chaque gène cible. Etant donné le treillis de corégulateurs fréquents (voir section 2), un gène cible g et un intervalle [minReg, maxReg] encadrant le nombre de corégulateurs candidats pour g, AdapSearch initialise le seuil de coré-gulation S coreg à 50% et appelle GenCoreg (lignes 1 à 3 de l\u0027algorithme 1).\nLa procédure GenCoreg (non décrite) effectue une recherche par niveau sur ce treillis de corégulateurs, à partir des corégulateurs les plus généraux jusqu\u0027à atteindre les corégulateurs les plus spécifiques (i.e., maximaux au sens de l\u0027inclusion) qui satisfont la contrainte de corégulation. Notons que GenCoreg termine le processus de génération dès que le nombre de corégulateurs candidats dépasse maxReg. Quand le résultat de GenCoreg est retourné, AdapSearch vérifie d\u0027abord si le nombre des corégulateurs candidats dépasse maxReg. Si c\u0027est le cas (ligne 6), c\u0027est-à-dire que le seuil de corégulation est trop bas, AdapSearch augmente  Si (minReg \u003e |R(g)|) Alors 10:\nR(g) :\u003d AdapMin(g,R(g),CL, maxReg, S coreg ) 12:\nFinSi 13: Fin TantQue 14: Retourner R(g) 15: Fin le seuil de corégulation (algorithme 2, ligne 1) : S coreg est incrémenté d\u0027un pas fixe ? et la procédure GenCoreg est appellée itérativement jusqu\u0027à ce que le nombre de corégulateurs soit inférieur à maxReg ou que S coreg dépasse la valeur maximum, i.e., 1. Si l\u0027augmentation du seuil engendre un ensemble de corégulateurs candidats dont la taille est inférieure à minReg (ligne 7 de l\u0027algorithme 2), alors qu\u0027il était supérieur à minReg dans l\u0027itération précédente (ligne 5 de l\u0027algorithme 2), alors l\u0027algorithme oscille entre deux valeurs de S coreg et AdapSearch retourne alors les meilleurs maxReg corégulateurs candidats parmi les deux ensembles de régulateurs résultats. R(g) :\u003d maxReg meilleurs corégulateurs de (R(g) ? tmp) 5: FinSi 6: Retourner R(g) Dualement, AdapSearch vérifie si le nombre de corégulateurs est inférieur à minReg. Si c\u0027est le cas, on décrémente le seuil de corégulation de ? jusqu\u0027à ce que le nombre de corégu-lateurs candidats soit supérieur ou égal à minReg ou que S coreg atteigne un seuil minimum (par exemple 30%). Dans ce dernier cas, on garantit que les corégulateurs ont un minimum de recouvrement avec le gène cible.\nExpérimentations : LICORN versus LICORN adaptatif\nDans cette étude, nous utilisons des données humaines de 85 échantillons. Ces échantillons se divisent en cinq échantillons normaux (sujets dépourvus de cancer), et quatre-vingts échan-tillons de carcinomes de vessie à différents stades de la maladie. Nous nous intéressons à trois tendances de variation du niveau d\u0027expression : dans un échantillon tumoral donné, le niveau d\u0027expression d\u0027un gène peut augmenter par rapport à son niveau normal (noté par 1 dans la matrice discrétisée) ; il peut diminuer par rapport à son niveau normal (noté par -1) ; ou encore être stable (noté par 0). Nous renvoyons à (Elati et al., 2007b) pour une description de la technique de discrétisation utilisée. Nous obtenons une matrice discrétisée de 8000 gènes et 699 facteurs de transcription.\nDans la partie gauche de la figure 1, nous présentons la distribution de nombre de corégu-lateurs candidats générés par LICORN avec S coreg fixé à 60%. La valeur médiane du nombre de corégulateurs candidats est 1024 et la valeur maximale est 35000. Il y aura donc un sousensemble des gènes, souvent de faible fréquence, pour lequel LICORN génère plusieurs milliers de corégulateurs candidats. Ces résultats nous montrent que l\u0027utilisation de LICORN adaptatif avec un intervalle de nombre de corégulateurs candidats autour de la médiane va réduire nettement le volume de corégulateurs candidats engendré, ce qui représente un gain considérable en terme de temps de calcul. Dans la deuxième phase de validation, nous avons lancé LICORN adaptatif avec les valeurs suivantes de paramètres : ? \u003d 0.1, valeur minimum de S coreg \u003d 30% et un intervalle de coré-gulateurs candidats de [200, 2000]. LICORN adaptatif a obtenu des résultats équivalents voire meilleurs que LICORN (voir figure 1, partie droite) : i) 85% de GRNs sont détectés comme significatifs avec un FDR de 5% (voir Elati et al. (2007a)) par les deux méthodes avec 92% d\u0027interactions régulateur/gène en commun. ii) 5% de GRNs détectés par LICORN seulement contre 10% de GRNs détectés par LICORN adaptatif seulement.\nConclusion\nAfin d\u0027améliorer notre algorithme en vue de l\u0027appliquer sur des données humaines de cancer, nous l\u0027avons étendu pour qu\u0027il puisse traiter une contrainte de corégulation adaptative pour chaque gène, prenant en compte le support du gène cible et en bornant le nombre de corégula-teurs candidats possibles. Cette recherche adaptative a rendu le calcul plus efficace, tout en engendrant des réseaux dont les performances sont dans une grande majorité des cas équivalentes voire meilleures. Enfin, des structures plus adéquates peuvent être utilisées ou imaginées pour améliorer les performances de la recherche adaptative, par exemple la structure de COFI-tree récemment utilisée par Ngan et al. (2005) pour optimiser l\u0027extraction de k-meilleurs motifs.\nRéférences\nAgrawal, R., T. Imielinski, et A. Swami (1993) \nSummary\nWe have previously proposed an original Data Mining algorithm LICORN, that infers cooperative regulation relations from expression datasets. LICORN has been successfully applied to public Yeast datasets, but running it on more complex datasets (e.g., human) is problematic. In order to overcome this limitation, we propose here an adaptive coregulation constraint that takes into account the support of the gene to look for a bound number of candidate regulators. Preliminary experiments on human bladder cancer data have shown that the adaptive constraint allows LICORN to learn much more efficiently regulation relations that show a similar (if not better) prediction performance.\n"
  },
  {
    "id": "868",
    "text": "Introduction\nLe problème de la comparaison de graphes est un sujet qui a été largement étudié dans la littérature depuis plusieurs décennies. S\u0027il existe des algorithmes pour la recherche d\u0027isomorphisme entre deux graphes, c\u0027est-à-dire dans le cas où les deux graphes ont la même structure, même nombre de noeuds et même nombre d\u0027arêtes, le cas plus général de comparaison entre deux graphes de tailles différentes est un problème NP-complet. Le problème est encore plus difficile lorsque les graphes sont valués et que l\u0027on recherche une mesure de similarité entre graphes, afin de pouvoir les ordonner, les classer, etc.\nOn est confronté à ce problème dans certaines approches de la reconnaissance des formes où on cherche à construire des classes d\u0027objets représentés par des ensembles structurés de ré-gions, lignes, points, etc. Une des problématiques de la recherche d\u0027image par le contenu est de retrouver dans une base, les images contenant un objet particulier ou un type d\u0027objet, d\u0027animal ou de personne, pouvant prendre des aspects très variables dans des environnements eux aussi variables. Les signatures globales ne permettent pas toujours de résoudre ce problème et les approches par points d\u0027intérêt ne sont pas bien adaptées aux changements d\u0027aspect d\u0027un animal ou d\u0027une personne, selon la prise de vue. Une approche prometteuse semble donc être de représenter un objet par un ensemble de régions adjacentes valuées à la fois par des caractéris-tiques intrinsèques de couleur, texture et forme, mais aussi par leurs dispositions relatives (cf. Philipp-Foliguet et Gony (2006)). Le graphe d\u0027adjacence de régions constitue donc la structure adaptée pour représenter des objets dans leur infinie variabilité. Cependant l\u0027obtention des ré-gions est un problème extrêmement difficile, qui ne possède en aucun cas une solution unique, car dépendant du niveau de détail souhaité, et qui est peu robuste aux changements d\u0027éclairage, de résolution et d\u0027aspect de l\u0027objet. Le nombre et les caractéristiques des régions sont donc très variables d\u0027une image à l\u0027autre pour représenter un même objet.\nDes approches récentes ont proposé de considérer les graphes comme des ensembles de chaînes (Kashima et Tsuboi (2004)) et les similarités employant des noyaux sur des graphes se ramènent alors à des noyaux sur des chaînes. Nous avons adopté cette approche par noyau de chaînes et proposons de comparer différents types de chaînes et différentes fonctions noyau sur ces chaînes.\nPour effectuer l\u0027appariement entre graphes ou sous-graphes d\u0027images en un temps compatible avec une utilisation en temps réel, nous avons opté pour l\u0027algorithme de \"branch and bound\". La classification ou la fouille d\u0027une base à partir d\u0027un ou plusieurs exemples se fait ensuite par apprentissage interactif à l\u0027aide de SVM (Support Vector Machine) pour la classification et de techniques d\u0027apprentissage actif pour la sélection des images à faire annoter par l\u0027utilisateur.\nMesures de similarité de graphes\nChaque image de la base est représentée par un graphe G ? G défini par un couple G \u003d (V, E), où V est un ensemble de sommets, et E ? V × V un ensemble d\u0027arêtes. Par exemple, lorsqu\u0027une image est segmentée en régions, on peut construire un tel graphe en considérant que chaque sommet v ? V est une région, et chaque arête e \u003d (v 1 , v 2 ) ? V × V représente une adjacence entre deux régions. On considère aussi les chaînes h ? H(G) présentes dans le graphe, à savoir les suites (v 1 , e 1 , v 2 , e 2 , ...) de sommets v i ? V reliées par des arêtes e i \u003d (v i?1 , v i ) ? V × V . Différentes fonctions H(.) qui à un graphe G font correspondre un ensemble de chaînes peuvent être considérées, comme celles qui renvoient les chaînes avec ou sans cycles par exemple. Dans cette partie, nous nous intéressons aux fonctions de similarité qui peuvent être utilisées avec n\u0027importe quel ensemble de chaînes, nous abordons le choix des fonctions H(.) dans la partie suivante.\nDans de nombreuses mesures de similarité\nqui ont été proposées, l\u0027idée est généralement de trouver les meilleurs appariements entre les sommets et les arêtes. Par exemple, Sorlin et al. (2006) propose une mesure de similarité qui renvoie la valeur moyenne des meilleures similarités en fonction des appariements entre sommets et arêtes. FReBIR (cf. Philipp-Foliguet et Gony (2006)) calcule la valeur du meilleur appariement entre une chaîne requête et les chaînes de l\u0027autre image. Cependant cette mesure de similarité ne respecte aucune propriété mathématique usuelle telle que la symétrie ou l\u0027inégalité triangulaire, ce qui la rend difficilement utilisable par certains outils puissants utilisés en classification ou en \"browsing\" par exemple.\nCertaines mesures de similarité s\u0027efforcent de respecter un ensemble de contraintes mathématiques permettant une utilisation facile par les moteurs de recherche, par exemple les fonctions noyaux au sens de Mercer. Dans ce cas, la mesure de similarité que nous noterons K(G, G ? ) sera le produit scalaire K(G, G ? ) \u003d ?(G ? ) dans un certain espace Hilbertien H, avec ? : G ? H une fonction d\u0027injection qui à un graphe fait correspondre un vecteur de H.\nCertaines approches de construction de fonctions noyaux s\u0027intéressent au calcul explicite du vecteur ?(G). Par exemple, Jurie et Triggs (2005) proposent de calculer un dictionnaire des prototypes de sommets du graphe (des points d\u0027intérêt) les plus répandus dans la base, puis projettent les sommets sur ce dictionnaire pour former des histogrammes -histogrammes qui seront les vecteurs ?(G) de l\u0027espace Hilbertien. Grauman et Darell (2005) intègrent les contraintes spatiales de manière implicite via une approche pyramidale. L\u0027inconvénient de ces méthodes basées sur des prototypes est leur faible capacité à généraliser.\nUne autre technique pour construire des fonctions noyaux sur graphes à laquelle nous nous intéressons plus particulièrement dans cet article, est le calcul implicite des images dans l\u0027espace Hilbertien via la fonction noyau. Le processus de construction repose principalement sur un ensemble de propriétés concernant les fonctions noyaux, comme le fait que la somme ou le produit de deux fonctions noyaux est encore une fonction noyau. Par exemple, dans le cas où la fonction noyau ne porte que sur les sommets du graphe, la fonction suivante est une fonction noyau sous réserve que la fonction K V (v, v ? ) l\u0027est aussi :\nv?G,v ? ?G ? Kashima et Tsuboi (2004) ont proposé de comparer deux graphes en comparant tous les parcours possibles de ces deux graphes le long des arêtes. La fonction noyau porte alors sur des ensembles (ou sacs) de chaînes, lesquels intègrent les similarités entre sommets et entre arêtes. Ils ont défini un modèle assez général pour le calcul d\u0027une fonction noyau sur graphe, en considérant l\u0027ensemble des chaînes h \u003d (v 1 , e 1 , v 2 , e 2 , ...) du graphe, puis en calculant la valeur moyenne de la similarité entre les chaînes de G et G ? de même longueur. Si on note |h| la longueur de la chaîne h, i.e. son nombre d\u0027arêtes, cette fonction noyau peut s\u0027exprimer :\navec p(h|G) la probabilité de trouver la chaîne h dans le graphe G. La fonction noyau K C (h, h ? ) mesure la similarité entre deux chaînes :\nLes noyaux mineurs qui interviennent dans cette équation sont K V , noyau sur les sommets et K E , noyau sur les arêtes. Pour K V , nous utilisons classiquement un noyau Gaussien, qui retourne des valeurs comprises entre 0 et 1. Le noyau K E permet de prendre en compte les similarité entre arêtes (cf. Suard et al. (2005)), ou plus simplement il peut prendre une valeur fixe.\nDans le contexte de graphes sur des molécules utilisé par Kashima, la similarité entre sommets est binaire, un sommet (un atome) est ou n\u0027est pas le même qu\u0027un autre. Cependant, dans notre contexte où la similarité entre deux sommets est continue, cette fonction a tendance à noyer dans la somme les similarités entre chaînes. Par exemple s\u0027il existe 3 appariements (valeur de similarité a élevée) parmi 10000 appariements possibles (9997 valeurs de similarité b faibles), alors la similarité globale vaudra 3a + 9997b. Les 3 appariements forts ne faisant pas le poids face aux 9997 appariements faibles. Dans le contexte d\u0027application aux images, les probabilités (connues pour des molécules) de l\u0027équation ? sont toutes mises à 1.\nL\u0027autre problème relatif au modèle de Kashima concerne sa complexité très élevée en terme de calculs. Afin de palier ces problèmes Suard et al. (2005) a proposé une fonction noyau aux propriétés plus intéressantes pour notre contexte. Il utilise une recherche du maximum, ce qui permet d\u0027une part d\u0027utiliser des algorithmes rapides, mais aussi d\u0027avoir une meilleure discrimination :\nNotons que cette fonction ne respecte pas les conditions de Mercer, cependant elles ne sont violées que dans des cas très particuliers, et s\u0027avèrent toujours vérifiées sur les bases de données que nous avons utilisées. Ce type de fonctions a aussi été utilisé par Eichhorn et Chapelle (2004) et Wallraven et al. (2003) qui en tirent les mêmes conclusions.\nD\u0027autres fonctions permettent d\u0027augmenter encore plus la discrimination ainsi que la vitesse de calcul, avec un processus de mise en correspondance proche du moteur FReBIR (cf. Philipp-Foliguet et Gony (2006)), qui effectue la recherche du meilleur appariement :\nDe même, cette fonction n\u0027est pas une fonction noyau au sens strict, cependant en pratique elle respecte aussi les conditions sur les bases expérimentales.\nAppariement de graphes\nAlgorithmes d\u0027optimisation\nUne fois définie la mesure de similarité entre deux graphes, le problème de trouver l\u0027appariement qui maximise cette similarité demeure très complexe, surtout si on ne se limite pas aux isomorphismes entre les deux graphes. Il y a souvent un compromis à faire entre solution optimale et temps de calcul. Les algorithmes par colonie de fourmis ou par recherche taboue (cf. Sorlin et al. (2006)) trouvent des solutions optimales mais sont trop longs pour les utilisations \"temps réel\" que nous envisageons. Une autre approche très répandue utilise des arbres de recherche. Chaque noeud de cet arbre représente un couple de sommets (v, v?) ? V × V ? candidats à l\u0027appariement. On construit une arborescence de proche en proche à partir d\u0027un noeud racine vide et en développant chaque noeud par les couples candidats. Les noeuds candidats sont les couples (v, v?) compatibles avec les noeuds déjà présents dans le chemin menant de la racine au noeud courant. L\u0027avantage de cette représentation par arborescence est que la similarité d\u0027un chemin se calcule au fur et à mesure de la construction du chemin. Dans le cas d\u0027une fonction de similarité qui utilise un max (comme K Suard ou K max ), l\u0027algorithme \"branch and bound\" permet de trouver une solution optimale sans explorer toutes les solutions possibles. On obtient d\u0027abord la solution la plus prometteuse qui fournit une borne inférieure de la similarité K(G, G?). Puis on construit les autres branches de l\u0027arbre seulement si elles sont susceptibles d\u0027améliorer la valeur de similarité. La solution la plus prometteuse est obtenue en explorant d\u0027abord le chemin construit avec les noeuds dont les valeurs de similarité sont les plus grandes.\nEnsembles des chaînes\nPuisque nous avons choisi de mesurer la similarité entre graphes par la similarité entre ensembles de chaînes, nous décrivons ici différentes propriétés que peuvent posséder les ensembles de chaînes et qui correspondent à des configurations d\u0027appariements.\nOn notera pour simplifier h \u003d abc... une chaîne du graphe  \nExpérimentations\nNous avons utilisé deux bases d\u0027images pour nos tests : Columbia modifiée et Caltech, où l\u0027objectif est de retrouver les images qui contiennent un objet particulier ou un objet appartenant à une catégorie. La première nous sert à tester et comprendre le comportement des différents noyaux selon la longueur des chaînes employées. La deuxième permet de comparer sur une base réelle différents ensembles de chaînes et aussi de comparer les méthodes par graphe d\u0027adjacence de régions à d\u0027autres méthodes soit globales, soit employant des ensembles de point d\u0027intérêt.\nLes images sont segmentées en régions floues (cf. Philipp-Foliguet et Gony (2006)), ce qui permet de segmenter une base entière avec un réglage de paramètres global sur la base. Les régions sont des ensembles flous, qui se chevauchent plus ou moins (cf Fig 3). Le nombre de régions formant l\u0027objet est très variable d\u0027une image à l\u0027autre, un visage par exemple peut être constitué par une seule région floue sur une image ou par 5 comme sur la \nProtocole expérimental\nNous possédons une vérité terrain pour chacune des bases, ce qui nous permet d\u0027une part de simuler la recherche interactive d\u0027un objet ou d\u0027une classe d\u0027objets, et d\u0027autre part d\u0027éva-luer les résultats renvoyés par le système. Muni d\u0027un noyau sur graphe, nous entraînons un classifieur SVM dans le but de classer les images par ordre de pertinence. Les noyaux nous permettent aussi d\u0027utiliser des techniques d\u0027apprentissage actif qui permettent de sélectionner les meilleures images à faire annoter par l\u0027utilisateur (cf. Gosselin et Cord (2006)). Nous éva-luons chaque méthode par la simulation d\u0027un grand nombre de sessions de recherche. Pour chaque session de recherche, une catégorie est choisie. Au sein de cette catégorie, une image choisie au hasard est annotée positivement, ce qui permet d\u0027obtenir un premier classement en se basant uniquement sur la similarité. Puis, les images sélectionnées par la technique d\u0027apprentissage actif sont annotées en fonction de la catégorie choisie au début de la session simulée. Ce premier jeu d\u0027annotation permet d\u0027entraîner le classifieur et ainsi d\u0027obtenir un meilleur classement. Le processus est ensuite répété en suivant le même principe de sélection et de classification. Nous répétons la simulation de session de recherche une centaine de fois pour chaque catégorie. Ainsi, pour chaque catégorie nous pouvons mesurer la qualité moyenne du classement à chaque étape d\u0027annotation à l\u0027aide de la Précision Moyenne, un critère d\u0027évalua-tion souvent utilisé dans les campagnes d\u0027évaluation telle que TRECVID 1 . Puis, dans le but d\u0027obtenir une mesure de la qualité globale du système, nous calculons la valeur moyenne des Précisions Moyennes (MAP, Mean Average Precision) sur toutes les catégories. \nColumbia modifiée\nComportement des noyaux en fonction de la longueur des chemins considérés\nLa Fig. 2(a) montre les résultats des simulations sur la base Columbia modifiée avec des ensembles de chaînes sans boucle ni cycles pour différentes longueurs de chaînes avec le noyau de Kashima (Eq. 2). Lorsque des chaînes sont de longueur fixe (|h| \u003d k), les performances sont d\u0027autant meilleures que la longueur est petite, ce qui s\u0027explique par le fait que les objets sont représentés par 1 à 3 régions. En utilisant des longueurs de chaînes variables (|h| ? k), les performances ne sont pas améliorées bien que l\u0027on considère davantage de possibilités. Cela peut s\u0027expliquer par le fait que le noyau de Kashima noie les appariements intéressants dans la somme(cf section 2).\nLa Fig. 2(b) montre les résultats de simulations similaires aux précédentes, mais cette fois ci avec un noyau K max (Eq. 5). On retrouve la même évolution avec des chaînes de longueur fixe (|h| \u003d k), mais les différences sont plus grandes. Par contre, lorsque des chaînes de longueur variables sont utilisées (|h| ? k), les résultats sont bien meilleurs. En effet la similarité entre les graphes avec ce noyau se résume à la similarité d\u0027un seul couple de chaînes appariées. On augmente les chances de trouver ce meilleur appariement si on le recherche dans un ensemble de chaînes de plusieurs longueurs et non limité à une seule longueur. image avec toutes les autres de la base. \nFIG. 2 -Performance (%) sur la base Columbia modifiée avec le noyau\nComparaison des deux noyaux\nSi on compare le meilleur résultat obtenu avec le noyau K max (|h| ? 2) et le meilleur résultat avec le noyau K Kashima (|h| \u003d 0) (Fig. 2(c)), on constate qu\u0027au début de l\u0027apprentissage (jusqu\u0027à une vingtaine d\u0027images annotées), le noyau K max conduit à des résultats lé-gèrement meilleurs que le noyau K Kashima . Ensuite les deux courbes sont très proches. Par contre le temps de recherche du meilleur appariement avec l\u0027algorithme de \"branch and bound\" est beaucoup plus rapide avec le noyau K max qu\u0027avec le noyau K Kashima (cf Fig. 2). Nous concluons que sur cette base où les objets d\u0027intérêt sont représentés par au plus 3 régions, le noyau K max calculé sur toutes les chaînes de longueur inférieure ou égale à 2 est préférable aux autres noyaux. \nCaltech\nNous utilisons la base Caltech avec la vérité terrain de la campagne d\u0027évaluation PASCAL 4 . Cette base contient 5775 images réparties en 5 catégories de 450 à 1370 images. Les graphes issus de la segmentation en régions floues comprennent entre 1 et 23 sommets et le nombre moyen de sommets est d\u0027environ 9 par image (cf. figure 3). Un exemple de session de recherche est donné sur la figure 4. Nous avons mené les expériences suivantes sur cette base :\n-Attributs de régions floues, avec le noyau de Kashima (Eq. 2) et des ensembles de chaînes H 0 . Nous n\u0027avons pas considéré de chaînes plus longues pour des raisons de complexité. -Attributs de régions floues, avec le noyau K max (Eq. 5) et différents ensembles de chaînes. -Attributs globaux sous la forme d\u0027un histogramme de couleurs et de textures. Les histogrammes sont calculés sur un dictionnaire adapté à la base suivant un processus de quantification vectorielle décrit dans Gosselin (2005). Un noyau Gaussien avec une distance du ? 2 est utilisé. -Attributs type points d\u0027intérêt avec régions MSER (cf. Matas et al. (2002)) décrites par des SIFT (cf. Lowe (2004) Les résultats de ces expériences sont présentées par catégories dans la table 2, et en fonction du nombre d\u0027annotations dans la figure 5. Tout d\u0027abord, si on s\u0027intéresse au noyau Kashima et max avec le meilleur paramétrage sur la base Columbia modifiée, on peut remarquer que nous avons ici un comportement inverse. Sur la base Caltech, le noyau de Kashima avec H 0 est plus performant quelle que soit la catégorie. Cela peut s\u0027expliquer par le fait que, sur cette base, le contexte joue un rôle important, contrairement à la base Columbia modifiée où il n\u0027existe pas de relation particulière entre un objet et son fond. En effet, étant donné que ce noyau somme toutes les similarités entre les différents sommets, et que l\u0027appariement entre les régions de fond, apporte une information pertinente. Par exemple, les voitures sont souvent en ville, dont les couleurs et les textures sont proches.\nPuis, si on s\u0027intéresse aux autres ensembles de chaînes pour le noyau max, nous pouvons remarquer qu\u0027il n\u0027y a pas de différence notable entre les différents ensembles. Cela peut s\u0027expliquer par le fait que, sur la base Caltech, il existe un sous-ensemble de chaînes communes aux chaînes Eulérienne et sans boucle ni cycles qui sont suffisantes pour pouvoir discriminer les images. Ce résultat est intéressant en terme de complexité étant donné que, plus l\u0027ensemble des chaînes considérées est petit, plus les calculs sont rapides.\nEnfin, si l\u0027on compare les résultats avec d\u0027autres attributs que les régions floues, nous pouvons constater que les régions floues sont particulièrement intéressantes étant donné qu\u0027elles permettent d\u0027obtenir les meilleurs résultats, sauf la catégorie \"background\". Néanmoins, dans ce cas a priori défavorable (la catégorie \"background\" correspond à une recherche de type globale), les régions floues arrivent toutefois à donner de très bon résultats. Ceci tend à montrer la capacité des régions floues à pouvoir s\u0027adapter plus facilement aux différents types de catégories, de la recherche globale à la recherche purement locale, en passant par les cas où le contexte peut jouer un rôle important. \nConclusion\nNous avons montré que l\u0027utilisation de noyau de graphe dans le cadre de la recherche ité-rative d\u0027objet était possible et efficace. D\u0027après nos expériences, il semble que la description d\u0027une image à l\u0027aide de régions soit plus efficace qu\u0027une description globale ou une description basée sur des points caractéristiques. En effet une primitive région, même si elle ne colle pas parfaitement à la sémantique de l\u0027image porte une information locale plus robuste aux variations.\nLes imprécisions de la segmentation et les différences d\u0027aspect d\u0027un objet d\u0027une image à l\u0027autre sont compensées par la mise en correspondance de graphes qui soit la plus générale possible. Les noyaux de graphes calculés à partir de noyaux sur des chaînes dans ces graphes permettent de trouver une solution optimale au problème de l\u0027appariement des graphes avec un algorithme de branch and bound. Cependant, l\u0027emploi des noyaux de Kashima, qui recherchent le couple de chaînes les plus semblables parmi tous les couples de chaînes possibles est incompatible avec l\u0027utilisation en temps réel. Il faut réduire la longueur des chaînes à au plus deux sommets (pour des graphes qui comportent 10 à 20 régions). A longueur de chaînes égale, il vaut mieux prendre un noyau K max , qui, associé à l\u0027algorithme de « branch and bound » trouve le meilleur appariement beaucoup plus rapidement que le noyau de Kashima. Sur les deux bases utilisées il s\u0027est avéré que l\u0027utilisation de petites chaînes suffisait à une bonne reconnaissance des objets. Et pour l\u0027instant nous n\u0027avons pas pu démontrer l\u0027intérêt d\u0027utiliser un ensemble particulier de chaînes.\nLe choix de la longueur maximale des chaînes reste le problème principal, il y a un compromis à faire entre temps de calcul et efficacité. Il semble que cette longueur soit liée d\u0027une part au nombre de régions formant l\u0027objet et d\u0027autre part à l\u0027importance du fond dans la reconnaissance de l\u0027objet. Dans le cas où le fond est important pour reconnaître un objet, un noyau basé sur les seules régions suffit. Par contre si l\u0027objet doit être trouvé quel que soit le contexte, la structure du graphe est importante. Nous avons dans cette première étude, fait intervenir que l\u0027adjacence entre les régions. L\u0027emploi d\u0027attributs plus précis de position relative, dans le noyau sur les arêtes, devraient améliorer considérablement la recherche, en la contraignant.\n"
  },
  {
    "id": "869",
    "text": "Introduction\nLa recherche d\u0027information dans les bibliothèques numériques est souvent une tâche ennuyeuse et fastidieuse. Les utilisateurs doivent répéter le processus d\u0027envoyer les requêtes, regarder les résultats et modifier les requêtes jusqu\u0027à ce qu\u0027ils trouvent les informations pertinentes. Une des raisons principales est que les requêtes des utilisateurs sont souvent courtes et donc ambiguës. Par exemple, la même requête «java» peut être formulée par une personne qui s\u0027intéresse au langage de programmation «java», et par une autre qui veut chercher des informations concernant une île en Indonésie. Cependant les moteurs de recherche renvoient le même résultat pour ces deux personnes. Même avec une plus longue requête comme «langage programmation java» ; nous ne savons pas quels types de document cet utilisateur veut chercher. Si c\u0027est un(e) programmeur(e), peut-être il/elle s\u0027intéresse aux documents techniques sur le langage Java, si c\u0027est un(e) enseignant(e), peut-être il/elle s\u0027intéresse aux tutoriels de Java pour ses cours.\nLe problème que nous avons mentionné peut être résolu en utilisant des techniques de personnalisation avec des profils utilisateurs. D\u0027une manière générale, nous pouvons définir un profil d\u0027utilisateur comme un ensemble structuré d\u0027informations qui décrit les intérêts et/ou les préférences de cet utilisateur.\nLe travail de Amato et Straccia (1999) est parmi les premiers travaux consacrés à définir un modèle de représentation de profil utilisateur dans les bibliothèques numériques, leur modèle est un modèle multidimensionnel dans lequel le profil utilisateur se compose de plusieurs catégories (ou dimensions) différentes : catégorie de données personnelles, catégorie de données de la source, catégorie de données de livraison, catégorie de données de comportement et catégorie de données de sécurité. Dans le système CiteSeer (Bollacker et al. (1999)), un profil hétérogène a été utilisé pour représenter des intérêts des utilisateurs. Lorsqu\u0027un nouvel article est disponible, CiteSeer va décider si cet article sera recommandé à l\u0027utilisateur ou non. CiteSeer utilise deux méthodes pour déterminer si l\u0027article est intéressant pour l\u0027utilisateur : i) jeu de contrainte (constraint matching) et ii) similarité de propriétés (feature relatedness).\nNotre travail se concentre sur la recherche d\u0027information personnalisée dans les bibliothèques numériques contenant des articles scientifiques. Cependant, tandis que la plupart des systèmes personnalisés utilisent des approches basées sur le contenu textuel pour construire les profils et représenter les documents de façon à calculer la similarité entre eux ; nous utilisons aussi des approches basées sur les citations des articles et des approches hybrides (contenu textuel et citations) pour ce but.\nMéthode des co-citations sur le Web\nDans cette section nous présentons la méthode des co-citations pour trouver la similarité entre les articles scientifiques, méthode proposée par Small (1973). Dans cette méthode, la similarité entre deux articles est basée sur leur nombre de co-citations : c\u0027est-à-dire le nombre de fois où ils sont cités ensemble par un autre article.\nCette méthode a été utilisée depuis longtemps. Cependant, elle a ses limites. Pour avoir les information de citation (nombre des co-citations), il faut avoir accès au graphe de citation de la collection actuelle ; ou il faut utiliser une base de données de citations 1 . Ces deux sources sont souvent limitées par la couverture soit de la bibliothèque numérique, soit de la base de données de citations par rapport aux publications qu\u0027elles ont collectées. Plusieurs travaux ont montré que si cette couverture n\u0027est pas suffisante, l\u0027efficacité de la méthode des co-citations sera diminuée (par exemple Couto et al. (2006)). C\u0027est pourquoi nous avons proposé une mé-thode (Van et Beigbeder (2007)) qui peut surmonter cette limite. Cette méthode est appelée la méthode des co-citations sur le Web. Dans cette méthode, nous calculons la similarité de co-citations entre deux articles scientifiques par le nombre de fois où ils sont «co-cités» sur le Web en utilisant le moteur de recherche Google.\nPour trouver la fréquence à laquelle un article est «cité» par Google, nous envoyons le titre de cet article (recherche d\u0027une expression exacte en utilisant des guillemets) à Google et notons le nombre de documents retournés. Similairement, pour trouver le nombre de fois où deux article sont «co-cités», nous envoyons les titres de ces deux articles à Google et notons le nombre de documents retournés. Dans la méthode des co-citations, la similarité entre deux Nos expérimentations sont des simulations de la recherche d\u0027information personnalisée en utilisant des profils utilisateurs. Dans ce cas, les topics représentent les besoins d\u0027information de personnes différentes. Pour chaque topic, nous sélectionnons manuellement quelques documents pertinents (5 en moyen) pour former un «pseudo profil utilisateur» de ce topic. Les articles qui sont inclus dans les profils seront exclus de la collection pour éviter l\u0027influence sur les résultats finaux. Chaque fois qu\u0027une requête est envoyée au moteur de recherche zettair 4 (le modèle par défaut utilisé dans zettair est le modèle Dirichlet-smoothed, Pehcevski et al. (2005)), les 300 premiers documents sont sélectionnés, puis on calcule la similarité entre chaque document dans cette liste avec le profil utilisateur. La similarité entre un document d et un profil utilisateur p est calculée par : \nEnfin, le score original calculé par zettair sera combiné avec la similarité document-profil pour donner le score final d\u0027un document. Les documents dans cette liste sont re-triés en utilisant ce nouveau score et puis présentés à l\u0027utilisateur.\nLes nouveaux travaux\nDans cette partie nous présentons nos nouveaux travaux. Dans les travaux antérieurs, pour chaque topic nous avons sélectionné manuellement quelques documents pertinents pour former un «profil utilisateur» de ce topic. Maintenant nous utilisons une autre méthode d\u0027éva-luation qui est basée sur le principe de la méthode de validation croisée à k blocs (k-fold cross-validation en anglais, Kohavi (1995)). Selon cette approche, pour chaque topic, nous partitionnons aléatoirement l\u0027ensemble des documents pertinents en k blocs (dans nos expéri-mentations, k \u003d 5). Les documents dans un bloc sont utilisés comme documents de test et les documents dans les autres k -1 blocs sont utilisés comme le «profil utilisateur» de ce topic. L\u0027expérimentation est répétée k fois, chaque fois avec un bloc différent contenant des documents de test. Avec cette approche, chaque document pertinent sera utilisé comme document de test 1 fois et dans le «profil» k -1 fois. Le résultat final sera une valeur moyenne de k résul-tats correspondant avec k blocs. La fiabilité des résultats est augmentée avec cette méthode de validation.\nDe plus, dans les expérimentations précédentes, la similarité document-profil est calculée avec plusieurs approches basées sur les citations (co-citations sur le Web, co-citations avec Web of Science, couplage bibliographique). Dans les nouvelles expérimentations, on utilise seulement l\u0027approche des co-citations sur le Web qui a donné la meilleure performance dans les expérimentations précédentes comme approche basée sur les citations. Par ailleurs, on ajoute l\u0027approche basée sur le contenu textuel et l\u0027approche hybride citation-texte. Dans l\u0027approche basée sur le contenu textuel, la similarité document-profil (cf. formule 2) est calculée par le modèle vectoriel (Pehcevski et al. (2005)) en utilisant le logiciel zettair. Le score final d\u0027un document sera une combinaison entre deux ou trois des scores suivantes : i) score original calculé par zettair (score_zettair) ii) similarité document-profil calculée par la méthode des co-citations sur le Web (sim_cocitations) et iii) similarité document-profil basée sur le contenu textuel (sim_texte). Les scores sont normalisés (divisés par le maximum des valeurs correspondantes) pour avoir des valeurs dans l\u0027intervalle de 0 à 1. Actuellement, nous considérons les combinaisons suivantes : i) score_zettair avec sim_cocitations ii) score_zettair avec sim_texte et iii) ces trois scores. Nous avons utilisé deux formule de combinaison : une formule linéaire et une formule produit. Dans la formule linéaire, nous avons essayé différents coefficients pour trouver la meilleure combinaison possible.\nRésultats et discussions\nPour évaluer la performance des différentes méthodes, nous utilisons la métrique précision à n document (avec n \u003d 5, 10,15,20,30). Le logiciel trec_eval 5 est utilisé pour l\u0027évaluation. La précision à n est la fraction des documents pertinents parmi les n premiers document. Puisque nous utilisons l\u0027approche de validation à k blocs, nous obtenons k valeurs de précision que nous moyennons :\nLes résultats sont montrés dans le tableau 1. La deuxième colonne représente le résultat original de zettair. La troisième colonne correspond à la méthode des co-citations sur le Web (score_zettair combiné avec sim_cocitations). La quatrième colonne représente le résultat avec la méthode basée seulement sur le contenu textuel des document (score_zettair combiné avec sim_texte). La cinquième colonne représente le résultat de la méthode hybride : score_zettair combiné avec sim_cocitations et sim_texte. Dans chaque méthode, p signifie d\u0027autres méthodes de citations avec les méthodes actuelles pour pouvoir gagner une meilleure performance. De plus, sachant qu\u0027il y a des points similaires entre citations des articles scientifiques et hyperliens des pages Web, nous avons l\u0027intention de faire des expérimentations similaires sur une collection des pages Web pour pouvoir comparer les performances de ces méthodes quand on les utilise dans l\u0027environnement Web.\n"
  },
  {
    "id": "870",
    "text": "Introduction\nDurant les dernières décennies, l\u0027utilisation de réseaux de capteurs a été largement déve-loppée pour mesurer et observer l\u0027évolution de systèmes complexes à forte dynamique. Les applications sont par exemple le trafic routier, le transport d\u0027énergie, les processus d\u0027entreprise et la météorologie. Dégager des liens de corrélations dans un tel réseau à travers le temps permet, par exemple, d\u0027établir des prévisions probabilistes à court ou moyen terme. Dans ce qui suit, on suppose que les capteurs, effectuant des mesures sur le trafic routier urbain, sont fixes et géoréférencés. Un graphe de connexion logique représente les échanges ou les causalités directes possibles entre ces différents lieux géographiques. Le graphe est supposé connu. A l\u0027aide d\u0027un outil d\u0027estimation efficace, on peut prédire le comportement usuel du trafic devant chaque capteur. Cependant, lorsque la circulation est atypique, au sens de l\u0027occurrence, la qualité des prévisions s\u0027en retrouve considérablement affectée. Nous proposons d\u0027identifier des motifs spatio-temporels de propagation de ces cas atypiques ayant pour objectif d\u0027aider à prévoir les conséquences d\u0027un évènement inhabituel sur l\u0027intégralité du réseau. Les motifs se réfèrent généralement à des structures répétitives sur le graphe sous-jacent dans l\u0027espace et le temps. Des motifs décrivant des changements dans l\u0027espace et le temps sont qualifiés de motifs spatio-temporels (spatiotemporal patterns). La notion de motifs spatio-temporels apparaît dans différents secteurs scientifiques tels que les géosciences (Knopoff et al. (2001)), la météorologie (Tourre et al. (1999), Imfeld (2000)) ou l\u0027écologie (Weigand et al. (1998)). Généralement, on utilise des techniques d\u0027analyse et de fouilles de données spatio-temporelles discrètes pour identifier ces motifs dans de grands ensembles (Tsoukatos et Gunopulos (2001), Bittner (2001)). Dans cet article, nous proposons dans un premier temps un outil permettant de détecter les comportements atypiques. Nous introduisons ensuite une méthode fondée sur la combinaison de l\u0027information mutuelle (Shannon et Weaver (1963)) et de l\u0027algorithme Isomap (Tenenbaum et Langford (2000)) calculant une première version des motifs de propagation que nous tentons d\u0027améliorer par la suite. Les tests expérimentaux sont effectués sur des données réelles de trafic routier intra-urbain. Ces données nous ont été fournie par l\u0027INRETS dans le cadre du projet CADDY (http: //norma.ecp.fr/wikimas/Caddy) de l\u0027ACI Masse de données 2003. Dans Joliveau et De Vuyst (2007), nous avons proposé une adaptation de la méthode SpaceTime Principal Component Analysis (STPCA) à un ensemble de données incomplètes calculant des estimations de séries temporelles définies pour chaque instant de mesure. L\u0027utilisation de cette méthode sur nos données de trafic intra-urbain nous a permis de dégager un ensemble complet de données de débit et de taux d\u0027occupation provenant d\u0027un réseau de capteur. Le débit moyen de véhicule (nombre de véhicules/instant) correspond à la quantité de véhicules étant passés dans la zone d\u0027activité du capteur lors du dernier intervalle de mesure. Le taux d\u0027occupation, exprimé en pourcentage symbolise quant à lui la densité de la circulation. Plus le taux d\u0027occupation est élevé, plus la circulation est dense. Une première difficulté est de combiner ces deux informations afin de posséder une variable ayant un sens pour l\u0027état du trafic. La loi fondamentale du trafic provenant de la théorie du transport indique la relation entre le flot et la densité des véhicules sur une route. A partir du diagramme représentatif de cette loi, nous proposons une nouvelle variable \nDétection de cas atypiques\nDéfinition d\u0027une variable d\u0027état de circulation continue\nEstimation du comportement moyen et détection de situations inhabituelles\nLa méthode STPCA définie dans  offre un moyen de résumer efficacement des séries temporelles tout en conservant la majeure partie de l\u0027information au  5  10  15  20  25  0  200  400  600  800  1000  1200  1400  1600  1800  2000   0  5  10  15  20   figure. sens de l\u0027énergie. L\u0027énergie est obtenue par la trace des matrices de corrélation définie par :  de l\u0027énergie). Pour détecter un comportement atypique, il suffit de comparer la valeur réelle à son estimée par STPCA. Si l\u0027écart entre ces deux valeurs est élevé, cela signifie que l\u0027activité actuelle au capteur n\u0027est pas représentative de la situation usuelle. On détecte alors un comportement atypique au sens de l\u0027occurrence. Notre raisonnement est illustré sur la figure 3. On remarque que, les deux courbes étant très ). Or, les mesures réelles relèvent une activité plus fluide que d\u0027habitude. Dans ce cas, le comportement atypique détecté correspond à une fluidité inhabituelle du trafic. Dans le cadre où on cherche à établir des prévisions sur le trafic à court ou moyen terme, l\u0027utilisation de la STPCA sur la variable ¡ est un outil très pratique. Si l\u0027écart entre la valeur mesurée par un capteur et son approximation est faible, on se réfère à la série estimée par l\u0027algorithme STPCA pour établir notre prévision. Dans le cas contraire, on détecte un comportement atypique. On s\u0027appuiera alors sur des motifs spatio-temporels de propagation pour prédire la circulation.\nRecherche de motifs spatio-temporels\nLes motifs intraday que nous cherchons à identifier ont pour but de nous aider à faire des prévisions sur le trafic. Ceux-ci se focalisent plus particulièrement sur l\u0027anticipation de la propagation d\u0027une situation atypique à travers le réseau. La propagation d\u0027une situation atypique sur un réseau est une notion de voisinage locale en espace et en temps indiquant sur quels points du réseau on observe un comportement atypique suite à la réalisation d\u0027un évènement inhabituel à l\u0027instant précédent. Deux outils principaux sont utilisés pour identifier les motifs spatio-temporels de propagation : l\u0027information mutuelle (Shannon et Weaver (1963)) et l\u0027algorithme Isomap (Tenenbaum et Langford (2000)).\nL\u0027information mutuelle\nL\u0027information mutuelle est tirée de la théorie des probabilités et de la théorie de l\u0027information. Cette quantité mesure la dépendance statistique entre deux variables. L\u0027information mutuelle mesurant la quantité d\u0027information apportée en moyenne par une réalisation d\u0027un évè-nement X sur les probabilités de réalisation d\u0027un évènement Y , et, en considérant qu\u0027une distribution de probabilité représente notre connaissance d\u0027un phénomène aléatoire, on peut mesurer l\u0027absence d\u0027information en utilisant l\u0027entropie de Shannon (Shannon et Weaver (1963)) de cette distribution. Ainsi, l\u0027information mutuelle est donnée par : \nIsomap\nIsomap est une méthode introduite par Tenenbaum et Langford (2000) dont l\u0027objectif est d\u0027identifier la structure cachée dans des observations multivariées de grandes dimensions. Le principe de cette méthode est de projeter les points d\u0027un ensemble de données dans un espace de plus faible dimension. L\u0027avantage d\u0027Isomap est que contrairement aux méthodes classiques telles que l\u0027analyse en composantes principales (ACP) ou l\u0027échelonnement multidimensionnel (multidimensional scaling -MDS), l\u0027algorithme est capable de découvrir des relations non linéraires régissant certaines observations complexes. Une fois le plongement (ou embedding) calculé, la distance entre deux points sur celui-ci est représentative de la similitude globale entre les évènements mesurés par ces points. Isomap se décompose en trois étapes définies de manière détaillée dans le tableau 1. Lors de la première étape, on cherche à déterminer pour chaque élément les points qui lui sont le plus proche en fonction d\u0027une distance \nCalcul des plus courts chemins\nInitialiser\n. La matrice des valeurs finales  \nConstruction des motifs spatio-temporels\nCalcul de motifs d\u0027origine\nAfin de construire une première version de motifs spatio-temporels de propagation de comportement atypique dans un réseau, nous appliquons l\u0027algorithme Isomap en utilisant l\u0027information mutuelle comme mesure de distance  et on relie ces deux couples capteur-instant par un arc. De cette manière, on obtient un graphe du même type que sur la figure 4 représentant les premiers motifs de propagation de cas atypiques. La notion de capteur le plus proche peut être définie par un rayon de distance minimale ? ou par l\u0027algorithme des K plus proches voisins. Dans nos expériences, nous avons combiné les deux approches en choisissant les K plus proches voisins dans un rayon de ? .\nAmélioration des motifs\nSuite à la combinaison d\u0027Isomap et de l\u0027information mutuelle, on dispose de premiers motifs spatio-temporels de propagation pouvant être représentés par une chaîne reliant les capteurs d\u0027un réseau entre les périodes de mesure. Nous proposons de pondérer les arcs de cette chaîne par la tendance de propagation du caractère atypique. Pour cela, on simule le déroulement du temps en détectant les cas atypiques par le procédé expliqué dans la section 2 et on calcule les probabilités  . La colonne \"voisins directs\" correspond aux résultats obtenus à l\u0027aide de la première version des motifs tandis que la colonne \"voisins indirect\" se réfère à la méthode avec les améliorations. On remarque que la solution basée sur l\u0027utilisation exclusive des voisins directs, déjà de bonne qualité, permet de prévoir jusqu\u0027à\nExpériences sur l\u0027ensemble des données\ndes situations atypiques avec un taux de faux positifs acceptable. Bien que notre but soit de prévoir la propagation de cas atypiques au sens de l\u0027occurrence, il nous faut distinguer les éléments atypiques reproductibles des éléments atypiques trop ponctuels. Le réglage du seuil\npermet de gérer ce compromis. De son coté, l\u0027ajout des voisins indirects augmente la précision\nVoisins directs\nVoisins Indirects ).\nVoisins directs Voisins Indirects\n). Il faut cependant le paramétrer de manière à ne pas trop augmenter le nombre de faux positifs engendrés. \nApprentissage sur un échantillon\n).\nLe but de notre travail étant de déterminer des motifs spatio-temporels afin de réaliser des prévisions en temps réel, nous aimerions estimer les facultés d\u0027apprentissage de notre méthode. Pour cela, nous sélectionnons aléatoirement un échantillon de nos données et nous procédons aux différentes étapes de notre méthode à partir de cet échantillon (détermination des modes principaux, calcul de l\u0027information mutuelle, détermination des premiers motifs avec Isomap, amélioration des motifs). Finalement, on teste les capacités de prédiction des motifs extraits de l\u0027échantillon sur l\u0027intégralité des données. Le tableau 4 illustre les résultats obtenus en utilisant un échantillon de V £ jours et en fixant les valeurs des paramètres à représente la topologie du réseau, les capteurs étant représentés par des carrés, les routes par des traits gris clairs et les cours d\u0027eau par des traits fins noirs. La partie droite illustre un morceau de la chaîne de motifs correspondant aux instants entre\n. L\u0027importance de la tendance de propagation est symbolisée par l\u0027épaisseur des flèches.\nPrévisions à moyen terme\nEn modifiant l\u0027échantillonnage temporel, on peut appliquer notre méthode sur des épisodes plus longs. Le tableau 5 illustre les résultats de la méthode à moyen terme (sur des épisodes de faux positifs similaire aux prévisions à court terme.\nConclusion\nDans ce papier, nous avons proposé une méthode de génération de motifs spatio-temporels de situations atypiques. Cette méthode s\u0027applique principalement à des données issues d\u0027un réseau de capteurs fixes géoréférencés. Nous utilisons l\u0027algorithme STPCA comme outil de prévision du comportement usuel de séries temporelles issues d\u0027un réseau de capteurs, nous permettant également de détecter les situations atypiques. Nous avons également introduit une méthode d\u0027identification de motifs spatio-temporels de propagation de situations atypiques fondée sur la combinaison d\u0027Isomap et de l\u0027information mutuelle. Ces motifs ont pour fonction\n"
  },
  {
    "id": "872",
    "text": "Introduction\nLe problème de la classification de données est identifié comme une des problématiques majeures en extraction des connaissances à partir de données. Depuis des décennies, de nombreux sous-problèmes ont été identifiés, comme par exemple la sélection des données ou des variables, la variété des espaces de représentation (numérique, symbolique, etc), l\u0027incrémen-talité, la nécessité de découvrir des concepts, ou d\u0027obtenir une hiérarchie, etc. La popularité, la complexité et toutes ces variantes du problème de la classification de données, (Jain et al. (1999)), ont donné naissance à une multitude de méthodes de résolution. Ces méthodes peuvent faire appel à des principes heuristiques ou encore mathématiques.\nLes méthodes qui nous intéressent dans ce travail, sont celles qui permettent de faire de la classification non supervisée de données en utilisant les cartes topologiques (appelées aussi SOM :Self-organizing Map). Celles-ci sont souvent utilisées parce qu\u0027elles sont considérées à la fois comme outils de visualisation et de partitionnement non supervisé de différents types de données (quantitatives et qualitatives). Elles permettent de projeter les données sur des espaces discrets qui sont généralement en deux dimensions. Le modèle de base, proposé par Kohonen (Kohonen (2001)), est uniquement dédié aux données numériques. Des extensions et des reformulations du modèle de Kohonen ont été proposées dans la littérature, (Bishop et al. (1998); Pour l\u0027apprentissage des cartes topologiques, les critères de qualité sont plus difficiles à dé-finir ; ils s\u0027articulent autour de l\u0027interprétation des regroupements ou des partitions obtenues. Par conséquent un premier problème se pose : celui de la segmentation (partitionnement) de la carte. On retrouve dans la littérature plusieurs méthodes ou propositions de segmentation de la carte qui utilisent des critères de similarité standard qui ne tiennent pas compte du voisinage introduit par la carte, (Vesanto et Alhoniemi (2000)). Elles se résument, souvent, à l\u0027utilisation d\u0027un algorithme de regroupement (classification hiérarchique ou les K-moyennes) combiné à un indice de qualité pour déterminer la partition idéale. Le second problème qui nous intéresse dans cet article est celui du choix de l\u0027algorithme de segmentation de la carte. Ainsi, nous avons introduit une nouvelle classification hiérarchique que l\u0027on va appliquer sur les référents (représentants) de la carte. Cette nouvelle méthode nommée AntTree introduite par (Azzag et al.) s\u0027inspire du comportement d\u0027auto-assemblage observé chez une population de fourmis réelles et leurs capacités à s\u0027accrocher entre elles pour construire des structures vivantes.\nLa suite de notre article est organisée comme suit : dans la section 2, nous présentons les principes généraux des cartes SOM avec la nouvelle mesure proposée, ainsi que la nouvelle méthode de classification hiérarchique utilisée pour la segmentation de la carte topologique. La section 3, quant à elle, est consacrée aux résultats et à l\u0027étude comparative réalisée sur des bases de données numériques. La dernière section rassemble les conclusions faites au cours des expérimentations et présente des perspectives.\nSegmentation topologique et hiérarchique\nLes cartes auto-organisatrices présentées par Kohonen ont été utilisées pour la classification et la visualisation des bases de données multidimensionnelles. Une grande variété d\u0027algorithmes des cartes topologiques est dérivée du premier modèle original proposé par Kohonen. Ces modèles sont différents les uns des autres, mais partagent la même idée de présenter les données de grande dimension en une simple relation géométrique sur une topologie réduite.\nDans cette section, nous décrivons la version originale des cartes auto-organisatrices. Ce modèle consiste en la recherche d\u0027une classification non supervisée d\u0027une base d\u0027apprentissage\nCe modèle classique se présente sous forme d\u0027une carte possédant un ordre topologique de N c cellules. Les cellules sont réparties aux noeuds d\u0027un maillage. La prise en compte dans la carte C de la notion de proximité impose de définir une relation de voisinage topologique. Ainsi, la topologie de la carte est définie à l\u0027aide d\u0027un graphe non orienté et la distance ?(c, r) entre deux cellules c et r étant la longueur du chemin le plus court qui sépare les deux cellules c et r. \nOù ? affecte chaque observation z à une cellule unique de la carte C. Dans cette expression ||z ? w r || 2 représente le carré de la distance euclidienne.\nA la fin de l\u0027apprentissage, la carte auto-organisatrice détermine une partition des données en p sous ensembles. Cette partition et les sous-ensembles associés seront notés par P \u003d {P 1 , . . . , P c , . . . , P p }. A chaque sous ensemble P c on associe un vecteur référent w c ? R d qui sera le représentant ou la \"moyenne\" de l\u0027ensemble des observations de P c .\nSouvent l\u0027utilisation des cartes topologiques est suivie par une segmentation des cartes ou un regroupement des référents. Cette tâche, est réalisé à l\u0027aide d\u0027algorithmes de partitionnement tel que K-moyennes, ou la classification ascendante hiérarchique CAH (Jain et Dubes (1988)). Le choix des deux sous-ensembles qui vont être regroupés est réalisé à l\u0027aide d\u0027une mesure de similitude définie entre deux sous-ensembles. Différents critères de similitude sont proposés dans la littérature, (Jain et Dubes (1988); Ambroise et al. (1998)). Souvent ces critères ne tiennent pas compte de la topologie ou de l\u0027auto-organisation des référents obtenue avec la carte topologique. La mesure de similitude la plus connue est celle du critère de Ward définie comme suit :\ntel que n c et n r indiquent le nombre d\u0027observations affectées pour le sous-ensemble P c et P r .\nEn considère deux partitions : P t?1 comme la partition avant regroupement des deux sousensembles P c et P r associés aux deux référents c et r, et P t la partition obtenue en regroupant les sous ensembles P c et P r . On peut montrer que la différence entre l\u0027inertie des deux partitions est égale au critère de regroupement de Ward (2). Ainsi à chaque étape de la classification on calcule une matrice de similarité associée à la nouvelle partition. Par conséquent, à chaque étape de l\u0027algorithme, on choisit une nouvelle partition qui limite l\u0027augmentation de l\u0027inertie intra-classe. Notons que cette propriété ne garantit pas l\u0027optimisation globale du critère. L\u0027algorithme peut être décrit en 5 étapes :\n1. Initialiser la matrice de similarité avec la partition obtenue avec la carte.\n2. Trouver les deux sous ensembles les plus proches selon le critère de Ward.\n3. Regrouper les deux sous ensembles P c et P r en un seul sous ensemble.\n4. Mettre à jour la matrice de similarité de la nouvelle partition.\nRépéter 2\nNous rappelons que la classification hiérarchique ou tout autre méthode de segmentation des cartes topologiques sont couplées à un indice de qualité qui permet de choisir la taille de la partition optimale. Afin d\u0027optimiser l\u0027algorithme de segmentation de la carte, nous proposons dans ce papier deux modifications. La première consiste à utiliser un algorithme de classification hiérarchique qui supprime l\u0027étape 4 et fournit automatiquement la taille de la partition \"idéale\". Ceci se résume à utiliser un algorithme de classification hiérarchique qui utilise une seule et unique matrice de similarité, qui est celle de la partition à l\u0027instant t \u003d 0 (P 0 ). Cet algorithme sera détaillé par la suite dans la section 2.1. Notre deuxième proposition consiste à modifier la mesure de similarité de regroupement, afin de prendre en compte le voisinage de la topologie fournie par la carte.\nLe critère de Ward mesure la perte d\u0027inertie après chaque fusion de deux sous ensembles, il est donc nécessaire de considérer la modification de la topologie de la carte après fusion en pondérant l\u0027indice de Ward par un paramètre quantifiant ce changement. Nous proposons de quantifier le changement topologique par une pondération du critère de Ward avec une mesure qui prend en compte la nouvelle structure de la carte, cette dernière est définie comme suit :\nCette pondération permet donc de quantifier ce changement topologique de la carte. Afin de de prendre en compte la proximité des référents sur la carte, nous proposons de soustraire une quantité à cette mesure de façon à diminuer la perte d\u0027inertie mesurée par le critère de Ward, selon la proximité des sous-ensembles sur la carte topologique. Finalement, la nouvelle mesure devient :\nCette mesure heuristique est constituée de deux termes. Le premier terme correspond à la perte d\u0027inertie des observations après fusion des deux sous ensembles P c et P r . le deuxième terme permet de rapprocher les sous-ensembles correspondants à deux référents voisins sur la carte, afin de conserver l\u0027ordre topologique entre les différentes partitions. En effet, si c et r sont voisins sur la carte, la valeur de ?(c, r) sera alors basse et dans ce cas celle de K(?(c, r)) sera élevé ; le second terme a donc comme effet de réduire davantage le premier terme. Il est évident que pour un voisinage nul, notre mesure se réduit à calculer le critère de Ward. Il est donc possible de dire que notre mesure permet d\u0027obtenir une solution régularisée du critère de Ward : la régularisation étant obtenue grâce à la contrainte d\u0027ordre topologique introduit dans notre proposition de mesure.\nFinalement l\u0027utilisation de notre mesure permet de définir une matrice de similarité qui tient compte à la fois de la perte d\u0027inertie et de la topologie de la carte. Pour traiter cette matrice nous allons présenter dans la section suivante l\u0027algorithme de classification hiérarchique basé sur les fourmis artificielles.\nClassification hiérarchique\nPour segmenter la carte nous avons utilisé une approche biomimétique inspirée du comportement d\u0027auto-assemblage observé chez les fourmis réelles. Ces dernières construisent des structures vivantes en se connectant progressivement entre elles, (Anderson et al. (2002)).\nLe modèle développé utilise des règles comportementales afin de construire des heuristiques pour la classification non supervisée hiérarchique. Dans notre modèle chaque fourmi artificielle représente une donnée z à classer. Ces fourmis artificielles vont construire de manière similaire un arbre en appliquant certaines règles où les déplacements et les assemblages des données sur cet arbre dépendent de leurs similarités.\nLe principe d\u0027AntTree est le suivant : chaque donnée (fourmi) à classer z i , i ? [1, N ] (N est le nombre de données initiales) représente un noeud de l\u0027arbre à assembler. \nFIG. 1 -Construction de l\u0027arbre par des fourmis : les fourmis qui sont en déplacement sont représen-tées en gris et les fourmis connectées en blanc.\nInitialement toutes les fourmis artificielles sont positionnées sur un support noté f 0 (voir figure 1). A chaque itération, une donnée z i est choisie dans la liste des données triée au départ. On notera par la suite par f i chaque fourmi représentant une donnée z i à classer dans l\u0027arbre. Cette fourmi va chercher alors à se connecter sur sa position courante, sur le support (f 0 ) ou sur une autre fourmi (donnée) déjà connectée (noté f pos ). Cette opération ne peut aboutir que dans le cas où elle est suffisamment dissimilaire à f + (la fourmi connectée au support f 0 ou à f pos et dont la donnée est la plus similaire à z i ). Dans le cas contraire, la fourmi f i associé à la donnée z i se déplacera de manière déterministe dans l\u0027arbre suivant le chemin le plus similaire indiqué par f + . Le seuil permettant de prendre ces décisions ne va dépendre que du voisinage local. Pour étiqueter les données nous allons ensuite considérer que chaque sous arbre va représenter une classe trouvée. Dans (Azzag et al.) l\u0027auteur détaille de manière plus complète les règles comportementales définissant les différents algorithmes de cette approche.\nNotons qu\u0027AntTree a l\u0027avantage d\u0027avoir une complexité proche du n log(n). Une étude dé-taillée a été réalisé dans (Azzag et al.), elle confirme que par rapport à d\u0027autres algorithmes en n 2 les temps nécessaires par AntTree peuvent être jusqu\u0027à 1000 fois inférieur à ceux de la CAH sur de grandes bases de données et ceci pour une qualité égale.\nCes temps vont encore être réduits puisque AntTree s\u0027applique sur l\u0027ensemble des référents fournit par la carte topologique, W \u003d {w 1 , ..., w p }. Ceci réduit considérablement la complexité de la segmentation de la carte. Ainsi la structure d\u0027arbre recherchée est celle qui représente le mieux l\u0027ensemble des référent W (avec la nouvelle mesure de similarité (3)). Chaque noeud parent de l\u0027arbre est plus représentatif du noeud fils. Ainsi l\u0027algorithme, AntTree-SOM-Neigh-W, résumant les étapes élémentaires pour la segmentation de la carte topologique (SOM) peut être présenté comme suit :\n-Entrée : W \u003d {w 1 , ..., w Nc }, l\u0027ensemble des référents constituant la carte topologique à la fin de l\u0027apprentissage.\n-Calcul de la matrice de similarité en utilisant la formule (3) -Construction de l\u0027arbre avec l\u0027algorithme AntTree. -Sortie : structure des référents sous forme d\u0027arbre.\nL\u0027arbre fournit une partition de la carte topologique P \u003d {P 1 , ..., P s } où la valeur de l\u0027indice s représente le nombre de sous arbres connectés au support fournit par AntTree. Ainsi, dans le même processus nous proposons de segmenter la carte et de fournir le nombre de sous ensembles constituant la partition sans aucun indice de qualité.\nValidation\nAfin de pouvoir évaluer la qualité de la classification obtenue, nous avons utilisé des bases de données comportant un nombre variable d\u0027observations (Blake et Merz (1998)). Nous avons également testé notre approche sur des données artificielles (Azzag et al.), engendrées par des lois gaussiennes avec des difficultés diverses (recouvrement des classes, variables non pertinentes, etc.) ainsi que des données réelles. Le tableau 1 présente pour chaque base, le nombre de classes réelles (Cl R ), la dimension de l\u0027espace des données (d) et le nombre de données total (N ). La figure 2 montre quelques exemples avec des difficultés variables utilisés pour tester notre modèle.\nNous avons comparé notre modèle avec la segmentation de la carte utilisant l\u0027algorithme AntTree combiné à l\u0027indice de Ward (formule2), sans aucune information du voisinage (modèle appelé SOM-AntTree-W). Dans ces expérimentations, la comparaison des différents résultats est mesurée à l\u0027aide de trois critères externes. On peut utiliser ces indices lorsque la segmentation souhaitée est connue, en particulier sur nos jeux de données. Il s\u0027agit de la comparaison entre la segmentation proposée et une segmentation souhaitée. Ainsi nous avons utilisé, le taux de bonne classification (appelé aussi pureté) en utilisant l\u0027étiquette connue de chaque donnée ; l\u0027indice de Rand, qui calcule le pourcentage du nombre de couples d\u0027observations ayant la même classe et se retrouvant dans le même sous ensemble après segmentation de la carte, et le troisième indice est celui de Jaccard qui est similaire à l\u0027indice Rand sans prendre en considération les couples d\u0027observations correctement classées dans des sous ensembles différents, (Saporta (2006)).\nAtom (2) TwoDiamonds (2) Lsun (3) Demicercle (2) \nFIG. 2 -Quelques exemples de jeux de données (Atom (2), TwoDiamon (2), Lsun (3), demi-cercle (2)).\nLe numéro qui suit le nom de la base indique le nombre de classe  Le tableau 2 indique les performances atteintes avec notre modèle AntTree-SOM-Neigh-W en comparaison avec le modèle utilisant simplement le critère de Ward. La figure 2 montre la variation des trois indices utilisés pour comparer ces deux segmentations de la carte.\nSur les graphiques de la figure 2, nous pouvons observer que les courbes obtenues par notre modèle sont de plus grandes amplitudes sur l\u0027indice de Rand ainsi que l\u0027indice de Jaccard. Ceci confirme l\u0027avantage de considérer le voisinage dans AntTree-SOM-Neigh-W. Cependant notre modèle obtient de moins bon résultats sur la base Anneaux (le pic représenté sur les deux premières courbes). Ceci s\u0027explique par le fait que notre modèle retrouve beaucoup plus de classes sur cette base. En effet AntTree-SOM-Neigh-W utilise l\u0027information du voisinage et cette dernière lui permet de détecter des sous-ensembles de manière plus précise que le modèle classique.\nNous observons également sur le tableau 2 un résultat global qui indique que les puretés sont améliorées à chaque fois que l\u0027on introduit le voisinage dans la segmentation de la carte. Par exemple, avec la base pima de 67% à 72.4%, avec le même nombre de classes trouvées. Avec la base Hepta on obtient des résultats identiques de l\u0027ordre de 43.4%. Pour la base Anneaux on constate une baisse de pureté, on passe de 97.8% à 81.5%.\nFinalement, on peut constater, globalement, une claire amélioration de la pureté lorsqu\u0027on utilise la nouvelle mesure de regroupement proposée (SOM-AntTree-Neigh-W). La prise en compte de la topologie de la carte améliore nettement les résultats de la segmentation. Nous rappelons ici, qu\u0027il existe d\u0027autres algorithmes de segmentation de la carte n\u0027utilisant que la\n85.87 (5) 99.9 (7) Anneaux (2) 97.8 (6) 81.5 (5) Demi-cercle (2) 58.833 (2) 72.67 (4) Engytime (2) 74.14 (5) 88.04 (7) Glass (7) 38.32 (5) 59.81 (6) GolfBall (1) 100 (3) 100 (4) Hepta (7) 43.4 (4) 43.4 (4) Lsun (3) 55 (3) 93 (5) Pima (2) 67 (5) 72.4 (5) Target (6) 83.25 (5) 94.42 (6) Tetra (4) 62.5 (3) 81.75 (5) Twodiamonds (2) 100 (4) 100 (5) WingNut (2) 95.67 (3) 87.11 (5) ART1 (4) 50.5 (4) 84.75 (4) ART2 (2) 94.9 (4) 97.7 (4) ART4 (2) 100 (3) 100 (5)   Ambroise et al. (1998)). Avec notre modèle couplé à la nouvelle mesure 3 aucun indice n\u0027est nécessaire pour obtenir la partition optimale. La figure 4 montre sur deux jeux de données le résultats de la segmentation de la carte. La figure à gauche indique comment la carte apprend la topologie du nuage suivi à droite de la structure de l\u0027arbre fourni par l\u0027algorithme de classification hiérarchique AntTree. Chaque classe est représentée par un sous arbre connecté au support. Aussi, chaque noeud de la carte et de l\u0027arbre représente un référent qui est associé à un sous-ensemble de données.\nConclusions et perspectives\nDans ce travail nous avons développé une nouvelle mesure de similarité pour la segmentation des cartes auto-organisatrices afin de prendre en compte le voisinage entre les référents d\u0027une carte. Nous avons également introduit une nouvelle approche de segmentation utilisant un algorithme de classification hiérarchique basé sur le principe des fourmis artificielles. En effet, ce dernier a l\u0027avantage d\u0027être très rapide tout en fournissant la taille de la partition en une seule étape. Par conséquent, le temps total de la segmentation de la carte est amélioré en comparaison aux autres méthodes qui parcourent l\u0027ensemble des partitions possibles en choisissant une avec un l\u0027indice de qualité optimal, (Vesanto et Alhoniemi (2000)). Lors de la comparaison avec la distance euclidienne simple nous avons pu remarquer que notre nouvelle approche apporte des résultats compétitifs sur plusieurs bases de données.\nNous avons pu constater, qu\u0027il existe des bases pour lesquelles notre méthode n\u0027est pas efficace. Pour améliorer ces résultats, des perspectives peuvent être déduites. La première consiste à améliorer la mesure de similarité qui constitue un critère important dans la segmentation de la carte. En effet dans cette mesure nous devons tenir compte plus du voisinage et de l\u0027inertie intra-classe, (Yacoub et al. (2001)). Par conséquent, il serait intéressant de vérifier si on retrouve les deux termes de notre mesure en calculant la perte d\u0027inertie de la carte topologique. La seconde perspective concerne la méthode de segmentation qui a été utilisée, Il serait intéressant de penser à améliorer l\u0027algorithme AntTree en l\u0027hybridant avec une heuristique qui supprime les sous ensembles (classes) de petit effectif. \n"
  },
  {
    "id": "873",
    "text": "Introduction\nData stream management systems (DSMS) have emerged to meet the needs of processing continuous changing, unbounded data and real-time responses. The applications include stock quoting, auction processing, network flow monitoring, moving objects monitoring [Abdessalem et al. (2007), Moreira et al. (2000)], etc. In these cases, the common features consist in : 1-the data sources are infinite and real-time changing, 2-queries over data have to produce continuous responses. To cope with the first feature, the window concept is proposed. The idea consists in transforming unbounded data stream into bounded data tables, then queries can be processed as in a traditional database system. For the second feature, query evaluation methods should be executed continuously resulting in a real-time changing of the response. As we mentioned above, window techniques are proposed for solving two issues in data stream processing : infinite data sources and continuous query. In current DSMS, the windowing operation is done using the timestamps of the input data (i.e. temporal attributes). For example, in a network traffic monitoring application it is not possible to store and analyze online the whole input data. We can just continuously monitor the situation for a bounded time interval, for instance the latest 1 hour. In this case, we have an infinite stream of traffic data and a temporal window of 1 hour. The set of data belonging to the specified window is finite. Queries are then evaluated periodically (for instance, every 2 minutes) on the set of data belonging to the window. So, the result of the queries will change continuously (every 2 minutes). DSMS also propose windowed operators such as aggregate operators, join operator, select operator, etc. to enable answering complex queries. In addition to initial applications of data streams, the development of location-sensitive devices has also spurred the on-line location-aware services and real-time moving objects monitoring applications. Besides the common characteristics of DSMS, these applications further need to process spatial and spatio-temporal data for location services. In this case, the input data is supposed to be composed of at least two attributes : a spatial attribute and a timestamp. In the previous literatures, two main approaches are considered for continuous queries processing over spatio-temporal data streams. In the first approach [Patroumpas et Sellis (2004)], the windowing operation is done using only the timestamp attribute. Then, spatial restrictions are evaluated on each set of data composing the obtained windows \"temporal windows\". The problem with this approach is that the fixed order of operations(temporal/spatial restrictions) in a query evaluation plan can have a important impact on the performance of the system [Elmongui et al. (2006)]. In other terms, it may be inefficient in some cases when the temporal restriction is always done before the spatial one. In addition, many location services don\u0027t involve temporal attribute at all and only care about spatial information, e.g. Range or kNN query ] etc. In the second approach, as proposed in the PLACE project ], the windowing operation is done using the spatial attribute. Then, temporal restrictions are evaluated on each set of data composing the obtained Spatial Windows.\nTo point out the necessity of spatial windows for spatio-temporal applications, we take the following example. Given the situation of a vehicle moving on the road. The vehicle sends the updates of its position periodically to the DSMS, and we would like to know its average speed in the last 50 kms. Temporal windowing is not efficient to answer this kind of query, because the size of the temporal window can\u0027t be decided in advance. The time spent by the observed object to cross the 50 kms may change significantly along the trajectory of the object.\nA spatial windowing is more suitable to answer this kind of query. We may fall back on the spatial window (specified by the criteria \"in the last 50 kms\") to catch a finite data relation from the unbounded data stream. Only the semantics of the windowing operation changes because we use the spatial dimension instead of the temporal one. When processing the incoming streams, spatial window maintains its contents according to the spatial attribute of the input data (tuples). Only data within a certain area scope are of interest and are kept in the window.\nIn this paper, we focus on the extension of DSMS to the management of spatio-temporal data. We propose two kinds of spatial windows : a static spatial window and a moving spatial window. These operations are presented as a possible extension to the query language CQL (Continuous Query Language) [Arasu et al. (2006); Arasu et Widom (2004)]. Their semantics are defined based on the abstract semantics of CQL, and their expressiveness is demonstrated using examples of queries. This paper is organized as follows. Section 2 presents the CQL query language. Section 3 describes our extension of CQL to support spatial windowing, and section 4 concludes the paper.\nPreliminaries\nIn this section, we present the data model and the basic operations proposed in CQL. This query language was defined in the STREAM project [Arasu et al. (2006); Arasu et Widom (2004)]. Its syntax is close to the SQL-99 syntax and it is especially designed for continuous data processing over data streams. CQL assumes that the input tuples come in a time ordered sequence and the windowing operation is processed on the timestamp of each tuple. First, we present the basic functions and data domains, then we give the definitions of stream and relation and their related operators.\nT is a discrete, ordered time domain. A time instant is any value from T. Time attributes belong to T.\nA tuple is a finite sequence of atomic values.\nThis is the domain of finite, but unbounded, bags of tuples.\nThis is the domain of multi-sets over TP × T (see Definition 2.1) -R : Relation domain ; R : T ? ?, this is the domain of functions that map time instants to bags of tuples (see Definition 2.2).\nThis the domain of functions that produce a bag of tuples from one or more bags of tuples. For example, the relational algebra operators (e.g. ?, ?, etc.).\nThis is the domain of functions that converse a stream to bags of tuples. For example, the CQL operators RANGE and SLIDE.\nThis is the domain of functions that converse finite bags of tuples to a stream. Such functions in CQL are IStream, DSream and RStream.\nDefinition 2.1 :\nA stream S is a (possibly infinite) bag (multi-sets) of elements t where s is a tuple belonging to the schema of S and t ? T is the timestamp of the element.\nDefinition 2.2 :\nA relation R is a mapping from each time instant t ? T to a finite but unbounded bag of tuples, denoted R(t), belonging to the schema of R.\nCQL queries are composed from operators belonging to the three classes : Relation-toRelation operators (from R2ROp domain), Stream-to-Relation operators (from S2ROp domain), and Relation-to-Stream operators (from R2SOp domain).\nLet us rebuild the example given in the Introduction, and consider that we look for the average speed of a car in the past 60 minutes every 10 minutes. In this situation, sliding temporal window can be used to extract tuples of latest 60 minutes with the sliding value of 10 minutes. By using CQL, query sentence can be expressed as follows :\nminutes\u0027, SLIDE \u002710 minutes\u0027] WHERE R.ID\u003dmycar\nThis query is constructed from three classes of operators : RANGE \u002760 minutes\u0027 and SLIDE \u002710 minutes\u0027 respectively design the window size and moving step of the spatial window. They belong to the S 2ROp domain. A relational restriction operator restricts tuples to have the ID value equal to \u0027mycar\u0027 after the conversion of stream to relation. An aggregate operator \u0027AvgSpd()\u0027 calculates the average speed of \u0027mycar\u0027 in the last 60 minutes by using tuples meeting the previous restriction. These two operators belong to the R2ROp domain. Finally, an RStream operator converts the content of result relation(object ID and average speed after previous calculation) to stream and transmits it to users. This operator belongs to the R2SOp domain. The three classes of operators in this example can be expressed by the semantics of CQL.\nThe semantics of CQL is specified using a meaning function M [Arasu et Widom (2004)]. The meaning function takes any query Q belonging to CQL and returns an \"input-output\" function M s, t) after computation by Q. M s, t) takes the streams and relations referenced in Q and a time instant t (e.g. now) as the input. Then it specifies the output produced by Q at time instant t. Let us consisder the example of query Q 1 given above. In the first stage, a Stream-to-Relation conversion is done on the data stream S tream_car. The meaning function of this operation is the following :\n}, where t high \u003d minutes × 10 minutes, and t low \u003d max{t high ? 60 minutes, 0}\nThe expression t high \u003d minutes minutes computes the largest time instant multiple of 10 minutes and smaller then t. Intuitively, the Stream-to-Relation conversion defines its output tuples each 10 minutes, only the tuples within the last 60 minutes are contained in the output. Based on the obtained output relation, the evaluation of the where operation is done according the the following meaning function :\nThe meaning function of the operator AvgSpd(R) is :\nIn this case, distance represents the path that the monitored car has passed in the last 60 minutes. It is calculated using the position information that must contain each tuples in the window.\nFinally, operator RStream of R2SOp domain converts its input relation to a stream. Its meaning function is :\n3 Adding a spatial window operation to CQL\nIn this section, we propose an extension to CQL in order to support spatial windowing. We consider two categories of spatial windows : stationary windows and moving windows. The spatial coordinates of a stationary window do not change over time. However, the spatial coordinates of a moving window can change over time. To illustrate this, let\u0027s consider the following two spatial queries :\n• Stationary Spatial Windows Suppose that we are interested in monitoring the trajectories of fishing ships in the east China sea. In this case, the interest area could be regarded as a stationary spatial window and the trajectory of each fishing ship consists in its continuous positions in that area.\n• Moving Spatial Windows\nRecall the example of section 1 and expand it :\"Query the average speed of a vehicle in the last 50 kms every time after it moves 10 kms\". The spatial window here has the window size of 50 kms and the sliding step of 10 kms, which indicates that the spatial window is moving.\nBased on this informal description of spatial windows, we extend in 3.1 the CQL data model and its basic operations presented in section 2. Then, we define in 3.2 the syntax and the semantics of the spatial window operation.\nExtending the Data model\nTo be able to deal with spatial data, we add the following domains to CQL data model.\n-G : Space domain ; This is the domain of spatial values. The spatial attributes of spatio-temporal data stream belong to this domain.\nthis is the domain of functions that map spatial restriction to bags of tuples (see Definition 3.2). R t : T ? ?, this is the domain of functions that map temporal restriction to bags of tuples (this domain is denoted R in the CQL data model, see section 2). -S : Stream domain ;\nThis is the domain of multi-sets over TP ×G × T (see Definition 3.1)\nThis is the domain of windowing functions that converse a spatio-temporal stream to bags of tuples. This will correspond to the new windowing operators RANGE BY ...\nThis is the domain of functions that converse finite bags of tuples to a spatio-temporal stream, by adding a spatial stamp and a temporal stamp to each tuple. This will correspond to the new CQL functions IStream, RStream and DStream.\nA Spatio-temporal data stream consists of a stream of tuples, each one is stamped with a temporal attribute and a spatial attribute (i.e. each tuple have two stamps). Temporal windowing operators are executed on the basis of the temporal stamps, and spatial windowing operators are executed according to the spatial stamps. We define the stream and relation model in the case of spatio-temporal data as follows.\nDefinition 3.1 :\nA stream S in the spatio-temporal case is a (possible infinite) bag of elements g, t where s is a tuple belonging to the schema of S , g ? G is a spatial stamp, corresponding for example to the spatial coordinates of a moving object, and t ? T is the temporal stamp of the element.\nDefinition 3.2 :\nA relation R(g,t) is a mapping from each location g ? G and each time instant t ? T to a finite but unbounded bag of tuples, belonging to the schema of R. The content of a relation R changes over space and time.\nIn a data stream S, we denote by g the spatial stamp, and we denote by t the temporal stamp, of each tuple. These stamps are necessary for spatial and temporal windowing operations. Updates from different sources (for instance, moving objects) can flow in separate streams or may be incorporated into the same stream. When employing a spatial windowing operation, tuples will be filtered into bags of tuples according to their spatial stamps. When employing a temporal windowing operation, they will be filtered according to their temporal stamps. A continuous query on a spatio-temporal stream may be composed of only spatial or temporal windowing operations. It can also be composed at the same time of both temporal and spatial windowing operations.\nSemantics and Syntax of Spatial Window\nSince a data stream is infinite and the memory size is limited, the windowing approach is fundamental for the processing of continuous queries in DSMS [Patroumpas et Sellis (2006)]. In CQL syntax [Arasu et al. (2006); Arasu et Widom (2004)], the window operation is denoted by the keywords RANGE, ROW and SLIDE. In TelegraphCQ [Chandrasekaran et al. (2003)], the window operation is denoted by the expression RANGE BY ... SLIDE BY and in ; Maier et al. (2005)] the same CQL keywords RANGE, SLIDE are used to denote the window operation. In this paper, we use a syntax similar to TelegraphCQ to illustrate our spatial window operations.\nThe syntax we consider for the windowing operation is as follows :\nWhere v 1 denotes the window size (range attribute) and v 2 denotes a step between two successive windows (slide attribute). The range and the slide attributes may be temporal or spatial values. This is indicated by the keywords RATTR SPACE and RATTR TIME for the range attribute, and by the keywords SATTR SPACE and SATTR TIME for the slide attribute. The range and the slide attribute may belong to the same domain (T or G) or not. In the following, we only consider the case where these two attributes are spatial values (v 1 ? G and v 2 ? G).\nStationary Spatial Window\nLet us take the example of the stationary spatial window given above at the beginning of Section 3. Since we only care about the trajectories of fishing ships in a certain region (east China sea), we can use a spatial window corresponding to the east China sea in order to only catch from the stream the tuples of interest. The trajectories consist of sets of position information. For a given object o 1 , the following query gives its trajectory in the east China sea.\nIn this case, Moving_Objects_Stream denotes a spatio-temporal stream. The schema of this stream is composed of the ID attribute and some other attributes that indicate the speed and the orientation of the monitored moving objects. Function Stamps(*) returns the spatial and temporal stamps (g and t) of each tuple, which indicate the trajectory of the observed object. This query contains a RANGE BY value without a SLIDE BY. It means that the spatial window is stationary and do not change over time or space.\nFormally, the semantics of the spatial window used in query Q 2 is specified by the following meaning function :\nWhile processing query Q 2 , only the tuples within the spatial range East_China_Sea are preserved in the window. The content of the window is updated when a new tuple comes in. The fact that a tuple is qualified for the window or not is determined by the spatial operator inside, which determines if the spatial location g (spatial stamp) is inside the spatial area denoted East_China_Sea.\nAfter the Stream-to-Relation operation (windowing operation), further processing on the window contents will be done. In this example, the Where clause and the Stamps operator will be performed. The meaning function of the where clause can be deduced easily from the semantics of CQL presented in section 2. The meaning function of the Stamps operator is as follows.\nAt the last stage, the result of query Q 2 is returned to the user in a stream format. This is done by the RStream operation. For each tuple added to its input relation, the RStream operator will re-evaluate completely its output and return all the tuples composing the output stream. Formally, the semantics of RStream is as follows :\nF??. 1 -Stream-to-Relation operation\nF??. 2 -Relation-to-Relation opertation for Window Content\nM R2S \u003d ?R.{e : e ? R(g, t)} Figure 1 shows an example of static spatial window over a spatio-temporal stream (an S2ROp). Three moving objects (o 1 , o 2 and o 3 ) are observed. Each new tuple represents an observation and contains the ID of the observed object, its velocity and its orientation. The temporal stamp of the tuple indicates the instant of the observation, and the spatial stamp indicates the location of the object at that instant. The trajectories shown in figure 1 represent the successive locations of the monitored objects. Figure 1.a represents the observations received up to now in the stream, and figure 1.b shows the subset of observations that are located inside the spatial window area East_China_Sea. Figure 2 shows the result of the R2ROp filtering operation. The Where clause restricts the observations to only those corresponding to object o 1 . Figure 3 illustrates the conversion from relation to stream by R2SOp. In this example, RStream will output all the tuples of its input relation. In the case of an IStream operation, the result relation will be compared to the previous one and only the new tuples will compose the output.\nF??. 3 -Relation-to-Stream opertation for Window Content\nF??. 4 -Stream-to-Relation operation for Moving Window\nMoving Spatial Window\nLet us take the example of the moving spatial window given at the beginning of Section 3. We add additional restrictions to this example, supposing that the car moves in a straight line and keeps the same direction. This query may be expressed as follows :\nSELECT The tuples in the relation are updated according to the parameter RANGE BY and SLIDE BY. Every time the monitored car moves 10 km ahead, the output result is re-evaluation and only the tuples having a spatial stamp g located inside the last 50 kms will be preserved in the window. Figure 4 illustrates the moving window operation in this example. The monitored car moves along a road. At time T 1 , the car completed the first 50 kms and the tuples received in the stream up to T 1 compose the first window. At time instant T 2 , the query window moves 10 kms ahead, and only the tuples corresponding to the last 50 kms are kept in the window. Note that the time needed by the car to cross the \"slide by\" distance is not constant : T 3 ? T 2 my be not equal to T 2 ? T 1 .\nSimilarly to query Q 1 , the Relation-to-Relation operator AvgSpd(R) and the Relation-toStream operator RStream are used here to calculate the average speed and to output the the result in a stream format. Their meaning functions can be deduced easily from the example of Q 1 .\nThe purpose of this paper was to define the semantics and general language syntax for a spatial windowing operation over data streams. Spatial windowing operation is useful for the querying of spatio-temporal data streams. Based on the continuous query language CQL, we proposed a syntax to express spatial windows and defined the semantics of this operation. this is the main contribution of this paper. Next steps will be the implementation of the windowing operation presented in this paper, its performance analysis on a real world application, and the analysis of more complex spatial windowing cases.\n"
  },
  {
    "id": "874",
    "text": "Introduction\nCet article présente une extension à la méthode de construction d\u0027ontologie à partir de textes Terminae Aussenac-Gilles et al. (2008). Lors de la création d\u0027une nouvelle ontologie, nous proposons de réutiliser une ontologie générique de référence afin de faciliter la phase de conceptualisation des termes d\u0027un corpus. Une ontologie générique de référence (traduction du terme core ontology) couvre un domaine composite (par exemple le droit) comportant de nombreux sous domaines (droit public, privé, européen, etc.). A ce titre, une telle ontologie constitue un cadre unifié pour la construction d\u0027ontologies de domaine composite puisqu\u0027elle décrit les concepts communs à l\u0027ensemble des sous-domaines.\nLa réutilisation constitue actuellement un point central de l\u0027ingénierie des ontologies soulevant des questions complexes. De nombreux travaux sont en cours dans ce domaine Euzenat et al. (2004), Noy (2004b), Shvaiko et Euzenat (2005), Predoiu et al. (2005), Bach (2006), Safar et al. (2007). Toutefois, peu de travaux exploitent la distinction entre les différents types d\u0027ontologies et leur articulation. En effet, parmi les concepts de l\u0027ontologie générique, certains jouent un rôle de pivot entre les ontologies des sous-domaines et permettent d\u0027ancrer l\u0027ontologie en cours de construction. Le processus d\u0027alignement proposé exploite également des informations lexicales et sémantiques de l\u0027ontologie de référence.\nDans le paragraphe 2, nous situons notre approche de la réutilisation d\u0027ontologies et son intégration dans la méthode Terminae. Le paragraphe 3 détaille l\u0027algorithme d\u0027alignement sé-mantique. Puis quelques exemples illustrent les premières expérimentations faites dans le domaine juridique. Enfin, nous concluons en discutant les apports et les limites de la méthode adoptée.\nMéthode\n2.1 Positionnement par rapport à alignement Lors de la construction d\u0027une nouvelle ontologie relative à un domaine de connaissances pour supporter une activité particulière, deux stratégies sont envisageables : la construction ex nihilo ou une réutilisation des ressources existantes. L\u0027existence d\u0027ontologies génériques disponibles, accessibles via les moteurs de recherche, conduit à définir des méthodes pour leur réutilisation. Les travaux relatifs à l\u0027intégration d\u0027ontologies cités supra concernent l\u0027éla-boration de méthodes d\u0027alignement semi-automatique ou automatique. Elles conduisent à la définition de mesures de similarité lexicales ou structurelles, dites sémantiques. Si les résultats obtenus sont indéniables, ils sont relativement peu nombreux dans le domaine de la construction d\u0027ontologies à partir de textes. Une des raisons vient sans doute de la difficulté à maîtriser le passage du niveau linguistique au conceptuel. Certaines des techniques utilisées pour l\u0027alignement ont recours à l\u0027exploitation structurelle des entités ou à des informations lexicales. Dans ce travail, l\u0027alignement sémantique utilise des informations linguistiques contenues dans la définition associée à chacune des entités considérées. L\u0027alignement est réalisé au fil de l\u0027éla-boration avec l\u0027ontologie générique de référence et il est dirigé de l\u0027ontologie en cours de construction vers l\u0027ontologie générique de référence.\nIntégration dans Terminae\nLes concepts de l\u0027ontologie en cours de construction sont définis et organisés à partir des connaissances exprimées dans les textes et en fonction des besoins de l\u0027application. L\u0027alignement intervient dans l\u0027étape de conceptualisation qui se décompose en deux phases, l\u0027amorçage et la consolidation. La phase d\u0027amorçage consiste à identifier, dénoter et définir les concepts terminologiques (en lien avec le corpus) du domaine puis à les organiser dans des hiérarchies locales. La dénotation et la définition en langue naturelle du concept terminologique sont éla-borées à partir des occurrences du terme étudié. Le repérage de propriétés structurelles et fonctionnelles liant ces concepts est obtenu à l\u0027aide de patrons lexico-syntaxiques. Les propriétés structurelles servent de support à leur organisation hiérarchique et les propriétés fonctionnelles qui sont propres au domaine permettent d\u0027établir des liens autres que hiérarchiques. La phase de consolidation a pour objectif de relier les hiérarchies locales obtenues et d\u0027enrichir le modèle. Trois processus y contribuent : la généralisation, la spécialisation et le regroupement. La généralisation selon un axe ascendant permet de déterminer les nouveaux concepts ancêtres et de définir des concepts plus abstraits ou de faire référence à des catégories de plus haut niveau dans la hiérarchie. La spécialisation intervient pour chaque concept existant afin de s\u0027assurer que ses sous-concepts ont bien été définis. Le regroupement qui permet de créer de nouveaux concepts partageant des propriétés identiques peut conduire à la définition de concepts non terminologiques. giques en cours de conceptualisation avec les entités de l\u0027ontologie générique. Cet appariement nécessite une comparaison lexicale et sémantique de ces entités. L\u0027aspect lexical vient accentuer les problèmes associés à la désignation des concepts terminologiques. L\u0027aspect sémantique conduit à comparer le sens des entités de l\u0027ontologie et des concepts terminologiques en s\u0027appuyant à la fois sur les définitions établies à partir des occurrences des termes dans le corpus et les commentaires décrivant les entités de l\u0027ontologie.\nL\u0027objectif poursuivi est de semi-automatiser cette mise en correspondance en exploitant la langue utilisée pour décrire les entités (dénotation de concepts, commentaires associés) de l\u0027ontologie générique, ce qui suppose une similitude avec le vocabulaire utilisé dans le corpus. L\u0027évaluation de la ressemblance entre les entités de deux ontologies conduit à utiliser des techniques qui permettent de comparer les concepts en mesurant la proximité lexicale Euzenat et Shvaiko (2007) des termes qui les dénotent, des propriétés structurelles et fonctionnelles qui les lient, de leur voisinage et de leurs extensions. La comparaison des propriétés peut être obtenue en comparant l\u0027intersection et l\u0027union des ensembles auxquelles elles appartiennent Staab et Maedche (2001), les domaine et codomaine des relations peuvent également être utilisés Cullot et al. (2003), Noy (2004a), ou encore les propriétés définissant les relations entre les concepts telles que la symétrie et la transitivité peuvent également être étudiées Ehrig et Sure (2004). Dans ce travail, la mise en correspondance repose sur une comparaison des chaînes de caractères des formes canoniques des entités (dénotation de concepts, de rôles et commentaires, les termes extraits du corpus).\nL\u0027algorithme d\u0027alignement sémantique\nOn suppose que les deux ontologies sont exprimées dans le même langage et que l\u0027ontologie générique est commentée en langue naturelle. On dispose de trois listes constituées à partir des entités de l\u0027ontologie : la liste des dénotations des concepts (LCO) ; la liste des dénotations des rôles (LRO) ; la liste des termes utilisés dans les commentaires (LCom). La dénotation du concept terminologique est désignée par CT. \nPseudo-Algorithme\n/ / r e c h e r c h e de c d a n s LCO t e l que Sim ( CT , c )\ni f ( sim ( CT , c ) ) { / / Sim ( CT , c ) s i m i l i t u d e l e x i c a l e de CT e t c 4 i f ( sem ( CT , c ) ) { / / Sem ( CT , c ) s i m i l i t u d e s é m a n t i q u e de CT e t c 5 i f ( f e u i l l e ( c ) ) {\n6\n/ / c r é a t i o n a n c r a g e\n7 } e l s e { 8\n/ / d é c i s i o n e x t e r n e p o u r a n c r a g e de CT\n9 \n/ / v e r s c ou un de s e s d e s c e n d a n t s\ns e { / / r e c h e r c h e d a n s l e s c o m m e n t a i r e s\n13\n/ / r e c h e r c h e d a n s l e s c o m p o s a n t s du s y n t a g m e n o m i n a l\n/ / r e c h e r c h e d a n s l e s c o m p o s a n t s du s y n t a g m e n o m i n a l 12\ne n s M o t s \u003d l e s s y n t a g m e s nominaux i n c l u s d a n s CT Si aucun ancrage n\u0027est trouvé, le processus ci-dessus est réitéré sur un hyperonyme jusqu\u0027à rencontrer un terme dénotant un concept de l\u0027ontologie générique. Au pire, l\u0027itération se poursuit jusqu\u0027aux concepts d\u0027ancrage de l\u0027ontologie de haut niveau tels que processus, objet abstrait, etc.\nIllustration\nLe domaine juridique sert de support à cette présentation et l\u0027ontologie à construire relève du droit européen. L\u0027ontologie générique LKIF-Core 1 notée LKIF (Legal Knowledge Interchange) exprimée en anglais est utilisée pour l\u0027alignement. Le corpus est constitué de directives, en langue anglaise, relatives au droit des travailleurs.\nNous présentons un exemple de mise en oeuvre de ce pseudo-algorithme dans les cas suivants : Terme : directive Le concept terminologique directive correspond à un concept de l\u0027ontologie générique qui a pour : -commentaire associé : Les termes de LRO dénotant les noms des rôles dans l\u0027ontologie générique sont comparés aux relations lexicales extraites du corpus. Seuls quelques termes de LRO apparaissent dans le corpus comme observe qui intervient dans la définition de NATURAL_PERSON. L\u0027étude des rôles fait apparaître au moins deux difficultés : au niveau conceptuel, la restriction des rôles de l\u0027ontologie générique, au niveau de la normalisation, la mise en correspondance des relations lexicales avec les termes de LRO.\nConclusion et perspectives\nDans ce papier, nous avons proposé un algorithme d\u0027alignement sémantique prenant en compte les informations linguistiques et sémantiques contenues dans une ontologie générique de référence (dénotation des concepts, des rôles et les commentaires associés). L\u0027avantage de cette approche semi-automatique est d\u0027affranchir l\u0027ingénieur de la connaissance d\u0027une exploration systématique de l\u0027ontologie générique et de lui permettre de juger du sens après une comparaison lexicale des entités étudiées. Néanmoins, les premières expérimentations soulèvent des difficultés relatives à la représentation des entités contenues dans les ressources exploitées comme l\u0027interprétation de la sémantique des commentaires. Des mesures plus précises relatives à l\u0027amélioration apportée par cette approche sont en cours d\u0027expérimentation.\n"
  },
  {
    "id": "875",
    "text": "Introduction\nMettre en oeuvre l\u0027une des méthodes de classification non supervisée consiste en premier lieu à choisir une manière de représenter les documents (Sebastiani, 2002) ; dans un second temps il faut choisir une mesure de similarité, et en dernier lieu choisir un algorithme de classification que l\u0027on va mettre au point à partir des descripteurs et de la métrique choisis. Tout document d j sera transformé en un vecteur de poids w kj des termes t k . La majorité des méthodes, pour calculer le poids w kj , sont axées sur une représentation vectorielle des textes de type TF-IDF (Sebastiani, 2002), qui attribue un poids d\u0027autant plus fort que le terme apparaît souvent dans le document et rarement dans le corpus complet. Il existe différentes approches pour la représentation des documents. Typiquement, la similarité entre documents est estimée par une fonction calculant la distance entre les vecteurs de ces documents. Plusieurs mesures de similarité ont été proposées (Jones \u0026 Furnas, 1987). Parmi ces mesures on peut citer la distance du cosinus. L\u0027algorithme SOM (Kohonen \u0026 al, 2000) a été depuis longtemps proposé et appliqué dans le domaine de la classification des documents textuels. Cependant, les combinaisons entre SOM et représentation conceptuelle de textes d\u0027une part, SOM et représentation basée sur les n-grammes d\u0027autre part n\u0027ont pas été beaucoup étudiées.\nExpérimentations, résultats et évaluation\nLes données utilisées dans nos expérimentations sont issues des textes du corpus Reuters21578. Dans l\u0027approche basée sur les n-grammes, on compte les fréquences des ngrammes trouvés. Dans l\u0027approche conceptuelle, on remplace les termes par les concepts qui leur sont associés dans l\u0027ontologie de références lexicales Wordnet (Miller, 1990). Cette représentation nécessitera deux étapes : la première est le « mapping » des termes dans des concepts et le choix de la stratégie de « merging », la deuxième est l\u0027application d\u0027une stratégie de désambiguïsation. On choisit la stratégie « Concept seulement », où il s\u0027agit de SOM pour la Classification Automatique Non supervisée de Textes basés sur Wordnet remplacer le vecteur des termes par le vecteur des concepts en excluant tous les termes de la nouvelle représentation, y compris les termes qui n\u0027apparaissent pas dans Wordnet. Pour la désambiguïsation nous utilisons la stratégie du « Premier concept » et la fonction TFIDF pour le calcul des poids de chaque terme pour les deux approches. Nous avons utilisé une carte de Kohonen 7x7. Pour chaque approche, quatre mesures de similarité ont été testées: les distances du cosinus, euclidienne, euclidienne au carré, et la distance de Manhattan. Nous avons calculé, pour chaque cas, le nombre de classes, le temps et le taux d\u0027apprentissage. Nous avons pu observer que malgré les bons résultats obtenus par la méthode des n-grammes particulièrement pour n\u003d3 et n\u003d4, ceux obtenus par la méthode conceptuelle, avec la distance du cosinus, sont plus performant. Pour évaluer la qualité des classifications obtenues nous avons utilisé la f-mesure et l\u0027entropie. La partition P considérée comme la plus pertinente et qui correspond le mieux à la solution externe attendue est celle qui maximise la F-mesure associée ou minimise l\u0027entropie associée. La plus grande valeur de la f-mesure est 62,5 et la plus petite valeur de l\u0027entropie est 37,5. Ces deux valeurs correspondent à l\u0027approche conceptuelle (Wordnet) avec la distance du cosinus, ce qui confirme les conclusions tirées.\nConclusion et perspectives\nDans cet article nous avons proposé deux nouvelles approches pour la classification non supervisée de textes, l\u0027une basée sur l\u0027utilisation des n-grammes et l\u0027autre sur WordNet. Les résultats obtenus montrent que malgré les bons résultats obtenus par la méthode des ngrammes, le fait d\u0027ajouter des connaissances lexicales dans la phase représentation permet de construire une classification de meilleure qualité. Nous projetons dans un premier temps d\u0027utiliser d\u0027autres stratégies de désambiguïsation et voir leur influence sur la classification, et dans un second temps utiliser d\u0027autres approches conceptuelles de références syntaxiques pour la classification par la méthode SOM des textes multilingues.\n"
  },
  {
    "id": "876",
    "text": "Introduction\nDans cet article, nous proposons trois stratégies de classification non supervisée appliquées sur fenêtres superposées. Notre objectif est de pouvoir repérer les changements de la distribution sous-jacente d\u0027un flux de donnés sur le temps. Notre approche consiste donc à fixer a priori la taille de la fenêtre et appliquer un algorithme de classification non supervisée sur les données contenues à l\u0027intérieur de la fenêtre. Nous définissons deux types de partitionnement de données sur les fenêtres : partitionnement par nombre d\u0027effectifs (fenêtre logique) et partitionnement par intervalle de temps (fenêtre de temps).\nL\u0027idée principale est de faire glisser la fenêtre sur le temps de telle façon que des nouvelles données soient rajoutées dans la fenêtre et par conséquence, les données les plus anciennes en soient éliminées. L\u0027action de glissement de la fenêtre sur les données est fait de telle manière à ce qu\u0027il y ait toujours une zone de chevauchement entre les deux ensembles de données contenues dans la fenêtre avant et après son glissement. Chaque fois qu\u0027une nouvelle fenêtre est définie, l\u0027algorithme de classification non supervisé est appliqué sur les données contenues dans la fenêtre, ce qui définit une partition et un ensemble de prototypes. La détection des possibles changements est faite par la comparaison de deux partitions obtenues sur le même ensemble d\u0027individus. Dans ce contexte, nous proposons trois types de comparaisons de partitions : comparaison sur les données de l\u0027intersection, comparaison sur les données de l\u0027union et comparaison sur la totalité des données.\nPour la classification de données dans notre approche, nous avons utilisé l\u0027algorithme Kmeans (MacQueen, 1967)  \nConclusion\nComme résultat de toutes expérimentations, nous avons des valeurs très élevées (compris entre 0.8 et 1) pour les deux critères d\u0027évaluation, ce qui nous montre un jeu de données assez stable et sans changements remarquables. Cela peut être dû à la courte période de temps disponible pour l\u0027analyse : pas plus de 22 jours, les premiers de l\u0027année. En conclusion, il est évident la nécessité d\u0027établissement d\u0027un compromis entre la taille de la fenêtre et le temps de réponse désiré, mais aussi en considérant la périodicité des changements.\nComme prolongement de ces expérimentations, nous citons l\u0027application de cette approche sur d\u0027autres jeux de données -aussi bien réelles qu\u0027artificielles -et aussi l\u0027exécution d\u0027autres simulations afin d\u0027analyser l\u0027influence des différentes valeurs des paramètres d\u0027entrée (tels que le pourcentage de chevauchement et le nombre de clusters). De plus, nous envisageons la mise en place de dispositifs permettant la découverte automatique de la périodicité présente dans le flux de données.\nRéférences\nHubert, L. et P. Arabie (1985). Comparing partitions. Journal of Classification 2, 193-218. MacQueen, J. (1967). Some methods for classification and analysis of multivariate observations. In 5th Berkley Symposium on Mathematics and Probability, Volume 1, pp. 281-297.\nvan Rijsbergen, C. J. (1979). Information Retrieval (second ed.). London : Butterworths.\nSummary\nA major difficulty is present in the data stream domain: the underlying data distribution may change over time. In this article, we propose three strategies of unsupervised classification based on overlapping windows. Our aim is to detect these changes over time. Our approach is applied on a benchmark of real data. The conclusions obtained are based on statistical analysis.\n"
  },
  {
    "id": "878",
    "text": "Introduction\nEtant donné un ensemble d\u0027objets et un ensemble d\u0027étiquettes de classes, le problème de classification est de chercher une fonction pour attribuer à chaque objet une étiquette de classe. Une telle fonction est appelée un classifieur. Les constructions de ces classifieurs sont en géné-ral basées sur les données d\u0027exemples (d\u0027entraînement). Il existe plusieurs méthodes de classification, telles que l\u0027arbre de décision Quinlan (1993), la méthode naïve-Bayes Duda et Hart (1973), les méthodes basées sur les règles Clark et Niblett (1995); Cohen (1995). Ce papier présente une approche à la construction de classifieurs basée sur les règles classe-associations Lent et al. (1997); Liu et al. (1998); Li et al. (2001), en utilisant une structure d\u0027arbre de pré-fixes pour l\u0027extraction des itemsets fréquents et les règles d\u0027association Agrawal et al. (1993).\nDans les approches telles que CMAR Li et al. (2001), HARMONY Wang et Karypis (2005), par optimisations, les règles d\u0027association sont essentiellement construites sur les itemsets clés Bastide et al. (2000). Ce présent travail montre que parmi ces itemsets clés, on peut s\u0027intéresser seulement à ceux de petites tailles. Ensuite, via un test de ? 2 , il montre que parmi ces derniers, il existe encore ceux qui ne sont pas significatifs pour la classification. Ces itemsets clés sont dits non essentiels. Les résultats d\u0027expérimentations sur les grands jeux de données de UCI Coenen (2004) montrent que l\u0027optimisation par la suppression de ces itemsets est correcte et efficace.\nPréliminaires\nUn jeu de données est un triplet D \u003d (O, I, R), où O, I, R sont des ensembles finis et non vides. Un élément de I est appelé un item, un élément de O est appelé un objet (ou une transaction) représenté par un identifiant, et R est une relation binaire entre O et I. Une paire (o, i) de R signifie que l\u0027item i est une valeur attribuée de l\u0027objet o. Un itemset est un sousensemble de I. Un k-itemset est un itemset avec k items ; k est la taille (ou la longueur) de l\u0027itemset.\nLa connexion de Galois Ganter et Wille (1999) est une paire de fonctions (f, g), où g(I) \u003d {o ? O | ?i ? I, (o, i) ? R} et f (O) \u003d {i ? I | ?o ? O, (o, i) ? R}. En fait, g(I) est l\u0027ensemble des objets de O qui ont en commun tous les items de I, et f est la fonction duale de g. La fonction g est anti-monotone : pour tout I 1 , I 2 ? I, si I 1 ? I 2 alors g(I 2 ) ? g(I 1 ).\nSoit un jeu de données D \u003d (O, I, R) et sa connexion de Galois (f, g). Les opérateurs de fermetures de Galois sont les fonctions suivantes : h \u003d f o g et h \u003d g o f , où o dénote la composition de fonctions. Les fonctions h et h sont monotones. Soit I un itemset. Alors h(I) \u003d f (g(I)) est appelée la fermeture de I. En effet, pour tout I ? I, I ? h(I) (Extension) et h(h(I)) \u003d h(I) (Idempotence). Un itemset I est dit fermé si I \u003d h(I). I est appelé un itemset clé (ou un générateur minimal Bastide et al. (2000)) si ?I ? I, h(I ) \u003d h(I) implique I \u003d I . Le support de I est sup(I) \u003d card(g(I)), où card dénote la cardinalité. Il est clair que si I ? I alors sup(I) ? sup(I ). Soit minsup un seuil de support. I est dit fréquent si sup(I) ? minsup. I est un itemset clé (respectivement, fermé) fréquent si I est fréquent et I est aussi un itemset clé (respectivement, fermé).\nUne règle d\u0027association (RA) est une expression de la forme X ? Y , où X et Y sont des itemsets disjoints. Soit r \u003d X ? Y une RA. Alors LHS(r) et RHS(r) dénotent respectivement la partie gauche et la partie droite de r. Le support de r est : sup(r) \u003d sup(X ? Y ). La confiance de r est : conf (r) \u003d sup(r)/sup(X).\nEn classification, on considère un ensemble C des éléments appelés les étiquettes de classes. Une règle classe-association (RCA) est une expression de la forme X ? c, où X ? I ? C et c ? C. Soit r \u003d X ? c une RCA. Un objet o est couvert par r si o a tous les items de X ; on dit que o satisfait r. Un objet o est classifié correctement par r si o satisfait r et o est effectivement de la classe d\u0027étiquette c. Le support de r, sup(r) \u003d sup(X ? {c}), est le nombre d\u0027objets de D qui sont classifiés correctement par r. La confiance de r, conf (r) \u003d sup(r)/sup(X), représente la fréquence d\u0027applications correctes de r dans D.\nExemple 1 Le tableau 1 représente un jeu de données d\u0027entraînement dans lequel\nSur les RCAs un ordre partiel, appelé l\u0027ordre de précédence et noté , est défini comme suit : Soient r et r des RCAs, r r (lire r précède r  Cohen (1995); Yin et Han (2003) calculent à chaque fois une règle, en utilisant des heuristiques basées sur l\u0027analyse statistique. En contraste, les algorithmes basés sur l\u0027extraction des règles classe-associations, comme CBA Liu et al. (1998), CMAR Li et al. (2001), cherchent un ensemble de règles de confiances élevées construites sur les itemsets fréquents.\nCBA adapte Apriori pour extraire les RCAs. Les règles sont triées dans l\u0027ordre de précé-dence avant d\u0027être sélectionnées pour le classifieur. Une règle est sélectionnée si elle classifie correctement au moins un des objets d\u0027entraînement. Dans ce cas, tous les objets couverts par la règle sont écartés du processus de sélection. Par l\u0027ordre de précédence, le classifieur préfère les règles formées sur les itemsets clés.\nBasé sur l\u0027idée de CBA, CMAR Li et al. (2001) adapte FP-growth pour extraire les RCAs. En plus de l\u0027ordre de précédence, CMAR considère la corrélation entre la partie gauche de règle et l\u0027étiquette de classe. D\u0027ailleurs, CMAR permet à chaque objet d\u0027être couvert par plusieurs règles et propose un schéma de classification basé sur de multiples règles. HARMONY Wang et Karypis (2005) utilise la même stratégie que FP-growth pour extraire les RCAs. Par défaut, les items d\u0027un jeu de données restreint sont triés dans l\u0027ordre croissant des coefficients de corrélation entre le préfixe correspondant et ces items. Les items et les jeux de données restreints non prometteurs sont exclus de l\u0027espace de recherche. Par ces exclusions, le classifieur préfère les règles formées sur les itemsets clés.\nA la différence de CBA et CMAR, pendant l\u0027extraction des règles, HARMONY maintient pour chaque objet une liste de règles de confiance la plus élevée qui classifient correctement l\u0027objet. A la fin du processus de l\u0027extraction, HARMONY regroupe les règles sélectionnées selon leurs étiquettes de classes et les trie dans l\u0027ordre décroissant des confiances et des supports. Pour classifier un objet de test ti, HARMONY calcule, pour chaque groupe du classifieur, la somme de confiances de k premières règles de confiance la plus élevée qui couvrent ti. La classe avec la somme la plus grande est sélectionnée pour prédire la classe de ti.\nContribution : La contribution de ce travail consiste en :\n-L\u0027étude d\u0027une relation entre les supports des itemsets qui s\u0027incluent, la généralisation d\u0027une propriété importante des itemsets non clés, et la notion d\u0027itemset clé non essentiel.\n-L\u0027application de la notion d\u0027itemset clé non essentiel pour optimiser la construction de classifieurs basée sur les RCAs.\n-L\u0027approche n\u0027explore pas les itemsets de toutes tailles. Elle se limite aux itemsets clés de petites tailles (? 5). Par conséquent, elle peut exploiter les règles de supports très bas, mais avec de meilleures confiances.\nLes résultats expérimentaux sur les grands jeux de données catégoriels montrent qu\u0027en moyenne l\u0027approche est efficace, en comparaison avec les approches importantes, et que l\u0027optimisation sur les itemsets clés non essentiels est correcte et efficace.\nItemsets clés non essentiels\nItemsets non clés\nIl est facile de voir que si X est un itemset clé, alors pour tout itemset I \u003d X, si X ? I ? h(X), alors sup(X) \u003d sup(I) \u003d sup(h(X)). Cette propriété permet de définir une relation d\u0027équivalence sur les RAs : une règle X ? Y , X ?Y \u003d ?, avec X et Y étant des itemsets clés, représente une classe d\u0027équivalence de règles, par rapport au support et à la confiance. Ces RAs représentatives sont très intéressantes : leur nombre est plus réduit, et avec les parties gauches réduites (au sens de clés), elles peuvent être appliquées à un plus large nombre d\u0027objets, par rapport aux règles aux itemsets non clés.\nProposition 1 Soient\nL\u0027application de la proposition 1 aux RCAs résulte en :\nD\u0027après le corollaire 1, en classification on peut construire les classifieurs avec seulement les RCAs aux itemsets clés. La fermeture de X, i.e. h(X), et tout I tel que X ? I ? h(X), sont des itemsets non clés. Ces itemsets ne sont pas intéressants pour les classifieurs. La propriété suivante est utile pour la suppression de ces itemsets Phan-Luong (2002).\nBastide et al. (2000) ont montré que, avec les conditions de la proposition 2, on a\nUne conséquence directe de la proposition 2 est la suivante :\nOn peut en déduire que si Y n\u0027est pas une clé, alors tout super ensemble de Y ne l\u0027est pas. Donc, pour la construction de classifieurs, il est plus intéressant de commencer par les itemsets de petites tailles. Quand on ajoute un nouvel item dans un itemset courant, si le résultat n\u0027est pas un itemset clé, alors il n\u0027est plus intéressant de continuer la recherche avec ce résultat. Dans la suite, nous étudions une généralisation de cette propriété pour pousser encore l\u0027optimisation sur les itemsets clés.\nItemsets clés non essentiels\nA cette étape on peut voir que la proposition 2 est une directe conséquence de la proposition 4. En effet, lorsque\nParmi les itemsets clés, il existe ceux qui ne sont pas très différents entre eux, aux niveaux de supports et d\u0027étiquettes de classes. Ces itemsets peuvent être considérés comme la même chose, dans le sens où les RCAs construites avec eux portent les informations très similaires sur le support et la confiance. Ces itemsets sont définis via un test ? comme suit. \nDéfinition 1 Soient X, Y des itemsets tels que\nPour les itemsets X et Y satisfaisant la définition 1, en construction d\u0027un classifieur, on peut s\u0027intéresser seulement aux itemsets clés comme X avec la taille la plus petite et oublier les itemsets comme Y . Les itemsets clés comme Y sont appelés itemsets clés non essentiels.\nPour élaguer les itemsets comme Y , on peut espérer une propriété similaire à celle spécifiée en proposition 2. C\u0027est-à-dire, commençant par les itemsets clés de petites tailles, et lors de l\u0027ajout d\u0027un nouvel item dans l\u0027itemset courant, si le résultat est une clé et, en comparaison avec l\u0027itemset courant, la définition 1 est satisfaite, alors on peut arrêter la recherche sur cet itemset courant. Précisément, si X ? Y ? Z et X CX Y , alors on attend que\nPour cette conjecture, on peut avoir trois arguments importants. D\u0027abord, bien que\nla différence entre ? |C (Z?Y )?X |,? et ? |CX |,? ne soit pas importante, surtout quand on considère les itemsets de petites tailles. Ensuite, par le corollaire 3, on a :\nEt dernièrement, d\u0027après la définition 1 :\nNous n\u0027avons pas prouvé formellement cette conjecture. Cependant, nous l\u0027appliquerons dans la méthode de construction de classifieurs, basée sur une structure d\u0027arbre de préfixes pour l\u0027extraction des itemsets.\nExtraction de classifieurs\nExtraction de règles de classe-association\nL\u0027approche utilise une technique d\u0027énumération de sous-ensembles Rymon (1992) sur un arbre de préfixes pour l\u0027extraction de RCAs. Commençant par un arbre de préfixes p vide, on lit successivement les objets d\u0027un jeu de données f pour mettre à jour p. Pour chaque objet o \u003d (l : c), où l est la liste d\u0027items de o, et c son étiquette de classe, les sous-itemsets de l sont énumérés dans l\u0027ordre lexicographique et stockés avec l\u0027étiquette c, dans l\u0027arbre p (fonction Build). Leurs supports ainsi que les occurrences de leurs étiquettes de classes sont mis à jour le long de la lecture du jeu de données. L\u0027approche peut adopter le calcul des itemsets fréquents par niveau comme Apriori. La fonction LevelBuild est un exemple spécifique de ce calcul. Par contraste avec Apriori : (i) l\u0027approche ne génère pas de candidats, (ii) le calcul de supports se fait pendant la construction de l\u0027arbre, et (iii) l\u0027approche peut commencer par les i-itemsets et passer de k-itemsets aux (k + j)-itemsets, avec i, j ? 1.\n// Construire l\u0027arbre p avec les sous-itemsets de taille maximale max du jeu de données f . function LevelBuild(f, p, max) { for (i \u003d 1 ; i ? max ; i++) { Build(f, p, i) ; // énumérer les itemsets de taille maximale i PruneInfrq(p, minsup) ; // élaguer les itemsets non fréquents } }\nL\u0027approche applique la contrainte de support aux i-itemsets, seulement pour i ? 2. Cependant, quand elle sélectionne les règles pour le classifieur, seules les règles dont la confiance et le support sont maximaux, par rapport à chaque objet d\u0027entraînement, sont retenues.  \nSuppression des Itemsets Clés Non Essentiels en Classification\nRéduction de l\u0027arbre de préfixes\nNous avons vu dans la section 4 que les RCAs formées sur les itemsets clés sont suffisantes pour les classifieurs (Corollaire 1). Nous avons aussi défini la notion d\u0027itemset clé non essentiel et pensé que ces itemsets se comporteront comme des itemsets non clés (Corollaire 3) et auront la même propriété spécifiée en proposition 2.\nLa suppression des itemsets non clés et les itemsets clés non essentiels se fait en deux étapes. La première étape s\u0027applique à l\u0027arbre de préfixes (fonction T reeReduction). Si un noeud N a le même support que son prédécesseur ou si l\u0027inéquation 1 de la définition 1 pour N et son prédécesseur est satisfaite, alors le sous-arbre avec N à la racine sera coupé. Pour tout noeud restant N , on réduit la liste lc(N ) en enlevant les paires (x, kx) telles que la valeur de kx n\u0027est pas maximale dans lc(N ) (fonction RedCls).\nLes paramètres de T reeReduction sont les suivants : N est un noeud de l\u0027arbre, k et precls représentent respectivement le support du prédécesseur de N et la liste des étiquettes de classes associées au prédécesseur de N avec leurs nombres d\u0027occurrences, et chi[ ] est un tableau de valeurs ? 2 pour le test de clés non essentiels.\nConstruction de Classifieurs\nLa deuxième étape de réduction est appliquée pendant la construction de classifieurs. Après la réduction utilisant T reeReduction, le jeu de données est lu à nouveau. Pour chaque objet, la fonction M atch cherche dans l\u0027arbre les noeuds correspondants aux RCAs qui classifient correctement l\u0027objet. Ces noeuds sont mis dans une liste temporaire nommée lnd (initialisée à vide pour chaque objet), dans l\u0027ordre de précédence des règles correspondantes. Quand l\u0027arbre est entièrement visité, les règles formées des noeuds de lnd sont rangées (Fonction AddRule) dans le classifieur, noté lrc, d\u0027après leurs étiquettes de classes. Cependant, soient r une règle en cours de considération et rc la règle de lrc en cours d\u0027être comparée avec r, si r et rc ont la même étiquette de classe et LHS(rc) ? LHS(r) et conf (r) ? conf (rc) alors r est rejetée.\nLa méthode pour classifier un objet suit le schéma de test de HARMONY : pour chaque étiquette de classe c, chercher dans lrc les règles qui couvrent l\u0027objet. La somme de confiances de ces règles est calculée. L\u0027objet est prédit de la classe dont la somme est maximale. Le tableau 2 rappelle les caractéristiques des 10 jeux de données de UCI Coenen (2004)  Maintenant nous présentons les résultats des expérimentations menées par ce travail (Tableau 3). Le programme exécutable de HARMONY est fourni gracieusement par les auteurs de Wang et Karypis (2005). Les paramètres sont configurés d\u0027après la description par ces auteurs : minsup \u003d 50 et les items sont ordonnés d\u0027après l\u0027ordre des coefficients de corrélation (l\u0027ordre avec lequel HARMONY atteint la meilleure performance). En particulier, pour connect, seuls les items de supports \u003c 20000 sont considérés. Les mêmes considérations sont appliquées pour SIM. La configuration des paramètres pour SIM :\n(i) L\u0027extraction des itemsets fréquents commence directement par les 2-itemsets, et tous k-itemsets considérés satisfont k ? 5.\n(ii) L\u0027élagage des itemsets non fréquents est effectué seulement pour les i-itemsets pour i ? 2, et avec minsup \u003d 50. Les étapes suivantes sont développées sur les 2-itemsets fréquents, mais l\u0027élagage n\u0027est plus effectué, sauf pour les itemsets de support 1.\n(iii) Les tests ? 2 sont effectués avec le risque d\u0027erreurs de 0, 5%. Dans le tableau 3, on utilise les notations suivantes. -Ts : le temps d\u0027exécution total en secondes des dix exécutions du test \"10-fold cross validation\" par jeu de données, pour construire les dix classifieurs et pour les tester.\n-Acc. : La précision moyenne de prédiction, en pourcentage.\n-L : la longueur maximale des itemsets considérés pendant la construction de l\u0027arbre.\n-#Rules : le nombre moyen de règles dans les classifieurs. Notons que dans les expérimentations de SIM, les objets sont lus toujours du disque. D\u0027ailleurs, bien que l\u0027on limite la longueur des itemsets en construction de l\u0027arbre de préfixes, on ne limite pas la longueur des objets dans les jeux de données. waveform HARMONY est environ 130 fois plus lent que SIM. La raison est que HARMONY peut considérer les itemsets de toute taille (pour waveform, la plupart des itemsets sont de tailles de 5 à 9, tandis que SIM ne considère que des itemsets de taille maximale 3).\n-En précision de classification : les deux approches sont comparables, sauf pour les jeux de données chess, nursery et waveform, SIM est plus précis. Ceci peut s\u0027expliquer par la sélection de RCAs de confiances et de supports maximaux parmi celles ayant de petits supports.\nBien que SIM soit similaire à HARMONY sur plusieurs points, les résultats expérimentaux des deux approches sont très différents. En général, SIM est meilleur en temps d\u0027exécution et en précision de classification. Avec un temps d\u0027exécution total environ quatre fois plus court, la prédiction par SIM est environ 2, 5% plus précise que celle de HARMONY, en moyenne.\nQuand SIM implémente la suppression des itemsets clés non essentiels, en moyenne, le nombre de RCAs diminue d\u0027environ 5%, le temps d\u0027exécution est amélioré d\u0027environ 3%, et la précision baisse de 0, 53%. Cependant, par rapport à HARMONY, cette précision est encore de 1, 98% plus grande.\nCes résultats valident l\u0027idée d\u0027utilisation des RCAs construites sur les itemsets clés de petites tailles et la possibilité de sélection des RCAs de confiances et de supports maximaux parmi celles ayant de petits supports. D\u0027ailleurs, ils montrent que la notion d\u0027itemset clé non essentiel est applicable et utile. Pour perspective, on peut penser que cette notion peut être développée pour l\u0027apprentissage dans le contexte d\u0027existence de bruits.\nSummary\nIn classification based on class-association rules, key itemsets (minimal generators) are essential in the built classifiers: non key itemsets can be pruned without affecting the accuracy of the classifiers. This work studies the generalization of a property of non key itemsets and shows that among the small size key itemsets, there still exist those which are not significant to the built classifiers, and can also be pruned. Those key itemsets are defined based on a ? 2 test. We apply this pruning to a method for building classifiers based on class-association rules, using a prefix tree structure for mining the frequent itemsets. Experiences on large datasets show that the pruning method is actually efficient and sound.\n"
  },
  {
    "id": "879",
    "text": "Introduction\nL\u0027objectif de la gestion des connaissances dans une entreprise est de favoriser la croissance, la transmission et la conservation des connaissances. Saad (2005) s\u0027intéresse au repérage des connaissances cruciales pour justifier le choix d\u0027investissement dans des opérations de capitalisation sur les connaissances. Dans la revue de la littérature, nous constatons qu\u0027il existe peu de travaux, s\u0027intéressant à la délimitation du champ des connaissances sur lesquelles il faut capitaliser. Les auteurs Dieng et al. (1998) ; Grundstein et al. (2003) ;B. Tseng et Huang (2005), précisent que le processus de détermination des connaissances cruciales est une action difficile à mener.\nDans cet article, nous proposons une approche multi-agents argumentative permettant de résoudre des conflits dans un système d\u0027aide à l\u0027identification des connaissances cruciales nommé K-DSS Saad (2005), Saad et Chakhar (pear). Les connaissances cruciales sont des savoirs et des savoir-faire nécessaires aux processus essentiels qui constituent le coeur des activités de l\u0027entreprise. Le système proposé est basé sur une méthode composée de trois phases. La première phase consiste à déterminer l\u0027ensemble d\u0027apprentissage que nous appelons les \"connaissances cruciales de référence\". La deuxième phase consiste à évaluer les \"connaissances cruciales de références\" sur une famille de critères et à inférer des règles de décision.\nLa troisième phase consiste à exploiter l\u0027ensemble des règles de décision inféré dans la phase précédente pour classifier des nouvelles connaissances que nous appelons \"connaissances potentiellement cruciales\".\nDurant la deuxième phase, des divergences concernant la crucialité des connaissances peuvent apparaître entre les décideurs et aboutir ainsi à des incohérences dans la base commune de connaissances la rendant inexploitable. La construction d\u0027un ensemble de règles de décision collectivement acceptées est basée sur une approche constructive qui s\u0027appuie sur les travaux de Belton et Pictet (1997). Ces auteurs s\u0027intéressent au problème de la prise en compte de l\u0027information individuelle dans un modèle multicritère.\nNotre objectif à travers ce travail est de proposer une approche multi-agent argumentative permettant d\u0027automatiser la résolution des conflits entre décideurs. Afin de concevoir cette approche, nous nous appuyons sur la théorie multi-agents pour représenter les acteurs humains par des agents logiciels connaissant leurs préférences et leurs règles de décision et pouvant ainsi argumenter leurs choix ou mettre à jour leurs croyances en fonction des arguments qu\u0027ils reçoivent des autres agents décideurs.\nL\u0027article est organisé comme suit. Dans la section 2, nous présentons des travaux qui traitent la problématique d\u0027évaluation des connaissances cruciales. Dans le section 3, nous décrivons la procédure d\u0027inférence des règles de décision collective. Dans le section 4, nous présentons le système multi-agent. Enfin, dans la section 5, nous décrivons les expérimentations et les résultats.\nTravaux antérieurs\nLa notion de besoin en connaissances pertinentes, en amont de toute opération de capitalisation, a été définie par plusieurs chercheurs Dieng et al. (1998) ;Grundstein (2000) ; Noh et al. (2000) ; B. Tseng et Huang (2005). Les travaux théoriques et empiriques proposés dans la littérature sont peu nombreux. Nous distinguons des méthodes centrées sur les domaines de connaissances, d\u0027autres centrées sur les processus. En analysant les démarches au niveau de la construction des critères et de l\u0027évaluation des connaissances, nous constatons que les auteurs Ermine (2003) ; Grundstein (2000) proposent des critères construits d\u0027une façon intuitive. En effet, ils n\u0027expliquent pas comment ils ont construit et validé les critères, ni comment ils gèrent les multiples points de vue des acteurs de terrain impliqués dans le processus d\u0027évaluation des connaissances. Or, il est déterminant de repérer les acteurs pertinents et s\u0027assurer qu\u0027ils adhérents aux critères retenus.\nComme nous l\u0027avons précédemment mentionné, la classification des connaissances se fait par plusieurs décideurs ne partageant pas forcément les mêmes points de vue. Ceci peut engendrer des problèmes lors de la classification des connaissances. En effet, dans un processus de prise de décision collective, des conflits peuvent être constatés principalement à cause de divergences de points de vue. Plusieurs travaux théoriques ont montré l\u0027apport d\u0027une approche argumentative dans plusieurs domaines tels que la négociation Kraus et al. (1998) et la ré-solution de conflits Sycara (1989). Plusieurs travaux traitent du problème de la résolution de conflits en ayant recours à l\u0027argumentation Elvang- Goransson et al. (1992), Simari et Loui (1992)) et particulièrement dans le domaine de la gestion de connaissances Chesñevar et al. (2006b), Chesñevar et al. (2006a).\nA travers cet article, notre objectif est double. D\u0027abord, démontrer l\u0027apport de la théorie multi-agent dans notre contexte et évaluer l\u0027impact d\u0027une approche argumentative sur la qualité de classification et sur le nombre de conflits entre décideurs.\n3 Inférence d\u0027une base de règles de décision collective La méthode proposée est composée de trois phases Saad (2005) : une première phase pour construire l\u0027ensemble d\u0027apprentissage \" connaissances cruciales de référence \". La deuxième phase consiste à construire un modèle de préférences des décideurs, puis une troisième phase de classification des nouvelles connaissances à évaluer. Durant la deuxième phase, des divergences concernant la crucialité des connaissances peuvent apparaître entre les décideurs et aboutir ainsi à des incohérences dans la base commune de connaissances la rendant inexploitable. Notre objectif à travers ce travail est de proposer une approche argumentative permettant d\u0027automatiser la résolution des conflits entre décideurs. Afin de concevoir cette approche, nous nous appuyons sur la théorie multi-agents pour représenter les acteurs humains par des agents logiciels connaissant leurs préférences et leurs règles de décision et pouvant ainsi argumenter leurs choix ou mettre à jour leurs croyances en fonction des arguments qu\u0027ils reçoivent des autres agents décideurs.\nConstruction d\u0027un modèle de préférences des décideurs\nCette phase consiste à déterminer des règles de décision à partir des informations préfé-rentielles des décideurs sur un ensemble de connaissances qui constituent des exemples d\u0027apprentissage et que nous nommons \" connaissances cruciales de référence \". Les informations préférentielles sont liées à la décision d\u0027affecter ces connaissances soit à la classe de déci-sion des \" connaissances non cruciales \", c\u0027est-à-dire des connaissances qui ne nécessitent pas une opération de capitalisation, soit dans la classe des \" connaissances cruciales \", c\u0027est-à-dire celles qui nécessitent une telle opération. Ce modèle de préférences du (des) décideur(s) se traduit sous forme de règles de décision de type \" Si conditions, alors conclusion \". Cette phase est composée de trois étapes présentées ci-dessous.\nNous proposons une procédure itérative permettant d\u0027inférer des règles de décision collectivement acceptées par les décideurs. Les différentes étapes de la procédure sont présentées dans la Figure 1. En s\u0027appuyant sur l\u0027ensemble des n \" connaissances de référence \" et les deux classes de décisions définies, la première étape consiste à déterminer avec chaque décideur des exemples d\u0027affectation de ces \" connaissances de référence \" dans les deux classes de déci-sion \" connaissances non cruciales \" et \" connaissances cruciales \". La deuxième étape permet alors d\u0027inférer un ensemble de règles pour chacun des exemples d\u0027affectation déterminé dans l\u0027étape précédente. La troisième étape consiste à modifier les exemples d\u0027affectation ou bien les critères avec le décideur concerné, si des incohérences sont détectées dans l\u0027ensemble des règles relatif à chaque décideur. La dernière étape consiste à déterminer, après concertation avec les décideurs, une base de règles collectivement acceptée. Nous présentons ci-dessous, la détermination des exemples d\u0027affectation des \" connaissances de référence \" dans les deux classes de décision, puis l\u0027inférence des règles de décision correspondant à chaque décideur. Nous déterminons enfin les règles collectivement acceptées par les différents décideurs.\n3.2 Détermination des exemples d\u0027affectation des \" connaissances de ré-férence \" dans les deux classes de décision\nAu cours de cette étape, en fonction des évaluations des connaissances sur les différents critères, l\u0027homme d\u0027étude demande à chaque décideur d\u0027affecter les \" connaissances de référence \" à l\u0027une des deux classes de décision ordonnées : -Cl1 : classe de décision \" connaissances non cruciales \" correspondant à des connaissances qui ne se révèlent pas nécessaires à capitaliser ; -Cl2 : classe de décision \" connaissances cruciales \" correspondant à des connaissances qui se révèlent nécessaires à capitaliser. -L\u0027 homme d\u0027étude est celui qui aide à modéliser les préférences des acteurs impliqués dans un processus de décision, en faisant émerger les diffé-rents points de vue qu\u0027il faut prendre en compte, sans pour autant influencer la décision prise par le décideur.\nChaque décideur affecte les \" connaissances de référence \" à l\u0027une des deux classes de déci-sion, classe des \" connaissances non cruciales \" ou classe des \" connaissances cruciales \". Nous obtenons un nombre de tableaux de décision égal au nombre des décideurs. Chaque tableau de décision contient les valeurs f (k i , g j ) correspondant à l\u0027évaluation de chaque connaissance k i sur chaque critère g j ainsi que son affectation dans l\u0027une des deux classes de décision .\nInférence des règles de décision correspondant à chaque décideur\nA partir des tables de décision, l\u0027homme d\u0027étude utilise un des algorithme d\u0027inférence (DOMLEM, Explore) proposé dans la méthode DRSA Greco et al. (2000)afin d\u0027inférer, pour chaque décideur, les règles de décision correspondant à ses exemples d\u0027affectations. Le choix de l\u0027algorithme est lié à la présence ou pas de données manquantes dans la table de décision, à la volonté de déterminer des règles non redondantes ou bien la liste de toutes les règles. L\u0027homme d\u0027étude analyse, avec chaque décideur, l\u0027ensemble des règles inférées à partir des exemples d\u0027affectation donnés par chacun d\u0027entre eux. Il vérifie tout d\u0027abord s\u0027il existe des incohérences dans la base des règles. L\u0027origine des incohérences peut provenir : -de l\u0027hésita-tion du décideur au moment de l\u0027affectation de la connaissance dans une classe de décision ; -du changement de point de vue du décideur au cours du processus de décision. Lors des expérimentations sur le terrain, nous avons constaté que le décideur pouvait changer d\u0027avis concernant l\u0027évaluation d\u0027une connaissance sur un critère donné ; -de l\u0027incohérence de la famille de critères : un critère manquant, un critère en trop.\nUne fois l\u0027origine de l\u0027incohérence déterminée, l\u0027homme d\u0027étude la corrige avec le déci-deur. Il procède de manière itérative tant que des incohérences sont identifiées dans la base de règles, et tant que le décideur a l\u0027intention de modifier les exemples d\u0027affectation et/ou les critères. Ce processus itératif permet alors d\u0027obtenir une meilleure compréhension des règles de décision choisies par chaque décideur. Pour chaque décideur, deux types de règles de décision sont déterminés, les règles couvrant les connaissances appartenant avec certitude à la classe de décision \" connaissance cruciales \" et les règles couvrant les connaissances qui peuvent appartenir à la classe \" connaissances cruciales \". Parmi ces règles, l\u0027homme d\u0027étude ne doit retenir que celles couvrant les \" connaissances de référence \" appartenant avec certitude à la classe de décision \" connaissances cruciales \". Après une analyse des différentes règles inférées, un ensemble de règles est retenu pour chaque décideur. Une règle de décision a la forme suivante :\nAvec g 1 , · · · , g m est une famille de critères, (r g1 , · · · , r gm ) Vgm sont les valeurs d\u0027évalua-tion d\u0027une connaissance sur les critères.\nDétermination des règles collectivement acceptées\nL\u0027homme d\u0027étude définit un ensemble de règles unique correspondant à l\u0027ensemble des règles collectivement acceptées par les décideurs à partir des règles de décision retenues pour chacun d\u0027eux. Dans le système K-DSS Saad (2005) nous avons suggéré que l\u0027homme d\u0027étude utilise la technique de \" comparaison \" proposée par Belton et Pictet (1997). Ainsi, il aide les décideurs à se concerter pour déterminer un ensemble collectivement accepté, à partir des différentes règles retenues par chacun d\u0027eux. La qualité de l\u0027ensemble de ces règles doit être vérifiée en les testant sur des exemples d\u0027affectation des nouvelles connaissances par les mêmes décideurs. Cette technique de concertation n\u0027est pas efficace pour évaluer un nombre important de connaissances.\nLe système multi-agent\nDans notre contexte, les systèmes multi-agents s\u0027avèrent d\u0027un grand intérêt. D\u0027une part, grâce à leur caractère autonome, les agents sont capables de représenter fidèlement les acteurs humains. D\u0027autre part, nous considérons qu\u0027un processus automatisé est adéquat à notre problématique vu le nombre important de connaissances à analyser et la difficulté de rassembler les décideurs humains pour argumenter sur toutes les affectations qui sont sources de conflit. Notre système multi-agents se compose d\u0027un agent médiateur et de N agents décideurs :\n1. l\u0027agent médiateur m responsable de la gestion de la base de connaissances. Son but consiste à aboutir à une base de connaissances cohérente, il a pour rôle de détecter les conflits d\u0027opinion, de mettre en contact les agents décideurs qui sont à l\u0027origine de ces conflits. Si un accord ne peut être atteint entre agents décideurs, l\u0027agent médiateur se charge grâce à ses méta-règles de prendre une décision objective quant à la classification en question. L\u0027agent médiateur repose sur deux modules : un module de communication et un module de décision. Le module de communication se charge de l\u0027échange de messages avec les autres agents du système. Le module de décision a pour rôle la résolution de conflits entre agents décideurs en ayant recours à une base de méta-règles. Notons que seul l\u0027agent médiateur a le droit d\u0027accès en mise à jour à la base de connaissances collective. La notion de méta-règle sera approfondie dans la section 4.4.\n2. les agents décideurs a i qui sont responsables de l\u0027affectation des connaissances en fonction de leurs croyances. Chaque agent décideur représente un décideur humain et détient une base de règles individuelle lui permettant de procéder à la classification et à l\u0027argumentation. Les agents décideurs impliqués dans le processus de classification des connaissances ont un même objectif final : partager une base de connaissances cohérente. Les agents décideurs reposent chacun sur trois modules interdépendants : un module de communication permettant l\u0027échange de messages avec les autres agents du système, un module d\u0027inférence chargé d\u0027inférer les règles de la base de règles individuelle et de déduire la classification à effectuer sur chaque connaissance et un module d\u0027argumentation capable de construire les arguments en fonction des affectations conflictuelles. Le module de communication est en relation avec le module d\u0027argumentation afin de construire les messages à envoyer aux autres agents décideurs.\nLe module d\u0027argumentation est en relation avec le module d\u0027inférence capable de lui fournir les arguments qui appuient une affectation donnée.\nFIG. 1 -Architecture multi-Agent\nProcessus et Protocole de résolution de conflits\nNotations préliminaires -Notons a 1 , a 2 . . . a n les agents décideurs impliqués dans le processus de classification des connaissances. \nProtocole de communication\nLe protocole de communication spécifie les actions que les agents sont autorisés à entreprendre, leur syntaxe et leur conditions. Les primitives de dialogue relatives au processus de classification des connaissances sont fournies dans la figure 2. Le processus d\u0027argumentation est initié par l\u0027agent médiateur qui, dès détection d\u0027un conflit (cf. Définition 2), envoie un message call ? f or ? arguments aux agents concernés les appelant à argumenter leurs dé-cisions de classifications respectives. Après avoir reçu cet appel, les deux agents commencent le processus d\u0027argumentation proprement dit qui peut être vu comme un échange de messages justif y achevé par un message accept ou un message reject.\nUn message d\u0027acceptation établit qu\u0027un accord a été atteint par les agents décideurs en question. Un message de rejet établit qu\u0027un accord n\u0027a pas pu être atteint, ce qui implique que l\u0027agent médiateur doit prendre une décision objective en tenant en compte sa propre base de méta-règles.\nAlgorithme de l\u0027agent médiateur\nL\u0027agent médiateur a pour rôle de résoudre les conflits entre classifications en se basant sur les méta-règles qu\u0027il détient et qui permettent d\u0027établir des classifications objectives.\nLa figure 3 montre le graphe d\u0027états de l\u0027agent médiateur. A la détection d\u0027un conflit, l\u0027agent médiateur envoie un message call-for-arguments aux agents concernés et reste en attente. A l\u0027issue du processus argumentatif, les agents décideurs concernés informent l\u0027agent médiateur de leur décision. S\u0027il reçoit un message accept alors le processus est terminé et l\u0027affectation convenue est établie par l\u0027agent médiateur. En revanche, si un message reject est envoyé, l\u0027agent médiateur est appelé à prendre une décision objective quant à l\u0027affectation de la connaissance et ce en se référant à ses méta-règles tel que détaillé dans la section suivante.\nMéta-règles\nLa figure 4 représente les critères de classification des connaissances ainsi que les règles qui leur sont associées. Une méta-règle consiste ainsi à déterminer un poids ? i associé à chaque \nU i étant une fonction de scores qui réduit les critères à une même échelle [O,100].\nx ? i étant la valeur de l\u0027affectation ? sur le critère i. \nFIG. 4 -Critères de classification et règles associées\nExpérimentations et résultats\nDans cette section, nous évaluons l\u0027apport d\u0027un système multi-agents automatisé dans un processus de classification collaborative des connaissances. Pour ce faire, nous avons implé-menté une plateforme sous Java composée de cinq modules : -un module de représentation des connaissances : responsable de la représentation des connaissances par leur nom, leur type ainsi que leur contenu ; -un module agents : responsable de la représentation des agents par plusieurs informations : le nom, la force, la base de règles associée, les affectations qu\u0027il a faites, . . .\n-un module d\u0027argumentation : responsable de la représentation, de l\u0027évaluation et de la construction des arguments ; -un module de génération aléatoire : offre des outils de génération de données aléatoires avec respect aux domaines de définition (discret ou continu) ; -un module de test : permet le paramétrage et la conduite des expérimentations. Nous avons conduit des expérimentations basées sur des données générées aléatoirement afin d\u0027évaluer l\u0027impact d\u0027un nombre croissant de connaissances à classer sur le nombre de conflits. Notre approche expérimentale a pour objectif de comparer une approche multi-agent argumentative à une approche non argumentative. Les entrées de nos simulations peuvent être résumées en ce qui suit :\n-10 agents décideurs représentés par une qualité d\u0027approximation générée aléatoirement ; -100 connaissances à classer ; -1000 affectations de connaissances en deux classes de décision. Afin de collecter les résultats préliminaires issus de nos premières expérimentations, nous avons basé le processus argumentatif sur la base unique de la qualité d\u0027approximation des agents décideurs. Ainsi, un agent accepte un argument uniquement lorsque l\u0027agent émetteur a une qualité d\u0027approximation meilleure que la sienne. Cette approche, bien que réductrice, nous permet d\u0027avoir une première appréciation de l\u0027apport d\u0027une approche multi-agents dans notre contexte d\u0027étude. Les premiers résultats montrent que pour tous les tests conduits, le nombre de conflits décroit en utilisant une approche argumentative. La figure 5 montre une comparaison entre une approche argumentative et une approche non argumentative. Nous avons noté que, dans la majorité des cas, le nombre de conflits a significativement baissé. Par exemple, nous pouvons baisser le nombre de conflits, en utilisant une approche argumentative, de 162 à 17. Par ailleurs, il convient de signaler que le nombre de conflits n\u0027a pas baissé dans tous les cas, ce qui est dû à une égalité des qualités d\u0027approximation des agents concernés.\nFIG. 5 -Comparaison entre une approche argumentative et une approche non argumentative\nPour évaluer l\u0027impact d\u0027un nombre croissant de connaissances sur le nombre de conflits, nous avons conduit des séries d\u0027expérimentations en faisant varier le nombre de connaissances à classer. nous observons sur la figure 6, au delà du fait que le nombre de conflits est nettement inférieur en utilisant une approche argumentative, qu\u0027une approche non argumentative est plus sensible à une variation du nombre de connaissances à classer qu\u0027une approche argumentative.\nFIG. 6 -Impact du nombre de connaissances sur le nombre de conflits\nConclusion\nNous avons présenté dans cet article une approche multi-agent argumentative pour la classification collaborative des connaissances cruciales. Après avoir présenté le système multiagents ainsi que l\u0027approche de classification des connaissances, nous avons conduit une approche expérimentale qui a permis de mettre en lumière l\u0027apport d\u0027une approche multi-agent argumentative dans un contexte de classification collaborative des connaissances cruciales.\nDans de travaux futurs, nous visons à proposer des expérimentations plus approfondies qui visent à valider l\u0027approche proposée sur des données issues d\u0027un contexte réel et s\u0027appuyant sur plusieurs critères.\n"
  },
  {
    "id": "880",
    "text": "Introduction\nLa classification non supervisée, ou clustering, est un outil très performant pour la détec-tion automatique de sous-groupes pertinents (ou clusters) dans un jeu de données, lorsqu\u0027on n\u0027a pas de connaissances a priori sur la structure interne de ces données. Les membres d\u0027un même cluster doivent êtres similaires entre eux, contrairement aux membres de groupes diffé-rents (homogénéité interne et séparation externe). La classification non supervisée joue un rôle indispensable pour la compréhension de phénomènes variés décrits par des bases de données. Un problème de regroupement peut être défini comme une tâche de partitionnement d\u0027un ensemble d\u0027items en un ensemble de sous-ensembles mutuellement disjoints. La classification est un problème de regroupement qui peux être considéré comme un des plus compétitifs en apprentissage non-supervisé. De nombreuses approches ont été proposées (Jain et Dubes, 1988). Les approches les plus classiques sont les méthodes hiérarchiques et les méthodes partitives.\nLes méthodes de classification hiérarchiques agglomératives (CAH) utilisent un arbre hiérar-chique (dendrogramme) construit à partir des ensembles à classifier. Dans ce cas, les noeuds de l\u0027arbre issus d\u0027un même parent forment un groupe homogène (Ward, 1963), alors que les méthodes partitives regroupent des données sans structure hiérarchique.\nUne méthode efficace utilisée pour la classification est la carte auto-organisatrice ou Self Organizing Map (SOM : Kohonen, 1984Kohonen, , 2001. Une SOM est un algorithme neuro-inspiré qui, par un processus non-supervisé compétitif, est capable de projeter des données de grandes dimensions dans un espace à deux dimensions. Cet algorithme d\u0027apprentissage non supervisé est une technique non linéaire très populaire pour la réduction de dimensions et la visualisation des données. Dans cette approche, la détection des regroupements est en général obtenue en utilisant d\u0027autres techniques de classification telles que K-Moyennes ou des méthodes hié-rarchiques. Dans la première phase du processus, une SOM standard est utilisée pour estimer les référents (moyennes locales). Dans la deuxième phase, les partitions associées à chaque référent sont utilisées pour former la classification finale des données en utilisant une méthode de classification traditionnelle. Une telle approche est appelée méthode à deux niveaux. Dans cet article, nous nous intéressons particulièrement aux algorithmes de classification à deux niveaux.\nUne des questions les plus importantes pour la plupart des applications réelles, aussi connue comme le \"problème de sélection de modèle\", est de déterminer un nombre approprié de groupes. Sans connaissances a priori il n\u0027y a pas de moyen simple pour déterminer ce nombre. L\u0027objectif de ce travail est de fournir une approche de classification à deux niveaux simultanés utilisant une SOM, qui peut être appliquée à de grandes bases de données. La méthode proposée regroupe automatiquement les données, c\u0027est-à-dire que le nombre de groupes est dé-terminé automatiquement pendant le processus d\u0027apprentissage, i.e. aucune hypothèse a priori sur le nombre de groupes n\u0027est exigée. Cette approche a été évaluée sur un jeu de problèmes fondamentaux pour la classification et montre d\u0027excellents résultats comparés aux approches classiques.\nLe reste de cet article est organisé comme suit. La Section 2 présente l\u0027algorithme de classification à deux niveaux simultanés. La Section 3 décrit les bases de données utilisées pour la validation ainsi que le protocole expérimental. Dans la section 4 nous présentons les résultats de la validation et leur interprétation. Une conclusion et des perspectives sont données dans la section 5.\n2 Algorithme de classification à deux niveaux simultanés basé sur une carte auto-organisatrice Dans un espace de grande dimension les données peuvent être fortement dispersées, ce qui rend difficile pour un algorithme de classification la recherche de structures dans les données. En réponse à ce problème, un grand nombre d\u0027approches basées sur une réduction de dimensions ont été développées et testées pour différents domaines d\u0027application. L\u0027idée principale de ces approches est de projeter les données dans un espace de faible dimension tout en conservant leur topologie. Les nouvelles coordonnées des données ainsi projetées peuvent alors être efficacement utilisées par un algorithme de classification. C\u0027est ce qu\u0027on appelle la classification à deux niveaux. De nombreuses approches ont été proposées pour résoudre des problèmes de classification à deux niveaux (Bohez, 1998;Hussin et al., 2004;Ultsch, 2005;Guérif et Bennani, 2006;Korkmaz, 2006). Les approches basées sur l\u0027apprentissage d\u0027une carte autoorganisatrice sont particulièrement efficaces du fait de la vitesse d\u0027apprentissage de SOM et de ses performances en réduction de dimensions non linéaire (Hussin et al., 2004;Ultsch, 2005;Guérif et Bennani, 2006).\nBien que les méthodes à deux niveaux soient plus intéressantes que les méthodes classiques (en particulier en réduisant le temps de calcul et en permettant une interprétation visuelle de l\u0027analyse, Vesanto et Alhoniemi (2000)), la classification obtenue à partir des référents n\u0027est pas optimale, puisqu\u0027une partie de l\u0027information à été perdue lors de la première étape. De plus, cette séparation en deux étapes n\u0027est pas adaptée à une classification dynamique de données qui évoluent dans le temps, malgré des besoins importants d\u0027outils pour l\u0027analyse de ce type de données. Nous proposons donc ici un nouvel algorithme de classification non-supervisée, S2L-SOM (Simultaneous Two Level -SOM), qui apprend simultanément les prototypes (référents) d\u0027une carte auto-organisatrice et sa segmentation.\nPrincipe\nUne SOM est un algorithme d\u0027apprentissage compétitif non-supervisé à partir d\u0027un réseau de neurones artificiels (Kohonen, 1984(Kohonen, , 2001. Lorsqu\u0027une observation est reconnue, l\u0027activation d\u0027un neurone du réseau, sélectionné par une compétition entre les neurones, a pour effet le renforcement de ce neurone et l\u0027inhibition des autres (c\u0027est la règle du \"Winner Takes All\"). Chaque neurone se spécialise donc au cours de l\u0027apprentissage dans la reconnaissance d\u0027un certain type d\u0027observations. La carte auto-organisatrice est composée d\u0027un ensemble de neurones connectés entre eux par des liens topologiques qui forment une grille bi-dimensionnelle. Chaque neurone est connecté à n entrées (correspondant aux n dimensions de l\u0027espace de représentation) selon n pondérations w (qui forment le vecteur prototype du neurone). Les neurones sont aussi connectés à leurs voisins par des liens topologiques. Le jeu de données est utilisé pour organiser la carte selon les contraintes topologiques de l\u0027espace d\u0027entrée. Ainsi, une configuration entre l\u0027espace d\u0027entrée et l\u0027espace du réseau est construite ; deux observations proches dans l\u0027espace d\u0027entrée activent deux unités proches sur la carte. Une organisation spatiale optimale est déterminée par la SOM à partir des données et quand la dimension de l\u0027espace d\u0027entrée est inférieure à trois, aussi bien la position des vecteurs de poids que des relations de voisinage directes entre les neurones peuvent être représentées visuellement. Le neurone gagnant met à jour son vecteur prototype, de façon à devenir plus sensible à une pré-sentation future de ce type de donnée. Cela permet à différents neurones d\u0027être entrainés pour différents types de données. De façon à assurer la conservation de la topologie de la carte, les voisins du neurone gagnant peuvent aussi ajuster leur vecteur prototype vers le vecteur pré-senté, mais dans un degré moindre, en fonction de leurs distances au prototype gagnant. Ainsi, les prototypes les plus proches d\u0027une donnée correspondent à des neurones voisins sur la carte. En général, on utilise pour cela une fonction de voisinage gaussienne à symétrie radiale K ij .\nDans l\u0027algorithme S2L-SOM, nous proposons d\u0027associer à chaque connexion de voisinage une valeur réelle qui indique la pertinence des neurones connectés. Étant donné la contrainte d\u0027organisation de la carte, les deux meilleurs représentants de chaque donnée sont reliés par une connexion topologique. Cette connexion sera \"récompensée\" par une augmentation de sa valeur, alors que toutes les autres connexions issues du meilleur représentant seront \"punies\" par une diminution de leurs valeurs. Les récompenses et les punitions sont d\u0027autant plus importantes que l\u0027apprentissage est bien avancé et donc que la structure de la carte est bien représentative de la structure des donnés.\nAinsi, à la fin de l\u0027apprentissage, un ensemble de prototypes interconnectés sera représen-tatif d\u0027un sous-groupe pertinent de l\u0027ensemble des données : un cluster.\nAlgorithme S2L-SOM\nL\u0027apprentissage connexionniste est souvent présenté comme la minimisation d\u0027une fonction de coût. Dans notre cas, cela correspond à la minimisation de la distance entre les données et les prototypes de la carte, pondérée par une fonction de voisinage K ij (Kohonen, 2001). Pour ce faire, nous utilisons un algorithme de gradient. La fonction de coût à minimiser est définie par :\nAvec N le nombre de données, M le nombre de neurones de la carte, w .j \u003d (w 0j , w 1j , ..., w nj ), N (x (k) ) est le neurone dont le vecteur prototype est le plus proche de la donnée x (k) . K ij est une fonction symétrique positive à noyau : la fonction de voisinage. L\u0027importance relative d\u0027un neurone i comparé à un neurone j est pondérée par la valeur de K ij , qui peut être définie ainsi :\n?(t) est une fonction de température qui contrôle l\u0027étendue du voisinage qui diminue avec le temps t de ? i à ? f (par exemple ? i \u003d 2 à ? f \u003d 0,5) :\nt max est le nombre maximum d\u0027itérations autorisé pour l\u0027apprentissage. d 1 (i, j) est la distance de Manhattan définie entre deux neurones i (de coordonnée (k, m)) et j (de coordonnée (r, s)) sur la grille de la carte :\nLe processus d\u0027apprentissage de S2L-SOM est proche d\u0027un \"Competitive Hebbian Learning\" (CHL, Martinetz, 1993). La différence essentielle est qu\u0027un CHL ne change pas les ré-férences des prototypes en cours d\u0027apprentissage. De plus, avec S2L-SOM, seuls les neurones voisins sur la carte peuvent être connectés, ce qui conserve la topologie en deux dimensions de la carte et permet une réduction de dimensions et une visualisation simple de la structure des données. Par ailleurs, l\u0027utilisation d\u0027une valeur de récompense associée aux connexions donne une information sur la représentativité locale des deux neurones connectés, ce qui n\u0027est pas le cas avec un CHL. Martinetz (1993) a montré que le graphe généré de cette manière préserve la topologie de façon optimale. En particulier chaque arc de ce graphe suit la triangulation de Delaunay correspondant aux vecteurs de référence.\nL\u0027algorithme S2L-SOM procède essentiellement en trois phases :\n-Définir la topologie de la carte.\n-Initialiser aléatoirement tous les prototypes w .j \u003d (w 0j , ..., w nj ) pour chaque neurone j. -Initialiser les connexions ? entre chaque couple de neurones i et j :\n) pour repré-senter cette donnée :\n-Augmenter la valeur de la connexion entre\nles valeurs des autres connexions issues de\nAvec : -Mettre à jour les prototypes w .j de chaque neurone j selon la règle d\u0027adaptation suivante :\noù ?(t) est le pas du gradient 4. Répéter les phases 2 et 3 jusqu\u0027à ce que t \u003d t max .\nÀ la fin de l\u0027apprentissage, chaque ensemble de neurones connectés entre eux par des connexions de valeurs positives est representatif d\u0027un groupe homogène de données. L\u0027algorithme attribue un numero à chacun de ces ensemble. Le nombre de groupe est ainsi obtenu automatiquement.\nValidation\nDescription des bases de données utilisées\nDe façon à démontrer les performances de l\u0027algorithme de classification proposé, huit bases de données présentant différentes difficultés de classification ont été utilisées.\nLes bases de données \"Hepta\", \"Chainlink\", \"Atom\" et \"TwoDiamonds\" proviennent du Fundamental Clustering Problem Suite (FCPS Ultsch, 2005). Nous avons généré aussi quatre autres bases de données intéressantes ( \"Rings\", \"Spirals\", \"HighDim\" et \"Random\"). \"Rings\" est composée de 3 groupes en 2 dimensions non linéairement séparables et de densité et variance différentes : un anneau de rayon 1 pour 700 points (forte densité), un anneau de rayon 3 pour 300 points (faible densité) et un anneau de rayon 5 pour 1500 point (densité moyenne). \"HighDim\" est constitué de 9 groupes de 100 points chacun bien séparés dans un espace à 15 dimensions. \"Random\" est un tirage aléatoire de 1000 points dans un espace à 8 dimensions. Enfin \"Spirals\" est constitué de deux spirales parallèles de 1000 points chacune dans un anneaux de 3000 points. La densité des points dans les spirales diminue avec le rayon.  \nProtocole expérimental\nNous avons comparé les performances de S2L-SOM en terme de qualité de la segmentation et de stabilité par rapport aux performances des méthodes classiques à un ou deux niveaux. Les algorithmes de comparaison choisis sont K-Moyennes, SingleLinkage et Ward appliqués sur les données ou sur les prototypes de la carte après apprentissage. L\u0027indice de Davies-Bouldin (Davies et Bouldin, 1979) est utilisé pour déterminer le meilleur découpage des arbres (SingleLink et Ward) ou le nombre optimal K de centroïdes pour K-Moyennes. S2L-SOM détermine automatiquement le nombre de classes et n\u0027a pas besoin d\u0027utiliser cet indice.\nDans cet article la qualité de la segmentation à été évaluée à partir d\u0027indices externes (indices de Rand et Jaccard) fréquemment utilisés (Halkidi et al., 2001(Halkidi et al., , 2002. En effet, si des catégo-ries indépendantes des données sont connues, on peut demander si la classification obtenue correspond à ces catégories. Rand \u003d a 00 + a 11 a 00 + a 01 + a 10 + a 11 Jaccard \u003d a 11 a 01 + a 10 + a 11\nIci a 00 est le nombre de paires d\u0027objets dont les deux éléments sont dans la même catégorie et le même cluster. a 01 est le nombre de paires d\u0027objets dont les deux éléments sont dans la même catégorie mais pas le même cluster, alors que a 10 est le nombre de paires d\u0027objets dont les deux éléments sont dans le même cluster mais pas la même catégorie. Pour finir, a 11 est le nombre de paires d\u0027objets dont les deux éléments ne sont ni dans le même cluster, ni dans la même catégorie.\nNous avons aussi utilisé pour ces travaux les indices internes de Davies-Bouldin (1979) et Calinski-Harabasz (1974). Ici la principale question est d\u0027estimer à quel point une segmentation des données correspond à sa structure interne. En l\u0027absence de connaissances a priori cette segmentation peut toujours être évaluée par des critères internes comme l\u0027homogénéité intra-groupes et la séparation entre groupes. Les valeurs de ces indices ont été normalisés entre 0 et 1 pour chaque test pour une meilleure lisibilité.\nL\u0027indice suggéré par Davies et Bouldin (1979) pour différentes valeurs de K (nombre de cluster) est typiquement introduit comme suit pour évaluer les concepts de séparation entre groupes (le dénominateur) et l\u0027homogénéité intra-groupes (le numérateur). Soit s i la racine carrée de l\u0027erreur standard (variance intra-groupes) du groupe i de centroïde c i : de Calinski et Harabasz ((1974) est le plus largement utilisé dans les méthodes de classification classiques. Il peut être défini comme suit :\nAvec N le nombre de points de données et K le nombre de groupes. trace(W ) est la somme des distances carrées inter-groupes alors que trace(B) est la somme des distances carrées intra-groupes. Cet indice présente une valeur élevée lorsque le nombre de groupes est optimum.\nLe concept de stabilité est aussi utilisé pour estimer la validité de la segmentation. Pour éva-luer la stabilité des différents algorithmes, nous utilisons une méthode de sous-échantillonnage (Ben-Hur et al., 2002). Pour chaque base de donnée, chaque sous-échantillon est segmenté par un algorithme de classification, et nous comparons deux à deux avec l\u0027indice de Jaccard les différences entre les segmentations obtenues. Ce processus est répété un grand nombre de fois et la moyenne de l\u0027indice est considérée comme une estimation fiable de la stabilité de la classification.\nLes résultats pour les indices externes montrent que pour ces données S2L-SOM est capable de retrouver sans aucune erreur la segmentation attendue. Ce n\u0027est pas le cas des autres algorithmes, en particulier lorsque les groupes sont de formes arbitraires ou lorsqu\u0027il n\u0027y a pas de structure dans les données (voir figure 4(a)). Les segmentations obtenues par S2L-SOM sont d\u0027excellentes qualités selon les indices internes lorsque les données sont regroupées en amas compacts et plus ou moins hypersphériques (\"Hepta\" ou \"HighDim\" par exemple). Par contre ces indices ne sont pas du tout adaptés à des groupes de formes arbitraires (\"Rings\", \"Spirals\", \"Chainlink\" ou \"Atom\", figure 4(b)), ce qui explique les mauvaises performances de S2L-SOM pour ces données. Il faut noter aussi que nous ne pouvons pas évaluer de cette façon une segmentation en un seul groupe, comme c\u0027est le cas pour la segmentation des données \"Random\" par S2L-SOM. En ce qui concerne la stabilité (figure 5), S2L-SOM montre des résultats excellents pour les données regroupées en hypersphères, quelle que soit la dimension (\"Hepta\" et \"HighDim\"), mais aussi dans les cas où les groupes sont de formes arbitraires en deux dimensions (\"Rings\" et \"Spirals\") et lorsque les données ne sont pas structurées (\"Random\"). On remarque que dans ce dernier cas la segmentation obtenue par les méthodes classiques est extrêmement instable. Lorsque les données ne sont pas linéairement séparables dans des dimensions supérieures à deux (\"Atom\" et \"Chainlink\"), l\u0027algorithme est limité par la contrainte topologique en deux dimensions de la carte auto-organisatrice et la stabilité de la segmentation n\u0027est pas maximale. On peut cependant noter que même dans ce cas S2L-SOM reste plus stable que la quasi totalité des méthodes classiques. Par contre, tout en présentant une stabilité relativement élevée, S2L-SOM est moins stable que la plupart des méthodes classiques lorsque les groupes pressentent un point de contact (\"Diamonds\"). En effet, ce point de contact favorise la création et l\u0027augmentation par l\u0027algorithme de la valeur des connexions entre les deux groupes. La visualisation des groupes obtenus confirme ces résultats. En effet, l\u0027algorithme S2L-SOM est un puissant outil pour la visualisation en deux dimensions de la segmentation obtenue. Les groupes sont aisément et clairement identifiables, ainsi que les zones sans données. Tel qu\u0027on peut le voir sur la figure 6, les résultats obtenus avec l\u0027algorithme S2L-SOM sont plus proches de la réalité que ceux obtenus par des méthodes classiques. Dans cet article, nous proposons une méthode de classification à deux niveaux simultanés. On utilise une SOM comme technique de réduction de dimensions et effectue en parallèle une classification optimisée. Les performances de cette méthode ont été évaluées à partir de tests sur une série de problèmes fondamentaux pour la classification, et comparées aux méthodes à deux niveaux classiques s\u0027appuyant sur CAH ou K-Moyennes. Les résultats expérimentaux démontrent que l\u0027algorithme proposé produit une classification de meilleure qualité que les approches classiques. Ils montrent aussi que le grand avantage de l\u0027algorithme S2L-SOM est qu\u0027il n\u0027est pas limité aux groupes de formes convexes, mais est capable d\u0027identifier des groupes de formes arbitraires. Pour finir, le nombre de groupes est déterminé automatiquement dans notre approche pendant l\u0027apprentissage, c\u0027est-à-dire qu\u0027aucun a priori sur ce nombre n\u0027est requis.\nCependant, cette méthode ne peut fonctionner que si les clusters sont suffisamment séparés dans l\u0027espace de données. En effet, des groupes qui se touchent ne sont définis que par une diminution de la densité dans la zone de contact, ce qui ne peut pas être détecté par S2L-SOM. Dans le futur, nous prévoyons donc d\u0027utiliser des informations sur la densité des données pour améliorer les performances de l\u0027algorithme. Nous prévoyons aussi d\u0027incorporer de la plasticité à l\u0027algorithme S2L-SOM, pour rendre le modèle incrémental et évolutif.\n"
  },
  {
    "id": "881",
    "text": "Introduction\nLe succès récent des représentations géographiques technologiques réalistes illustré par l\u0027omniprésence des globles virtuels de type Google Earth dans notre quotidien, ne doit pas pour autant nous faire oublier l\u0027importance des représentations abstraites pour l\u0027analyse et la compréhension de phénomènes spatiotemporels complexes. L\u0027intérêt des représentations abstraites réside souvent dans leur capacité à se détacher partiellement des contraintes liées à l\u0027espace euclidien, en favorisant ainsi la prise en compte de dimensions non spatiales mais néanmoins fondamentales. Ces formes abstraites offrent en effet plus de flexibilité que les représentations réalistes pour communiquer simultanément les trois aspects du schéma « triad spatio-temporel » (Peuquet 1994) que sont les dimensions spatiale (où), temporelle (quand) et thématique (quoi).\nGéovisualisation et spatio-temporalité\nGéovisualisation : présentation\nLa Visualization in Scientific Computing (ViSC), qui est apparue à la fin des années 80 (McCormick et al. 1987), correspond à la création de représentations visuelles facilitant la réflexion et la résolution de problèmes à l\u0027aide de technologies sophistiquées (Hearnshow and Urwin 1994). Cette approche s\u0027est avérée particulièrement bien adaptée dans différents domaines tels que : la représentation d\u0027objets physiques sous des angles ne pouvant être visualisés dans la réalité (ex. les couches géologiques) ; l\u0027exploration de bases de données complexes à l\u0027aide de métaphores visuelles (Peuquet 2002) ; la production d\u0027hypothèses dans les démarches exploratoires (Goodchild and Janelle 2004) ; ou encore pour pallier certaines limites inhérentes aux méthodes d\u0027inférence statistique ou d\u0027analyse multivariée (Gahegan 2000 ;Koua and Kraak 2004). Stan Openshaw et al. (1994) estiment même que les résultats obtenus à l\u0027aide de la visualisation peuvent servir de base pour le développement de modèles ou théories, voire même pour produire des conclusions suffisantes qui ne requièrent pas nécessairement d\u0027autres formes d\u0027analyse. Ces différents éléments contribuent à faire de l\u0027analyse visuelle de larges bases de données un axe de recherche majeur des sciences de l\u0027information géographique (Koua and Kraak 2004), généralement regroupé sous le terme de géovisualisation.\nLa géovisualisation se rapporte plus particulièrement à la visualisation de données ayant une dimension géographique. La géovisualisation cherche à rendre visible les contextes et problèmes spatiaux. Elle se base sur les capacités de l\u0027oeil et du cerveau à détecter des structures et des anomalies ainsi qu\u0027à se remémorer des informations spatiales de manière à favo-riser l\u0027analyse scientifique (Peterson 1994 ;Goodchild and Janelle 2004). Elle remplace ou complète la « visualisation mentale » lorsque celle-ci est rendue impossible par le volume et la complexité des données (McEachren 1995). Son principal intérêt réside dans sa capacité à combiner la puissance informatique analytique avec l\u0027aptitude de l\u0027être humain pour l\u0027interprétation des représentations graphiques de manière à favoriser l\u0027émergence de phé-nomènes non anticipés (Peuquet and Kraak 2002).\nMême si sa dimension visuelle confère à la géovisualisation un caractère quasi universel qui la destine à un public large et diversifié, il a été mis en évidence que les utilisateurs novices ont généralement plus de difficultés pour utiliser et comprendre des représentations abstraites (Bishop 1994;Peuquet 2002). Les formes abstraites de visualisation de l\u0027information géographique sont donc principalement destinées à un public d\u0027experts dans le domaine étudié pour lesquels elles se révèlent généralement plus efficaces que les formes réalistes (DiBiase et al. 1992;Dorling 1992).\nD\u0027après Ronald Finke et al. (1992) les images qui stimulent le plus l\u0027imagination sont celles qui sont nouvelles, incongrues, abstraites et/ou ambiguës. La création de formes de géovisualisations abstraites et originales peut donc être envisagée comme un moyen de stimuler la réflexion et de favoriser la découverte. D\u0027autre part, les formes abstraites offrent souvent plus de flexibilité pour représenter simultanément les trois aspects fondamentaux du schéma « triad spatio-temporel » que sont les dimensions spatiale (où), temporelle (quand) et thématique (quoi) (Peuquet 1994). La représentation simultanée de ces trois composantes est cruciale pour l\u0027analyse des phénomènes spatio-temporels. Les réponses potentielles aux trois principales questions qu\u0027elle implique -quoi ? quand ? où ? -participent de la puissance et de l\u0027intérêt réel de la géovisualisation (McEachren 1995). Plus que l\u0027une ou l\u0027autre de ces composantes, c\u0027est véritablement leur combinaison qui peut permettre une meilleure analyse des données existantes ainsi qu\u0027une meilleure compréhension des phénomènes géospatiaux qu\u0027elles sous-tendent. Cette représentation simultanée demeure complexe et par conséquent marginale.\nGéovisualisation et analyse spatiotemporelle : exemples d\u0027applications\nDepuis le XVIIIe siècle et l\u0027apparition des premières isochrones (cf. Husson 2004), les cartographes et ingénieurs ont développé des trésors d\u0027ingéniosité pour intégrer le temps dans les cartes statiques (cf. Monmonier 1990 ;Kraak 2005) à l\u0027image de la fameuse « Carte figurative des pertes successives en hommes de l\u0027armée française dans la campagne de Russie, 1812-1813 » publiée en 1869 par Joseph Minard. Cette carte représente simultanément les dimensions temporelles, spatiales et quantitatives de l\u0027évolution de l\u0027armée napoléo-nienne au cours de la campagne de Russie. Elle privilégie la simplification du message au détriment de son exactitude géographique et préfigure ainsi le potentiel des cartogrammes pour la représentation et l\u0027analyse simultanée des trois dimensions de l\u0027information géogra-phique.\nLe format numérique offre désormais de multiples potentialités pour représenter des phénomènes spatio-temporels sous des formes animées et/ou dynamiques (cf. Andrienko and Andrienko 2005). Ces représentations privilégient généralement la dimension spatiale au détriment de la dimension temporelle ou inversement. Il est donc possible de différencier ces formes visuelles en fonction de leur position le long d\u0027un continuum espace / temps. À une extrémité de ce continuum se situent des représentations à forte dominance spatiale telles que Google Earth -qui offre désormais la possibilité de visualiser des séries temporelles -ou encore TimeMap qui permet de représenter des séries temporelles à partir de cartes conventionnelles (Johnson 2004). Le développement de ces applications augure d\u0027une généralisation de l\u0027intégration du temps dans les représentations cartographiques conventionnelles, malgré la complexité que cette intégration implique au niveau de la structure des bases de données géographiques (Peuquet 1994 ;Sellis 1999 ;Beard 2004).\nCes applications privilégiant la dimension spatiale sont particulièrement bien adaptées pour l\u0027étude de phénomènes pour lesquels la distance physique est primordiale tels que ceux caractérisés par une auto corrélation spatiale forte (ex. hauteurs de précipitations). En revanche ils sont beaucoup moins pertinents dans les contextes nombreux pour lesquels les relations géographiques ne sont que partiellement influencées par la proximité spatiale tels que les échanges commerciaux de matières premières.\nLes applications situées à l\u0027autre extrémité du continuum espace / temps, c\u0027est-à-dire celles privilégiant la dimension temporelle, fournissent ici des solutions intéressantes à l\u0027image de l\u0027application Gapminder (Gapminder 2007) qui présente sous une forme innovante, interactive, animée, ludique et extrêmement sophistiquée des séries statistiques temporelles à l\u0027échelle du monde. Cette application qui utilise le logiciel Trendalyzer couplé à une interface développée en flash privilégie l\u0027étude des changements temporels et des similarités entre entités géographiques distantes spatialement (Figure 1 3 Un cyber cartogramme gravitationnel : présentation\nContexte général de la cybercartographie\nUn cartogramme est généralement considéré comme une abstraction de la réalité dans laquelle la surface des entités est proportionnelle a des données autres que géographiques (Dent 1990). Dans le cas du cartogramme présenté dans cet article, ni la taille, ni la forme, ni la localisation latitudinale des objets géographiques représentés -en l\u0027occurrence les pays du monde -n\u0027ont été conservées. Ce cartogramme a été conçu dans le cadre d\u0027un large projet de recherche intitulé « Cybercartographie et la nouvelle économie » mené à l\u0027université Carleton (Ottawa, Canada). Ce projet explore le concept de cybercartographie introduit et déve-loppé par Fraser Taylor (1997Taylor ( , 2003Taylor ( , 2005Taylor and Caquard 2006). Ce concept vise à fournir des éléments de réponse aux multiples défis contemporains auxquels fait face la cartographie, que ce soit du point de vue technologique, représentationnel, conceptuel, ou socié-tal. Ces défis sont notamment étudiés à travers le développement de différents atlas cybercartographiques. Ces atlas ont été conçus pour aborder les questions relatives à la mise à disposition d\u0027informations géospatiales sur Internet sous forme multimédia, multi sensorielle, interactive et animée. Ils sont conçus pour être évolutifs et pour être développés de manière collaborative. Ils offrent en effet la possibilité à des individus et communautés de concevoir, créer et gérer leur propre atlas dédié aux thématiques et lieux de leurs choix.\nLe cyber cartogramme a été initialement développé pour l\u0027atlas cybercartographique du commerce canadien (cf. Eddy and Taylor 2005). L\u0027objectif principal de cet atlas est de faire émerger des tendances et changements en termes d\u0027échanges commerciaux à partir de l\u0027analyse d\u0027une base de données complexe et volumineuse. Cette base de données, compilée au fil des ans par Statistiques Canada, comprend la valeur en dollars canadiens des exportations et importations du Canada en direction de 120 pays du monde, pour une cinquantaine de critères sur une période de 25 ans , ce qui correspond à une base de données volumineuse.\nLes représentations cartographiques conventionnelles se sont rapidement révélées limitées pour la représentation simultanée des dimensions spatiales, temporelles et thématiques de ces données. En effet, la prégnance visuelle des formes géographiques et des distances physiques inhérentes aux cartes conventionnelles est apparue surdimensionnée par rapport à leur importance réelle dans les échanges commerciaux. Par exemple, malgré sa proximité relative, la Russie ne constitue pas pour autant un partenaire commercial privilégié du Canada. Si la distance physique reste un des éléments affectant les échanges commerciaux, ceuxci sont aussi largement influencés par d\u0027autres critères non spatiaux tels que les coûts énergé-tiques, les choix politiques ou stratégiques. La notion de « proximité économique » s\u0027est donc avérée beaucoup plus pertinente que celle de proximité spatiale pour l\u0027analyse des échanges commerciaux. Cette notion de proximité économique privilégie les caractéristiques commerciales au détriment de la distance géographique. Deux pays distants géographique-ment peuvent être proches économiquement du fait de leurs similarités commerciales. C\u0027est cette volonté de représenter visuellement cette proximité économique qui a inspiré la conception du cyber cartogramme.\nCe cyber cartogramme a été conçu à l\u0027aide de Nunaliit (http://nunaliit.org), un logiciel source ouverte développé spécifiquement pour la création d\u0027atlas cybercartographiques. En Inuktitut, qui est le nom donné par les canadiens au dialecte Inuit, Nunaliit signifie « communauté », « implantation », « habitat ». Le choix de ce nom pour l\u0027infrastructure cybercartographique illustre d\u0027une part la dimension canadienne du projet et d\u0027autre part sa finalité communautaire. Nunaliit est un logiciel qui aspire à être développé par une communauté d\u0027informaticiens afin de permettre à des communautés d\u0027auteurs de créer leurs propres atlas et de les mettre à la disposition de l\u0027ensemble des communautés d\u0027utilisateurs intéressés. Nunaliit a été conçu à partir de concepts et modèles développés tout au long du projet Cybercartographie et la Nouvelle Economie. Il intègre une partie des résultats obtenus au cours de ce projet. Ce logiciel vise à permettre à des auteurs sans connaissances informatiques poussées, de combiner relativement aisément des données géospatiales disponibles en ligne avec différents médias (ex. son, texte, narration, vidéo), de manière à produire de nouvelles formes d\u0027exploration et d\u0027expression géospatiale. Ces formes d\u0027expression sont généralement regroupées au sein de « modules » qui correspondent à des composantes de l\u0027atlas dédiées à l\u0027étude d\u0027un lieu et/ou d\u0027un thème spécifique et qui combinent pour cela des éléments cartographiques, narratifs et multimédias. Nunaliit permet la création, la mise à jour et l\u0027interconnexion des modules correspondants à autant de chapitres ou sections d\u0027un même atlas.\nDe manière un peu plus spécifique, Nunaliit se présente comme un kit de développement logiciel composé d\u0027un schéma, d\u0027une librairie d\u0027outils source ouverte et d\u0027un compilateur. Le schéma dicte la structure des documents XML au sein desquels l\u0027auteur de l\u0027atlas définit les cartes, les géovisualisations, le texte et les éléments multimédias nécessaires à la création de son module. Chaque document XML comporte les liens et actions associés à chaque module. Ces liens et actions sont générés à l\u0027aide d\u0027une librairie d\u0027outils incluant aussi bien des outils courants des SIG (ex. le zoom), que des outils beaucoup plus novateurs tels que la géosonori-sation (cf. Brauen 2006 ; Caquard et al. à paraître) ou le cyber cartogramme. C\u0027est donc au sein du document XML que le contenu, la structure et la représentation du module sont défi-nis par l\u0027auteur. Une fois créé ou modifié, le fichier XML est ensuite compilé par le compilateur qui le transforme en fichiers HTML dynamiques (DHTML) et SVG, produisant ainsi des documents hautement interactifs générés à partir de différentes applications géomatiques en ligne (cf. Figure 2)  \nLes composantes du cyber cartogramme\nLe cyber cartogramme correspond donc à un des outils de la librairie cybercartographique. Il est constitué de trois composantes principales que sont : (1) un espace géographique unidimensionnel représenté sous forme semi-circulaire et centré sur une origine servant de pôle (ex. le Canada) autour duquel viennent graviter (2) des entités géographiques (ex. pays). Ces entités se répartissent autour du demi-cercle en fonction de leur position géographique par rapport à l\u0027origine. Elles se déplacent ensuite le long du rayon du cercle en direction de l\u0027origine en fonction de valeurs attributaires (ex. le volume des échanges commerciaux). Enfin, (3) une ligne de temps interactive permet d\u0027explorer la dimension temporelle de l\u0027information. Chacune de ces entités est générée automatiquement à la volée par le logiciel Nunaliit.\nL\u0027espace géographique est donc matérialisé sous une forme circulaire correspondant à une représentation unidimensionnelle (1D) du monde centrée sur une entité géographique définie. Dans le cas de l\u0027atlas du commerce canadien, le cartogramme est centré sur le Canada et le monde est représenté de manière semi-circulaire (cf. figure 3). Le choix d\u0027une forme semi-circulaire, plutôt que circulaire, s\u0027explique par la position septentrionale du Canada. Le Canada n\u0027ayant pas de partenaires commerciaux au nord, il devenait inutile de représenter cette direction sur le cartogramme. Les partenaires commerciaux du Canada sont représentés par des points qui se répartis-sent autour de l\u0027hémicycle en fonction de la position longitudinale de leurs centroïdes par rapport à celle du Canada (cf. Tab. 1). Ces points correspondent à la deuxième composante du cartogramme. Les pays situés à l\u0027ouest du Canada (Asie) se retrouvent à gauche, ceux situés au Sud (Amérique) se retrouvent en bas et ceux situés à l\u0027est (Europe, Afrique, Moyen Orient) se retrouvent à droite. Seule la dimension longitudinale est utilisée pour représenter l\u0027espace géographique, ce qui permet de réserver la deuxième dimension du plan pour repré-senter des valeurs attributaires. Chaque point se positionne donc sur le rayon du demi-cercle en fonction d\u0027une valeur attributaire : plus cette valeur est élevée et plus le point se rapproche du centre, c\u0027est-à-dire plus le pays qu\u0027il représente est proche économiquement et visuellement du Canada. Dans notre exemple la position de chaque pays est définie par le pourcentage de ses échanges commerciaux avec le Canada dans un domaine spécifique (ex. l\u0027énergie) par rapport à l\u0027ensemble des autres pays pour une année donnée. \nTAB. 1 -Méthode de calcul du positionnement des objets sur le cartogramme.\nDifférentes variables visuelles telles que la taille, la couleur, la transparence ou la valeur peuvent aussi être attribuées à chaque point multipliant ainsi les combinaisons possibles de données représentées simultanément. Dans notre exemple la taille est utilisée pour représen-ter le montant en dollars canadiens des échanges commerciaux de chaque pays avec le Canada (cf. Figure 4). Plus la taille des symboles s\u0027accroît et plus les échanges commerciaux qu\u0027ils représentent sont importants ; plus ces symboles se rapprochent du centre du demicercle et plus le rôle des pays qu\u0027ils représentent est proportionnellement important par rapport aux autres pays. L\u0027auteur peut aussi décider du seuil au-delà duquel les pays apparaissent sur le cartogramme. Ici, les pays totalisant moins de 1% des échanges commerciaux avec le Canada pour chaque année n\u0027apparaissent pas pour des raisons de clarté et de lisibilité.\nFIG. 4 -Chaque pays se positionne sur le rayon du demi-cercle en fonction d\u0027une valeur attributaire (ici le pourcentage du commerce de chaque pays par rapport aux autres pays). La taille du point varie en fonction d\u0027une autre valeur attributaire (ici le volume total des échanges en dollars canadiens). L\u0027utilisateur peut sélectionner les données temporelles à l\u0027aide d\u0027une ligne de temps interactive générée automatiquement à la volée à partir du champ de la base de données sélectionné.\nLa troisième composante du cartogramme est la ligne de temps interactive. Tout comme les deux composantes précédentes cette ligne de temps est générée à la volée à partir de la sélection d\u0027un champ de la base de données comportant des données temporelles. Cette ligne de temps se présente sous la forme d\u0027un rectangle allongé de la longueur de la fenêtre de l\u0027atlas subdivisé en autant de rectangles qu\u0027il y a de valeurs temporelles dans le champ sélec-tionné. Dans notre exemple, la base de données comporte une série temporelle composée de 25 années. La ligne de temps est donc subdivisée automatiquement en 25 rectangles identiques. Chaque fois qu\u0027une nouvelle année est ajoutée dans la base de données, un nouveau rectangle vient s\u0027ajouter automatiquement à la ligne, réduisant ainsi proportionnellement la taille de chacun. Lorsque l\u0027utilisateur clique sur un des rectangles matérialisant une année, les données relatives à cette année apparaissent sur le cartogramme. Lorsque les données temporelles sont trop nombreuses ou correspondent à des échelles temporelles différentes (ex. annuel Vs. mensuel) la ligne de temps peut être dédoublée afin de faciliter la visualisa-tion et l\u0027analyse. La création automatisée de cette ligne de temps à partir de bases de données géographiques diverses, pouvant être distantes, ainsi que son haut niveau d\u0027interactivité, en font un outil extrêmement utile pour l\u0027intégration de la dimension temporelle dans l\u0027analyse géographique.\nLa combinaison de ces trois composantes offre de multiples possibilités et présente diffé-rents avantages. D\u0027un point de vue spatial elle favorise la comparaison intercontinentale (Asie, Etats-Unis, Europe) qui est un élément clé de l\u0027analyse des relations commerciales du Canada. Chaque continent est matérialisé par une agrégation visuelle de différents points représentant chacun un pays. Chaque pays participe donc au regroupement visuel (clustering) nécessaire à la comparaison intercontinentale. À une échelle plus fine, chaque pays peut rapidement être identifié et comparé visuellement et quantitativement à tout autre pays.\nLa ligne de temps interactive permet à l\u0027utilisateur de sélectionner une date de référence et de comparer visuellement la situation à d\u0027autres dates. Le fait que les points se déplacent de manière linéaire le long d\u0027un axe qui leur est propre (le rayon du cercle) accroît leur pré-gnance visuelle et favorise la mise en évidence des changements. En effet, comme le souligne Alan MacEachren (1995) sur une carte dynamique, les éléments qui bougent attirent plus l\u0027attention que ceux qui ne bougent pas et ceux qui se déplacent attirent plus l\u0027attention que ceux qui bougent sur place, c\u0027est-à-dire que ceux qui se dilatent ou se rétractent. L\u0027impression d\u0027animation qui découle du déplacement des points le long des rayons du demi-cercle génère une variable cartographique supplémentaire représentant de manière intuitive les processus dynamiques et stimulant notre capacité à identifier les changements (MacEachren 1994). Enfin, le fait que les composantes du cartogramme (hémicycle, points et ligne de temps) soient indépendantes les unes des autres, tout en étant interconnectées, multiplie les possibilités combinatoires et étend ses domaines potentiels d\u0027application.\nLe cyber cartogramme : applications et discussion\nLes échanges commerciaux canadiens relatifs aux produits de la mer\nUne première phase de l\u0027analyse exploratoire des échanges commerciaux du Canada a été de mettre en évidence les principales tendances spatio-temporelles pour chacune des théma-tiques étudiées. Pour cela, une première version du cartogramme a été utilisée. Elle comportait deux hémicycles (importations et exportations) sur lesquels se répartissaient les pays agrégés par grandes régions géographiques (Etats-Unis, Amérique du Sud, Afrique, Europe, Moyen Orient, Asie, Pacifique). Ce cartogramme a été utilisé pour analyser les 15 principales thématiques de la base de données de Statistiques Canada. Ce cartogramme exploratoire a permis de faire rapidement émerger différentes tendances spatio-temporelles du commerce canadien comme les variations interannuelles importantes dans l\u0027origine de l\u0027approvisionnement énergétique du Canada, comparées à la stabilité de ses exportations dans ce domaine en direction des Etats-Unis. Cette première version a donc permis d\u0027identifier certaines thématiques pour lesquels un approfondissement de l\u0027analyse pourrait s\u0027avérer pertinent, ce qui a été fait de manière plus spécifique pour le commerce des produits de la mer.\nAu Canada, l\u0027industrie de la pêche commerciale représente annuellement plus de 5 milliards de dollars canadiens, ce qui en fait un des principaux producteurs mondiaux. Le Cana--10 -RNTI-E-13 da exporte près de 85% de ses produits de la pêche et importe annuellement environ 2 milliards de dollars ce qui contribue à faire de ce secteur un domaine économique stratégique (Agriculture et Agroalimentaire Canada 2007). Ce secteur est aussi fondamental d\u0027un point de vue social puisqu\u0027il emploie plus de 130 000 personnes et que 207 communautés à travers le pays étaient encore dépendantes de la pêche en 2001 (Atlas du Canada 2007). Un cyber cartogramme dédié à l\u0027analyse des échanges commerciaux par pays a donc été conçu et inté-gré dans un module spécifique de l\u0027atlas du commerce canadien dédié à cette thématique. Ce cartogramme a permis de faire apparaître de nombreuses tendances intéressantes telles que la réduction relative des exportations en direction de l\u0027Europe au profit principalement du Japon à la fin des années 90 ou encore l\u0027accroissement constant des importations en provenance de la Thaïlande durant la période 1985-1994 (cf. Figure 5).\nFIG. 5 -Copie d\u0027écran du cartogramme intégré dans le module de l\u0027atlas du commerce canadien traitant de l\u0027industrie de la pêche (http://gcrc.carleton.ca/graphomap ). Dans cet exemple, l\u0027année sélectionnée est 1976 (points de couleur foncée) et elle est comparée à 2000 (points plus clairs) pour les exportations (en haut) et les importations (en bas). La taille des points représente les montants (en dollars canadiens) alors que la proximité du centre représente le pourcentage de chaque pays dans la valeur commerciale annuelle du Canada. Dans la partie droite de la fenêtre, apparaissent notamment les valeurs numériques des entités sélectionnées.\nAu-delà de ces résultats, la capacité du cartogramme à faire émerger ces tendances a suscité l\u0027intérêt de nombreux utilisateurs. Ce cartogramme a ainsi été utilisé de manière quasi exclusive par l\u0027experte en commerce international chargée d\u0027analyser les données et de mettre en évidence des structures et tendances. Ce cartogramme a par ailleurs fait l\u0027objet d\u0027une évaluation informelle qui s\u0027est déroulée le 27 novembre 2007 dans les locaux de Statistiques -11 -RNTI-E-13 En effet, si ce cartogramme a été conçu pour l\u0027analyse géographique de proximités éco-nomiques, il peut tout aussi bien être utilisé pour l\u0027analyse de proximités culturelles, linguistiques, démographiques, sociales, économiques, environnementales ou écologiques. Celles-ci peuvent concerner des pays, aussi bien que des communautés (ex. communautés autochtones), des entités géographiques cohérentes (ex. agglomérations urbaines), administratives (ex. régions), environnementales (ex. parcs naturels), ou écologiques (ex. biotopes). La multiplicité des formes de proximité pouvant ainsi être générées favorise l\u0027émergence de nouvelles perspectives sur les relations existantes entre des entités spatiales diverses et distantes.\nLes limites du cyber cartogramme\nMême si les possibilités combinatoires des dimensions thématiques, temporelles et spatiales qu\u0027offre ce cartogramme apparaissent illimitées, cette application n\u0027en est pas pour autant dénuée de limites. La première limite de ce cartogramme concerne la représentation d\u0027objets géographiques de manière unidimensionnelle. Cette approche comporte en effet un risque évident de superposition d\u0027entités éloignées latitudinalement mais situées sur la même longitude. C\u0027est par exemple le cas entre l\u0027Europe et l\u0027Afrique et plus particulièrement entre des pays aussi distants géographiquement, économiquement, historiquement et culturellement que la Finlande et la République Démocratique du Congo. Ce problème reste néan-moins marginal dans notre application étant donnée la faible part des échanges commerciaux du Canada avec l\u0027Afrique et par conséquent la présence limitée des pays Africains sur le cartogramme. Ce problème pourrait par ailleurs facilement être résolu par l\u0027utilisation de couleurs ou formes distinctes pour différencier des entités géographiques se situant à des latitudes éloignées, ou sur des continents différents.\nUne deuxième limite concerne les changements subits par les entités géographiques au cours du temps. Pendant les 25 années étudiées, certains pays ont disparu (ex. Yougoslavie), d\u0027autres sont apparus (ex. Bosnie Herzégovine) et d\u0027autres enfin se sont transformés (ex. l\u0027URSS est devenue la Russie). Ces évolutions ont affecté les objets géographiques (ex. déplacement des centroïdes), et par conséquent leur position sur le cartogramme. Par exemple, au cours de la transformation de l\u0027URSS en Russie le centroïde de ce pays s\u0027est légère-ment déplacé vers l\u0027est du fait de ses modifications territoriales, ce qui a affecté radicalement sa position sur le cartogramme : de manière quelque peu paradoxale, la Russie s\u0027est retrouvée à l\u0027extrême gauche du cartogramme alors que l\u0027URSS était positionnée à l\u0027extrême droite. Ce type de problème peut aussi être aisément réglé (ex. déplacement manuel du centroïde ou affectation d\u0027un coefficient spatial) et demeure de toute façon inhérent à toute représentation spatiale de séries temporelles.\n-12 -RNTI-E-13\nEnfin une dernière limite concerne la complexité relative de la mise à jour des représenta-tions. Pour l\u0027instant, les efforts concernant le développement du logiciel Nunaliit ont été principalement dévolus à assurer sa stabilité et son efficacité. Si l\u0027objectif final est d\u0027offrir aux auteurs d\u0027atlas cybercartographiques une interface facile d\u0027utilisation, il reste encore beaucoup de chemin à parcourir dans ce domaine. En effet, la création de modules se fait actuellement à partir d\u0027un fichier XML qu\u0027il faut modifier et compiler (cf. Figure 6). Par conséquent, la mise à jour des modules, et donc du cartogramme, nécessite une connaissance de base du langage XML. Même si l\u0027utilisation de ce langage s\u0027avère de plus en plus courante dans les applications géomatiques destinées à l\u0027Internet, il n\u0027en reste pas moins que son usage peut rebuter nombre de créateurs potentiels d\u0027applications cybercartographiques. Ce problème devrait être résolu sous peu grâce au développement d\u0027une interface de type wiki, matérialisant ainsi la véritable finalité communautaire de Nunaliit et des atlas cybercartographiques en général.\nFIG. 6 -Exemple du fichier XML utilisé pour générer le cartogramme représentant les échanges commerciaux canadiens appliqués aux produits de la mer (cf. Figure 5).\nConclusion\nMalgré les quelques limites qui viennent d\u0027être évoquées, le cyber cartogramme présenté dans cet article constitue un véritable outil de géovisualisation, original et innovant permettant de représenter de manière simple, intuitive et automatisée, les trois composantes interreliées du schéma « triad spatio-temporel » -où, quand, quoi -à partir de données variées et interopérables. De plus, ce cartogramme développé en source ouverte offre la possibilité de décliner ces combinaisons sur de nombreux modes, permettant ainsi d\u0027explorer de multiples formes de proximités appliquées à divers domaines. Cette diversité des applications potentielles de ce cartogramme est illustrée par son utilisation au sein de l\u0027atlas cybercartographique du cinéma canadien en cours de développement (www.atlascine.org). Dans le contexte de cet atlas, le cartogramme est utilisé pour analyser l\u0027évolution spatio-temporelle des recettes générées par différents films canadiens lors de leur sortie en salle. Ce cartogramme se caractérise donc par sa simplicité d\u0027utilisation, son évolutivité et son adaptabilité aux besoins de ses utilisateurs potentiels, mais aussi par la multiplicité de ses domaines d\u0027application.\nEnfin, il est important de rappeler que la conception de ce cartogramme participe d\u0027une logique de diversification des modes de représentation de l\u0027information géographique de manière à favoriser : (1) l\u0027exploration des structures et phénomènes géographiques sous différents angles à travers différentes formes de proximité ; (2) la revalorisation de l\u0027abstraction comme moyen de stimuler l\u0027intérêt et l\u0027imagination des utilisateurs d\u0027informations géographiques. Cette logique vise à fournir une alternative aux modèles ré-alistes dominants dans les sciences de l\u0027information géographique et stimuler ainsi l\u0027émergence de nouveaux modes d\u0027expression géospatiale ; et (3) une approche communautaire de la construction du savoir géographique et des outils de géovisualisation permettant cette construction et sa diffusion. \nRéférences\nSummary\nThe cartogram introduced in this paper has been designed to improve visual analysis of complex spatio-temporal data. It provides the means for representing simultaneously the three dimensions intrinsic to geospatial information: space (or location), theme (or event) and time (specific moments). The cartogram does so using three major components: (1) a onedimensional (1D) geographical space represented by a semi-circle centered on a specific location (e.g. Canada); (2) certain geographical entities (e.g. countries) that gravitate around this location according to specific attributes; and (3) an interactive timeline offering the possibility to explore the temporal dimension of the information. Together these three components provides a range of possibilities for analyzing different forms of spatio-temporal proximity, including proximity expressed in economic, cultural, social and demographic terms.\nThe potential this open source cartogram offers is illustrated by examples from the Cybercartographic Atlas of Canada Trade with the World. This article is based on a paper given at the SAGEO 2007 conference.\n"
  },
  {
    "id": "882",
    "text": "Introduction\nLes mesures de similarité sont des éléments clés dans les algorithmes de traitement automatique des langues. Elles sont utilisées pour orienter le processus d\u0027extraction de connaissance. Ainsi, elles sont les principales responsables des performances d\u0027un algorithme. Si une mesure de similarité pertinente améliorera les performances, une mauvaise mesure risque de mener à des résultats incohérents. La définition d\u0027une bonne mesure n\u0027est pas un processus aisé. En effet, la mesure doit donner une bonne indication sur le degré de similarité entre deux documents. La notion de sémantique n\u0027est pas clairement définie. Bien que nous essayons d\u0027imiter la perception humaine, l\u0027information sémantique peut prendre différente forme selon l\u0027approche adoptée. Il existe deux grandes approches : l\u0027une basée sur l\u0027information statistique tel que la fréquence de co-occurrence des termes et l\u0027autre basée sur des sources de connaissances externes telles que les ontologies.\nDans la communauté de l\u0027apprentissage, les noyaux (Shawe-Taylor et Cristianini, 2004) sont utilisés depuis une décennie comme fonctions de similarité basées sur le cosinus formé par deux vecteurs. Les noyaux sont, en réalité, des produits scalaires définis dans un espace de Hilbert. Ils ont la spécificité de plonger, implicitement, les données dans un espace de Hilbert, dit espace de description, avant de calculer le produit scalaire. Les noyaux peuvent être intégrés dans tout algorithme d\u0027apprentissage basé sur le produit scalaire tel que les Séparateurs à Vaste Marge (SVM) (Vapnik, 1995). Ainsi, ils étendent l\u0027utilisation de l\u0027apprentissage numérique aux tâches du TAL (Traitement Automatique des Langues). En effet, aucune contrainte n\u0027étant imposée sur l\u0027espace des données, les noyaux peuvent être définis pour tout type de données tel que les données textuelles. Le modèle d\u0027espace vectoriel (VSM) (Salton et al., 1975), repré-sentant un document sous la forme d\u0027un vecteur de fréquences de termes, est largement utilisé. Les noyaux basés sur ce modèle ont permis d\u0027obtenir des résultats très prometteurs dans le domaine de la catégorisation de texte (Joachims, 2002(Joachims, , 1998.\nDans cet article, nous présentons un modèle d\u0027espace vectoriel de concepts (CVSM) pour la représentation des documents textuels. Cette représentation, induite par des connaissances a priori, est présentée, ici, comme une alternative au modèle classique d\u0027espace vectoriel. Le VSM se base uniquement sur la fréquence d\u0027occurrence des termes. Pour le CVSM, une taxonomie de concepts linguistiques est utilisée comme source de connaissance pour définir l\u0027espace vectoriel. De plus, nous proposons deux noyaux basés sur les concepts. Le premier noyau, le noyau CVSM linéaire, est défini dans le CVSM où chaque document est représenté par un vecteur de concept intégrant l\u0027information sur la taxonomie des concepts. Le second noyau, le noyau CVSM latent, mélange l\u0027approche agnostique basée sur l\u0027information statistique et l\u0027approche a priori utilisant des connaissances externes propres au domaine. Basé sur le noyau LSA (Cristianini et al., 2002), le noyau CVSM latent utilise une décomposition en valeurs singulières (SVD) pour découvrir des structures latentes entre les concepts linguistiques du CVSM.\nL\u0027utilisation des noyaux CVSM est illustrée par une tâche de catégorisation de texte dans le domaine biomédical. L\u0027Unified Medical Language System (UMLS) est utilisé tant que source a priori de connaissances biomédicales pour l\u0027extraction de concepts à partir documents textuels. Les performances de ces noyaux sont évaluées sur cette tâche en utilisant un classifieur SVM. Le corpus Ohsumed qui est connu pour être un corpus difficile, est utilisé pour l\u0027évaluation expérimentale.\nPour raffiner le VSM, l\u0027Analyse Sémantique Latente (LSA) (Deerwester et al., 1990) a été proposée. L\u0027idée principale est de représenter un document dans le VSM non pas par ses termes mais par ces concepts. L\u0027hypothèse est que les concepts sont plus adaptés pour modéliser le sens des documents que les termes. Dans la LSA, un espace sémantique de faible dimension est défini en appliquant une Décomposition en Valeurs Singulières (SVD) sur la matrice de termes par document. Les vecteurs documents du VSM sont alors projetés dans l\u0027espace sémantique. En utilisant ce nouvel espace, un Noyau Sémantique Latent a été proposé dans (Cristianini et al., 2002). Ce noyau a été utilisé pour projeter les documents du VSM vers l\u0027espace sémantique et calculer le produit scalaire. Le noyau a été utilisé avec succès sur une tâche de catégorisation de texte. Il a été montré que le noyau LSA peut atteindre les mêmes performances, dans un espace de très faible dimension, que le noyau du VSM utilisé dans (Joachims, 2002(Joachims, , 1998).\nBien que l\u0027espace sémantique de la LSA permet d\u0027obtenir de très bonnes performances, l\u0027espace étant défini par des concepts statistiques, il est très difficile, linguistiquement, d\u0027interpréter cet espace et les concepts.\nNoyau linéaire du modèle d\u0027espace vectoriel de concepts\nPré-traitement des documents\nLes documents textuels sont formatés pour une utilisation humaine. Ainsi, ils contiennent une riche variété de symboles et de protocoles tels que les règles typographiques. Ces informations sont ajoutées pour rendre la lecture et la compréhension plus aisées. Toutefois, le traitement automatique de ces données est rendu compliqué. En effet, si les éléments d\u0027un document ne sont pas correctement gérés, ils peuvent être une source de bruits et d\u0027ambiguïtés qui entraînera inexorablement une baisse des performances du système. Une façon d\u0027éviter ce problème est de pré-traiter le document pour avoir une représentation adaptée au système de traitement.\nDans le cadre de ce travail, nous utiliserons le pré-traitement suivant : 1. Nettoyage du document :Tous les éléments qui peuvent introduire du bruit sont éliminés. Ainsi, tous les nombres, aussi bien sous un format numérique que sous un format littéral et les données non-textuelles sont retirés. Il est à noter que nous utilisons le terme \"bruit\" dans un sens général. 2. Segmentation du texte : Le document est segmenté en unité lexicale composée d\u0027un ou plusieurs mots. Pour cela, un lexique doit être utilisé. Dans notre, cas, le lexique médical \"The Specialist Lexicon\" de l\u0027UMLS a été utilisé.\nSuppression des mots vides :\nLes unités lexicales ne contenant que des mots vides, à savoir, sans signification sont éliminés. 4. Normalisation des termes : Pour éviter les différentes formes fléchies des mots, un processus de normalisation est effectué à l\u0027aide d\u0027un lexique. La normalisation consiste à lemmatiser chaque mot d\u0027une unité lexicale et à les ordonner alphabétiquement au sein de cette unité. Les unités lexicales absentes du lexique sont décomposées en unité lexicale composée d\u0027un seul mot. Un processus de stemming est ensuite appliqué à chacun de ces mots. 5. Annotation Sémantique : Chaque unité lexicale est associée à un groupe de concepts (composé d\u0027un ou plusieurs concepts). Cette étape nécessite l\u0027utilisation d\u0027un thésaurus. En outre, une taxonomie de relation \"est-un\" entre les concepts sera utilisée lors de la phase de traitement. Dans le cadre de notre travail, le Metathesaurus de l\u0027UMLS a été utilisé. Ce thésaurus intègre une ontologie de concepts complexe. L\u0027ontologie a été transformée en taxonomie en ne conservant que les relations \"est-un\" et en supprimant les cycles.\nLe modèle d\u0027espace vectoriel de concepts\nLe modèle d\u0027espace vectoriel (VSM) est basé sur la forme morphologique des termes. Il est, ainsi, hautement dépendant de la langue du texte et de la fréquence d\u0027occurrence des termes. En effet, le VSM utilise simplement la fréquence des termes pour capturer l\u0027information d\u0027un document. Il est, ainsi, limité par le fait qu\u0027il ne peut correctement gérer les termes synonymes et les termes polysémiques. De plus, les liens entre les termes sémantiquement proches ne peuvent être modélisés. Toutefois, l\u0027espace à haute dimension induit par le VSM permet aux systèmes, utilisant le VSM, d\u0027obtenir des performances qui sont parmi les meilleures (Joachims, 1998). Dans cette section, nous présentons le modèle d\u0027espace vectoriel de concepts (CVSM), basé sur le VSM. Ce modèle devrait permettre de gérer les problèmes, rencontrés par le VSM, énumé-rés ci-dessus. Le CVSM est un espace vectoriel dans lequel chaque axe représente un concept défini dans un dictionnaire de concepts. Le dictionnaire est constitué des concepts définis dans un thésaurus et des mots racines, obtenus par stemming, lorsque ces mots ne peuvent être associés à des concepts. Nous prenons, alors, l\u0027hypothèse que ces mots expriment un concept à part entière. Dans le CVSM, les documents sont pré-traités selon la méthode décrite plus haut. Une fois qu\u0027un document d a été pré-traité, chaque unité lexicale l du document d est associée à un vecteur de concepts local ?(l). Le i ème composant ? i (l) de ?(l) associé au concept c i est alors donné par :\noù N (t) est l\u0027ensemble de termes normalisés d\u0027une unité lexicale l et ? n i (t) et le i ème composant, associé au concept c i , du vecteur de concept ? n (t) pour le terme normalisé t. ? n i (t) est défini par :\nC(t) est l\u0027ensemble des concepts associés à t, P(c) est l\u0027ensemble des chemins allant du concept c au concept racine dans la taxonomie, c\u0027est à dire les chemins allant du concept spéci-fique c au concept le plus général, d p (c i , c j ) est la distance entre le concept c i et le concept c j sur le chemin p et ? ? [0, 1] est une valeur exprimant la puissance de décroissance. ? est utilisé pour décroître l\u0027influence des concepts généraux sur la représentation d\u0027un terme. En effet, étant donné un terme t, un concept c associé à t, et p un chemin allant de c au concept le plus général (le concept racine), les concepts proches, en terme de distance, de c fourniront le sens principal de t comparés aux concepts éloignés de c. Les concepts généraux n\u0027expriment pas clairement le sens pertinent de t et peuvent même mener à des ambiguïtés voire du bruit. Par conséquent, il peut être intéressant de diminuer le pouvoir expressif des concepts selon leurs distances par rapport au concept spécifique c. En fixant ? à un, nous pouvons considérable-ment réduire l\u0027expressivité des concepts généraux et en fixant ? à zéro, nous pouvons donner le même pouvoir d\u0027expression à tous les concepts en ne faisant aucune différence entre les concepts généraux et spécifiques. ? doit être fixé de manière empirique en fonction du corpus.\nÉtant donné un ensemble de vecteurs de concepts locaux {?(l 1 ), . . . , ?(l n )} pour un document d, le vecteur de concept global ?(d) pour d est donné par la formule suivante :\nLa normalisation dans l\u0027équation 4 est utilisée pour représenter des documents de longueur différente avec une même échelle. De plus, les concepts qui apparaissent dans beaucoup de documents du corpus, ne fourniront pas d\u0027information utile pour la discrimination de documents alors que les concepts rares dans un corpus peuvent être significatifs. Par conséquent, nous proposons d\u0027utiliser un vecteur pondéré ? IDF (d) en utilisant la pondération IDF (Inverse Document Frequency). Ce vecteur est défini tel que :\noù N est le nombre de document dans le corpus et D(c i ) est l\u0027ensemble des documents du corpus contenant le concept c i . La figure 1 illustre la modélisation d\u0027un document textuel dans le CVSM.\nLe noyau linéaire\nNous définissons le noyau linéaire dans le modèle d\u0027espace vectoriel de concept comme ceci :\n4 Le noyau CVSM latent L\u0027analyse sémantique latente permet de mettre à jour des relations entre des termes. Ces relations sont extraites par une décomposition linéaire en valeurs singulières de la matrice des fréquences de termes par document. Elles sont par conséquent des relations de co-occurrences des termes dans les documents. Les relations extraites sont appelés \"concepts latents\". La LSA permet, ainsi, non seulement de diminuer la dimension de l\u0027espace en ne conservant que les relations les plus importantes mais aussi de gérer la polysémie et la synonymie.\nDans un même esprit, nous proposons, dans ce papier, l\u0027utilisation de la LSA dans le CVSM pour capturer les structures latentes entre les concepts. Les concepts LSA peuvent être perçus\nFIG. 1 -La représentation d\u0027un document texte selon le modèle d\u0027espace vectoriel de concepts.\ncomme des concepts abstraits de haut niveau. Il peut être intéressant d\u0027analyser expérimenta-lement l\u0027effet du mélange entre concepts linguistiques et concepts statistiques. Nous définissons la matrice M de concept par document dans le CVSM comme ceci :\navec m la dimension du CVSM (le nombre total de concepts) et n le nombre de documents. La LSA étant une méthode non-supervisée, à savoir que la méthode n\u0027utilise pas l\u0027information sur les étiquettes des documents, nous utiliserons les documents étiquetés (documents d\u0027apprentissage) et les documents non-étiquetés (documents de test) pour M. La SVD de M donne :\nNous dénotons par U k , les k vecteurs singuliers associés aux k valeurs singulières les plus élevées et ? k la matrice diagonale contenant les valeurs singulières. La projection du vecteur ? IDF (d) du CVSM dans l\u0027espace sémantique latente est donnée par :\nLe noyau CVSM latent est alors défini par :\nÉvaluation expérimentale\nNous avons mené plusieurs expérimentations sur un corpus médical pour évaluer la représentation CVSM. Nous avons utilisé le modèle d\u0027espace vectoriel standard, VSM, comme base de comparaison. Dans cette section, nous présentons le corpus médical utilisé, le mode opératoire pour les expérimentations et un résumé des résultats expérimentaux.\nLe corpus Ohsumed\nLes expérimentations ont été menées sur le corpus Ohsumed qui est un corpus médical contenant 6286 documents d\u0027apprentissage et 7643 documents de test (Hersh et al., 1994). Les documents sont des résumés d\u0027articles de médecine issus de la base bibliographique médicale MEDLINE. Chaque document est, ou doit être dans le cas des documents de test, étiqueté avec une ou plusieurs des 23 étiquettes. Les étiquettes correspondent à des catégories cardiovasculaires. La tâche de catégorisation sur ce corpus est connue pour être difficile. En effet, les systèmes de catégorisation qui fonctionnent relativement bien sur les corpus tels que le Reuters-21578 et le 20-NewsGroups voient leurs performances diminuées. Par exemple, dans (Joachims, 1998), le classifieur linéaire SVM sur la VSM atteint une performance de 65.9% alors qu\u0027il atteint une performance de 86% sur le corpus Reuters 21578. Les difficultés de catégorisation sont dues au fait que les données sont bruitées avec des termes médicaux très spécifiques et que les catégories ont un haut degré de corrélation.\nLa préparation des expérimentations\nDans toutes nos expérimentations, les documents ont été pré-traités pour la représentation selon le modèle d\u0027espace vectoriel de concepts (CVSM). Pour le modèle d\u0027espace vectoriel, VSM, nous avons utilisé le stemming pour réduire les mots fléchis à leurs bases communes. Nous avons, en outre, éliminés les mots vides. Pour le stemming, nous avons utilisé l\u0027algorithme de Porter (Porter, 1980). Pour la gestion du problème à multi-catégories, nous avons utilisé la stratégie \"un-contretous\". Cette stratégie a mené à la décomposition du problème principal en 23 sous-problèmes de catégorisation binaire. La librairie libSVM (Chang et Lin, 2001) a été utilisée pour l\u0027apprentissage des classifieurs SVM.\nAfin d\u0027évaluer le CVSM, nous avons utilisé le VSM comme base de comparaison. Nous avons utilisé un noyau linéaire, dans le VSM, avec une pondération TF-IDF et une normalisation des vecteurs selon le mode opératoire defini dans (Joachims, 1998). Ce noyau est nommé \"noyau Sac-de-mots\"(Bag Of Words -BOW -kernel). De plus, nous avons aussi utilisé un noyau LSA dans le VSM en utilisant l\u0027équation 10. Ce noyau LSA est utilisé comme base de comparaison pour le noyau CVSM latent. Nous utiliserons la dénomination \"BOW SVD\" pour désigner le noyau LSA dans le VSM.\nLa mesure utilisée pour évaluer les performances des classifieurs est, principalement, la mesure F1 (Sebastiani, 2002). La mesure F1 est la moyenne harmonique de la précision et du rappel d\u0027un système de catégorisation.\nÉvaluation de la puissance de décroissance ?\nDans la première expérimentation, nous cherchons, empiriquement, la valeur optimale de la puissance de décroissance ? pour le corpus Ohsumed. Nous rappelons que ? contrôle la manière dont les concepts généraux sont pris en compte dans l\u0027équation 3. Une valeur de zéro donnera un poids égal aux concepts généraux et spécifiques. Pour cette expérimentation, nous utilisons uniquement 10% des données d\u0027apprentissage et 10% des données de test pour éva-luer la performance. De plus, nous utilisons un échantillonnage stratifié pour conserver les proportions. La figure 2 montre les scores micro-F1 pour différentes valeurs de ? ? [0, 1]. Le meilleur score est obtenu pour ? \u003d 0.1 avec une valeur micro-F1 de 53.9%. En outre, une meilleure performance est obtenue pour ? \u003d 0 que pour ? \u003d 1. Ce point signifie que les concepts généraux jouent un rôle important dans la tâche de catégorisation pour le corpus Ohsumed. C\u0027est, effectivement, le cas lorsque plusieurs mots, avec un sens general commun, sont utilisés dans différents documents d\u0027une même catégorie.\nFIG. 2 -La variation de la micro-F1 en fonction du pouvoir de décroissance ?. Les résultats sont obtenus en utilisant 10% des données d\u0027apprentissage et 10% des données de test.\nÉvaluation du nombre de concepts latents\nPour les noyaux basés sur la LSA, la dimension de l\u0027espace sémantique doit être fixée empiriquement. Par conséquent, nous avons mené un ensemble d\u0027expérimentations sur le corpus entier en faisant varier le nombre de concepts latents, à savoir le nombre de vecteurs singuliers associés aux valeurs singulières les plus élevées. La figure 3 montre la variation du score micro-F1. Les noyaux CVSM latent et BOW SVD atteignent leurs performances quasioptimales pour approximativement 2000 concepts latents. En fait, il y a une croissance rapide des performances pour un nombre de concepts compris allant de 0 à 1000. Ceci montre que les 1000 premiers vecteurs singuliers fournissent l\u0027information principale pour décrire les documents. Puis, la croissance des performances diminue pour un nombre de concepts de 1000 à 2000, indiquant ainsi la présence d\u0027une faible quantité d\u0027information. Les performances du noyau CVSM latent sont meilleures de près de 2% par rapport au noyau BOW SVD. De plus, les différences sont plus prononcées pour un nombre de dimensions faible. Ceci signifie que le noyau CVSM latent est capable de capturer et d\u0027exprimer l\u0027information principale dans un espace de faible dimension, c\u0027est à dire que l\u0027information principale est résumée par un faible nombre de vecteurs singuliers. Pour le reste des expérimentations, nous avons fixés le nombre de concepts latents à 3000.\nFIG. 3 -La variation de la micro-F1 en fonction du nombre de concepts latents.\nÉvaluation des noyaux\nLes performances des noyaux sur le corpus Ohsumed sont reportées dans les tableaux 1 et 2. Les expérimentations ont été réalisées sur l\u0027ensemble des données sans sélection de termes. Les noyaux dans le CVSM obtiennent de meilleurs résultats, jusqu\u0027à 2% de plus, que les noyaux dans le VSM. Comme attendu, les noyaux LSA ont de meilleurs performances que les noyaux linéaires. Ceci est dû au fait que les concepts abstraits de haut niveau de l\u0027espace sémantique résument l\u0027information principale et réduisent le bruit. En outre, le noyau CVSM linéaire est plus performant que le noyau BOW SVD. Nous en déduisons que les concepts basés sur l\u0027ontologie (les concepts linguistiques) sont plus adaptés pour exprimer le sens des documents médicaux que les concepts abstraits (les concepts statistiques) obtenus par la dé-composition linéaire de la LSA. \nNoyau\nRéduction de la quantité de données d\u0027apprentissage\nLa figure 4 montre l\u0027impact de la quantité de données utilisées pour l\u0027apprentissage sur les performances des noyaux. Comme précédemment, les noyaux LSA sont plus performants que les noyaux linéaires. Néanmoins, il existe deux points intéressants dans ces résultats. Premiè-rement, le noyau BOW SVD est plus performant que le noyau CVSM linéaire lorsque 30% ou moins des données d\u0027apprentissage sont utilisées. Au delà des 30% le noyau CVSM linéaire est plus performant. Ceci signifie que les concepts statistiques ont un pouvoir discriminatoire supé-rieur au concepts du CVSM pour des petits échantillons de données d\u0027apprentissage. Toutefois, quand la base d\u0027apprentissage est suffisamment importante, les concepts CVSM deviennent plus expressifs. Deuxièmement, tous les noyaux ne réussissent pas à capturer l\u0027information principale avec une faible quantité de données. En effet, les performances ne cessent de s\u0027amé-liorer en fonction de la quantité de données d\u0027apprentissage. Par conséquent, nous pouvons en déduire que chaque document d\u0027apprentissage fournit une nouvelle information qui améliore la performance de catégorisation. Ceci est principalement dû au fait que ce corpus contient des termes spécifiques avec une faible fréquence d\u0027occurrence.\nConclusion\nDans cet article, nous avons présenté un modèle d\u0027espace vectoriel de concepts pour la représentation de documents. Ce modèle utilise des connaissances a priori pour capturer le sens des documents textuels. Nous avons montré une façon simple d\u0027utiliser efficacement les ontologies pour les intégrer au modèle d\u0027espace vectoriel, VSM, standard. Nous avons, aussi, adaptés le noyau linéaire et le noyau LSA pour le CVSM. Nous avons illustré l\u0027utilisation du CVSM par une application au domaine biomédical. Le Metathesaurus de l\u0027UMLS a été utilisé comme source a priori de connaissance pour définir le CVSM.\nLes performances des noyaux CVSM ont été, expérimentalement, évaluées sur une tâche de catégorisation de documents biomédicaux. Les noyaux ont été comparés au noyau sac-demots (BOW) et au noyau LSA. Les résultats ont montré que les noyaux CVSM améliorent\nFIG. 4 -La variation de la micro-F1 en fonction du pourcentage de documents d\u0027apprentissage utilisés.\nles performances de catégorisation de près de 2%. De plus, les expérimentations ont montré que l\u0027utilisation des concepts latents, extraits à partir des concepts linguistiques par une SVD, permettent d\u0027améliorer les résultats.\nPour un travail futur, nous pensons améliorer la représentation CVSM en intégrant une pondération d\u0027attributs plus adaptée que la pondération IDF. Il a été montré dans (Debole et Sebastiani, 2003) que la pondération supervisée des termes pouvait améliorer la catégorisation de documents. En effet, des recherches récentes, en matière de pondération des termes, ont donné des résultats prometteurs (Soucy et Mineau, 2005;Lan et al., 2006Lan et al., , 2007.\n"
  },
  {
    "id": "883",
    "text": "Introduction\nL\u0027interconnexion croissante des systèmes d\u0027information, de même que des initiatives telles que le Web sémantique requièrent la création de nombreuses ontologies pour assurer la cohérence sémantique des opérations. Il devient donc nécessaire de développer des systèmes de gestion qui permettent non seulement de les stocker mais également de les aligner et de les combiner pour créer de nouvelles ontologies adaptées à des besoins particuliers, favorisant ainsi la réutilisation.\nContrairement à une démarche d\u0027intégration où l\u0027on ne cherche à obtenir qu\u0027une seule ontologie homogénéisée, notre approche s\u0027attache à conserver au sein d\u0027une même base les différents points de vue (c\u0027est-à-dire les différentes ontologies), mettant ainsi en évidence les apports de chaque contributeur. Cependant, les outils utilisant des ontologies ont besoin d\u0027ontologies \"normales\" (mono-point de vue) pour fonctionner. Nous proposons donc un ensemble d\u0027opérations et laissons le soin aux utilisateurs de les utiliser pour extraire de la base une ontologie \"sur mesure\", dans un contexte et un but spécifiques. Axiomes. Un axiome établit une relation de subsomption entre deux expressions de concepts E 1 et E 2 . Dans le cas où E 1 est un concept atomique on parle de définition.\nUn modèle de bases d\u0027ontologies\nUne expression de concept (ici nous prendrons le langage SHOIN qui est celui de OWL, Patel-Schneider et al. (2003), mais la démarche s\u0027applique à toute logique descriptive) obéit à la syntaxe\nLe graphe d\u0027une telle expression est son arbre syntaxique étiqueté de manière à distinguer les descendants de chaque noeud. Pour un noeud and, or ou not, les arcs vers les descendants sont étiquetés par Arg. Pour les noeuds some et all on utilisera les étiquettes P rop et V al pour distinguer la propriété sur laquelle porte la restriction et le concept dans lequel la propriété prend ses valeurs. Pour atLeast et atM ost on ajoute des arcs N um pour le nombre et pour oneOf et hasV alue des arcs Ind pour indiquer les individus. Annotations. Les annotations constituent la partie non formalisée de l\u0027ontologie. On distingue des annotations d\u0027ordre terminologique, qui lient un concept atomique à un ou plusieurs termes, éventuellement dans plusieurs langues, ainsi qu\u0027à des définitions ou commentaires en langue naturelle. Les annotations peuvent également être d\u0027ordre argumentatif, par exemple pour soutenir ou au contraire critiquer une définition ou une partie de définition, Falquet et Mottaz Jiang (2000). Dans notre modèle, les annotations sont des objets possédant une valeur (en général un texte) qui sont liés aux concepts atomiques où aux expressions de concepts par des arcs étiquetés. \nOpérations sur les ontologies\nLes opérations que nous présentons ci-dessous ont été conçues pour réaliser de manière simple des tâches typiques d\u0027adaptation ou de combinaison d\u0027ontologies que nous avons observées lors de la réalisation de différents projets basés sur des ontologies.\nProjection. On définit la projection d\u0027une ontologie sur un ensemble X de propriétés en distinguant ce qui se passe pour les arbres d\u0027expression de concepts et pour le reste du graphe de l\u0027ontologie.\nLa projection sur X d\u0027un arbre d\u0027expression de concept de racine e est l\u0027arbre obtenu en supprimant tous sous-arbres de racine f tels que le chemin de f à e contient un noeud all, some, atLeast ou atM ost avec un arc V al qui fait référence à une propriété n\u0027appartenant pas à X.\nPar exemple, la projection sur {R, S} de l\u0027expression and(all(R, C), some(T, all(R, D)), all(S, or(A, all(T, A)))\ndonnera and(all(R, C), all(S, or(A)))\nIl est parfois utile de limiter la projection à un seul niveau. Dans ce cas, tous les sous-arbres d\u0027un noeud faisant référence à une propriété appartenant à X seront conservés. La projection sur {R, S} limitée au premier niveau de l\u0027expression 1 sera\nand(all(R, C), all(S, or(A, all(T, A))))\nLa projection d\u0027une ontologie est obtenue en remplaçant chaque arbre d\u0027expression par sa projection. Pour le reste de l\u0027ontologie (les annotations et arcs d\u0027axiomes) on élimine les arcs dont l\u0027étiquette n\u0027appartient pas à X et on supprime les éventuelles composantes connexes ainsi créées qui ne contiennent aucun axiome (et ne participent donc plus à l\u0027ontologie).\nIl est clair qu\u0027en général les expressions obtenues par projection ne seront pas équivalentes, par contre les alignements sont conservés car on considère qu\u0027il s\u0027agit de nouvelles définitions (moins précises) des mêmes concepts.\nSélection. La sélection a pour but d\u0027extraire un sous-ensemble de l\u0027ensemble des concepts définis dans une ontologie. Pour spécifier une sélection on définit une expression logique portant sur (le graphe de) l\u0027ontologie, c\u0027est-à-dire sur les définitions de concepts, leurs annotations et éventuellement leurs instances.\nUn expression de sélection est une expression de logique du premier ordre possédant une variable libre et définie sur le vocabulaire constitué de prédicats unaires correspondant aux types de noeuds du graphe d\u0027une ontololgie (And, Or, Some, . . . ) de prédicats binaires correspondant aux types d\u0027arcs (Arg, P ro, V al, N um, plus tous les noms de propriétés d\u0027annotation) et de constantes les noms de propriétés et les littéraux.\nPar exemple, pour sélectionner dans O 1 tous les concepts qui ont une restriction some sur la propriété S au premier niveau on écrira :\nEtant donné un prédicat de sélection S(x), la sélection ? S O de l\u0027ontologie O par S a pour graphe le sous-graphe de O qui contient tous les concepts atomiques (noeuds) satisfaisant S, les arcs d\u0027alignement et d\u0027annotation de ces concepts, leurs expressions de définition, les concepts atomiques référencés (à travers des arcs Arg et V al depuis ces définitions et les noeuds.\nLes concepts non-sélectionnés mais référencés doivent être conservés, mais sans leur défi-nition, pour maintenir la consistance des définitions des concepts sélectionnés. Ils passent donc du statut de concept défini à concept de base.\nEtant donné que les ontologies sont interconnectées, l\u0027expression de sélection peut utiliser d\u0027autres ontologies. Par exemple, si l\u0027on veut sélectionner les concepts de O 1 qui sont alignés avec un concept de O 2 , on écrira ? ?(y:$1)Aligned(x,y) O 1 (O 2 ) où y : $1 indique que la variable y doit être un objet de la première ontologie passée en paramètre (ici O 2 ) Jointure. Contrairement à l\u0027union, la jointure combine les définitions de concepts par une opération ensembliste (union, inter). L\u0027idée est que pour toute paire d\u0027axiomes\n. Il en va de même pour les axiomes de la forme A 1 def E 1 . Si l\u0027un des axiomes est de la forme A i def E 1 , l\u0027axiome obtenu sera A def (E 1 E 2 ). On procède de la même manière s\u0027il y a plus de deux concepts alignés sur le même objet d\u0027alignement. On remplace également tous les arcs de ou vers l\u0027un des A i par un arc de ou vers A.\nSi l\u0027on élimine les concepts qui ne sont pas alignés avec au moins un autre concept, on obtient la jointure stricte. Si on les garde on peut obtenir la jointure externe à gauche ou à droite.\nExemples d\u0027utilisation\nDans le projet UNL (Universal Networking Language www.undl.org), des équipes dispersées dans le monde développent indépendamment des parties de l\u0027UNLKB (ontologie linguistique du langage UNL). Ces parties, qui portent sur des domaines disjoints ou non, sont ensuite importées dans la base d\u0027ontologies UNLPLAZA. Chaque concept est associé à un \"mot universel\" qui joue le rôle d\u0027objet d\u0027alignement entre ontologies. L\u0027une des fonctions de l\u0027UNLPLAZA est de produire (exporter) des ontologies construites par union, dans le cas où il existe plusieurs définitions de concepts pour le même mot universel, un ordre de priorité déter-mine laquelle choisir. Si l\u0027ordre de priorité des ontologies est\nDans le cas où la base contient des ontologies portant sur un même domaine, mais déve-loppées par des personnes qui s\u0027intéressent à des caractéristiques différentes, on peut utiliser la jointure pour regrouper les caractéristiques en des définitions plus complètes. Par exemple : une ontologie des aliments selon le point de vue des cuisiniers (propriétés : saveur, temps de cuisson, prix, saison) pourra être jointe avec une ontologie selon le point de vue des diététiciens (valeur énergétique, vitamines, graisse, hydrate de carbone, protéines) Pour importer dans O 1 uniquement les parties de définition concernant une propriété P on projette O 2 sur P puis on joint (à gauche) le résultat à O 1 ) :\nLe remplacement de définitions s\u0027effectue de la manière suivante. Soit E(x) l\u0027expression permettant de sélectionner les concepts de O 1 dont on veut remplacer la définition par celles existant dans O 2 . l\u0027expression de remplacement est O 1 ? x.?(y:$1)Aligned(x,y) O 2 (? E(x) O 1 ) où l\u0027on commence par sélectionner les concepts de O 2 alignés avec les concepts sélection-nés de O 1 , puis on fait une jointure externe gauche des deux ontologies.\n"
  },
  {
    "id": "884",
    "text": "Contexte\nDans le but de résoudre des problèmes complexes du monde réel dans des domaines différents tels que l\u0027optimisation, la détection d\u0027anomalies ou la robotique, des heuristiques inspirées de mécanismes naturels ont été exploitées avec succès. Plusieurs chercheurs se sont intéressés aux systèmes immunitaires biologiques (SIB) comme un nouveau paradigme de l\u0027intelligence artificielle et ont développé des applications industrielles en ordonnancement, en robotique, ou en détection d\u0027intrusion. Néanmoins, peu de travaux ont traité la probléma-tique de la détection de fraude de comportement en télécommunications.\nDans ce papier, on propose un nouveau système immunitaire artificiel (SIA) pour la dé-tection du comportement du soi non soi avec une approche non supervisée basée sur le mé-canisme SIB dit inné de cellule NK. Un tel système diffère des SIA existants qui se basent sur le mécanisme supervisé adaptatif de SIB des cellules T et B (Garrett 2005).\nPrésentation du système NK proposé\nL\u0027algorithme de notre système NK, décrit dans le tableau TAB1, comporte quatre phases qui concernent la reconnaissance et l\u0027extraction de modèles d\u0027instances puis leur transformation en signaux d\u0027inhibition et d\u0027activation. La dernière phase concerne la détection de la présence de comportements anormaux sur la base de l\u0027analyse des densités spectrales ou de filtrage des signaux. Notons ici que la terminologie signal utilisée correspond à un signal discret à temps discret et que l\u0027entrée de l\u0027algorithme est une série chronologique vectorielle.\nL\u0027algorithme du système NK élaboré a été testé sur des données simulées de 10100 instances de télécommunication, relatives aux trafics de certains usagers chez un intermédiaire, et qui sont infectées par un comportement frauduleux pour les instances entre 10000 et 10100 et dont la proportion représente 0.01% de l\u0027échantillon. Les résultats obtenus sont satisfaisants car, malgré la proportion très faible des opérations frauduleuses dans l\u0027échantil-lon, notre système NK a réussi à les détecter (cf . FIG. 3) et à identifier les instances de comportements frauduleux (cf . FIG. 1 et FIG. 2 \nSummary\nWe propose a new artificial immune system called NK system for the detection of self non self behaviour with an unsupervised approach based on the mechanism of NK cell. In this paper, the NK system is applied to the detection of fraud in mobile telephony.\n"
  },
  {
    "id": "885",
    "text": "Introduction\nLes données dans un SIG (Faïz, 1999), sont souvent recueillies pour les besoins propres d\u0027une institution, voire d\u0027un service. Face à cette réalité, il devient judicieux de déployer de nouvelles sources pour répondre aux besoins d\u0027un nombre plus important d\u0027utilisateurs. Ceci est qualifié d\u0027enrichissement de bases de données géographiques (BDG). C\u0027est dans ce contexte que s\u0027inscrit notre approche b , Faïz et Mahmoudi, 2005. Cette dernière utilise la technique de résumé de documents multiples (Barzilay et McKeown, 2005) permettant d\u0027extraire l\u0027information pertinente sous une forme abrégée. Pour assurer l\u0027extraction dans des temps raisonnables et conformément au paradigme multi-agents (Ferber, 1999), nous adoptons trois classes d\u0027agents: agent interface, agent géographique et agent tâche. L\u0027interaction entre les agents est achevée par envoi de messages. L\u0027enrichissement est réalisé en trois phases : une identification de segments et de thèmes, une délégation et enfin, un filtrage textuel. S\u0027ajoute à ces étapes de base, une approche, exercée à la demande, pour un raffinement du processus.\nLa section 2 présente, certains travaux d\u0027enrichissement des BDG dans les SIG ainsi que notre approche pour cet enrichissement. La section 3 est dédiée à la mise en oeuvre et l\u0027évaluation de notre système.  (Plazanet, 1996), par exemple, l\u0027enrichissement procure les BDG d\u0027informations en terme de structure des formes, des connaissances se rapportant à l\u0027ordre des opérations et aux algorithmes appropriés. Un autre flot de travaux se rapporte à l\u0027aspect sémantique (dit aussi factuel ou descriptif) des BDG. Dans cette catégorie, nous pouvons citer Metacarta (MetaCarta, 2005) et GeoNode (Hyland et al., 1999).\nLe projet Metacarta accompli l\u0027enrichissement par son produit Geographic Text Search (GTS). GTS permet de relier des documents textuels à des entités géographiques localisées sur la carte venant enrichir les données de la BDG. MetaCarta GTS est offert comme une extension au système d\u0027information géographique ArcGIS.\nGeoNode (Geographic News On Demand Environment) exploite la technique d\u0027extraction d\u0027informations pour aboutir à l\u0027enrichissement, via le système Alembic. GeoNode permet d\u0027extraire les entités nommées et les évènements associés qui seront visualisées d\u0027une manière geospatiale. Le SIG ArcView supporte GeoNode.\nCe qui marque notre approche est qu\u0027à l\u0027opposition des travaux existants, elle va au-delà de la simple localisation de l\u0027information, pour permettre la synthèse de ces informations. De même, à l\u0027opposé des travaux existants dont la source de données aurait été déjà pré-établies sous forme de librairies (données traitées et classées) par exemple, notre approche préconise la génération en temps réels (information toujours récente et mise à jour) des documents requis peu importe le type d\u0027information réclamée (nous ne pouvons jamais prédire tous les besoins des utilisateurs). Egalement, et outre l\u0027aspect sémantique, nous exploitons la composante spatiale des BDG afin d\u0027améliorer les résultats de l\u0027enrichissement.\nNotre approche pour l\u0027enrichissement des BDG\nNotre processus d\u0027enrichissement émane d\u0027un besoin informationnel réclamé par les utilisateurs des SIG. Un utilisateur soumettant une requête à la BDG, mais, se trouvant en situation d\u0027insatisfaction, peut lancer notre processus d\u0027enrichissement. En fait, les documents relatifs à une ou plusieurs entités géographiques sont confiés à l\u0027agent interface qui va les distribuer entre les agents tâche. Chaque agent tâche procède à la segmentation de son document en parties thématiquement cohérentes. Notre proposition est une adaptation de l\u0027algorithme de TextTiling (Hearst, 1997). Le texte est initialement découpé en blocs de taille fixe. En fait, il arrive que lors du découpage, deux phrases hétérogènes soient classées sous le même bloc, ainsi, lors du calcul de la similarité, nous obtenons forcément des valeurs élevées faisant preuve de l\u0027homogénéité des deux blocs. Pour résoudre le problème de localisation non précise des frontières, nous intervenons lors du découpage initial en appliquant la procédure C99 (Choi, 2000), reconnue pour être la plus appropriée pour les textes courts. Cette procédure remplace la similarité entre phrases par le rang dans un contexte local. Un score de cohésion (métrique du cosinus) est attribué aux blocs adjacents. Les similarités sont présentées sous forme de graphe et aplanie. Les vallées observées sur le graphe correspondent à des scores faibles indiquant une potentielle rupture thématique.\nLes segments précédemment détectés, subissent une annotation (étiquetage) par les agents tâche moyennant l\u0027attribution de thème. Le mot le plus fréquent est assigné comme le thème du texte en cas de distribution hétérogène des fréquences des mots. Pour une distribution homogène, nous partons de l\u0027idée que le texte est un ensemble de termes contribuant à développer un thème donné. Pour déterminer ce thème, nous exploitations les relations sémantiques du thesaurus WordNet (Miller, 1990 Les thèmes décelés sont assignés aux agents tâche par une délégation entreprise par l\u0027agent interface (cas d\u0027une seule entité géographique) ou un des agents géographique (en cas d\u0027une multitude d\u0027entités). Cette délégation consiste à distribuer les différents thèmes entre les agents tâche, et ce tenant compte d\u0027un coût qui reflète le coût de communication (en terme de messages échangés) et la charge du travail (en terme de volume textuel à prendre en charge). Dorénavant, chaque délégué détient à sa disposition des documents générés qui sont l\u0027ensemble des segments textuels relatifs aux thèmes qui lui sont affecté.\nPar la suite, chaque délégué entame une phase de filtrage visant la condensation dans un format de résumés de ses documents générés pour ne retenir que l\u0027essentiel de l\u0027information (Mann et Thompson, 1988). Ainsi, pour une phrase formée de deux unités lexicales et reliée par une relation d\u0027exemplification (par la présence de cues ; for instance, for example), nous ne retenons que la partie (appelée nucléus) qui ne contient pas ce cue, obligatoire pour la compréhension du texte. L\u0027autre contenant le cue est subsidiaire (appelée satellite). En cas d\u0027absence de cues, nous calculons la similarité entre les unités et nous ne retenons que les unités les plus dissimilaires.\nA l\u0027issue des étapes susmentionnées, il se trouve que parfois l\u0027utilisateur reste insatisfait par les résultats de l\u0027enrichissement. Ceci peut provenir d\u0027une des raisons suivantes : (i) la recherche de l\u0027information n\u0027a pas ciblé les documents les plus pertinents dans le web, (ii) l\u0027utilisateur est submergé par une masse colossale de documents qui ne sont pas tous pertinents par rapport à l\u0027entité en question, (iii) L\u0027information visée n\u0027est pas disponible dans les documents du corpus. L\u0027idée est d\u0027examiner le voisinage de l\u0027entité sujette de la recherche pour dégager les entités avec les quelles, elle détient des relations spatiales (Mahmoudi et Faïz, 2006 b ). Notre thèse est argumentée par la première loi de la géographie décrivant la nature des systèmes géographiques dans lesquels \"everything is related to everything else, but near things are more related than distant things\" (Bin et Itzhak, 2006). Dans ce cas, l\u0027utilisateur peut lancer un processus de raffinement que nous avons intégré au processus de base. Il exploite les relations spatiales (adjacence, superpositions…) afin de mieux décrire les entités sujettes de la recherche et ainsi mieux cibler les informations pertinentes.\nlangage Java ce qui a permis une implantation d\u0027un système distribué transposant les concepts du système multi-agents. Cette mise en oeuvre a donné naissance à un outil que nous avons baptisé Semantic Data Enrichment Tool ou SDET (Mahmoudi et Faiz, 2007, Mahmoudi et Faiz, 2006 ). SDET offre un ensemble de fonctionnalités visant l\u0027enrichissement des données initialement stockées dans la BDG allant de la création de documents générés relatifs aux thèmes décelés du corpus textuel jusqu\u0027à la condensation des idées essentielles incarnées dedans sous forme de résumés. En plus, SDET offre le moyen d\u0027exploiter les relations spatiales que les entités géographiques y maintiennent pour un éventuel raffinement de la recherche. Ce système à été intégré au SIG OpenSource : OpenJump.\nPour l\u0027évaluation des résumés générés, nous avons utilisé l\u0027outil ROUGE (RecallOriented Understudy for Gisting Evaluation) adopté par la campagne d\u0027évaluation de systèmes de résumé DUC (Document understanding conference). ROUGE calcule le taux de chevauchement entre les résumés produits par un système de résumé et les résumés produits par des humains (appelé modèle ou référence) en utilisant des n-grammes. Formellement, ce score s\u0027exprime par la formule suivante :\nROUGE-N, dénote un score basé sur le nombre de n-grammes (1\u003c\u003dn\u003c\u003d 4) communs entre les résumés automatiques et les résumés modèles. Count match (n-gram) est le nombre de n-grammes partagés entre le résumé produit par le système et le résumé modèle et Count (ngram) est le nombre de n-grammes dans le résumé modèle.\nRouge-2, Rouge-SU-4 et BE se sont imposés. ROUGE-2 calcule le nombre de paires de mots successifs (nombre de bigrammes) en commun entre les résumés automatiques et les résumés modèles. Rouge-SU-4 correspond au rappel en \"bigrammes à trous\" (skip units) de taille maximum 4. Pour, BE (Basic Elements) elle a est définie comme unité sémantique minimale qui consiste en deux éléments et une relation (entités-relation) entre ces éléments. Par exemple, à partir de la phrase \"two Libyans were indicted for the Lockerbie bombing in 1991\" nous détectons la BE suivante : indicted|libyans|obj, tel que obj est la relation objet entre indicted et libyans. La similarité entre le résumé référence et le résumé système en terme de BEs, permet de juger que le résumé système est un bon résumé.\nComme pour la conférence DUC, nous avons défini deux types de références basses, appelées aussi baseline. Ces résumés sont créés automatiquement en fonction des règles suivantes : une baseline sélectionne les 150 premiers mots du document le plus récent, une autre baseline sélectionne la première phrase dans les (1, 2,…, n) documents de l\u0027ensemble à résumer trié par ordre chronologique jusqu\u0027à atteindre 150 mots. Ces règles découlent du fait que d\u0027après des études de différents documents textuels, l\u0027importance du commencement du document par rapport à sa fin a été approuvée. Ainsi, les premières lignes du texte couvrent en général une partie importante de l\u0027essentiel du texte. Pour notre évaluation, nous avons généré des résumés automatiques avec les systèmes suivants : le système MEAD (Radev et al., 2003) et les méthodes baselines (baseline1 et baseline2). Nous avons utilisé l\u0027outil ROUGE pour comparer les résumés obtenus avec notre système SDET, MEAD, baseline1 et baseline2 avec les résumés humains comme le montre le Tableau 1.\nLe score le plus élevé étant le meilleur, il indique le système le plus performant. SDET est classé au premier rang avec les meilleures notes d\u0027évaluation. Les deux baselines donnent les résultats les plus faibles. Notons que si l\u0027hypothèse de base qui stipule que les premières lignes du texte couvrent en général une partie importante de l\u0027essentiel, cela reste insuffisant dans le cas de résumé multi-documents qui nécessite une prise en charge particulière des similarités et des différences informationnelles à travers tous les documents.\nSystème\nRouge \nConclusion\nLa disponibilité des informations pertinentes répondants aux besoins de la quasi totalité des utilisateurs SIG est devenue un objectif visé par les concepteurs de tels systèmes. En effet, les informations manipulées sont fournies par des BDG qui deviennent très vite insuffisantes et ne répondent plus à l\u0027ensemble des besoins des utilisateurs. Face à cette situation, nous avons proposé et mis en oeuvre un processus d\u0027enrichissement pour les BDG. Il s\u0027agit d\u0027une identification de segments et de leurs thèmes, une délégation puis un filtrage. Une phase supplémentaire peut être invoquée en cas de résultats insatisfaisants, il s\u0027agit d\u0027un raffinement visant un ciblage plus pertinent des informations requises. La mise en oeuvre du processus d\u0027enrichissement a donné lieu à un outil que nous avons baptisé SDET. Les résultats générés par notre système ont été évalués en utilisant le package ROUGE. Les résultats de cette évaluation sont très convaincants faisant preuve de l\u0027intérêt de notre démarche pour l\u0027enrichissement des BDG.\n"
  },
  {
    "id": "886",
    "text": "Introduction\nLa classification de textes a pour objectif le regroupement de documents selon différents critères. Dans les travaux présentés dans cet article, nous nous intéressons à la classification de textes d\u0027opinion qui consiste à classer les textes selon un jugement tel que l\u0027aspect positif ou négatif d\u0027une critique, l\u0027aspect favorable ou défavorable donné par un expert, etc. Nous proposons dans cet article une approche fondée sur plusieurs classifieurs combinés à un système de vote. Dans un premier temps, nous présentons les corpus du défi DEFT\u002707 (Grouin et al., 2007) sur lesquels nous avons mené nos expérimentations ainsi que les représentations des textes utilisées. La section 3 décrit les classifieurs et les systèmes de vote proposés. Enfin, la partie 4 présente les résultats obtenus.\nReprésentation des données textuelles\nLa troisième édition du défi francophone DEFT\u002707 consistait à déterminer des catégories de jugements à partir de quatre corpus français très différents en terme de thème, taille, tournures de phrases, richesse du vocabulaire, représentation des catégories de jugement : ? Corpus 1 : Critiques de films, livres, spectacles et bandes dessinées. La première étape de notre approche consiste à appliquer un certain nombre de prétraitements linguistiques. Ceux-ci consistent à extraire du corpus toutes les unités linguistiques (mots lemmatisés ou lemmes) utilisées pour la représentation des textes. En effet, le prétraitement consistant à lemmatiser les données textuelles a globalement tendance à améliorer les tâches de classification (Plantié, 2006). Par ailleurs, ces prétraitements consistent également à éliminer certains mots ayant des types grammaticaux peu discriminants pour classifier les textes d\u0027opinion : articles, ponctuations. Dans notre approche nous avons souhaité conserver les lemmes associés à tous les autres types grammaticaux. En effet, le fait de traiter spécifiquement des textes d\u0027opinion nous encourage à conserver un maximum de types grammaticaux susceptibles d\u0027exprimer des nuances d\u0027opinions ou des contributions à des opinions (comme les adverbes). Les expériences que nous avons menées sur les textes d\u0027opinion propres aux corpus DEFT\u002707 ont en effet montré que la suppression de types grammaticaux diminuait les performances. Dans la suite de cet article, nous appellerons « index » la liste de lemmes constitués pour chacun des corpus.\nChaque corpus est représenté sous forme matricielle en utilisant l\u0027approche classique dite de Salton (Salton et al., 1975). Dans cette représentation, les lignes sont associées aux différents textes du corpus et les colonnes sont relatives aux lemmes. Chaque cellule de la matrice représente le nombre d\u0027occurrences du lemme dans chaque texte du corpus.\nL\u0027ensemble des textes d\u0027un corpus et donc les vecteurs associés constituent dans notre approche l\u0027ensemble d\u0027apprentissage qui permet d\u0027identifier un classifieur associé. L\u0027espace vectoriel défini par l\u0027ensemble des lemmes du corpus d\u0027apprentissage et dans lequel sont définis ces vecteurs comporte un nombre important de dimensions. Nous avons choisi d\u0027effectuer une réduction de l\u0027index. Nous utilisons la méthode présentée par Cover qui mesure l\u0027information mutuelle associée à chaque dimension de l\u0027espace vectoriel (Cover \u0026 Thomas, 1991). Cette méthode expliquée en détail dans (Plantié, 2006) permet de mesurer l\u0027interdépendance entre les mots et les catégories de classement des textes par la différence d\u0027entropie entre celle de la catégorie et celle de la dimension en cours d\u0027étude de l\u0027espace vectoriel. Notons que plus la différence est grande, plus la quantité d\u0027information de discrimination est importante et plus le mot est important pour la tâche de catégorisation. Dans notre approche, nous avons appliqué un seuil de zéro pour effectuer cette réduction. Un tel seuil signifie que les mots retenus sont réellement discriminants. Cette opération diminue de manière très importante l\u0027ensemble des corpus avec un pourcentage de réduction de plus de 90% de tous les corpus de DEFT\u002707. Cette étape améliore de façon sensible les résultats de classification (environ 10% sur la « F-mesure » comme indiqué ci-après).\nDans le cadre de DEFT\u002707, nous avons appliqué un prétraitement supplémentaire. Ainsi, avant d\u0027effectuer la classification des textes, nous avons cherché à améliorer les traitements « linguistiques » des textes. Dans ce but, les termes (groupes de mots respectant des patrons syntaxiques spécifiques tels que « Nom Adjectif », « Adjectif Nom », etc.) ont été extraits avec EXIT (Roche, et al., 2004). Ainsi, nous avons considéré la liste des termes extraits comme l\u0027index du corpus à partir duquel tous les textes ont été vectorisés. Puis la procédure classique a été appliquée : réduction d\u0027index et classification. Le nombre de termes extraits peut se révéler assez faible pour certains textes ce qui réduit significativement la taille de l\u0027index. Ceci met alors en défaut les méthodes statistiques qui ont été mises en place dans le cadre du défi. Par ailleurs, les termes sélectionnés après réduction d\u0027index ne sont pas suffisamment significatifs pour représenter la diversité des textes. Ceci peut expliquer que notre approche uniquement fondée sur les termes dégrade nos résultats. Ainsi, nous proposons ci-dessous une méthode plus générale fondée sur l\u0027utilisation de bigrammes de mots.\nDans l\u0027approche développée, outre les mots qui sont pris en compte, les vecteurs sont aussi constitués de bigrammes de mots. Seuls les bigrammes contenant des caractères spéciaux sont rejetés (caractères mathématiques, ponctuations, etc.). Cette représentation plus riche des textes permet d\u0027obtenir des informations plus adaptées aux textes d\u0027opinion. A titre d\u0027exemple, dans le corpus de relectures d\u0027articles les bigrammes tels que pas convainquant, mieux motiver, pas assez sont des groupes de mots beaucoup plus porteurs d\u0027opinion comparativement à chacun des mots constituant ces bigrammes. Deux différences majeures sont à relever par rapport à la méthode fondée sur la terminologie : (1) L\u0027approche prend en compte les mots ainsi que les bigrammes pour constituer l\u0027index, (2) Le nombre de bigrammes retournés est beaucoup plus important que le nombre de termes respectant des patrons syntaxiques définis.\nLes résultats en utilisant cette représentation enrichie des textes par la prise en compte des bigrammes améliore les résultats comme nous allons le montrer dans la section 4. Outre la qualité de la représentation des textes qui améliore les tâches de classification, la prise en compte de différents classifieurs (voir section suivante) reste déterminante pour retourner un résultat de bonne qualité.\nProcessus de classification\nAfin d\u0027améliorer la méthode générale de classification des textes d\u0027opinion, nous avons mis en oeuvre un système de vote fondé sur différents classifieurs. Le même processus de classification a été appliqué en nous appuyant sur les représentations de textes présentées dans la section précédente. Dans la suite de ce papier, nous appellerons Copivote (Classification de textes d\u0027OPInion par un système de VOTE) le système de classification appliqué à la représentation par lemmes uniquement et CopivoteBi (Classification de textes d\u0027OPInion par un système de VOTE avec Bigrammes) le système de classification appliqué à la représentation par lemmes et bigrammes.\nLe système de vote mis en place s\u0027appuie sur différentes approches de classification. Ces dernières sont fondées sur trois méthodes principales : -Vote à la majorité simple : choix des classes à la majorité des résultats des classifieurs.\n-Vote par choix du maximum (respectivement, minimum) : choix de la classe allouée par le classifieur qui a donné la probabilité la plus élevée (respectivement, faible).\nd\u0027appartenance. Dans ce cas, il y a nécessité que les probabilités exprimées par les différents classifieurs soient comparables. -Vote par somme pondérée : pour chaque document et pour chaque classe la moyenne des probabilités de tous les classifieurs est calculée et le choix de la classe attribuée au document est alors fondé sur la plus forte moyenne. Notons que plusieurs travaux s\u0027appuient également sur un système de vote de classifieurs (Kuncheva, 2004 ;Kittler et al., 1998).\nLe choix de la procédure de classification a été appliqué sur chaque ensemble d\u0027apprentissage. Nous avons conservé la méthode de classification la plus performante pour un corpus donné. Les données sont évaluées par validation croisée sur l\u0027ensemble du corpus en s\u0027appuyant sur les mesures de précision, rappel et F-mesure. La précision d\u0027une classe i correspond à la proportion de documents correctement attribués à leur classe i par rapport aux documents attribués à la classe i. Le rappel calcule la proportion de documents correctement attribués à leur classe i par rapport aux documents appartenant à la classe i. La précision et le rappel moyens peuvent alors être calculés par rapport à l\u0027ensemble des classes. Un compromis entre la précision et le rappel est alors calculé en mesurant la Fmesure (la F-mesure est la moyenne harmonique du rappel et de la précision).\nLe système de vote mis en place étant décrit, les différents classifieurs utilisés par Copivote sont succinctement présentés ci-dessous. Une description plus précise de ces derniers est donnée dans (Plantié, 2006). ? Bayes Multinomial. La méthode de Bayes Multinomial (Wang et al., 2003) qui est une approche classiquement utilisée pour la catégorisation de textes combine l\u0027utilisation de la loi de Bayes bien connue en probabilités et la loi multinomiale.\n? Les Machines à Vecteurs Support (Support Vector Machine -S.V.M.). La méthode\nMachines à Vecteurs Support (Joachims, 1998;Platt, 1998) consiste à délimiter par la frontière la plus large possible les différentes catégories des échantillons (ici les textes) de l\u0027espace vectoriel du corpus d\u0027apprentissage. Les vecteurs supports constituent les éléments délimitant cette frontière : plus la frontière est large, plus les risques d\u0027erreurs de classification sont rares. ? Les réseaux RBF (Radial Basis Function). Les réseaux RBF sont fondés sur l\u0027utilisation d\u0027un réseau de neurones à fonctions radiales de base. Cette méthode utilise un algorithme de « clustering » de type « k-means » (MacQueen, 1967) avec application d\u0027une méthode de régression linéaire. Cette technique est présentée dans (Parks \u0026 Sandberg, 1991).\nRésultats\nLe tableau 1 montre que la procédure de vote que nous avons mise en place améliore globalement les résultats. Notons que toutes les procédures de vote permettent une amélioration du même ordre même si des résultats légèrement meilleurs sont retournés avec le « vote par somme pondérée ». Dans un deuxième temps, nous pouvons noter que l\u0027utilisation des bigrammes (CopivoteBi) améliore globalement les résultats par rapport au traitement sans les utiliser (Copivote).\nLe tableau 1 présente les méthodes de classification prises en compte dans le système de vote. Nous constatons que le classifieur de Bayes Multinomial est très performant avec un temps de calcul très faible. Les meilleurs résultats sont dans la grande majorité des cas obtenus par le classifieur SVM. Le classifieur RBF Network donne quant à lui des résultats décevants. \nCorpus\nConclusion et perspectives\nLa classification de textes d\u0027opinion est un thème porteur. La mise en place du défi DEFT\u002707 montre l\u0027intérêt de l\u0027étude de ce type de textes par la « communauté fouille de textes ». Cet article présente une approche consistant en une combinaison de représentations mots-clés et bigrammes tout en utilisant un système de vote de plusieurs classifieurs. Les résultats obtenus se sont révélés particulièrement intéressants avec une valeur de F-mesure légèrement supérieure à la meilleure soumission de DEFT\u002707.\nDans nos futurs travaux, nous comptons utiliser des représentations combinées motsclés, bigrammes et trigrammes qui pourraient encore améliorer les résultats. Nous souhaitons également utiliser des procédures de vote avec un nombre plus important de classifieurs. Enfin, une étude plus globale en utilisant d\u0027autres corpus et surtout des données textuelles de langues différentes pourraient être menée.\n"
  },
  {
    "id": "887",
    "text": "Introduction\nNous nous intéressons à la découverte de correspondances, ou mappings, entre ontologies distribuées modélisant les connaissances de pairs du système de gestion de données P2P (PDMS) SomeRDFS. Un PDMS est un système constitué de pairs autonomes qui communiquent pour répondre collectivement à une requête. Les communications entre pairs s\u0027éta-blissent grâce à des mappings qui définissent des relations sémantiques entre leurs connaissances. Un PDMS est sollicité via l\u0027interrogation d\u0027un des pairs qui pourra ensuite faire appel aux autres pour répondre. Une spécificité des PDMS est que chaque pair ne connaît que ses propres connaissances et les mappings le connectant à d\u0027autres pairs. Dans ce cadre, nous cherchons à augmenter le nombre de mappings de chaque pair afin d\u0027améliorer les réponses fournies globalement par le système, en quantité et en qualité.\nNous travaillons, dans le cadre du projet MediaD (projet financé par France Telecom R\u0026D), dont l\u0027objectif est la création d\u0027un environnement déclaratif de construction de systèmes de gestion de données P2P. Ces travaux ont conduit au développement de la plate-forme SomeRDFS (Adjiman et al., 2006) au sein de laquelle nous situons notre travail.\nNous présenterons dans un premier temps le contexte de notre travail. Nous montrerons ensuite comment les requêtes des utilisateurs peuvent être exploitées pour identifier des raccourcis de mappings ainsi que des relations cibles à partir desquelles des mises en correspondances intéressantes peuvent être trouvées. Étant données ces relations cibles, nous proposerons alors des techniques basées sur l\u0027interrogation du système pour construire des ensembles de candidats à un mapping. Nous présenterons ensuite quelques travaux proches. Enfin, nous conclurons et présenterons quelques perspectives.\nContexte de travail\nDans le contexte de SomeRDFS, les ontologies et les mappings sont exprimés en RDF(S) tandis que les données sont représentées en RDF. Il est ainsi possible de définir des classes, des sous-classes, des propriétés, des sous-propriétés, de typer le domaine et le co-domaine des propriétés. Les constructeurs autorisés sont l\u0027inclusion de classes, l\u0027inclusion de propriétés, le typage de domaine et de co-domaine. Ce langage est basé sur des relations unaires qui représentent les classes et des relations binaires qui représentent des propriétés.\nLes ontologies des pairs sont représentées à l\u0027aide d\u0027expressions RDFS composées uniquement de relations appartenant au vocabulaire du pair. Nous notons P:R la relation R (classe ou propriété) de l\u0027ontologie du pair P.\nLes données d\u0027un pair sont associées à des relations faisant partie de son vocabulaire. Un mapping correspond à une inclusion entre classes ou propriétés de 2 pairs différents ou au typage d\u0027une propriété d\u0027un pair donné avec une classe d\u0027un autre pair (cf. TAB. 1). Ainsi les mappings sont également des expressions RDF(S). Leur spécificité est d\u0027être construites à partir du vocabulaire des ontologies de pairs différents qui établissent ainsi des correspondances sémantiques entre eux. Chaque pair P peut être sollicité à l\u0027aide de requêtes exprimées avec son propre vocabulaire.\nMappings\nNotation LD Traduction en logique du premier ordre Inclusion de classes P1:C1 P2:C2 ?X, P1:C1(X) ? P2:C2(X) Inclusion de propriétés\nTAB. 1 -Mappings\nLe calcul des réponses aux requêtes se fait en deux temps. Les requêtes sont d\u0027abord ré-écrites en un ensemble de requêtes les subsumant. Le calcul des réécritures maximales de chaque atome d\u0027une requête fournit un ensemble de conjonctions de relations (classes ou propriétés). Les requêtes sont ensuite propagées du fait de l\u0027existence de mappings avec des relations d\u0027autres pairs. Ces derniers transmettront les réécritures obtenues. Ces réécritures seront ensuite évaluées afin d\u0027obtenir les données associées.\nExploitation des réponses aux requêtes utilisateurs 3.1 Découverte de raccourcis de mappings\nUn raccourci de mappings est un mapping résultant de la composition de mappings existants. Les raccourcis de mappings renforcent le réseau en créant des liens directs entre des relations de deux pairs différents là où jusqu\u0027alors n\u0027existaient, dans le PDMS, que des liens indirects. L\u0027objectif n\u0027est pas de rajouter ce type de mapping systématiquement mais de façon sélective. En effet ces mappings peuvent être intéressants à représenter car, bien qu\u0027ils ne permettent pas d\u0027aboutir à des réponses plus riches, ils peuvent être très utiles en cas de disparition de pairs du PDMS. La découverte des raccourcis de mappings à repésenter peut être automatisée. Nous proposons, pour cela, d\u0027exploiter le mécanisme de réponse aux requêtes des utilisateurs et d\u0027appliquer ensuite des techniques de sélection des raccourcis pertinents à représenter.\nConcernant le traitement des requêtes utilisateurs aboutissant à l\u0027obtention des données, nous proposons de dissocier les deux étapes du traitement qui sont : (1) le calcul des réécritures maximales de chaque atome de la requête posée, (2) l\u0027évaluation des réécritures. Cette dissociation est intéressante lorsqu\u0027un utilisateur ne trouve pas, au sein du vocabulaire du pair auquel il s\u0027adresse, la relation lui permettant de définir précisément les données qu\u0027il recherche, et qu\u0027il est dans l\u0027obligation d\u0027indiquer une autre relation. Cette autre relation peut être plus spécifique. Dans ce cas, toutes les réponses attendues ne seront pas obtenues. Elle peut être plus générale. Dans cet autre cas, elle permettra d\u0027obtenir toutes les réponses attendues par l\u0027utilisateur mais contiendra, en revanche, également des réponses inutiles. Ainsi, par exemple, si l\u0027utilisateur s\u0027adresse au pair P 1 dans l\u0027espoir d\u0027obtenir les données de la classe SteelSculptor, il peut, en l\u0027absence de la classe SteelSculptor dans P 1 , s\u0027intéresser aux données d\u0027une classe plus générale en posant la requête Q 2 (X) ? P 1 :Artist(X). Il obtiendra, parmi les réécritures, P 2 :SteelSculptor(X) indiquant que les données instanciant SteelSculptor(X) sont des artistes mais également P 2 :W oodSculptor(X) ou P 2 :GlassSculptor(X) dont il n\u0027est pas utile de rechercher les données, compte tenu du besoin précis de l\u0027utilisateur. Le fait de dissocier le calcul des réécritures de leur évaluation est, dans ce cas, intéressant. Il permet à l\u0027utilisateur de sélectionner les réécritures pour lesquelles il demande l\u0027évaluation.\nNous proposons de stocker les raccourcis de mappings correspondant à des liens directs avec des relations qualifiées d\u0027intéressantes pour les utilisateurs. Ces relations, dites intéres-santes, appartiennent au vocabulaire de pairs distants. Elles sont plus spécifiques que celles du pair étudié. Elles n\u0027apparaissent pas, pour l\u0027instant, dans ses mappings, mais sont apparues dans des réécritures et les utilisateurs ont, à plusieurs reprises, demandé leur évaluation. L\u0027ensemble des mappings potentiels ainsi stockés seront traités globalement ultérieurement pour sélectionner ceux qui seront effectivement ajoutés.\nIdentification de relations cibles\nL\u0027intérêt principal des mappings est de permettre de propager des requêtes à des pairs distants pour qu\u0027ils contribuent aux réponses. Lorsqu\u0027un utilisateur s\u0027adresse à un pair, il arrive toutefois que les réponses proviennent toutes de ce pair. Cette situation révèle un manque de mappings de spécialisation entre les relations de ce pair et celles de pairs distants. L\u0027identification de ce phénomène est possible par analyse des réponses obtenues aux requêtes utilisateurs, ou, plus précisément, par observation de la localisation des éléments qui composent les ré-ponses.\nUne étude nous a permis de définir deux cas pour lesquels le calcul de réécritures est un indice pour trouver des relations appartenant à des pairs distants qui peuvent être rapprochées de relations du pair interrogé. Nous présentons successivement ces deux cas en nous limitant aux classes, le fonctionnement étant similaire pour les propriétés.\nCas 1 (cf . FIG 1) : Soient les pairs P 1 , P 2 et P 3 contenant respectivement les classes C 1 , C 2 et C 3 et les mappings suivants : P 1 :C1(X) ? P 2 :C 2 (X) et P 3 :C 3 (X) ? P 2 :C 2 (X) représentés chacun dans les deux pairs concernés.\n1. L\u0027utilisateur s\u0027adresse à P 3 et pose la requête Q 4 (X) ? P 3 :C 3 (X) 2. SomeRDFS ne fournit aucune réécriture.\nLa découverte de mappings dans SomeRDFS\nCas 2 (cf . FIG 1) : Soient le pair P 1 contenant la classe C1 et le pair P 2 contenant les classes C 2 et C 3 telles que P 2 :C 2 (X) ? P 2 :C 3 (X). Nous supposerons que le mapping P 2 :C2(X) ? P 1 :C1(X) est représenté à la fois dans P 1 et dans P 2 .\n1. L\u0027utilisateur s\u0027adresse à P 2 et pose la requête Q 5 (X) ? P 2 :C 3 (X). 2. SomeRDFS fournit la réécriture suivante : P 2 :C 2 (X). Aucune réécriture composée de relations de pairs distants n\u0027est donnée.\nCas 1\nCas 2\nFIG. 1 -Cas 1 et Cas 2\nC 3 dans le cas 1 et C 2 dans le cas 2 sont respectivement appelées relations cibles car c\u0027est à partir d\u0027elles que des mises en correspondance intéressantes peuvent être trouvées comme nous le montrons dans la section qui suit. Les deux cas présentés ci-dessus sont des cas élémentaires qui, s\u0027ils sont combinés, peuvent permettre de traiter des cas plus complexes. La notion de relation cible pourra être étendue à toute relation locale apparaissant dans une réécriture et généralisant une relation cible.\nIdentification de candidats pour la mise en correspondance\nNous proposons une procédure d\u0027identification de candidats à un mapping, qui s\u0027appuie sur les relations cibles, notées R CIBLE , supposées appartenir au pair P CIBLE . Sans liens de spécialisation avec d\u0027autres pairs via des mappings, ces relations ne permettent pas de propager le raisonnement vers un autre pair. Pour chaque relation cible, nous nous proposons alors de déterminer, dans un second temps, un ensemble des relations entre lesquelles il serait pertinent de rechercher des mises en correspondance. Cet ensemble de relations sera appelé candidats au mapping et noté CM . Le processus de découverte de mappings nouveaux est donc un processus en trois étapes : (1) recherche des relations cibles, (2) recherche d\u0027ensembles de candidats au mapping, (3) alignement des éléments de l\u0027ensemble de candidats au mapping.\nNotre approche pour la recherche d\u0027ensembles de candidats au mapping est basée sur l\u0027idée selon laquelle il est pertinent de rechercher des mises en correspondance entre des relations ayant des points communs. Dans le cadre de SomeRDFS, les points communs considérés seront (1) l\u0027existence d\u0027une relation commune plus générale, ou bien (2) l\u0027existence d\u0027une relation commune plus spécifique. Nous proposons un processus de recherche de candidats au mapping pour chacune de ces situations. Nous nous basons sur l\u0027existence d\u0027une relation commune plus générale lorsque R CIBLE n\u0027a qu\u0027une relation R g (au sein du même pair ou dans la définition d\u0027un mapping) qui la généralise. Dans ce cas, nous proposons de poser une requête sur cette relation (Q(X) ? R g (X)) et d\u0027en calculer les réécritures. Ces dernières fournissent un ensemble de relations plus spécifiques que R g entre lesquelles il est pertinent de rechercher l\u0027existence de correspondances. Cet ensemble constitue CM . Si ?!R g /R CIBLE ? R g et Q(X) ? R g (X) alors CM \u003d l\u0027ensemble des relations correspondant à des réécritures de Q(X). Nous nous basons sur l\u0027existence d\u0027une relation commune plus spécifique lorsque R CIBLE a plusieurs relations R g (au sein du même pair ou dans la définition d\u0027un mapping) qui la généralisent. Dans ce cas, l\u0027ensemble de ces généralisants constitue un ensemble de candidats au mapping\nCes deux situations correspondent aux cas 1 et 2 décrits dans la section précédente.\nLa recherche de mappings devra ensuite être effectuée entre les éléments de l\u0027ensemble CM . Cet ensemble comprend des relations de différents pairs, celui à qui la requête a été posée pour obtenir cet ensemble, et les pairs distants ayant contribué à la réponse à la requête via des réécritures. Le pair interrogé a une certaine compréhension des relations de CM faisant partie de son vocabulaire puisque celles-ci font partie de son ontologie. En revanche, les relations de CM appartenant au vocabulaire de pairs distants sont inconnues du pair interrogé. Pour lui, il s\u0027agit de noms de relations isolées. Le problème d\u0027alignement qui se pose consiste alors à mettre en correspondance des relations prises isolément avec des relations appartenant à une ontologie. Le travail que nous avons réalisé jusqu\u0027alors permet d\u0027isoler les ensembles de relations à aligner. Il doit être complété par l\u0027application de techniques d\u0027alignement appropriées. L\u0027étude des techniques les plus adaptées fait partie de nos perspectives.\nTravaux proches\nAppliqué aux systèmes P2P, le problème d\u0027alignement peut être résolu de différentes façons. Certains travaux comme Piazza (Halevy et al., 2004) proposent l\u0027application de techniques éprouvées testées dans le cadre de systèmes d\u0027intégration mais supposent, pour l\u0027alignement, que les ontologies de tous les pairs sont connues de tous, ou du moins accessibles dans leur totalité par tous. D\u0027autres travaux font intervenir des ontologies dites de référence avec lesquelles l\u0027ontologie de chaque pair peut être liée, ce qui évite la mise en relation directe des ontologies des pairs les unes avec les autres (Herschel et Heese, 2005). Enfin, une troisième catégorie de travaux consiste à étudier le problème de l\u0027alignement d\u0027ontologies lorsque toutes les ontologies composant le système P2P sont distribuées et qu\u0027il en existe aucune qui soit connue de tous et qui puisse servir d\u0027ontologie de référence. Ce problème a été étudié dans le système Helios (Castano et al., 2003) où la découverte de mappings est basée sur l\u0027interrogation des pairs du réseau.\nPar rapport à cet état de l\u0027art, l\u0027approche que nous proposons dans ce papier est spécifique au contexte distribué, tous les pairs étant considérés de la même façon et les ontologies étant réparties entre tous les pairs du système. En ce sens, elle se rapproche de la 3ème catégorie de travaux décrite ci-dessus. Nous exploitons toute la richesse du raisonnement pouvant être mis en oeuvre au sein d\u0027un PDMS, suite à l\u0027envoi de requêtes. Toutefois, contrairement aux travaux de (Castano et al., 2003), les requêtes ne sont propagées qu\u0027à un nombre réduit de pairs, elles ne sont pas envoyées à l\u0027ensemble des pairs du PDMS. Par ailleurs, les requêtes exploitées pour la découverte de mappings ont toutes la forme de requêtes utilisateurs. Elles ne néces-sitent donc pas de concevoir des modules de traitement de requêtes spécifiques. L\u0027originalité de notre approche consiste donc à réutiliser les mécanismes de raisonnement mis en oeuvre dans SomeRDFS de façon à cibler les éléments à rapprocher puis à réutiliser ou adapter les techniques d\u0027alignement qui ont été expérimentées dans d\u0027autres contextes et dont les résultats se sont avérés être de qualité.\nConclusion\nAu travers du travail décrit dans ce papier, qui s\u0027inscrit dans le cadre de systèmes P2P, nous avons étudié comment tirer parti du processus de raisonnement logique mis en oeuvre dans SomeRDFS, dans le but d\u0027aider à la découverte de correspondances entre ontologies. Nos perspectives portent sur l\u0027étude des techniques usuelles capables, en particulier, d\u0027aligner des éléments considérés de façon isolée avec d\u0027autres dont on connait précisément l\u0027ontologie à laquelle ils appartiennent (Reynaud et Safar, 2007;Kefi et al., 2006). Nous étudierons égale-ment la découverte de mappings prenant appui sur des requêtes ayant la forme de conjonction de relations. Enfin, des tests à plus grande échelle seront réalisés, mettant en jeu un nombre de pairs importants dotés d\u0027ontologies de taille significative.\n"
  },
  {
    "id": "888",
    "text": "Introduction\nIl est courant de séparer le domaine de l\u0027apprentissage automatique en deux domaines distincts. D\u0027un coté, l\u0027apprentissage supervisé désigne un cadre où les exemples sont reliés à une information relative à leur classe, à un concept. Les méthodes supervisées produisent par la suite, à partir d\u0027une base d\u0027exemples d\u0027apprentissage pour lesquels la classe est connue, une règle de décision visant à prédire la classe de nouvelles observations. Cette règle de décision, appellée aussi classifieur ou hypothèse, peut être considérée géométriquement comme une hypersurface séparant les exemples représentés dans un espace multidimensionnel.\nA contrario, cette notion de classe ou de concept est absente dans le cadre de l\u0027apprentissage non supervisé. Aucune information a priori n\u0027étant disponible, les techniques non supervisées visent à détecter des structures de groupes fondées sur des notions de distance ou de similarité entre les exemples (Lerman (1970)). C\u0027est précisement dans ce cadre que se place ce travail de recherche qui part d\u0027un constat simple : en classification supervisée, des méthodes dites ensemblistes ont au cours des dernières années montré des performances tout à fait intéressantes. Dans le cas particulier du boosting, il est question d\u0027un processus itératif qui va repondérer les exemples en insistant sur ceux mal classés par la méthode d\u0027apprentissage à une itération donnée (Freund et Schapire (1997)).\nDès lors, il est tentant de vouloir s\u0027inspirer d\u0027un processus comme le boosting dans le domaine de la classification non supervisée. Cette transposition n\u0027est pas du tout immédiate et soulève un certain nombre de problèmes : tout d\u0027abord, le boosting repose sur des justifications théoriques solides et il serait présomptueux de prétendre appliquer rigoureusement une telle méthode. C\u0027est pourquoi la contribution de ce travail doit simplement être considérée comme une approche ensembliste inspirée des grands principes du boosting. Dans un premier temps, il faut donc se confronter aux problèmes classiques liés aux ensembles de regroupeurs, présentés dans la section 2. D\u0027autre part, la notion d\u0027exemple \"difficile\", intuitive en apprentissage supervisé (les exemple difficiles sont par essence les exemples mal classés par la méthode d\u0027apprentissage de base), reste, dans le domaine non supervisé, à définir. Si l\u0027on veut, par analogie avec le boosting, insister à chaque itération sur les exemples difficiles, il faut précisement pouvoir les détecter, c\u0027est-à-dire évaluer la qualité individuelle de bonne classification d\u0027un exemple. Dans cet article, l\u0027approche UBLA (Unsupervised Boosting-Like Approach) est proposée, qui détecte et repondère les exemples difficiles à regrouper à partir d\u0027une partition floue, pour construire itérativement une matrice de co-association qui permettra de former finalement une partition dure plus pertinente au sens de certains critères de qualité.\nLes ensembles de regroupeurs\nLa problématique des ensembles de regroupeurs consiste à combiner les résultats de plusieurs algorithmes de partitionnement (ex : centres mobiles) afin de former une partition plus pertinente des différentes instances. On trouvera une présentation des méthodes les plus populaires dans Hornik (2004). Les applications dans les domaines du rassemblement et de la réutilisation des connaissances sont nombreuses mais les ensembles de regroupeurs peuvent aussi permettre de combiner des partitionnements obtenus à partir de sous-ensembles d\u0027individus ou d\u0027attributs différents. La formation d\u0027un ensemble de regroupeurs se heurte à certaines difficultés. D\u0027une part, l\u0027absence d\u0027information sur la classe des instances pose le réel problème de la correspondance entre les classes construites par les différents partitionnements. D\u0027autre part, la construction de la partition finale doit s\u0027appuyer sur une méthode de consensus efficace, qui tient aussi compte de la qualité des différents partitionnements.\nEn ce qui concerne la correspondance des classes, Fred (2001) a proposé un index de cohérence qui va calculer la similarité entre deux classes de deux partitions différentes au sens du plus grand nombre de points partagés. La procédure, connue aussi sous le nom de high matching score, va donc désigner itérativement les deux classes possédant le plus grand score de correspondance. Strehl et Ghosh (2002) ont quant à eux proposé une fonction objectif qui pourrait assurer de trouver le partitionnement idéal, partageant le plus d\u0027information possible avec les partitionnements de base. Ils font appel à la notion d\u0027information mutuelle normalisée. Dans Dimitriadou et al. (2001), il est question d\u0027une méthode pour laquelle un ensemble de partitionnements durs ou flous peut être combiné afin de former une partition floue du jeu de données. La partition finale est formée par un vote qui minimise une fonction de dissimilarité entre les partitions initiales. Cette approche ne résiste pas à l\u0027obstacle de la correspondance des groupes en considérant toutes les permutations possibles des matrices d\u0027appartenance. Fred (2001) propose de contourner le problème de la correspondance des groupes en utilisant une matrice de co-association individus-individus qui ne considère que la fréquence avec laquelle deux exemples se retrouvent dans un même groupe.\nCertaines méthodes ensemblistes supervisées ont déjà été l\u0027objet de transpositions au domaine non supervisé. Leisch (1999) a par exemple appliqué le bagging en contexte non supervisé pour proposer l\u0027algorithme bagged clustering, que nous utiliserons comme sous-procédure de notre contribution. L\u0027application du boosting à la classification non supervisée a déjà été abordée dans Frossyniotis et al. (2004), où la méthode boost-clust repondère itérativement les exemples difficiles et forme une partition floue optimale. Toutefois, cet algorithme transpose par analogie et sans réelles justifications des concepts justifiés en contexte supervisé. L\u0027apport le plus significatif des auteurs réside dans l\u0027utilisation des vecteurs d\u0027appartenance des partitions floues pour détecter les exemples sensibles. Les expérimentations proposées, trop succintes, ne permettent malheureusement pas d\u0027évaluer de façon satisfaisante l\u0027efficacité de la méthode et de ses différentes variantes.\nL\u0027algorithme UBLA\nL\u0027algorithme UBLA (Unsupervised Boosting-Like Approach) constitue une nouvelle approche inspirée du boosting, qui va construire itérativement une partition dure des données. L\u0027algorithme se déroule en quatre phases, trois phases sont répétées à chaque itération de la procédure tandis que la quatrième et dernière phase établit la partition finale :\n1. Chacune des t itérations de la procédure \"boostée\" commence par une phase d\u0027évalua-tion qui va permettre la sélection des exemples difficiles. Au cours de cette évaluation les c-moyennes floues (Bezdek (1981)) sont effectuées une seule fois. Le caractère flou est utilisé pour calculer les critères de qualité à partir des degrés d\u0027appartenance et les exemples vont être repondérés directement à la fin de cette phase d\u0027évaluation.\n2. Une deuxième phase commence alors, appelée phase de regroupement/stabilisation, qui constitue la sous-procédure de bagged clustering, où les c-moyennes floues classiques sont appliquées sur dix échantillons bootstrap obtenus à partir de la nouvelle distribution des poids. Il est important de noter que le fait d\u0027opérer la repondération à la fin de la première phase entraîne la non utilisation du jeu de données initial pendant tout l\u0027algorithme UBLA. Ainsi, même à la première itération, la procédure de bagging est effectuée à partir d\u0027une nouvelle distribution des poids. Cet aspect implique que toutes les partitions sont obtenues à partir de distributions de poids non uniformes.\n3. Le problème de correspondance entre les différents groupes est ici contourné par l\u0027utilisation d\u0027une matrice de co-association (Fred (2001)) qui intervient lors de la phase 3 de l\u0027algorithme. Ainsi, les points sont considérés deux à deux et la matrice A, individus-individus, est mise à jour lorsque deux points sont dans une même classe pour une partition d\u0027une itération donnée. 4. A la fin des itérations dites de \"boosting\", la partition finale est formée par un vote majoritaire sur la matrice A construite itérativement (phase 4). Avant de détailler le processus (algorithme 1), il semble indispensable de s\u0027attarder dans les sous-sections suivantes sur quelques points essentiels de la méthode UBLA.\nLe critère local de qualité d\u0027un exemple\nUne approche inspirée du boosting doit mettre l\u0027accent par itérations successives sur les exemples dits difficiles à classer. Comment détecter de tels exemples ? Là où cette notion est très intuitive en apprentissage supervisé (un exemple difficile est par essence mal classé par le classifieur choisi), il convient en apprentissage non supervisé de quantifier la qualité du positionnement de chaque exemple, du fait -dans la grande majorité des cas-de la non connaissance a priori de la structure des données. Il est judicieux, tout comme Dans ce cas, l\u0027entropie de Shannon de U 1 est maximale, soit E 1 \u003d 2. Nous sommes dans l\u0027incertitude la plus totale, l\u0027exemple pouvant appartenir avec la même croyance à n\u0027importe quelle classe. Au contraire, un exemple très bien classé, dont l\u0027appartenance à une des quatre classes apparaît clairement, possédera un vecteur U de la forme suivante :\nIci, l\u0027entropie de Shannon est minimale, soit E 2 \u003d 0. Le cas succinctement exposé traduit le fait qu\u0027un exemple devra être d\u0027autant plus repondéré que son critère de qualité local, à savoir l\u0027entropie de ses degrés d\u0027appartenance, est fort. Ainsi, les exemples semblant difficiles à classer seront favorisés (aucun dégré d\u0027appartenance à une classe ne se détache). Ce type de critère n\u0027est d\u0027ailleurs pas sans rappeler la notion de rejet en contexte supervisé (Leray et al. (2000)).\nLe critère global de qualité\nParallèlement à la qualité locale d\u0027un exemple, la repondération doit tenir compte conjointement de la qualité globale du partitionnement. D\u0027aucuns y verront une analogie avec l\u0027erreur en apprentissage utilisée par Freund et Schapire (1997) dans le boosting pour construire un coefficient qui entre en compte dans la favorisation exponentielle des exemples mal classés. Cette qualité globale est aussi et surtout envisagée par souci de logique. En effet, pourquoi repondérer un exemple apparemment difficile si la qualité de la partition dans laquelle il se trouve est médiocre ?\nPour conserver une certaine cohérence dans l\u0027approche, il est logique que le critère global de qualité du partitionnement en cours soit la moyenne des critères locaux. Ainsi, à la b ieme itération, le critère de qualité C b est la moyenne des E i courants. Plus précisement, ce critère offre la possibilité de repondérer un exemple en tenant compte non seulement de la qualité de classement intrinsèque au partitionnement (c\u0027est le critère local E i ) mais aussi de la qualité globale du regroupement dans lequel il se trouve (critère C b ).\nLa repondération des exemples difficiles\nLes performances du boosting reposent en partie sur la construction d\u0027un ensemble d\u0027hypothèses faibles construites à partir de distributions statistiques dans lesquelles sont favorisés les exemples dits difficiles. C\u0027est ce principe qui est transposé dans notre proposition en prenant pour hypothèse de base l\u0027algorithme classique des c-moyennes floues. L\u0027intérêt du choix d\u0027une partition floue réside bien sûr dans la possibilité de raisonner en terme de degré d\u0027appartenance. L\u0027erreur en apprentissage est donc remplacée ici par deux critères de qualité, local et global, définis ci-dessus. Il s\u0027agit par la suite de mettre à jour les poids de chaque exemple en tenant compte de la qualité globale de la partition puis de la qualité locale de classification d\u0027un point. A chaque itération b, le poids d\u0027un exemple p est ajusté de la façon décrite en (1) (cf algorithme 1). Par conséquent, les poids des exemples dont l\u0027entropie des degrés d\u0027appartenance est supérieure à la moyenne de l\u0027échantillon sont augmentés (E i \u003e C b ), tandis que ceux des autres se voient diminuer par normalisation. L\u0027argument de l\u0027exponentielle comporte deux termes. Le premier terme (log 2 K ? C b ) se rapproche de zéro lorsque la qualité globale de la partition est mauvaise (log 2 K est la borne supérieure du coefficient d\u0027entropie à minimiser). Le deuxième terme (E i ) représente la qualité individuelle de classification d\u0027un exemple i et est d\u0027autant plus élévé que l\u0027exemple est difficile. Logiquement, les exemples qui prendront exponentiellement le plus d\u0027importance seront les points mal classés dans une partition de bonne qualité.\nUtilisation d\u0027une sous-procédure : le bagged clustering\nUne autre approche inspirée des méthodes supervisées, en l\u0027occurence le bagging (Breiman (1996)), peut être transposée en contexte non supervisé. Cette méthode, appelée bagged clustering a été proposée par Leisch (1999). La procédure combine une méthode de regroupement (centres-mobiles, c-moyennes floues) avec une classification hiérarchique. La nouveauté réside dans l\u0027application de la méthode de clustering à plusieurs échantillons bootstrap du jeu de données de départ, puis dans la transposition du problème initial dans l\u0027espace des centres formés par la méthode de base sur ces échantillons bootstrap. Le nombre de classes a priori est donc donné suffisament grand, le regroupement (centres-mobiles ou c-moyennes floues) est appliqué sur chaque échantillon bootstrap puis tous les centres finaux sont regroupés dans une matrice. Une classification hiérarchique sur les centres est ensuite effectuée puis chaque point est assigné à la classe qui contient le centre dont il est le plus proche. La figure 1 résume la procédure.\nETAPE 1 : A partir de la distribution des poids W, tirage de 10 échantillons Bootstrap du jeu de données.\n-MEANS -MEANS -MEANS -MEANS FUZZY-C-MEANS FUZZY-C-MEANS FUZZY-C-MEANS FUZZY-C FUZZY-C FUZZY-C FUZZY-C-MEANS -MEANS FUZZY-C-MEANS -MEANS FUZZY-C FUZZY-C FUZZY-C-MEANS -MEANS FUZZY-C FUZZY-C-MEANS -MEANS FUZZY-C FUZZY-C FUZZY-C-MEANS -MEANS FUZZY-C-MEANS\nETAPE 2 : Application des c-moyennes floues sur chacun des échantillons avec un nombre élévé de centres. Par exemple, 15 centres. Récupération de la matrice finale des centres de taille (10 * 15 lignes, P colonnes), où P est le nombre d\u0027attributs. ETAPE 3 : Classification hiérarchique ascendante sur la nouvelle matrice des centres. 1500   1000   500   Height   0   65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  97  98  99  100  89  90  91  92  93  94  95  96  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 ETAPE 4 : La partition dure finale est obtenue en coupant le dendrogramme au nombre de classes K désiré. Ensuite, parmi les K classes obtenues, chaque exemple est affecté à la classe qui contient le centre dont il était le plus proche parmi les 10*15 centres.\nCluster Dendrogram\nFIG. 1 -La procédure de bagged clustering.\nIl a semblé intéressant d\u0027introduire ce processus comme une sous-procédure de l\u0027algorithme, qui stabilisera les résultats et donc la partition formée, sans pour autant dénaturer l\u0027idée originale de repondérations successives.\nComplexité de l\u0027algorithme\nA l\u0027intérieur des phases 1 et 2, la complexité algorithmique reste linéaire en fonction de N . Lors de la phase 3, la complexité est quadratique (O(N 2 )), à une constante près liée au nombre d\u0027itérations T de boosting. On remarquera que l\u0027utilisation d\u0027une classification hié-rarchique n\u0027augmente pas la complexité car celle-ci est effectuée sur la matrice des centres, dont la taille est liée à des constantes. Enfin, la formation de la partition finale (phase 4) s\u0027effectue en O(N 2 ) par parcours de la matrice A. Finalement, c\u0027est l\u0027utilisation d\u0027une matrice de co-association qui borne de façon générale la complexité de la procédure UBLA en O(N 2 ). , regrouper les deux exemples dans la même classe. Les éventuels exemples \"seuls\" formeront des singletons. fin\nExpérimentations\nChoix du critère de qualité externe de la partition finale\nUn problème majeur de l\u0027apprentissage non supervisé réside dans l\u0027absence de juge de paix pour départager les différentes méthodes. En classification supervisée, ce juge existe et est aussi bien local (erreur de prédiction) que global (taux d\u0027erreur). Nous avons donc arbitrairement choisi un juge de paix qui n\u0027est ici que global et qui compare la qualité des partitions finales. Les différentes mesures de validation des partitions ont été bien résumées dans Halkidi et al. (2001). Pour valider expérimentalement les performances de l\u0027approche UBLA, à savoir la qualité de la partition dure proposée, des indices fondés sur des mesures de compacité (dispersion intra-classe) et de séparation (dispersion inter-classe) semblent bien adaptés car les algorithmes de regroupement classiques cherchent également à optimiser des critères liés aux mêmes notions. Ainsi, des indices comme l\u0027indice de Dunn, la silhouette moyenne et le ratio intra/inter (wb) sont tout à fait pertinents. Une combinaison de ces trois indices peut être intéressante dans les cas où les valeurs seront très serrées. Rappelons que l\u0027indice de Dunn et la silhouette moyenne d\u0027une partition sont à maximiser tandis que le ratio intra/inter est à minimiser. L\u0027indice de qualité suivant : Ind \u003d Dunn + silhouette ? ratiowb combinera donc simplement les trois indices et sera lui aussi à maximiser. La comparaison s\u0027est effectuée entre la méthode UBLA, les centres-mobiles de MacQueen (1967), les c-moyennes floues de Bezdek (1981) et la procédure de bagged clustering classique de Leisch (1999), qui constitue rappelons-le une sous-procédure de notre proposition. Dans la grande majorité des cas, la meilleure partition domine les trois autres au sens des trois indices (maximisation de Dunn et de la silhouette moyenne, minimisation du ratio wb par rapport aux deux autres). Mais pour les rares cas plus incertains, la valeur finale de Ind est prise pour sélectionner la méthode formant la meilleure partition pour une expérimentation précise.\nRésultats\nLa méthode UBLA a été testée dans l\u0027environnement statistique R sur onze jeux de données, disponibles dans les bases de données bien connues du web. Comme le rappelait Diday (1974), les algorithmes de regroupement peuvent fournir des solutions satisfaisantes, mais qui ne sont pas forcément optimales. Ainsi, plusieurs répétitions des centres-mobiles sur un même jeu de données peuvent donner des résultats différents. Par conséquent, nous avons pour nos expérimentations rélancé les différents algorithmes dix fois et sélectionné le meilleur résultat pour chaque méthode. Il serait d\u0027ailleurs intéressant d\u0027analyser la stabilité des partitions des algorithmes de regroupement comme dans Bertrand et Bel Mufti (2006). Les caractéristiques des jeux de données (nombre d\u0027individus et de variables) sont rappellées dans le tableau suivant. La valeur de l\u0027indice de qualité Ind, le nombre d\u0027itérations T et le nombre final de groupes formé par notre méthode (qui peut rappelons-le être différent de K), sont également introduits dans le tableau 1. Dans le cas où le nombre K final de classes formées par U BLA était différent du K initial, c\u0027est avec ce K final que le calcul de l\u0027indice de qualité, qui dépend du nombre de classes, s\u0027effectue pour les centres-mobiles et les c-moyennes floues (ceci assure la cohérence des comparaisons des partitions formées).\nLes résultats obtenus sont assez intéressants. Sur un total de 55 expérimentations, la mé-thode UBLA surpasse les centres mobiles et les c-moyennes floues dans 47 cas. (Il y a en plus un ex-aequo). En considérant les comparaisons de UBLA aux deux méthodes de manière indé-pendante, il est observé que, toujours au sens de l\u0027indice de qualité Ind, notre approche domine d\u0027une part les centres-mobiles dans 39 cas sur 45, de l\u0027autre améliore les c-moyennes floues dans 41 cas sur 45. La méthode UBLA est supérieure au bagged clustering dans 37 cas sur 55 et égale dans 8 cas sur 55. Nous pouvons constater que le nombre d\u0027itérations est généralement assez faible, contrairement à ce qui pouvait être attendu d\u0027une procédure de boosting. Un trop grand nombre d\u0027itérations peut même dans certains cas entraîner des chutes de performances, les répondérations successives des exemples déformant trop la structure initiale du jeu de données. Pour conclure, nous pouvons dire que les résultats montrent qu\u0027un processus ensembliste \nIllustration graphique\nLa figure 2 compare les partitions formées par l\u0027approche UBLA et des c-moyennes floues pour le jeu de données des IRIS de Fisher, bien connu des statisticiens. Il s\u0027agit de 150 individus (des plantes) décrits par 4 variables. Il existe donc six plans de représentation possibles pour ces données. Pour cet exemple, le nombre de classes donné a priori était K \u003d 2. La figure 3 illustre quant à elle la classification en trois classes du jeu de données DNase (deux dimensions).\nclassification des iris par les c?moyennes classiques c l assification des iris par la méthode UBLA \nConclusion\nDans cet article, nous avons proposé une nouvelle approche ensembliste en apprentissage non supervisé, qui s\u0027inspire des principes du boosting. A chaque itération, l\u0027algorithme repère au sens de certains critères les exemples difficiles pour leur donner plus d\u0027importance à l\u0027itéra-tion suivante. Cette approche se sert ainsi dans un premier temps d\u0027une partition floue, formée par la méthode des c-moyennes floues pondérées, pour glisser ensuite vers une partition finale dure. \nFIG. 3 -Visualisation de la classification du jeu de données DNase (2D) par les c-moyennes classiques (à gauche) puis UBLA (à droite). Nous remarquons une zone sensible au milieugauche de la figure ou deux classes ne sont pas nettement séparées par les c-moyennes floues. A droite, l\u0027incertitude est corrigée.\nLes premiers résultats sont prometteurs et laissent penser qu\u0027une telle approche peut amé-liorer la qualité des partitions finales formées en termes de compacité et de séparation. A présent, il serait pertinent de comparer les performances de la méthode UBLA avec plusieurs autres approches ensemblistes proposées dans le domaine non supervisé (ex : Frossyniotis et al. (2004)). Parmi les perspectives de ce travail, il faudrait trouver une alternative à la matrice de co-association qui est coûteuse et proposer de nouveaux critères de qualité. Par exemple en implémentant des critères qui ne soient pas essentiellement fondés sur des mesures de dispersion intra-et inter-classe. Il serait de plus envisageable de s\u0027affranchir de l\u0027utilisation de la procé-dure de bagged clustering pour implémenter notre propre procédure de stabilité, fondée sur des tirages bootstrap ou des modifications de l\u0027échantillon par rapport à la nouvelle pondération.\nPour conclure, nous pouvons dire que la transposition du boosting à l\u0027apprentissage non supervisé doit se faire avec la plus grande prudence. En effet, des concepts justifiés théo-riquement en contexte supervisé ne s\u0027appliquent pas de façon directe en classification non supervisée. Notre travail se veut donc simplement inspiré de certains concepts du boosting, principalement la repondération des exemples difficiles. Il reste maintenant à approfondir le cadre théorique de la méthode proposée.\nRéférences Bertrand, P. et G. Bel Mufti (2006 \n"
  },
  {
    "id": "889",
    "text": "Introduction\nDans le domaine du bâtiment, une masse croissante de normes régissent l\u0027exécution des projets de construction (e.g. bâtiments publics, maisons individuelles) et de nombreuses initiatives 1 sont lancées pour fournir des services électroniques de régulation. Un des objectifs généraux en est l\u0027automatisation du contrôle de la conformité d\u0027un projet de construction par rapport à un ensemble de normes techniques du bâtiment en vigueur. Cela constitue le cadre de notre travail au CSTB 2 et nous proposons ici un modèle de contrôle de conformité. Les projets de construction sont maintenant communément décrits dans le modèle IFC 3 , un modèle orienté objet développé par l\u0027IAI 4 pour faciliter l\u0027interopérabilité dans le domaine de la construction. Il est pourvu d\u0027une syntaxe ifcXML 5 ; des données ifcXML peuvent être automatiquement générées par les outils de COA dédiés à l\u0027architecture ou par les convertis- www.iai-international.org/Model/IFC(ifcXML)Specs.html seurs de données EXPRESS 6 . Cependant, le langage de représentation ifcXML ne permet pas de capturer toute la sémantique des données d\u0027un projet de construction indispensables pour vérifier la conformité de ce projet (Lima et al. 2006). Pour ce faire, nous proposons de construire une ontologie dédiée au contrôle de conformité à partir des classes du modèle IFC dont le choix est guidé par les concepts retrouvés dans les normes techniques. Ensuite, nous extrayons des données IFC des représentations de projets de construction qui reposent sur cette ontologie. Dans la perspective d\u0027un service électronique de régulation disponible sur le web, nous adoptons les langages du web sémantique pour représenter formellement projets de construction, normes techniques et ontologie de contrôle de conformité.\nLe coeur de notre modèle du contrôle de conformité consiste en l\u0027appariement des repré-sentations des normes avec celles des projets de construction. Son efficacité repose sur l\u0027acquisition de connaissances ontologiques et sur le processus d\u0027extraction de la représenta-tion d\u0027un projet orienté contrôle de conformité, qui prend en compte les connaissances ontologiques acquises. Elle repose ensuite sur des méta-connaissances acquises auprès des experts du CSTB qui permettent de guider le contrôle lui-même. Enfin, nous extrayons des annotations des normes et organisons leurs représentations selon ces méta-annotations. Cela permet de définir un ordonnancement des appariements à réaliser pour valider un projet par rapport à ces normes. Ces annotations sont également utilisées pour expliquer à l\u0027utilisateur les résultats du processus de validation, en particulier en cas d\u0027échec.\nDans la section 2 nous décrivons le processus d\u0027acquisition de la description d\u0027un projet de construction, guidé par l\u0027ontologie de contrôle de conformité. La section 3 est dédiée au modèle de contrôle de conformité proprement dit et la section 4 présente l\u0027ensemble de la méthode et du système C3R.\nAcquisition de la représentation utile d\u0027un projet\nGuidés par le but spécifique du contrôle de conformité, nous définissons un modèle de représentation d\u0027un projet de construction qui ne contient que les éléments utiles au processus de contrôle.\nNotre méthode d\u0027acquisition de connaissances requiert dans une première étape d\u0027expliciter des représentations formelles des normes techniques. Nous ne nous intéressons pas ici au problème d\u0027extraction de connaissances à partir de textes qui dépasse le cadre de notre travail. Dans notre cas, nos avons explicité manuellement auprès d\u0027experts du CSTB et à partir de documentations techniques une base de contraintes représentant des normes relatives à l\u0027accessibilité des bâtiments. Nous les représentons dans le langage SPARQL. En effet, une norme de construction peut être représentée par un ensemble de couples des requê-tes. Dans chaque couple, la première requête exprime une condition d\u0027application (e.g. D\u0027autre part, nous exploitons le CD REEF 7 , afin de développer des annotations RDF des requêtes de conformité contenant des méta-connaissances sur les normes de conformité, à partir desquelles nous avons formalisé ces requêtes. Ce second type d\u0027annotation est relatif à l\u0027origine de la requête de conformité (e.g. référence à un texte légal d\u0027où elle a été extraite, article d\u0027extraction, date de publication, etc.). Elles sont utilisées pour organiser la base de requêtes et pour expliquer à l\u0027utilisateur les résultats du contrôle de conformité d\u0027un projet.\nLa seconde étape de notre méthode d\u0027acquisition est dédiée à la construction automatique d\u0027une ontologie de contrôle de conformité, à partir des entités du modèle IFC intervenant dans les représentations des normes. Précisément, nous construisons une hiérarchie de concepts dans le langage RDFS et nous l\u0027enrichissons ensuite par des concepts non IFC qui apparaissent dans la description des normes. La place de ces nouveaux concepts dans l\u0027ontologie est identifiée par un expert du domaine.\nVient alors l\u0027étape d\u0027acquisition de la représentation d\u0027un projet de construction utile au contrôle de conformité. Cette représentation est extraite des données IFC relatives au projet. Le processus d\u0027acquisition est guidé par l\u0027ontologie de contrôle construite lors de l\u0027étape précédente. Nous extrayons une représentation RDF d\u0027un projet de construction par transformation XSLT de données ifcXML qui ne conserve que la partie de la description des entités/relations IFC utile au contrôle de conformité.\nLes requêtes SPARQL qui représentent les normes techniques que les projets de construction doivent satisfaire jouent ainsi le rôle de design pattern dans la construction des représentations de projets. De fait, nous ne conservons de la transformation XSLT des données ifcXML que les triplets RDF qui font intervenir des concepts présents dans l\u0027ontologie de contrôle de conformité.\nLa représentation RDF d\u0027un projet de construction extraite par transformation XSLT de données ifcXML est ensuite enrichie automatiquement par l\u0027application en chaînage avant des règles ontologiques définies par des experts. C\u0027est ce qui pourra rendre possible l\u0027appariement de la représentation d\u0027un projet avec celle d\u0027une règle lorsque celle-ci fait intervenir des concepts non IFC.\nModèle de contrôle de conformité\nNotre modèle du contrôle de conformité repose sur l\u0027appariement des représentations des normes avec celles des projets de construction : l\u0027appariement d\u0027une requête SPARQL de conformité à une représentation RDF d\u0027un projet de construction. Ces opérations d\u0027homomorphisme de graphes sont maintenant bien connues et nous nous reposons sur les travaux de (Baget, 2005) et (Corby et al., 2006).\nAfin de rendre efficace le processus de contrôle, la base des requêtes de conformité représentant les normes est organisée selon leurs méta-annotations (extraites du CD-REEF) de sorte qu\u0027un ordre des appariements à effectuer peut être défini qui capture l\u0027expérience experte acquise. Nous avons ainsi explicité auprès d\u0027experts du CSTB des critères de classification des requêtes correspondant a une classification des normes que représentent les requê-tes : thématique (e.g. accessibilité, acoustique), domaine d\u0027application (e.g. établissements recevant du public), sujet d\u0027une règle (e.g. elle s\u0027applique à un « hall d\u0027entrée »), etc. D\u0027autre part, nous classons les requêtes de conformité sur la base des relations de spécialisa-tion/généralisation qui existent entre leurs patterns de graphes. Ainsi, s\u0027il n\u0027existe pas d\u0027appariement d\u0027une requête donnée à l\u0027annotation d\u0027un projet de construction, nous évite-rons de chercher inutilement des appariements des requêtes dont les graphes patterns seraient des spécialisations de celui de la première : il n\u0027en existe pas.\nLa formalisation du raisonnement expert passe premièrement par l\u0027analyse de l\u0027ensemble des requêtes de conformité choisies par l\u0027utilisateur, afin de vérifier la non existence de contradiction entre ces requêtes. Ensuite le choix par l\u0027utilisateur de certaines requêtes peut suggérer la prise en compte d\u0027autres requêtes complémentaires qu\u0027il s\u0027agit alors de lui proposer d\u0027intégrer à sa sélection. Le traitement de requêtes dépend de l\u0027organisation de la base de requêtes. Un algorithme d\u0027application des requêtes de conformité modélisant un raisonnement expert prend en compte les principes suivants : (i) l\u0027ordre de traitement des requêtes dépend des priorités relatives entre les classes de requêtes (e.g. sur une classification des requêtes selon leur niveau hiérarchique, celles représentant des lois sont prioritaires à celles représentant des décrets) ; (ii) au sein d\u0027une même classe de requêtes, celles relatives aux connaissances les plus spécialisées sont prioritaires, c\u0027est-à-dire celles dont le graphe pattern est une spécialisation de celui d\u0027une autre (e.g. une requête relative à une porte d\u0027entrée est prioritaire par rapport la même requête relative à une porte car dans le cas de non-conformité à la première, on en déduit la non-conformité à la seconde).\nLes résultats de l\u0027appariement sont ensuite analysés pour interpréter les causes d\u0027une éventuelle non validation d\u0027un projet de construction. Il s\u0027agit tout d\u0027abord de lister les requêtes de conformité qui échouent : (i) qui ont effectivement échoué ; (ii) pour lesquelles on peut déduire automatiquement l\u0027échec (celles dont le graphe pattern est plus général que d\u0027autres qui ont échoué et celles dont l\u0027annotation représentant la condition d\u0027application est plus générale que celles d\u0027autres pour lesquelles l\u0027appariement de leur annotation avec l\u0027annotation du projet a échoué) ; (iii) qui ont échoué car l\u0027annotation du projet de construction ne contient pas les connaissances nécessaires à la réalisation un appariement.\nL\u0027aboutissement du processus de contrôle de conformité est la génération d\u0027un rapport de conformité. Pour chaque requête, le rapport indique si elle a réussi ou échoué ; dans le cas d\u0027un échec, il indique les causes de non-conformité ou les insuffisances de l\u0027annotation du projet (pour chaque requête qui échoue, quels éléments de l\u0027annotation du projet de construction sont responsables de l\u0027échec, c\u0027est-à-dire empêchent l\u0027appariement).\nC3R : un modèle et un système\nNous avons baptisé le modèle de contrôle de conformité que nous proposons C3R ( fig.1). Les principales étapes de C3R sont les suivantes : (1) Acquisition des requêtes de conformité ; (2) Acquisition des connaissances ontologiques ; (3) Organisation hiérarchique des requêtes ; (4) Acquisition de l\u0027annotation d\u0027un projet de construction ; (5) Validation par appariement et méta-raisonnement ; (6) Génération d\u0027un bilan de conformité. L\u0027implémentation de C3R est en cours ; elle repose sur le moteur sémantique CORESE 8 développé à l\u0027INRIA (Corby et al., 2006). Il offre une implémentation des langages RDF,8 Conceptual Resource Search Engine, http://www-sop.inria.fr/acacia/soft/corese RDFS et SPARQL. Les mécanismes de raisonnement reposent sur une représentation interne des connaissances ontologiques et assertionnelles dans le modèle des graphes conceptuels (Sowa, 1984) (Berners-Lee, 2001). Il permet de chercher les réponses à une requête SPARQL relativement à une base RDF, en prenant en compte des connaissances ontologiques représentées en RDFS (Corby et Faron-Zucker, 2007). Il est en outre muni d\u0027un moteur de règles d\u0027inférence qui permet en chaînage avant de compléter une base de faits RDF à l\u0027aide de règles ontologiques représentées sous la forme de couples de graphes RDF (Baget et Mugnier, 2002).\nFIG. 1 -C3R in a nutshell\nTravaux connexes\nD\u0027autres travaux de recherche sont en cours visant à la standardisation des représenta-tions de projets de construction -sans s\u0027intéresser cependant au problème particulier du contrôle de conformité. Parmi eux, certains adoptent comme nous une approche ontologique, (i) buildingSMART (Bell et Bjorkhaus, 2006)  Le problème du contrôle de conformité se pose dans d\u0027autres domaines d\u0027applications et nos travaux se rapprochent en ce sens de ceux sur la validation, parmi lesquels nous pouvons citer (Dibie-Barthélemy et al., 2004). Leur travail est consacré au problème de la validation de bases de connaissances construites sur le modèle des graphes conceptuels (Sowa, 1984), des contraintes étant exprimées sous la forme de graphes conceptuels certifiés fiables.\nConclusion et perspectives\nNous avons présenté le modèle C3R pour le contrôle semi-automatique de la conformité d\u0027un projet de construction aux normes techniques du bâtiment en vigueur. Nous adoptons une approche ontologique pour résoudre ce problème et le système qui implémente C3R repose sur les langages et techniques du web sémantique. Un projet est valide par rapport à une norme si la représentation RDF du projet est une réponse à la requête SPARQL de conformité ; une norme s\u0027applique à un projet si l\u0027annotation de la requête de conformité peut être RNTI -X -appariée à celle du projet. L\u0027acquisition de connaissances ontologiques est la clé de voûte de notre approche. L\u0027extraction de la représentation d\u0027un projet de construction à partir de données IFC est guidée par une ontologie de contrôle de conformité que nous construisons à partir du modèle IFC et par explicitation de connaissances expertes. Les requêtes de conformité et leurs annotations sont acquises à partir de données techniques et par explicitation de connaissances expertes au CSTB. L\u0027organisation de la base de requêtes repose sur les annotations de celles-ci ; elle rend plus efficace et explicable à l\u0027utilisateur le processus de contrôle de conformité d\u0027un projet dans son ensemble. L\u0027implémentation de C3R est en cours ; un premier prototype simple existe et nous pré-parons l\u0027organisation de la base de requêtes et son évaluation auprès d\u0027experts du CSTB.\n"
  },
  {
    "id": "890",
    "text": "Introduction\nLes systèmes de supervision de la plupart des applications industrielles génèrent une très grande quantité d\u0027informations et les collectent dans des bases de données. Ce papier concerne la découverte de modèles de chroniques à partir de séquences d\u0027événements. Chaque événe-ment appartient à une certaine classe. Selon l\u0027approche stochastique (Le Goc et al. (2005)), un ensemble de séquences est représenté sous la forme d\u0027une chaîne de Markov afin de l\u0027utiliser par la suite pour générer un modèle de chroniques (Le Goc et al. (2005)) sous forme de relations binaires entre classes d\u0027événements C i ? C o . Le nombre des relations binaires peut être très grand, par conséquent une réduction de ce nombre est nécessaire. Pour cela, nous proposons une adaptation de la J-Measure de la théorie de l\u0027information aux chaînes de Markov, la BJ-Measure, pour formuler des heuristiques d\u0027élimination d\u0027hypothèses.\nÉlagage d\u0027un modèle de chroniques\nConsidérant la propriété d\u0027absence de mémoire de la chaîne de Markov, la relation C i ? C o entre deux classes C i et C o peut être considérée comme l\u0027une des quatre relations entre deux variables aléatoires binaires\nNous mesurons cet écart par la formule suivante :\nSoit S \u003d {C i ? C o } un ensemble de relations binaires construites à partir de la séquence ?. Selon la propriété d\u0027absence de mémoire de la chaîne de Markov, les relations binaires contenues dans S sont indépendantes. L\u0027ensemble S est vu comme une succession de plusieurs canals binaires de transmissions sans mémoire. La BJ-Measure d\u0027un chemin M \u003d {C i ? C i+1 } i\u003d0...n?1 est le produit de nombre de relations binaires et la somme des BJ-Measure de chaque relation binaire\ni\u003d0,...,n?1 \nFIG. 1 -Expertise (1995)\nFIG. 2 -Relations observées en 2007\ndes occurrences associées à la variable appelée omega. La séquence étudiée contient 7682 occurrences de classes d\u0027événements. Le nombre des relations binaires générées est 3199999. L\u0027application de l\u0027heuristique L(M ) permet d\u0027élaguer l\u0027ensemble des relations afin de garder que 195 ? 1 relations binaires. Grâce à la définition de la notion de classe, nous avons construit un modèle fonctionnel en substituant chacun des identifiants de classe par la variable associée. Le graphe de la figure 2 indique les variables ayant un impact sur la variable omega. Ce graphe peut être comparé avec les connaissances a priori formulées par les experts en 1995 (cf Figure  1). Le graphe (Figure 1) donné par les connaissances des experts est inclus dans celui donné par l\u0027Approche Stochastique (figure 2) sauf en ce qui concerne le sens de la relation entre les variables F T et BD.\nRéférences\nLe Goc, M., P. Bouché, et N. Giambiasi (2005) \nSummary\nIn this paper, we propose to adapt the Information Theory J-Measure to Markov chains, the BJ-Measure, to define heuristics to prune the set of binary relations generated by the stochastic approach.\n"
  },
  {
    "id": "891",
    "text": "Introduction\nL\u0027objectif de nos travaux est de faciliter la recherche d\u0027information dans des pages Web par l\u0027utilisation conjointe de treillis de Galois et d\u0027ontologies, qui constitue ce que nous appelons un « contexte conceptuel ». Les regroupements conceptuels fournis par les treillis, associés aux liens sémantiques de l\u0027ontologie, permettent d\u0027améliorer la recherche d\u0027information en fournissant des niveaux de navigation plus abstraits et complémentaires.\nLes treillis de Galois restent néanmoins complexes du fait du nombre élevé de concepts qu\u0027ils sont susceptibles de contenir et l\u0027objectif de ce papier est de proposer une mesure de similarité entre ces concepts pour trouver les plus pertinents à conseiller à un utilisateur durant sa navigation dans un treillis.\nCe papier est organisé comme suit : dans un premier temps, la section 2 présente les outils mis en oeuvre dans nos travaux, à savoir les treillis de Galois et les ontologies, ainsi que la manière dont ils peuvent être associés pour la recherche d\u0027information. Dans la section 3, nous présentons un état de l\u0027art des mesures de similarité définies pour l\u0027appariement d\u0027ontologies. Nous étendons l\u0027une de ces mesures pour proposer, dans la section 4, une mesure de similarité adaptée aux concepts d\u0027un treillis de Galois ; nous présentons également la manière dont nous la mettons en oeuvre pour faciliter la navigation en indiquant les concepts les plus pertinents pour une requête donnée ou à partir de la position courante dans le treillis. Finalement, nous décrivons dans la section 5 une petite expérimentation menée sur un ensemble de pages Web dédiées au tourisme, avant de conclure et de présenter les perspectives de ce travail.\nExistant\nLes travaux présentés ici s\u0027inscrivent dans une méthodologie plus large décrite dans  dont le but est de faciliter la recherche d\u0027information dans des systèmes d\u0027information complexes. Cette méthodologie est fondée sur l\u0027utilisation conjointe de treillis de Galois et de structures sémantiques, par exemple des thesaurus ou des ontologies, pour fournir trois niveaux de navigation dans les données explorées. Ces trois niveaux de navigation proposés sont illustrés sur la figure 1 et décrits dans la section suivante.\nArchitecture\nLa complexité des systèmes d\u0027information peut se traduire de différentes manières, qu\u0027il s\u0027agisse d\u0027un volume important, d\u0027un nombre élevé de dimensions, d\u0027un manque de structure ou des relations et corrélations entre les données du système. Concernant ces deux derniers points en particulier, la construction de treillis de Galois à partir des données initiales permet de construire une structure sur les données et de montrer leurs relations en regroupant, sous forme de classes recouvrantes, les données présentant des caractéristiques communes. Le treillis nous fournit ainsi un niveau d\u0027abstraction au-dessus des données brutes. Ce niveau est appelé niveau conceptuel. A ce niveau, plusieurs treillis peuvent être construits sur différents ensembles de données. Chaque treillis représente un contexte conceptuel et constitue un espace de recherche pour l\u0027utilisateur. Nous distinguons le contexte conceptuel global, défini par le treillis, et le contexte conceptuel instantané, qui dépend du treillis mais aussi de la requête de l\u0027utilisateur ou de sa navigation. Ces deux notions sont présentées dans  et définies formellement dans (Polaillon et al., 2007). Les ensembles de données sur lesquels les différents treillis sont calculés peuvent posséder des intersections non vides. Dans ce cas, une même donnée pourra appartenir à plusieurs contextes conceptuels globaux permettant ainsi une navigation entre différents treillis. Le niveau sémantique est construit au-dessus du niveau conceptuel. Il est peuplé d\u0027ontologies dont les concepts sont reliés à des concepts des treillis du niveau conceptuel. L\u0027objectif du niveau sémantique est double : il permet à l\u0027utilisateur de naviguer vers des concepts dont la généralisation n\u0027existe pas dans RNTI -X -les contextes conceptuels globaux ainsi que de passer d\u0027un contexte global à un autre lorsqu\u0027ils ne possèdent pas d\u0027intersection.\nFIG.1 -Trois niveaux de navigation\nMise en oeuvre de l\u0027architecture\nLa mise en oeuvre de l\u0027architecture consiste à sélectionner les ensembles de données à explorer, choisir les ontologies du niveau sémantique, construire les treillis de Galois sur les ensembles de données sélectionnés puis à labelliser les concepts des treillis avec un vocabulaire contrôlé issu des ontologies choisies au niveau sémantique. C\u0027est grâce à cette labellisation que sont construits les liens entre les contextes globaux et les ontologies. Cette mise en oeuvre est effectuée hors ligne du fait des temps de calcul engendrés par la construction des treillis et par la labellisation.\nUne fois cette étape terminée, l\u0027utilisateur peut explorer les ensembles de données au travers des concepts des treillis du niveau conceptuel ou du niveau sémantique. A un instant t, l\u0027utilisateur se trouve sur un concept du niveau conceptuel ou sémantique. A partir du concept courant où se il trouve, l\u0027utilisateur peut naviguer vers un autre concept plus général ou plus spécialisé que le concept courant.\nOutils conceptuels et sémantiques\nDans cette section, nous nous positionnons et rappelons rapidement les outils nous permettant l\u0027utilisation conjointe de treillis de Galois et d\u0027ontologies.\nTreillis de Galois\nLes treillis de Galois (Godin et al 1993)   Les treillis de Galois regroupent les données sous forme de concept en fonction de leurs caractéristiques communes et permettent d\u0027exprimer de manière explicite toutes les relations entre les données.\nTreillis de Galois et ontologies\nPour améliorer la recherche d\u0027information, nous utilisons les treillis conjointement à des informations sémantiques « extérieures », qui peuvent prendre la forme de taxonomies, de thesaurus ou d\u0027ontologies, selon leur expressivité .\nPlusieurs travaux se sont intéressés à l\u0027extraction des données et à la représentation des informations sous la forme d\u0027un treillis de Galois.\nL\u0027enrichissement des treillis par des ontologies a fait l\u0027objet de nombreux travaux dans le cadre de la recherche d\u0027informations permettant une analyse plus efficace des différentes relations existantes entre les données : enrichissement d\u0027ontologies pour l\u0027organisation et la recherche d\u0027information (Delteil et al 2002) ; les ontologies de domaine ont été utilisées pour guider la construction du treillis selon les préférences de l\u0027utilisateur (Safar et al 2004) et pour enrichir l\u0027indexation du treillis par un thesaurus (Priss 2000).\nCe processus d\u0027extraction a été employé pour divers types de données : gestion de publications (Szathmary et Napoli 2004); aéronautique (Zenou et Samuelides 2004) ; biologie (Messai et al 2005), pages Web (Carpineto et Romano 2004), etc.\nPar ailleurs, des travaux ont été réalisés pour que l\u0027utilisateur, face à des treillis de grande taille lors d\u0027une navigation, puisse focaliser son attention sur seulement une partie du treillis par décomposition ou par effet de zoom sur des vues abstraites et plus détaillées (Carpineto et Romano 1995)  (Godin et al 1993).\nDiscussion\nL\u0027architecture à trois niveaux que nous avons présentée permet d\u0027apporter une aide pertinente et contextualisée à la recherche d\u0027information par l\u0027utilisateur. Ce dernier se déplace de manière transparente parmi ces niveaux et passe aisément de l\u0027un à l\u0027autre selon ses besoins en termes de recherche d\u0027information. Nous avons également présenté les qualités qui nous ont conduits à utiliser les treillis de Galois dans notre architecture. Néanmoins, la navigation dans un treillis de Galois présente des difficultés. En effet, le nombre de concepts créés augmente avec le nombre d\u0027individus et le nombre de propriétés, ce qui en fait des structures complexes et très difficiles à interpréter au-delà d\u0027une certaine taille. Compte tenu de cette complexité, une aide est nécessaire pour conseiller l\u0027utilisateur dans le choix du concept de départ de sa navigation et du prochain concept à explorer en termes de pertinence par rapport à sa position actuelle. Pour apporter cette aide, nous avons besoin d\u0027une mesure de similarité entre les concepts d\u0027un treillis ; il nous faut pour cela une mesure de similarité adéquate.\nDans la section 3, nous présentons un état de l\u0027art des mesures de similarité définies dans le cadre de l\u0027appariement d\u0027ontologies et dans la section 4 nous proposons l\u0027extension de l\u0027une d\u0027entre elles pour l\u0027adapter aux treillis de Galois RNTI -X -3 Etat de l\u0027art sur les mesures de similarité pour l\u0027appariement d\u0027ontologies\nApproche basée sur les arcs\nParmi les solutions classiques pour les mesures de similarité, on peut trouver les approches basées sur les arcs qui reposent uniquement sur la structure de l\u0027ontologie. Les deux mesures les plus utilisées sont la mesure de Rada (Rada et al 1989) et celle de Wu \u0026 Palmer (Wu et Palmer 1994) décrites ci-dessous. Elles se basent sur la distance en termes de nombre d\u0027arcs séparant un concept d\u0027un autre. (Rada et al 1989) suggèrent que, pour mesurer la distance entre deux concepts ontologiques, notée dist (c1, c2), on se base sur le nombre d\u0027arcs minimum à parcourir pour aller du concept c1 au concept c2. La mesure de similarité est ainsi de la forme :\nDans le même ordre d\u0027idée, (Wu et Palmer 1994) définissent la similarité en fonction de la distance qui sépare deux concepts ontologiques dans la hiérarchie et également par leur position par rapport à la racine. On a ainsi :\ndepth(c) / depth (c1) +depth (c2).\navec : -depth(c) le nombre d\u0027arcs qui séparent le plus petit généralisant de c1 et c2 de la racine. -depth (ci) le nombre d\u0027arcs qui séparent le concept ci de la racine en passant par c. Ces mesures ont l\u0027avantage d\u0027être faciles à implémenter et peuvent donner une idée sur le lien sémantique entre les concepts. Cependant, elles ne prennent pas en compte le contenu du concept lui-même, ce qui peut conduire, dans certains cas, à une marginalisation de l\u0027apport du concept en terme d\u0027information.\nApproche utilisant le contenu informationnel\nLes mesures de similarité suivant cette approche sont fondées sur la notion de Contenu Informationnel (CI) qui utilise conjointement l\u0027ontologie et le corpus. Le Contenu Informationnel d\u0027un concept traduit sa pertinence dans le corpus en tenant compte de sa spécificité ou de sa généralité. Pour ce faire, la fréquence des concepts dans le corpus est calculée et elle regroupe la fréquence d\u0027apparition du concept lui-même ainsi que les concepts qu\u0027il subsume (concepts fils). Les deux mesures les plus connues dans cette catégorie sont celles de Resnik (Resnik 1995) et Jiang-Conrath (Jiang et Conrath 1997.\nResnik (Resnik 1995) définit la similarité sémantique entre deux concepts par la quantité d\u0027information qu\u0027ils partagent : elle est égale au contenu informationnel du concept le plus spécifique (plus petit généralisant ppg) qui subsume les deux concepts dans l\u0027ontologie. Elle est définie comme suit :\nSim (c1, c2) \u003d CI (ppg (c1, c2)) avec : CI\u003d -log (P(c)) où : P(c) est la probabilité de retrouver une instance du concept. Ces probabilités sont calculées par la fréquence de c sur le nombre total des concepts.\nRNTI -X -La mesure de (Jiang et Conrath 1997) prend en compte à la fois le contenu informationnel du ppg et celui des concepts concernés. Par conséquent, elle peut pallier les limites de la mesure de Resnik et est définie de la manière suivante : \nMesure de similarité adaptée aux treillis de Galois\nNous proposons une mesure de similarité entre des concepts d\u0027un treillis de Galois, construit ici pour la classification de pages Web. Cette mesure est une extension d\u0027une mesure de similarité entre les concepts d\u0027une ontologie, que nous avons adaptée à nos besoins, afin de permettre une meilleure étude du treillis. Cette mesure tient compte à la fois de la sémantique et de la topologie, à travers le voisinage d\u0027un concept ainsi que sa profondeur.\nL\u0027objectif de cette mesure de similarité est de permettre une meilleure navigation dans le treillis et une restriction du champ des concepts visités. En particulier, nous cherchons à quantifier l\u0027apport d\u0027information des éléments de l\u0027intention d\u0027un concept par rapport à tout le corpus. Nous nous intéressons tout particulièrement à la fréquence des éléments communs à deux concepts, afin de pouvoir avoir une idée sur le contexte partagé.\nChaque concept du treillis de Galois est constitué de deux sous-ensembles (extensionintension). La fréquence des termes de l\u0027intention dans les pages Web constituant le corpus est calculée pour mesurer l\u0027Information Moyenne (IM) de chaque concept, qui intervient, par la suite, dans le calcul de la mesure de similarité entre les concepts. L\u0027IM est dérivée du Contenu Informationnel (CI) défini dans la section 3.2. L\u0027intérêt de l\u0027IM est d\u0027évaluer le poids des termes dans les pages des sites web.\nNotre mesure de similarité tient également compte de la topologie du treillis de Galois en faisant intervenir la profondeur de chaque concept c (notée depth(c) et correspondant au nombre d\u0027arcs qui séparent c du concept le plus spécifique du treillis) ainsi que l\u0027information moyenne du plus petit généralisant des deux concepts dont on mesure la similarité.\nOn définit l\u0027information moyenne d\u0027un concept c constitué d\u0027une extension E et d\u0027une intention I comme le Contenu Informationnel de l\u0027intention, défini de la manière suivante :\nRNTI -X -\nIM(c)\u003d -log (P(I))\noù P(I) est la probabilité de retrouver les termes de l\u0027intention (i.e. les termes fréquents) simultanément dans le corpus (i.e. les pages Web). Cette probabilité correspond au rapport entre le nombre de pages Web possédant les termes de I et le nombre total de pages Web du corpus.\nNous nous intéressons ici au calcul de la similarité entre deux concepts du treillis de Galois. Si l\u0027on considère deux concepts c1 et c2 du treillis de Galois, on note ppg (c1,c2) le plus petit généralisant de c1 et de c2, qui est le « plus proche ancêtre » commun à c1 et c2.\nLa mesure de similarité entre c1 et c2 est alors définie de la manière suivante :\noù depth(c) est la profondeur du concept c.\nNotre mesure est une adaptation de la mesure de Jiang \u0026 Conrath (cf. section 3.2). Notre apport consiste à prendre en compte, non seulement l\u0027information moyenne de chaque concept mis en jeu, mais aussi sa profondeur afin de préserver la particularité des treillis : plus on descend au niveau de la hiérarchie et plus on se spécialise. Un autre apport de cette mesure est de tenir compte de la structure du treillis puisqu\u0027elle fait intervenir l\u0027information moyenne du plus proche ancêtre commun aux deux concepts dont on mesure la similarité.\nLa mise au point d\u0027une telle mesure de similarité au niveau du treillis de Galois permet de retrouver le voisinage du concept requête favorisant la redirection de l\u0027utilisateur au cours du processus de recherche de l\u0027information pertinente, comme nous l\u0027illustrons dans la section 5.\nMéthodologie\nLa mesure de similarité présentée dans ce papier permet de faciliter la navigation de l\u0027utilisateur à l\u0027intérieur d\u0027un treillis de Galois. Nous distinguons la phase d\u0027initialisation de la navigation et la phase de navigation elle-même.\nInitialisation de la navigation\nCette phase a pour objectif de trouver le point d\u0027entrée le plus adapté dans le treillis, c\u0027est-à-dire le concept qui servira de point de départ à la navigation, que nous appelons le concept de départ et notons CD. Ce concept est constitué d\u0027une extension E CD et d\u0027une intention I CD . La recherche d\u0027information peut être précise, dans le cas où l\u0027utilisateur sait la formuler sous forme de requête, ou non ; nous envisageons ces deux scénarios dans cet article.\nA partir d\u0027une requête de l\u0027utilisateur Dans le cas d\u0027une recherche d\u0027information précise, nous proposons de formuler la requête par R termes de l\u0027ontologie, afin d\u0027utiliser un vocabulaire contrôlé et éviter ainsi les ambiguïtés. Par exemple, si le treillis est construit à partir d\u0027un ensemble de pages Web, les requêtes des utilisateurs pourront être formulées sous la forme d\u0027un ensemble de mots-clés appartenant à ce vocabulaire contrôlé. L\u0027ensemble des mots-clés choisis par l\u0027utilisateur constitue l\u0027intention d\u0027un concept cible noté CT. Au sein du treillis, le concept de départ,CD RNTI -X -choisi pour démarrer la navigation sera le concept dont l\u0027intention possèdera le maximum d\u0027éléments contenus dans l\u0027intention du concept CT. Cette intention est constituée de tous les termes de la requête et notée : I CT \u003d {p 1 , p 2 , …, p R }. Si plusieurs concepts du treillis possèdent le même nombre de propriétés recherchées, il faut choisir le concept initial parmi ces concepts dits candidats. Pour cela, on calcule leur similarité deux à deux et le concept retenu comme concept de départ est celui dont la valeur moyenne de similarité avec les autres concepts candidats est la plus élevée :\nOn note c 1 , c 2 , …, c G les concepts du treillis, G étant le cardinal du treillis. Soient cc 1 , cc 2 , …, cc N , N concepts candidats. On note Sim (cc i , cc j ) la similarité entre le concept cc i et le concept cc j , avec i et j appartenant à [1-N]. La similarité moyenne entre un concept candidat cc k et les autres concepts candidats et dite partielle, notée Sim ck mp est égale à la moyenne des valeurs de similarité entre cc k et les autres concepts candidats. Le concept de départ est celui dont la similarité moyenne avec les autres concepts candidats est la plus élevée, soit :\nEn sélectionnant le concept de départ comme étant, en moyenne, le plus proche des autres candidats, on choisit en quelque sorte le concept le plus « central » parmi ceux qui correspondent le mieux à la requête de l\u0027utilisateur. Si M concepts répondent au critère de similarité moyenne maximale, le concept de départ retenu est celui dont la similarité moyenne avec l\u0027ensemble des concepts du treillis (y compris les concepts non candidats) est la plus élevée. Pour le concept candidat cc k , cette similarité moyenne dite totale est notée Sim ck mt et correspond à la moyenne des valeurs de similarité entre cc k et tous les autres concepts du treillis. Si l\u0027on note cc 1 \u0027, cc 2 \u0027, …, cc M \u0027 les M concepts candidats résiduels, on a : ( cc i \u0027, c k ) ) pour k allant de 1 à G, G étant le cardinal du treillis et k?i.\nSans requête de l\u0027utilisateur\nSi l\u0027utilisateur n\u0027a pas formulé de requête et souhaite simplement naviguer pour explorer un ensemble de pages Web, le concept de départ est celui dont la similarité moyenne totale avec tous les autres objets du treillis est la plus élevée, soit :\nEn sélectionnant le concept de départ comme étant, en moyenne, le plus proche des autres concepts du treillis, on choisit en quelque sorte le concept le plus « central », ce qui semble être un point de départ pertinent et représentatif du treillis.\nNavigation\nLorsque l\u0027utilisateur se trouve dans cette phase, il est déjà positionné sur l\u0027un des concepts du treillis : le concept courant qu\u0027il est en train de visiter, noté CP. Quand RNTI -X -l\u0027utilisateur désire quitter CP pour explorer d\u0027autres concepts, l\u0027objectif est alors de lui proposer le concept le plus proche -sémantiquement et conceptuellement parlant-du concept CP. Le concept le plus similaire au concept CP semble, en effet, être le choix le plus pertinent pour la poursuite de sa navigation. La similarité entre chaque paire de concepts du treillis de Galois peut être pré-calculée, et il suffit alors d\u0027indiquer à l\u0027utilisateur la valeur de la similarité entre son concept courant et tous les autres concepts du treillis, par exemple par ordre décroissant. En phase de navigation, l\u0027unicité du concept proposé n\u0027est pas une nécessité comme dans la phase d\u0027initialisation et, sous réserve de contraintes ergonomiques, plusieurs concepts candidats peuvent être proposés l\u0027utilisateur.\nExpérimentation\nDans cette section, nous illustrons la méthodologie présentée précédemment à partir d\u0027un corpus constitué de pages Web relatives au domaine du tourisme.\nLa première étape consiste à construire les objets et les propriétés qui serviront d\u0027entrée au calcul du treillis de Galois. Dans notre exemple, chaque page Web représente un objet et ses propriétés correspondent aux termes les plus fréquents qu\u0027elle contient. L\u0027extraction des termes les plus fréquents est effectuée à partir d\u0027un thesaurus sur le domaine du tourisme.\nLes objets et propriétés ainsi extraits sont rassemblés dans une base de données servant à la construction d\u0027un treillis de Galois. Nous avons implémenté un algorithme incrémental reposant sur celui de (Godin et al 1991)   {vacance,voyage,location,Hôtel,annuaire}) et ({1},{croisière,vacance,voyage,port,bateau,location,destination,promo}). Plusieurs questions se posent à l\u0027utilisateur : comment choisir entre plusieurs pères sans devoir faire un choix a priori sur les propriétés retenues ? Comment choisir entre plusieurs fils sans devoir faire un choix a priori sur les pages retenues ? Par ailleurs, n\u0027est-il pas possible que le concept le plus pertinent pour poursuivre sa navigation ne soit ni un père, ni un fils du concept courant, c\u0027est-à-dire un concept qui n\u0027est pas relié à celui-ci par un arc du treillis (par exemple un frère tel que le concept ({1, 4), {croisière, voyage, location})) ?\nAfin de répondre à toutes ces questions, on regarde la valeur de la mesure de similarité entre le concept courant et tous les autres concepts du treillis, afin de choisir le concept le plus pertinent pour poursuivre la navigation à partir du concept que l\u0027utilisateur est en train de visiter. Toutes ces valeurs sont présentées à l\u0027utilisateur par ordre décroissant comme le montre la figure 3. En l\u0027occurrence, le concept le plus similaire est le concept ({2), {vacance, voyage, location, hôtel, annuaire}). Il est à noter que toutes les valeurs de similarité entre chaque paire de concepts peuvent être pré-calculées afin d\u0027optimiser le temps de réponse.\nConclusion et perspectives\nL\u0027extraction d\u0027informations pour en faciliter la recherche peut être réalisée de différentes manières : par des techniques de clustering numériques, basées sur la fréquence d\u0027apparition des termes dans un document, ou par des techniques de clustering conceptuel, permettant d\u0027effectuer des regroupements d\u0027objets partageant les mêmes propriétés. L\u0027avantage de ces dernières est de permettre une structuration des données, et d\u0027offrir des mécanismes de généralisation/spécialisation bien adaptés à l\u0027utilisateur final. Un niveau sémantique, ontologie ou thesaurus, a été proposé au dessus du niveau conceptuel dont les avantages ont été détaillés en section 2.\nLa problématique abordée dans cet article est de trouver un moyen d\u0027indiquer à l\u0027utilisateur les concepts les plus pertinents par rapport à sa requête, ou de pouvoir lui proposer un point d\u0027entrée dans la structure conceptuelle (treillis de Galois) qui peut être de grande taille. Nous avons donc proposé une mesure de similarité tenant compte à la fois de la sémantique et de la topologie du treillis (position du concept dans le treillis, prise en compte du voisinage). Cette mesure permet donc d\u0027ordonner l\u0027ensemble des concepts se trouvant dans le voisinage d\u0027un concept donné dans le treillis, et ainsi de guider la navigation de l\u0027utilisateur.\nCette mesure a été expérimentée sur des treillis construits à partir de pages Web, les propriétés communes étant des termes extraits de ces pages. Les résultats ont été comparés avec d\u0027autres mesures comme celle de Wu\u0026Palmer, décrite en section 3. Les résultats obtenus montrent un meilleur ordonnancement du voisinage avec notre mesure de similarité.\nRNTI -X -Les perspectives de ce travail consistent dans un premier temps à développer une interface permettant à des utilisateurs de valider cette approche de manière expérimentale. Nous travaillons également sur la visualisation spatiale des corpus de documents à la base de la construction des treillis, ainsi que sur la manière de mettre en correspondance ces régions spatiales avec les concepts du treillis, pour une meilleure interprétation du voisinage des concepts. Dans la perspective de traiter de gros volumes de données, on pourra aussi s\u0027intéresser à la manipulation de gros volumes de données avec les treillis de Galois grâce à l\u0027utilisation d\u0027algorithmes de construction de treillis récents tel que proposé par (Diday et Emilion, 2003).\n"
  },
  {
    "id": "892",
    "text": "Introduction Générale\nL\u0027émergence des bases de données modernes qui présentent d\u0027énormes capacités de stockage et de gestion, associée à l\u0027évolution des systèmes de transmission et des techniques d\u0027acquisition automatique des données contribuent à la construction d\u0027une masse de données qui dépasse de loin les capacités humaines à les traiter. Ces données sont des sources d\u0027informations pertinentes qui nécessitent des outils de synthèse et d\u0027interprétation. Les recherches se sont orientées vers des systèmes d\u0027intelligence artificielle puissants permettant l\u0027extraction des informations utiles et aidant à la prise des décisions. Pour une meilleure synthèse et interprétation, la fouille de données ou data mining est née en puisant ses outils au sein de la statistique, de l\u0027intelligence artificielle et des bases de données. La méthodologie du data mining offre la possibilité de construire un modèle de prédiction d\u0027un phénomène à partir d\u0027autres phénomènes plus facilement accessibles, qui lui sont liés, en se basant sur le processus d\u0027extraction des connaissances à partir des données qui n\u0027est qu\u0027un processus de classification intelligente des données. Cependant, le modèle construit peut parfois engendrer des erreurs de classification que même une classification aléatoire n\u0027aurait pas engendrée. Pour réduire ces erreurs, de nombreux travaux en data mining et spécifiquement en apprentissage automatique ont porté sur les méthodes d\u0027agrégation de classifieurs. Leur but suprême est d\u0027améliorer, par des techniques de vote, les performances d\u0027un classifieur unique. Ces méthodes d\u0027agréga-tion sont efficaces d\u0027un point de vue compromis Biais-variance, mais aussi grâce aux trois raisons fondamentales (raison statistique, raison informatique et raison de représentation) expliquées dans l\u0027étude menée par (Dietterich, 2000). Ces méthodes d\u0027agrégation de classifieurs peuvent être regroupées en deux catégories : celles qui fusionnent des classifieurs prédéfinis, on trouve par exemple : le vote simple (Bauer et Kohavi, 1999), le vote pondéré (Bauer et Kohavi, 1999) et le vote à la majorité pondérée (Littlestone et Warmuth, 1994) et celles qui fusionnent des classifieurs selon les données durant l\u0027apprentissage, on trouve des stratégies adaptatives (boosting) et son algorithme de base AdaBoost (Shapire, 1990) ou des stratégies aléatoires (bagging) (Breiman, 1996). Nous nous intéresserons, par la suite, au boosting, dans la mesure où l\u0027étude comparative menée dans (Dietterich, 1999) montre bien que dans le cas où les données sont faiblement bruitées, AdaBoost est plus performant que le bagging, tout en semblant être immunisé contre le sur-apprentissage. En effet, AdaBoost essaye directement d\u0027optimiser les votes pondérés. Cette constatation s\u0027est traduite non seulement par le fait que l\u0027erreur empirique sur l\u0027échantillon d\u0027apprentissage décroît exponentiellement avec le nombre d\u0027itérations, mais également par le fait que l\u0027erreur en généralisation baisse elle aussi. Cependant, cette même étude montre que, sur des données fortement bruitées, AdaBoost présente un taux d\u0027erreur parfois plus important que le bagging. Une raison à ces mauvaises performances, mise en évidence par Dietterich, vient du fait que le boosting tend à augmenter le poids des exemples bruités à travers les itérations. La conséquence immédiate est le sur-apprentissage des exemples bruités. La vitesse de convergence du boosting se trouve également pénalisée sur ce type de données. Pour surmonter les difficultés rencontrées par le boosting face aux données bruitées, nous proposons une nouvelle approche qui associe les hypothèses déjà construites à l\u0027hypothèse courante pour définir les erreurs de l\u0027itération courante. De par sa nature, cette approche est appelée approche Hybride. La suite de cet article est organisée comme suit : la section 1 est consacrée à un état de l\u0027art synthétique des principaux travaux visant à amélio-rer le boosting. En section 2, nous présentons l\u0027amélioration du boosting que nous proposons. Dans la section 3, nous effectuons une large étude expérimentale visant à comparer, sur de nombreuses bases de données réelles, les performances d\u0027AdaBoost et AdaBoost Hybride, en termes d\u0027erreur, de rappel et de vitesse de convergence. Enfin, en section 4, nous terminons par une conclusion et des perspectives.\nEtat de l\u0027art\nEn présence de données bruitées, le boosting présente différentes faiblesses, telles que le sur-apprentissage et la dégradation de la vitesse d\u0027apprentissage. Diverses améliorations ont été proposées qui opèrent sur la mise à jour du poids des exemples ou parfois sur le principe même du boosting. De ce fait, nous allons présenter les principales méthodes ayant comme objectif l\u0027amélioration du boosting par rapport à ces deux faiblesses. Cet état de l\u0027art est effectué selon deux axes de recherches. Le premier axe regroupe les approches abordant le problème de la gestion des données bruitées, sans laquelle des phénomènes de sur-apprentissage peuvent survenir. Le deuxième axe regroupe les approches portant sur les problèmes de vitesse de convergence.\nLe sur-apprentissage\nL\u0027émergence et l\u0027évolution des bases de données modernes contraignent aujourd\u0027hui les chercheurs à étudier et à améliorer les capacités de tolérance au bruit du boosting. En effet, ces bases de données sont fortement bruitées en raison des nouvelles technologies d\u0027acquisition de données telles que le web. En parallèle, des études telles que celles de (Dietterich, 1999), (Rätsch, 1998) et (Schapire et Singer, 1999) montrent bien qu\u0027 AdaBoost tend à surapprendre les données lorsqu\u0027elles sont bruitées. De ce fait, un certain nombre de travaux récents ont tenté de limiter ces risques de sur-apprentissage. Les améliorations proposées se fondent essentiellement sur le fait qu\u0027AdaBoost tend à augmenter le poids des exemples bruités de manière exponentielle. Deux solutions se présentent pour réduire ces données bruitées. Soit ces données sont détectées et supprimées avant l\u0027apprentissage à l\u0027aide d\u0027heuristiques efficaces (Brodley et Friedl, 1996), (Wilson et Martinez, 2000). Soit ces données sont détectées tout au long du processus de boosting, on parle alors d\u0027une bonne gestion de bruit. Pour ce faire, les chercheurs se sont orientés vers l\u0027amélioration des points forts du boosting tels que la mise à jour des exemples mal classés, la maximisation de la marge, la signification des poids qu\u0027Adaboost associe aux hypothèses et enfin le choix de l\u0027apprenant faible.\n-Modification des poids des exemples : la mise à jour adaptative de la distribution des exemples, visant à augmenter le poids de ceux mal appris par le classifieur précédent, permet d\u0027améliorer les performances de n\u0027importe quel algorithme d\u0027apprentissage. En effet, à chaque itération, la distribution courante favorise les exemples ayant été mal classés par l\u0027hypothèse précédente. De ce fait, plusieurs chercheurs ont proposé des stratégies portant sur une modification de la mise à jour des poids des exemples, pour évi-ter le sur-apprentissage. Madaboost (Domingo et Watanabe, 2000) a pour principe de borner le poids des exemples suspects par leur probabilité initiale. Il agit ainsi sur la croissance incontrôlée du poids des exemples bruités qui est à l\u0027origine des problèmes d\u0027AdaBoost. Une autre approche qui rend l\u0027algorithme du boosting résistant au bruit est BrownBoost (McDonald et al., 2003), un algorithme basé sur le Boost-by-Majority en incorporant un paramètre temporel. Ainsi par une bonne évaluation de ce paramètre, BrownBoost est-il capable d\u0027éviter le sur-apprentissage. On citera encore Logitboost (Schapire et Singer, 1999), qui adapte au principe d\u0027AdaBoost un modèle de régression logistique. LogitBoost réduit au minimum son critère en employant les étapes de Newton-like pour adapter un modèle de régression logistique en optimisant le logarithme de la vraisemblance. Une autre approche, qui produit moins d\u0027erreur en généralisation comparée à l\u0027approche classique mais au prix d\u0027une erreur d\u0027apprentissage légèrement plus élevée, est celle de (Vladimir et Vezhnevets, 2002). En fait, sa mise à jour se base sur la diminution de la contribution des classifieurs, si cela fonctionne \"trop bien\" sur les données qui ont déjà été correctement classifiées. C\u0027est pourquoi la méthode est appelée Modest AdaBoost. Elle force les classifieurs à être \"modestes\" et travaille seulement dans le domaine défini par une distribution bien dé-terminée et un critère d\u0027arrêt normal. SmoothBoost (Servedio, 2001) essaye de réduire l\u0027effet de sur-apprentissage par des limites imposées à la distribution produite pendant le processus de boosting. En particulier, lors de chaque itération, on fixe une limite de poids. Un exemple dont le poids dépasse cette limite est considéré comme bruité et retiré de l\u0027ensemble d\u0027apprentissage. Une dernière approche, IAdaBoost (Sebban et Suchier, 2003), se base sur l\u0027idée de construire autour de chacun des exemples une mesure d\u0027information locale permettant d\u0027évaluer les risques de surapprentissage, en utilisant un graphe de voisinage qui permet de mesurer l\u0027information autour de chaque exemple. Grâce à ces mesures, on calcule une fonction qui évalue la nécessité de mettre à jour l\u0027exemple avec AdaBoost. Cette fonction permet de gérer à la fois les outliers, les recoupements de classes et les centres de clusters.\n-Modification de la marge : certaines études, après une observation des algorithmes de boosting, ont montré que l\u0027erreur en généralisation décroît encore une fois que l\u0027erreur en apprentissage est stable ou même nulle. L\u0027explication est que même si tous les exemples d\u0027apprentissage sont déjà bien classés, le boosting tend à maximiser davantage les marges (Servedio, 2001), d\u0027où les performances du boosting. À la suite de cette explication, certains ont cherché à modifier la marge explicitement soit en la maximisant soit en la minimisant dans le but d\u0027améliorer les performances de boosting contre le sur-apprentissage. Plusieurs approches se sont succédées telles que AdaBoostReg (Rätsch et al., 2001) qui essaye d\u0027identifier et d\u0027enlever les exemples mal étiquetés de l\u0027ensemble d\u0027apprentissage, ou d\u0027appliquer la contrainte de la marge maximale sur des exemples supposés mal étiquetés, en utilisant la Soft Margin qui est moins sensible au sur-apprentissage par rapport à la marge d\u0027 Adaboost. Dans l\u0027algorithme proposé par (Friedman et al., 1998), les auteurs utilisent un schéma de pondération qui exploite une fonction des marges qui croît moins vite que la fonction exponentielle.\n-Modification de poids des classifieurs : lors de l\u0027évaluation des performances du boosting, des chercheurs se sont également interrogés sur la signification des poids ?(t) qu\u0027AdaBoost associe aux hypothèses produites. Ce poids est une valeur déterminée en fonction des échecs et des réussites de classification sur un échantillon bien déterminé. Cependant, ils ont noté lors d\u0027expériences sur des données très simples que l\u0027erreur en généralisation diminuait encore alors que l\u0027apprenant faible avait déjà fourni toutes les hypothèses possibles. Autrement dit, lorsqu\u0027une hypothèse apparaît plusieurs fois, elle vote finalement avec un poids, cumul de tous ses ?(t), qui a peut-être un caractère plus absolu. De ce fait, plusieurs auteurs ont espéré approcher ces valeurs par un processus non adaptatif, tel que Locboost (Meir et al., 2000), une alternative à la construction de l\u0027ensemble de représentation des hypothèses, qui permet aux coefficients ?(t) de dé-pendre des données. On aura donc des poids locaux attribués à chaque exemple.\n-Choix de l\u0027apprenant faible : Plusieurs auteurs se sont intéressés au choix du classifieur de base du boosting. GloBoost (Torre, 2004) utilise un apprenant faible qui produit des hypothèses correctes. Celles-ci peuvent donc s\u0027abstenir sur une partie des exemples mais en aucun cas se tromper sur un exemple. Il s\u0027agit de moindres généralisés maximalement corrects. RankBoost (Dietterich, 1999) se base sur un apprenant faible qui accepte comme données d\u0027entrées des attributs de préférences (rank) qui ne sont que des fonctions.\nCertes, ces méthodes ont pu améliorer d\u0027une façon ou d\u0027une autre la performance du boosting contre le bruit. Toutefois, d\u0027autres paramètres du boosting se trouvent pénalisés, tels que l\u0027erreur d\u0027apprentissage, le temps de détection du bruit et la vitesse de convergence.\nLa vitesse de convergence\nEn plus du problème de sur-apprentissage rencontré par le boosting dans les bases de données modernes déjà évoqué précédemment, il existe un autre problème qui est celui de la vitesse de convergence des algorithmes de boosting (spécialement Adaboost). En effet, en cas de présence de données fortement bruitées, l\u0027erreur optimale de l\u0027algorithme d\u0027apprentissage utilisé est atteinte tardivement. En d\u0027autres termes, Adaboost \"perd\" du temps, et donc des itérations à pondérer ces exemples qui ne méritent aucune attention, puisqu\u0027il s\u0027agit de bruit. Des recherches ont été menées pour détecter les données bruitées et améliorer ainsi les performances du boosting en termes de convergence tel que iBoost (Kwek et Nguyen, 2002), qui vise à spécialiser les hypothèses faibles sur les exemples qu\u0027elles sont supposées correctement classer. iAdaboost est aussi une approche qui contribue à améliorer Adaboost contre sa convergence. En fait, l\u0027idée de base de l\u0027amélioration est la modification du théorème de (Schapire et Singer, 1999). Cette modification est réalisée afin d\u0027intégrer le risque de Bayes et de mettre en exergue les situations où certains exemples de classes différentes partagent la même représen-tation. Les effets de cette modification sont une convergence plus rapide vers le risque optimal et une réduction du nombre d\u0027hypothèses faibles à construire. Enfin, RegionBoost (Maclin, 1998) est une nouvelle stratégie de pondération des classifieurs. Cette pondération est évaluée au moment du vote par une technique basée sur les k plus proches voisins de l\u0027exemple à étiqueter. Cette approche permet de spécialiser chaque classifieur sur des régions de l\u0027ensemble d\u0027apprentissage.\nAmélioration proposée : Adaboost Hybride\nPour améliorer les performances d\u0027Adaboost et éviter de le forcer à apprendre des exemples a priori bruités ou des exemples qui deviendraient trop difficiles à apprendre durant le processus du boosting, nous proposons une nouvelle approche qui s\u0027inspire du fait qu\u0027Adaboost construit, à chaque itération, des hypothèses sur un échantillon bien défini. La mise à jour et le calcul de l\u0027erreur d\u0027apprentissage sont faits à partir des résultats de ces seules hypothèses et n\u0027exploitent pas les résultats fournis par les hypothèses construites aux itérations antérieures sur d\u0027autres échantillons. Cette approche est appelée approche Hybride au sens où elle se base sur les hypothèses antérieures : la mise à jour des exemples à l\u0027itération courante tiendra compte non seulement des résultats de l\u0027itération courante mais aussi de ceux des itérations antérieures.\nPseudo code de Adaboost Hybride\nSoit X 0 la classe à prévoir et S \u003d (x 1 , y 1 ), ....., (x n , y n ) un échantillon -Pour i \u003d 1, 2...n, faire -Initialiser les poids p 0 (x i ) \u003d 1/n ; -Fin pour -t ? 0 -Tant que t ? T faire -Tirer un échantillon d\u0027apprentissage S t dans S selon les probabilités p t . -Construire une hypothèse h t sur S t par un algorithme d\u0027apprentissage A. -Soit t l\u0027erreur apparente de h t sur S avec t \u003d poids des exemples\nH(x) \u003d argmax y ? Y T t\u003d1 ? t La modification de l\u0027algorithme porte sur la prise en compte de l\u0027ensemble des itérations passées pour effectuer la prédiction courante, ce qui modifie notamment le poids des exemples et le calcul de l\u0027erreur.\n-Modification des poids des exemples : à chaque itération, on fait appel aux avis des experts déjà utilisés (hypothèses des itérations antérieures) pour mettre à jour les poids des exemples. En effet, on ne compare pas seulement la classe prédite par l\u0027hypothèse à l\u0027itération courante avec la classe réelle mais la somme des hypothèses pondérées depuis la première itération jusqu\u0027à l\u0027itération courante. Si cette somme vote pour une classe différente de la classe réelle alors une mise à jour exponentielle semblable à celle effectuée par Adaboost est appliquée à l\u0027exemple mal classé. Ainsi, cette modification ne concerne-t-elle que les exemples qui sont soit mal classés soit pas encore classés. Il est donc logique de s\u0027attendre à une amélioration de la vitesse de convergence, de même pour la réduction de l\u0027erreur en généralisation étant donnée le lissage des hypothèses à chaque itération. -Modification du calcul de l\u0027erreur de l\u0027hypothèse à l\u0027itération t : la méthode que nous proposons, prend en considération les hypothèses antérieures à l\u0027itération courante pour former l\u0027hypothèse courante. De ce fait, à chaque itération, l\u0027erreur apparente est le poids des exemples prédits de façon erronée par la moyenne pondérée des hypothèses des itérations antérieures. Du coup, le coefficient attribué à l\u0027hypothèse courante, ?(t), est lui aussi modifié, puisque ce coefficient dépend du calcul de l\u0027erreur apparente Cette modification a un effet de lissage et laisse l\u0027algorithme à chaque itération très dépendant des autres itérations. Des résultats améliorant surtout l\u0027erreur en généralisa-tion sont attendus puisque le vote de chaque hypothèse (coefficient ?(t)) est calculé à partir des autres hypothèses.\nExpérimentations\nDans cette section, nous allons comparer les résultats de notre algorithme AdaBoostHyb avec ceux fournis par AdaBoost, l\u0027algorithme de référence et par BrownBoost, un algorithme connu pour être résistant face aux données bruitées. En fait, BrownBoost utilise la fonction de pondération suivante, qui n\u0027est rien d\u0027autre que la loi de probabilité d\u0027une variable binomiale, qui dépend du nombre d\u0027itérations finales k (temps total d\u0027exécution), de l\u0027itération courante i, du nombre de fois où l\u0027exemple a déjà été correctement étiqueté r, et enfin de la probabilité de succès 1 ? ? imposée à toute hypothèse faible. ? i r \u003d (\nL\u0027avantage de cette approche est que les données bruitées seront probablement détectées à un certain moment, et leur poids cessera d\u0027augmenter. Au cours des expérimentations, la comparaison se fait à travers l\u0027erreur en généralisation, le rappel et la vitesse de convergence. L\u0027apprenant faible utilisé est l\u0027algorithme C4.5 choisi suite à l\u0027étude de (Dietterich, 1999) qui a montré que C4.5 est très sensible au bruit. Pour estimer sans biais le taux de succès théorique, nous avons fait appel à une procédure de validation croisée en 10 parties. Afin d\u0027évaluer le comportement d\u0027AdaBoostHyb vis à vis tant des performances que de la vitesse de convergence, nous avons séparé nos expérimentations en plusieurs parties. Dans la première, où nous avons travaillé sur 15 bases de l\u0027UCI (D.J. Newman et Merz, 1998), nous rapportons la valeur de l\u0027erreur en généralisation et le rappel, choisis comme critère de performance. Dans la deuxième partie, nous avons bruité aléatoirement ces bases de données avec un taux de bruit de 20%, pour analyser le comportement des trois algorithmes retenus. Ce taux est choisi en référence à l\u0027étude de (Dietterich, 1999)   \nComparaison en termes d\u0027erreur en généralisation\nLe tableau 2 présente les résultats obtenus pour cette partie en ayant choisi pour chacun des algorithmes d\u0027effectuer 20 itérations. Le choix du nombre d\u0027itérations sera expliqué dans la dernière partie des expériences. Nous avons indiqué le taux d\u0027erreur en généralisation es-timé pour chacun des algorithmes AdaBoostM1, BrownBoost et AdaBoostHyb. Nous avons par ailleurs utilisé les mêmes échantillons pour la validation croisée des différents algorithmes afin d\u0027avoir une comparaison plus fine.\nL\u0027observation des résultats montre déjà les effets positifs de l\u0027approche hybride. En effet, pour 14 bases sur 15, l\u0027algorithme AdaBoostHyb présente un taux d\u0027erreur inférieur ou égal à celui d\u0027AdaBoostM1. C\u0027est seulement pour la base LYMPH que notre approche donne une erreur de généralisation plus élevée que l\u0027approche classique. Nous remarquons, aussi, des améliorations significatives de l\u0027erreur en généralisation correspondant aux bases de données NHL, CONTACT-LENS et BREAST-CANCER. Par exemple l\u0027erreur en généralisation de la base BREAST-CANCER passe de 45.81% à 30.41%.\nDe même, si nous comparons l\u0027approche proposée avec BrownBoost, nous remarquons que pour 11 bases de données sur 15 l\u0027algorithme AdaBoostHyb présente un taux d\u0027erreur inférieur ou égal à BrownBoost. Ce gain en faveur de AdaBoostHyb nous montre bien qu\u0027en exploitant les hypothèses générées aux itérations antérieures pour corriger le poids des exemples, il est possible d\u0027améliorer les performances du boosting. Ceci peut être expliqué par le mode de calcul de l\u0027erreur apparente et par conséquent le calcul du coefficient du classifieur ?(t) ainsi que par l\u0027hybridation de l\u0027hypothèse courante et des hypothèses antérieures. \nComparaison en terme de rappel\nLes résultats encourageants auxquels nous sommes parvenus, nous mènent à approfondir l\u0027étude de cette nouvelle approche. Dans cette partie, nous essayons de connaître l\u0027impact de notre approche sur le taux de rappel, puisque celle-ci n\u0027améliore effectivement le boosting que si elle agit positivement sur le rappel. Le tableau 3 présente les résultats obtenus pour cette partie, ayant choisi pour chacun des algorithmes d\u0027effectuer 20 itérations comme précédemment. Nous avons indiqué le rappel pour chacun des algorithmes AdaBoostM1, BrownBoost et AdaBoostHyb. Les résultats obtenus ici confirment les précédents. En effet, AdaBoostHyb augmente le rappel des bases de données ayant des taux d\u0027erreur moins important. Le rappel des deux algorithmes est le même dans le cas où les taux d\u0027erreur des bases de données sont égaux. Si nous considérons BrownBoost, nous remarquons que ce dernier améliore le rappel d\u0027AdaBoostM1, pour chaque base de données (sauf la base de données TITANIC). Cependant, le rappel obtenu par notre approche est meilleur que celui obtenu par BrownBoost, sauf pour la base de données ZOO.\nNous constatons aussi que notre approche améliore le rappel dans le cas de la base LYMPH où l\u0027erreur était plus importante. Nous notons alors que la nouvelle approche n\u0027agit pas néga-tivement sur le rappel mais elle l\u0027améliore même lorsque l\u0027on a une erreur de généralisation plus importante. \nComparaison sur des données bruitées\nDans cette partie, on s\u0027est basé sur l\u0027étude déjà faite par Dietterich (Dietterich, 1999) en ajoutant du bruit aléatoire aux données. Cet ajout de bruit de 20% est effectué, pour chacune de ces bases, en changeant aléatoirement la valeur de la classe prédite à l\u0027aide d\u0027un programme par une autre valeur possible de cette classe. Le tableau 4 nous montre le comportement des algorithmes vis-à-vis du bruit. Nous remarquons bien que l\u0027approche hybride est sensible elle aussi au bruit puisque le taux d\u0027erreur en généralisation est augmenté pour toute les bases des données. Cependant cette augmentation reste toujours inférieure à celle de l\u0027approche classique sauf pour les bases de données telles que CREDIT-A, HEPATITIS et HYPOTHYROID. Nous avons donc étudié de près ces bases de données et nous avons noté un point commun, les valeurs manquantes. En fait, CREDIT-A, HEPATITIS et HYPOTHYROID possèdent respectivement 5%, 6%et 5,4% de valeurs manquantes.\nNous constatons alors que notre amélioration perd son effet avec l\u0027accumulation de deux types de bruit : les valeurs manquantes et le bruit artificiel, bien que l\u0027algorithme AdaBoostHyb améliore les performances d\u0027AdaBoost contre le bruit sur le reste des bases de données. Considérant BrownBoost, nous remarquons qu\u0027il améliore l\u0027erreur en généralisation de toute les bases de données en comparaison avec AdaBoostM1. Cependant, BrownBoost donne des résultats meilleurs que l\u0027approche hybride sur seulement 6 bases de données. Notre approche donne les meilleurs résultats avec les 9 autres bases de données. Ces résultats nous encouragent à étudier en détails le comportement de notre approche vis-à-vis du bruit. \n"
  },
  {
    "id": "894",
    "text": "Problématique\nL\u0027Extraction d\u0027Information consiste à identifier de l\u0027information bien précise d\u0027un texte en langue naturelle et à la représenter sous forme structurée. Les composantes de l\u0027information recherchée sont généralement prédéfinies et circonscrites à un domaine spécifique, et les principaux travaux réalisés en matière d\u0027identification de relations sémantiques ont essentiellement concerné les relations portées par une structure de type prédicats-arguments. Les principales approches d\u0027identification de ces relations ont été basées sur l\u0027analyse syntaxique (identification du verbe et ses arguments) (Khélif, 2006), ou sur la définition de patrons lexico-syntaxiques (Aussenac et al., 2000). L\u0027étude de corpus de domaines différents montre que bonne partie de l\u0027information pertinente peut aussi être distribuée sur plusieurs phrases, par le biais de relations exprimées à l\u0027aide de variations linguistiques, comme la coréférence, l\u0027anaphore ou l\u0027ellipse. Les métho-des classiques d\u0027extraction de relations ne sont alors plus adaptées.\nLa résolution de relations non prédicatives, et plus particulièrement d\u0027une certaine forme elliptique (formes passives où l\u0027argument agent est effacé) utilisée fréquemment, nous a conduits à proposer une représentation des connaissances du domaine considéré, à l\u0027aide du modèle des graphes conceptuels, car ce modèle est doté d\u0027opérations et offre des procédures de raisonnement (Salvat, 1997).\nIdentification de relations non prédicatives\nNous avons étudié un corpus de résumés d\u0027articles scientifiques décrivant des expérien-ces génétiques menées par des chercheurs sur un ensemble de patients porteurs d\u0027une même maladie génétique, le but étant de localiser les régions chromosomiques affectées. Une des relations pertinentes identifiées est Conditions Expérimentales qui relie l\u0027ensemble des patients observés au type d\u0027analyse subie. Dans l\u0027exemple \"A study was conducted on 22 MM patients. The authors used G-banding\", la relation Conditions Expérimentales ne peut être détectée par une approche classique. La mise en oeuvre d\u0027une procédure de raisonnement qui établit un lien entre les auteurs (authors) et l\u0027étude (study) menée par les auteurs (authors) Extraction de relations non prédicatives permet d\u0027inférer le lien existant entre l\u0027échantillon (22 MM patients) et l\u0027analyse (Gbanding).\nChacune des phrases est traduite à l\u0027aide d\u0027un graphe dit de référence (GRef) qui modé-lise le domaine de l\u0027étude et auquel des règles d\u0027inférence ont été associées :\nPour cet exemple, l\u0027opération de jointure entre les deux graphes fournit un graphe résul-tat, auquel sont appliquées les règles de déduction.\nConclusion\nCe travail, qui dans un premier temps est dédié à une forme d\u0027ellipse particulière, doit être étendu aux relations exprimant des liens de cohésion et de cohérence entre différents éléments du texte, pour une meilleure compréhension du texte. Aussenac N. et P. Séguéla (2000). \nRéférences\nSummary\nSemantic relations generally recognized by information extraction tasks are of a predicative form. The useful information is often distributed over several sentences. To detect this kind of complex relations, we propose a knowledge representation based on the conceptual graph model.\n"
  },
  {
    "id": "895",
    "text": "Introduction\nL\u0027extraction d\u0027informations à partir de messages électroniques (mails) n\u0027a pas été très étu-diée dans la communauté du TAL 1 . Ceci est dû principalement à la présentation informelle des mails et à leurs faibles apports d\u0027informations. Cependant, les mails peuvent être parfois la principale source de connaissances pour une organisation ou une communauté de pratique (CoP). C\u0027est le cas d\u0027@pretic 2 qui est une association ouverte à tous les enseignants exploitant les TIC 3 en Belgique durant leurs interactions avec les apprenants pour préparer leurs leçons. La communication dans cette CoP se fait essentiellement par échanges de mails sur une liste de diffusion décrivant des problèmes rencontrés. Dans le but de faciliter la navigation dans cette liste de diffusion et la recherche de solutions pour des problèmes déjà posés, nous proposons une approche de création d\u0027annotations séman-tiques pour cette liste, ces annotations reposant sur une ontologie qui est elle-même extraite en partie à partir du corpus de mails. La base d\u0027annotations créée servira pour la navigation guidée par l\u0027ontologie en s\u0027appuyant sur le moteur de recherche sémantique CORESE (Corby et al., 2004). Dans ce qui suit, nous présentons l\u0027ontologie @pretic puis nous présentons un scénario d\u0027utilisation de cette ontologie avant de conclure.\nConstruction de l\u0027ontologie @pretic\nAfin de construire l\u0027ontologie d\u0027@pretic, nous optons pour une approche modulaire composée de quatre ontologies, chacune dédiée à une tâche particulière : (i) une ontologie pour les 1 Traitement Automatique des Langues 2 Association des professeurs exploitant les TIC en Belgique francophone : http://www.apretic.be/ 3 Technologies de l\u0027information et de la communication composants informatiques, (ii) une ontologie pour la description des mails, (iii) une ontologie qui décrit les membres de la CoP, et (iv)  \nNettoyage du corpus\nLa première phase de nettoyage consiste à extraire les mails sous un format XML, supprimer les « spams », restaurer les liens entre les messages d\u0027origines avec leurs réponses et filtrer les signatures en concevant un algorithme de détection de signature par comparaison de messages. Bien que notre corpus provienne d\u0027une communauté francophone il était trilingue (quelques messages en anglais et en flamand). Dans une seconde phase, nous avons utilisé l\u0027outil TEXTCAT (Canvar et Trenkle., 1994) permettant de détecter la langue. Une des origines majeures de la dégradation des textes des mails est la non accentuation. Cette erreur orthographique a un impact négatif sur l\u0027extraction des candidats termes. Pour résoudre ce problème, nous avons utilisé l\u0027outil REACC 5 .\nÀ l\u0027opposé de corpus formels où le nettoyage fournit un corps de texte non bruité, les corps des mails contiennent beaucoup de bruit même après le nettoyage. Ce bruit n\u0027a pas de forme générique : formes de salutations, remerciements, signatures non filtrées, etc. L\u0027écriture de filtres pour éliminer ce bruit nécessite le parcours du corpus, la localisation des zones de dégradation et l\u0027ajout d\u0027un filtre pour chaque zone détectée. Or cette méthode manuelle est lente et les filtres écrits peuvent être valables seulement pour les zones examinées. Nous avons adopté une méthode de nettoyage semi-automatique dont le but est d\u0027accélérer la détection du bruit. Dans cette approche, la chaîne de TAL est fermée par un retour sur le texte original.\nExtraction des candidats termes\nL\u0027extraction des candidats termes a pour objectif d\u0027extraire un maximum de termes significatifs afin de construire une ontologie assez riche et couvrir la majorité des problèmes informatiques. Pour cela, nous utilisons deux approches de TAL, à savoir : syntaxique par le biais de l\u0027outil FASTR (Jacquemin, 1997) et syntaxico-statistique implémentée par l\u0027outil ACABIT (Daille, 1994).\nAmorçage et enrichissement de l\u0027ontologie\nPour l\u0027amorçage de l\u0027ontologie des problèmes, nous considérons les candidats termes provenant de messages initiaux, c\u0027est-à-dire les messages qui ouvrent une discussion et qui sont susceptibles de soulever un problème. Ces messages partagent une régularité syntaxique par rapport aux termes utilisés pour exprimer un problème. Cette régularité consiste à utiliser le mot « problème » suivi du composant informatique concerné par ce problème. Cette étude nous a menés à amorcer la construction de l\u0027ontologie par la sélection des candidats termes ayant comme tête le mot « problème ».\nProblème de réception -Problème de port -Problème de réseau Problème de câblage -Problème de connexion -Problème de rapidité Problème avec hotmail -Problème sur le disque -etc.\nLa formalisation d\u0027une partie de ces termes nous a permis d\u0027avoir un premier schéma de l\u0027ontologie qui a été validé par les formateurs de la CoP @pretic. Cependant, cet embryon d\u0027ontologie, quoique intéressant en couvrant en grande partie des problèmes rencontrés, est assez générique et risque d\u0027induire une ambiguïté lors de la génération des annotations. Afin d\u0027enrichir notre ontologie et la rendre de plus en plus spécifique, nous avons effectué une analyse manuelle de tous les candidats termes générés par les outils de TAL. La liste suivante montre des exemples de termes extraits par les deux outils et utilisés pour l\u0027enrichissement de l\u0027ontologie.\n\"lenteur de connexion\", \"manque de mémoire\", \"perte de donnée\", \"retard dans la réponse\",\"ordinateur contaminé\", \"mémoire insuffisante\", \"manque d\u0027efficacité\", etc.\nL\u0027étude de cette liste de termes nous permet, dans une première étape, de : -Détecter de nouveau termes significatifs permettant d\u0027enrichir directement l\u0027ontologie (\"manque de mémoire\", \"lenteur de connexion\", etc.). -Détecter des relations de synonymie entre certains termes significatifs (\"mémoire insuffisante\" \u003d \"insuffisance de mémoire\" ou \"message infecté\" \u003d \"infection de message\"). Ces termes synonymes se traduiront par un même concept de l\u0027ontologie. -Faire émerger des régularités structurelles (i.e. patrons syntaxiques) dans une grande partie des termes (\"lenteur de X\", \"perte de X\", \"difficulté de X\", \"retard de X\", \"manque de X\", etc.), X étant un terme de l\u0027ontologie des Composants.\nDans une deuxième étape, nous nous sommes inspirés du travail réalisé dans SAMO-VAR 6 (Golebiowska et al., 2001), afin de proposer des règles heuristiques qui permettent d\u0027alimenter de façon semi-automatique l\u0027ontologie. Ces règles détectent des structures prédéfinies dans le texte et enrichissent l\u0027ontologie par des candidats termes qui n\u0027ont pas été nécessai-rement détectés par les outils de TAL. Ces règles sont écrites en syntaxe JAPE (Cunningham, 2002) et greffées dans le processus d\u0027annotation par l\u0027ontologie des composants. \nRattachement semi-automatique des concepts\nÀ l\u0027issue de ces phases de détection de termes révélateurs de problèmes, l\u0027ontologie des problèmes informatiques ne montre pas de relations hiérarchiques entre les concepts. Par conséquent, nous avons conçu un algorithme de rattachement automatique pour lier chaque terme à un concept générique de l\u0027ontologie Problème (Problème Matériel, Problème Logiciel, etc.). Pour chaque concept de l\u0027ontologie Problème, nous générons une liste de concepts voisins (dans le même message) annotés par l\u0027ontologie Composants. Nous avons choisi ensuite un ensemble de concepts pivots de l\u0027ontologie Composants utilisés dans la majorité des discussions. Pour chaque liste obtenue, nous calculons la somme des distances sémantiques entre les concepts de cette liste et les concepts pivots. Nous calculons ces distances grâce au moteur de recherche sémantique CORESE. La catégorie retenue pour un terme est celle qui a la distance sémantique globale la plus petite. Par exemple le terme « lenteur du réseau » est rattaché à un « problème de modems » et le terme « cas d\u0027infraction » est rattaché à un « problème de sécurité ». La méthode adoptée pour la construction de l\u0027ontologie des problèmes est géné-ralisable pour d\u0027autres listes de diffusion, ainsi qu\u0027à d\u0027autres domaines. En effet :\n-La première phase de nettoyage des messages est indépendante du domaine.\n-Nous supposons que les termes utilisés pour poser un problème ne diffèrent pas d\u0027un domaine à l\u0027autre, ce qui rend la phase d\u0027amorçage réutilisable. -Des régularités structurelles dans les termes utilisés existent ou apparaissent au fur et à mesure au sein de la même communauté (i.e. notre cas et celui de SAMOVAR), ce qui facilite la tâche d\u0027enrichissement de l\u0027ontologie.\nExploitation des ontologies\nAnnotation sémantique\nNous avons développé un module d\u0027annotation sémantique qui interroge le serveur d\u0027annotation de KIM (Popov et al., 2004). Nous avons enrichi la plate-forme KIM par nos ontologies OntoPedia et l\u0027ontologie des problèmes pour fournir pour chaque message les entités nommées détectés que nous sauvegardons en RDF 7 .\nNavigation guidée par l\u0027ontologie\nL\u0027ontologie @pretic a pour but de guider la recherche dans les questions fréquemment posées pour résoudre les problèmes rencontrés dans la CoP. Nous avons ainsi développé une interface Web (cf. figure 1)  \nConclusion et perspectives\nDans cet article, nous avons présenté une approche pour la génération semi-automatique d\u0027une ontologie et d\u0027annotations sémantiques à partir d\u0027une liste de diffusion de support informatique. La finalité de ce travail est la proposition d\u0027une FAQ sémantique utilisant des ontologies, des annotations sémantiques et des raisonnements offerts par un moteur de recherche sémantique (i.e. CORESE). L\u0027approche proposée s\u0027inspire en partie de celle de (Golebiowska et al., 2001) et plus généralement de celle de (Aussenac-Gilles et al., 2000), proposées pour la construction d\u0027ontologies à partir des textes. Deux des originalités de ce travail consistent en (i) l\u0027utilisation d\u0027un corpus très dégradé, à savoir le corpus de mails, et (ii) l\u0027utilisation effective de deux approches et outils correspondants de TAL, ce qui à notre avis rend les résultats plus riches.\nLe caractère informel des messages échangés sur la liste de diffusion a rendu les tâches de nettoyage et d\u0027extraction très prenantes et différentes de celles présentées dans (Even et Enguehard, 2002) où les connaissances sont extraites à partir de textes dégradés mais plus au moins formels. Le nettoyage de mails a été aussi traité dans (Tang et al., 2006). Enfin, notons que l\u0027ontologie des problèmes a été validée par les membres de la CoP @pretic ; ils seront aussi impliqués dans la génération semi-automatique de la FAQ.\n"
  },
  {
    "id": "896",
    "text": "Introduction\nDurant la dernière décade, les machines à vecteurs de support (ou Séparateurs à Vaste Marge : SVM) ont connu un immense succès, principalement comme puissants classifieurs. Cependant, une des principales limitations des SVM est le manque d\u0027intelligibilité des résul-tats. En effet, les SVM ne produisent pas d\u0027explications ni d\u0027indices quant aux raisons d\u0027une classification et les résultats produits doivent être pris tels quels, en faisant confiance au système. Nous proposons de rendre les SVM actionnables en classant (ordonnant) les exemples, pas seulement en les classifiant. En effet, les moyens d\u0027action sont la plupart du temps limités, ce qui ne permet d\u0027agir que sur une petite partie des exemples de la population. De plus, le classement peut être très utile pour \"tamiser\" les exemples d\u0027apprentissage afin de ne garder que les exemples réellement importants, représentatifs des classes. L\u0027idée sous-jacente de notre méthodologie consiste à contraster les résultats ordonnés des SVM afin de découvrir les principales propriétés caractéristiques discriminantes entre la partie haute (désignée Top dans la suite) et la partie basse (désignée par Bottom) du classement produit par SVM. Nous sommes concernés par le problème d\u0027intelligibilité, car de notre expérience pratique, les experts du domaine sont clairement beaucoup plus confiants quand il s\u0027agit d\u0027agir sur des exemples hautement classés, si les raisons d\u0027un tel classement sont fournies avec la liste ordonnée des exemples. De plus, il peut être aussi important de comprendre la partie basse du classement. Ceci peut être d\u0027un grand intérêt pour l\u0027expert du domaine pour diriger des actions et comprendre le système.\nLe schéma général de notre approche est comme suit : -Ordonner les exemples en utilisant les SVM.\n-Créer deux sous-ensembles des données ordonnées : les n exemples du haut (Top n) et les m exemples du bas (Bottom m) de l\u0027ordonnancement. Typiquement, n \u003d m, et ||n + m|| \u003d 5 ? 20% du total des exemples. -Extraire l\u0027ensemble des propriétés les plus importantes en analysant les motifs d\u0027attributs dans les sous-ensembles Top et Bottom. Notons qu\u0027ignorer la partie milieu de l\u0027ordonnancement et se concentrer seulement sur les extrémités, permet de faciliter l\u0027extraction de motifs intéressants. Nous voulons identifier les caractéristiques des exemples du haut de la liste en les contrastant aux exemples du bas. En effet, il est possible d\u0027analyser la fréquence relative des différentes propriétés (attribut\u003dvaleur) et de calculer l\u0027importance de telles propriétés en utilisant des indices statistiques tels que le \"Leverage\". Nous présentons une application de notre méthode sur diverse données dont des données médicales concernant des patients de l\u0027athérosclérose. Nos résultats empiriques semblent très prometteurs et montrent l\u0027utilité de notre approche quant à l\u0027intelligibilité et l\u0027actionnabilité des résultats produits par SVM.\nÉtat de l\u0027Art\nUn certain nombre de travaux récents (Barakat et Diederich (2004); Barakat et Bradley (2006) Nunez et al. (Nunez et al. (2002)) ont suggéré une méthode géométrique permettant de transformer un classifieur SVM en règles de classification. Pour ce faire, les auteurs utilisent les k-moyennes pour déterminer un ensemble de vecteurs prototypes. Lorsqu\u0027ils sont combinés avec les vecteurs de support se trouvant sur la marge SVM, ces vecteurs aident à construire les frontières d\u0027ellipsoïdes ou d\u0027hyper-rectangles. Ces derniers sont traduits en équations Si-Alors ou à des règles d\u0027intervalles respectivement. Une approche similaire est proposée par Zhang et al. (Zhang et al. (2005)) et suggère un algorithme pour l\u0027extraction de règles d\u0027hyper-rectangles (HRE) pour SVM. La principale différence avec l\u0027approche précédente est que le regroupement des vecteurs de support (Ben-Hur et al. (2002)) est utilisé pour trouver les vecteurs prototypes de chaque classe au lieu des k-moyennes. Ceci évite de choisir le nombre de groupes à priori.\nBarakat et Bradley (Barakat et Diederich (2004)) combinent les arbres de décision aux SVM pour produire les explications. Ceci est réalisé comme suit : d\u0027abord, construire un classifieur SVM. Ensuite, sélectionner les vecteurs de support générés par le modèle et écarter leurs étiquettes de classes. Le modèle SVM est ensuite utilisé pour prédire la classe des vecteurs de support ce qui conduit à une nouvel ensemble de données. Enfin, construire un arbre de décision en utilisant les nouvelles données pour produire des règles symboliques. Les règles de décision ainsi produites sont alors évaluées sur un ensemble test afin de vérifier que les nouveaux exemples sont classés correctement par l\u0027arbre de décision. Les mesures utilisées ici, principalement exactitude (accuracy) et fidélité, ont été étendues dans (Barakat et Bradley (2006)) à l\u0027aire sous la courbe ROC.\nFung et al. (Fung et al. (2005)) proposent une approche pour convertir les SVM linéaires en un ensemble de règles ne se chevauchant pas de la forme : ? n i\u003d1 l i ? x i \u003c u i équivalentes au classifieur linéaire. Ceci est réalisé en résolvant un simple problème de programmation linéaire a 2n variables, n étant le nombre d\u0027attributs. Chaque règle représente un hyper-cube dans un espace de dimension n avec des surfaces à axes parallèles. L\u0027ensemble des règles optimales est calculé soit en utilisant un critère de maximisation de volume de l\u0027hyper-cube ou un critère de couverture qui maximise le nombre de points dans le demi-espace. Les règles sont exprimées sous forme de disjonctions de conjonctions. L\u0027approche que nous proposons diffère fondamentalement des approches décrites ci-dessus dans le sens où nous n\u0027utilisons pas les vecteurs de support pour extraire les explications des SVM. A la place, nous focalisons plutôt sur les exemples se trouvant en haut (Top) et en bas (Bottom) des exemples ordonnés par SVM, ce qui est moins couteux que d\u0027utiliser un ensemble de vecteurs de support potentiellement grand. Notre principal argument est que l\u0027utilisation des vecteurs de support n\u0027est pas nécessairement un bon choix. En effet, la zone autour de la marge SVM est très bruitée et le fait que les vecteurs de support séparent les classes ne veut nullement dire qu\u0027ils sont représentatifs de ces classes.\nNotre Approche\nNotre approche se décompose en deux étapes. La première consiste à classer (ordonner) l\u0027ensemble des exemples en utilisant les SVM. Cette étape est décrite en Section 3.1. La seconde étape concerne l\u0027extraction des explications proprement dites et ce à partir de l\u0027ensemble ordonné d\u0027exemples. Nous décrivons cette étape dans 3.2 et donnons l\u0027algorithme YSVM.\nClassement via les SVM\nNous nous intéressons au problème de classement des données. Le terme classement dé-signe le processus qui consiste à considérer un ensemble de données et les classer dans un ordre significatif et utile. Le classement supervisé permet d\u0027atteindre tel objectif en utilisant les attributs des exemples ainsi que leurs étiquettes de classe. Plus formellement, nous voudrions classer des exemples (x 1 , y 1 ), . . . , (x n , y n ), où x 1 , . . . , x n sont des vecteurs attributs décrivant les objets o 1 , . . . , o n , et chaque objet o i est étiqueté par la classe y i ? {+1, ?1}. Même si le but est de produire un classement, les données du problème sont similaires à ceux d\u0027un problème de classification. C\u0027est pourquoi, nous utilisons une méthode de classification (SVM) et convertissons les résultats fournis en classement. Plus précisément, nous classons les objets en triant les objets par leur valeurs de décision de SVM linéaires (Vapnik (1995)) :\noù les ? sont des variables dites ressort qui pénalisent l\u0027erreur commise et le paramètre C détermine le compromis entre régularisation et pénalisation des erreurs de classification. Le paramètre R ajuste la pénalité pour la classe positive. Puisque nous voudrions pénaliser les erreurs d\u0027étiquetage d\u0027un exemple par la proportion de la popultation de la classe, nous pouvons fixer le paramètre R par : R \u003d nombre de vrais négatifs nombre de vrais positifs (2) Typiquement, SVM produit un classifieur qui étiquette les exemples x par y \u003d sign(w T x+b), mais nous ne mettons pas de seuil à nos résultats de telle manière à pouvoir ordonner nos exemples selon la fermeté avec laquelle le classifieur prédit la classe de chaque exemple. Autrement dit, nous gardons plutôt le score donné par SVM à l\u0027exemple et non pas seulement son signe. Nous utilisons les courbes ROC (Receiver Operating Characteristic) (Bradley (1997)) pour évaluer la qualité de notre classement. Ils procurent une bonne façon de mesurer la qualité du classement lorsque la seule vérité dont nous disposons est si un exemple appartient au haut du classement (étiqueté +1) ou plutôt en bas (étiqueté ?1). ROC est essentiellement normalisé par la cardinalité de la classe ce qui est similaire à la normalisation de la fonction de perte (loss) que nous avons pris pour apprendre le SVM. La qualité d\u0027une courbe ROC est facilement mesurée à l\u0027aide de l\u0027aire sous la courbe (Area Under the Curve : AUC), qui se trouve dans l\u0027intervalle [0, 1]. Une aire de 0.5 peut être atteinte avec un classement aléatoire des données alors qu\u0027une aire 1.0 est atteinte en ordonnant parfaitement les exemples positifs en haut et les négatifs en bas.\nExemple Considérons un ensemble de 100 composants électriques. Chaque composant est décrit par son numéro de série, âge, taille et fabriquant et est étiqueté par son statut de panne (label\u003d1 pour en panne, -1 autrement). Un classement SVM permet d\u0027ordonner les composants selon leur susceptibilité aux pannes. Le haut du classement aurait dans ce cas les composants les plus sensibles aux pannes alors que les composants du bas du classement ont une moindre tendance aux pannes. Ainsi, l\u0027expert du domaine peut focaliser sur les n composants du haut du classement pour agir, par exemple en planifiant des inspections/remplacements.\nExtraction d\u0027Explications\nLa phase d\u0027extraction d\u0027explications consiste à sélectionner et comparer d\u0027abord les exemples du haut et du bas du classement. Ainsi, nous groupons les exemples en trois ensembles, où les exemples les plus \"purs\", i.e. les \"très positifs\" et les \"très négatifs\" se trouvent en haut et bas du classement respectivement. Les exemples du milieu du classement, autour de la marge SVM, sont plutôt les exemples bruités. Une question que l\u0027on peut se poser est pourquoi considérer le haut et le bas du classement au lieu de comparer les exemples de la classe positive et ceux de la classe négative. La raison pour laquelle nous ne comparons pas simplement les deux classes est que certains exemples sont considérés comme étant négatifs alors que leur classe est en vérité inconnue. Ceci peut arriver fréquemment dans les applications réelles. Dans l\u0027exemple précédent, nous ne sommes pas surs si les exemples négatifs le sont réelle-ment : même si ces composants ne sont pas en panne, ils peuvent l\u0027être dans un avenir proche. Les SVM ne devraient pas classer ces exemples profondément dans le demi-espace de la classe négative, mais plutôt autour de la marge séparant les deux classes. L\u0027approche que nous suggé-rons ici écarterait ces exemples en focalisant seulement sur le haut et le bas du classement, là ou les étiquettes des exemples sont les plus fiables. Une fois que l\u0027on a focalisé sur les extrémités du classement, nous recherchons l\u0027ensemble des règles intéressantes de la forme :\noù P ropriete est une paire attribut-valeur et Concept est soit le concept \"être en haut\" ou \"être en bas\". Nous évaluons l\u0027importance des propriétés en utilisant l\u0027indice statistique de Leverage (Piatetsky-Shapiro (1991)). La raison pour laquelle nous avons choisi cette mesure est le fait qu\u0027elle combine bien un haut pouvoir discriminant avec la capture des propriétés associées les plus fréquentes (support élevé). La mesure de Leverage a été utlisé dans d\u0027autres tâches d\u0027apprentissage telles que la caractérisation ( voir par exemple Turmeaux et al. (2003)), On la trouvera aussi dans la littérature avec d\u0027autres noms tels que Nouveauté. Le Leverage de la règle ci-dessus est donné par :\nLa mesure de Leverage évalue la proportion d\u0027exemples additionnels couverts par la partie gauche et droite de la règle au dessus de ceux attendus si les deux cotés de la règle sont indépendants l\u0027un de l\u0027autre. Clairement, nous avons ?0.25 ? Leverage(R) ? +0.25. Une propriété est dite intéressante pour un concept donné si la valeur de son Leverage est fortement positive. Ceci indique une forte association entre la propriété et le concept, alors qu\u0027une forte valeur négative indique une forte association entre la propriété et la négation du concept. Dans notre approche, le Leverage d\u0027une règle peut etre estimé empiriquement par :\noù Concept est soit T (l\u0027ensemble Top) ou B (l\u0027ensemble Bottom), p est une propriété et la notation V p (x) est une fonction booléenne telle que pour un exemple x, nous avons V p (x) \u003d vrai ou f aux ce qui veut dire que la propriété p peut être satisfaite par x ou non. \nL\u0027algorithme YSVM (donné en Algorithme 1) explore l\u0027espace de recherche des proprié-tés possibles pour découvrir celles qui sont les plus importantes (parties gauches de règles) qui ont conduit SVM à classer des exemples avant d\u0027autres. Pour une meilleure visualisation, l\u0027algorithme extrait aussi un histogramme pour chaque attribut donnant la fréquence relative de ses valeurs dans Top et Bottom. Nous avons étendu YSM pour traiter des conjonctions de propriétés. Nous l\u0027avons également étendu pour essayer différentes valeurs de Top et Bottom afin de sélectionner les tailles qui conduisent au plus grand nombre de propriétés intéressantes.\nExemple Considérons la liste ordonnée de composants électriques illustrée dans la Table 1. Extraire les principales propriétés comme le montre le Tableau 2 aide à identifier les facteurs de pannes. Par exemple, il peut être important de trouver des motifs dans les attributs des exemples ordonnés, comme savoir que des composants particuliers d\u0027un certain fabriquant sont disproportionnellement responsables de failles. Le but ultime est d\u0027aider l\u0027expert dans son choix quant à l\u0027achat de composants fiables, planification des inspections, etc. \nTests Empiriques\nNous avons implémenté YSVM en Python et avons conduit des tests empiriques sur divers benchmarks. Nous avons utilisé SVMLight 1 pour obtenir les classements SVM des différentes bases d\u0027exemples.\nDonnées Synthéthiques\nNous avons d\u0027abord vérifié si YSVM capturait les \"bons attributs\". Pour cela, nous avons généré une base d\u0027exemples aléatoires synthétiques de 1000 exemples. Chaque exemple est décrit par 50 attributs tels que X ? {?1, 1} 50 . Les étiquettes de classes sont assignées comme suit : Y \u003d sign( k\u003d11 k\u003d1 X k ). En d\u0027autres termes, l\u0027étiquette de classe est une combinaison linéaire des 11 premiers attributs parmi les 50 attributs. YSVM a réussi à re-découvrir ces attributs en focalisant seulement sur le top 5% et bottom 5% du classement avec Leverage minimum de 0.08. Il n\u0027a pas été possible de découvrir ces attributs en utilisant tous les exemples jusqu\u0027á ce qu\u0027on ait réduit le Leverage minimum à une valeur très faible. On en conclut qu\u0027il y a plus de pouvoir discriminant dans Top+Bottom que dans toute la base d\u0027exemples.\nDonnées de l\u0027Atherosclerose\nNous décrivons dans ce qui suit les tests\nque nous avons effectué sur des données mé-dicales dans le contexte du projet Stulong 2 . Les données concernent une étude qui s\u0027est étalée sur 20 ans concernant les facteurs de risque de l\u0027athérosclérose dans une population de 1 419 hommes. Nous avons utilisé un ensemble de données préparé par (Lucas et al. (2002)) en se fixant le but d\u0027identifier les principaux facteurs de risque de cette maladie. Les attributs utilisés sont donnés en Annexe 1 Tableau 4. Les patients ont été classés en 3 groupes : un groupe normal, un groupe à risque et enfin un groupe ayant la pathologie. Alors que cet attribut n\u0027a pas été utilisé durant l\u0027aprentissage du classement avec les SVM, les patients ayant la maladie et ceux qui sont à risques ont été classés avant les patients normaux. La cible d\u0027apprentissage est l\u0027attribut \"death\". La Figure  1 montre la courbe ROC pour les résultats s\u0027apprentissage avec les parties Top 10% et Bottom 10% mises en évidence où Top regroupe les patients les plus malades et Bottom ceux en meilleure santé par rapport à la maladie. Nous avons utilisé YSVM avec différentes valeurs de Top et Bottom et avons retenu les valeurs Top\u003d5% et Bottom\u003d5% donnant le plus grand nombre de propriétés intéressantes. Les résul-tats sont reportés en Figure 2 et les histogrammes associés en Annexe 1, Figure 5. Nos tests avec différents Top et Bottom ont montré que plus on augmentait ||T+B||, moins on obtenait de propriétés intéressantes. Conernant le temps d\u0027éxecution, il faut compter quelques secondes pour générer des propriétés de taille ? 2. (par manque de place, les détails ne sont pas fournis ici). Les données de Stulong ont déjà fait l\u0027objet de nombreuses publications (voir par exemple Lucas et al. (2002)). Les facteurs de l\u0027athérosclérose sont connus pour être principalement la consommation et durée de consommation de tabac, le surpoids, l\u0027activité physique alors qu\u0027il n\u0027y pas d\u0027évidence quant à l\u0027impact de la consommation d\u0027alcool comme facteur de la maladie. Tous ces facteurs ont bien été découverts par YSVM comme le montre la Figure 3 et la Figure 5. Enfin, la \nSummary\nSupport Vector Machines (SVMs) have attracted a great deal of attention and achieved huge success mainly as powerful classifiers. However, one of the main drawbacks of SVMs is the lack of intelligibility of the results. SVMs are \"black box\" systems that do not provide insights on the reasons of a classification or explanations -the results produced must be taken on faith. We are concerned about the problem of intelligibility because from our practical experience, domain experts strongly prefer Machine Learning with explanations. In that context, we have developed a new approach to provide explanations and make SVMs results more actionable. The underlying idea is to produce explanations by applying symbolic Machine Learning models to SVM-produced ranking results. More precisely, we are contrasting SVM results from the top and bottom of rankings to detect the main characteristic properties of the classes which can be useful for the practitioner to direct actions and understand the system. We applied our approach on several datasets. Our empirical results seem promising and show the utility of our methodology with regard to the intelligibility and actionability of an SVM output. Key words: Support Vector Machines (SVMs); Ranking, Rule Extraction; Actionability. \n"
  },
  {
    "id": "897",
    "text": "Introduction\n, le Data Mining est un processus non-trivial d\u0027identification de structures inconnues, valides et potentiellement exploitables dans les bases de données. Plusieurs intervenants industriels ont proposé une formalisation de ce processus, sous la forme d\u0027un guide méthodologique nommé CRISP-DM pour CRoss Industry Standard Process for Data Mining, voir Chapman et al (2000). Le modèle CRISP-DM (FIG 1) propose de découper tout processus Data Mining en six phases:\n1. La phase de recueil des besoins fixe les objectifs industriels et les critères de succès, évalue les ressources, les contraintes et les hypothèses nécessaires à la réalisation des objectifs, traduit les objectifs et critères industriels en objectifs et critères techniques, et décrit un plan de résolution afin d\u0027atteindre les objectifs techniques. 2. La phase de compréhension des données réalise la collecte initiale des données, en produit une description, étudie éventuellement quelques hypothèses à l\u0027aide de visualisations et vérifie le niveau de qualité des données. 3. La phase de préparation des données consiste en la construction d\u0027une table de données pour modélisation (Pyle, 1999;Chapman et al, 2000). Nous nous y intéressons plus particulièrement par la suite. 4. La phase de modélisation procède à la sélection de techniques de modélisation, met en place un protocole de test de la qualité des modèles obtenus, construit les modèles et les évalue selon le protocole de test. 5. La phase d\u0027évaluation estime si les objectifs industriels ont été atteints, s\u0027assure que le processus a bien suivi le déroulement escompté et détermine la phase suivante.\n6. La phase de déploiement industrialise l\u0027utilisation du modèle en situation opérationnelle, définit un plan de contrôle et de maintenance, produit un rapport final et effectue une revue de projet.\nFIG 1: Processus Data Mining CRISP DM\nLe modèle CRISP-DM est essentiellement un guide méthodologique pour la conduite d\u0027un projet Data Mining. La plupart des praticiens du Data Mining s\u0027accordent pour dire que les phases de préparation de données et de déploiement consomment à elles seules 80 % des ressources des projets. L\u0027explication est simple. L\u0027utilisation des méthodes statistiques nécessite de représenter les données sous la forme d\u0027un tableau croisé : en ligne les instances et en colonnes les variables caractérisant ces instances. Or, afin d\u0027optimiser le stockage, les données sont stockées dans des bases de données relationnelles, et ce quel que soit le phénomène étudié : les gènes, les transactions de cartes bancaires, les sessions IP, les informations sur les clients… Lors de la phase de préparation de données, la première tâche de l\u0027analyste est donc d\u0027extraire un tableau croisé du système d\u0027information. Cette étape n\u0027est pas anodine car le nombre de représentations potentielles des données relationnelles dans un tableau croisé est gigantesque (FIG 2). En pratique, l\u0027analyste doit faire un choix a priori de l\u0027ensemble des variables explicatives sur lesquelles se fera l\u0027étude. La conséquence est que la perte d\u0027information due à la mise à plat des données relationnelles est très importante.\nLors de la phase de déploiement, le modèle construit préalablement doit être appliqué à toute la population concernée, afin de produire un score pour chaque instance. Toutes les variables explicatives pour toutes les instances doivent être construites. Cette étape est potentiellement très couteuse lorsque le nombre d\u0027instances et de variables explicatives est important.\nLes principaux produits commerciaux de Data Mining, comme SAS, ou SPSS, proposent des plateformes permettant de construire et de déployer des modèles prédictifs. Néanmoins, ils n\u0027offrent pas de solution satisfaisante pour exploiter tout le potentiel de l\u0027information contenue dans la base de données source. Dans les applications industrielles construites sur ces plateformes logicielles, le nombre de variables explicatives à partir desquelles sont construits les modèles reste limité à quelques centaines. Or le potentiel est tout simplement d\u0027un autre ordre. Dans l\u0027exemple illustratif présenté (FIG 2), la base de données ne contient que deux tables. L\u0027étude du nombre d\u0027usages par type de service, par mois, et par jour de la semaine pourrait conduire à elle seule à construire 10000 variables explicatives ! RNTI -X - Avec ces technologies, un difficile compromis doit être trouvé entre le coût de mise en oeuvre et la qualité de l\u0027information produite.\nParmi les produits commerciaux, KXEN propose un module permettant de construire automatiquement des agrégats à partir de données temporelles. L\u0027avantage est de pouvoir explorer un plus grand nombre de variables explicatives. Nous avons très largement généralisé et systématisé cette approche pour automatiser entièrement le processus de préparation de données. La construction de variables explicatives est entièrement pilotée par un algorithme de sélection de représentation très performant (Boullé, 2007 2 L\u0027automatisation de la préparation de données 2.1 Généralités L\u0027objectif de la sélection de représentation est triple: améliorer la performance prédictive des modèles, le temps d\u0027apprentissage et de déploiement des modèles, et permettre leur interprétation (Guyon et Elisseeff, 2003). La sélection de variables est un sujet bien couvert dans la recherche en fouille de données, si bien qu\u0027aujourd\u0027hui, les méthodes de sélection de variables sont suffisamment robustes pour permettre la construction de modèle en très grande dimension (Guyon et al, 2006) \nL\u0027architecture de traitements\nContrairement à l\u0027architecture actuelle de la fouille de données, les variables explicatives ne sont pas calculées à l\u0027avance dans un datamart. Dans notre architecture de traitements, les données permettant de construire les variables explicatives sont stockées dans une base de données relationnelle simple, le data folder (FIG 3). Les variables explicatives sont construites et sélectionnées automatiquement en fonction de l\u0027étude menée. Le modèle de données du data folder permet d\u0027assurer une normalisation des différentes sources de données qui seront toujours présentées sous la forme d\u0027un schéma en étoile : \nLe pilotage des extractions\nLes extractions entre le data folder et les tableaux croisés sont paramétrées par trois types de dictionnaires :\n-le dictionnaire de sélection pour filtrer les instances, -le dictionnaire de requêtes pour spécifier les mises à plat des données du data folder,\n-le dictionnaire de préparation pour spécifier le recodage des variables. Ces dictionnaires d\u0027extraction permettent de définir des requêtes suffisamment simples pour être pilotées automatiquement par les processus de sélection de représentation et suffisamment expressives pour produire une très grande variété de variables explicatives.\nQuelque soit son objet, une requête portera toujours sur deux tables au plus : la \nLa sélection de représentation\nL\u0027architecture de traitements permet à un algorithme de piloter efficacement des extractions de tableaux croisés pouvant compter des dizaines de milliers de variables. Pour sélectionner la meilleure représentation possible, nous avons besoin d\u0027une méthode de sélection de variable particulièrement robuste et rapide. Deux approches principales, filtre et enveloppe (Kohavi et John, 1997), ont été proposées dans la littérature pour sélectionner les variables. Les méthodes enveloppes sont très coûteuses en temps de calcul (Féraud et Clérot, 2001, Lemaire et Féraud, 2006. C\u0027est pourquoi nous avons retenu une approche de type filtre pour déterminer et construire les variables explicatives pertinentes. L\u0027approche filtre la plus fréquemment utilisée repose sur la mise en oeuvre de tests statistiques (Saporta, 1990), comme par exemple le test du Khi2 pour les variables explicatives catégorielles, ou les tests de Student ou de Fisher-Snedecor pour les variables explicatives numériques. Ces tests d\u0027indépendance sont simples à mettre en oeuvre, mais présentent de nombreux inconvénients.\nRNTI -X -Ils se limitent à une discrimination entre variables dépendantes et indépendantes, sans permettre un ordonnancement précis des variables explicatives, et sont contraints par des hypothèses d\u0027applicabilité fortes (effectifs minimaux, hypothèse de distribution gaussienne dans le cas numérique…). De nombreux autres critères d\u0027évaluation de la dépendance entre deux variables ont été étudiés dans le contexte des arbres de décision (Zighed et Rakotomalala, 2000). Ces critères sont basés sur une partition de la variable explicative, en intervalles dans le cas numérique et en groupe de valeurs dans le cas catégoriel. En recherchant de façon non paramétrique un modèle de dépendance entre variables explicatives et cible, ils permettent une évaluation fine de l\u0027importance prédictive des variables explicatives. Dans le cas où tous les modèles de partitionnement de la variable explicative sont envisagés, un compromis doit être trouvé entre finesse de la partition et fiabilité statistique. Ce compromis est réalisé dans l\u0027approche MODL (Minimum Optimized Description Length, voir Boullé, 2005Boullé, , 2006   Les trois premiers termes représentent l\u0027a priori du modèle: choix du nombre d\u0027intervalles, des bornes des intervalles, et de la distribution des valeurs cibles dans chaque intervalle. Le dernier terme représente la vraisemblance d\u0027observer les valeurs de la variable cible connaissant le modèle de discrétisation. A titre illustratif (FIG 4), nous donnons le modèle de discrétisation de la variable SepalWidth obtenu par l\u0027optimisation de ce critère pour séparer les différents iris (Blake et Merz, 1996). Le même type de critère est construit pour le groupement de valeurs. Le critère possède une structure similaire à celle du critère de discrétisation, en remplaçant dans les deux premiers termes la probabilité a priori d\u0027une partition en intervalles par celle d\u0027une partition en groupes de valeurs.\n( )  La FIG 5 illustre le résultat du groupement des valeurs de la variable couleur de chapeau grâce à l\u0027optimisation du critère pour la classification des champignons comestibles et vénéneux (Blake et Merz, 1996).\nLa discrétisation et le groupement de valeurs optimaux sont recherchés en optimisant les critères d\u0027évaluation, au moyen de l\u0027heuristique gloutonne ascendante décrite dans (Boullé, 2005). La complexité algorithmique en O (JN log (N)) de cette heuristique associée à l\u0027excellente fiabilité de la méthode nous permet de traiter un grand nombre de variables, de l\u0027ordre de 50 000. Un modèle bayesien naïf sélectif (Boulle 2007), basé sur une sélection régularisée des variables et une moyenne de modèles, est ensuite construit pour supprimer les variables redondantes et pour produire les scores. \nRNTI -X -\nLa sélection des parangons\nLa table des parangons est déterminante pour la performance finale du système. Une table de parangons peu représentative pourrait conduire à la construction de scores inefficace sur l\u0027ensemble de la population. A contrario, une base de parangons de très grande taille diminuerait sensiblement l\u0027intérêt de l\u0027utilisation des parangons. Nous devrons donc gérer au mieux le compromis entre la réduction de volumétrie et la représentativité de la base.\nPour sélectionner efficacement les instances, notre approche est d\u0027utiliser un algorithme d\u0027échantillonnage en une seule passe optimisant un critère de représentativité de l\u0027échantillon. L\u0027algorithme Reservoir Sampling permet de construire un échantillon grâce à un réservoir maintenu en ligne en ajoutant et supprimant aléatoirement des instances du réservoir (Vitter 1985). Cet algorithme stochastique est très bien adapté au cas des flux de données de taille infinie et pas nécessairement stationnaires. Néanmoins le problème que nous cherchons à résoudre est de nature différente : nous connaissons la table de données que nous voulons échantillonner. Dans ce cas un algorithme déterministe est plus adapté. Il permet d\u0027optimiser un critère de représentativité de l\u0027échantillon (Li 2002) en un nombre connu à l\u0027avance d\u0027itérations. Des versions déterministes de Reservoir Sampling existent. L\u0027algorithme Deterministic Reservoir Sampling (Akcan et al 2006) minimise sur le réservoir un critère local inspiré de l\u0027algorithme FAST (Chen et al 2002) : la distance L2 entre l\u0027échantillon et l\u0027ensemble total est minimisée en ajoutant et supprimant en ligne des instances au réservoir.\nAfin de réduire le temps de traitement, pour optimiser le critère local nous nous contentons de remplir au fur et à mesure un réservoir jusqu\u0027à ce qu\u0027il atteigne la taille P désirée sans supprimer d\u0027instances. Le risque théorique est de tomber dans un minimum local. En pratique, la taille de notre échantillon est suffisamment importante tant en termes de proportion que de nombre d\u0027instances pour que ce risque soit faible (de l\u0027ordre de 1% pour une taille de 10000). L\u0027algorithme utilisé est le suivant :\n1. Le réservoir est initialisé par les K premières instances rencontrées. 2. Pour p allant de K à P : -une instance est choisie dans une fenêtre de recherche de taille M de manière à minimiser C(p) le critère de qualité de l\u0027échantillon, RNTI -X --la fenêtre est ensuite décalée de L instances de manière à obtenir un échantillon de taille P lorsque la table complète de taille N sera parcourue, avec L \u003d (N-M)/P. La taille de la fenêtre de recherche permet de régler un compromis entre le coût de traitement et la précision de l\u0027algorithme : plus M est grand, mois l\u0027algorithme est rapide, mais plus il est précis. Le paramètre K permet d\u0027initialiser l\u0027algorithme d\u0027optimisation. Une trop petite valeur de K risque de rendre totalement inefficace les premières itérations de l\u0027optimisation du Khi².\nNous utilisons le critère du Khi² pour mesurer la proximité entre l\u0027échantillon et la table complète. L\u0027algorithme de discrétisation et groupement de valeurs, décrit à la section précédente, nous permet d\u0027extraire une représentation binaire : chaque variable binaire i correspond à la fréquence d\u0027une modalité d\u0027une variable discrétisée. Le critère à minimiser s\u0027écrit alors :\nOù i S est la fréquence théorique de la variable binaire i donnée par l\u0027algorithme de discrétisation et groupement de valeurs, et p i S la fréquence observée dans l\u0027échantillon de taille p.\nL\u0027indexation\nLe problème qu\u0027on cherche à résoudre est simple à énoncer: étant donné un individu, trouver son plus proche voisin parmi la base des parangons; on souhaite faire cela pour tous les individus de l\u0027entrepôt.\nLa recherche du plus proche voisin est une opération coûteuse. Son implémentation naïve implique une recherche exhaustive parmi les parangons, donc une complexité en O(n.m.p), n étant le nombre d\u0027individus dont on cherche les voisins, m le nombre de variables dans l\u0027espace de représentation et p le nombre de parangons. Afin d\u0027accélérer la recherche du plus proche voisin, on peut être amené à préférer un compromis entre vitesse et performance plutôt que de viser la performance maximale (trouver le plus proche voisin). C\u0027est précisément ce que permet l\u0027algorithme Locality Sensitive Hashing (Gionis et al 1999). Il repose sur une technique de hachage pour sélectionner de bons candidats parmi les parangons pour être proche voisin de l\u0027instance considérée. On applique ensuite une recherche exhaustive parmi ces candidats. Notre implémentation de cette technique permet de ramener la complexité de la recherche du plus proche voisin à ( ) p m n o . . soit un gain d\u0027un facteur 300 pour 100 000 parangons, et laisse à l\u0027utilisateur le contrôle du compromis vitesse / performance.\nExpérimentations\nPour mesurer la fiabilité des scores produits par notre approche, nous avons comparé des scores construits pour les campagnes marketing de France Télécom avec ou sans notre technologie. Nous avons alimenté la plateforme avec différents jeux de données provenant des applications décisionnelles du groupe France Télécom. Nous avons consolidé des informations de 1 000 000 de clients du groupe sur un passé récent entre janvier et juin 2005.\nRNTI -X -\nLes quatre premiers mois ont été utilisés pour construire les profils des clients et les deux derniers pour calculer la variable cible. 20% des clients sont réservés pour l\u0027évaluation les modèles. Pour évaluer la qualité d\u0027un modèle, nous utilisons une courbe de gain (FIG 6). Ce type de courbe permet de sélectionner un modèle par rapport à son efficacité économique. L\u0027axe des abscisses correspond à la proportion de la population visée par les courriers et donc au coût de la campagne. L\u0027axe des ordonnées identifie le pourcentage de la population cible touchée, et donc le gain de la campagne marketing. Le modèle utilisé actuellement par les services marketing de France Télécom se base sur 200 variables explicatives. Lorsque 20% de la population est contactée, 45% des clients fragiles sont contactés. Le gain est de + 25% par rapport au ciblage aléatoire. L\u0027automatisation de la recherche de représentation, nous a conduit à sélectionner un modèle se basant sur 191 variables explicatives choisies dans un ensemble de 50 000 variables. Le modèle est ensuite déployé sur toutes les instances en utilisant un nombre variable de parangons : 500, 5 000, 15 000, et déploiement direct sur la population. Avec un déploiement direct sur toutes les instances, lorsque 20% de la population est contactée en se basant sur ce ciblage, 65% des clients qui vont rendre leur abonnement dans les deux prochains mois sont touchés. Par rapport à la technique actuelle, pour le même nombre de courriers, 20% supplémentaire de la population cible est touchée. Cette amélioration du ciblage est vraie sur toute la courbe de gain.\nLorsque la technique de déploiement des scores par les parangons est utilisée, il y a une perte potentielle de fiabilité qui dépend du nombre de parangons utilisés. Plus ce nombre est important, plus le ciblage est proche du meilleur possible, mais plus il est couteux à utiliser. Lorsque 5 000 parangons sont utilisés pour représenter 1 000 000 clients, à 20% de la population, 60% des clients fragiles sont touchés. Le gain reste de + 40% par rapport à RNTI -X -l\u0027aléatoire et de + 15% par rapport à la technique actuelle. Avec 15 000 parangons, les performances obtenues sont quasiment similaires à celles obtenues avec un déploiement direct. Pour évaluer la qualité de l\u0027algorithme de sélection de parangons, nous avons comparé les performances obtenues lorsque les parangons sont sélectionnés de manière aléatoire, avec les performances obtenues lorsque les parangons sont sélectionnés en optimisant le Khi² entre la distribution théorique des variables et celle obtenue dans l\u0027échantillon. Avec 500 parangons obtenus avec K\u003d200 et M\u003d1000, à 20 % de la population, 50 % de la cible est atteinte pour la sélection aléatoire contre 55 % pour l\u0027optimisation locale du Khi² (FIG 6).\nLe processus complet d\u0027extraction de la table des parangons à partir d\u0027un million de clients et d\u0027un espace de recherche de 50 000 variables correspond à 20 heures de calcul sur un serveur comprenant quatre processeurs à 3 Ghz muni chacun de 3 Go de mémoire. Deux tiers du temps de traitement correspond à la sélection de la représentation et un tiers par la recherche et l\u0027indexation des parangons. Une fois les parangons obtenus, la production des scores à partir de la table des parangons est faite en moins d\u0027une minute, alors que 2 heures de traitements sont nécessaires avec la méthode habituelle pour générer une table d\u0027un million d\u0027instances caractérisées par 191 variables explicatives et appliquer le modèle sur cette table. L\u0027utilisation de parangons est très efficace pour déployer un score récurrent. : en utilisant de l\u0027ordre de 1 % de parangons pour représenter la population totale, la perte de précision du modèle est négligeable et le coût de déploiement est divisé par 100.\nConclusion\nNous avons décrit une plateforme de fouille de données permettant de construire des modèles de prévision basés sur un nombre de variables explicatives de deux ordres de grandeurs au-dessus de ce qui se fait actuellement. La conséquence est une nette augmentation de la qualité des modèles. Cette plateforme repose sur une architecture novatrice permettant d\u0027automatiser les traitements couplée avec des méthodes performantes de construction, sélection, et indexation de variables et / ou d\u0027instances. Le temps de traitement du à la mise à plat des données reste la principale limite à l\u0027exploration d\u0027un espace de recherche plus grand. Pour aller plus loin dans l\u0027exploration des grandes masses d\u0027information, nous devrons élaborer une méthode de parcours de l\u0027espace des variables permettant de se diriger plus rapidement vers les zones contenant les variables pertinentes.\n"
  },
  {
    "id": "899",
    "text": "Introduction\nContexte\nLe traitement des documents de propriété intellectuelle, tels que les brevets, est important pour l\u0027industrie, les affaires et les communautés juridiques. Récemment, les communautés de recherche académiques et en particulier, les chercheurs de traitement automatique de la langue naturelle et de la recherche documentaire ont reconnu l\u0027importance du traitement des brevets. En fouillant les brevets scientifiques, nous pouvons remarquer un volume important d\u0027informations sur la biologie, les substances et les procédures médicales. En effet, l\u0027extraction des informations de ces brevets permet de donner une idée précise sur : (i) par exemple les interactions biomédicales et l\u0027effet pharmacologique résultant, et (ii) la propriété intellectuelle dans un certain contexte biologique.\nDurant ces dernières années, de grands efforts ont été exercés pour mettre les données relatives aux brevets sous une forme électronique et les présenter au public via les services en ligne. De nos jours, nous remarquons que ces services présentent et fournissent des structures de données hétérogènes, ce qui rend difficile à mettre en oeuvre une analyse automatique des brevets.\nDans ce papier, nous présentons l\u0027approche PatAnnot fondée sur les principes du web sémantique et qui se réfère aux notions de métadonnée et ontologies pour faciliter l\u0027extraction des connaissances et la recherche d\u0027informations relatives aux brevets.\nCe travail rentre dans le cadre du projet européen Sealife (Schroeder et al, 2006) qui a pour objectif la réalisation d\u0027un navigateur Web sémantique pour le domaine des sciences de la vie, qui exploitera les ressources du Web en les rendant partageables, accessibles et manipulables par plusieurs utilisateurs dans différents domaines biomédicaux et ce afin de favoriser le partage des connaissances.\nAnnotations sémantiques sur les brevets\nNotre travail vise à faciliter la génération automatique des annotations sémantiques à base d\u0027ontologies sur les brevets accessibles en ligne et repose sur une approche basée sur les principes et les technologies du web sémantique. Ces annotations peuvent être utilisées par les moteurs de recherche sémantiques afin d\u0027extraire les connaissances incluses dans les brevets et les présenter selon le profil de l\u0027utilisateur.\nUne annotation est une description permettant d\u0027avoir une information du type métadon-née facilitant l\u0027exploitation, l\u0027accès, la recherche et la reconnaissance d\u0027une ressource. L\u0027annotation peut se baser sur un modèle conceptuel comme par exemple une ontologie afin d\u0027avoir un aspect sémantique lui permettant d\u0027être utilisable, accessible et reconnue par un ensemble d\u0027acteurs ou d\u0027agents. Ainsi une annotation sémantique permet d\u0027établir un lien entre une entité d\u0027une ressource donnée et sa représentation sémantique décrite dans le modèle qui est en général une ontologie relative au domaine où la ressource évolue.\nLa formalisation du modèle d\u0027annotation se base sur des ontologies ; l\u0027utilisation de la hiérarchie de l\u0027ontologie peut (i) permettre aux annotateurs de choisir le niveau approprié de détail de l\u0027annotation, (ii) diminuer l\u0027ambiguïté de la connaissance et (iii) aider à réduire les erreurs au cours du processus d\u0027annotation. Dans le contexte du web sémantique, l\u0027utilisation des formalismes standards tels que RDF (Lassila et Swick, 2001)  \nL\u0027approche PatAnnot\nL\u0027idée capitale est de pouvoir prendre en considération la structure des brevets afin de retrouver un lien entre, d\u0027une part, les connaissances contenues dans les documents et, d\u0027autre part, les concepts de l\u0027ontologie utilisée. Nous avons commencé par parcourir la littérature des brevets existante en ligne à travers les sites officiels des offices nationaux et internationaux des brevets et quelques moteurs de recherche spécifiques aux brevets afin de déterminer les caractéristiques de ces documents en termes de structure, contenu et typologie. Ainsi l\u0027annotation sémantique générée portera sur trois aspects principaux : la structure, les méta-données et le contenu textuel des documents de brevets. Vers une fouille sémantique des brevets Nous pouvons remarquer que l\u0027ontologie de brevet intervient dans différents niveaux de l\u0027architecture du système grâce à sa modularité, ce qui permet de raffiner l\u0027annotation sé-mantique et de l\u0027enrichir à chaque étape.\nOntologie de brevet : PatOnto\nLa connaissance de la structure du document de brevet fournit non seulement une meilleure image sur la morphologie du document mais peut aussi réellement mener le procédé d\u0027analyse. Ainsi la représentation de ces documents est une tâche importante. Jusqu\u0027ici la recherche et le développement dans le domaine de l\u0027analyse des brevets ont été limités à des approches qui ne tiennent pas compte du contenu sémantique des brevets. Ainsi les tâches orientées contenu sont assurées manuellement. De ce fait, une représentation sémantique des documents de brevets ne doit pas porter sur un seul aspect de ces documents. Nous avons donc conçu une ontologie modulaire PatOnto qui modélise une représentation sémantique des brevets. La figure 3 décrit les différentes composantes de cette ontologie. 2. Les métadonnées du document de brevet sont modélisées par l\u0027ontologie \u0027\u0027Auxiliary-Data\u0027\u0027. C\u0027est une modélisation des informations explicites qui décrivent directement des documents de brevets, et des informations implicites qui exigent un traitement linguistique avancé. Les métadonnées explicites dans ces documents sont utiles pour la description des ressources documentaires puisqu\u0027elles concernent le titre de l\u0027invention, le nom de l\u0027inventeur, la classification du document et toute autre information bibliographique. Ainsi les métadonnées explicites sont bien définies, elles suivent des règles spécifiques. Les métadonnées implicites se composent des concepts qui doivent être extraits en appliquant des niveaux plus élevés d\u0027association entre les documents de brevets avec leur contenu textuel. Ce genre d\u0027informations peut être extrait de la section des références située au niveau de la section de page de garde. La figure 5 décrit un extrait de cette ontologie que nous avons développé en OWL et qui est composée de 109 concepts et 54 relations. 3. L\u0027ontologie \u0027\u0027Patent Media-Content\u0027\u0027 décrit le contenu non textuel du document de brevet, en effet les illustrations d\u0027une invention font croître la complexité du document car ce ne sont pas des images ordinaires mais divers genres de schémas qui n\u0027ont pas la même interprétation. Le but d\u0027une telle ontologie est de modéliser les objets multimédias qui constituent une partie du document de brevet. Cette ontologie est constituée de 59 concepts et 18 relations, modélisés en OWL. Les concepts que nous avons conçus sont disjoints, par exemple, un objet multimédia ne peut pas être classé en même temps comme étant une formule et une figure. La figure 6 pré-sente un extrait de cette ontologie. ChemicalFormula Figure   FIG. 6 -Extrait de l\u0027ontologie « Patent Media-Content ».\nEn conclusion, l\u0027ontologie de brevets PatOnto que nous avons conçue et développée est constituée de trois sous-ontologies décrites précédemment, cette ontologie servira comme référence dans le processus de génération des annotations sémantiques sur les brevets.\nL\u0027annotation sémantique est basée aussi sur une ontologie de domaine, de ce fait comme nous traitons les brevets du domaine biomédical nous avons choisi le vocabulaire UMLS (Humphreys et Lindberg, 1993)   (Khelif et al., 2007).\nLa section suivante reprend le processus de génération des annotations sémantiques dé-cris par le schéma d\u0027architecture de la figure 2.\nGénération des annotations sémantiques\nLa génération des annotations sémantiques sur les brevets est un processus qui suit les étapes suivantes que nous allons détailler par la suite :\n1. Structurer le document de brevet ; 2. Générer les annotations sémantiques correspondant aux métadonnées utilisant l\u0027ontologie de brevet. 3. Générer les annotations sémantiques sur le contenu en se basant sur l\u0027ontologie de brevet et l\u0027ontologie de domaine et fusionner les annotations. Nous comptons sur la structure quasi uniforme des brevets fournis par les organismes principaux de la propriété intellectuelle dans le monde qui éditent plus de 80% 2 des brevets accordés dans le monde (OMPI, USPTO 3 ,…). Nous supposons que prendre en considération la structure et le contenu textuel des documents de brevets va nous permettre de générer des annotations sémantiques plus riches.\nStructuration des documents de brevets\nLes documents de brevets accessibles en ligne sont en majorité en format HTML. En analysant progressivement cette représentation, nous avons proposé une méthode de transforma-tion de ce format initial vers le format standard XML. L\u0027utilisation de XML résout le problème en assurant une description structurée du document de brevet, mais il n\u0027y a aucune manière générique simple qui permet de convertir un document HTML vers XML en tenant compte de l\u0027arborescence de ses métadonnées. Ainsi nous avons proposé un processus basé sur les transformations XSLT qui permet de (i) charger le document de brevet en le représen-tant sous forme d\u0027une arborescence et (ii) le transformer en XML.\nPhase 1 : Construction de l\u0027arbre HTML du document de brevet Les balises HTML sont utilisées par les navigateurs pour gérer l\u0027affichage d\u0027une page HTML. En effet ces balises sont conçues pour permettre le repérage des structures logiques ayant une influence sur la présentation physique.\nLe format HTML tolère la non fermeture de certaines balises comme \u003cbr\u003e, \u003chr\u003e, \u003cp\u003e, \u003cli\u003e etc. ainsi que la fermeture d\u0027une balise avant la fermeture de ses fils, par exemple, \u003cstrong\u003e\u003cfont\u003etoto\u003c/strong\u003e\u003c/font\u003e. Par conséquent, la construction d\u0027un arbre de repré-sentation du document est impossible puisque nous avons une ambiguïté dans la classification des noeuds. Comme solution, nous avons appliqué une API existante appelée \u0027\u0027HTMLCleaner\u0027\u0027, cette API permet de corriger le code HTML d\u0027un document et d\u0027extraire son arborescence. Un tel arbre se compose d\u0027une racine, des noeuds internes et des feuilles ; un noeud correspond à une étiquette HTML, une feuille peut être un texte ou une étiquette. Phase 1 : Annotation basée sur l\u0027ontologie « Patent Content-Form » Cette annotation est la principale qui sera raffinée pendant les étapes suivantes du processus de génération des annotations sémantiques. En utilisant un fichier de configuration que nous avons proposé, la correspondance consiste à parcourir le document XML du brevet et l\u0027ontologie et à construire un fichier de correspondance (figure 9).\nSi le noeud courant correspond à un concept C de l\u0027ontologie, alors ce concept C est ajouté au document de correspondance et une recherche dans l\u0027ontologie est effectuée afin de trouver les propriétés appropriées qui peuvent relier C à d\u0027autres concepts de l\u0027ontologie :\n-Pour chaque propriété P trouvée, le processus explore le document XML de brevet afin de trouver un autre concept C\u0027 pouvant être lié à C par cette propriété. Si un tel concept C\u0027 existe, alors la propriété P et le concept C\u0027 sont ajoutés au document de correspondance; -Si aucune propriété n\u0027est trouvée, alors le concept C est considéré comme un noeud isolé; Le document de correspondance résultant est de la forme suivante : \u003cResultMapping\u003e \u003cConcept\u003e \u003cCname\u003ePatentDocument\u003c/Cname\u003e \u003cOnProperty\u003e \u003cProperty\u003ehasPatentType\u003c/Property\u003e \u003cAppliedOn\u003ePatentType\u003c/AppliedOn\u003e \u003c/OnProperty\u003e ……………….\nFIG. 8 -Extrait du fichier de correspondance.\nParcourant ce fichier de correspondance et le document de brevet, un processus récursif permet la génération de l\u0027annotation selon les étapes suivantes :\n-Construire en mémoire un objet « annotation » de type DOM qu\u0027une méthode spéci-fique se charge de l\u0027initialiser avec tous les « espaces de noms » requis ; -Pour un concept C du document de correspondance : -Construire une instance de C dans l\u0027objet « annotation » avec la description formelle de la syntaxe RDF ; -Pour chaque propriété P figurant dans la description « OnProperty » du document de correspondance comme noeud fils du C, construire une instance de P suivant la syntaxe RDF et trouver le concept C\u0027 qui est associé à l\u0027étiquette XML figurant dans le noeud fils « AppliedOn » : o si C\u0027 se trouve dans le document de configuration comme concept qui a des propriétés comme noeuds fils, alors le processus refait le même traitement en gardant la référence du noeud XML ; o Sinon, instancier le concept et mettre la valeur du noeud XML correspondant comme attribut qui donne la valeur de cette ressource en RDF ; L\u0027annotation générée est une première version de l\u0027annotation sémantique de brevet qui va être enrichie tout au long du processus d\u0027annotation.\nPhase2 : Annotation basée sur l\u0027ontologie « Auxiliary-Data » Des parties textuelles telles que les références, les revendications et la description sont employées pour raffiner l\u0027annotation de métadonnées. Par exemple, au niveau des références, nous classons ces références en nous basant sur les concepts de cette ontologie (références de brevets américains, références étrangères et citations, …) puisque les brevets sont liés et une analyse de plusieurs brevets fournit des résultats plus consistants que ceux obtenus en étu-diant chaque brevet à part. Ainsi, l\u0027annotation sur la bibliographie peut être enrichie. Pour cela nous avons conçu une méthode qui permet lors de traitement d\u0027une partie appropriée du document, de générer une annotation sémantique en se fondant sur l\u0027ontologie \u0027\u0027AuxiliaryData\u0027\u0027, ensuite cette annotation est ajoutée à son emplacement dans l\u0027annotation principale.\nGrâce à cette ontologie, nous pouvons décomposer les parties textuelles du brevet en plusieurs petites parties significatives ayant chacune un lien avec tout le document pour préparer la phase de traitement automatique de la langue et l\u0027annotation de contenu basée sur l\u0027ontologie de domaine. \nGénération de l\u0027annotation du contenu basée sur l\u0027ontologie de domaine\nLes documents de brevet ont une terminologie spécifique et concrète qui affecte n\u0027importe quel genre de traitement linguistique. La terminologie biologique semble fréquente et importante de ce fait, nous avons utilisé UMLS (Humphreys et Lindberg, 1993) comme ontologie de domaine pour traiter les brevets biomédicaux. Si au cours de l\u0027annotation le processus rencontre une partie textuelle, il la sauvegarde en mémoire et garde une trace du noeud approprié de l\u0027objet « annotation », ce contenu textuel va être annoté par un autre processus. Ainsi nous avons conçu un ensemble de méthodes qui interrogent le module Mea-tAnnot (Khelif et al, 2007). MeatAnnot fait partie du projet MEAT (Memoire d\u0027Expériences pour l\u0027Analyse du Trascriptome) élaboré au sein de l\u0027équipe edelweiss/Acacia, il est généri-que et indépendant de toute ontologie ou plateforme et assure la détection des concepts existant dans le texte en se basant sur l\u0027ontologie du domaine en question. MeatAnnot repose sur des outils TALN (Traitement Automatique de la Langue Naturelle) tel que TreeTagger (Helmut, 1994), GATE (Cunningham et al, 2002) ainsi que d\u0027autres extensions propres à edelweiss dédiées à la détection des relations sémantiques et l\u0027extraction des concepts UMLS.\nAprès avoir collecté toutes ces informations linguistiques, MeatAnnot permet de générer une annotation RDF décrivant le texte proposé en entrée.\nGrâce à l\u0027utilisation des ontologies, nous avons pu générer des annotations sémantiques sur les documents de brevets décrivant leur contenu sémantique, ces annotations sont regroupées dans une base d\u0027annotations qui est chargée dans le moteur de recherche sémantique CORESE (Corby et al, 2004). Deux services Web ont été également développés pour encapsuler les différents processus, le premier prend en entrée un document brevet et fournit son annotation sémantique, et le deuxième permet de répondre aux requêtes en interrogeant Corese via SPARQL 4 .\nConclusion\nDiscussion\nL\u0027approche \u0027\u0027PatAnnot\u0027\u0027 que nous venons de présenter dans cet article vise à fournir un support méthodologique et technique pour faciliter la fouille des documents de brevets, considérés comme une source inestimable d\u0027informations scientifiques. En effet, le système implémentant l\u0027approche est un système modulaire développé en java et utilise les technologies standards du web sémantique. Notre système est générique et se compose de modules réutilisables ; (i) l\u0027ontologie modulaire \u0027\u0027PatOnto\u0027\u0027 que nous avons conçue et construite est indépendante du domaine et couvre toute information structurelle dans un document de brevet, (ii) l\u0027intégration des feuilles de style XSLT qui permettent de générer les documents XML est facile et flexible, (iii) l\u0027outil MeatAnnot est générique et indépendant des ontologies utilisées, (iv) le générateur permet de rassembler toutes les parties des annotations relatives à plusieurs ontologies en une seule annotation sémantique d\u0027un document de brevet qui porte sur plusieurs aspects de cette ressource. Enfin, PatAnnot a été testé et validé sur les deux plus grandes bases des brevets : USPTO et EPO, et ce en annotant automatiquement plus de 1000 brevets.\nTravaux connexes\nDans de nombreux domaines l\u0027analyse des brevets est une tâche très importante dont le but est de décrire l\u0027état de l\u0027art des inventions et préserver la propriété intellectuelle. Parmi les outils aidant à effectuer cette tâche nous citons :\nLe système Vigitext qui repose sur la méthode d\u0027exploration contextuelle pour la fouille des documents techniques (en particulier les brevets) à des fins de veille technologique (Gougon, 2000). Patent Cafe 5 , qui aide à mener des recherches professionnelles dans diffé-rentes bases de brevets et exploite l\u0027information sur la propriété intellectuelle provenant des différents offices. BioPatentMiner (Mukherjea et al, 2005) facilite la recherche d\u0027information dans les brevets biomédicaux, en identifiant les termes biologiques et les relations entre les brevets dans le but de fournir une approche basée sur le web sémantique. PATExpert (Mark et al, 2006) est un nouveau projet visant à fournir une approche web sémantique et des techniques avancées de traitement de brevets. Notre approche fondée sur les ontologies et les annotations sémantiques est originale par rapport à ces outils. En outre, notre correspondance entre les documents XML et les ontologies qui diffère de (Amann et al, 2001) et (Xiao et al, 2006) par sa généricité et sa manipulation des technologies du Web Sémantique.\nPerspectives\nCe travail sur l\u0027exploitation de brevets illustre une application intéressante de Web sé-mantique, très utile pour la gestion de connaissances des compagnies et des communautés collaborant par le Web. Comme perspective nous pouvons ajouter un module basé sur des outils de TALN qui permet d\u0027extraire à partir de la partie revendications le type du brevet (modèle d\u0027utilité, brevet de conception ou brevet d\u0027usine). L\u0027approche peut être appliquée à d\u0027autres domaines que le domaine biomédical : par exemple les domaines techniques. D\u0027ailleurs, les principes de notre approche (c.-à-d. l\u0027exploitation de la structure des documents et des techniques de TALN sur leur contenu textuel pour produire des annotations sémantiques) pourraient être généralisés à d\u0027autres genres de documents structurés (par exemple fiches patients, fiches d\u0027incident, etc.).\nRemerciements\nNous remercions la commission européenne pour le financement de ce travail dans le cadre du projet européen Sealife (IST-2006-027269).\n"
  },
  {
    "id": "900",
    "text": "Introduction\nLes travaux présentés dans cet article répondent aux besoins d\u0027une experte médiéviste souhaitant découvrir des connaissances nouvelles dans un corpus de textes écrits en Ancien Français. Les connaissances extraites à partir de ce corpus sont sous forme de motifs séquentiels. Dans notre contexte, un motif séquentiel est une suite ordonnée d\u0027itemsets (phrases). Un itemset est un ensemble d\u0027items (mots). Par exemple, le motif \u003c(chevalier dam)(roi)\u003e extrait à partir de notre corpus signifie que, souvent, les mots \"chevalier\" et \"dam\" apparaissent ensemble au sein d\u0027une même phrase avant l\u0027apparition de \"roi\" dans une phrase suivante. Ceci permet aux experts d\u0027analyser, sans a priori, les mots et enchaînements de mots qui apparaissent dans un même contexte, mettant ainsi en relief des associations susceptibles d\u0027apporter des connaissances nouvelles à un expert. Notons que dans l\u0027étude actuellement menée, l\u0027experte médiéviste souhaite plus particulièrement découvrir des motifs séquentiels faisant intervenir des mots propres à la parenté. Les différentes étapes et fonctionnalités de notre logiciel sont décrites dans la section suivante.\nProcessus d\u0027extraction des motifs séquentiels\nLa première étape du prétraitement des données textuelles consiste à appliquer le Tree Tagger de Schmid (1994) qui possède des règles et des lexiques adaptés à l\u0027Ancien Français. Ce système apporte des informations grammaticales aux différents mots du texte (par exemple, étiquettes \"adjectif\", \"nom\", etc). Les mots qui sont davantage porteurs de sens tels que les noms peuvent alors être filtrés. Par ailleurs, l\u0027utilisation du Tree Tagger permet de lemmatiser les mots du corpus. Après ce prétraitement, l\u0027extraction des motifs séquentiels à partir des données textuelles peut s\u0027effectuer à l\u0027aide de la méthode SPaC (Sequential PAtterns for Text Classification) qui est décrite dans (Jaillet et al. (2006)).\nUn thème pouvant être privilégié par l\u0027utilisateur (dans notre cas la parenté), notre logiciel permet de n\u0027extraire que des motifs relatifs à cette thématique au travers d\u0027une liste de\nFIG. 1 -Le moteur de recherche de l\u0027interface de visualisation des résultats et la consultation de l\u0027origine du motif séquentiel (chevalier)(enfant).\nmots pertinents du domaine. Les motifs séquentiels extraits du corpus ne seront alors que ceux dont au moins un item est un mot de la liste manuellement établie pas l\u0027utilisateur. L\u0027ajout de connaissances permet donc de filtrer les motifs offrant ainsi à l\u0027utilisateur des informations à la fois complètes (recherche sur l\u0027ensemble du corpus) et pertinentes (adaptées à la thématique).\nAfin de répondre à la difficulté liée au nombre de motifs qui peut être élevé, notre application s\u0027accompagne d\u0027un moteur de recherche permettant de mettre en relief les motifs contenant un ou plusieurs mots spécifiés par l\u0027utilisateur (figure 1). Les résultats d\u0027une recherche peuvent également être triés selon plusieurs critères (support, nombre d\u0027itemsets). Par ailleurs, notre logiciel permet de visualiser les phrases qui valident un motif séquentiel donné.\nConclusion\nNous proposons dans cet article un ensemble de méthodes implantées au sein d\u0027une interface dédiée aux utilisateurs non informaticiens mais experts des données. Une perspective envisageable pourrait exploiter le fait que l\u0027approche présentée s\u0027avère adaptée à une classification des paragraphes du corpus en fonction des thématiques présentes dans ce dernier. \nSummary\nThis paper introduces a tool to visualize sequential patterns extracted from textual data in Old French.\n"
  },
  {
    "id": "901",
    "text": "Résumé. Cet article propose une méthodologie pour la visualisation et la classification des parcours de vie. Plus spécifiquement, nous considérons les parcours de vie d\u0027individus suisses nés durant la première moitié du XXème siècle en utilisant les données provenant de l\u0027enquête biographique rétrospective menée en 2002 par le Panel suisse de ménages. Nous nous sommes concentrés sur ces évé-nements du parcours de vie : le départ du foyer parental, la naissance du premier enfant, le premier mariage et le premier divorce. A partir des données de base sur ces événements, nous discutons de leur transformation en séquences d\u0027états. Nous présentons ensuite notre méthodologie pour extraire de la connaissance des parcours de vie. Cette méthodologie repose sur des distances calculées par un algorithme d\u0027optimal matching. Ces distances sont ensuite utilisées pour la classification des parcours de vie et leur visualisation à l\u0027aide de techniques de « Multi Dimensional Scaling ». Cet article s\u0027intéresse en particulier aux problé-matiques entourant l\u0027application de ces méthodes aux données de parcours de vie.\nIntroduction\nNous proposons dans ce travail d\u0027étudier et de comparer diverses techniques de visualisation et de classification de parcours de vie 1 . Plus spécifiquement, nous considérons les parcours de vie familiale d\u0027individus suisses nés durant la première moitié du XXème siècle à partir de données récoltées par le Panel suisse de ménages. Les parcours de vie familiale sont composés d\u0027événements constitutifs de la vie familiale, comme le départ du foyer parental, le premier enfant, le premier mariage ou le premier divorce. Il est possible, à partir de ces événements, de considérer des parcours de vie individuels sous la forme de séquences d\u0027états, chaque événement survenant dans la vie de l\u0027individu correspondant à un changement d\u0027état. Une méthodologie ad hoc destinée à créer une typologie des parcours de vie et à visualiser les comportements individuels et l\u0027évolution des normes sociales les régulant est présentée ici. La méthode principale consiste à calculer une distance entre chaque séquence à l\u0027aide d\u0027un algorithme d\u0027optimal matching ; on obtient ainsi une distance qui respecte le caractère temporel des séquences de parcours de vie. Les résultats sont ensuite visualisés à l\u0027aide de méthodes de type « Multi Dimensional Scaling ».\nCet article est construit de la manière suivante. La première partie présente les données utilisées ainsi que les transformations nécessaires pour construire des séquences d\u0027états à partir d\u0027événements. La deuxième partie présente la méthode d\u0027optimal matching, son fonctionnement et la problématique de la définition du coût des opérations. La troisième partie concerne les méthodes de visualisation de type Multi Dimensional Scaling et leur principe de fonctionnement. La quatrième partie présente les résultats de l\u0027application de notre méthodologie aux données du Panel suisse de ménages. Les résultats sont interprétés à l\u0027aide de graphiques et de modèles de régression logistiques. Nous concluons finalement sur les possibilités que nous permettent d\u0027envisager l\u0027application de cette méthodologie aux données en sciences sociales.\nDonnées\nA partir des réponses à un questionnaire, nous extrayons des données sous la forme d\u0027un tableau où chaque ligne est un individu et chaque colonne une variable (tableau 1).\nind. naissance départ mariage enfant divorce 1 1974 1992 1994 1996 n/a TAB. 1 -Exemple de données sous la forme d\u0027événements\nLe passage à une représentation sous forme de séquences d\u0027états n\u0027est pas trivial. La difficulté consiste à représenter sous la forme d\u0027un unique état une combinaison d\u0027événements qui se sont déjà produits ou non à chaque âge. De manière plus formelle, nous définissons l\u0027état qui définit un individu à un âge précis comme une information sur les événements réalisés. On peut dire, à partir d\u0027un état, quels événements se sont déjà produits. La réalisation d\u0027un ou de plusieurs événement durant une année t entraîne le passage de l\u0027état dans lequel se trouvait l\u0027individu à t ? 1 à un nouvel état. La définition des états à partir des événements est un problème propre au type de données et à la problématique de recherche. Une manière simple de procéder consisterait à créer un état pour chaque combinaison d\u0027événements. Avec cette solution, le nombre d\u0027états s\u0027élèverait à 2 n pour n événements, ce qui rend l\u0027interprétation difficile dès lors qu\u0027on prend en considération beaucoup d\u0027événements. Nous avons donc choisi d\u0027agglomérer certaines combinaisons en accord avec les objectifs de recherche.\nDans le cadre de cette étude, nous avons décidé de retenir quatre événements constitutifs de la vie familiale : le départ du foyer parental, le premier mariage, le premier divorce et la naissance du premier enfant. Le tableau 2 présente le codage des états que nous avons établi par rapport aux quatre événements retenus. Le nombre d\u0027états a été réduit de 16 à 8, notamment en supprimant des états impossibles (tous ceux qui contiennent un divorce sans un mariage préalable) ou en combinant deux états (par exemple l\u0027état 2 concerne les individus mariés qui ne sont pas partis du foyer parental, qu\u0027ils aient eu des enfants ou non  1974 ... 1991 1992 1993 1994 1995 1996 1997 1998  \nMéthode\nNous reprenons ici la formulation de Rohwer et Pötter (2002). Prenons ?, l\u0027ensemble des opérations possibles, et a[w] le résultat de l\u0027application des opérations w ? ? sur la séquence a. Nous considérons trois types d\u0027opérations : l\u0027insertion d\u0027un élément, la suppression d\u0027un élé-ment, ou la substitution d\u0027un élément par un autre. Si l\u0027on attribue un coût c(w) qui correspond au coût d\u0027appliquer l\u0027opération w ? ?, la distance entre une séquence a et une séquence b peut être formalisée de la manière suivante :\nAutrement dit, pour chaque paire de séquences, on cherche la combinaison d\u0027opérations pour rendre les séquences identiques dont la somme des coûts est la plus petite. L\u0027algorithme utilisé pour trouver cette distance minimale utilise une méthode de programmation dynamique qui est décrite dans (Deonier et al., 2005). L\u0027implémentation de l\u0027algorithme que nous avons utilisée est celle présente dans le logiciel TDA ; son fonctionnement est détaillé dans son manuel d\u0027utilisation (Rohwer et Pötter, 2002).\nDéfinition des coûts\nComme nous l\u0027avons vu précédemment, un coût c peut être attribué aux opérations w ? ?. Les coûts de substitution, auxquels nous nous sommes intéressés en particulier, peuvent être représentés sous la forme d\u0027une matrice symétrique qui définit une valeur pour chaque paire d\u0027état. L\u0027attribution de ces valeurs en se basant sur un modèle théorique est particulièrement difficile dans le cadre d\u0027une utilisation en sciences sociales, ce qui fait l\u0027objet d\u0027un débat (Wu, 2000). Il est en effet délicat de décider du coût du passage d\u0027un état à un autre, mais il est pourtant intéressant et parfois capital de pouvoir différencier ces coûts. Pour cela, deux méthodes disponibles ont été essayées sur notre jeu de données. La première est implémentée dans le logiciel TDA (Rohwer et Pötter, 2002) et définit le coût de chaque substitution en fonction des taux de transition observés dans les données. Le coût du passage d\u0027un état i à un état j est donc calculé de la manière suivante : c i,j \u003d c j,i \u003d 2 ? P (i t |j t?1 ) ? P (j t |i t?1 ). Le coût de base est fixé à 2, et plus la probabilité P (i t |j t?1 ) de passer de l\u0027état i à l\u0027état j, et inversement, est grande, plus ce coût baisse. Ainsi, les substitutions correspondantes aux transitions observées fréquemment seront moins coûteuse que celles qui n\u0027arrivent jamais. Une autre méthode, proposée dans le logiciel T-COFFEE/SALTT (Notredame et al., 2005), consiste à calculer une matrice des coûts de substitution optimale par un processus itératif (Gauthier et al., 2007). Les tableaux 4 et 5 contiennent les résultats de l\u0027application de ces deux méthodes de définition des coûts de substitutions sur nos données. Une analyse visuelle du tableau 4 permet d\u0027observer qu\u0027un passage de l\u0027état 0 (aucun événement) à l\u0027état 7 (divorce) ne s\u0027observe jamais dans nos données, puisque son coût est de 2 dans les coûts tirés des taux de substitution (en gras). Cette transition correspondrait à un individu qui dans l\u0027espace d\u0027une année se marie puis divorce. Le passage de l\u0027état 3 (départ et mariage) à l\u0027état 6 (départ, mariage et enfant) est quant à lui beaucoup plus fréquent, et par conséquent moins coûteux. Le tableau 5 semble cohérent avec les coûts définis, même si la comparaison est difficile en raison de la plus grande variabilité des valeurs.\nLe coût des opérations d\u0027insertion et de suppression a quant à lui été fixé à une valeur unique de 3 dans la solution basée sur les taux de transition . Ce choix a pour but de favoriser au maximum les opérations de substitution (qui ont un coup maximum de 2) afin d\u0027éviter les phénomènes de distorsion du temps qu\u0027engendrent les opérations d\u0027insertion. solution, les seules situations où sont utilisées les insertions/suppressions sont en cas de léger décalage (p.ex. 0-1-2-3-4-4 à aligner avec 0-0-1-2-3-4). Dans le cas de la solution basée sur la matrice des coûts optimaux, le coût d\u0027insertion/suppression a été fixé selon les recommandations de Gauthier et al. (2007), c\u0027est-à-dire égal à la moyenne des coûts de substitution. La figure 1 donne une vision graphique de la disparité entre les matrices de distances calculées avec les différentes solutions de coût. Le graphique de gauche confronte les distances calculées avec les coûts de substitution fixés en fonction des taux de transition aux distances calculées avec un coût de substitution fixé à 2. Il apparaît très nettement que les résultats fournis par ces deux solutions sont quasiment identiques ( fig. 1 partie gauche). La comparaison de la solution des taux de substitution avec la solution des coûts optimaux montre une plus grande disparité des distances et un effet d\u0027échelle dû à la plus grande variabilité des coûts optimaux ( fig. 1 partie droite). On peut en conclure qu\u0027avec ce jeu de données, l\u0027utilisation des taux de transition plutôt qu\u0027un coût fixe n\u0027a que peu d\u0027influence sur les distances. En revanche, la différence entre la solution des taux de transition et la solution des coûts optimaux est plus marquée.\nClassification\nNous sommes maintenant capables de produire une matrice de distances mesurant les différences entre les parcours de vie des individus. Celle-ci peut être utilisée dans une procédure de classification hiérarchique ascendante selon la méthode de Ward. Le tableau 6 croise les FIG. 1 -La partie gauche présente les distances obtenues par la méthode avec les coûts substitutions basés sur les taux de transition selon les valeurs des distances obtenues avec des coûts de substitution fixés à 2 (le fait que les valeurs soient sur la diagonale indique que les distances obtenues par ces deux distances sont égales). La partie droite présente ces mêmes distances fondées sur les coûts de transition en fonction de celles calculées avec les coûts optimaux. Ces vues sont des graphiques en densité (plus la quantité de points associé à une unité de surface est grande, plus l\u0027unité de surface est foncée), ainsi ces figures restent lisibles malgré la grande quantité de points présentés (environ 2000 2 /2). \nMulti Dimensional Scaling\nAvant de procéder à une classification hiérarchique ascendante, la matrice de distances apporte peu d\u0027information aux experts. Ainsi, pour leur permettre d\u0027appréhender les résultats, nous proposons de générer des « cartes » exprimant les relations de proximité entre les parcours des individus. Une telle représentation intuitive des données peut être obtenue par des méthodes de type « Multi Dimensional Scaling ». De cette manière, on dispose d\u0027un outil qui permet de visualiser graphiquement les distances et d\u0027aider à la décision du nombre de groupes à retenir dans une classification hiérarchique.\nDD-HDS\nNous constatons que les représentations bidimensionnelles et tridimensionnelles obtenues à partir de ces données par Classical Multi Dimensional Scaling (Torgerson, 1952) sont peu efficaces (résultats non présentés). Nous formulons donc l\u0027hypothèse que l\u0027inefficacité de cette méthode pourrait être due à des relations non linéaires, puisqu\u0027elle fait implicitement appel à des projections linéaires. Dans ce cas, l\u0027utilisation d\u0027une méthode de réduction de dimension non-linéaire est recommandée (on peut citer par exemple dans ce cadre les SOM (Kohonen, 1997), Isomap (Tenenbaum et al., 2000) ou l\u0027analyse en composantes curvilignes (Desmartines et Hérault, 1997). Leur but commun est d\u0027offrir une configuration de points sur un espace de faible dimension qui préserve les distances entre les données (avec un effort particulier pour la conservation des distances courtes). Parmi elles, nous avons choisi DD-HDS (Data-Driven High Dimensional Scaling, (Lespinats et al., 2007b) Nous constatons en effet qu\u0027une représentation sur un espace bidimensionnel permet de rapprocher les individus dont les parcours de vie sont proches. Par exemple on peut observer que les individus divorcés se rassemblent sur la droite de la représentation ( fig. 3). Notons que plus le divorce est précoce, plus l\u0027individu s\u0027écarte vers la droite. La même analyse peut bien sûr être menée pour les 7 états, ce qui permet d\u0027appréhender facilement l\u0027organisation spatiale des individus (données non présentées).\nFIG. 3 -Représentation bidimensionnelle des parcours de vie. Le code couleur permet de visualiser l\u0027âge des divorces. Les points noirs de taille réduite correspondent aux individus qui n\u0027ont pas divorcé. Le niveau de gris des autres points exprime l\u0027âge de l\u0027individu au moment du divorce. Plus l\u0027individu est jeune au moment de son divorce, plus le point associé est clair.\nCe type de représentation permet également de visualiser d\u0027autres types d\u0027information. Par exemple, la figure 4 montre la répartition des dates de naissance des individus sur la représen-tation. Ainsi, nous observons que certains comportements ont eu tendance à disparaître comme le fait de rester chez ses parents (en haut au centre) et que des nouveaux comportements apparaissent comme les mariages tardifs (zone sur la gauche de la partie centrale).\nFIG. 4 -Organisation des dates de naissance dans la représentation. La représentation est divisée en unité de surface, le niveau de gris de chaque zone dépend de la moyenne des dates de naissance (plus la date moyenne est ancienne, plus l\u0027unité de surface associée est foncée). \nRankVisu\nEn termes de réduction de dimension, on cherche classiquement à préserver les distances entre données. RankVisu propose un nouveau point de vue sur les données en cherchant à conserver les rangs de voisinages (Lespinats et al., 2007a). Cette méthode renforce les groupes de données et permettra ainsi de valider notre clustering. La représentation obtenue à l\u0027aide de RankVisu est mise en relation avec le résultat d\u0027une classification hiérarchique (critère de Ward).\nNotons que ces deux méthodes se basent sur des informations relativement différentes : la classification s\u0027appuie sur les distances tandis que RankVisu utilise les rangs de voisinage entre données. La figure 5 présente la représentation obtenue par RankVisu, en distinguant les groupes identifiés par la classification en cinq classes. Chaque classe forme sur le graphique un groupe bien défini, ce qui renforce le crédit de notre classification ( fig. 5). En effet, les deux méthodes aboutissent à des conclusions comparables.\nInterprétations\nNous analysons maintenant les caractéristiques de chacun des groupes. L\u0027interprétation peut se faire de plusieurs manières ; nous privilégions ici une méthode visuelle pour la distinction des groupes. Nous disposons de deux types de graphique pour représenter la forme des séquences individuelles. Le premier type consiste à représenter, pour chaque âge entre 15 et 45 ans, la proportion d\u0027individus se trouvant dans chaque état. La figure fig. 7 donne les représentations pour les groupes 2 et 4. Le deuxième type de graphique représente quant à lui chaque séquence individuelle. Ainsi, on lit sur l\u0027abscisse l\u0027âge de l\u0027individu, et les séquences sont dessinées horizontalement. L\u0027ordre dans lequel les séquences apparaissent est définie par la distance qui les sépare d\u0027une séquence de référence choisie au hasard parmi toutes les sé-quences du groupe ( fig. 8)   16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45   15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  \n"
  },
  {
    "id": "902",
    "text": "Introduction\nLe Web représente aujourd\u0027hui la principale source d\u0027information. Ce gisement contenant une grande quantité de données non-structurées, distribuées et multi-medias a besoin d\u0027être maintenu, filtré et organisé pour permettre un usage efficace. Cette tâche s\u0027avère difficile à réaliser avec la large distribution, l\u0027ouverture et la forte dynamicité du Web. Par conséquent, plusieurs travaux de recherche ont tenté d\u0027analyser le contenu des sites Web et comprendre le comportement des utilisateurs de ces sites. L\u0027approche que nous proposons dans cet article se situe dans ce cadre. Notre objectif est d\u0027analyser un site Web en se basant sur le contenu et indépendamment de l\u0027usage. En d\u0027autres termes, nous cherchons à réduire la quantité d\u0027information contenue dans le site Web en un groupe de thèmes qui pourraient susciter l\u0027intérêt des internautes. Il sera par la suite possible d\u0027analyser le comportement des utilisateurs vis-à-vis de ces thèmes.\nApproche du Web Content Data Mining\nLe Web Content Data Mining (WCDM) est l\u0027application des techniques de Data mining au contenu du Web (textes, images, hyperliens…). Il est défini comme étant une analyse textuelle avancée intégrant l\u0027étude des liens hypertextes et la structure sémantique des pages Web. L\u0027approche que nous proposons pour le WCDM est présentée par le schéma suivant.\nPages Web\nTypage des pages\nPages hybrides Pages de contenu Pages de navigation Conversion des pages Web en fichiers Textes\nFichiers Textes\nLemmatisation et étiquetage à l\u0027aide de TreeTagger\nPrétraitement des textes\nChoix des descripteurs : sélection et extraction\nBase des attributs\nCatégorisation des textes : classification simultanée\nBase des descripteurs\nBlock clustering\nTextes catégorisés par thèmes\nFIG. 1 -Approche proposée pour le Web Content Data Mining.\nRNTI\nCette approche se déroule en trois étapes principales. La première est celle de la classification des pages Web en pages de navigation et pages de contenu. L\u0027objectif de cette étape est de limiter le travail postérieur aux pages de contenu. La deuxième est celle du prétraitement des textes et préparation des données et la dernière est celle de la classification simultanée ou block clustering.\nTypage des pages\nL\u0027objectif de cette étape est de distinguer les pages qui servent à faciliter la navigation sur le site, appelées pages de navigation ou pages auxiliaires, des pages contenant de l\u0027information qui pourrait intéresser l\u0027internaute. Ces pages sont appelés « pages de contenu ». Certaines pages Web sont à la fois des pages de contenu et des pages de navigation. Il s\u0027agit de pages hybrides.\nDescription des variables\nLe typage des pages est effectué à l\u0027aide d\u0027une classification appliquée aux pages du site caractérisées par un ensemble de variables. Dans (Charrad et al. 2006) les variables utilisées sont : le nombre de liens entrants (inlinks), le nombre de liens sortants (outlinks), la durée moyenne de consultation de chaque page et le nombre de visites à chaque page. Ces variables sont déterminées à partir des fichiers Logs résultant de la navigation sur le site. Comme nous nous intéressons à l\u0027analyse textuelle du site indépendamment de l\u0027usage, et que le site étudié est un site de tourisme (site de Metz) 1 , nous utilisons les variables suivantes: le nombre de liens entrants (inlinks), le nombre de liens sortants (outlinks), la taille du fichier (size), le nombre d\u0027images par page, la présence d\u0027un contenu textuel (texte).\nComme la variable « texte » est binaire, une première classification est effectuée sur les pages du site pour différencier les pages présentant un contenu textuel des pages présentant seulement des liens ou des images. Ainsi, la première hypothèse utilisée est que les pages ne contenant pas du texte sont des pages de navigation. Par contre, les pages présentant un contenu textuel peuvent servir de pages de navigation ou de pages de contenu. La détermination du type de la page est effectuée par la méthode de K-means.\nAnalyse des résultats\nLa méthode K-means (l\u0027algorithme de Forgy (1965)) est appliquée aux pages présentant un contenu textuel c.à.d appartenant à la première classe. Les variables utilisées pour caractériser ces pages sont le « Nombre Outlinks », le « Nombre Inlinks », le « Size » et le « Nombre Images ». Les caractéristiques des sous-classes obtenues permettent d\u0027attribuer une étiquette à chacune d\u0027entre elles. En effet, la sous-classe C1 comporte environ 9% de pages caractérisées par la présence d\u0027un nombre élevé de liens entrants et sortants utilisés pour passer d\u0027une page à une autre, une taille importante et un nombre faible d\u0027images. Ces pages sont destinés principalement pour passer d\u0027une page à une autre. Elles correspondent alors à des pages de navigation bien qu\u0027elles présentent un contenu textuel. Par contre, la sous-classe C2 comporte environ 27% de pages caractérisées par un nombre élevé de liens entrants et d\u0027images, en plus du contenu textuel, et un nombre faible de liens sortants. Ainsi, plusieurs pages pointent vers les pages de la sous-classe C2 mais elles pointent vers un nombre faible de pages. Ces pages correspondent à des pages de contenu. La sous-classe C3 présente les caractéristiques de pages de contenu (C2) et de pages de navigation (C3) à la fois, on les considère comme des pages hybrides. En résumé, trois classes sont obtenues à la fin de la classification : les pages auxiliaires résultant de la première et la seconde phase de classification, les pages de contenu et les pages hybrides.   \nVariables\nComparaison entre typage manuel et typage par Kmeans\nLe typage manuel consiste à analyser manuellement le site Web et attribuer une étiquette à chaque page selon qu\u0027elle soit une page de contenu, une page hybride ou une page de navigation. L\u0027attribution de l\u0027étiquette dépend seulement du motif de la visite que pourrait faire l\u0027internaute au site. En effet, une page de contenu pour un visiteur peut être considérée comme étant une page de navigation pour un autre. Le typage manuel du site de Metz a montré que 12% des pages du site sont considérées comme des pages de navigation, 11% comme des pages de contenu et 77% des pages sont des pages hybrides. Nous remarquons que les résultats de la classification sont proches des résultats du typage manuel. Ces résultats montrent que le site de Metz présente peu de pages auxiliaires et de pages de contenu. La majorité de pages sont de type hybride.\nPrétraitement des textes\nL\u0027objectif du prétraitement est de représenter chaque page du site par un vecteur de descripteurs qui donne une idée sur son contenu. Le prétraitement est réalisé en deux étapes. La première étape est celle de représentation des documents. La seconde est celle du choix des descripteurs.\nReprésentation des textes\nPlusieurs méthodes ont été proposées dans la littérature pour la représentation des textes. La méthode la plus utilisée est la représentation vectorielle, appelée «bag-of-words » ou « sac de mots » dans laquelle chaque texte est représenté par un vecteur de n termes pondérés (Salton et Buckley, 1998). À la base, les n termes sont les n différents mots apparaissant dans les textes de l\u0027ensemble d\u0027entraînement. Scott et Matwin (1999) ont fait des nombreux essais pour représenter les textes pour des fins de classification. Ils ont utilisé les groupes nominaux (des suites de noms et d\u0027adjectifs) pour construire les termes de l\u0027espace vectoriel et une application pour l\u0027analyse de la nature grammaticale des mots du texte. Ils ont aussi évalué l\u0027impact de regrouper les mots synonymes en un même méta-descripteur. La notion d\u0027hyperonymes a été aussi mise à l\u0027épreuve pour regrouper des mots. Aucune de ces méthodes n\u0027a produit de résultats équivalents ou supérieurs à l\u0027approche «bag-of-words». Lewis (1992) a également représenté les textes à l\u0027aide de groupes nominaux mais les résultats n\u0027étaient pas satisfaisants.\nEn adoptant l\u0027approche « bag of words », le poids associé à chaque terme peut être une valeur binaire, indiquant la présence ou l\u0027absence du terme dans le document (1 si le mot est présent dans le texte, 0 sinon), ou un entier positif représentant le nombre d\u0027occurrences du terme dans le document. L\u0027inconvénient d\u0027introduire ce comptage est qu\u0027il accorde un poids important aux termes qui apparaissent très souvent à travers toutes les classes des documents et qui sont peu représentatifs d\u0027une classe en particulier. Une autre méthode largement utilisée pour calculer le poids d\u0027un terme est la fonction TFIDF (acronyme de Term Frequency Inverse Document Frequency») (Salton et Buckley, 1998).\nChoix des descripteurs\nL\u0027inconvénient de l\u0027utilisation directe du vocabulaire contenu dans les textes d\u0027entraînement est la dimension très élevée de l\u0027espace vectoriel. L\u0027utilisation de tous les mots contenus dans les textes peut influencer négativement la précision de la classification. D\u0027autre part, un mot présent dans un nombre élevé de textes ne permet pas de décider de l\u0027appartenance d\u0027un texte qui le contient à l\u0027une ou l\u0027autre des catégories car son pouvoir de discrimination est faible.\nPour pallier ces problèmes, certaines techniques de réduction de la dimension du vocabulaire ont été mises en place. Ces techniques se divisent en deux grandes familles.\n-Les techniques basées sur la sélection de descripteurs («feature selection») : Ces techniques conservent seulement les descripteurs jugés utiles à la classification, selon une certaine fonction d\u0027évaluation. L\u0027avantage de la sélection de descripteurs consiste à éliminer les descripteurs réellement inutiles ou les descripteurs erronés («noisy») (mots mal orthographiés par exemple). Plusieurs techniques de sélection de descripteurs ont été développées en vue de réduire la dimension de l\u0027espace vectoriel. Chacune de ces techniques utilise des critères lui permettant de rejeter les descripteurs jugés inutiles à la tâche de classification. Le processus de sélection de descripteurs débute généralement par la suppression de mots très fréquents tels que les mots grammaticaux ou les mots de liaisons. La recherche de radical («stemming») et la lemmatisation sont d\u0027autres méthodes utilisées pour créer un vocabulaire réduit. Leur but est de regrouper en un seul descripteur les multiples formes morphologiques des mots qui ont une sémantique commune. Les mots très peu fréquents, qui n\u0027apparaissent qu\u0027une ou deux fois dans l\u0027ensemble de documents, sont également supprimés, car il n\u0027est pas possible de construire des statistiques fiables à partir d\u0027une ou deux occurrences (Stricker, 2000). -Les techniques basées sur l\u0027extraction de descripteurs («feature extraction») : Ces techniques créent des nouveaux descripteurs à partir des descripteurs de départ, en faisant des regroupements ou des transformations afin de réduire le nombre de descripteurs redondants. Le processus d\u0027extraction de descripteurs consiste à créer à partir des descripteurs originaux un ensemble de descripteurs synthétiques en effectuant des regroupements de termes « term clustering » ayant une sémantique commune (Stricker, 2000). Les classes obtenues deviennent les descripteurs d\u0027un nouvel espace vectoriel. Blum et Mitchell (1998) rapportent des résultats intéressants à propos de ce regroupement.\nRésultats du prétraitement\nDans notre cas, le prétraitement nécessite tout d\u0027abord la conversion des pages Web en fichiers Textes, et le remplacement des images qu\u0027ils contiennent par leurs légendes. Ces textes sont par la suite traités par l\u0027algorithme TreeTagger 2 qui a été développé à l\u0027Institut de Linguistique Computationelle de l\u0027Université de Stuttgart (Schmid, 1994).\nL\u0027étiquetage et la lemmatisation à l\u0027aide de TreeTagger permettent de remplacer les verbes par leur forme infinitive, les noms par leur forme au singulier et certaines formes des verbes tels que les participes présents et les participes passés par leurs racines. Afin de réduire la dimension de l\u0027espace vectoriel des vecteurs représentant les textes, il s\u0027avère nécessaire de supprimer : -Les formes de ponctuation, -Les mots vides tels que les prépositions, les déterminants, les numéros, les conjonctions, les pronoms et les abréviations, -les mots inutiles à la classification tels que les adverbes et les adjectifs, -Les termes de type non reconnu par TreeTagger sont examinés manuellement afin de ne garder que les noms et les verbes. D\u0027autre part, les termes auxquels TreeTagger attribue l\u0027étiquette « Nom » sont examinés afin de supprimer les noms propres que TreeTagger n\u0027arrive pas à identifier. Ainsi, seuls les noms et les verbes sont conservés dans la base des descripteurs. -Les mots très fréquents : Nous avons adopté la méthode proposée par Stricker (2000). En effet, le rapport )\n, tel que TF(m,t) est l\u0027occurrence du mot m dans un texte t et CF(m) est l\u0027occurrence du mot m dans l\u0027ensemble des documents, permet de classer les mots par ordre décroissant. Plus le mot m est fréquent, plus le ratio est faible et, inversement, plus un mot est rare, plus le ratio est élevé. Dans le cas limite où un mot n\u0027apparaît qu\u0027une seule fois dans l\u0027ensemble de documents, ce ratio vaut 1 et le mot est classé en tête de liste.\n-Les mots très peu fréquents : ce sont les mots -dont le nombre de documents dans lesquels ils apparaissent est inférieur à un certain seuil. Dans notre cas, nous supprimons les mots qui apparaissent dans une seule page du site Web. Ceci réduit la base des descripteurs à 652 descripteurs au lieu de 1500. -dont le nombre d\u0027occurrences dans la base est égal à 1 i.e les mots qui apparaissent une seule fois dans toute la base. Le prétraitement des textes aboutit à la construction d\u0027une matrice croisant 418 descripteurs à 125 pages avec le nombre d\u0027occurrences du descripteur dans une page du site comme poids. Les méthodes de classification automatique appliquées à des tableaux mettant en jeu deux ensembles de données agissent de façon dissymétrique et privilégient un des deux ensembles en ne faisant porter la structure recherchée que sur un seul ensemble. Ainsi, la détermination des liens entre les deux partitions est difficile. La recherche simultanée de partitions sur les deux ensembles a donné naissance à des méthodes de classification simultanée (block clustering) telles que les méthodes de classification directe de Hartigan (1972de Hartigan ( , 1975, les méthodes de classification croisée de Govaert (1983) et les méthodes de biclustering utilisée généralement en bioinformatique. Ces méthodes de classification simultanée fournissent des blocs homogènes à partir d\u0027une partition des instances et une partition des attributs recherchées simultanément.\nApplication de Croki2\nComme notre tableau des données est un tableau de contingence, nous avons appliqué l\u0027algorithme CROKI2 (classification CROisée optimisant le Khi2 du tableau de contingence) proposé par Govaert (1983) pour les tableaux de contingence. L\u0027objectif de cet algorithme est de trouver une partition P de I en K classes et une partition Q de J en L classes telle que \nmarginales définies sur le tableau T1 s\u0027écrivent :\nL\u0027algorithme CROKI2 consiste à déterminer une série de couples de partitions (P n ,Q n ) optimisant le 2 ? du tableau de contingence en appliquant alternativement sur I et sur J une variante de la méthode des nuées dynamiques. Les entrées de l\u0027algorithme sont : le tableau de contingence, le nombre de classes en ligne et en colonne et le nombre de tirages de départ. Les sorties de l\u0027algorithme sont : la valeur du Khi2 du tableau initial, la valeur du Khi2 du tableau (P,Q), le pourcentage d\u0027inertie (ou d\u0027information) conservée et le tableau de contingence initial réordonné en ligne et en colonne suivant les classes de deux partitions.\nAnalyse des résultats\nL\u0027application de l\u0027algorithme CROKI2 du logiciel SICLA permet d\u0027obtenir la nouvelle matrice suivante (  Comme aucun critère n\u0027est proposé pour choisir le nombre de classes en lignes et en colonnes dans l\u0027algorithme CROKI2, nous utilisons le coefficient T de Tschuprow \nTAB. 3 -Informations sur les biclasses.\nL\u0027examen de ces biclasses a pour objectif d\u0027attribuer un thème à chaque groupe de pages. Par exemple, la biclasse (2, 3) composée de la classe 2 de descripteurs et la classe 3 des pages a pour thème « spécialités de cuisine » sachant que presque tous les descripteurs sont en relation avec l\u0027alimentation (amande, crème, eau de vie, flamber, fruit, glacer, mirabelle, recette, purée…etc). Les biclasses (8,4) et (8,5) présentent le même thème « hébergement » puisque il s\u0027agit de la même classe en ligne. Par conséquent, il est possible de regrouper les pages de la classe 4 et de la classe 5 dans une même classe (4+5). La nouvelle biclasse obtenue après fusion des biclasses (8,4) et (8,5) a pour thème « informations sur les hôtels ou hébergement». Par contre, les biclasses (6,7) et (7,7) présentent en commun la même classe en colonne. Ainsi, les pages de la classe 7 auront deux thèmes différents « horaires et tarifs des lieux à visiter » et « réservation ».\nFIG. 4 -Exemple de biclasses.\nLe tableau suivant présente les meilleures biclasses et les thèmes qui leur sont associés. \nAnalyse factorielle des correspondances\nDans cette section, nous appliquons au tableau de contingence l\u0027analyse factorielle des correspondances. La projection des pages et des descripteurs sur le même plan factoriel (figure 5) permet d\u0027associer des groupes de pages à des groupes de descripteurs. A titre d\u0027exemple, Le nuage de points situé à gauche est composé des descripteurs (points bleus) appartenant à la classe 8 de descripteurs et des pages (triangles rouges) appartenant aux classes 4 et 5 de pages. Ce nuage de points correspond aux deux biclasses (8,4) et (8,5) déterminées à l\u0027aide de CROKI2. Le nuage de points en bas comporte les descripteurs appartenant à la classe 5 de descripteurs et les pages appartenant à la classe 8 de pages. Ce nuage correspond à la biclasse (5,8). L\u0027ensemble des points au centre est composé des descripteurs appartenant à la classe 1 et la classe 3 et des pages appartenant à la classe 1 et la RNTI classe 2 de pages. Ce nuage correspond aux deux biclasses (1,1) et (3,2). Le nuage situé en haut est un mélange de descripteurs et de pages appartenant aux biclasses (6,7), (7,7), (4,6) et (2,3). Ainsi, on trouve dans les résultats de l\u0027analyse factorielle des correspondances certains résultats obtenus dans la section précédente.\nProjection des pages (a)\nProjection des descripteurs (b)\nProjection des pages et des descripteurs ©\nFIG. 5-Projection des descripteurs et des pages sur les axes factoriels.\nConclusion\nDans cet article nous avons proposé une approche d\u0027analyse du contenu textuel d\u0027un site Web basée sur la catégorisation simultanée des pages et des descripteurs de pages et indépendamment de l\u0027usage du site. Les thèmes découverts vont servir par la suite à l\u0027analyse du site du point de vue de ses usagers. Les résultats obtenus par cette approche prouvent son applicabilité sur des sites Web non volumineux.\n"
  },
  {
    "id": "903",
    "text": "Introduction\nAvec l\u0027émergence du web sémantique, l\u0027exploitation des ressources sémantiques pour annoter des documents, personnaliser des services ou décrire des ressources disponibles sur le web est devenue essentielle. Créées par des communautés distinctes, existant déjà en grand nombre, les ressources sémantiques se différencient par des niveaux différents de formalisation et de conceptualisation engendrant un certain degré d\u0027hétérogénéité. Ainsi, l\u0027utilisation conjointe des éléments (documents, services web) décrits par des ressources distinctes est soumise à leur mise en correspondance. Un nouveau problème émerge, qui concerne la mise en correspondance des ressources sémantiques. Parmi les techniques proposées pour apporter des solutions à ce problème, l\u0027alignement sera considéré dans ce papier. L\u0027alignement établit des appariements entre les entités appartenant à deux ressources distinctes. Au-delà d\u0027une certaine complexité, taille, ou nombre de ressources il est impossible d\u0027établir manuellement ces appariements. La nécessite des méthodes automatiques (ou semi automatiques) pour l\u0027alignement des ressources sémantiques est évidente.\nDans le cadre de ce travail, une ressource sémantique représente un modèle de connaissance spécifique à un domaine. Elle est constituée de concepts, qui modélisent les objets spécifiques au domaine et de rôles, qui correspondent à des relations entre ces objets. Les concepts et les rôles peuvent être structurés dans une hiérarchie. L\u0027alignement est réalisé automatiquement et met en correspondance des entités ayant la même nature. Ainsi, soient S s et S c deux ressources qui décrivent le même domaine. L\u0027alignement associe à chaque entité de e s de S s une entité e c appartenant à S c , si et seulement si les deux entités ont la même nature et modélisent des objets ou des relations qui sont similaires, voir identiques. Un ensemble de règles d\u0027appariement est utilisé pour établir ces correspondances. Les règles sont déduites empiriquement et interviennent à plusieurs niveaux : formel, conceptuel, etc.. Des algorithmes sont ensuite implémentés qui combinent ces règles afin de mettre en oeuvre l\u0027alignement.\nLe papier est structuré en trois parties : la première définit l\u0027alignement et introduit l\u0027ensemble des règles d\u0027appariement utilisées. La deuxième présente l\u0027alignement de deux ressources du domaine de l\u0027accidentologie et met en évidence les problèmes spécifiques à cette étude de cas. Des travaux connexes sont présentés dans la troisième partie.\nAlignement de ressources sémantiques\nNous avons proposé une approche pour aligner deux ressources d\u0027un même domaine qui est fondée sur le cadre proposé par Ehrig et Sure (2004). Ce cadre définit des règles empiriques pour estimer le degré de ressemblance entre les entités (concepts ou rôles) des deux ressources. Il a été choisi car : les règles définies prennent en compte des éléments situés à différents niveaux conceptuels ; il n\u0027existe pas de contrainte concernant la modélisation et la formalisation des ressources ; il est possible d\u0027enrichir le cadre initial en ajoutant de nouvelles règles d\u0027appariement. Par la suite on introduit l\u0027alignement des ressources, tel qu\u0027il est défini pour ce travail, le cadre proposé par Ehrig et Sure (2004) ainsi que les règles d\u0027appariement ajoutées pour l\u0027enrichir.\nDéfinition\nL\u0027alignement des ressources sémantiques peut être défini formellement comme suit : soient S s (source) et S c (cible) deux ressources sémantiques du même domaine ; E Ss , E S c les ensembles d\u0027entités (concepts et rôles), appartenant à la ressource S s , respectivement S c . L\u0027alignement s\u0027exprime par une fonction : \nDans le cadre de ce travail, l\u0027alignement associe une seule entité de S c à une entité de e s j . Ceci représente un choix de modélisation, déterminé par l\u0027utilisation ultérieure des résultats issus de cet alignement.\nRègles pour l\u0027alignement des ressources sémantiques\nDes règles empiriques pour supporter l\u0027alignement des ressources sémantiques ont été identifiées par Ehrig et Sure (2004). Les règles définies estiment la similarité entre deux entités en prenant en compte des éléments qui se trouvent à différents niveaux conceptuels. Ces niveaux sont repartis sur une échelle, présentée dans la fig. 1. Ehrig et Sure (2004) Les éléments situés sur chaque niveau ainsi que les règles définies à partir de ces éléments sont présentés infra.\nFIG. 1 -Niveaux conceptuels, d\u0027après\nAu niveau des entités Les entités constituent le premier niveau de l\u0027échelle. Elles repré-sentent les concepts et les rôles modélisés dans la ressource. Les entités s\u0027identifient à l\u0027aide des termes (étiquettes) qui sont attribués par les humains lors de la construction de la ressource. L\u0027approche d\u0027alignement étant proposée pour des ressources décrivant le même domaine, une première règle peut être définie. Elle s\u0027énonce : Le niveau des logiques de description Le troisième niveau concerne les ressources exprimées dans le formalisme des logiques de description, voir Baader et al. (2003). Dans ce cas, les concepts sont structurés dans une hiérarchie, dont la racine est un concept générique, le topConcept. Les liens hiérarchiques correspondent à des relations de généralisation/spécialisation entre concepts. Par conséquent, tout concept, excepté la racine, a des concepts pères (ascendants) et des concepts fils (descendants). Un concept de l\u0027ontologie est plus spécifique que ses concepts ascendants et plus générique que ses concepts descendants. Les règles qui peuvent être définies à ce niveau sont : Le niveau des axiomes Si des relations entre des entités ont été représentées sous la forme d\u0027axiomes, ces relations peuvent être utilisées pour estimer le degré de ressemblance entre les entités. Néanmoins, dans la pratique, de telles modélisations sont presque inexistantes.\nNous avons proposé une approche d\u0027alignement qui enrichit l\u0027ensemble de règles définies par ce cadre. Deux règles ont été ajoutées, qui seront appelées transversales, car elles font ap-pel à des éléments qui se trouvent à deux niveaux conceptuels. Ces règles s\u0027énoncent comme suit : R 8 : tout concept de la ressource S s qui n\u0027a pas été assigné par la règle R 1 à un concept de la ressource S c sera assigné au concept auquel son concept père a été assigné par la règle R 1 . Si les rôles de la ressource sont organisés dans une taxinomie, on déduit :\nR 9 : tout rôle de la ressource S s qui n\u0027a pas été assigné par la règle R 1 à un rôle de la ressource S c sera assigné au rôle auquel son rôle père a été assigné par la règle R 1 . Ces règles prennent en compte à la fois les étiquettes des entités, car elles exploitent les ré-sultats fournis par la règle R 1 et la structuration hiérarchique des entités. Elles sont utilisées si la règle R 1 ne réussit pas à associer à un concept (rôle) de S s un concept (rôle) similaire, voire identique, appartenant à S c . Dans ce cas, les règles transversales essayent d\u0027assigner un concept (rôle) de S s à un concept (rôle) de S c qui le généralise, en utilisant le concept (rôle) auquel son concept père (rôle père) a été assigné.\n3 Aligner des ressources sémantiques de l\u0027accidentologie L\u0027approche proposée a été employée pour aligner une ontologie et une ressource terminoontologique (RTO) de l\u0027accidentologie. L\u0027ontologie de l\u0027accidentologie modélise des connaissances expertes. La RTO a été construite à partir d\u0027un corpus constitué de procès verbaux (PV) d\u0027accidents de la route. Les deux ressources modélisent des connaissances propres à des communautés distinctes et ont été construites par des approches différentes. Ce paragraphe présente les ressources utilisées, la manière dont l\u0027approche proposée a été mise en oeuvre, les résultats obtenus et l\u0027évaluation de ces résultats.\n3.1 Ressources sémantiques de l\u0027accidentologie L\u0027ontologie de l\u0027accidentologie, voir Desprès (2002) modélise les connaissances expertes du domaine. Elle a été construite ex nihilo (from scratch en anglais), et est fondée sur des entretiens réalisés avec des chercheurs en sécurité routière et en utilisant comme principale ressource textuelle les scénarios d\u0027accidents. L\u0027éditeur Protégé, Noy et al. (2000) a été utilisé pour construire cette ontologie et OWL est le langage de représentation choisi. Les connaissances sont modélisées selon un point de vue systémique, les principaux concepts du domaine (l\u0027Humain, le Véhicule et l\u0027Environnement) étant mis en évidence ainsi que les relations qui les lient. Les concepts sont dénommés par des termes du domaine, (conducteur, piéton). Des attributs sont définit pour chaque concept (le concept Humain a l\u0027attribut âge), qui sont implémentés à l\u0027aide du type DataTypeProperty. Les relations entre concepts sont modélisées par des rôles à partir des verbes du domaine. Les rôles sont implémentés à l\u0027aide du type ObjectProperty et sont organisés hiérarchiquement.\nLa ressource termino-ontologique, voir Ceausu et Desprès (2005), a été construite à partir de procès verbaux (PV) d\u0027accidents de la route rédigés par les gendarmes ou les policiers. Elle décrit les accidents de la route en mettant en évidence les particularités du vocabulaire employé par les forces d\u0027ordre pour décrire les accidents de la route. L\u0027éditeur Terminae, Aussenac-Gilles et al. (2002) qui offre des facilités pour la construction des ressources sé-mantiques à partir de textes et leur gestion a été choisi pour structurer les connaissances. Un modèle du domaine a été élaboré utilisant deux types d\u0027entités : les concepts et les rôles. Les concepts sont structurés dans une hiérarchie. Un concept est dénommé par un terme du domaine, et il ne possède pas d\u0027attributs. Les concepts sont liés par des rôles créés à partir de verbes du domaine. La RTO est représentée en OWL. Le nombre de concepts et de rôles de la RTO construite à partir des PV est plus important que celui de l\u0027ontologie. L\u0027explication tient au fait que la RTO est construite à partir de textes rédi-gés en langage courant par des communautés de personne différentes dont l\u0027objectif n\u0027est pas la rigueur dans la présentation mais la description de l\u0027accident dans lequel ils sont impliqués. Tandis que les chercheurs utilisent un langage de spécialité et se contraignent à la concision pour écrire leurs textes.\nSélection des règles pour aligner les ressources\nPour aligner les deux ressources, des règles d\u0027appariement introduites dans le paragraphe 2.2 sont utilisées. L\u0027alignement réalisé a comme ressource source la RTO construite à partir de PV et comme ressource cible l\u0027ontologie de l\u0027accidentologie. Chaque concept (respectivement rôle) appartenant à la RTO est assigné à un concept (respectivement rôle) de l\u0027ontologie. Orienté de cette manière, l\u0027alignement permet de mettre en évidence la manière dont les connaissances expertes modélisées dans l\u0027ontologie de l\u0027accidentologie sont exprimées dans le langage commun employé par les forces d\u0027ordre. Le choix des règles utilisées est guidé par les particularités des deux ressources et par le sens de l\u0027alignement. Les deux ressources modélisent des entités (concept et rôles) du domaine, par conséquent il est possible d\u0027utiliser les règles définies au premier niveau. Ces règles prennent en compte les étiquettes des entités pour estimer leur degré de ressemblance. Les deux ressources adoptent le formalisme des logiques de description et sont représentées en OWL. Cependant, les règles définies au niveaux 2 (réseau sémantique) ne sont pas utilisées, car utilisées conjointement elles engendrent des calculs circulaires. Les rôles modélisés par la RTO ne sont pas structurés hiérarchiquement, par conséquent les règles définies au niveau de la logique de description ne sont pas appliquées. Les règles des deux derniers niveaux sont ignorées, car les ressources ne font pas appel à des directives du langage OWL tels que OWL :sameClassAs et elles ne contient pas d\u0027axiomes. Le nombre d\u0027entités modélisées par la RTO étant plus important que le nombre d\u0027entités de l\u0027ontologie, les règles transversales (R 8 et R 9 ) sont choisies pour mettre en oeuvre l\u0027alignement. L\u0027ensemble des règles utilisé pour aligner les deux ressources est constitué de R 1 , R 8 et R 9 .Ces règles sont utilisées par deux algorithmes développés pour aligner les concepts, respectivement les rôles des deux ressources, qui sont présentés dans le paragraphe suivant.\nAligner les concepts des ressources L\u0027alignement des concepts est fondé sur la règle R 1 et R 8 . L\u0027application de la première (R 1 ) identifie pour chaque concept de la RTO un concept de l\u0027ontologie ayant la même étiquette. Un coefficient égal à 1 sera assigné à chaque couple de concepts. La règle R 8 s\u0027applique successivement en prenant en compte les assignations ainsi obtenues. Chaque application de cette règle entraîne une diminution de la valeur du coefficient qui caractérise le degré de similarité des entités. Un concept modélisé dans la RTO sera assigné à un concept de l\u0027ontologie d\u0027autant plus général que le nombre d\u0027applications de la règle R 8 est important. L\u0027alignement est terminé si tout concept de la RTO est assigné à un concept de l\u0027ontologie. Un extrait des résultats obtenus en alignant les concepts est présenté dans le tab. 3.\nAligner les rôles des ressources Les rôles sont structurés hiérarchiquement dans l\u0027ontologie de l\u0027accidentologie, mais ils se trouvent au même niveau dans la RTO. Par conséquent, les règles d\u0027alignement fondées sur la hiérarchie ne peuvent pas être appliquées. Pour pallier cet inconvénient, la règle R 1 a été adaptée. Ainsi, elle est implémentée en utilisant comme mesure de similarité lexicale la mesure de Monge-Elkan, voir par exemple Ceausu et Desprès (2006). Cette mesure fait des comparaisons récursives au niveau des sous-chaînes et fournit la valeur maximale si la chaîne s 1 est une sous-chaîne de la chaîne s 2 . La règle devient : les rôles dénommés par des étiquettes dont la similarité (calculée par le coefficient MongeElkan) est supérieure à une valeur seuil donnée seront considérés similaires (voir identiques). Les valeurs calculées par Monge-Elkan représentent les coefficients caractérisant le degré de ressemblance des rôles. Le tab. 4 montre un extrait des résultats obtenus en alignant les rôles des deux ressources. \nConcept\nEvaluation des résultats\nLes résultats de l\u0027alignement de chaque type d\u0027entité sont évalués indépendamment. Cette évaluation distincte est nécessaire car les concepts et les rôles sont structurés différemment et les algorithmes utilisés pour les aligner sont distincts. Les résultats de l\u0027alignement ne peuvent pas être évalués globalement car ils sont issus de deux approches différentes. L\u0027évaluation réalisée concerne seulement l\u0027étude de cas présentée, car la qualité des résultats de l\u0027alignement est influencée par la complexité des ressources alignées. Le scénario d\u0027éva-luation proposé compare, pour chaque type d\u0027entité, les résultats fournis par l\u0027alignement des ressources avec des résultats obtenus en établissant, manuellement, des correspondances entre des entités. L\u0027évaluation fait appel à des mesures proposées dans le domaine de la recherche de l\u0027information, qui reposent sur les notions classiques de Rappel et de P recision. Ces mesures ont été adaptées au nouveau contexte de l\u0027alignement des ressources sémantiques. Ainsi, on peut définir le Rappel comme suit :\noù N oCorrects représente le nombre d\u0027alignements corrects et N oRef représente le nombre d\u0027alignements de référence. La P recision est définie par :\noù N oCorrects représente le nombre d\u0027alignements corrects et N oRef représente le nombre d\u0027alignements proposés. L\u0027évaluation des résultats peut aussi être exprimée en terme de Bruit ou Silence, comme suit :\nF-measure est une mesure d\u0027efficacité globale qui combine Precision et Rappel en une mesure unique donnée par :\nOn suppose qu\u0027un alignement de référence a été établit manuellement, et que c\u0027est par rapport à cet alignement que seront évalués les résultats obtenus. La comparaison par rapport à l\u0027alignement de référence sera effectuée en comparant les couples de concepts, respectivement rôles, engendrés par l\u0027alignement. La valeur du coefficient qui exprime le degré de ressemblance est ignorée lors de l\u0027évaluation. Ainsi, un alignement (e s , e c , coef f icient) sera considéré correct si, dans le set de référence il existe un couple (e s , e c ), quelque soit la valeur du coefficient coef f icient. Les valeurs obtenues sont présentées dans le tab. \nTAB. 5 -Evaluation des résultats\npeut observer que l\u0027algorithme utilisé pour aligner les concepts est plus performant. Cela s\u0027explique par la structuration hiérarchique des concepts dans les deux ressources, qui fait possible l\u0027utilisation de la règle transversale R 8 . Par conséquent, l\u0027alignement des concepts est réalisé par un algorithme combinant deux règles d\u0027appariement. Malgré l\u0027adaptation de la règle R 1 (qui utilise une mesure lexicale particulière pour apparier des rôles), l\u0027algorithme proposé pour aligner les rôles est moins performant. D\u0027un point de vue pratique, un nombre important de rôles de la ressource source (la RTO construite à partir de PV) sont assignés au rôle générique de la ressource cible (l\u0027ontologie de l\u0027accidentologie).\nTravaux connexes\nUn recueil des méthodes et des outils permettant la mise en correspondance de sémantiques est réalisé dans : Kalfoglou et Schorlemmer (2003) et Euzenat (2004. Des comparaisons entre les outils sont présentées dans Do et al. (2002), Rahm et Bernstein (2001). Parmi les outils proposés, on retrouve :\nAnchor Prompt , Fridman Noy et Musen (2001) est un outil permettant l\u0027alignement et l\u0027intégration des ontologies. Il reçoit en entrée deux ontologies et une liste de paires de termes. Les paires de termes sont fournies par l\u0027utilisateur ou identifiées en utilisant des métriques lexicales. Un processus semi-automatique permet d\u0027identifier des concepts qui sont similaires en utilisant ces paires de termes, les structures des ontologies et les choix de l\u0027utilisateur.\nChimaera , McGuinness et al. (2000) est un outil qui permet d\u0027aligner des ontologies de grande taille. Un algorithme engendre des paires de concepts similaires en comparant : les termes utilisés pour designer les concepts, les définitions des concepts, et, selon le cas, les acronymes ou les expansions de ces noms. Chimaera est aussi capable d\u0027identifier des concepts qui sont corrélés par d\u0027autres types de relations, telles que la subsomption, ou des termes qui sont disjoints.\nCupid, Madhavan et al. (2000) est un système qui implémente un algorithme d\u0027alignement fondé sur les similarités lexicales et structurelles. Des coefficients de similarité sont calculés et trois étapes sont exécutées pour générer des paires de concepts similaires. Une première étape calcule des similarités au niveau lexical en utilisant les noms des entités, des mesures de similarité lexicale et en faisant appel à un thesaurus ; la deuxième étape estime la similarité d\u0027un point de vue structurel, en considérant les contextes d\u0027apparition des concepts dans les ontologies. La dernière phase engendre les paires des concepts similaires, en choisissant, parmi les paires générées, celles ayant un coefficient de similarité supérieur à une valeur seuil donnée.\nAsco est un système développé à INRIA Sophia Antipolis, qui peut identifier des correspondances entre : deux concepts appartenant à deux ontologies distinctes ; deux relations modéli-sées dans deux ontologies distinctes ; un concept et une relation appartennant à deux ontologies distinctes. Asco implémente un algorithme qui met en correspondance les entités en exploitant le maximum d\u0027éléments disponibles : les noms des entités ; les structures des ontologies ; les structures des entités,des concepts ou des rôles. Cet algorithme est implémenté en Java et est fondé sur le moteur de recherche sémantique CORESE, décrit dans Corby et Faron (2002).\nDes méthodes ont été proposées qui estiment la similarité en prenant en compte les instances des concepts, voir par exemple les systèmes Glue, Doan et al. (2002) et FCA Merge, Stumme et Maedche (2002) ou les axiomes présentes dans une ontologie, voir Furst (2002).\nL\u0027approche que nous avons proposée pour aligner deux ressources sémantiques est fondée sur les travaux de Ehrig et Sure (2004). Ce choix a été guidé par les particularités des ressources à aligner qui sont constituées de concepts et de rôles. Elles ne contiennent pas d\u0027axiomes ou des individus, par conséquent les méthodes faisant appel à ces éléments ne peuvent pas être appliquées. L\u0027alignement doit mettre en évidence des similarités entre les concepts et les rôles des ressources. Des outils tels que Anchor Prompt, Chimaera ou Cupid s\u0027avèrent inappropriés, car ils établissent des correspondances seulement entre les concepts des ressources. Asco est un outil récent qui n\u0027a pas pu être considéré dans le cadre de ce travail. L\u0027approche proposée par Ehrig et Sure (2004) à l\u0027avantage de mettre en correspondance les concepts et les rôles appartenant à deux ressources. Nous avons adaptée cette approche en enrichissant l\u0027ensemble de règles d\u0027appariement utilisées.\nConclusion\nCe papier présente une approche pour aligner deux ressources sémantiques. L\u0027alignement est fondé sur des règles d\u0027appariement entre les entités des deux ressources et se traduit par des correspondances entre ces entités. Une première étude a été réalisée qui emploie cette approche pour aligner une ontologie et une ressource termino-ontologique de l\u0027accidentologie. L\u0027évaluation des résultats issus de cette expérimentation montre qu\u0027elle est sensible à la manière dont les entités sont modélisées. En perspective, la méthode d\u0027alignement des ressources peut être améliorée en enrichissant l\u0027ensemble de règles utilisées. De nouveaux algorithmes faisant appel à ces règles peuvent être proposés. Une méthode d\u0027évaluation prenant en compte à la fois les couples d\u0027entités obtenus ainsi que les coefficient caractérisant leur degré de similarité est également envisageable.\nRéférences\nAussenac-Gilles, N., B. Biébow, et S. Szulman (2002) \n"
  },
  {
    "id": "904",
    "text": "Introduction\nLa fouille archéologique est un processus technique visant à recueillir toutes les informations pertinentes sur les manifestations présentes dans un site archéologique [1]. Le processus de fouille d\u0027un site archéologique passe par les étapes suivantes \nAnnotation XML des objets archéologiques\nLes données archéologiques sont décrites en utilisant les informations recueillies et enregistrées. Les informations de description concernent : les aspects matériaux, le contexte de fouille et la sémantique des oeuvres (c.-à-d. ce que les objets représentent). Un standard de description appelé \"CIDOC-ICOM\" est développé par le groupe de travail CIDOC-IDOC [3] sur les sites archéologiques. Ce dernier définit les catégories minimales d\u0027informations à enregistrer sur des objets archéologiques afin d\u0027en faciliter la recherche dans un cadre international. Notre modélisation des oeuvres archéologiques s\u0027appuie sur ce standard avec une structure XML qui permette de générer des associations de façon dynamique.\nL\u0027architecture générale de notre application «musée virtuel TARCHNA» est illustrée dans la Figure 1. Les différents composants qui constituent notre application sont : le moteur de présentation, le gestionnaire de profiles utilisateurs et le processeur de sémantique.\n• Le moteur de présentation (\"Presentation Engine\") : il gère la présentation. Son but est de supporter un maximum de technologies clients : support des différents navigateurs, type d\u0027interfaces tout en assurant l\u0027adaptation des structures de données renvoyées au client.  \nSummary:\nIn this paper, we propose an annotation framework and a navigation tool of archaeological data. Our goal is to structure the annotation to allow incremental navigation. By using the answer set of a query, users can discover approximate links with other objects in the database. This approach has been implemented and is in the process of validation.\n"
  },
  {
    "id": "905",
    "text": "Introduction\nDans le monde scientifique, de nombreuses données sont produites en continu : il est difficile de se maintenir à jour avec le flot d\u0027informations, et de synthétiser les données venant de sources diverses au moment où on en a besoin. Notre but est la construction d\u0027un entrepôt de données XML sur un domaine d\u0027application précis, où différentes données collectées sur le Web seront annotées avec une ontologie du domaine, de manière à être facilement interrogeables. Notre travail se concentre sur l\u0027annotation des tableaux de données, qui sont un moyen de présenter l\u0027information de façon synthétique, très utilisé dans les domaines scientifiques et économiques.\nLa structure des tableaux de données que l\u0027on trouve dans les rapports et publications scientifiques collectés sur le Web est très hétérogène : elle varie d\u0027un auteur à l\u0027autre, et on observe même souvent différentes formes de tableaux dans un même article scientifique. De plus, le fait que l\u0027on s\u0027intéresse à des tableaux nous prive de l\u0027utilisation d\u0027un contexte linguistique : les techniques de wrapper induction basées sur la structure (Baumgartner et al., 2001) ou le contexte linguistique (Freitag et Kushmerick, 2000) ne sont donc pas adaptées à notre problème d\u0027annotation. Notre but est de construire un outil d\u0027annotation sans phase d\u0027apprentissage, reposant uniquement sur une ontologie. Nous ne cherchons pas, comme présenté par Pivk et al. (2004), à découvrir des relations à partir de tableaux de données et d\u0027outils linguis-tiques généraux tels que WordNet et GoogleSets, mais nous voulons au contraire reconnaître des relations prédéfinies dans une ontologie spécifique au domaine d\u0027application.\nNotre approche utilise les idées développées par Gagliardi et al. (2005) concernant l\u0027annotation de tableaux guidée par une ontologie, sur la base d\u0027égalités de mots entre les termes de l\u0027ontologie et ceux du Web. Cependant, nous allons plus loin dans le sens où nous distinguons deux méthodes de traitement selon que les données sont numériques ou symboliques, et que nous proposons une annotation floue pour les données symboliques.\nLa section 2 décrit l\u0027ontologie, élément central de notre système. Nous présentons ensuite notre système d\u0027annotation dans l\u0027ordre d\u0027application des différentes étapes : distinction entre données numériques et symboliques en section 3, annotation des données symboliques en section 4 et annotation des données numériques en section 5. Chaque étape de ce travail est validée expérimentalement.\nL\u0027ontologie dans le système MIEL++\nLe domaine d\u0027application de notre entrepôt de données est défini dans une ontologie, et tout notre système est guidé par cette ontologie : pour changer de domaine d\u0027application, il suffit de changer d\u0027ontologie.\nLe travail présenté ici est appliqué au domaine de la microbiologie alimentaire, et l\u0027entrepôt de données construit s\u0027intègre dans un système existant appelé MIEL 1 (Buche et al., 2005). Dans le système MIEL, les données de microbiologie alimentaire sont entrées manuellement dans une base de données relationnelle, et les utilisateurs interrogent cette base via une interface de requêtes où ils sélectionnent dans l\u0027ontologie les microorganismes, produits alimentaires et facteurs expérimentaux qui les intéressent, avec la possibilité de définir des pré-férences : la base de données est interrogée selon ces critères, qui sont cependant élargis pour ramener plus de données, et les résultats sont ordonnés suivant leur proximité avec la requête de l\u0027utilisateur. Dans le système MIEL++ (MIEL élargi à l\u0027entrepôt de données XML, voir Buche et al., 2006), on souhaite conserver le même mode d\u0027interrogation, la base de données et l\u0027entrepôt étant interrogés simultanément, de façon transparente pour l\u0027utilisateur. Pour cela, les données de l\u0027entrepôt XML doivent être annotées avec la même ontologie que celle déjà utilisée dans le système MIEL.\nL\u0027ontologie utilisée dans le système MIEL++ décrit les relations sémantiques intéressantes pour le domaine de la microbiologie alimentaire, et les types de données impliqués dans ces relations. Par exemple, la relation Growth kinetics est composée des types Food product, Microorganism, Temperature, Time, Colony count. Les types sont décrits dans l\u0027ontologie de deux manières, suivant qu\u0027ils sont symboliques (Food product, Microorganism) ou numériques (Temperature, Time, Colony count). Les types symboliques sont décrits par une taxonomie des valeurs possibles (par exemple, taxonomie des microorganismes). Les valeurs qui peuvent être prises par un type symbolique sont appelées termes. Les types numériques sont décrits par les unités dans lesquelles on peut les exprimer (par exemple,\n• C ou • F pour Temperature) ainsi que, le cas échéant, par un intervalle de valeurs possibles (par exemple un pH est compris entre 0 et 14). Cette ontologie, construite manuellement lors de la création du système MIEL, est représentée dans un format spécifique. Nous étudions la possibilité de la représenter dans un des langages standards du web sémantique.\nDistinction entre colonnes numériques et symboliques\nOn suppose dans cette section qu\u0027un prétraitement permet de mettre les tableaux issus du Web dans un format standard, avec des entêtes de colonnes, puis des lignes composées d\u0027un ensemble de cellules : chaque ligne est une instance de la relation sémantique présentée par le tableau. Notre objectif est de reconnaître le type des colonnes du tableau, pour en déduire la relation sémantique représentée par le tableau. Un traitement différent est appliqué suivant qu\u0027une colonne contient des données numériques ou symboliques : notre premier travail est donc de faire la distinction entre les colonnes numériques et symboliques.\nClassification par règles des colonnes numériques et symboliques\nFaire la différence entre des colonnes numériques et symboliques dans un tableau n\u0027est pas si simple qu\u0027il y paraît, surtout dans le domaine de la microbiologie alimentaire où de nombreuses données symboliques comportent des chiffres (par exemple la souche de microorganisme \"E. coli O 157 : H7\") alors que les données numériques comportent souvent des mots (unités, précision d\u0027un intervalle de confiance...). La solution que nous proposons pour distinguer les colonnes numériques des colonnes symboliques tient compte des unités définies dans l\u0027ontologie pour les types numériques.\nTout d\u0027abord, les nombres sont reconnus selon l\u0027expression régulière (digit)+((\u0027,\u0027|\u0027.\u0027)(digit)+) * , avec digit correspondant à l\u0027un des dix chiffres (\u00270\u0027|\u00271\u0027|...|\u00279\u0027). Les nombres en notation scientifique sont reconnus suivant l\u0027expression régulière digit \u0027.\u0027 (digit)+ \u0027x 10 \u0027 (digit)+. On recherche égale-ment dans le tableau les occurrences des unités définies dans l\u0027ontologie, et des indicateurs de résultat absent, qui sont des chaînes de caractères prédéfinies (par exemple, \"No result\", ou \"NS\" pour Not Specified \nRésultats expérimentaux\nNotre méthode de classification a été expérimentée sur 60 tableaux intéressants pour le domaine de la microbiologie alimentaire. Une classe symbolique ou numérique a été manuellement assignée à chacune des colonnes de ces tableaux, résultant en 264 colonnes numériques et 85 colonnes symboliques. Les résultats de notre classification ont été comparés à ceux d\u0027un classifieur « naïf », dans lequel toute cellule contenant un chiffre est considérée comme numé-rique et une colonne est considérée comme numérique si plus de la moitié de ses cellules sont numériques. Les résultats de cette classification sont donnés dans le tableau 1.\nclassification utilisant les unités classification naïve P P P P P P P P La précision globale de la classification (proportion de colonnes bien classifiées par rapport au nombre total de colonnes classifiées) atteint 98% pour la classification utilisant les unités définies dans l\u0027ontologie, contre 86% pour le classifieur naïf. On voit ici à quel point il est intéressant d\u0027utiliser l\u0027ontologie dès ce stade de l\u0027annotation.\nAnnotation des données symboliques\nLorsqu\u0027on a affaire à une colonne de données symboliques, on cherche d\u0027une part à annoter le contenu de chaque cellule avec les termes de l\u0027ontologie, et d\u0027autre part à reconnaître le type de la colonne. La première étape consiste en l\u0027annotation du contenu des cellules par les termes de l\u0027ontologie qui en sont lexicalement les plus proches, comme présenté en section 4.1. En deuxième étape, les résultats de cette annotation sont utilisés pour déduire le type de la colonne, comme présenté en section 4.2. Enfin, l\u0027annotation des cellules obtenue en première étape est modifiée afin de ne conserver que les termes correspondant bien au type trouvé pour la colonne.\nAnnotation des cellules au sein d\u0027une colonne de valeurs symboliques\nDans notre système, le contenu d\u0027une cellule symbolique, ci-après appelé « terme du Web », est annoté non pas uniquement avec un terme de l\u0027ontologie, mais avec plusieurs termes possibles. Contrairement aux travaux de Gagliardi et al. (2005), les différents termes de l\u0027ontologie proposés pour l\u0027annotation n\u0027ont pas tous la même importance, mais sont ordonnés selon leur similarité avec le terme du Web. Nous utilisons pour représenter notre annotation le modèle des sous-ensembles flous.\nLes sous-ensembles flous\nLe système MIEL, que nous souhaitons étendre pour l\u0027interrogation des données annotées, utilise en effet le formalisme des sous-ensembles flous (Zadeh, 1965(Zadeh, , 1978 pour l\u0027expression des requêtes. Nous utilisons ce même formalisme pour représenter nos annotations.\nLa notion de sous-ensemble flou est un assouplissement de la notion de sous-ensemble classique d\u0027un ensemble de référence X. Dans le cas classique, les éléments de X qui possèdent une certaine propriété constituent un sous-ensemble A de X, les éléments de X qui ne possèdent pas cette propriété appartiennent au complémentaire de A dans X. Dans le cas d\u0027un sous-ensemble flou, les éléments peuvent appartenir partiellement à un sous-ensemble, avec un degré d\u0027appartenance compris entre 0 (élément n\u0027appartenant pas au sous-ensemble) et 1 (élément appartenant totalement au sous-ensemble). Définition. Un sous-ensemble flou A d\u0027un ensemble de référence X est défini par une fonction d\u0027appartenance µ A de X dans [0, 1] qui associe à chaque élément x de X le degré µ A (x) avec lequel x appartient à A.\nFIG. 1 -Exemple de sous-ensemble flou sur un ensemble de définition à valeurs symboliques.\nDans notre système d\u0027annotation, nous utilisons les sous-ensembles flous pour décrire la similarité d\u0027un terme du Web avec différents termes de l\u0027ontologie. L\u0027ensemble de définition du sous-ensemble flou est l\u0027ensemble de tous les termes de l\u0027ontologie, la fonction d\u0027appartenance est une mesure de similarité entre le terme du Web et chacun des termes de l\u0027ontologie. Par exemple, la figure 1 représente l\u0027annotation du terme \"minced beef\" trouvé sur le Web. Ce terme n\u0027existe pas tel quel dans l\u0027ontologie, mais est similaire à divers termes de l\u0027ontologie : \"ground beef\", et dans une moindre mesure \"minced meat\" ou \"minced poultry\". Les termes de l\u0027ontologie dont le degré de similarité avec \"minced beef\" est nul ne sont pas représentés.\nDegré de similarité d\u0027un terme du Web avec un terme de l\u0027ontologie\nIl nous faut maintenant définir quelle mesure de similarité nous utilisons comme fonction d\u0027appartenance pour nos sous-ensembles flous servant à l\u0027annotation des termes du Web. Différentes mesures de similarité sémantique entre deux termes ont été présentées par Lin (1998);Resnik (1999); Seco et al. (2004) : ces mesures ont en commun qu\u0027elles nécessitent l\u0027utilisation d\u0027une ontologie tierce comprenant les deux termes à comparer. Dans notre cas, une telle ontologie n\u0027existe pas. Des essais que nous avons menés avec WordNet ont montré que cette ontologie était trop généraliste pour comprendre tous les noms d\u0027aliments trouvés dans les publications issues du Web ; le thésaurus AgroVoc, utilisé par la FAO 2 et spécialisé dans l\u0027agriculture et l\u0027agro-alimentaire, ne contient pas lui non plus les noms d\u0027aliments ré-pertoriés dans les publications scientifiques en microbiologie alimentaire que nous cherchons à exploiter. Une mesure de similarité lexicale fondée sur les n-grammes est présentée par Lin (1998), mais cette mesure cherche à retrouver des mots de même racine plutôt que de même signification.\nNous proposons une mesure de similarité lexicale entre deux termes, fondée sur des égali-tés de mots. Chaque terme de l\u0027ontologie, ainsi que chaque terme du Web, est décomposé en un ensemble de mots, qui sont lemmatisés (par exemple, \"carrot cuts\" et \"cut carrots\" donnent tous deux le même ensemble {carrot, cut}). Tous les mots d\u0027un terme n\u0027ont pas la même importance dans la signification du terme, et ce de façon différente suivant le domaine d\u0027application : dans le terme \"minced poultry\", c\u0027est \"minced\" qui est le plus important si on se concentre sur les procédés alimentaires, tandis que c\u0027est \"poultry\" si on s\u0027intéresse plutôt à l\u0027origine des produits. La distinction entre mots importants et moins importants est donc une affaire d\u0027experts, qui ajoute de la connaissance sur un domaine. Dans l\u0027ontologie, chaque mot de chaque terme se voit attribuer manuellement un poids entre 0 et 1 correspondant à son importance dans la signification du terme. Pour simplifier le travail d\u0027attribution de poids dans les termes de l\u0027ontologie, on conserve trois niveaux de poids :\n-poids de 0 pour les mots n\u0027apportant pas de sens, tels qu\u0027articles et conjonctions, listés dans une stop-list ; -poids de 1 pour les mots d\u0027importance majeure pour la signification du terme ; -poids de 0, 2 pour les mots d\u0027importance moyenne (la valeur 0, 2 a été définie lors d\u0027expériences préliminaires qui ont montré une annotation de meilleure qualité avec ce poids qu\u0027avec le poids intermédiaire de 0, 5). Comme on ne dispose pas de connaissances d\u0027expert pour les termes du Web, tous les mots des termes du Web se voient attribuer un poids de 1 (sauf les mots de la stop-list qui gardent un poids de 0).\nLors de l\u0027annotation d\u0027un terme du Web, tous les termes de l\u0027ontologie et celui du Web sont représentés comme des vecteurs, dont les coordonnées représentent l\u0027ensemble de tous les mots lemmatisés possibles (i.e. tous les mots présents dans l\u0027ontologie et les mots du terme du Web), les valeurs de ces coordonnées correspondant au poids du mot dans le terme, ou 0 si le mot n\u0027est pas présent dans le terme. Un exemple de représentation vectorielle de termes est donné dans le tableau 2. Dans cet exemple, nous ne montrons que les termes de l\u0027ontologie ayant au moins un mot en commun avec le terme du Web. Une fois les termes représentés en tant que vecteurs, on définit la similarité entre un terme du Web et un terme de l\u0027ontologie comme la mesure de similarité par cosinus (Van Rijsbergen, 1979) entre les deux vecteurs. Cette mesure a été choisie car c\u0027est l\u0027une des plus répandues pour la comparaison de vecteurs pondérés, et qu\u0027une comparaison expérimentale avec d\u0027autres mesures ne nous a pas apporté de meilleurs résulats.\nDéfinition. La similarité entre un terme w du Web et un terme o de l\u0027ontologie, représentés comme des vecteurs pondérés w \u003d {w 1 , ..., w n } et o \u003d {o 1 , ..., o n } est définie par la formule suivante :\nExemple. Selon les poids donnés dans le tableau 2, on calcule les degrés de similarité utilisés dans la figure 1 :\nRésultats expérimentaux\nLa validation de notre méthode d\u0027annotation floue des termes du Web a été faite sur la partie aliments de l\u0027ontologie : 185 termes distincts ont été manuellement annotés par leur terme le plus proche (ci-après appelé best match) dans la taxonomie des aliments. Pour valider la gé-néralité de notre approche, nous avons également fait des tests d\u0027annotation avec la taxonomie du Codex Alimentarius (taxonomie d\u0027aliments utilisée par l\u0027OMS 3 ). Les deux taxonomies ont été retravaillées pour donner des poids aux mots de tous les termes. Chaque terme du Web a été annoté selon la méthode présentée en section 4.1.2 dans chacune des deux taxonomies, une fois avec les poids des mots définis manuellement, une fois avec des poids de mots de 1, comme si tous les mots avaient la même importance (mis à part les mots de la stop-list qui conservent un poids de 0). Les termes de la taxonomie proposés pour l\u0027annotation d\u0027un terme du Web sont ordonnés selon leur degré de similarité avec le terme du Web, et l\u0027on regarde en quelle position se trouve le best match. Cette position est évaluée « au pire », c\u0027est à dire que s\u0027il y a plusieurs termes ayant le même score de similarité avec le terme du Web, le best match est considéré comme étant en dernière position. Cette méthode d\u0027évaluation est due à notre souhait de proposer une annotation semi-automatique, où une liste des n meilleurs termes sera proposée à un utilisateur pour l\u0027annotation du terme du Web : si plusieurs termes ont le même score, on ne maîtrise pas si le « bon » sera affiché dans la liste ou non. Les résultats de l\u0027annotation sont présentés dans le tableau 3.\ntaxonomie Codex Alimentarius aliments dans MIEL++ termes dont le best match a un score non nul 60% 78%\nh TAB. 3 -Résultats de l\u0027annotation de 185 noms d\u0027aliments.\nh h h h h h h h h h h h h h h\nTout d\u0027abord, on s\u0027aperçoit que les résultats d\u0027annotation sont meilleurs avec l\u0027ontologie de MIEL++ qu\u0027avec le Codex Alimentarius. Ceci s\u0027explique par le fait que l\u0027ontologie de MIEL++ a été construite spécialement pour le domaine de la microbiologie alimentaire, contenant notamment des noms d\u0027aliments transformés, alors que le Codex Alimentarius a été construit à d\u0027autres fins et est essentiellement tourné vers les matières premières (par exemple, on n\u0027y trouve pas \"butter\" mais \"cow milk fat\", \"goat milk fat\",...). Cela plaide en faveur de l\u0027utilisation d\u0027ontologies de domaine vraiment adaptées au centre d\u0027intérêt applicatif.\nPour qu\u0027un terme de l\u0027ontologie ait un score de similarité non nul avec un terme du Web, il faut et il suffit que ces termes aient un mot en commun : utiliser une méthode fondée sur l\u0027égalité de mots n\u0027est pas dénuée de sens, puisque 78% des best match ont effectivement un mot commun avec le terme du Web dans le cas de l\u0027ontologie de MIEL++. L\u0027utilisation de poids sur les mots dans les taxonomies ne modifie pas les termes qui seront retenus pour l\u0027annotation, mais modifie l\u0027ordre dans lequel ils sont présentés : on voit que l\u0027utilisation de poids apporte une amélioration, faible mais systématique. En revanche, que l\u0027on utilise ou non les poids sur les mots, l\u0027utilisation d\u0027une annotation floue avec termes ordonnés selon un degré de similarité est un gain important, puisqu\u0027il y a en moyenne 16 termes de la taxonomie des aliments de MIEL++ ayant au moins un mot commun avec le terme du Web (avec un maximum à 94 termes pour le terme du Web \"raw milk cheese\") : une présentation non ordonnée de tous les termes possibles pour l\u0027annotation est donc à proscrire. En utilisant le score de similarité, on arrive à obtenir 66% des best match dans les 5 premières positions pour l\u0027ontologie de MIEL++, ce qui est bien plus intéressant dans le cadre d\u0027une annotation semi-automatique, ou pour une interrogation où les résultats sont ordonnés selon leur similarité à la requête.\nDétermination du type d\u0027une colonne de valeurs symboliques\nUne fois les termes des cellules d\u0027une colonne annotés en utilisant le degré de similarité avec les termes de l\u0027ontologie qu\u0027on vient de présenter, on peut déterminer le type de la colonne symbolique.\nUtilisation des degrés de similarité avec les termes de l\u0027ontologie\nOn détermine le type de chaque terme de la colonne d\u0027après ses scores de similarité avec les différents termes de l\u0027ontologie. Les types de tous les termes d\u0027une colonne sont ensuite utilisés pour déterminer le type de la colonne. Soit col une colonne d\u0027un tableau, type un type symbolique défini dans l\u0027ontologie. Soit T type l\u0027ensemble de tous les termes de l\u0027ontologie appartenant au type type. Soit t ext le terme du Web contenu dans une cellule de la colonne col. Alors le score du type type pour le terme t ext est le suivant :\nt?Ttype Pour chaque terme de la colonne, on calcule le score de chaque type de l\u0027ontologie. Soit bestT ype(t ext ) le type qui a le meilleur score pour le terme t ext . Si ce score est supérieur à un seuil ? défini par l\u0027utilisateur, alors le terme t ext est considéré comme étant du type bestT ype(t ext ). Si par contre score(t ext , bestT ype(t ext )) \u003c ?, alors t ext est considéré comme de type inconnu. Le score du type type pour la colonne col est la proportion de termes de la colonne ayant le type type. Soit T col l\u0027ensemble de tous les termes de la colonne et T type col l\u0027ensemble de tous les termes de la colonne ayant le type type, alors\nConsidérons le type bestT ype(col) qui a le meilleur score pour la colonne col. Si ce score est supérieur à un seuil ? défini par l\u0027utilisateur, avec ? ? [0, 1], alors la colonne est classifiée comme étant du type type. Sinon le type de la colonne est considéré comme non reconnu.\nLorsque le type de la colonne est reconnu, on restreint le domaine de définition des sousensembles flous servant à l\u0027annotation des termes au sein de la colonne : le nouveau domaine de définition est l\u0027ensemble de tous les termes de l\u0027ontologie correspondant au type de la colonne.\nRésultats expérimentaux\nLes 80 colonnes ayant bien été reconnues comme symboliques lors de la classification numérique/symbolique (section 3.2) ont été utilisées pour cette expérience. Les colonnes ont été manuellement classées en trois types : aliment(46 colonnes), microorganisme(16 colonnes) et autre(18 colonnes). Tous les termes de ces colonnes ont ensuite été automatiquement annotés avec les termes de l\u0027ontologie correspondant aux types aliment et microorganisme. Les types des colonnes ont été calculés selon la méthode présentée ci-dessus, avec pour paramètres ? \u003d 0, 2 et ? \u003d 0, 5, les colonnes de type non reconnu étant classées comme de type autre.\nLa qualité de cette classification a été comparée avec une classification automatique par apprentissage : on a utilisé la méthode de classification SMO, une optimisation des SVM (voir Platt, 1999), implémentée dans Weka 4 , en conservant les paramètres par défaut, les colonnes étant transformées en vecteurs pondérés de tous les mots qu\u0027elles contiennent. La classification par SMO a été évaluée en validation croisée par leave one out (chaque colonne est classifiée en utilisant un classifieur entraîné avec l\u0027ensemble des 79 autres colonnes). Les résultats obtenus sont présentés dans le tableau 4. utilisation de l\u0027ontologie SMO P P P P P P P P Avec notre méthode sans apprentissage utilisant l\u0027ontologie, on obtient une précision de 94% et une couverture de 74% pour les aliments : la classification par apprentissage suivant la méthode SMO donne certes une couverture de 100%, mais avec une précision plus faible à 81%. Pour les microorganismes, notre méthode donne une précision de 100% et une couverture de 75%, alors que SMO permet aussi une précision de 100% mais avec une couverture plus basse (69%). Notre méthode donne donc des résultats tout à fait comparables (voire meilleurs dans le cadre de notre application où l\u0027on cherche avant tout une bonne précision) aux mé-thodes classiques de classification par apprentissage. L\u0027avantage de notre méthode est qu\u0027elle ne nécessite pas de phase d\u0027apprentissage, en utilisant une ontologie déjà existante. Nous avons également testé la sensibilité de notre méthode au choix des paramètres. La sensibilité pour ? est faible : on obtient les résultats présentés dans le tableau 4 pour tout ? entre 0, 01 et 0.4 ; cependant pour des valeurs de ? plus élevées, on perd en couverture plus vite qu\u0027on ne gagne en précision. On atteint une précision de 100% pour les aliments et microorganismes avec ? \u003d 1 : on a alors une couverture de 65% pour les aliments et 69% pour les microorganismes. Notre méthode est un peu plus sensible pour le choix du paramètre ? : plus ? est grand, plus la précision est grande, avec une moindre couverture. Cependant les variations ne sont que de quelques points par tranche de 0, 1 ajoutée ou enlevée à ?.\nAnnotation des données numériques\nDe même que nous avons recherché le type des colonnes symboliques, nous recherchons le type de colonnes numériques afin de pouvoir ultérieurement déterminer la signature de la relation représentée dans chaque tableau.\nReconnaissance du type d\u0027une colonne numérique\nAfin de déterminer le type d\u0027une colonne numérique, on combine deux scores : le score de similarité du titre de la colonne avec les noms des différents types numériques, et un score déduit des unités utilisées dans la colonne.\nOn considère tout d\u0027abord le titre de la colonne : on ne conserve que les mots qui ne correspondent ni à une unité, ni à un mot « sans intérêt » de la stop-list et on leur attribue un poids de 1. On calcule ensuite le score de similarité entre le titre de la colonne et chacun des types numériques de l\u0027ontologie, selon la formule du score de similarité donnée en section 4.1.2, avec comme terme du Web le titre de la colonne et comme terme de l\u0027ontologie le nom du type numérique dans l\u0027ontologie (par exemple, \"Samples tested\" pour le nombre d\u0027échantillons sur lesquels l\u0027expérience porte). Soit t titre le titre de la colonne col et t type le nom du type type, alors le score de similarité du titre de la colonne col avec le type type est :\nExaminons maintenant les unités utilisées dans la colonne. Soit u une unité et T u l\u0027ensemble de tous les types numériques pouvant s\u0027exprimer dans cette unité. Le score du type type pour l\u0027unité u est score(u, type) \u003d 1 Tu si u est une unité valable pour type, et score(u, type) \u003d 0 si le type type ne s\u0027exprime pas dans l\u0027unité u. Soit U col l\u0027ensemble de toutes les unités présentes dans la colonne : on considère également les unités présentes dans le titre de la colonne à condition qu\u0027elles ne fassent pas partie d\u0027un couple nombre-unité, qui représente généralement une précision de condition expérimentale (par exemple \"at 37\n• C\"). Le score sur les unités de la colonne col avec le type type est :\nAinsi le score de la colonne col avec le type type est : -si toutes les valeurs numériques contenues dans les cellules de la colonne sont compatibles avec l\u0027intervalle de valeurs possibles associé au type type, alors score f inal (col, type) \u003d 1?(1?score titre (col, type))×(1?score unit (col, type)) (6) Ce score est inspiré de Yangarber et al. (2002), où une mesure similaire est utilisée pour combiner les confiances que l\u0027on a en différentes règles de reconnaissance d\u0027une entité nommée. Les deux scores se renforcent ainsi mutuellement, mais il suffit que l\u0027un des deux scores soit bon pour que le score final soit bon.\n-s\u0027il existe une valeur dans les cellules de la colonne qui est en dehors de l\u0027intervalle de valeurs défini dans l\u0027ontologie, alors score f inal (col, type) \u003d 0\nLe type retenu pour la colonne est celui qui a le meilleur score f inal . Si tous les scores sont nuls, le type de la colonne est considéré comme non reconnu.\nRésultats expérimentaux\nLes 263 colonnes numériques reconnues lors de la classification numérique/symbolique (section 3.2) ont été utilisées pour la validation de notre approche. Ces colonnes ont été manuellement classées suivant 19 types numériques définis dans l\u0027ontologie. Les résultats de notre classification utilisant l\u0027ontologie ont été comparés à ceux de la méthode de classification par apprentissage SMO, les colonnes étant représentées par des vecteurs pondérés de tous les mots contenus dans les cellules et le titre de colonne, toutes les valeurs numériques étant remplacées par le mot-clef #NUM. La classification par SMO a été évaluée en validation croisée par leave one out.\nAvec notre méthode de classification sans apprentissage utilisant l\u0027ontologie, on obtient une précision globale de 96% et une couverture de 95%, sur l\u0027ensemble des 19 types. La méthode SMO classe toutes les instances, avec une précision globale et une couverture globale de 96%. Notre méthode de classification, qui ne nécessite pas de données d\u0027entraînement, donne donc des résultats de classification tout à fait comparables à une méthode classique de classification par apprentissage, plus gourmande en temps d\u0027expert si l\u0027on part du postulat que l\u0027ontologie existe de toute manière (ce qui est le cas puisque l\u0027objet de la classification est de reconnaître les types définis dans l\u0027ontologie).\nConclusion et perspectives\nNous avons présenté une méthode d\u0027annotation de tableaux de données guidée par une ontologie, sans phase d\u0027apprentissage. Nous distinguons tout d\u0027abord les données numériques et symboliques, pour les traiter différemment. Les données symboliques sont annotées avec les termes de l\u0027ontologie, et ces annotations permettent de déduire le type de chaque colonne symbolique. Pour les colonnes numériques, on utilise à la fois le titre de la colonne et les valeurs et unités contenues dans la colonne pour déterminer le type de la colonne : là encore nous utilisons l\u0027ontologie, dans laquelle sont définis les unités et intervalles de valeur valables pour chaque type numérique. Notre approche donne des résultats comparables à une méthode classique de classification par apprentissage, mais sans nécessiter la construction d\u0027un jeu d\u0027entraînement.\nA partir des types de colonnes ainsi identifiés, il nous reste maintenant à reconnaître les relations représentées par le tableau de données. Ensuite nous travaillerons sur les techniques d\u0027interrogation de l\u0027entrepôt de données, en tenant compte du fait que les critères d\u0027interrogation permettent d\u0027exprimer des préférences, que l\u0027annotation des données symboliques est floue et que le type de certaines colonnes n\u0027est pas reconnu.\n"
  },
  {
    "id": "906",
    "text": "Introduction\nL\u0027apprentissage de la structure des réseaux bayésiens (RB) à partir de données est un problème ardu ; la taille de l\u0027espace des graphes orientés sans circuits (DAG en anglais) est super-exponentielle en fonction du nombre de variables et le problème combinatoire associé est NP-difficile (Chickering et al., 2004). Deux grandes familles de méthodes existent pour l\u0027apprentissage de la structure des RB : celles fondées sur la satisfaction de contraintes d\u0027indé-pendance conditionnelle entre variables et celles à base de score fondées sur la maximisation d\u0027un score (BIC, MDL, BDe, etc.). Les deux méthodes ont leurs avantages et leurs inconvé-nients. Les méthodes sous contraintes sont déterministes, relativement rapides et bénéficient des critères d\u0027arrêt clairement définis. Les contraintes imposées à la structure du graphe proviennent des informations statistiques sur les dépendances et indépendances conditionnelles observées dans les données. Elles reposent cependant sur un niveau de signification arbitraire du test d\u0027indépendance employé. En outre, les erreurs commises au début peuvent se répercu-ter en cascade dans la suite de l\u0027execution de l\u0027algorithme, et conduire à un graphe erroné. Les méthodes à base de score ont, quant à elles, l\u0027avantage d\u0027incorporer des probabilités a priori sur la structure du graphe et de traiter plus facilement les données manquantes. En revanche, elles sont facilement piégées dans les nombreux minima locaux et le graphe final obtenu dépend fortement des conditions initiales.\nPlusieurs méthodes ont été proposées durant ces quinze dernières années mais quelques avancées prometteuses ont été réalisées très récemment. Dans un article paru en 2006, Tsamardinos et al. montrent par des simulations exhaustives sur une vingtaine de bancs d\u0027essais (Child, Insurance, Alarm, Hailfinder etc.) l\u0027avantage significatif d\u0027un algorithme sous contraintes, dénommé Min-Max Hill-Climbing (MMHC), au regard des algorithmes majeurs (score et contraintes) en fonction de plusieurs métriques de performance (Tsamardinos et al., 2006). Son inconvénient, toutefois, est sa complexité au pire cas en O(n2 n ) où n désigne le nombre de noeuds. En réalité, aucun algorithme exact n\u0027échappe à une complexité exponentielle car les méthodes exactes reposent toutes sur une recherche exhaustive des indépendances entre deux variables X et Y conditionnellement à un ensemble Z. Cette recherche nécessite O(2 |Z| ) opérations au pire cas. C\u0027est pourquoi toutes les approches polynômiales reposent sur une heuristique particulière pour parcourir les ensembles Z.\nBrown et al. ont donc proposé une version polynomiale en O(n 4 ) de MMHC dénommée Polynomial Min-Max Skeleton (PMMS) (Brown et al., 2005) en adoptant une heuristique ingénieuse. L\u0027algorithme séduit par ses nombreux attraits : outre sa grande simplicité, les auteurs ont montré empiriquement l\u0027excellent compromis entre faible complexité et qualité de reconstruction comparé aux autres algorithmes, surtout en présence de faibles jeux de données. Cet avantage est décisif à nos yeux car nous disposons d\u0027une base de données d\u0027une étude épi-démiologique cas-témoins du cancer du nasopharynx (NPC) de seulement 1289 observations. L\u0027idée est donc d\u0027utiliser PMMS afin de construire par apprentissage la structure du RB associé aux données. Néanmoins, comme tous les algorithmes sous contraintes, PMMS échoue lorsque des dépendances fonctionnelles (DF) déterministes existent entre des groupes de variables. Une DF, notée X ? Y , est une contrainte entre un ensemble de variables, telle que tout ensemble de valeurs prises par les X j ? X determine la valeur de Y de façon univoque. Or rien n\u0027exclue cette éventualité compte tenu du faible nombre d\u0027observations. En outre, PPMS ne s\u0027applique qu\u0027aux données complètes, ce qui n\u0027est pas notre cas. Aussi, dans cet article, nous apportons quelques modifications algorithmique à PMMS pour remédier à ces deux problèmes : le traitement des DF et son adaptation aux données manquantes.\nAprès un rappel indispensable de la problématique et des principes de l\u0027algorithme, ces modification sont présentées en détail et validées sur deux bancs d\u0027essai Asia et Asia8 avec une DF entre trois variables, 1289 données et 5% de données manquantes pour nous ramener au cas du NPC. La nouvelle version a été développée sous Matlab à l\u0027aide de la Toolbox BNT de (Murphy, 2001) et de la Toolbox BNT-SLP de (Leray et Francois, 2004). Ensuite, nous appliquons la méthode aux données du NPC de 1289 observations. A la différence des travaux préliminaires menés dans (Aussem et al., 2006) avec seulement 10 variables binaires synthé-tiques et sans données manquantes, une base plus vaste de 61 variables qualitatives ordinales et 5% de données manquantes est analysée. L\u0027objectif est de dresser un profil statistique type de la population étudiée et d\u0027apporter un éclairage utile sur les différents facteurs impliqués dans le NPC.\nPréliminaires\nNotons l\u0027indépendance conditionnelle entre X et Y sachant l\u0027ensemble Z dans une loi de probabilité P par Ind P (X; Y |Z) et la dépendance par Dep P (X; Y |Z). Les lettres majuscules en gras, Z, désignent des ensembles de variables aléatoires, les autres majuscules, X, désignent des variables uniques, les minuscules (X \u003d x, X \u003d x) désignent les attributs ou modalités des variables. Soit P , une loi de probabilité conjointe sur un ensemble de variables aléatoires V, et G \u003d\u003c V, E \u003e un graphe orienté sans circuit (DAG en anglais). On dira que le tuple \u003c G, P \u003e est un réseau bayésien si \u003c G, P \u003e vérifie la condition dite de Markov : chaque variable, X ? V, doit être indépendante de ses non descendantes (N D X ) dans G conditionnellement à ses parents (Neapolitan, 2004;Pearl, 2000). Cette condition se note Ind P (X; N D X |Pa G i ) où Pa G i désigne l\u0027ensemble des parents de X i dans G. La condition de Markov implique la factorisation de la loi jointe :\nCette propriété importante montre qu\u0027il suffit de stocker les valeurs de  Naim et al. (2004);Neapolitan (2004)). La propriété de Markov impose en revanche une condition forte aux lois de probabilité P qui peuvent être représentées par le même graphe G. Contraintes du graphe -La d-séparation est un critère important qui permet de caractériser graphiquement toutes les contraintes d\u0027indépendance des lois P qui peuvent être représentées par un même DAG. Pour éclaircir son rôle, il faut introduire la notion de chaîne d\u0027information bruité. Par chaîne, on entend une succession d\u0027arcs orientés entre X et Y mais vus comme des arêtes non orientés (Neapolitan, 2004). Pour comprendre la d-séparation, il faut symboliser les noeuds sur ces chaînes par des vannes d\u0027information, ouvertes ou fermées selon le cas. Un chemin est dit ouvert si toutes les vannes sont ouvertes auquel cas il laisse passer l\u0027information. A l\u0027inverse, si l\u0027une des vanne est bloquée, la chaîne est dite bloquée. En extrapolant, l\u0027information qu\u0027apporte X sur Y peut se voir comme la somme des flots sur tous les chaînes ouvertes reliant X à Y . Il reste à spécifier le mécanisme d\u0027ouverture et de fermeture des vannes. Il existe 3 types de connexions : les connexions en série \nCondition de fidélité -G et P sont dits fidèles (faithful) l\u0027un à l\u0027autre ssi toutes les indépen-dances conditionnelles sont strictement identifiées par les d-séparations, i.e., Dsep G (X; Y |Z) ? Ind(X; Y |Z). On parle alors de réseau bayésien \u003c G, P \u003e fidèle. PMMS repose sur l\u0027hypothèse de fidélité ; l\u0027algorithme construit un DAG sensé être fidèle à la loi de probabilité P sous-jacente aux données. Cela pose un problème car toutes les distributions ne sont pas fidèles à un DAG, c\u0027est le cas notamment du réseau Asia en raison de la variable O. O est un OU logique entre T et L, du coup Ind P (O; X|{T, L}) est vérifié sans pour autant avoir Dep P (O; X|{T, L}). L\u0027existence de dépendances fonctionnelles (DF), accidentelles ou non, entre des variables, est hélas fréquent. C\u0027est souvent le cas dans les données sont issues de questionnaires pour de multiples raisons (e.g. questions redondantes ou mal comprises, ré-ponses groupées etc.).\nPolynomial Min-Max Skeleton revisité\nDans ce paragraphe, nous rappelons le principe de PMMS avant de présenter les modifications que nous avons opérées pour traiter l\u0027existence des DF et les données incomplètes. PMMS exploite ingénieusement le critère de d-séparation. Il construit itérativement le voisinage, parents et enfants, de chaque variable cible T , en observant que ce sont les seuls noeuds qui ne peuvent être d-séparés de T . Notons par PC T les noeuds parents et enfants du noeud T dans G. PC T est unique à tous les DAG tels que \u003c G, P \u003e soient fidèles, il ne dépend donc pas de G. PMMS emploie une mesure d\u0027association probabiliste conditionnelle notée\nC\u0027est-à-dire les plus petites associations entre X et Y pour tous les sous-ensembles S de Z. PMMS appelle successivement la procédure Polynômial Min-Max Parents and Children PMMPC pour chaque variable de G (voir algorithme 1). PMMPC identifie PC T étant donné une variable cible T . Ainsi, connaissant le voisinage direct de chaque variable cible, il suffit de connecter les noeuds pour obtenir le squelette du graphe (non connecté). La version originale de PMMPC opère en 2 phases. Nous y avons adjoint une troisième phase pour traiter les DF (voir algorithme 2).\nPhase I -Les variables entrent séquentiellement dans un ensemble de candidats noté CPC à l\u0027aide d\u0027une heuristique Max-Min. L\u0027idée est de sélectionner itérativement les variables qui ne peuvent pas être d-séparées par l\u0027ensemble CPC courant. Celle qui rentre est celle qui présente l\u0027association résiduelle la plus forte avec la cible T malgré notre effort pour bloquer toutes les chaînes les reliant. Le calcul de M inAssoc(X; Y |CP C) requiert normalement un nombre d\u0027appels exponentiel à la fonction Assoc. Dans PMMS, ce calcul est réduit à l\u0027aide de l\u0027heuristique gloutonne GreedyM inAssoc(X; T |CPC, minval, MinSet) ; minval est l\u0027estimation courante du minimum d\u0027association et MinSet est l\u0027estimation courante de S ? Z qui réalise ce minimum. Initialement MinSet \u003d ? et minval \u003d Assoc(X; Y |?) ; MinSet croît itérativement par adjonction d\u0027une variable de CPC après l\u0027autre jusqu\u0027à ne plus pouvoir décroître l\u0027association résiduelle. La notation min{x \u003d désigne le plus petit x différent de s\u0027il existe. Si les données sont en nombre insuffisant, alors Assoc \u003d la phase de croissance est stoppée et la dépendance est supposée. Néanmoins la valeur de l\u0027association est fixée à la plus petite valeur possible, comme discuté au chapitre 3.1.\nPhase II -Dans cette phase backward, les faux-positifs entrés par erreur dans la phase I (voir Tsamardinos et al. (2006)) sont itérativement éliminés de CPC. Pour ce faire, on teste pour chaque\nPhase III -Cette étape est nouvelle. Si CPC ? T est une DF, le graphe G n\u0027est pas fidèle avec P , auquel cas T sera d-séparé de tous ses enfants par CPC alors que c\u0027est faux. Dans le cas d\u0027Asia par exemple, PPMPC lancé sur la cible O conduit à CPC \u003d {T, L} car l\u0027association de T et L avec O est la plus forte, et par suite, ni X ni D ne pourront plus entrer dans CPC. Aussi, pour y ajouter les enfants de la cible T , nous testons si la relation CPC ? T est un DF par un appel à IsF uncDep(T ; CPC; D) (simple parcours de l\u0027hypercube de contingences). Si la DF est observée, un nouvel appel récursif à PMMPC est fait en retirant CPC à l\u0027ensemble des variables V jusqu\u0027aucune DF ne subsiste. On récupère au final un ensemble CPC qui contient non seulement les enfants de T mais aussi ses grands-parents. Dans Asia, le CP C de O sera au final l\u0027ensemble {T, L, X, D, A, S} mais PMMS ne connectera pas O avec ses grands-parents A et S car réciproquement O ne sera ni dans le CP C de A, ni dans celui de S. Ainsi, la phase 3 permet de détecter les DF susceptibles de tromper PMMPC (et uniquement celles-ci) pour ensuite trouver tous les parents et enfants de la variable cible.\nAu final, PMMS construit un graphe non orienté (le squelette) qui est sensé, une fois les arêtes dirigées, représenter la structure du réseau bayésien. Comment diriger les arêtes du graphe du squelette ? Il faut garder à l\u0027esprit que plusieurs DAG peuvent encoder la même loi de probabilité conjointe, seule importe la position des V-structures (i.e., X ? Y ? Z tel que X et Z ne soit pas connectés). Cet article ne porte que sur l\u0027apprentissage du squelette, la partie de loin la plus difficile. Pour la recherche des V-structures et la direction des arcs, le lecteur est invité à consulter (Naim et al., 2004;Neapolitan, 2004) pour de plus amples informations. \nMesure d\u0027association\nfinsi 8: fin pour 9: return E au risque ? du test, et zéro sinon. Le ? 2 XY |Z suit une loi ? 2 à ? \u003d (n X ? 1)(n Y ? 1)c degrés de liberté où c \u003d Zj ?Z n Z j est le produit du nombre de modalités de chaque variable dans Z. Intuitivement, plus la valeur de p est petite, plus l\u0027association entre X et Y sachant Z est forte. Aussi, une valeur de p supérieure au seuil ? indiquera une association nulle. En pratique, le test n\u0027est utilisé que si n est suffisamment grande devant ?. Dans le cas contraire, il faudrait idéalement procéder à des regroupements de modalités voisines. Des méthodes heuristiques existent pour évaluer empiriquement le nombre effectif de degrés de liberté (voir Tsamardinos et al. (2006)). Dans notre cas, le test est appliqué dès lors que n \u003e 10? comme dans l\u0027heuristique PC Spirtes et al. (2000), sinon Assoc(X; Y |Z) retourne une constante inférieur au risque du test 0 \u003c \u003c 1 ? ? et l\u0027on suppose la dépendance, faute de pouvoir statuer.\nLa présence de données manquantes dans les données du cancer pose une difficulté supplémentaire. L\u0027apprentissage de RB en présence de données manquantes est en soi un domaine de recherche actif dans lequel plusieurs solutions ont été proposées, en majorité pour les algorithmes bayésiens (à base de score) François (2006). Nous optons pour une solution simple connue sous le nom \"available case analysis\" : le ? 2 XY |Z est calculé uniquement sur les observations pour lesquelles les n variables X, Y et Z j ? Z sont présentes. Les autres sont ignorées. Ce faisant, le risque est toutefois d\u0027introduire un biais dans les estimateurs (Ramoni et Sebastiani, 2001;Friedman, 1998;Dash et Druzdzel, 2003) en particulier si l\u0027hypothèse Missing Completely at Random (MCAR) n\u0027est pas vérifiée, i.e., le processus de perte est indépendant de la valeur des données complétées D (observées et manquantes).\nOn distingue donc trois cas : 1) le cas où l\u0027indépendance est supposée, 2) le cas du manque de données n ? 10? et enfin 3) le cas l\u0027hypothèse d\u0027indépendance est rejetée : Les résultats obtenus avec la nouvelle version de PMMS sont consignés dans la Table 1 en fonction du risque ? du test. L\u0027objectif est de montrer la qualité de la reconstruction du squelette (Figure 1) malgré le faible nombre de données, partiellement manquantes, et de choisir empiriquement le meilleur seuil ?. Des tests avec ? \u003e 0.1 ne sont pas affichés car ils se sont avérés décevants. Dix jeux de données ont été synthétisés à partir du RB original. La Table 1  \nApplication au cancer\nLes données\nNous appliquons la nouvelle version de PMMS aux données d\u0027une étude épidémiologique cas-témoins du cancer du nasopharynx (NPC) dans la lignée des travaux de recherche récents (Aussem et al., 2006;Antal et al., 2004;Getoor et al., 2004 \nRésultats\nLa base est contient 5% de données manquantes mais on ignore si l\u0027hypothèse M CAR est valide. Le nombre de modalité varie de 2 (binaire) à 11 pour la classe d\u0027âge. La majorité des variables sont codées en trois classe \"pas du tout\", \"un peu\" et \"beaucoup\". La variable à expliquer est \"cas NPC\", les autres sont les variables explicatives, voir le lexique de la Figure  2. Elles portent sur les conditions socio-professionnelles, l\u0027habitat, l\u0027exposition aux produits toxiques, la nourriture industrielle ou faite maison, les maladies, allergies, les drogues locales etc. Le squelette du RB obtenu avec la nouvelle version de PMMS est représenté sur la Figure  2 avec ? \u003d 0.1. Il s\u0027agit d\u0027une visualisation graphique des interactions entres les variables qui dresse le profil statistique de la population considérée. En observant uniquement la structure du graphe, des groupes thématiques cohérents de variables de A à P ont été exhibés avec notre expert. Leur homogénéité est frappante. A est la le seul groupe lié au NPC (variable 1), il est lié à l\u0027aération de l\u0027habitat (variables 30, 31 et 32 mais pas 33 car indépendamment de l\u0027orientation des arcs, 1 peut être d-séparée de 33 par un sous-ensemble de 30, 31 et 32) ; B, conditions socio-professionnelles liées à l\u0027âge. C, lieu d\u0027habitat ; D, catégorie de logement ; E, produits toxiques et fumées ; F , drogues ; G, animaux domestiques ; H, encens, parfums et feu de bois ; I, maladies ; J, graisse rance ; L, protéines maison ; M , piment et harrissa ; N , nourriture industrielle ; O, conserves ; P légumes et fruits. On retrouve donc des résultats de bon sens : les hommes (3) sont plus enclins à fumer, à consommer des drogues (F ) et être exposés à des produits toxiques au travail (E), que l\u0027exposition aux fumées (encens, parfums, feu de bois etc.) (H) est plus fréquente dans les gourbis que les appartements en villes (C) ; que la nourriture industrielle (N ) se consomme en conserve (O) ; que d\u0027une manière géné-rale, les habitudes acquises à l\u0027enfance se conservent à l\u0027âge adulte ; que les produits toxiques (E) provoquent des maladies (I) etc. Nous avons de plus testé les qualités du clasifieur selon le principe du 10-fold cross validation. Après chaque apprentissage, on retrouve toujours la V-structure 1 ? 30 ? 31, l\u0027inférence est alors immédiate et seules 30 et 31 doivent être renseignées. Le taux de réussite moyen est de 74% sur les données en test à comparer aux 51% d\u0027individus atteints par le NPC, ce qui semble a priori un bon résultat. Toutefois, le NPC semble être la cause de la mauvaise aération cuisine à l\u0027enfance et non l\u0027inverse d\u0027après l\u0027orientation des arcs ! Après discussion avec l\u0027expert, ces résultats curieux confirment surtout un biais dit de classement : les individus atteints du NPC (cancer des voies respiratoires) sont plus enclins à chercher les causes de leur maladie dans la mauvaise aération de leur habitat. Souffrant de difficultés respiratoires, ils ont une tendance à imputer à tort la cause de leur cancer à l\u0027aération. Ce résultat curieux n\u0027est donc pas à imputer à PMMS mais aux données elles-mêmes.\nEn conclusion, ce graphe nous renseigne, certes, sur le mode de vie des sujets maghrébins mais révèle par ailleurs des biais propres aux comportements psychologique des individus, à la façon dont ils comprennent (ou non) les questions. Malgré la pertinence des groupes de variables obtenus et leurs associations, il ne dit rien en revanche sur les \"causes potentielles\" du NPC, en gardant à l\u0027esprit que, même en supposant l\u0027hypothèse de suffisance causale et la fiabilité de la mesure d\u0027association, il est impossible de déceler les relations causales à partir de données sans mener des expérimentations supplémentaires.\n"
  },
  {
    "id": "907",
    "text": "Introduction\nLes moteurs de recherche classiques sur le web ont des caractéristiques étonnantes : ils possèdent des milliards de documents dans leur index, ils peuvent traiter des millions de requêtes quotidiennement, ils donnent des réponses très volumineuses quasiment en temps réel et ils nécessitent des ressources informatiques et humaines considérables. On peut dire aujourd\u0027hui qu\u0027il est pratiquement impossible de concevoir une approche alternative pour un moteur de recherche sans passer par l\u0027un de ces « géants ». Même si les points forts de ces moteurs sont nombreux, ils ont aussi des faiblesses, comme des requêtes très simples, une présentation des résultats souvent pauvre en information, ou encore la nécessité pour l\u0027utilisateur d\u0027explorer un à un les nombreux liens qu\u0027ils donnent en sortie.\nEn effet, lorsque l\u0027on recherche une information précise ou spécialisée, comme par exemple trouver une entreprise répondant parfaitement à notre préoccupation du moment, les moteurs classiques se révèlent difficile d\u0027usage et peu pertinents à la fois. Pour arriver à ses fins, l\u0027internaute a alors le choix entre plusieurs types d\u0027outils destinés à l\u0027aider dans sa tâche. D\u0027un côté, les méta-moteurs de recherche spécialisés, bien que de plus en plus sophistiqués, se heurtent à deux écueils principaux : le Web invisible et la masse gigantesque d\u0027informations gérées. D\u0027autre part, les annuaires professionnels représentent une alternative intéressante par une meilleure exhaustivité des données managées. Cet avantage est cependant tempéré par le cloisonnement des entités à l\u0027intérieur de rubriques préétablies, manquant de discernement. Ces différentes solutions montrent leurs limites dans le manque d\u0027efficacité en terme de pertinence.\nNous proposons dans cet article un outil de recherche spécialisé dans la recherche d\u0027entreprises mêlant efficacité de formulation de la demande initiale et pertinence des résultats affichés. Afin d\u0027assister le mieux possible l\u0027internaute, ce nouveau moteur de recherche dispose d\u0027une fonctionnalité de géo-localisation autorisant la restriction géographique de la requête et le repérage graphique des entreprises atteintes.\nL\u0027élaboration d\u0027un moteur de recherche, à usage des entreprises, bâti sur une infrastructure Web sémantique constitue une nouvelle voie qu\u0027il convenait d\u0027exploiter. L\u0027idée maîtresse est de donner une signification au contenu des documents présents sur le Web. De cette façon, les machines sont en mesure de comprendre le sens des documents et d\u0027effectuer des raisonnements automatisés. La réalisation de ce moteur de recherche « intelligent » passe par la modélisation d\u0027une ontologie. Celle-ci est nécessaire à la formalisation des connaissances du domaine, permettant leur partage et leur interprétation opérationnelle.\nLa suite de cet article est organisée comme suit ; la section 2 présente un état de l\u0027art de différents systèmes de recherche d\u0027information se basant sur le principe du web sémantique et mettant en oeuvre des ontologies. La section 3 détaille l\u0027architecture retenue pour l\u0027élaboration de notre moteur de recherche ainsi que la description des différentes ontologies permettant d\u0027organiser l\u0027ensemble des données accessible. La section 4 donne des résultats expérimentaux obtenus par comparaison au principal outil du genre : les Pages Jaunes. La section 5 conclut sur les nombreuses perspectives qui découlent de ce travail.\nSystèmes de recherche d\u0027information et ontologie\nLe Web sémantique, proposé par le W3C (World Wide Web Consortium), est une nouvelle approche qui vise, à partir de la structure actuelle du Web, à donner un sens au contenu des pages. Selon Tim Berners-Lee, inventeur du Web et directeur du W3C, \"The semantic Web is not a separate Web but an extension of the current one, in which information is given welldefined meaning, better enabling computers and people to work in cooperation\" Burners-Lee et al. (2001). C\u0027est une manière de donner un sens bien défini aux informations permettant une interprétation aussi bien par les machines, que par les humains.\nPar ce biais là, l\u0027objectif du Web sémantique est de décharger les utilisateurs d\u0027une grande partie de leurs tâches de recherche et d\u0027exploitation des résultats. Mais pour que le Web sé-mantique fonctionne il est nécessaire que les machines aient accès à des collections structurées d\u0027informations et de règles d\u0027inférence qu\u0027elles peuvent utiliser pour parvenir à un raisonnement automatisé. Cette modélisation des données, appelée ontologie, est destinée à jouer un rôle clé car dispensant une connaissance commune, et partagée, du domaine. Le monde des sciences de l\u0027information s\u0027est approprié ce terme pour désigner \"une spécification formelle et explicite des termes d\u0027un domaine ainsi que des relations que ces termes entretiennent entre eux\" Gruber (1993). Il est important de compléter cette définition en précisant qu\u0027une \"ontologie est indépendante des considérations d\u0027exécution, son objectif principal étant de spécifier la conceptualisation du domaine sous-jacent à l\u0027application\" Welty et Guarino (2001).\nDans le cadre du Web sémantique, l\u0027ontologie a ainsi pour enjeu de \"proposer une compréhension partagée et commune, pour un domaine donné, qui peut être transmise aussi bien aux personnes qu\u0027aux applications\" Davies et al. (2002). Elle doit traduire un certain consensus, explicite, de manière à être partagée par la communauté l\u0027ayant construite et acceptée. Ceci est vital pour permettre l\u0027exploitation des ressources présentes sur le Web par différentes applications ou autres agents logiciels. Le Web sémantique doit ensuite ajouter de la logique, c\u0027est-à-dire lui donner la possibilité d\u0027utiliser les règles pour faire des inférences. Par l\u0027exploitation de ces règles, les moteurs d\u0027inférence peuvent raisonner intelligemment et offrir des réponses automatiques à des questions posées par une personne. Ils doivent être en mesure de déduire de nouvelles informations à partir d\u0027informations et de ressources déjà existantes dans l\u0027environnement.\nNous voyons apparaître depuis quelques années des outils exploitant ces données dans des cadres divers et notamment concernant les systèmes de recherche d\u0027information. Les informations contenues dans les ontologies permettent de déterminer un sens non ambigu aux différents éléments rencontrés. Les résultats produits sont par conséquent plus pertinent que ceux issus des moteurs de recherche traditionnels se basant sur des considérations purement statistiques pour effectuer leurs recherches Lawrence et Giles (1999) ;Brin et Page (1998); Baeza-Yates et Ribeiro-Neto (1999).\nUn des premiers domaine à avoir bénéficié des avancées des ontologies sur le Web est certainement celui des annuaires, tel Yahoo !. Pour ces applications, la localisation d\u0027une information est assurée par un système de catalogues thématiques hiérarchiques (classification) consultables à l\u0027aide de mots clés ou par navigation de thèmes en sous thèmes. Par exemple, Labrou et Finin (1999) utilise cette classification afin de décrire les documents à la manière d\u0027une ontologie. L\u0027annotation de chaque document permet de rendre le sens compréhensible par des outils automatique. En utilisant un mécanisme d\u0027inférence, il devient alors possible d\u0027améliorer la qualité des résultats produits par les moteurs de recherche sémantiques Mayfield et Finin (2003). En particulier Shah et al. (2002) utilise le texte sémantiquement enrichi afin de lever certaines ambiguïtés du texte libre et procède ensuite par inférence pour améliorer la qualité de l\u0027indexation. Guha et al. (2003) a, quand à lui, développé un système de recherche d\u0027information utilisant une ontologie afin d\u0027améliorer les résultats des moteurs de recherche classiques en ajoutant des sources issus des concepts de l\u0027ontologie associés aux résultats originaux.\nCette information sémantique peut aussi être utilisée afin d\u0027affiner des mesures utilisés dans les algorithmes de scoring des moteurs de recherche. L\u0027indexeur Swoogle Ding et al. (2004), Ding et al. (2005), découvre, indexe et analyse les ontologies des documents du web. Il utilise les informations sémantiques contenus dans les méta-données des documents afin de produire une mesure de similarité la plus pertinente possible. OntoSearch, Gao et al. (2005), analyse les méta-données afin de déterminer des poids dans un vecteur de concept pour chaque document. La pertinence des documents à la requête de l\u0027utilisateur est alors mesurée en calculant une similarité entre vecteur de document et vecteur de requête.\nUn moteur de recherche d\u0027activités géo-localisées\nLe domaine de la recherche d\u0027informations est, en partie, lié aux langues que ce soit lors de l\u0027interprétation d\u0027une requête ou de l\u0027analyse des documents traités. Il apparaît selon plusieurs auteurs (de Loupy (2000)), qu\u0027un système de recherche d\u0027informations devra, pour être efficace, conjuguer l\u0027approche statistique avec un traitement linguistique. De nombreux problèmes de polysémie et de synonymie limitent en effet l\u0027efficacité d\u0027une recherche purement statistique par mots clés. Les relations sémantiques précédentes sont génératrices de non-conformité des résultats produits par une recherche. Une des solutions destinée à lever ces ambiguïtés séman-tiques est d\u0027utiliser des liens thématiques de Loupy et Crestan (2004). La démarche consiste à regrouper les termes par affinités : par exemple, le domaine services informatiques pourrait regrouper les termes infogérance, développement et maintenance logiciels.\nAfin de cataloguer et de gérer les ambiguïtés pouvant intervenir dans la formulation des activités, nous avons décidé de créer une ontologie de description de ce domaine de connaissance. A chaque activité recensée, nous associerons plusieurs termes synonymes nous permettant de lever une partie des problèmes de polysémie et de synonymie. Et de manière analogue nous organiserons la connaissance des entreprises dans une ontologie particulière.\nLa recherche d\u0027une entreprise par son activité consistera donc à rechercher, dans un premier temps, l\u0027activité recensée la plus proche de ce que recherche l\u0027utilisateur en utilisant les termes synonymes introduits précédemment, puis de lister les références des entreprises dont au moins une activité y est associée.\nOntologie de description d\u0027activités géolocalisée\nPour produire une ontologie relativement complète des différentes activités d\u0027entreprises pouvant exister, nous nous sommes appuyés sur les données recueillies auprès des organismes officiels chargés d\u0027enregistrer les déclarations de toute entreprise française : l\u0027INSEE. Une initiative similaire à déjà été utilisée dans Marquet et al. (2003) afin d\u0027unifier l\u0027accès à des ressources médicales en se référant aux terminologies standard du domaine. Ces différentes structures, constituent une source capitale, dans le cadre du présent projet. Les nomenclatures économiques présentent l\u0027avantage d\u0027être maintenues par des experts reconnus du domaine ce qui certifie exhaustivité et qualité du vocabulaire employé. Elles comprennent aussi bien la liste complète des activités pouvant être exercées par les entreprises que les produits développés. En outre, toute unité économique exerçant en France est rattachée à la NAF, via le code APE.\nLa NAF et la CPF ont été élaborées dans un cadre européen, harmonisé, afin de clarifier l\u0027information sur le marché unique européen. Elles sont organisées sur plusieurs niveaux hié-rarchiques : sections et sous-sections comme le montre la figure 1.\nLes nomenclatures permettent le classement de toutes les activités économiques et de tous les produits (biens et services). Elles constituent un outil pour ordonner l\u0027information éco-nomique mais proposent, aussi, un langage commun présentant un intérêt dans de nombreux domaines.\nLe code APE représente l\u0027activité principale exercée par l\u0027entreprise ce qui correspond au code de la classe issue de la nomenclature française des activités. Dans l\u0027hypothèse d\u0027une entreprise exerçant plusieurs types d\u0027activités, l\u0027INSEE, par le biais d\u0027une estimation statistique, détermine celle qui demeure prédominante. Il est important de souligner que, dans le cas d\u0027une entreprise disposant de plusieurs établissements, chacun d\u0027eux dispose d\u0027un code APE.\nCette notion d\u0027activité principale de l\u0027entreprise est importante, notamment en droit, pour déterminer, par exemple, les champs d\u0027application des conventions collectives.\nGéo-localisation et description des entreprises\nLes attributs caractéristiques d\u0027une entreprise, sur le Web, se matérialisent par les pages constituant son site. En pratique, une entreprise peut être spécialisée dans plusieurs activités et plusieurs produits localisés sur plusieurs lieux. Il en résulte qu\u0027une page donnée peut être liée à un ou plusieurs attributs du répertoire des activités. En outre, la construction d\u0027un moteur de recherche à usage des entreprises implique que les sociétés retenues dans l\u0027index ne possèdent pas nécessairement de site internet. Dans ce cas précis, une adresse Email de contact sera prise en compte pour référencer l\u0027activité ou le produit concerné. On peut imaginer une entité commerciale exploitant plusieurs activités avec, pour chacune d\u0027elles, un responsable possédant une adresse Email. L\u0027entité de l\u0027entreprise prise en compte dans notre organisation suit alors l\u0027organisation du répertoire SIRENE de l\u0027INSEE. Ce répertoire se base sur la notion d\u0027unité administrative dans laquelle un établissement est localisé géographiquement et rattaché à une unité légale (entité juridique déclarée aux administrations compétentes) qui est elle-même rattachée à un groupe financier. Le numéro de SIREN identifie alors de manière unique une unité légale tandis que le numéro SIRET l\u0027établissement en tant qu\u0027unité géographiquement localisée.\nLa structure du répertoire SIRENE par son organisation spécifique va répondre de façon satisfaisante à notre problématique de géo-localisation. En effet, la notion d\u0027établissement, vue par l\u0027INSEE comme une unité géographiquement localisée, via le code SIRET, nous garantit le référencement géographique, sans ambiguïté, de ce type d\u0027unité administrative.\nOrganisation de l\u0027ontologie\nLes différentes classes constituant l\u0027ontologie (décrite dans la figure 2) répondent, chacune d\u0027elles, à un ou des services spécifiques mis en évidence durant la modélisation. Ceux-ci peuvent être classés à l\u0027intérieur de quatre catégories principales : 1) la classification des produits, 2) la nomenclature des activités, 3) l\u0027organisation interne de l\u0027entreprise et 4) l\u0027organisation géographique. Si les deux premières catégories s\u0027imposent de façon triviale, car issues directement du modèle INSEE, les deux suivantes ont été constituées pour mieux structurer l\u0027ontologie définitive. Ces quatre ensembles peuvent être considérés comme des sous-ontologies que nous utiliserons respectivement dans le but de 1. rechercher les concepts liés à un mot clé ; généraliser, spécialiser un concept lié à une activité ou un produit ; récupérer les produits associés à une activité particulière ; \nArchitecture du moteur de recherche\nL\u0027architecture retenue en vue de la réalisation du démonstrateur envisagé s\u0027appuiera sur la plate-forme Sesame Broekstra (2005) associée à une base de données MySql. Cette solution présente l\u0027avantage d\u0027être robuste en terme de volumétrie ce qui convient pour la partie moteur de recherche. Le langage de requêtage exploité sera RDQL qui permet de traiter la représenta-tion de l\u0027ontologie comme une base de données. Il autorise l\u0027exécution de requêtes complexes, utilisant des modèles et des contraintes sur les triplets RDF, tout en permettant des jointures.\nPour permettre à l\u0027utilisateur de retrouver facilement l\u0027information qu\u0027il recherche, notre système doit être capable 1) de récupérer rapidement les concepts liés aux mots clés de l\u0027utilisateur, 2) d\u0027offrir un module de désambiguïsation sémantique, 3) de permettre la navigation dans l\u0027ontologie et 4) d\u0027afficher des résultats sous la forme définitive (raison sociale, descriptif entreprise, référence professionnelle).\nLe système propose à l\u0027utilisateur de saisir sa requête à l\u0027aide de 2 champs : le Quoi, obligatoire permet de préciser les mots clés d\u0027activité à rechercher, et le Où, facultatif, définissant la limitation géographique concernée. Les mots-clés saisis par l\u0027utilisateur feront l\u0027objet d\u0027une désambiguïsation sémantique. La composante géographique sera limitée à la saisie d\u0027un ou plusieurs codes postaux.\nLa récupération des concepts, à partir des mots clés saisis par l\u0027utilisateur, demeure une phase fondamentale du processus. Celle-ci, outre le fait de rechercher les activités et produits rattachés à une liste de mots clés, se charge de supprimer, de factoriser et de définir un ordre de pertinence sur les concepts.\nFIG. 3 -Interface de l\u0027application présentant l\u0027écran de sélection des concepts pour la désa-mbiguïsation.\nSuppression des concepts généraux Ce traitement consiste à ne considérer que le concept le plus spécialisé en cas de concurrence sur une même branche de l\u0027arbre de concepts. Cela permet préciser au mieux la requête de l\u0027utilisateur en éliminant les termes généraux.\nFactorisation des concepts\nLa factorisation a pour objectif de ne sélectionner que les concepts les plus généraux en cas de concurrence sur deux branches d\u0027un même sous-arbre. Lorsque la distance hiérarchique entre le concept général et les deux concepts fils est très proche (1 à 2 branches traversées), il peut être judicieux de résumer la pensée de l\u0027utilisateur par le concept général.\nOrdre de pertinence des concepts Pour donner un ordre de pertinence aux concepts sélec-tionnés, nous déterminons un poids représentatif du nombre de mots clés issus de la requête utilisateur associés à chaque concept. Ceux dont le poids est le plus important apparaissent alors en tête de la liste proposée par l\u0027écran de désambiguïsation sémantique.\nLorsqu\u0027un mot clé saisi par l\u0027utilisateur correspond à plusieurs concepts dans l\u0027ontologie, nous proposons de manière interactive un module de désambiguïsation sémantique. Il appartient alors à l\u0027utilisateur de déterminer dans quel champ d\u0027application il souhaite réaliser sa recherche. La liste des concepts associée sera affichée en tenant compte de leur pertinence respective (taux de mots clés associés). Une sélection multiple sera admise.\nDe plus, nous permettons dans ce cas à l\u0027utilisateur de naviguer par spécialisation / généra-lisation dans la hiérarchie des concepts afin de préciser au mieux sa pensée. La figure 3 illustre l\u0027interface de notre application comprenant les champs Quoi et Où, la zone de résultat et la fenêtre de désambiguïsation.\nPremiers résultats\nMéthodologie de test\nPour valider notre méthode, nous avons entrepris de construire un jeu d\u0027essaie réduit à une région particulière. Nous avons ensuite étudié les réponses obtenues aux différentes questions proposées dans la Lors de cette phase de test, nous n\u0027avons pas mis en oeuvre le module de désambigüa-tion afin de ne pas avantager notre modèle en guidant plus précisément l\u0027utilisateur dans sa formulation de mots clés.\nAnalyse des résultats\nL\u0027ensemble des résultats obtenus par notre prototype et les Pages Jaunes sont donnés en terme de précision/rappel dans le tableau 2. Afin de valider la pertinence du classement des résultats retournés par chaque moteur, nous avons évalué la précision et le rappel pour chaque requête en ne considérant dans un premier temps que les 5, puis les 10, puis 20, puis 30 premiers résultats.\nIl est évident que dans les résultats obtenus, notre approche apporte systématiquement une meilleure précision et un meilleur rappel que les Pages Jaunes. Ceci est du en particulier aux opérations de suppression et de factorisation de concepts permettant de cibler d\u0027avantage le souhait d\u0027un utilisateur. On peut toutefois noter que l\u0027annuaire testé fait apparaître une qualité hétérogène des réponses obtenues. Dans certains cas, les résultats pertinents du corpus sont pris en compte mais noyés dans l\u0027ensemble des réponses. Dans d\u0027autres exemples, certains éléments corrects ne sont pas considérés dans les résultats. À travers les exemples traités, deux grandes familles de mots-clés se dégagent : les motsclés directement rattachés à une activité ou un produit, ce qui induit une forte adhérence avec les nomenclatures de type NAF (ex : restaurant, logiciel) ; les autres mots-clés qualifiant un terme commun généraliste (ex : agent, dimanche). Il apparaît, clairement, que l\u0027annuaire est performant tant qu\u0027il s\u0027agit d\u0027interpréter des mots-clés liés à une activité ou un produit. Son architecture repose, manifestement, sur les nomenclatures de type NAF et les mots-clés associés. En revanche, l\u0027annuaire est dans l\u0027incapacité de traduire des mots-clés de la deuxième famille. Cette lacune se traduit par un manque de précision pouvant conduire à des excès en terme de bruit et de silence. Par opposition, le prototype, construit sur une ontologie, est capable d\u0027interpréter tout type de mots-clés. Ceci suppose que les concepts, toutefois en partie basés sur les nomenclatures activités et produits, soit sémantiquement affectés aux mots-clés indépendamment de la famille d\u0027appartenance.\nIl est également important de souligner que le prototype permet une sélection géographique régionale ce que ne permet pas l\u0027annuaire testé. Pour parvenir à ce résultat, il a été nécessaire d\u0027implémenter plusieurs stratégies. Tout d\u0027abord, il était important que la modélisation de l\u0027ontologie tienne compte des trois caractéristiques suivantes : dualité des mots-clés traités, sépa-ration concepts concernés / classement des résultats et relation entreprise / lieu géographique. Et il a été également essentiel de mettre à jour l\u0027ontologie en associant les mots-clés aussi bien du côté des nomenclatures activités et produits que du côté des entreprises indexées.\nConclusion\nNous avons décrit dans cet article un nouveau moteur de recherche géo-localisé à usage des entreprises. La méthodologie que nous avons adoptés, issue du web sémantique, nous a permis d\u0027améliorer significativement l\u0027efficacité de solutions du domaine largement répandu comme les Pages Jaunes, en intégrant une ontologie. La phase de modélisation de l\u0027ontologie a mise en évidence l\u0027intérêt des structures économiques maintenues par l\u0027INSEE. Les nomenclatures économiques ont été retenues pour gérer la classification des activités et produits pouvant être dispensés par les entreprises tandis que la structure des unités administratives, telle que gérée au sein du fichier SIRENE, s\u0027est avérée judicieuse pour réponse à la problématique de géo-localisation des entreprises.\nNous avons élaboré un démonstrateur mettant en oeuvre différentes stratégies se servant de l\u0027ontologie afin de guider l\u0027utilisateur dans la formulation de sa requête. Nous envisageons de poursuivre dans ce sens en expérimentant notre prototype sur des bases de données plus importantes et en menant des études afin de déterminer plus précisément l\u0027amélioration qu\u0027apporte ce système de feed-back. Actuellement, la base de travail est alimentée manuellement et il serait également intéressant de profiter de la connaissance présente dans l\u0027ontologie afin d\u0027introduire une indexation automatique plus pertinente que celle réalisée par les moteurs de recherche classiques. Le prototype réalisé dans ce papier sert de base à l\u0027élaboration d\u0027un outil de recherche géo-localisé plus complet (Géoternet) développé par la société IP\u0026moteur.\n"
  },
  {
    "id": "908",
    "text": "Introduction\nGrâce à des techniques récentes de traitement de la parole, de nombreux centres d\u0027appels téléphoniques automatisés voient le jour. Ces serveurs vocaux permettent aux utilisateurs d\u0027exécuter diverses tâches en dialoguant avec une machine. Les entreprises cherchent à amé-liorer la satisfaction de leurs clients en les redirigeant en cas de difficulté vers un opérateur humain. L\u0027aiguillage des utilisateurs mécontents revient à détecter les émotions négatives dans leurs dialogues avec la machine, sous l\u0027hypothèse qu\u0027un problème de dialogue génère un état émotionnel particulier chez le sujet.\nLa détection d\u0027émotions dans la parole est généralement traitée comme un problème d\u0027apprentissage supervisé. Cela s\u0027explique par le fait que les descripteurs utilisés sont relativement éloignés du concept d\u0027émotion, dans la pratique l\u0027étiquetage d\u0027exemples s\u0027avère nécessaire. La détection d\u0027émotions se limite généralement à une classification binaire, la prise en compte de labels plus fins pose le problème de l\u0027objectivité de l\u0027étiquetage (Liscombe et al., 2005). Dans ce cadre, les données sont coûteuses à acquérir et à étiqueter. L\u0027apprentissage actif peut diminuer ce coût en étiquetant uniquement les exemples jugés informatifs pour le modèle.\nCet article propose une approche d\u0027apprentissage actif pour la redirection automatique d\u0027appels. La première section présente le contexte de l\u0027étude ainsi que les données utilisées. Les différentes stratégies d\u0027apprentissage envisagées ainsi que le modèle utilisé sont traités dans la section 2. La dernière section est consacrée aux résultats obtenus et à leur discussion.\nClassification d\u0027émotions : caractérisation des données\nCet article se base sur des travaux antérieurs (Poulain, 2006) cherchant à caractériser au mieux des échanges vocaux en vue d\u0027une classification d\u0027émotions. Le but est de réguler le dialogue entre des utilisateurs et un serveur vocal. Cette étude porte plus particulièrement sur la pertinence des variables décrivant les données par rapport à la détection d\u0027émotions.\nLes données utilisées sont issues d\u0027une expérience mettant en jeu 32 utilisateurs qui testent un service boursier fonctionnant sur un serveur vocal. Du point de vue de l\u0027utilisateur, le test consiste à gérer un portefeuille fictif d\u0027actions, le but étant de réaliser la plus forte plus value. Les traces vocales obtenues constituent le corpus de cette étude, soit 5496 \"tours de parole\" échangés avec la machine. Les tours de parole sont caractérisés par 200 variables acoustiques, décrivant notamment la variation du volume sonore, la variation de la hauteur de voix, le rythme d\u0027élocution. Les données sont également caractérisées par 8 variables dialogiques décrivant notamment l\u0027âge du locuteur, le rang du dialogue, la durée du dialogue. Chaque tour de parole est étiqueté manuellement comme étant porteur d\u0027émotions positives ou négatives.\nLe sous-ensemble des variables les plus informatives vis-à-vis de la détection d\u0027émotions est déterminé grâce à un sélecteur bayésien naïf (Boullé, 2006). Au début de ce procédé, l\u0027ensemble des attributs est vide, à chaque itération on ajoute l\u0027attribut qui améliore au plus la qualité prédictive du modèle. L\u0027algorithme s\u0027arrête lorsque l\u0027ajout d\u0027attributs n\u0027améliore plus la qualité du modèle. Finalement, 20 variables ont été sélectionnées pour caractériser les échanges vocaux 1 . Dans le cadre de cet article, les données utilisées sont issues du même corpus et de cette étude antérieure. Chaque tour de parole est donc caractérisé par 20 variables. • L\u0027ensemble d\u0027apprentissage T constitué de couples \"instance, étiquette\" notés (u, f (u)).\n• La fonction Utile : X × M ? ? qui estime l\u0027utilité d\u0027une instance pour l\u0027apprentissage.\nAlgorithme 1: échantillonnage sélectif, Muslea (Muslea, 2002) 3 Classification active d\u0027émotions\nIntroduction\nLa mise en oeuvre d\u0027un système automatique de détection d\u0027émotions requiert générale-ment l\u0027entraînement d\u0027un classifieur. Ici, le modèle qui va classifier les émotions est réalisé grâce à un processus d\u0027apprentissage actif. A la différence de l\u0027apprentissage passif, qui utilise un ensemble de données déjà étiquetées, l\u0027apprentissage actif permet au modèle de construire lui-même son ensemble d\u0027apprentissage au cours de son entraînement. Parmi les stratégies d\u0027apprentissage actif existantes (Castro et Nowak, 2005), on se place dans le cadre de l\u0027échan-tillonnage sélectif où le modèle dispose d\u0027un \"sac\" d\u0027instances non étiquetées dont il peut demander les labels.\nMuslea (Muslea, 2002) a formalisé de manière générique l\u0027échantillonnage sélectif à travers l\u0027Algorithme (1). Celui-ci met en jeu la fonction Utile(u, M) qui estime l\u0027intérêt d\u0027une instance u ? U x pour l\u0027apprentissage du modèle M. La problématique centrale de l\u0027apprentissage actif est de \"préjuger efficacement\" de l\u0027intérêt des exemples avant de les étiqueter.\nLe choix du modèle\nLa grande variété des modèles capables de résoudre des problèmes de classification et parfois le grand nombre de paramètres nécessaires à leur utilisation rend souvent l\u0027apport d\u0027une stratégie d\u0027apprentissage difficile à mesurer. On choisit d\u0027utiliser une fenêtre de Parzen à noyau gaussien et de norme L2 (Parzen, 1962) car ce modèle prédictif n\u0027utilise qu\u0027un seul paramètre et est capable de fonctionner naturellement avec peu d\u0027exemples. La \"sortie\" de ce modèle est une estimation de la probabilité d\u0027observer l\u0027étiquette y i conditionnellement à l\u0027instance u n :\nLa valeur optimale (? 2 \u003d0.24) du paramètre du noyau a été déterminée grâce à une crossvalidation sur l\u0027erreur quadratique moyenne (Chappelle, 2005). Cette valeur est utilisée par la suite pour fixer le paramètre de la fenêtre de parzen.\nPour que le modèle puisse affecter une étiquettê f (u n ) à l\u0027instance u n , un seuil de décision noté Seuil(L x ) est calculé. Ce seuil minimise l\u0027erreur de prédiction 2 sur l\u0027ensemble d\u0027apprentissage. L\u0027étiquette attribuée estˆfestˆ estˆf (u n ) \u003d 1 si { ˆ P (y 1 |u n ) \u003e Seuil(L x )}, etˆfetˆ etˆf (u n ) \u003d 0 sinon. Puisque le seul paramètre de la fenêtre de Parzen est fixé, l\u0027apprentissage du modèle se réduit au \"comptage\" des instances (au sens du noyau gaussien). Cela permet de comparer uniquement les stratégies de sélection d\u0027exemples sans être influencé par l\u0027apprentissage du modèle.\nDeux stratégies d\u0027apprentissage actif\nLa première stratégie d\u0027apprentissage actif proposée a pour but de réduire l\u0027erreur de géné-ralisation du modèle, cette erreur peut être estimée par le risque empirique (Zhu et al., 2003).\nIci, le risque R(M)est définit comme étant la somme des probabilités que le modèle prenne une mauvaise décision sur l\u0027ensemble d\u0027apprentissage. On note P (y i |l n ) la probabilité réelle d\u0027observer la classe y i pour l\u0027instance l n ? L x . Le risque empirique s\u0027écrit alors selon l\u0027équa-tion 2, avec ½ la fonction indicatrice égale à 1 si f (l n ) \u003d y i et égale à 0 sinon. La fenêtre de parzen estime P (y i |l n ), on peut donc approximer le risque empirique en adoptant un apriori uniforme sur les P (l n ) (voir équation 3). Le but de cette stratégie est de sélectionner l\u0027instance non étiquetée u i ? U x qui minimisera le risque à l\u0027itération t + 1. On estime R(M +un ) le risque \"attendu\" après l\u0027étiquetage de l\u0027instance u n . Pour se faire, on se base sur les données étiquetées dont on dispose et on suppose que\n. L\u0027équation 4 montre comment agréger les estimations de risque selon les probabilités d\u0027observer chacune des classes. Pour exprimer la stratégie de réduction du risque sous forme algorithmique, il suffit de remplacer l\u0027étape (B) de l\u0027algorithme 1 par : \"Rechercher l\u0027instance q \u003d argmin u?UxˆRu?Uxˆ u?UxˆR(M +un )\".\nLa deuxième stratégie d\u0027apprentissage consiste à choisir l\u0027instance pour laquelle la prédic-tion du modèle est la plus incertaine possible. On considère que l\u0027incertitude d\u0027une prédiction est maximale quand la probabilité de sortie du modèle se rapproche du seuil de décision (voir équation 5). L\u0027algorithme correspondant à cette stratégie s\u0027obtient en remplaçant l\u0027étape (B) de l\u0027algorithme 1 par : \"Rechercher l\u0027instance q \u003d argmax u?Ux Incertain(u n )\".\nEn dehors de ces deux stratégies actives, une approche \"stochastique\" sélectionne uniformément les exemples selon leur distribution de probabilité. Cette dernière approche est notre juge de paix et tient lieu de référence pour mesurer l\u0027apport des stratégies actives. 4 . Selon les résultats de la figure 1, la \"réduction du risque\" est la stratégie qui maximise la qualité du modèle. En étiquetant 60 exemples grâce à cette approche, la performance du modèle est très proche du BER asymptotique 5 (l\u0027écart n\u0027est que de 0.04). La stratégie de \"maximisation de l\u0027incertitude\" n\u0027est pas performante au début de l\u0027apprentissage. Il faut étiqueter 180 exemples pour que cette stratégie donne de meilleurs résultats que l\u0027approche \"stochastique\". Cette approche à l\u0027avantage d\u0027être rapide, pour 300 exemples étiquetés on observe une performance comparable à la \"réduction du risque\", avec un temps de calcul 5 fois moins important.\nRésultats\nRésultats et Discussion\nLa figure 2 compare les performances d\u0027un modèle passif entraîné sur la totalité de l\u0027ensemble d\u0027apprentissage et de deux modèles actifs entraînés sur 100 exemples. Les résultats sont présentés sous la forme de courbes de lift réalisées sur l\u0027ensemble de test. Ces courbes montrent la proportion d\u0027émotions \"négatives\" détectées par le modèle, en considérant une certaine proportion de la population totale. Par exemple, le modèle entraîné grâce à la minimisation du risque détecte 74% des émotions négatives en utilisant 20% de la population totale (voir point \"A\" de la figure 2). La \"maximisation de l\u0027incertitude\" permet de détecter plus efficacement les émotions négatives que la \"réduction du risque\" (ce résultat ne prend pas en compte le taux de fausses alertes). Les deux modèles actifs offrent des performances proches de celle du modèle passif, en utilisant 37 fois moins d\u0027exemples d\u0027apprentissage. \nConclusion et perspectives\nCet article montre l\u0027intérêt de l\u0027apprentissage actif pour un domaine où l\u0027acquisition et l\u0027étiquetage des données sont particulièrement coûteux. Au vu des résultats obtenus lors de nos expériences, l\u0027apprentissage actif est pertinent pour la détection d\u0027émotions dans la parole.\n"
  },
  {
    "id": "909",
    "text": "Introduction\nAvec le développement des bibliothèques électroniques, il est devenu nécessaire de concevoir des méthodes automatiques pour la recherche de données pertinentes par rapport à une requête donnée. Pour de telles applications, il s\u0027agit plus d\u0027ordonner les exemples que de les discriminer.\nLa communauté d\u0027apprentissage a formulé cette problématique à travers le nouveau paradigme d\u0027apprentissage supervisé de fonctions d\u0027ordonnancement. Dans ce cas, il s\u0027agit d\u0027apprendre une correspondance entre un ensemble d\u0027instances et un ensemble d\u0027alternatives capable d\u0027ordonner les alternatives par rapport à une instance donnée. Par exemple, dans le cas de la recherche documentaire (RD), une instance représente une requête et les alternatives sont les documents concernés par cette requête et le but est d\u0027inférer un ordre partiel sur l\u0027ensemble des alternatives de façon à ce que les documents pertinents par rapport à la requête soient mieux ordonnés que les documents non-pertinents.\nDans ce papier, nous nous plaçons dans le cadre de l\u0027ordonnancement bipartite dans lequel les instances sont soit positives soit négatives et où il s\u0027agit d\u0027ordonner les instances positives au-dessus des instances négatives. Ce cadre restreint englobe de nombreuses applications de la recherche d\u0027information telle que le résumé automatique  ou la recherche de passages pertinents dans les systèmes de questions/réponses  et a ré-cemment fait l\u0027objet de plusieurs études aussi bien sur un plan pratique que théorique (Agarwal et Roth (2005); Rudin et al. (2005); Freund et al. (2003)).\nLe principal inconvénient de l\u0027apprentissage supervisé de fonctions d\u0027ordonnancement est que l\u0027étiquetage des instances nécessite l\u0027intervention d\u0027un expert qui doit examiner manuellement une grande quantité de données. Dans le cadre de la classification, la communauté d\u0027apprentissage s\u0027est intéressée depuis la fin des années 90 au problème d\u0027apprentissage semisupervisé qui consiste à prendre en compte les données étiquetées et non-étiquetées dans le processus d\u0027apprentissage. L\u0027originalité de notre approche est que nous proposons un algorithme d\u0027apprentissage semi-supervisé pour la tâche d\u0027ordonnancement bipartite. La plupart des algorithmes d\u0027ordonnancement semi-supervisés sont des techniques transductives à base de graphes, qui permettent d\u0027étiqueter les exemples non-étiquetés d\u0027une base test fixe. Nous préconisons une approche inductive à ce problème où il s\u0027agit d\u0027apprendre une fonction d\u0027ordonnancement à partir de deux bases d\u0027apprentissage, étiquetée et non-étiquetée, et qui est capable d\u0027ordonner de nouveaux exemples qui n\u0027ont pas été utilisés pour entraîner le modèle. Notre algorithme adopte une approche itérative en initialisant d\u0027abord une fonction d\u0027ordonnancement à partir des exemples étiquetés de la base d\u0027apprentissage et en apprenant la structure des données non-étiquetées de la base d\u0027apprentissage par une méthode transductive. Il répète ensuite deux étapes jusqu\u0027à ce que les critères de convergence ou d\u0027arrêt soient atteints. La première étape consiste à ordonner un sous-ensemble d\u0027exemples non-étiquetés avec la sortie de la fonction d\u0027ordonnancement et ensuite à calculer une dissimilarité entre cet ordre et celui inféré par la méthode transductive sur ce sous-ensemble. Dans la deuxième étape, l\u0027algorithme apprend une nouvelle fonction d\u0027ordonnancement à partir de l\u0027ensemble des données étiquetées et du sous-ensemble d\u0027exemples non-étiquetés trouvé à l\u0027étape précédente. Nous montrons l\u0027efficacité de cette approche pour la tâche RD.\nDans ce qui suit, nous reviendrons en section 2 à la tâche d\u0027ordonnancement bipartite dans le cas supervisé. Dans la section 3 nous présenterons notre algorithme d\u0027ordonnancement semi-supervisé et dans la section 4, nous présenterons les résultats obtenus sur la base CACM 1 constituée des titres et des résumés du journal Communications of the Association for Computer Machinery. Finalement nous discuterons des résultats obtenus en section 4.2.  \nOù [[pr]] est la fonction indicatrice valant 1 si le prédicat pr est vrai et 0 sinon. L\u0027erreur moyenne d\u0027ordonnancement R D1,D?1 (h) est la probabilité qu\u0027un exemple positif échantillonné aléatoirement suivant D 1 ait un score plus faible qu\u0027un exemple négatif échantillonné aléatoi-rement suivant D ?1 (Cortes et Mohri, 2003). L\u0027erreur empirique d\u0027ordonnancement correspondante de h sur une base d\u0027apprentissage S \u003d (S 1 , S ?1 ) est (Freund et al. (2003)) \nIl a été démontré que dans le cas où lim (x,x )?(0,0) C g (x, x , h) \u003d 1, minimiser (2) revient à minimiser (1) (Bartlett et Long, 1998;Clémençon et al., 2005). Nous allons présenter dans la section suivante l\u0027algorithme supervisé LinearRank optimisant le critère (2). Cet algorithme a été appliqué avec succès à la tâche de résumé automatique de textes .\nL\u0027algorithme supervisé LinearRank\nNous cherchons ici à apprendre les poids B \u003d (\n. L\u0027apprentissage de la fonction score h revient alors à trouver les poids B \u003d (? i ) qui optimisent le critère\nUn avantage d\u0027utiliser un coût exponentiel pour C g (x, x , h) et une fonction score linéaire est que le critère (3) peut se calculer avec une complexité linéaire par rapport au nombre Apprentissage semi-supervisé de fonctions d\u0027ordonnancement d\u0027exemples. En effet, le critère L exp peut s\u0027écrire dans ce cas comme suit :\nUn autre intérêt de la fonction de coût exponentiel est que des algorithmes d\u0027optimisation standard permettent d\u0027effectuer sa minimisation. Dans notre cas nous avons utilisé l\u0027algorithme LinearRank  qui est une adaptation de l\u0027algorithme iterative scaling déve-loppé pour la classification par (Lebanon et Lafferty, 2001). \nMéthode d\u0027ordonnancement transductive\nLes méthodes semi-supervisées qui ont été proposées en ordonnancement bipartite sont basées sur une hypothèse de variétés (Zhou et al., 2004b;Chu et Ghahramani, 2005;Agarwal, 2006). Une variété peut être définie comme un espace topologique qui est localement euclidien. Par exemple, toute ligne dans un espace euclidien est une variété de dimension 1 et toute surface constitue une variété de dimension 2. Les applications des variétés sont nombreuses en mathématiques et en physiques et ont été récemment introduites en apprentissage, principalement pour la tâche de discrimination. Les méthodes utilisant la notion de variété supposent que les exemples se trouvent sur une variété de dimension inférieure à l\u0027espace de départ et que les scores des exemples proches sur la variété sont assez similaires.\nCes algorithmes cherchent alors à exploiter la nature intrinsèque des données (c-à-d une variété) pour améliorer l\u0027apprentissage de la fonction de décision. Par exemple, les méthodes semi-supervisées faisant l\u0027hypothèse de variétés utilisent la grande quantité d\u0027exemples nonétiquetés pour pouvoir estimer cette structure. Pour ce faire, un graphe incorporant l\u0027information de voisinage local est construit avec une méthode telle que les K plus proches voisins. Les noeuds sont alors constitués des exemples étiquetés et non-étiquetés de la base d\u0027apprentissage et les poids reflètent la similarité entre les exemples voisins. La définition de cette similarité dépend des algorithmes proposés.\nAprès avoir estimé la variété, la plupart de ces méthodes s\u0027attachent à trouver les étiquettes des exemples non-étiquetés en exploitant directement le graphe en propageant par exemple les étiquettes des données étiquetées à leurs voisins non-étiquetés (Zhou et al., 2004a). Ces algorithmes ne peuvent ainsi pas étiqueter les exemples absents de la phase d\u0027apprentissage puisqu\u0027ils ne font pas parti des noeuds du graphe. Ces méthodes sont dites transductives par opposition aux méthodes inductives, qui sont capables d\u0027ordonner d\u0027autres exemples que ceux qui ont été utilisés pour apprendre.\nRécemment, des méthodes transductives à base de graphes ont été adaptées à la tâche d\u0027ordonnancement bipartite. Par exemple, (Zhou et al., 2004b) a adapté ses travaux de classification semi-supervisée (Zhou et al., 2004a) au cas d\u0027ordonnancement transdutif. Son algorithme construit d\u0027abord un graphe valué et non orienté en connectant petit à petit les points les plus proches jusqu\u0027à ce que le graphe devienne connexe. Il affecte ensuite un score pour chacune des instances, 1 pour les instances positives et 0 pour les autres. Les scores sont alors propagés à travers le graphe jusqu\u0027à la convergence. À la fin, les scores obtenus permettent d\u0027induire un ordre sur l\u0027ensemble des instances non-étiquetées. L\u0027algorithme proposé dans ce papier étant en partie basé sur cette méthode, nous allons le décrire plus en détails :\nSoit d : X × X ? R une métrique sur X et soit f la fonction score qui donne à chaque instance x i un score noté f i . f peut alors être vue comme un vecteur \n2 avec D la matrice diagonale telle que d ii est égal à la somme des éléments de la i ème ligne de W . -Construire le vecteur y telle que y i \u003d 1 si x i est une instance positive, 0 sinon.\nComme préconisé par (He et al., 2004), nous avons utilisé la méthode des K plus proches voisins dans la construction du graphe pour avoir plus de connexions entre les instances. En effet, la base CACM utilisée dans nos expériences comporte peu d\u0027exemples positifs pour chaque requête. Augmenter le nombre de connexions permet ainsi d\u0027augmenter l\u0027influence des exemples étiquetés positifs sur les scores des exemples non-étiquetés.\nLe modèle semi-supervisé inductif\nLa méthode supervisée LinearRank est une technique inductive dans le sens où, une fois le critère (3) optimisé, elle est capable d\u0027ordonner les instances non vues durant la phase d\u0027apprentissage. La méthode transductive quant à elle exploite la structure des données pour ordonner les instances en se basant sur leur similarité par rapport aux exemples positifs. Dans ce papier, nous nous intéressons à combiner les deux méthodes pour profiter de chacun de leurs avantages. Notre approche consiste à trouver un compromis entre optimiser le coût exponentiel (3) et respecter l\u0027ordre trouvé à partir de la variété sur un sous-ensemble de données étiquetées.\nNotre approche consiste dans un premier temps à apprendre (1) une fonction score h avec l\u0027algorithme LinearRank en minimisant le coût exponentiel sur l\u0027ensemble des données étique-tées et (2) un ordre total sur les données non-étiquetées avec la méthode transductive décrite dans la section précédente. La partie itérative de notre algorithme répète alors deux étapes jusqu\u0027à ce que le critère de convergence ou qu\u0027un nombre maximum d\u0027itérations soit atteint (algorithme 2) : la première étape consiste à sélectionner les n instances non-étiquetées les mieux ordonnées par la sortie de la fonction h. Suite à cela, nous calculons une dissimilarité entre l\u0027ordre trouvé par la fonction h et celui trouvé par la méthode transductive. À cette étape, nous faisons l\u0027hypothèse que l\u0027ordre sur ce sous-ensemble trouvé par la méthode transductive est plus pertinente que celle trouvée par la fonction h : la méthode transductive exploite en effet la structure des données. Nous définissons la dissimilarité entre les deux ordres par le nombre de paires de préférence différentes :\nj\u003d1 avec ?(j) une fonction qui retourne l\u0027index de l\u0027instance ordonnée au rang j par la méthode transductive. Dans une deuxième étape nous cherchons à trouver une nouvelle fonction score qui minimise le coût exponentiel régularisé L exp (S, h) + ?? exp (h, ?), où :\nj\u003d1 avec ? exp qui est la borne supérieure de (5). Le terme de régularisation ? permet de pondérer l\u0027apport des données non-étiquetées dans l\u0027apprentissage et il est fixé pour toutes les itérations de notre algorithme. Nous essayons ainsi de trouver une fonction d\u0027ordonnancement qui minimise le nombre de couples de préférence mal ordonnées et qui donne un ordre sur les instances non-étiquetées les mieux ordonnées le plus proche de celui trouvé par la méthode transductive.\nL\u0027algorithme général peut ainsi résumer par ce qui suit :\n) -Apprendre un ordre total sur les exemples non-étiquetés avec la méthode transductive de l\u0027algorithme 1 -t ? 0 répéter -Sélectionner les exemples non-étiquetés S (t)\nunl les mieux ordonnés par h (t) . -Produire la fonction index ? (t) à partir de la méthode transductive. -Apprendre une nouvelle fonction score en optimisant le coût exponentiel régularisé\nDans ce papier, nous avons utilisé la fonction coût exponentielle ainsi qu\u0027une mesure de dissimilarité de même nature. D\u0027autres fonctions de coût et de dissimilarité sont néanmoins envisageables en utilisant d\u0027autres fonctions convexes qui bornent la fonction indicatrice ( le logit par exemple). Un travail similaire au nôtre est celui de (Agarwal, 2006) qui a récemment proposé d\u0027étendre les travaux de (Belkin et Niyogi, 2004) au cadre de l\u0027ordonnancement bipartite semi-supervisé. L\u0027étude de (Agarwal, 2006) concerne plus un cadre d\u0027ordonnancement transductif mais l\u0027auteur propose d\u0027utiliser la technique développée dans (Sindhwani et al., 2005) pour rendre l\u0027algorithme inductif. Notre algorithme est inductif dans sa construction itérative ce qui présente l\u0027avantage d\u0027être plus rapide à l\u0027exécution.\nExpériences\nBase utilisée\nPour montrer de façon empirique que les instances non-étiquetées peuvent être utiles pour l\u0027ordonnancement bipartite, nous avons comparé notre algorithme à l\u0027algorithme supervisé LinearRank . Pour évaluer ces méthodes, nous nous sommes basés sur deux critères couramment utilisés dans la communauté de recherche d\u0027information : l\u0027aire sous la Courbe ROC (AUC) et la précision moyenne. Les expériences ont été menées sur la base CACM qui rassemble les titres et les résumés provenant du journal Communications of the Association for Computer Machinery (CACM).\nNous avons dans un premier temps prétraité les données pour chaque requête. Nous avons ainsi retiré pour chaque requête les documents pertinents ne contenant aucun mot de la requête. À partir de l\u0027ensemble des documents obtenu, un ensemble de test a été crée aléatoire-ment en sélectionnant la moitié des documents. Nous avons gardé uniquement les requêtes qui contenaient suffisamment de documents pertinents dans la base d\u0027apprentissage et de test. 12 requêtes ont été alors retenues.\nPour chaque requête, nous avons évalué les méthodes en formant cinq bases d\u0027apprentissage et de test différentes. La méthode semi-supervisée a été appliquée sur toute la base d\u0027apprentissage (étiquetée et non-étiquetée) en fixant un taux de données étiquetées permettant d\u0027avoir au moins une instance positive dans la partie étiquetée. Pour connaître l\u0027apport des données non-étiquetées, nous avons entraîné le modèle supervisé uniquement sur cet ensemble étiqueté. Les résultats obtenus en fixant les paramètres ? à 1, n et K à 10 ont été reportés dans le tableau 1. Les valeurs de test correspondent aux moyennes des résultats en AUC et en pré-cision moyenne sur les 5 bases d\u0027apprentissage et test considérées. Le tableau 2 donne les résultats moyennés par rapport aux requêtes.\nRésultats et discussion\nLes résultats obtenus sur la précision moyenne montrent que le modèle supervisé obtient de meilleurs performances que celui semi-supervisé. Ce résultat peut s\u0027expliquer par le fait que notre algorithme n\u0027optimise pas ce critère. Cependant le critère AUC a été de nombreuses fois utilisé en Recherche d\u0027Information. En effet, ce critère est plus facile à optimiser que la précision moyenne et plusieurs études ont montré que ces deux critères étaient en général fortement corrélés (Caruana et Niculescu-Mizil, 2004). Optimiser le critère AUC permet ainsi d\u0027optimiser la précision moyenne. Dans notre cas, le déséquilibre pourrait expliquer en partie les résultats que l\u0027on obtient. En effet, pour chaque requête, il existe un nombre très limité d\u0027exemples positifs. Pour améliorer ce critère, nous aurions pu aussi optimiser directement la précision moyenne (Metzler, 2005) ou un critère dérivé de l\u0027AUC (Rudin et al., 2005), qui permet à l\u0027algorithme de se concentrer sur les instances ordonnées de la liste.\nPar contre, les résultats obtenus sur la mesure AUC montrent que notre méthode semisupervisée obtient clairement de meilleures performances sur l\u0027ensemble des requêtes. Nous notons néanmoins une baisse conséquente pour la requête 17. En regardant de plus près les ré-sultats, nous avons remarqué que cette requête contient très peu de mots, ce qui pourrait ainsi biaiser la dissimilarité basée sur la variété. Néanmoins, nous obtenons des gains importants pour plus de la moitié des requêtes. En moyenne, l\u0027approche semi-supervisée permet ainsi un gain d\u0027environ 4, 2%.\nLes résultats empiriques obtenus sur l\u0027AUC montrent des résultats plus qu\u0027encourageant. En effet, notre méthode cherche à améliorer ce critère en utilisant des exemples non-étiquetés. Cependant, les résultats sur la précision moyenne sont plus surprenants. La baisse des performances sur ce critère que l\u0027on observe en moyenne et pour une grande partie des requêtes tempère les résultats obtenus sur l\u0027AUC. \nConclusion\nLa principale contribution de ce papier est une méthode d\u0027ordonnancement bipartite semisupervisée inductive. Notre approche est une combinaison d\u0027une méthode supervisée et d\u0027une méthode transductive à base de graphe. Elle est générale dans le sens où d\u0027autres fonctions coût que le coût exponentiel et d\u0027autres méthodes transductives peuvent être utilisées. Les résultats obtenus sur le critère AUC montrent une amélioration significative par rapport à la méthode supervisée, tendant à montrer ainsi l\u0027apport possible des exemples non étiquetés. Cependant, les résultats obtenus sur la précision moyenne montrent une dégradation des performances. Les deux critères étant généralement corrélés, ce résultat est assez surprenant. Pour la suite des travaux, nous allons ainsi tester notre algorithme sur d\u0027autres bases pour confirmer les performances obtenues. Ce papier étant une première étude pour la tâche d\u0027ordonnancement semisupervisée, nous avons uniquement fourni une partie pratique pour la tâche plus restreinte de l\u0027ordonnancement bipartite. Il est néanmoins à noter que les techniques inductives d\u0027apprentissage semi-supervisé ont été exclusivement développées dans le cadre de la classification (Amini et Gallinari, 2003) et que les résultats obtenus dans ce papier sont un bon présage quant à l\u0027utilisation des données non-étiquetées dans d\u0027autres cadres d\u0027apprentissage supervisé. Une direction intéressante à explorer serait d\u0027apprendre conjointement avec les données étiquetées et non-étiquetées pour la tâche de l\u0027extraction d\u0027information (Amini et al., 2000).\n"
  },
  {
    "id": "911",
    "text": "Introduction\nL\u0027algorithme des cartes auto-organisatrices de Kohonen, Kohonen (1994) représente un véritable outil de visualisation des données multidimensionnelles. Il permet de convertir des relations statistiques complexes et non-linéaires entre les données de grande dimension en une simple relation géométrique sur une topologie réduite. Cet algorithme permet de compresser l\u0027information tout en préservant les relations topologiques et métriques les plus importantes à partir de l\u0027espace des données primaires. De plus, l\u0027algorithme de Kohonen définit un niveau d\u0027abstraction par la possibilité d\u0027interprétation qui devient plus facile avec une carte bi-dimensionnelle, à la fois simple et significative comparée à l\u0027espace initial des données. Bien que l\u0027algorithme de Kohonen décrit une méthode connexionniste qui appartient à la famille des algorithmes neuronaux, il peut être formulé par une méthode de classification statistique type : nuées dynamiques. Ce formalisme transforme le problème d\u0027auto-organisation en un problème d\u0027optimisation. Dans ce contexte, nous décrivons une variante de classification neuronale non-supervisée proposée par Lebbah et al. (2000). Cette variante, appelée carte topologique binaires, est dédiée aux données qualitatives et consiste en la recherche d\u0027une classification automatique d\u0027un nuage de points App \u003d {(z i , p i ), i \u003d 1..N } où l\u0027individu z i \u003d (z1 i , z2 i , ..., z d i ) muni de la pondération p i appartient à l\u0027ensemble des données binaires\nCette variante s\u0027inspire de la version nuées dynamiques de Kohonen, Anouar (1998), mais utilise un critère spécifique pour déterminer l\u0027ordre topologique. Dans cet article, nous étudions le comportement des cartes topologiques binaires face à des données qualitatives présentant des valeurs manquantes. Ces données sont issues d\u0027une étude épidémiologique castémoins du cancer du nasopharynx (NPC). Cette base est constituée d\u0027une population divisée, de manière équitable, en deux cas : cancer et non-cancer. D\u0027une part, notre étude consiste à extraire des profils de gens atteints du NPC et des gens qui ne le sont pas et d\u0027autre part, elle consiste à déterminer pour chaque profil extrait, l\u0027ensemble des variables explicatives de la population qui lui est associée. L\u0027objectif de cette étude vise donc à détecter dans le profil statistique général du NPC, des profils \"types\" sous formes de groupes de population homogènes. Chaque groupe, étant un représentant d\u0027un cas particulier de la population globale, est muni d\u0027un ensemble de caractéristiques résultants de la classification non-supervisée faite par les cartes topologiques binaires et optimisée par une classification statistique automatique.\nDonnées qualitatives et codage\nIl existe de nombreuse variables, dites discrètes, ne pouvant prendre par nature qu\u0027un nombre restreint de valeurs Marchetti (1989). Citons par exemple les variables associées aux caractéristiques physiques tel que la taille (grande, moyenne, petite) ou encore à la situation familiale (célibataire, veuf, divorcé, marié). Les variables ainsi définies sont appelées variables qualitatives. Elle se répartissent en deux groupes : les variables qualitatives ordinales et les variables qualitatives nominales. Si l\u0027on utilise un codage adapté, les données qualitatives deviendront des données binaires. Les codages utilisés le plus souvent sont : (a)Le codage binaire additif : Ce codage permet essentiellement de rester cohérent avec la notion d\u0027ordre entre les modalités d\u0027une variable.(b) Le codage disjonctif complet : Ce tableau résulte de la transformation, par le codage disjonctif complet, d\u0027un tableau de variables qualitatives nominales encore appelé questionnaire multiple. Une seule modalité est choisie pour chaque variable, TAB.1.\nQue les données initiales soient dans l\u0027espace des données avec modalités ou après transformation dans l\u0027espace des données binaires, nous aboutissons à des caractéristiques identiques Leich et al. (1998). L\u0027espace des données binaires peut être muni de la distance euclidienne, il est souvent beaucoup plus intéressant de le munir de distances adaptées permettant de mieux traduire ses particularités. Dans ce papier nous utilisons la distance de Hamming appelée H.  \nmise :\nLes données étant binaires, ? j est la médiane binaire de l\u0027ensemble des valeurs prises par la variable j sur l\u0027ensemble des individus. La médiane est la valeur 1 ou 0 correspondant à la plus grande sommation des pondérations de la valeur 1 et 0. Dans le cas particulier où z i sont munis d\u0027une même pondération (p i \u003d 1, ?i) la règle fournit une médiane ayant une interprétation particulièrement simple, ? j est alors la valeur 0 ou 1 la plus souvent choisie par les individus sur la variable j.\nValeurs manquantes\nLe problème des valeurs manquantes est un véritable problème de recherche. Ceci étant, il existe un certain nombre de façons pour détourner ce problème, par exemple en les remplaçant par la médiane ou par l\u0027apprentissage d\u0027un prédicteur automatique,...etc. Cependant, dans notre cas, les variables sont qualitatives. Nous proposons, pour une raison de simplicité, de définir une modalité supplémentaire pour les valeurs manquantes. Cette pseudo-solution, ne pose aucun problème dans le cas d\u0027un codage disjonctif. Cependant, le problème se pose pour le codage additif où l\u0027ordre entre les modalités est important. Nous proposons donc de définir la modalité des valeurs manquantes de telle façon à ce qu\u0027elle soit la plus proche de la médiane entre toutes les autres valeurs de la variable.\nLa carte topologique binaire\nNous rappelons ici comment l\u0027utilisation de la médiane peut permettre de définir un modèle de carte auto-organisatrice adapté aux données binaires. Comme pour le modèle classique des cartes topologiques, nous utilisons un réseau de neurones avec une couche d\u0027entrée pour les entrées et une carte possédant un ordre topologique de k cellules. La prise en compte dans la carte C de la notion de proximité impose de définir une relation de voisinage topologique. Les neurones sont répartis aux noeuds d\u0027un maillage. Comme dans le cas de l\u0027algorithme de Kohonen nous définissons la topologie de la carte à l\u0027aide d\u0027un graphe non orienté et la distance ?(c, r) entre deux cellules c et r étant la longueur du chemin le plus court qui sépare la cellule c et r. Afin de modéliser la notion d\u0027influence d\u0027un neurone r sur un neurone c, qui dépend de leur proximité, on utilise une fonction à la fonction noyau\nL\u0027influence mutuelle entre deux cellules c et r est définie par la fonction K(?(c, r)). A chaque cellule c de la grille est associé un vecteur de poids binaire w c de dimension d. L\u0027ensemble des poids associés constitue l\u0027ensemble des référents noté W. L\u0027auto-organisation de la carte va maintenant se faire à l\u0027aide du formalisme des nuées dynamiques et donc par l\u0027intermédiaire de la minimisation d\u0027une fonction de coût.\nPour utiliser l\u0027algorithme des nuées dynamiques, Diday et C.Simon. (1976), nous avons utilisé la fonction de coût E(?, W) déjà définit dans Lebbah et al. (2000), adaptées aux traitements des données binaires . Donc la fonction de coût à minimiser est alors :\nzi?App r?C\nOù ? affecte chaque observation z à une cellule unique de la carte C. La minimisation de la fonction de coût est réalisée à l\u0027aide d\u0027une procédure itérative en deux phases :\n1. Phase d\u0027affectation : mise à jour de la fonction d\u0027affectation ? associée à l\u0027ensemble W fixé. On affecte chaque observation z au référent défini à partir de l\u0027expression suivante :\n2. Phase d\u0027optimisation : La fonction d\u0027affectation étant fixée à sa valeur courante, choisir le système de référents qui minimise la fonction E(?, W) dans l\u0027espace ? m . ce point n\u0027est autre que le centre médian de App lorsque chaque observation z i est pondérée\nLa minimisation de E(?, W) s\u0027effectue par itérations successives jusqu\u0027à stabilisation des deux phases.\nDans la pratique nous avons utilisé une fonction noyaux\n) en faisant varier le paramètre T entre deux valeurs T max et T min . On obtient alors pour décrire la carte un ensemble de référents binaires W. Ces référents sont du même genre que les données initiales : Le décodage (additif ou exclusif ) de différents vecteurs permet l\u0027interprétation symbolique des référents trouvés.\nRésultats\nNous appliquons la méthode aux données d\u0027une étude épidémiologique cas-témoins du cancer du nasopharynx (NPC). Pour clarifier le rôle de l\u0027environnement dans l\u0027étiologie du NPC, le CIRC a mené en 2004 une étude cas-témoins multicentrique dans la région endé-mique du Maghreb. Le NPC présente une incidence très variable selon les régions du monde. C\u0027est un cancer relativement rare sauf en Chine, en Asie du sud-est et au Maghreb, où les taux d\u0027incidence sont élevés. Dans ces régions, le NPC est un problème majeur de santé publique. Les études ont suggéré l\u0027existence d\u0027un grand nombre de facteurs de risques environnementaux incluant habitudes alimentaires et environnement domestique et professionnel. Afin de focaliser la présente analyse sur la forme adulte du NPC seuls les individus âgés de plus de 35 ans recrutés dans lŠ étude ont été sélectionnés, soit un total de 986 individus, dont 499 sont atteint par le cancer (les cas), et 487 ne le sont pas (les témoins). Chaque individu est décrit par 61 caractères.\nDans les 61 caractères, il y a la variable à expliquer 1-\" NPC \" (c\u0027est la variable du cancer) ; les autres sont les variables explicatives. Lexique : 2-âge, 3-sexe, 4-niveau dŠinstruc-tion, 5-catégorie professionnelle, 6-habitat dans lŠenfance, 7-habitat a lŠage adulte, 8-parents consanguins, 9-fréquentes otites, 10-fréquentes angines, 11-fréquentes rhume, 12-asthme, 13-eczéma, 14-allergie, 15-exposition aux engrais chimiques et pesticides, 16-exposition aux produits chimiques, 17-exposition aux fumées, 18-exposition aux poussières, 19-exposition aux formaldéhyde, 20-consommation dŠalcool, 21-consommation de tabac, 22-consommation de neffa, 23-consommation de cannabis, 24-25-type de logement enfant. et adulte., 26-27-lits séparés enfant. et adulte., 28-29-animaux dans la maison enfant. et adulte., 30-31-aération cuisine enfant. et adulte., 32-33-aération maison enfant. et adulte., 34-35-exposition aux fumées d\u0027encens enfant. et adulte., 36-37-exposition aux fumées de kanoun et tabouna enfant. et adulte., 38-39-exposition aux fumées de feu de bois enfant. et adulte., 40-41-42-allaité et age au sevrage et modalité de sevrage, 43-contact avec la salive adulte par le sol ou les aliments, 44-traitements traditionnels dans lŠenfance, 45-consommation de piment, 46-47-consommation de smen et graisse enfant. et adulte., 48-49-légumes fruits agrumes enfant. et adulte., 50-51-harrissa maison enfant. adulte., 52-53-harrissa industrielle enfant. adulte., 54-55-protéines maison enfant. adulte., 56-57-protéines industrielles enfant. adulte., 58-59-conserves légumes industrielles enfant. adulte., 60-61-conserves légumes maison enfant. adulte.\nNous appliquons la variante des cartes topologiques binaires sur les données décrites cidessus avec une architecture de (10 × 10) cellules. l\u0027apprentissage de cette carte fournit pour chaque cellule un référent w c prenant en compte les deux codages : disjonctif et additif. Fig.1 représente la distribution de la population sur la carte. Fig.2 représente la répartition en distinguant les cas cancer et cas non-cancer. Grâce à la cartographie obtenue, nous pouvons déjà effectuer quelques analyses sur la répartition des individus. En effet, il existe 5 neurones vides, i.e. des neurones qui n\u0027incluent aucun individu. Ces neurones représentent la propriété de lis-\nFIG. 1 -Carte topologique 10 × 10 avec les cardinalites des neurones\nsage, une des propriétés fortes qui caractérisent les cartes auto-organisatrices. Les neurones sont mélangés, ce qui veut dire que quelques individus n\u0027ayant pas le cancer, même s\u0027ils sont minoritaires dans un neurone, peuvent avoir les mêmes caractéristiques que les individus ayant le cancer ! Pour optimiser le nombre de neurones obtenu dans la carte, nous avons appliqué la méthode des K-means sur les référents avec différentes partitions et nous avons choisi celle qui minimise le mieux l\u0027indice de boulding. Le nombre de classes optimal trouvé étant égal à 6 (Fig.3). A partir de l\u0027indice optimal indiqué dans cette dernière figure, nous avons partitionné la carte en 6 grandes classes (Fig.4). Dans cette figure, nous avons constaté les statistiques suivantes : Classe 1 : la zone bleue qui regroupe 13% de la population dont 67% ayant le cancer. Classe 2 : La zone mauve, regroupant 13% de la population dont 53% ayant la cancer. Classe 3 : la zone verte représentant 25% de la population dont 65% de Non-cancer. Classe 4 : zone rouge, représentant 18% dont 60% ayant le cancer. Classe 5 et 6 (31% de la population) : zones jaune et orange, représentent des zones de conflit, que nous ne pouvons interpréter.\nAu vu de ces résultats, deux classes attirent notre attention : la classe 1 et la classe 3 en raison de : 1) la proportion anormale d\u0027individus atteints du cancer au regard des 50% dans la population d\u0027origine, et 2) le nombre significatif d\u0027individus dans ces classes. Pour caractériser les profils des individus dans ces classes, nous avons étudié la distribution des variables explicatives. La divergence de Kullback-Leibler Bishop (2006) notée KL(p||q) (i.e., ? x p(x) ln(q(x)/p(x)) dans le cas discret) est classiquement utilisée pour mesurer la dissimilarité entre la distribution d\u0027origine p(x) d\u0027une variable et celle observée dans la classe considérée, q(x). Aussi, nous avons classé les 60 variables explicatives dans chaque classe dans l\u0027ordre décroissant de la divergence KL. Le principe sous-jacent est de dire qu\u0027une variable est d\u0027autant plus discriminante que sa distribution dans la classe considérée est significativement modifiée par rapport à la distribution d\u0027origine.\nDans la classe 1 pour laquelle il y a une sur-représentation des individus atteints du NPC, Plus précisément, on observe dans cette classe que 60% des individus ont consommé de l\u0027harrissa à l\u0027enfance alors que ce taux n\u0027est que de 20% dans la population étudiée. Dans la classe 3 caractérisé par un sous-représentation des individus atteints du NPC, on observe que la variable 26 a une valeur de KL élévée (0.8) suivie de 32, 30 et 24 (0.38, 0.32, 0.31) au regard des autres (\u003c0.25). La variable 26 est associée au lits séparés à l\u0027enfance, les autres portent sur l\u0027aération de la maison, de la cuisine et de la catégorie du logement à l\u0027enfance. On observe dans cette classe que 90% des individus avaient des lits séparés à l\u0027enfance alors que ce taux n\u0027est que de 40% dans la population étudiée. De même, 22% ont vécu dans une maison bien aérée durant l\u0027enfance alors que ce taux n\u0027est que de 2% dans la population étudiée.\nA titre d\u0027exemple, nous illustrons pour la classe 1, la distribution de la variable 50 (Harissa maison enfant) sur la carte topologique obtenue (Fig.5). Cette figure représente l\u0027impact des modalités de la variable 50 sur les individus atteins du NPC. La première modalité est distribuée sur tous les neurones de la carte, ce qui est normal à cause du codage additif de la variable. La quatrième modalité n\u0027est représentée sur aucun neurone. Il s\u0027agit d\u0027une modalité additionnelle pour représenter les valeurs manquantes de la variable. La deuxième et la troisième modalités sont fortement présentes dans la classe étudiée (zone bleue).\nAu final, ce type d\u0027analyse nous renseigne sur l\u0027existence de profils statistiques distincts dans la population étudiée. L\u0027analyse des profils dans chacune des classe nous a permis d\u0027identifier des facteurs corrélés avec le cancer (ou l\u0027absence du cancer). Par ce type d\u0027analyse nous avons pu dresser des profils statistiques sémantiques des différentes catégories d\u0027individus atteints ou non du cancer et ainsi extraire l\u0027ensemble de variables explicatives de chaque profil.\nConclusion\nDans ce papier, nous avons présenté un système connexionniste pour l\u0027extraction de profils cas témoins du cancer du nasopharynx (NPC) à partir des données issues d\u0027une étude épi-démiologique. Ce système utilise un codage spécifique aux données qualitatives représentant des valeurs manquantes. Basé sur une carte topologique binaire, le système ainsi développé a permis d\u0027une part, de trouver des groupes homogènes à partir d\u0027une population globale regroupant des cas cancer et non-cancer et d\u0027autre part, d\u0027extraire les variables explicatives de chaque profil extrait. Nous avons pu grâce à ce système fournir aux épidémiologistes un outil d\u0027aide à la décision qui leurs fournit un véritable outil de visualisation à partir des données multidimensionnelles. Cet outil permet également, d\u0027éclater le profil général de la population des gens atteints du NPC en un ensemble de profils \"type\". Chaque profil étant caractérisé par un ensemble de variables explicatives des cas cancer ou non-cancer. Le système développé sera prochainement comparé avec d\u0027autres méthodes d\u0027extraction de connaissances.\n"
  },
  {
    "id": "912",
    "text": "Introduction\nLe problème de réconciliation de références est un problème majeur pour l\u0027intégration ou la fusion de données provenant de plusieurs sources. Il consiste à décider si deux descriptions provenant de sources distinctes réfèrent ou non à la même entité du monde réel (e.g., la même personne, le même article, le même gène, le même hôtel).\nIl est très difficile d\u0027attaquer ce problème dans toute sa généralité car les causes d\u0027hétéro-généité dans la description de données provenant de différentes sources sont variées et peuvent être de nature très différente. L\u0027hétérogénéité des schémas est une des cause premières de la disparité de description des données entre sources. De nombreux travaux, dont on peut trouver une synthèse dans Rahm et Bernstein (2001); Shvaiko et Euzenat (2005); Noy (2004) ;Euzenat et Valtchev (2004), ont proposé des solutions pour réconcilier des schémas ou des ontologies par des mappings. Ces mappings peuvent ensuite être utilisés pour traduire des requêtes de l\u0027interface de requêtes d\u0027une source vers l\u0027interface de requête d\u0027une autre source.\nL\u0027homogénéité ou la réconciliation de schémas n\u0027empêchent cependant pas les variations entre les descriptions des instances elles-mêmes. Par exemple, deux descriptions de personnes avec les mêmes attributs Nom, Prénom, Adresse peuvent différer sur certaines valeurs de ces attributs tout en référant à la même personne, par exemple, si dans l\u0027un des tuples le prénom est en entier alors que dans l\u0027autre tuple il n\u0027est donné qu\u0027en abrégé.\nLes travaux en nettoyage de données qui visent la détection de doublons dans des bases de données sont confrontés exactement à ce problème. La plupart des travaux existants (e.g., Galhardas et al. (2001); Bilenko et Mooney. (2003); Ananthakrishna et al. (2002)) se fondent sur des comparaisons entre chaînes de caractères pour calculer la similarité entre valeurs d\u0027un même attribut, puis calculent la similarité entre deux tuples en combinant les similarités trouvées entre les valeurs de chaque attribut de ces deux tuples. Dans l\u0027approche proposé par Benjelloun et al. (2006) la comparaison de références est générique mais reste une comparaison locale deux à deux. Quelques travaux très récents (Bhattacharya et Getoor. (2004); Kalashnikov et al. (2005); Dong et al. (2005); Singa et Domingos. (2005)) ont une approche globale exploitant les dépendances qui peuvent exister entre réconciliations de références. Souvent, ces dépendances découlent de la sémantique du domaine. Par exemple, la réconciliation entre deux références à des cours décrits par leur intitulé et le nom de l\u0027enseignant responsable peut entraîner la réconciliation entre deux références à des personnes. Cela nécessite l\u0027explicitation de connaissances supplémentaires sur le domaine d\u0027application, comme le fait qu\u0027un enseignant est une personne, et qu\u0027un cours est identifié par son intitulé et n\u0027a qu\u0027un enseignant responsable. Dans Dong et al. (2005), des connaissances du domaine de ce type sont prises en compte mais doivent être codées dans le poids des arcs du graphe de dépendances dont les noeuds correspondent aux paires de références potentiellement réconciliables.\nDans cet article, nous étudions le problème de réconciliation de références dans le cas où les données à réconcilier sont décrites relativement à une même ontologie, vue comme un schéma sémantiquement riche, décrit en RDFS(http ://www.w3.org/TR/rdf-schema/ ) étendu par certaines primitives de OWL-DL (http ://www.w3.org/2004/OWL). OWL-DL sert à poser des axiomes qui enrichissent la sémantique des classes et des propriétés déclarées en RDFS. On peut ainsi par exemple exprimer que deux classes sont disjointes ou que telle ou telle propriété (ou son inverse) est fonctionnelle. Nous montrons l\u0027intérêt d\u0027une approche logique basée sur des règles de réconciliation qui peuvent être générées automatiquement à partir des axiomes du schéma. Ces règles traduisent de façon déclarative les dépendances entre réconciliations qui découlent de la sémantique du schéma. Elles permettent d\u0027inférer de façon sûre des réconcilia-tions ainsi que des non réconciliations. Nous obtenons ainsi une méthode ayant une précision de 100%, et nous montrons que le rappel est augmenté de façon significative si on enrichit la sémantique du schéma en ajoutant des axiomes. Cette méthode permet d\u0027obtenir comme produit dérivé direct un dictionnaire de synonymie entre chaînes de caractères. Ce dictionnaire peut être exploité dans une phase ultérieure de réconciliation des paires de références non traitées par la méthode logique et pour lesquelles nous envisageons une méthode numérique dont nous décrivons brièvement le principe à la fin de l\u0027article.\nL\u0027article est organisé de la façon suivante. La section 2 définit le modèle de données (RDFS+) et le problème de réconciliation de références que nous considérons. La section 3 décrit la méthode logique que nous proposons qui repose sur des règles d\u0027inférences traduisant les contraintes du schéma en des dépendances logiques entre réconciliations de référence. La section 4 fournit le résultat d\u0027expérimentations que nous avons effectué sur deux jeux de données : celui de CORA qui sert de benchmark dans plusieurs travaux de nettoyage de données ; et un jeu de données fourni par France Telecom R\u0026D dans le cadre du projet PICSEL3. La section 5 conclue l\u0027article en situant notre approche par rapport à l\u0027existant et en indiquant quelques perspectives.\nDéfinition du problème\nNous décrivons d\u0027abord le modèle des données que nous considérons, que nous appelons RDFS+ car il étend RDFS avec des primitives de OWL-DL. D\u0027un point de vue \"bases de données\", RDFS+ peut être vu comme un fragment du modèle relationnel (restreint à des relations unaires et binaires) enrichi par la possibilité d\u0027exprimer des contraintes de typage, d\u0027inclusion ou d\u0027exclusion entre relations, et de dépendances fonctionnelles.\nLe modèle de données RDFS+\nLe schéma : Nous considérons que nous disposons d\u0027un schéma RDFS consistant en un ensemble de classes (relations unaires) structurées en une taxonomie et d\u0027un ensemble de propriétés (relations binaires) qui peuvent être elles-mêmes structurées en une taxonomie de propriétés. Les propriétés sont typées. Dans la terminologie RDFS, on distingue les propriétés qui sont des relations, dont les domaines et co-domaines sont des classes, de celles dont le co-domaine est un ensemble de valeurs de base (numériques ou alpha-numériques), et qu\u0027on appelle des attributs. On notera :\n-R(C, D) pour indiquer que le domaine de la relation R est la classe C et que son codomaine est la classe D, et -A(C, Litteral) pour indiquer que l\u0027attribut A a comme domaine C et comme codomaine un ensemble de valeurs (numériques ou alpha-numériques).\nLes axiomes : Nous donnons la possibilité de déclarer des axiomes OWL-DL pour enrichir la sémantique d\u0027un schéma RDFS. Les axiomes que nous considérons sont de plusieurs types. Nous ne donnons pas leur notation standard en XML, très verbeuse, mais nous précisons leur sémantique logique.\n-Axiomes de disjonction entre classes. Nous notons DISJOIN T (C, D) l\u0027axiome dé-clarant que les classes C et D sont disjointes, dont la sémantique logique est : ?X C(X) ? ¬D(X). -Axiomes de fonctionnalité d\u0027une propriété. Nous notons P F (P ) l\u0027axiome déclarant que la propriété P (relation ou attribut) est fonctionnelle, dont la sémantique logique est :\nl\u0027axiome déclarant que l\u0027inverse de la propriété P (relation ou attribut) est fonctionnelle, dont la sémantique logique est :\nLes données : Une donnée a un identifiant (appelé référence) et une description qui est l\u0027ensemble des faits RDF (http ://www.w3.org/RDF/ ) qui mentionnent cet identifiant. Un fait RDF est : -soit un fait-classe de la forme C(i) où i est un identifiant, -soit un fait-relation de la forme R(i 1 , i 2 ) où R est une relation et i 1 et i 2 sont des identifiants, -soit un fait-attribut de la forme A(i, v) où A est un attribut, i un identifiant et v une valeur (numérique ou alpha-numérique).\nNous supposons que les données peuvent provenir de plusieurs sources et nous préfixons l\u0027identifiant d\u0027une donnée par l\u0027identifiant de la source dont elle provient. Sauf mention explicite, nous posons par défaut l\u0027hypothèse du nom unique sur chaque source. Cette hypothèse (notée UNA) a la sémantique suivante : deux données d\u0027une même source ayant des identifiants distincts font référence à des entités distinctes du monde réel.\nExemple\nAfin d\u0027illustrer le modèle de données RDFS+, nous montrons ici un exemple de schéma RDFS, un ensemble d\u0027axioms OWL-DL et enfin un exemple de données conformes à ce schéma qui porte sur le domaine des lieux culturels. Nous utiliserons pour l\u0027exemple la notation graphique de RDFS. Ces axiomes expriment, par exemple, qu\u0027une peinture ne peut être contenue que dans un seul musée et qu\u0027un nom de peinture n\u0027est associé qu\u0027à une seule peinture.\nFIG. 1 -Exemple de schéma RDFS\nSoient S1 et S2 deux sources de données RDF conformes au schéma RDFS de la figure 1 et vérifiant les axiomes ci-dessus. Nous présentons le contenu de ces deux sources sous forme d\u0027un ensemble de faits RDF.\nLa source S1 :\nCulturalPlace(S1_m1) ; Painting(S1_p1) ; Artist(S1_a1) ; Museum(S1_m2) ; PaintingName(S1_p1,\"La Joconde\") ; PaintedBy(S1_p1, S1_a1) ; Contains(S1_m1, S1_p1), MuseumName(S1_m1,\"musée du LOUVRE\") ; ArtistName(S1_a1, \"Leonard De Vinci\") ; YearofBirth(S1_a1,\"1452\") ; Painting(S1_p2) ; PaintingName(S1_p2,\"La Cene\") ; PaintedBy(S1_p2, S1_a1), Painting(S1_p3) ; PaintingName(S1_p3,\"Sainte Anne\") ; PaintedBy(S1_p3, S1_a1) ; MuseumName(S1_m2, \"musée des arts premiers\") ; Located(S1_m2, S1_c1) ; CityName(S1_c1,\"Paris\") ; MuseumAddress(S1_m2, \"quai branly\") La source S2 : Museum(S2_m1) ; Museum(S2_m2) ; Artist(S2_a1) ; MuseumName(S2_m1,\"Le LOUVRE\") ; Located(S2_m1,S2_c1) ; CityName(S2_c1, \"Ville de paris\") ; Contains(S2_m1, S2_p1) ; PaintingName(S2_p1, \"la Joconde\") ; Contains(S2_m1,S2_p2) ; PaintedBy(S2_p2,S2_a1) ; PaintingName(S2_p2, \"Vierge aux rochers\") ; ArtistName(S2_a1, \"De Vinci\") ; YearofBirth(S2_a1,\"1452\") ; Contains(S2_m1,S2_p3) ; Located(S2_m2,S2_c1) ; MuseumName(S2_m2,\"Musée du quai Branly\") ; PaintingName(S2_p3, \"Sainte Anne, la vierge et l\u0027enfant jésus \"), MuseumAddress(S2_m2, \"37 quai branly, portail Debilly\"),\nFIG. 2 -Exemple de données RDF\nLe problème de réconciliation\nSoient S 1 et S 2 deux sources de données ayant le même schéma RDFS+. Soient I 1 et I 2 les deux ensembles d\u0027identifiants de leurs données respectives.\nLe problème de réconciliation entre S 1 et S 2 consiste à partitioner l\u0027ensemble I 1 × I 2 des paires de références en 2 sous-ensembles Reconcile et N onReconcile regroupant respectivement les paires de références représentant une même entité, et les paires de références représentant deux entités différentes.\nDans la suite de l\u0027article, on utilisera la notation relationnelle plutôt que la notation ensembliste : Reconcile(i1, i2) (respectivement N onReconcile(i1, i2)) pour (i1, i2) ? Reconcile (respectivement pour (i1, i2) ? N onReconcile ).\nUne méthode de réconciliation est totale si elle produit un résultat (Reconcile(i1, i2) ou N onReconcile(i1, i2)) pour tout couple (i 1 , i 2 ) ? I 1 × I 2 .\nLa précision d\u0027une méthode de réconciliation est la proportion, parmi les couples pour lesquels la méthode a produit un résultat (de réconciliation ou de non réconciliation), de ceux pour lesquels le résultat est correct.\nLe rappel d\u0027une méthode de réconciliation est la proportion, parmi tous les couples possibles de I 1 × I 2 , de ceux pour lesquels la méthode a produit un résultat correct.\nLa méthode de réconciliation que nous décrivons dans la section suivante est une méthode de réconciliation partielle qui a la caractéristique d\u0027être globale et fondée sur la logique : elle traduit les axiomes du schéma par des règles logiques de dépendances entre réconciliations. L\u0027intérêt d\u0027une approche logique est qu\u0027elle garantit une précision de 100%. Notre expérimen-tation se focalise donc sur l\u0027estimation du rappel.\nMéthode logique de réconciliation à base de règles\nNotre approche consiste à traduire les axiomes associés au schéma, incluant l\u0027UNA (quand elle s\u0027applique), par des règles logiques (Section 3.1), et à appliquer un algorithme de raisonnement pour inférer des réconciliations et des non réconciliations (Section 3.2), ainsi que des synonymies entre valeurs de base qui seront conservées dans un dictionnaire (Section 3.3).\nGénération des règles de réconciliation et de non réconciliation\nTraduction de l\u0027hypothèse du nom unique par des règles de non réconciliation : On introduit les prédicats unaires src1 et src2 pour typer chaque référence en fonction de sa source d\u0027origine (srci(X) signifie que la référence X provient de la source Si). La contrainte de l\u0027UNA au niveau des sources S1 et S2 se traduit par les quatre règles suivantes :\nLes deux premières règles traduisent la non réconciliation de deux références provenant d\u0027une même source. Les deux dernières traduisent le fait qu\u0027une référence provenant d\u0027une source S1 (resp. S2) peut être réconciliée avec au maximum une référence de la source S2 (resp. S1). -Pour toute relation R déclarée comme fonctionnelle par un axiome PF(R), la règle R6.1(R) est générée, qui traduit le fait que pour une instance de la classe du domaine de R il existe au plus une instance du co-domaine.\nR6.1(R) : Reconcile (X, Y) ? R(X, Z) ? R(Y, W)\n? Reconcile(Z,W) -Pour tout attribut A déclaré comme fonctionnel par un axiome PF(A), la règle R6.2(A) est générée, qui exprime que pour une instance de la classe du domaine de A il existe au plus une valeur de base appartenant au co-domaine. Le prédicat binaire EquiVals permet d\u0027exprimer que deux valeurs de base sont synonymes. Il est l\u0027équivalent sur les valeurs de base du prédicat Reconcile.\nR6.2(A) : Reconcile (X, Y) ? A(X, Z) ? A(Y, W) ? EquiVals(Z,W)\n-Pour toute relation R déclarée comme fonctionnelle inverse par un axiome PFI(R), la règle règle R7.1(R) est générée, qui exprime que pour une instance de la classe du co-domaine de R il existe au plus une instance du domaine.\nR7.1(R) : Reconcile (X, Y) ? R(Z, X) ? R(W, Y) ? Reconcile(Z,W)\n-Pour tout attribut A déclaré comme fonctionnel inverse par un axiome PFI(A), la règle R7.2(A) est générée, qui traduit le fait que pour deux valeurs de base synonymes appartenant au co-domaine il existe au plus une instance de la classe du domaine.\nR7.2(A) : EquiVals (X, Y) ? A(Z, X) ? A(W, Y) ? Reconcile(Z,W)\nRemarque. On peut traduire par des règles analogues aux règles précédentes des dépen-dances fonctionnelles impliquant plusieurs relations ou attributs.\nInférence de réconciliations et de non réconciliations\nL\u0027algorithme du chaînage avant est appliqué sur la base de connaissances composée de la base des règles présentées ci-dessus et de la base de faits contenant : -l\u0027ensemble de faits-classe, faits-relation et de faits-attribut représentant les descriptions de l\u0027ensemble de références des deux sources étendue par les propriétés obtenues par héritage ; -un fait de type src1(X) ou src2(X) pour toute référence X permettant de représenter sa provenance ; -un ensemble de faits instance du prédicat EquiVals(X,Y) qui expriment l\u0027égalité à une normalisation près (élimination des ponctuations, des mots vides) des valeurs de base. Ainsi, le fait EquiVals(\"La Joconde\", \"la joconde\") est posé car les deux chaines de caractères ne diffèrent que par deux majuscules.\nEn nous appuyant sur les données de la figure 2 et sur le schéma de la figure 1, nous allons montrer comment les décisions de réconciliations et de non réconciliation se propagent grâce à l\u0027enchainement des règles.\nLes règles R1 et R2 (traduisant l\u0027UNA) puis les règles R5(CulturalPlace, Painting) et R5(Artist, Painting), traduisant des axiomes de disjonction entre classes, permettent d\u0027infé-rer, entre autres, les non réconciliations suivantes :\nNonReconcile(S1_m1, S1_m2), NonReconcile (S1_p1, S1_p2), NonReconcile(S2_m1, S2_p1), NonReconcile (S2_p1, S2_p2), NonReconcile(S1_m1, S2_p1), NonReconcile (S1_a1, S2_p1).\nLa règle R7.2(PaintingName) traduisant la propriété de fonctionnalité inverse de l\u0027attribut PaintingName, et les faits PaintingName(S1_p1, \"La joconde\"), PaintingName(S2_p1, \"La Joconde\") et EquiVals(\"La Joconde\", \"la joconde\") permettent d\u0027inférer Reconcile(S2_p1, S1_p1), c\u0027est-à-dire que S2_p1 et S1_p1 réfèrent à la même peinture.\nLes musées S1_m1 et S2_m2 contenant ces peintures sont eux mêmes réconciliés par dé-clenchement de la règle R7.1(Contains), qui permet d\u0027inférer Reconcile(S1_m1,S2_m2).\nLa propagation de ces nouvelles réconciliations permettra grâce à la règle R6.2(MuseumName) d\u0027inférer Equivals(\"Le LOUVRE\", \"musée du LOUVRE\"), éta-blissant la synonymie entre les valeurs de base \"Le LOUVRE\" et \"musée du LOUVRE\", et grâce à la règle R6.1(Located) d\u0027inférer la réconciliation des deux références sur les villes S1_c1 et S2_c1. Le fait inféré Reconcile(S1_c1 , S2_c1) permet ensuite d\u0027inférer, par le déclenchement de la règle R6.2(CityName), le nouveau fait EquiVals(\"ville de Paris\",\"Paris\") qui établit la synonymie entre les deux valeurs de base \"ville de Paris\" et \"Paris\".\nLes règles R3 et R4 de traduction de l\u0027UNA permettent d\u0027éliminer toute autre possibilité de réconciliation. La règle R4 permet ainsi d\u0027inférer NonReconcile(S2_m2, S1_m1) à partir des faits Reconcile(S1_m1, S2_m1) src1(S1_m1), src2(S2_m1) et scr2(S2_m2) : par UNA, on sait que les deux musées S2_m1 et S2_m2 sont différents, et comme on a réconcilié les deux références S1_m1 et S2_m2 au musée du Louvre, on est sûr que la référence S1_m1 de la source S1 au musée du Louvre n\u0027est pas réconciliable avec la référence S2_m2 de la source S2 (qui est une référence au musée du quai Branly).\nGénération et exploitation du dictionnaire\nL\u0027ensemble des synonymies qui sont inférées entre valeurs de base (prédicat Equivals) sont conservées dans un dictionnaire qui est alimenté au fur et à mesure des réconciliations de références provenant de différentes sources.\nLe dictionnaire contient différents types de synonymies : -Des codifications telles que 1 pour oui, 75 pour Paris et (*) pour étoile -Des abréviations telles que apt pour appartement, ou acronymes tels que ACM pour Association for Computing Machinery -Des vrais synonymes tels que bon pour confortable -Des traductions telles que Milano pour Milan Nous avons vu que ces équivalences sont exploitées durant la phase de réconciliation ellemême. Le fait de conserver ces valeurs dans un dictionnaire permet également d\u0027utiliser ces connaissances lorsque l\u0027approche logique est appliquée à d\u0027autres sources. Le dictionnaire peut permettre d\u0027alimenter la base de faits intiale par tous les faits EquiVals qu\u0027il contient. Ce dictionnaire peut aussi être exploité dans une méthode ou une étape de réconciliation numérique fondée sur le calcul de similarité entre chaînes de caractères.\nExpérimentation\nNous présentons dans cette section les premiers résultats de l\u0027expérimentation de notre approche logique de réconciliation de références. Cette méthode a été testée sur des données de deux domaines différents : des données du domaine du tourisme et des données d\u0027un portail de publications scientifiques en informatique. Dans le premier jeu de données, en présence de l\u0027UNA au niveau de chaque source, la réconciliation de références a pour objectif l\u0027intégration de données entre différentes sources. Dans le second jeu de données, l\u0027objectif de la récon-ciliation est de nettoyer une source (ie. éliminer les doublons) pour laquelle l\u0027UNA n\u0027est pas posée. \nPrésentation des données de test (FT_HOTELS et CORA)\nRésultats et validation\nComme l\u0027ensemble des réconciliations et des non réconciliations est obtenu par un algorithme d\u0027inférence à base de règles logiques, nous avons donc une précision à 100%. Il nous reste donc à évaluer le rappel. Pour calculer le rappel sur les données de CORA, nous avons comparé le nombre de réconciliations et de non réconciliations par rapport au nombre de celles qu\u0027il fallait effectivement trouver. L\u0027information concernant des réconciliations et des non ré-conciliations effectives est représentée dans les données de CORA sous forme d\u0027annotations sur les références. En revanche, nous ne disposons pas de cette information pour le jeu de données FT_HOTELS. Nous avons donc effectué la validation à la main sur un échantillon de 1796 références représentant les références de deux sources de données de taille (404 x 1392). Pour nous faciliter la tâche de recherche des réconciliations oubliées, nous avons examiné l\u0027ensemble des réconciliations possibles, après filtrage, par des recherches mots-clés.\nNous présentons dans le tableau 3 le rappel global concernant les décisions de réconci-liations (paires réconciliées ou non réconciliées). De plus, nous donnons le détail du rappel concernant l\u0027ensemble des paires de références réconciliées et l\u0027ensemble des paires de réfé-rences non réconciliées.\nDans le but de montrer l\u0027intérêt de la richesse du schéma sur le rappel obtenu par notre algorithme, nous présentons les résultats sur le jeu de données FT_HOTELS dans deux cas : (1) pas d\u0027axiomes de disjonctions entre classes d\u0027hotels et (2) ajout au schéma d\u0027un ensemble d\u0027axiomes de disjonctions traduisant le fait que deux hotels situés dans deux pays différents sont forcément différents. Comme le montre la figure 3, sur le jeu de données FT_HOTELS, nous avons obtenu un rappel global de 8.3 % avec un rappel correspondant à celui du sous-ensemble des NonReconcile 8.2 % qui représente uniquement les inférences réalisées à partir des règles de l\u0027UNA. Le rappel correspondant au sous-ensemble des Reconcile est de 54 % malgré les irrégularités dans les valeurs. Il s\u0027agit essentiellement d\u0027absences d\u0027information (eg. adresse non renseignée) ou de variabilités dans les valeurs, en particulier dans les adresses : \"parc des fées\" vs. \"parc des fées, (près de Royan)\", ou encore \"11, place d\u0027arme\" vs. \"place d\u0027arme\". De plus, certaines données de FT_HOTELS sont décrites en plusieurs langues : \"Chatatoa\" vs. en basque \"Chahatoenia\". Ces résultats montrent également l\u0027apport de l\u0027enrichissement du schéma par une connaissance telle que la disjonction entre hotels situés dans des pays différents. Cet ajout est peu coûteux et pourtant il permet d\u0027augmenter le rappel du sous-ensemble NonReconcile de 8.2 % à 75.9 %.\nFT_HOTELS sans\nEn ce qui concerne le jeu de données CORA, nous avons obtenu un bon rappel sur le sous-ensemble Reconcile 79 %. Dong et al. (2005) obtiennent un meilleur rappel (97%) mais n\u0027ont pas une précision de 100%. Le rappel sur le sous-ensemble NonReconcile est seulement de 33 %. En effet, ce dernier résultat est obtenu en exploitant uniquement deux axiomes de disjonction du schéma et sans hypothèse d\u0027UNA.\nNous avons également inféré un ensemble de synonymes que nous avons stockées dans un dictionnaire. Par exemple, pour le jeu de données FT_HOTELS, pour lequel nous avons obtenu 1063 réconciliations (au total), le dictionnaire généré contient 3671 synonymies. Ces dernières représentent essentiellement des codifications d\u0027information (eg.EquiVals(\"1\",\"Y\")), des traductions telles que EquiVals(\"Florence\",\"FIRENZE\") et des descriptions syntaxiquement proches telles que EquiVals(\"2100\",\"DK-2100\") qui sont des codes postaux au Danemark mais aussi EquiVals(\"Avignon -Le Pontet\",\"LE PONTET\").\nConclusion\nL\u0027approche logique que nous venons de présenter pour la réconciliation de références pré-sente l\u0027intérêt de ne produire que des réconciliations et des non réconciliations sûres, ce qui la distingue des autres travaux existants. La sûreté de cet ensemble de réconciliations est un atout dans un domaine où il est difficile d\u0027estimer à l\u0027avance le taux d\u0027erreurs d\u0027une approche non supervisée Winkler (2006). Cette approche permet également de découvrir des synonymies entre valeurs de base, conservées dans un dictionnaire qui s\u0027enrichit au fur et à mesure que de nouvelles réconciliations sont inférées, et dont l\u0027enrichissement entraîne de nouvelles réconci-liations. Tout en garantissant une précision de 100%, les premières expérimentations montrent que notre méthode obtient un taux de rappel très satisfaisant, qui augmente significativement quand on rajoute des connaissances sur le schéma des données.\nD\u0027autres travaux -appelés blocking method -utilisent des connaissances du domaine pour réduire le nombre de paires de références à considérer Baxter R. (2003). Ces travaux considèrent seulement les paires qui possèdent une caractéristique donnée commune (exemple : le nom de famille pour des personnes). Ce type de connaissance peut être déclaré comme disjonction dans le schema afin d\u0027être exploité comme le sont les autres connaissances du domaine. C\u0027est ce que nous avons fait pour tirer parti des valeurs de pays des hotels. Nous avons mentionné dans l\u0027introduction que des connaissances du domaine pouvait être également traduites par des poids dans approche numérique Dong et al. (2005).\nLa méthode que nous avons proposé est partielle car elle ne produit pas de résultat pour toutes les paires de références possibles. Nous envisageons d\u0027augmenter le rappel des non réconciliations en exploitant les contraposées de règles. Cela permettra également d\u0027enrichir le dictionnaire par de nouvelles connaissances sur les valeurs de base : l\u0027inférence de non Equivals(v1,v2) nous permettra de sélectionner les valeurs dont la similarité syntaxique est bonne et qui, pourtant, ne signifient pas la même chose (exemples : musée du prado vs musée du lido, chatillon sur marne vs chatillon sur seine).\nPour obtenir une méthode totale de réconciliation de références, nous prévoyons d\u0027appliquer une méthode numérique de réconciliation à l\u0027ensemble des paires de référence pour lesquelles l\u0027étape logique n\u0027a pas produit de résultat. Pour cette étape numérique, nous avons plusieurs pistes pour exploiter les résultats de réconciliation et non réconciliation produit par\n"
  },
  {
    "id": "914",
    "text": "Introduction\nLes services Web constituent la nouvelle génération des technologies du Web pour l\u0027inté-gration d\u0027applications. Ce sont des composants logiciels mis à disposition par des fournisseurs, invocables sur Internet par des clients (des utilisateurs ou d\u0027autres services), et communiquant de façon asynchrone, par le biais de messages. Ils permettent de réaliser une intégration à faible couplage et à moindre coût, du fait qu\u0027ils utilisent des standards généralistes fortement répan-dus (XML, HTTP). Toutefois, cette souplesse d\u0027intégration n\u0027est possible que si les utilisateurs d\u0027un service savent comment interagir avec celui-ci. A un service doivent donc être associées des descriptions assez riches pour permettre de comprendre sa sémantique d\u0027exécution.\nLe langage WSDL, par exemple, spécifie l\u0027interface d\u0027un service : les opérations, les types de messages, le format des entrées-sorties. Cependant, Benatallah et al. (2004) ont montré que ceci était insuffisant dans l\u0027optique d\u0027une utilisation automatique des services Web, et ont dé-fini le protocole de conversation, qui permet de spécifier quelles sont les séquences ordonnées de messages (appelées conversations) qu\u0027un service peut émettre ou recevoir. Benatallah et al. (2005a,b) ont ensuite ajouté des contraintes temporelles à leur modèle, rebaptisé protocole de conversation temporisé. Son utilisation offre de nombreuses applications, pour la vérifica-tion automatique de bon fonctionnement, de compatibilité, etc. Néanmoins, en pratique, de nombreux services ne possèdent pas une telle spécification. Il est donc légitime de chercher à obtenir le protocole de conversation d\u0027un service s\u0027il n\u0027a pas été défini lors de la conception.\nFournir le protocole d\u0027un service à ses partenaires et clients est bien sûr l\u0027application la plus directe de ce problème de découverte ; mais il possède un intérêt bien plus grand pour l\u0027ingé-nierie des services Web. Par exemple, un concepteur pourrait connaître le protocole « effectif » (réellement utilisé) d\u0027un service, et savoir s\u0027il correspond bien aux contraintes de conception ; il serait possible de faire évoluer un service plus facilement : l\u0027utilisation d\u0027un modèle visuel de son comportement (plutôt que la simple analyse de code) permettrait de faciliter l\u0027ajout de nouvelles fonctionnalités, de nouvelles contraintes ou règles, etc.\nL\u0027extraction du protocole de conversation d\u0027un service englobe de nombreux défis techniques. Le premier réside dans la modélisation du protocole découvert : il est important de prendre en compte l\u0027incertitude du résultat, et de proposer des indices de confiance et des critères de qualité, permettant d\u0027évaluer sa pertinence. Cette incertitude provient principalement du fait que les logs d\u0027un service peuvent contenir des erreurs d\u0027enregistrement (du « bruit »). Des outils sont donc nécessaires, pour analyser et nettoyer les données avant de les traiter. Il est également important de proposer à l\u0027utilisateur des outils lui permettant de modifier et corriger le protocole découvert. Un autre point délicat est la corrélation des messages, i.e. l\u0027identification et la séparation, dans les logs, des différentes conversations (qui peuvent se chevaucher).\nCe problème de découverte constitue en fait un cas particulier d\u0027une problématique beaucoup plus large : la découverte d\u0027un modèle à partir d\u0027instances de celui-ci. De nombreux travaux sont liés à cette thématique ; on peut citer par exemple l\u0027inférence grammaticale (Parekh et Honavar, 2000), la fouille de workflow (Cook et Wolf, 1998;van der Aalst et al., 2004), ou la fouille d\u0027interactions de services Web (Dustdar et al., 2004). En inférence grammaticale, le but consiste à trouver, à partir d\u0027un ensemble de mots appartenant ou pas à un langage donné, une grammaire permettant de générer ce langage. Pour la fouille de workflow (ou découverte de processus), le problème réside dans la construction, à partir des logs d\u0027exécution d\u0027un processus, d\u0027un modèle formel permettant de représenter le fonctionnement de ce processus. En fouille des interactions de services Web, l\u0027objectif est de découvrir, à partir des logs d\u0027un ensemble de services, un workflow modélisant les interactions possibles entre ces services.\nDans le contexte applicatif des services Web, Motahari et al. (2006) ont proposé une mé-thode d\u0027extraction de protocoles à partir des archives de conversation entre services (fichiers « logs »). Ce travail porte sur plusieurs des enjeux mentionnés plus haut. Cependant, il ne considère pas les aspects temporels du protocole de conversation.\nContribution. Notre travail se place également dans le contexte de cette problématique. Nous traitons un sous-problème important qui est la découverte des transitions temporisées du protocole de conversation, i.e. des changements d\u0027état liés non pas à l\u0027émission d\u0027un message mais à l\u0027existence d\u0027une contrainte de temps. Bien qu\u0027une telle transition ne figure pas de façon explicite dans les logs du service, il est possible d\u0027identifier les conséquences de son existence. Ce sont donc ces « traces » que nous formalisons et que nous extrayons des données. Pour cela, nous introduisons la notion d\u0027expiration propre, qui représente dans les logs ce que la notion de transition temporisée représente dans le protocole de conversation. Certains changements d\u0027état du service ne sont pas liés à l\u0027envoi de messages explicites, mais à des contraintes temporelles (durée de validité, échéance, etc.). Le modèle de base a donc été enrichi de transitions temporisées, et rebaptisé de ce fait protocole de conversation temporisé. Une transition temporisée se produit de façon automatique, après qu\u0027un certain laps de temps se soit écoulé à partir du moment où elle a été permise (i.e. où l\u0027état source de la transition est devenu l\u0027état courant), ou bien après qu\u0027une certaine date soit atteinte ; elle est étiquetée par la contrainte de temps associée. Notons que, puisque le modèle est déterministe, un état ne peut admettre plusieurs transitions temporisées comme transitions sortantes.\nFIG. 1 -Exemple de protocole de conversation temporisé (Benatallah et al., 2005b).\nExemple 1 La figure 1 représente un protocole de conversation temporisé décrivant le comportement externe d\u0027un service de commande de marchandises. Les transitions explicites sont représentées en trait plein, et les transitions temporisées en pointillés. Ce protocole spécifie que le client du service doit d\u0027abord se connecter (opération login), puis chercher des produits (searchGoods). Il peut alors ajouter ou enlever des produits de son caddie (addToCart, removeFromCart), chercher d\u0027autres marchandises (searchGoods), ou demander un devis (quoteRequest) qui sera valide seulement pendant 3 jours (i.e. 4320 minutes). Il peut alors commander les marchandises (order). S\u0027il ne le fait pas, au bout des 3 jours la conversation se termine (par le biais de la transition temporisée sortant de l\u0027état Quoted), et la commande est annulée.\nRNTI -E -\nLogs de conversation\nLes différentes façons de collecter les logs d\u0027interaction d\u0027un service ont été décrites par Dustdar et al. (2004). En fonction de la façon dont les services sont implémentés et du type d\u0027outils utilisés pour gérer leur exécution, différents types d\u0027informations peuvent être présents dans les logs. Dans un scénario réaliste, les informations collectées sont en général, en plus du contenu du message, l\u0027émetteur, le receveur et la date.\nCes informations peuvent ne pas suffire pour identifier une conversation de façon unique, dans le cas où l\u0027on ne dispose pas d\u0027un identifiant de chaque conversation enregistrée dans les logs. Il est mis en avant par Motahari et al. (2006) que le fait de fournir automatiquement un tel identifiant (s\u0027il n\u0027est pas présent par défaut) est un véritable problème en soi. Aussi, ont-ils supposé, pour mener à bien leur tâche de découverte du protocole de conversation, que cette information était présente dans les logs. Nous en ferons de même.\nLes logs que nous traitons sont les enregistrements des messages émis ou envoyés par un service. Ces enregistrements sont effectués par le serveur hébergeant le service. Nous ne considérons pas les logs « internes » du service, qui peuvent être ajoutés au code par le concepteur. Les informations dont nous disposons sont les intitulés des messages, et leurs dates d\u0027émis-sion ; la connaissance de l\u0027émetteur ou du receveur ne nous est d\u0027aucune utilité. Précisons également que nous ne tenons pas compte de la polarité des messages.\nExemple 2 Pour un service vérifiant le protocole de conversation représenté par la figure 1, on peut par exemple obtenir les conversations suivantes : (login, 9:18) (searchGoods, 9:20) (addToCart, 9:21) (quoteRequest, 9:22) (cancel, 9:51) ; (login, 11:03) (searchGoods, 11:04) (addToCart, 11:08) (quoteRequest, 11:12).\nSpécification du problème\nL\u0027ensemble des intitulés des messages appartenant au protocole de conversation temporisé sera noté M sg. On notera L les logs de conversation dont on dispose. Formellement, L sera un multi-ensemble de conversations. Une conversation sera notée :\n.. \u003c t n C ; elle représentera une séquence d\u0027occurrences de messages. On notera de plus :\nTâche de découverte\nConsidérons le cas où l\u0027on ne connaît pas le protocole de conversation qui a permis de géné-rer les logs L. Le problème consiste à exhiber le fait que ces données traduisent la présence de transitions temporisées dans le protocole. Notre méthode vise à examiner les couples de messages dans les logs, afin de déterminer si, entre deux transitions explicites, a été déclenchée une transition implicite ou pas. Pour ce faire, nous allons calculer, pour chaque conversation, le laps de temps écoulé entre l\u0027émission de deux messages consécutifs.\nRNTI -E -D. Devaurs et al. Exemple 3 Les logs L 1 , associés au protocole P 1 représenté dans la figure 2, constituerons l\u0027exemple que nous développerons dans la suite de cet article. Ici, nous devons par exemple mettre en évidence le fait, qu\u0027après l\u0027émission du message a, les messages c, d et e peuvent être émis uniquement avant un certain laps de temps, et que les messages g et h peuvent être émis uniquement après ce laps de temps. Il est important de noter que les dates d\u0027enregistrement des messages sont les dates relatives au début de chaque conversation.\nHypothèses de travail\nNous supposerons dans la suite que les protocoles associés aux logs que nous traiterons, comme dans l\u0027exemple précédent, ne possèdent pas de transition temporisée menant dans un état final, bien que cela puisse se produire dans les cas réels. Le fait de se ramener à ce cas particulier -qui est déjà relativement complexe en soi -nous aidera par la suite à mieux appré-hender le cas général. Nous supposerons également que les transitions du protocole de conversation sont étiquetées de façon unique, même si certaines correspondent à un même message. Si nous ne sommes pas dans un tel cas de figure, nous pourrons envisager de nous y ramener en effectuant un pré-traitement sur les données.\nEn ce qui concerne les logs, nous allons supposer qu\u0027ils ne sont pas bruités, c\u0027est-à-dire que les messages sont correctement enregistrés, et dans une séquence correcte. Ceci nous permettra, dans un premier temps, de proposer une méthode « complète ». Une approche probabiliste pourra ensuite être envisagée pour pallier le fait que les logs puissent être bruités dans les cas réels. Nous supposerons également que les logs sont suffisamment « complets » pour retrouver les transitions temporisées, c\u0027est-à-dire que tous les « chemins » du protocole de conversation ont été parcourus.\nDéfinition 1 (Episode) Un épisode constitue une séquence de deux intitulés de messages :\nSi une telle séquence existe, on dira que l\u0027épisode ? se produit dans la conversation C.\nOn notera Occ(?) l\u0027ensemble des occurrences de l\u0027épisode ? dans L. On dira que l\u0027épi-sode ? se produit dans les logs L si ? se produit dans au moins une conversation C de L, i.e. si Occ(?) \u003d ?. On notera Ep l\u0027ensemble des épisodes se produisant dans les logs L.\nCette proposition exprime le fait que l\u0027on peut construire une partition de l\u0027ensemble des épisodes, où chaque partie est constituée de l\u0027ensemble des épisodes dont le premier élément est un message m donné. Nous ne donnons pas de preuve pour ce résultat, qui est relativement trivial. Cette propriété va nous permettre de décomposer notre tâche de découverte. Au lieu d\u0027examiner les épisodes dans leur ensemble, nous allons traiter chaque élément de cette partition séparément. Pour ce faire, nous allons définir la notion de durée d\u0027occurrence.\nIntuitivement, la durée d\u0027une occurrence d\u0027un épisode est la différence des dates des messages de l\u0027épisode dans l\u0027occurrence. A partir de ceci, on définit la durée d\u0027occurrence minimale (respect. maximale) d\u0027un épisode comme étant la plus petite (respect. la plus grande) durée de toutes les occurrences de cet épisode. L\u0027intervalle de durée d\u0027occurrence d\u0027un épisode est l\u0027intervalle qui englobe l\u0027ensemble des durées d\u0027occurrence de cet épisode. Du fait qu\u0027elle soit la conséquence de la présence d\u0027une transition temporisée, la relation de précédence présentée dans cet exemple nous sera utile dans la suite. En effet, si l\u0027on inverse le raisonnement, trouver qu\u0027une telle relation est vérifiée par les données pourrait nous conduire à la découverte d\u0027une transition temporisée. Nous formalisons donc cette relation.  Cette proposition constitue une condition nécessaire à l\u0027existence d\u0027une transition temporisée. Notre problème de découverte serait résolu si cette condition était également suffisante (nous disposerions d\u0027un objet équivalent à une transition temporisée), mais ce n\u0027est pas le cas.\nRelation d\u0027ordre sur les ensembles d\u0027épisodes\nRemarque : La réciproque de la proposition 2 est fausse.\nContre-exemple On a {{a, c ? {{a, e (car D max ({{a, c \u003d 3 \u003c 4 \u003d D min ({{a, e dans les logs L 1 , alors que les transitions étiquetées par c et e sortent du même état (cf. fig. 2).\nLa proposition 2 et l\u0027hypothèse de complétude des logs nous assurent que l\u0027ensemble des expressions de la forme A ? B vérifiées par les logs englobe l\u0027ensemble des transitions temporisées. Toutefois, ces expressions peuvent aussi nous donner, en plus, de fausses informations, sur des transitions inexistantes. Ceci vient du fait que la relation ? ne prend pas en compte l\u0027ensemble des informations induites par la présence d\u0027une transition temporisée. C\u0027est pourquoi nous définissons dans la suite une relation plus riche sur les ensembles d\u0027épisodes.\nExpiration\nDéfinition 6 (Expiration) Soient m ? M sg, et A, B ? P m (A, B \u003d ?). On dira que les logs L satisfont l\u0027expiration E(m, A, B), ce que l\u0027on notera L E(m, A, B), si : Exemple 6 Les logs L 1 satisfont les expirations, E(a, {c, d, e}, {g}), E(a, {c, d}, {g}), Remarque : La réciproque de la proposition 3 est fausse.\nContre-exemple L\u0027expiration E(b, {f }, {g, h}) est satisfaite par les logs L 1 , bien qu\u0027il n\u0027y ait pas de transition temporisée entre les états s 1 et s 3 du protocole P 1 (cf. figure 2). Par contre, il existe une chaîne formée de deux transitions temporisées, reliant ces deux états.\nRNTI -E -La proposition 3 et l\u0027hypothèse de complétude des logs nous assurent que chaque transition temporisée du protocole de conversation peut être retrouvée par l\u0027intermédiaire d\u0027une certaine expiration satisfaite par les logs. Cependant, une expiration est satisfaite entre deux ensembles d\u0027épisodes, aussi bien en présence d\u0027une transition temporisée entre les états correspondant à ces ensembles d\u0027épisodes, que d\u0027une chaîne de transitions temporisées. Nous allons donc défi-nir une classe d\u0027expirations plus restreinte, afin d\u0027éviter cette ambiguïté. Un autre problème lié aux expirations est qu\u0027elles sont beaucoup plus nombreuses que les transitions temporisées.\nExemple 7 Dans le cas du protocole P 1 (cf. figure 2), la transition temporisée présente entre les états s 1 et s 2 entraîne la satisfaction, par les logs L 1 , de l\u0027expiration E(b, {f }, {c, d, e}), mais aussi de E(b, {f }, {c, d, e, g, h}) ; celle qui relie les états s 2 et s 3 entraîne la satisfaction de l\u0027expiration E(a, {c, d, e}, {g}), mais aussi de E(a, {c, d}, {g}).\nCet exemple illustre le fait que plusieurs formes de « redondance » apparaissent. Or, nous voulons apporter le minimum d\u0027informations nécessaires à l\u0027utilisateur pour retrouver les transitions temporisées. C\u0027est donc dans ce sens que nous définissons les expirations propres. \nExpiration propre Définition 7 (Expiration propre)\nRNTI -E -\nExemple 8 Les expirations propres satisfaites par les logs L 1 (cf. figure 2) sont reportées dans le tableau 1 (rappelons que les ensembles P a et P b sont traités indépendamment l\u0027un de l\u0027autre). On vérifie également que : {c, d, e, g, h}), car {{b, c d e ? {{b, g h ; -L 1 EP (a, {c, d}, {g}), car {{a, e {{a, c d g \nRemarque : La réciproque de la proposition 4 est fausse.\nContre-exemple On a L 1 EP (a, {h}, {g}), alors que les transitions étiquetées par h et g sortent du même état (cf. figure 2). Ceci s\u0027explique par le fait que, dans les logs L 1 , après que le message a ait été émis, le message g est toujours plus long à émettre que le message h.\nD\u0027après la proposition 4 et l\u0027hypothèse de complétude des logs, puisque chaque transition temporisée engendre la satisfaction d\u0027une expiration propre dans les logs, il est possible de toutes les retrouver. Toutefois, on peut découvrir plus d\u0027expirations propres qu\u0027il n\u0027y a de transitions temporisées, dans le cas où certains messages sont toujours plus long à envoyer (ou à recevoir) que tous les messages associés aux autres transitions du même état. Le théorème suivant exprime le fait que ceci constitue le seul cas d\u0027erreur possible.\n, alors il existe dans le protocole de conversation : -ou bien deux états s 1 et s 2 tels que s 2 soit relié à s 1 par une transition temporisée, A corresponde à un sous-ensemble des transitions sortant de s 1 , et B corresponde à un sousensemble des transitions sortant de s 2 , -ou bien un état s tel que A ? B corresponde à un sous-ensemble des transitions sortant de s, et les messages de B soient toujours plus longs à émettre que les messages de A.\nBien que l\u0027on ne puisse établir une correspondance totale entre ces objets, les expirations propres représentent, en pratique, le meilleur équivalent possible des transitions temporisées. En effet, le théorème 1 nous assure que, si l\u0027on découvre une expiration propre dans les logs, alors il existe une transition temporisée dans le protocole de conversation, ou alors on est en présence de messages plus longs à émettre que d\u0027autres, sachant que les logs seuls ne permettent pas de déceler si l\u0027on se trouve dans un tel cas de figure. Ce résultat justifie la pertinence de la mise en place d\u0027une méthode de découverte des transitions temporisées basée sur la recherche des expirations propres satisfaites par les logs.\nLa méthode de découverte « naïve » consiste à générer toutes les expirations propres possibles, et à tester pour chacune d\u0027entre elles si les conditions de la définition 7 sont vérifiées. Cette méthode est cependant doublement exponentielle car, d\u0027une part le nombre d\u0027expirations propres possibles est exponentiel, et d\u0027autre part pour chaque couple (A, B) de sousensembles de P m comparables grâce à la relation ?, il est nécessaire de vérifier que tous les RNTI -E -sous-ensembles de A et de B sont incomparables. Aussi, travaillons-nous actuellement à la définition d\u0027une caractérisation des expirations propres conduisant à un algorithme de décou-verte polynomial. Cette caractérisation est basée sur la construction d\u0027une partition de chaque sous-ensemble P m d\u0027épisodes. Elle est actuellement en cours de démonstration.\nRappelons que les éléments de la partition {P m | m ? M sg} de Ep sont traités séparé-ment. A chaque partie P m va donc correspondre un ensemble d\u0027expirations propres qui lui sont associées. Ce procédé peut sembler redondant, dans le sens où, si deux transitions étiquetées respectivement par les messages a et b arrivent sur un même état, d\u0027où sort une transition temporisée, nous allons trouver que deux expirations propres différentes sont satisfaites dans les logs (une pour P a et une autre pour P b ), et les interpréter comme étant deux transitions temporisées différentes. Il est possible de résoudre ce problème en faisant des recoupements entre les différents ensembles d\u0027expirations propres. Ceci permettra également de rejeter certaines expirations propres qui ne peuvent correspondre à des transitions temporisées.\nExemple 9 Considérons les expirations propres satisfaites par les logs L 1 (cf. tableau 1). L\u0027expiration propre EP (a, {h}, {g}) (associée à P a ) ne peut correspondre à une transition temporisée, car h et g interviennent ensemble dans l\u0027expiration propre EP (b, {c, d, e}, {g, h}) (associée à P b ). D\u0027après le théorème 1, on sait que h et g correspondent à des transitions sortant du même état. Finalement, on trouve deux transitions temporisées : l\u0027une correspondant à EP (a, {c, d, e}, {h}) et EP (b, {c, d, e}, {g, h}), et l\u0027autre à EP (b, {f }, {c, d, e}).\nConclusions et perspectives\nNotre travail se situe dans le contexte de l\u0027extraction du protocole de conversation temporisé d\u0027un service Web à partir de ses logs d\u0027exécution. Il traite de la découverte des transitions temporisées, et constitue, à notre connaissance, la première contribution apportée à la résolu-tion de ce problème. Notre apport consiste en un cadre formel aboutissant à la définition de la notion d\u0027expiration. Nous avons montré que l\u0027ensemble des expirations propres satisfaites par les logs constitue une caractérisation de l\u0027ensemble des transitions temporisées présentes dans le protocole de conversation d\u0027un service.\nDu fait qu\u0027il concerne les aspects temporels du protocole de conversation, notre résultat s\u0027inscrit en complément des travaux existants. Nous envisageons d\u0027intégrer l\u0027algorithme de découverte des transitions temporisées sur lequel nous travaillons à la méthode d\u0027extraction du protocole de conversation (non temporisé) proposée par Motahari et al. (2006), au sein d\u0027une plateforme commune de gestion de services Web. Ceci nous permettra de pouvoir effectuer des tests à grande échelle de notre méthode.\nSignalons également que nous avons comme objectif d\u0027élargir le cadre formel présenté ici. Nous envisageons pour cela d\u0027essayer de relâcher les contraintes fixées au départ (par exemple le fait que les transitions du protocole de conversation soient étiquetées de façon unique, ou qu\u0027il n\u0027existe pas de transition temporisée menant dans un état final). La solution permettant de pallier ces limites pourrait être d\u0027effectuer un pré-traitement sur les données, afin de se ramener au cas particulier défini par nos hypothèses de travail. Il serait également intéressant de prendre en compte le « bruit » présent dans les données.\nRNTI -E -\n"
  },
  {
    "id": "915",
    "text": "Introduction\nLa gestion des connaissances dans l\u0027organisation est abordée dans cet article comme la finalité des traitements cognitifs sur les informations générées, transformées ou acquises par l\u0027organisation. Ainsi, nous pouvons distinguer deux types complémentaires de connaissances qui permettent à l\u0027organisation d\u0027être notamment réactive : la connaissance de l\u0027environnement dans lequel l\u0027organisation s\u0027insère et avec lequel elle interagit, ainsi que la connaissance propre de l\u0027organisation. Notre démarche s\u0027adresse essentiellement à toutes les connaissances détenues par l\u0027organisation. En effet, une bonne « connaissance » organisationnelle, avec tout ce qu\u0027elle comprend de formel (organigramme, relations et hiérarchies explicites) et de plus informel (relations personnelles, affinités,…) est à la base de la performance. Cette connaissance constitue un élément essentiel de continuité pour le pilotage de l\u0027entreprise : elle englobe le savoir relatif au marché (stratégies, fournisseurs, clients, concurrents), mais aussi et surtout les caractéristiques internes de l\u0027entreprise (alliances, jeux de pouvoir, relations interpersonnelles). Cette connaissance est principalement détenue par les acteurs de l\u0027organisation et il est nécessaire d\u0027en assurer la conservation ou du moins d\u0027en éviter l\u0027évaporation. Cependant, comment cette connaissance de l\u0027organisation est-elle appréciée par les acteurs de l\u0027organisation ? Comment voient-ils et perçoivent-ils l\u0027organisation ? Comment les connaissances sont-elles réparties dans l\u0027organisation ? Toutes ces questions restent en suspens du fait notamment de la difficulté à identifier, localiser, et évaluer à proprement parler ces connaissances. Dans ce contexte, l\u0027information détenue par l\u0027organisation est vue comme le support de connaissances. Nous proposons une approche basée sur ces informations afin d\u0027obtenir une vue globale des connaissances de l\u0027organisation. Une représentation macroscopique des connaissances peut s\u0027appuyer sur des systèmes de capitalisation (mémoire d\u0027entreprise par exemple), mais aussi, et surtout, sur les connaissances détenues par les individus et non forcément capitalisées dans un système global de gestion de connaissances. Outre cette vue macroscopique, nous proposons une déclinaison microscopique de cette visualisation au niveau des différents acteurs. Cette visualisation met en évidence les connaissances détenues par chaque acteur mais également les connaissances « dynamiques » (connaissances en action) véhiculées au travers des échanges qu\u0027ils entretiennent avec les autres acteurs de l\u0027organisation. Ainsi, outre la vision statique des connaissances, nous proposons une vision de leur dynamique (circulation, échange, transformation) au sein de l\u0027organisation. Le but de ces niveaux de visualisations est de permettre aux acteurs de mieux apprécier les connaissances de l\u0027organisation pour, par exemple, rapidement et simplement identifier ou localiser une personne experte d\u0027un domaine ou pour aider un nouvel arrivant à s\u0027insérer dans l\u0027organisation (problème de « Turn-Over »). Cette proposition repose sur des modèles d\u0027individu associés à des cartes auto-organisatrices permettant de proposer une représentation graphique et un support de navigation dans l\u0027espace des connaissances de l\u0027organisation.\nLe présent papier est structuré comme suit. La section 2 vise à faire une synthèse sur la gestion des connaissances associée aux outils de cartographie afin de souligner la complémentarité de notre démarche avec les propositions de ces domaines. Dans la section 3, nous proposons les visualisations macroscopiques et microscopiques des connaissances basées sur les modèles d\u0027individu et sur les connaissances en action, avant de conclure.\nGestion des connaissances\n« Dans une économie où la seule certitude est l\u0027incertitude, l\u0027unique source d\u0027avantage concurrentiel durable est le savoir » (Nonaka, 2000). De ce constat est née la nécessité de capitaliser les connaissances d\u0027une organisation et dans le même temps de cartographier ces connaissances afin d\u0027en assurer une bonne visibilité pour les membres organisationnels. Le but de cette cartographie est de permettre aux acteurs de localiser les connaissances dans l\u0027organisation pour les réutiliser afin d\u0027en créer de nouvelles ou de prendre des décisions évitant ainsi des coûts conséquents pour l\u0027organisation. Les coûts visés ici sont ceux pouvant être liés à la complexité de la recherche de la « bonne connaissance », de non-utilisation de la connaissance ou de reconstruction de connaissances existantes et, enfin, d\u0027évaporation de connaissances.\nCapitalisation des connaissances\nLes connaissances, d\u0027après (Skyrme, 1999), « ne sont ni des données, ni des informations, mais se définissent bien plus comme une capacité humaine acquise avec le temps et consistant à relier les informations en leur donnant du sens ». Ainsi, du point de vue des sciences de Gestion, l\u0027information peut être considérée comme étant la matière première des connaissances. Cette idée est confortée par (Grundstein, 2002), qui présente le processus de création des connaissances par un individu à partir des informations qu\u0027il reçoit et la création d\u0027informations à partir de ses propres connaissances.\nDu point de vue stratégique, la gestion des connaissances a deux finalités : ? une finalité patrimoniale : les organisations tentent de consolider leur patrimoine « intellectuel » afin d\u0027accroître leur compétitivité. Comment préserver les connaissances, les réutiliser et les actualiser ?, ? une finalité d\u0027innovation durable : au travers de l\u0027apprentissage collectif (ou organisationnel), l\u0027organisation tente d\u0027accroître les connaissances de chaque individu tout en améliorant la connaissance collective (partagée entre les individus). En ce qui concerne la finalité patrimoniale, la communauté des chercheurs a beaucoup travaillé sur les mémoires d\u0027entreprise. Une mémoire d\u0027entreprise permet de structurer la connaissance sous forme d\u0027une collection documentaire organisée et primordiale pour l\u0027organisation. Elle permet donc de mémoriser et organiser le capital intellectuel détenu par cette dernière. La structuration de la mémoire d\u0027entreprise offre un support à la manipulation (interrogation, visualisation de l\u0027évolution) de la connaissance capitalisée. Les travaux relatifs à la mémoire d\u0027entreprise s\u0027intéressent donc aux différentes problématiques de la gestion des connaissances : repérer, actualiser, valoriser, préserver, « manager » la connaissance (Grundstein, 2004). Le lecteur pourra se reporter à (Balmisse, 2002) pour un panorama de ces technologies et leur utilisation en gestion des connaissances. La capitalisation des connaissances permet donc d\u0027aider un individu à (re)construire les connaissances nécessaires à ses activités à partir de connaissances préexistantes. Un critère important de ces mémoires d\u0027entreprise est la façon dont celles-ci restituent les connaissances de l\u0027organisation à ses membres. Il s\u0027agit là d\u0027une problématique de visualisation plus que de construction des connaissances : ce problème bien connu est central, du fait de la difficulté à apprécier une masse importante d\u0027informations/connaissances ; les nombreux appels d\u0027offres autour de la problématique des masses de données en témoignent, tout comme la littérature sur la visualisation de telles grandes masses de données.\nCartographie des connaissances\nMême si de nombreuses représentations de la cartographie de l\u0027information existent (Kartoo, 2005), la cartographie des connaissances sous la forme d\u0027une représentation visuelle est essentiellement produite sous forme de graphes. Par exemple, (Debourges et al, 2001) ou (Trébucq, 2005) se basent sur des lexicogrammes. Nous pouvons également citer (Abdenour, 2004) qui présente l\u0027outil VICOTEXT proposant une approche originale de visualisation automatique et dynamique des connaissances textuelles. Ces travaux proposent une représentation visuelle des connaissances sous forme de graphes de termes. Cette visualisation se justifie car ces travaux se basent sur les techniques de Traitement Automatique du Langage Naturel (TALN) et des graphes conceptuels.\nDans un contexte plus général nous pouvons signaler l\u0027outil de visualisation Umap 1 qui repose sur les « arbres de la connaissance » (figure 1). Cette représentation propose une visualisation du résultat sous forme d\u0027îlots correspondant à des ensembles thématiques d\u0027informations.\nFIG. 1 -Visualisation proposée par Umap\nLa principale limite de ces approches est qu\u0027elles reposent essentiellement sur la base documentaire détenue par l\u0027organisation, pour effectuer cette représentation graphique : c\u0027est ce que nous appellerons la visualisation macroscopique des connaissances.\nCertaines approches proposent également de visualiser l\u0027activité de communication dans les organisations comme le propose (Wittaker et al., 2002). Cependant, nous n\u0027avons pas identifié dans la littérature d\u0027approche qui complète cette visualisation par une visualisation microscopique c\u0027est-à-dire au niveau des acteurs même de l\u0027organisation autre qu\u0027au travers de leurs communications. En effet, celles-ci se limitent à la gestion des contacts et des communications. Or, les connaissances de l\u0027organisation sont exploitées, diffusées, recherchées : qu\u0027elles soient partagées ou tout simplement transmises, ces connaissances vivent. Il est donc important d\u0027évaluer dans quelle mesure chaque acteur intervient dans cette vie de la connaissance au sein de l\u0027organisation, pour identifier par exemple les acteurs clés de la connaissance. Ainsi, l\u0027organisation pourra veiller à préserver le capital ainsi acquis si ces acteurs venaient à quitter l\u0027entreprise. C\u0027est pour cela que la visualisation microscopique que nous proposons intègre à la fois l\u0027aspect connaissance et l\u0027aspect communication pour évaluer l\u0027implication de chaque acteur dans la diffusion des connaissances.\nEn réponse à ces problématiques, nous proposons une approche de visualisation et de navigation dans les connaissances de l\u0027organisation. La visualisation macroscopique donne une image globale des connaissances dans l\u0027organisation alors que la visualisation microscopique donne une image locale de ces connaissances au niveau de chaque acteur. Cette dernière visualisation basée sur les modèles d\u0027individu permet en outre d\u0027identifier les connaissances en action au travers des échanges que chaque acteur entretient avec les autres membres organisationnels. La navigation s\u0027effectue du niveau macroscopique au niveau microscopique et vice et versa.\nCartographie macroscopique et microscopique des connaissances dans l\u0027organisation\nLes cartographies proposées reposent donc sur deux niveaux de représentation : le niveau macroscopique et le niveau microscopique. Ces niveaux sont détaillés dans les sections 3.3 et 3.4. Ces deux types de cartographies reposent sur une visualisation des connaissances basée sur les cartes auto-organisatrices et sur un modèle caractérisant les individus (sections 3.1 et 3.2).\nLes cartes auto-organisatrices\nLes cartes auto-organisatrices (Kohonen, 1982) permettent, par le biais d\u0027une classification non supervisée des informations, d\u0027obtenir une représentation graphique et topologique sous forme de cartes en 2D ou 3D. Ces cartes ont été utilisées par exemple en recherche d\u0027information pour présenter de façon globale les résultats de recherche d\u0027information de manière intuitive. La carte est une grille où chaque cellule correspond à une classe de documents similaires. Les classes sont positionnées les unes par rapport aux autres suivant leur similarité respective (figure 2). (Lesteven, 1996) propose une utilisation de ces cartes dans le domaine de l\u0027astronomie tandis que Websom propose une application des cartes auto-organisatrices sur le web (Lagus, 1996). (Poinçot, 1999) Un grand avantage de ce type de visualisation est que l\u0027on peut naviguer à l\u0027intérieur c\u0027est-à-dire aller vers le détail des connaissances contenues (« forer »). Par ailleurs, le niveau de détails est visible directement au travers des étiquettes car elles permettent de localiser des « zones » de connaissances, liées au même domaine, par exemple.\nFIG. 2 -Carte auto-organisatrice réalisée à partir d\u0027un corpus documentaire en astronomie\nLe choix de cette visualisation a été fait pour sa simplicité d\u0027interprétation et la navigation qu\u0027elle permet contrairement à d\u0027autres visualisations comme Umap par exemple. \nFIG. 3 -Diagramme des classes du Modèle de l\u0027individu\nModèle d\u0027individu sous-jacent à la cartographie\nAfin de caractériser les connaissances individuelles ainsi que collectives, notre approche repose sur un modèle d\u0027individu regroupant différents aspects descriptifs des membres organisationnels. Ce modèle d\u0027individu (cf. figure 3) repose sur celui proposé dans (Canut et al., 2005). L\u0027individu est caractérisé au travers de 4 dimensions complémentaires : ? Les caractéristiques cognitives qui correspondent à une formalisation des connaissances que possède l\u0027individu. Ces dernières reposent sur tous les documents possédés par lui, que ce soient les documents situés physiquement sur son espace personnel (disque dur, répertoire réseau…) ou les documents internet pour lesquels il a sauvegardé un signet dans son navigateur. L\u0027individu s\u0027approprie les documents au travers de son espace d\u0027information personnel qu\u0027il organise sous forme de thème et sous-thèmes (Topics). La gestion de cette organisation des documents peut être assistée par des outils semiautomatiques. ? Les caractéristiques sociales ou conatives (en gras dans le diagramme UML). Ces caractéristiques permettent d\u0027évaluer le « faire-savoir » (Canut et al., 2005)  Ces différentes caractéristiques sont exploitées à différents niveaux dans la cartographie des connaissances dans l\u0027organisation. Nous détaillons dans les sections qui suivent les deux niveaux de cartographie proposés.\nCartographie macroscopique\nCette cartographie a pour but de proposer une vue globale des connaissances détenues dans l\u0027organisation.\nElle peut être effectuée pour un sous-ensemble des membres de l\u0027organisation, qu\u0027il soit formel (groupe de travail, équipe…) ou informel (détecté au travers des échanges d\u0027emails). Ceci permet par exemple d\u0027identifier les connaissances mises en jeu dans ce groupe.\nPour ce faire, un premier pas consiste à rassembler les différents documents détenus par les individus concernés par l\u0027étude. A ce stade, un lien peut être fait avec un système de connaissances existant pour intégrer d\u0027autres documents par exemple (mémoire d\u0027entreprise ou autre SI). Ensuite, une extraction des mots-clés caractérisant au mieux le contenu des documents est nécessaire : il s\u0027agit de la phase d\u0027indexation.\nPour cela nous utilisons les méthodes classiques bien connues en recherche d\u0027information : ? Elimination des mots vides (qui n\u0027apportent pas de sens au contenu du document) (Zipf, 1949), ? Radicalisation des termes (Porter, 1980) permettant de réduire l\u0027espace des mots-clés en rapprochant les termes ayant des racines proches voire similaires, ? Pondération des termes en fonction de leur importance dans les documents (Salton, 1983), (Singhal, 1997).\nCette indexation sert de base à la construction d\u0027une carte auto-organisatrice dans laquelle il est possible de naviguer ou d\u0027obtenir des détails (« forer »). De cette carte, il est à tout moment possible de basculer vers une visualisation microscopique c\u0027est-à-dire de détailler la connaissance au niveau des individus. Ainsi, chacun, en parcourant la carte, découvrira les membres organisationnels qui sont liés aux connaissances qu\u0027il est en train de découvrir. Cette démarche est expliquée dans la section 3.5.\nCartographie microscopique\nLa cartographie microscopique met en évidence les quatre dimensions d\u0027un membre organisationnel présentes dans le modèle de l\u0027individu ( fig. 3).\nLes caractéristiques cognitives sont présentées sous la forme d\u0027une carte autoorganisatrice. Celle-ci est construite à partir de tous les documents possédés par l\u0027individu. La construction de la carte est régie par le même procédé d\u0027indexation que celui présenté dans la cartographie macroscopique.\nLes caractéristiques socio-cognitives sont également présentées sous la forme d\u0027une carte auto-organisatrice. Celle-ci est construite à partir du contenu de tous les champs « sujet » des emails qu\u0027un membre a reçus. Nous posons l\u0027hypothèse que l\u0027organisation possède un outil anti-spam permettant ainsi de limiter la cartographie des connaissances aux seules informations réellement liées à l\u0027activité de l\u0027individu.\nNous avons décidé de séparer arbitrairement les caractéristiques cognitives et sociocognitives car elles correspondent à une sémantique différente. En effet, la carte cognitive correspond à des connaissances capitalisées par l\u0027acteur sur le long terme alors que la carte socio-cognitive souligne l\u0027implication de certaines de ses connaissances dans les échanges avec les autres membres organisationnels. Il est à souligner que ces deux cartes peuvent présenter des différences.\nLes caractéristiques formelles sont simplement affichées dans la cartographie et permettent de localiser un membre dans l\u0027organisation, offrant par exemple la possibilité de le contacter.\nLes caractéristiques conatives sont présentées sous la forme d\u0027un indicateur de communication qui est calculé en fonction des autres indicateurs (Canut et al., 2005). Cette valeur numérique permettra aux autres membres organisationnels d\u0027apprécier le comportement d\u0027un individu vis-à-vis des autres : est-il utile de contacter un individu qui ne répond pas aux emails ? Ne doit-on pas préférer quelqu\u0027un de plus réactif ? La réponse à ces questions peut être trouvée en recoupant les caractéristiques conatives avec les caractéristiques formelles (participations à des équipes de projet, fonctions..). De plus, une représentation cartographique du réseau des relations d\u0027un individu peut être utilisée pour visualiser les personnes en relation avec lui (Canut et al., 2005). Ces caractéristiques ne sont que secondaires et permettent d\u0027affiner le choix de la personne à contacter par exemple.\nDu point de vue fonctionnel, ces deux cartes sont distinctes mais elles permettent toutes deux de faire le lien entre la cartographie macroscopique et la cartographie microscopique. Ainsi, à partir de ces cartes, il est possible de revenir à la cartographie macroscopique aux différents points sur la carte où l\u0027on peut retrouver le membre organisationnel.\nPassage macroscopique microscopique\nLe passage du niveau macroscopique au niveau microscopique est effectué grâce aux cartes elles-mêmes. Chaque cellule correspondant à un ensemble de documents ou de sujets d\u0027emails, nous pouvons réutiliser ces ensembles pour trouver des « ponts » entre les niveaux macroscopiques et microscopiques.\nLe procédé peut être réalisé par le biais d\u0027une mesure de similarité entre les différentes cellules des cartes. Pour cela, nous utilisons un descripteur du contenu de chaque cellule que nous allons comparer pour obtenir cette similarité. Ce descripteur peut par exemple être calculé par une méthode de construction de classifieur (Sebastiani, 2002) comme la technique dite de Rocchio (Rocchio, 1971). Cette technique permet, à partir d\u0027une représentation des documents sous la forme de liste pondérée de termes (issue par exemple d\u0027une phase d\u0027indexation) de construire un vecteur représentant de l\u0027ensemble des documents. Ce vecteur représentant maximise le poids des termes des documents faisant effectivement partie de la cellule et minimise le poids des termes se trouvant dans les autres cellules. Nous pouvons alors utiliser la mesure cosinus par exemple pour calculer la similarité entre deux vecteurs représentants caractérisant deux cellules.\nSi la similarité est suffisante (i.e. supérieure à un seuil fixé), un lien entre ces cellules est établi et une possibilité de navigation de l\u0027une à l\u0027autre des cartes est alors possible. Ce procédé est illustré dans la figure 4. Ce seuil de similarité peut être fixé ou mis à jour automatiquement en utilisant un calcul de seuil optimal (Wu, 2001). Ainsi nous proposons de calculer et de mettre à jour ce seuil au même rythme que la génération des cartes ellesmêmes. Pour ce faire, nous utilisons la répartition des documents sur les cartes autoorganisatrices générées. Pour chaque cellule des cartes, un seuil optimal est calculé. Ce seuil permet de maximiser le nombre de documents pertinents et limiter le nombre de documents non pertinents dans la cellule. Afin d\u0027obtenir un seuil de similarité suffisamment discriminatoire c\u0027est-à-dire optimal pour chaque cellule, différentes approches ont été proposées (Hoashi, 1999)  (Wu, 2001). Dans notre cas, nous utiliserons une approche qui maximise la formule F 1 (1) comme dans (Ruiz, 2001). Cela signifie que le seuil correspond à la similarité entre, d\u0027une part, le vecteur représentant la cellule, et, d\u0027autre part, le document qui maximise cette fonction F1 dans cette cellule. Cette mesure de performance facile à calculer est définie comme suit :\noù P représente la valeur de Précision (3) et R la valeur de Rappel (2) pour chaque cellule de la carte, calculées selon les formules (Salton, 1983)  \nFIG. 4 -Evaluation des liaisons entre niveau macroscopique et microscopique\nCes différentes techniques supposent que toutes les cartes soient générées au moment de la navigation. Or, la limite d\u0027utilisabilité des cartes auto-organisatrices est le temps de calcul. Même si des optimisations existent (Prudhomme, 2005), nous proposons que ces cartes soient établies de manière régulière puis stockées. En effet, même si l\u0027organisation est un environnement en perpétuel mouvement dans lequel les acteurs évoluent, nous pouvons considérer que les modifications ne sont que « légères » et donc que l\u0027échelle de temps entre deux cartographies peut être assez importante (semaine, mois). Une échelle de temps trop petite n\u0027occasionnerait qu\u0027une surcharge du système d\u0027information de l\u0027organisation, sans amélioration significative des visualisations proposées aux acteurs.\nConclusion et perspectives\nLes travaux actuels sur les mémoires d\u0027entreprise apportent des solutions significatives au problème de la gestion des connaissances dans les organisations, en particulier dans les phases d\u0027acquisition, mémorisation, capitalisation des connaissances. Parallèlement et en complémentarité de ces travaux, l\u0027accès et la réutilisation de ces connaissances reste une problématique très actuelle. C\u0027est dans ce contexte que nous proposons un modèle de représentation associé à des possibilités de visualisation des connaissances à deux niveaux.\nLe premier niveau de visualisation, dit niveau macroscopique, permet de visualiser les connaissances de l\u0027organisation dans leur ensemble en s\u0027appuyant sur le système d\u0027information de l\u0027organisation et utilise des techniques déjà bien identifiées.\nLe deuxième niveau de visualisation, dit niveau microscopique, permet de visualiser les connaissances qui sont détenues mais également qui transitent au niveau de chaque membre de l\u0027organisation. Pour ce faire, nous nous appuyons sur un modèle de l\u0027individu décrit selon quatre dimensions : formelle, cognitive, conative, socio-cognitive.\nCes deux niveaux de visualisation sont mis en oeuvre au travers de cartes autoorganisatrices.\nLe passage d\u0027un niveau à l\u0027autre permet de passer d\u0027une connaissance détenue par l\u0027organisation à l\u0027individu qui la possède, la diffuse, la réutilise et vice versa.\n"
  },
  {
    "id": "916",
    "text": "Introduction\nLes arbres de décision utilisés depuis longtemps en statistique (Morgan et Sonquist, 1963;Kass, 1980) sont devenus, suite aux ouvrages de Breiman et al. (1984) et Quinlan (1993) des outils très populaires pour générer des règles de classification et plus généralement des règles de prédictions. On parle ainsi d\u0027arbre de classification lorsque la variable à prédire est catégo-rielle et que ses valeurs représentent donc des classes. Cependant, et contrairement à ce que l\u0027expression d\u0027arbre de classification peut laisser entendre, la classification n\u0027est pas le seul intérêt des arbres de décisions. Par exemple, en sciences sociales où il s\u0027agit plus de comprendre comment des prédicteurs peuvent affecter les valeurs prises par la variable à prédire que de classer des individus, ils peuvent avoir un intérêt descriptif, ou encore, comme en marketing notamment, on peut les utiliser dans une optique de ciblage. Dans ce dernier cas, plutôt que de prédire la valeur de la réponse, il s\u0027agit de repérer les profils typiques des individus appartenant à chacune des classes de la variable à prédire. On inverse en quelque sorte le problème en cherchant à caractériser les profils propres à la classe, plutôt que la classe à partir du profil. L\u0027évaluation de la qualité de l\u0027arbre se fonde le plus souvent sur le taux d\u0027erreur de classification. Ce taux de cas mal classés par les règles, qu\u0027il soit calculé sur les données d\u0027apprentissage, des données test ou encore en validation croisée est évidemment pertinent comme mesure de qualité des règles quand l\u0027objectif est la classification proprement dite. Il ne l\u0027est cependant plus lorsque l\u0027on utilise l\u0027arbre à d\u0027autres fins, et il s\u0027agit alors d\u0027exploiter d\u0027autres mesures mieux adaptées. Dans (Ritschard et Zighed, 2004;Ritschard, 2006), nous avons par exemple proposés des mesures de type déviance qui permettent de juger de la qualité descriptive de l\u0027arbre en évaluant son aptitude à prédire la distribution de la variable réponse pour un profil donné. Ici, nous nous intéressons au cas du ciblage. Quelle information nous donne la règle sur la typicité du profil -la prémisse de la règle -pour sa conclusion ? Nous proposons de mesurer cette typicité à l\u0027aide des concepts de l\u0027analyse statistique implicative.\nL\u0027analyse statistique implicative introduite par Régis Gras (Gras, 1979;Gras et Larher, 1992;Gras et al., 1996) comme outil d\u0027analyse de données, a connu ces dernières années un essor remarquable dans le cadre de la fouille de règles d\u0027association du type « si l\u0027on observe A alors on devrait aussi observer B » (Suzuki et Kodratoff, 1998;Gras et al., 2001Gras et al., , 2004. Son principe fondamental consiste à juger de la pertinence d\u0027une relation de dépendance en fonction de la fréquence de ses contre-exemples. Une règle avec peu de contre-exemples est considérée comme plus implicative qu\u0027une règle pour laquelle les contre-exemples sont fré-quents. Curieusement, et bien que nous ayons montré (Ritschard, 2005) qu\u0027elle s\u0027appliquait sans difficulté aux règles issues d\u0027arbres, cette idée de force d\u0027implication n\u0027a guère été exploitée dans le contexte de l\u0027apprentissage supervisé. Or, la force d\u0027implication d\u0027une règle évaluée par l\u0027écart entre le nombre observé de contre-exemples et le nombre moyen que générerait le seul hasard correspond précisément à la notion de typicalité du profil pour la conclusion qui nous intéresse ici.\nL\u0027article est organisé comme suit. En section 2, nous rappelons les concepts d\u0027indice et d\u0027intensité d\u0027implication et leur utilisation associée à un arbre de classification. Nous discutons ensuite (toujours en section 2) de l\u0027analogie entre indice d\u0027implication et résidus issus de la modélisation de tables de contingence et de l\u0027intérêt de ces résidus comme mesures alternatives de la force d\u0027implication. En section 3 et 4, nous illustrons sur un exemple réel deux utilisations de l\u0027intensité d\u0027implication : évaluer a posteriori les règles issues d\u0027un arbre, et effectuer le choix de la conclusion d\u0027une règle. Enfin, nous présentons des remarques conclusives et des perspectives de développement à la section 5.\nArbres et indice d\u0027implication\nLes arbres de classification sont des outils de classification supervisés. Ils déterminent des règles de classification en deux temps. Dans une première étape, une partition de l\u0027espace des prédicteurs (x) est déterminée telle que la distribution de la variable (discrète) à prédire (y) diffère le plus possible d\u0027une classe à l\u0027autre de la partition et soit, dans chaque classe, la plus pure possible. La partition se fait successivement selon les valeurs des prédicteurs. On commence par partitionner les données selon les modalités de l\u0027attribut le plus discriminant, puis on répète l\u0027opération localement sur chaque noeud ainsi obtenu jusqu\u0027à la réalisation d\u0027un critère d\u0027arrêt. Dans un second temps, après que l\u0027arbre ait été généré, on dérive les règles de classification en choisissant la valeur de la variable à prédire la plus pertinente, en général simplement la plus fréquente, dans chaque feuille (noeud terminal) de l\u0027arbre.\nPratiquement, on relève dans chaque feuille j, j \u003d 1, . . . , le nombre n ij de cas qui sont dans l\u0027état y i . Ainsi, on peut récapituler les distributions au sein des feuilles sous forme d\u0027une table de contingence croisant les états de la variable y avec les feuilles (Tableau 1). On peut noter que la marge de droite de ce tableau qui donne le total des lignes correspond en fait à la distribution des cas dans le noeud initial de l\u0027arbre.  Gras et al., 2004, p 19) d\u0027une règle se définit à partir des contre-exemples. Dans notre cas il s\u0027agit dans chaque feuille (colonne du tableau 1) du nombre de cas qui ne sont pas dans la catégorie majoritaire. Ces cas vérifient en effet la prémisse de la règle, mais pas sa conclusion. En notant b la conclusion (ligne du tableau) 1 de la règle j et n bj le maximum de la jème colonne, le nombre de contre-exemples est n¯ bj \u003d n ·j ? n bj . L\u0027indice d\u0027implication est une forme standardisée de l\u0027écart entre ce nombre et le nombre espéré de contre-exemples qui seraient générés en cas de répartition entre valeurs de la réponse indépendante de la condition de la règle.\nFormellement, l\u0027hypothèse de répartition indépendante de la condition, que nous notons H 0 , postule que le nombre N¯ bj de contre-exemples de la règle j résulte du tirage aléatoire et indépendant d\u0027un groupe de n ·j cas vérifiant la prémisse de la règle j et d\u0027un autre de n¯ b· \u003d n ? n b· cas qui ne vérifient pas la conclusion de la règle. Sous H 0 et conditionnellement à n b· et n ·j , le nombre aléatoire N¯ bj de contre-exemples est réputé (Lerman et al., 1981) suivre une loi de Poisson de paramètre n e ¯ bj \u003d n¯ b· n ·j . Ce paramètre n e ¯ bj est donc à la fois l\u0027espérance mathématique et la variance du nombre de contre-exemples sous H 0 . Il correspond au nombre de cas de la feuille j qui seraient des contre-exemples si l\u0027on répartissait les n ·j cas de j selon la distribution marginale, celle du noeud initial de l\u0027arbre (ou marge de droite du tableau 1).\nL\u0027indice d\u0027implication de Gras est l\u0027écart n¯ bj ? n e ¯ bj entre les nombres de contre-exemples observés et attendus sous l\u0027hypothèse H 0 , standardisé par l\u0027écart type, soit\nEn termes de cas vérifiant la condition, cet indice s\u0027écrit encore Pour expliciter le calcul de l\u0027indice, on considère la variable « classe prédite » qui prend la valeur 1 pour chaque cas (exemple) appartenant à la classe majoritaire de sa feuille d\u0027appartenance, et 0 pour les autres (contre-exemples). On note cette variable cpred. En croisant cette variable avec les conditions des règles, on obtient le tableau 2 où la première ligne donne pour chaque règle j son nombre n¯ bj de contre-exemples et la seconde ligne le nombre n bj de cas vérifiant la règle. De même, le tableau 3 donne les nombres espérés n e bj d\u0027exemples et n e ¯ bj de contre-exemples dans le cas d\u0027une répartition des cas couverts par chaque règle j selon la distribution marginale. Il est important de noter que ces effectifs attendus ne se déduisent pas des marges du tableau 2. Ils s\u0027obtiennent en répartissant tout d\u0027abord les cas selon la distribution marginale du tableau 1 et en procédant ensuite aux regroupements selon la classe majoritaire observée dans chaque colonne du tableau 1.\nIndice d\u0027implication et résidus\nDans sa formulation (1), l\u0027indice d\u0027implication a l\u0027apparence d\u0027un résidu standardisé du type (racine signée de) contribution au khi-deux de Pearson (voir par exemple Agresti, 1990, p.224). Il s\u0027agit en fait de la contribution au khi-deux mesurant la « distance » entre les tableaux 2 et 3. En effet, il suffit de remarquer que le khi-deux ainsi défini peut s\u0027écrire :\nOn reconnaît alors sous le premier signe de sommation dans l\u0027expression (3) le carré de l\u0027indice d\u0027implication de Gras. Cette interprétation de l\u0027indice d\u0027implication en termes de ré-sidu (résidu de l\u0027ajustement du nombre de contre-exemples par le modèle d\u0027indépendance H 0 ), suggère que d\u0027autres formes de résidus utilisés dans le contexte de la modélisation de tables de contingence puissent également s\u0027avérer intéressantes pour mesurer la force d\u0027implication d\u0027une règle. En particulier on peut citer : \n)|, qui est la racine signée de la contribution (en valeur absolue) au khi-deux du rapport de vraisemblance (Bishop et al., 1975, pp.136-137).  (Bishop et al., 1975, p.137).\nLe résidu standardisé, qui correspond à l\u0027indice d\u0027implication de Gras, est connu pour avoir une variance inférieure à 1. Le problème est que dans la pratique les nombres n b· et n ·j dé-pendent de l\u0027échantillon considéré et sont donc eux-mêmes aléatoires. Ainsi n e ¯ bj n\u0027est qu\u0027une estimation du paramètre de la loi de Poisson. On doit alors tenir compte du fait que dans la formule (1), le dénominateur n\u0027est qu\u0027une estimation de l\u0027écart type. Les résidus déviance, de Freeman-Tukey et ajusté sont mieux adaptés à cette situation et sont réputés avoir dans la pratique une distribution plus proche de la normale N (0, 1) que le simple résidu standardisé. Ce dernier, et par conséquent l\u0027indice d\u0027implication de Gras, tend à sous-estimer la force d\u0027implication.\nLa figure 1 montre les valeurs des résidus (en ordonnée) en fonction de la proportion n bj /n ·j (en abscisse) de cas qui dans la feuille j vérifient la conclusion b de la règle. Les courbes sont représentées pour n \u003d 100, une règle j associée à une feuille couvrant 20% des cas, et une proportion marginale n b· /n de cas vérifiant la conclusion b de 50%. A gauche de ce seuil, les indices prennent tous une valeur positive indiquant que la règle fait moins bien que le hasard. On peut relever le comportement curieux du résidu déviance dont la valeur tend vers 0 lorsque le nombre de contre-exemples tend vers 0. Cela suggère que la règle devient non implicative quand le nombre de contre-exemples devient nul, ce qui n\u0027est évidemment pas sastisfaisant. L\u0027indice de Gras et le résidu ajusté évoluent de manière linéaire avec la proportion n bj /n ·j , le résidu ajusté prennant ses valeurs sur une étendue plus importante. Quant au résidu de Freeman-Tukey, on relève que sa variation s\u0027accélère lorsque le taux de biens classés de la règle approche de 1.\nIntensité d\u0027implication et p-valeur\nIl est naturel de s\u0027intéresser à la p-valeur, ou degré de signification, des indices d\u0027implication observés. Cette p-valeur correspond à la probabilité p(N¯ bj ? n¯ bj |H 0 ). Quand n e ¯ bj est petit, le calcul peut se faire, conditionnellement à n b· et n ·j , avec la loi de Poisson de paramètre n e ¯ bj . Pour n e ¯ bj grand (? 5), la loi normale donne une bonne approximation, à condition toutefois de procéder à la correction pour la continuité, la différence pouvant atteindre encore 2.6 points de pourcentage pour n \u003d 100. La figure 2 montre les fonctions de répartition de la loi de Poisson et de la loi normale avec et sans correction de continuité pour n e ¯ bj \u003d 5. On peut relever que l\u0027approximation par la loi normale, en particulier avec la correction pour la continuité, reste bonne même pour n e ¯ bj relativement petit. Ainsi, en notant ?(·) la fonction de distribution d\u0027une normale standardisée, on a\nOn appelle intensité d\u0027implication (Gras et al., 1996) le complémentaire à 1 de cette pvaleur. Gras et al. (2004) la définissent en termes de l\u0027approximation normale (4), mais sans la correction pour la continuité. Pour notre part, nous la calculerons pour une règle j comme\nDans tous les cas, cette intensité s\u0027interprète comme la probabilité d\u0027obtenir, sous l\u0027hypothèse H 0 , un nombre de contre-exemples supérieur à celui observé pour la règle j. majoritaire et leurs prémisses définies par les chemins, mutuellement exclusifs, qui mènent aux feuilles. Les sept règles sont explicitées au tableau 5. Remarquons en premier lieu que toutes les règles sauf une concluent à « buveur occasionnel », modalité majoritaire au noeud initial de l\u0027arbre. Ceci est typique de situations de déséquilibre (répartition des modalités de y éloignée de la situation d\u0027équiprobabilité), où les algorithmes d\u0027apprentissage ont parfois du mal à discriminer les différentes classes. On relève également, qu\u0027aucune règle ne conclut à « jamais ». La faible représentativité au noeud initial de l\u0027arbre de cet état induit une difficulté à trouver des règles isolant ces individus. Le tableau 6 présente la classique matrice de confusion associée à cet arbre. Les défauts cités jusqu\u0027ici y apparaissent de façon plus flagrantes, en particulier l\u0027erreur associée à la modalité « jamais ». Les valeurs des résidus définis en section 2.1 sont présentées pour chacune des règles dans le tableau 7. Les valeurs négatives indiquent que le nombre de contre-exemples observé est inférieur à celui attendu sous la condition d\u0027indépendance entre la prémisse et la conclusion de la règle. Dès lors, les valeurs négatives sont synonymes de « qualité ». La règle R6 ((syst2 ? 111) et (chlst \u003c 278.5) et (syst1 \u003c 157.5) ? buveur occasionnel) pour laquelle les résidus sont positifs, est une règle qui fait moins bien que l\u0027indépendance au sens que le nombre de contre-exemples observé est supérieur au nombre moyen que générerait le hasard. La règle peut donc être considérée comme non pertinente.\nIl est intéressant ici de faire une comparaison de la qualité implicative avec le taux d\u0027erreur communément utilisé pour l\u0027évaluation de règles de classification. Le nombre de contreexemples considérés est précisément le nombre d\u0027erreurs produites par la règle sur l\u0027échan-tillon d\u0027apprentissage. Le taux d\u0027erreur correspond ainsi au pourcentage de contre-exemples parmi les cas couverts par la règle, soit n¯ bj /n ·j pour la règle j, ce qui est encore le complé-mentaire à 1 de la confiance. Le taux d\u0027erreur souffre donc des mêmes inconvénients que la confiance. En particulier, il ne nous dit rien sur ce que la règle apporte de plus qu\u0027une clas- TAB. 7 -Valeurs des résidus pour chacune des règles de l\u0027arbre.\nsification indépendante de toute condition. Pour notre règle R6 par exemple, la confiance est de 55% contre 56% pour le classifieur naïf consistant à classer tout le monde comme « buveur occasionnel », classe la plus fréquente au noeud initial. La question est évidemment de savoir quoi faire d\u0027une règle non pertinente d\u0027un point vue implicatif. On peut soit décider de la conserver si le but est la qualité globale de classification. Dans le cas où, au contraire, l\u0027on veut privilégier la force implicative de chaque règle, deux solutions sont envisageables :\n-fusionner la règle avec une de ses règles soeurs ; -changer la conclusion de la règle. En fusionnant les règles R6 et R7, ce qui revient à élaguer la branche non pertinente de l\u0027arbre, on obtient une nouvelle règle ((syst2 \u003e\u003d 111) et (chlst \u003c 278.5) ? buveur occasionnel). Les valeurs des résidus pour cette nouvelle règle sont : res std \u003d 1.11, res dev \u003d 4.5, res F T \u003d 1.11, res adj \u003d 2.73. Ils sont positifs et indiquent clairement une détérioration par rapport à la situation précédente. En fait, on peut observer sur l\u0027arbre de la figure 3 qu\u0027en remontant la branche à partir de la feuille correspondant à la règle R6, on ne rencontre que des noeuds où la classe majoritaire « occasionnellement » a une fréquence inférieure à celle relevée au noeud initial. La fusion ne peut donc pas être une solution dans ce cas particulier tant que l\u0027on garde le principe de la classe majoritaire pour le choix de la conclusion. Ceci nous amène donc à discuter l\u0027autre solution consistant à changer la conclusion de la règle en choisissant la modalité qui maximise l\u0027intensité d\u0027implication.\nChoix de la conclusion des règles\nSi l\u0027objectif est de maximiser l\u0027intensité d\u0027implication des règles, dans le but en particulier de déterminer les profils les plus caractéristiques de chaque état de la variable à pré-dire, il semble naturel de choisir la conclusion de la règle qui maximise cette intensité plutôt que la classe majoritaire. L\u0027idée de choisir ainsi la classe maximisant l\u0027intensité d\u0027implication (i.e. minimisant le résidu) a notamment déjà été exploitée par Zighed et Rakotomalala (2000, pp.282-287). A titre d\u0027exemple, nous donnons dans le tableau 8 la conclusion sélectionnée par cette procédure pour chacune des règles et selon le résidu utilisé comme critère de choix. On observe que si les conclusions restent celles de la classe majoritaire pour les règle R1, R4 et R5, le principe de la maximisation de l\u0027implication donne des conclusions différentes Imp(j) Déviance Freeman- Tukey  Ajusté  Majorité  R1  2  2  2  2  2  R2  1  1  1  1  2  R3  2  3  2  3  2  R4  2  2  2  2  2  R5  2  2  2  2  2  R6  1  1  1  1  2  R7  3  3  3  3  3 1 \u003d jamais, 2 \u003d occasionnellement, 3 \u003d régulièrement TAB. 8 -Conclusion selon le résidu utilisé comme critère.\npour les quatre autres règles. Pour la règle R3, la conclusion varie entre « occasionnellement » et « fréquemment » selon le critère implicatif retenu. Pour les règles R2, R6 et R7 les quatre indices d\u0027implication conduisent au même résultat. Il est intéressant de relever également, qu\u0027avec ce critère implicatif, chacune des trois modalités de la variable à prédire est retenue comme conclusion pour au moins une règle. De plus, on peut souligner que, dans tous les cas, les indices d\u0027implication -dont les valeurs ne sont pas montrées ici -restent négatifs. Une expérience intéressante consiste à recalculer la matrice de confusion nouvellement obtenue. Le tableau 9 montre cette dernière pour le cas où l\u0027on utilise le résidu standardisé, soit l\u0027indice de Gras. Le taux d\u0027erreur global est évidemment plus élevé qu\u0027au tableau 6 ce qui n\u0027est pas surprenant puisqu\u0027on ne vise plus ici à minimiser l\u0027erreur de classification. Le tableau fournit cependant des enseignements utiles sur deux plans. Premièrement, on peut observer que la maximisation de l\u0027intensité d\u0027implication améliore considérablement la valeur des mesures de rappel intra-classe pour les modalités faiblement représentées. On a également confirmation qu\u0027il n\u0027y pas ici, et contrairement au tableau 6, d\u0027état de y qui ne puisse être prédit par au moins une règle.\nEnsuite, et c\u0027est ici l\u0027intérêt principal du choix de la conclusion selon le principe de la maximisation de l\u0027intensité, la matrice fait ressortir que les règles sont ici plus discriminantes par rapport à la répartition au noeud initial de l\u0027arbre. Ainsi, l\u0027arbre généré peut être vu comme la représentation d\u0027une typologie des modalités de la variable à prédire y. L\u0027interprétation des règles, qui ne sont plus alors des règles de classification, doit elle être revue en terme de typicité de la condition pour la conclusion choisie. Ainsi, les personnes ayant une pression artérielle systolique 2 inférieure à 111, une pression artérielle systolique 1 inférieure à 113.5 et un indice de masse corporelle inférieur à 26.04 sont caractéristiques des « jamais buveurs ». Au contraire, prédiction état réel  jamais  occasionnellement  régulièrement  jamais  107  12  12  occasionnellement  516  194  38  régulièrement  307  105  50 TAB. 9 -Matrice de confusion, maximisation intensité implicative, taux d\u0027erreur \u003d 72%.\nles buveurs réguliers sont caractérisés par une pression artérielle systolique 2 élevée (? 111) et un taux de cholestérol également élevé. Enfin, les buveurs occasionnels ne sont pas clairement caractérisés, bien qu\u0027il existe des circonstances typiques comme un taux de cholestérol élevé allié à un BMI également élevé, sans toutefois connaître de problème au niveau de la pression artérielle. La difficulté à discriminer les buveurs occasionnels des autres peut également venir du fait qu\u0027il y a différents types de buveurs, la définition « occasionnellement » étant elle-même subjective.\n5 Conclusion\n"
  },
  {
    "id": "917",
    "text": "Introduction\nL\u0027une des conséquences de la prolifération de l\u0027information en ligne de nos jours est la diversité des données. XML se distingue comme le format par excellence pour la représentation, le stockage et l\u0027échange de données sur Internet.\nLes systèmes de recherche d\u0027information dans les documents XML (RI-XML) utilisent soit le paradigme de l\u0027appariement exact soit celui de l\u0027appariement approximatif (ou appariement par classement). Dans le premier cas, la requête doit vérifier les contraintes sur le contenu et la structure spécifiées dans la requête, ainsi chaque item (document, fragment de document ou élément XML) sur lesquels la recherche est effectuée et jugé pertinent ou non. Dans le second cas, les items sont classés selon leur pertinence à la requête. Dans le contexte du Web, l\u0027appariement approximatif est plus approprié. En effet, l\u0027appariement exact nécessite un langage d\u0027interrogation structuré et une connaissance a priori de la structure des documents recherchés. Cependant, dans un environnement ouvert comme le Web, les utilisateurs ne sont pas nécessai-rement aptes à exprimer leur besoin d\u0027information avec un langage d\u0027interrogation complexe. En outre, la structure des documents XML recherchés n\u0027est pas toujours disponible. L\u0027appariement exact naturellement nécessite le classement des résultats dans le but de présenter les items les plus pertinents en premiers.\nLa plupart des méthodes de classement existantes proposent d\u0027étendre les modèles traditionnels de classement utilisés dans la RI classique. Ces méthodes définissent une fonction globale qui calcule un score (RSV : Relevance Status Value) pour chaque item. Les documents sont alors triés d\u0027une façon globale selon leurs scores et présentés à l\u0027utilisateur. Le score d\u0027un document est calculé en fonction d\u0027un ensemble de critères ayant un impact sur la pertinence du document, comme par exemple la fréquence d\u0027apparition des termes de la requête. La moyenne pondérée des valeurs des critères est par exemple souvent utilisée pour le calcul du score. Notre approche diffère dans ce sens que nous ne trions pas les résultats d\u0027une façon globale mais partielle. Les items sont comparés deux-à-deux pour déterminer un classement partiel les résultats de ce classement sont combinés pour obtenir un classement global. Dans ce but nous avons choisi d\u0027utiliser une méthode d\u0027aide à la décision et notre choix s\u0027est porté sur la méthode PROMETHEE.\nLa suite de cet article est organisée comme suit : Dans la section 2 nous clarifions certaines différences primordiales entre la recherche d\u0027information classique et la recherche d\u0027informations dans les documents XML. Ceci est important pour la suite de cet article. Nous présentons dans la section 3 l\u0027architecture générale de notre système de recherche d\u0027information XML. La section 4 est un état de l\u0027art des approches de classement dans XML. Dans la section 5 nous détaillons notre approche de classement ainsi que la méthode d\u0027aide à la décision utilisée. Et nous terminons enfin par une conclusion\nL\u0027indexation\nDeuxièmement, l\u0027indexation d\u0027un document XML, en plus du contenu, doit prendre en compte aussi la structure du document. Il est fréquent de considérer un document XML comme un arbre où les noeuds sont les éléments ou les noms d\u0027attributs, où les arêtes représentent l\u0027appartenance du noeud fils au noeud père et où les feuilles sont les contenus des éléments où les valeurs des attributs. Ainsi, l\u0027indexation de la structure revient à garder une trace des relations père-fils et frère de entre les noeuds. Pour indexer le contenu des documents XML, les technique de la RI sont reprises dans la RI-XML, mais souvent avec des adaptation liées à l\u0027existence de la structure dans les documents. En effet, l\u0027unité d\u0027indexation en RI-XML pouvant être l\u0027élément au lieu du document entier, des statistiques comme par exemple la fréquence d\u0027apparition des termes représentera dans la RI-XML la fréquence d\u0027apparition d\u0027un terme dans un élément non pas dans le document entier. Idem pour la fréquence documentaire (DF : Document Frequency) qui représentera en RI-XML le nombre d\u0027éléments et non pas de documents contenant un terme donné.\nLes résultats\nLa problématique de la RI-XML est similaire à la \"recherche de segments de documents\" (passage retrieval). En effet, les documents structurés peuvent contenir un large spectre d\u0027information hétérogènes, il est donc préférable de retrouver une partie (ou des parties) de document qui répond à la requête de l\u0027utilisateur plutôt que le document entier Wilkinson (1994). Survient alors une difficulté supplémentaire qui est de déterminer le niveau de granularité de l\u0027élément retourné. Lorsque l\u0027utilisateur exprime sa requête dans un langage structuré, il a la possibilité de définir la garnularité des éléments qu\u0027il désire. Par exemple, avec la requête XPATH suivante : //poisson[milieu\u003d\"eaux douces\"]/nom l\u0027utilisateur spécifie qu\u0027il désire seulement le contenu du noeud nom. Lorsque en revanche nous sommes en situation où l\u0027utilisateur s\u0027exprime uniquement à l\u0027aide de mots clés, le système devra calculer la granularité adéquate.\nUne vue globale de notre système de RI-XML\nLors de la conception de notre système, notre but principal était son accessibilité à l\u0027utilisateur. Ainsi, l\u0027interrogation du système devait être adaptable au niveau de l\u0027utilisateur. Les requêtes peuvent varier en complexité en partant d\u0027un simple ensemble de mots clés à de complexes expressions booléennes. Le système analyse les requêtes, et détermine les éléments les plus susceptibles de correspondre au besoin de l\u0027utilisateur et les retourne à l\u0027utilisateur triés par ordre de pertinence. Les résultats sont présentés de façon à ce que l\u0027utilisateur puisse naviguer dans la structure du document d\u0027où l\u0027élément est extrait.\nNotre système est constitué de deux parties principales (figure 1. Dans la première partie, un parseur de documents XML analyse et indexe la structure des documents et leurs contenus. Le résultats de cette analyse est sauvegardé dans une base de données. La deuxième partie est constituée de trois composants l\u0027analyseur de requête, le module d\u0027appariement et le module de classement des résultats. L\u0027analyseur de requête décompose la requête lorsqu\u0027elle contient des opérateurs booléens et crée une structure qui permet son appariement avec les documents. Le module d\u0027appariement retrouve dans la base de données les éléments les plus appropriés à la requête. Enfin le module de classement des résultats se charge de trier selon l\u0027ordre de pertinence les éléments issus du module d\u0027appariement.\nDans Abbaci et al. (2006) nous avons détaillé l\u0027indexation des documents XML dans notre système, l\u0027analyse d\u0027une requête ainsi que le processus d\u0027appariement d\u0027une requête aux documents de la base de données. Dans cet article nous présentons le fonctionnement du module de classement des éléments.\nLes approches de classement dans la RI-XML\nDans la RI un certain nombre de critères ont été décelés importants dans le jugement de pertinence d\u0027un document à une requête donnée. Quelque uns de ces critères sont devenus classiques tels la fré-quence d\u0027apparition des termes (TF) ainsi que leur pouvoir de discrimination (IDF Inverse Document Frequency). Il existe d\u0027autres critères comme par exemple la proximité des termes de la requête dans le Certains critères sont utilisés tels qu\u0027ils sont connus dans la RI dans ce cas le document XML est considéré comme un simple document texte, comme par exemple la proximité entre les termes de la requête. Ainsi, dans Sauvagnat et al. (2003) les auteurs considèrent la proximité entre deux termes comme étant le nombre de mots séparant ces termes dans une fenêtre de x termes et Kotsakis (2002)  et Sauvagnat et al. (2003) calculent TF et IDF de la même manière qu\u0027en RI. Les auteurs de Theobald et Weikum (2002) quant à eux, intègrent la proximité sémantique (calculée sur la base d\u0027une ontologie) entre les termes pour augmenter la performance de la fonction de calcul du score. D\u0027autres critères liés à la structure des documents XML sont utilisés. Nous citons la distance entre les noeuds, dans Guo et al. (2003)   Cohen et al. (2003) des poids sont attribués aux noms d\u0027élément afin de favoriser les scores des élément qu\u0027on juge plus intéressant à retourner (par exemple préférer retourner le résumé d\u0027un l ivre plutôt que son titre). Un autre critère lié à la structure des documents XML et qui rentre en compte dans le calcul du score des éléments est la spécificité des éléments à classer Guo et al. (2003)  Sigurbjörnsson et al. (2004), plus un élément est profond plus il est spécifique.\nNotre approche de classement\nLa majorité des méthodes de classement dans la RI adoptent une approche global de classement. En d\u0027autres termes, les critères de tri sont rassemblés dans une fonction unique qui doit être maximisée. L\u0027inconvénient majeur de cette approche globale réside dans le fait que les critères peuvent mutuellement se compenser. Ainsi une solution dont l\u0027un des critères présente une valeur faible peut ne pas être pénalisée si un de ses critères restants présente une valeur élevée.\nPar conséquent nous avons opté pour une approche de classement partiel qui classe les items en les comparant deux-à-deux. Dans ce qui suit, nous décrivons la méthode d\u0027aide à la décision que nous avons choisie. Le principe de cette méthode est qu\u0027elle compare les solutions paire-par-paire. Ainsi, une solution a surclasse une autre solution b si au vue de la plupart des critères, a est meilleure que b. Les résultats de la comparaison par paire sont alors combinés afin d\u0027établir un classement total des solutions.\nPROMETHEE une méthode d\u0027aide à la décision\nUne méthode d\u0027aide à la décision permet d\u0027effectuer un classement d\u0027un ensemble de solutions possibles à un problème donné en commençant par la solution la plus adéquate au vue d\u0027un ensemble de critères et de l\u0027importance relative accordée à chacun de ces derniers.\nNous décrivons dans ce qui suit la méthode PROMETHEE Vincke (1989)  \n. . .\n. . . a n f 1 (a n ) f 2 (a n ) · · · f j (a n ) · · · f k (a n )\nTAB. 1 -Table d\u0027évaluation.\nUne fonction de préférence Pj(a, b) est définie afin d\u0027attribuer un degré de préférence d\u0027une solution a à une solution b au vue d\u0027un critère fj. En général, Pj(a, b) modélise les différences des valeurs des solutions pour un critère donné d \u003d fj (a) ? fj (b). La fonction Pj(a, b) est normalisée comme suit :\nDeux paramètres q et p de seuil d\u0027indifférence et de préférence respectivement sont définis. Lorsque la différence entre les évaluations de a et b est inférieure à q alors elle n\u0027est pas significative. La fonction de préférence est donc égale à 0. Lorsque cette différence est supérieure à p elle est considérée comme trés significative et la fonction de préférence est dans ce cas égale à 1. Un classement des deux solutions a et b est construits en prenant en compte tous les critères et ce selon l\u0027expression suivante (1) :\noù wj \u003e 0 sont les poids associés aux critères. Ces poids son des nombres naturels positifs qui ne dépendent pas des échelles des critères. ?(a, b) exprime le degré de préférence de la solution a à b au vue de tous les critères. Les valeurs de ?(a, b) et ?(b, a) sont calculées pour chaque paire de solutions a, b ? A. De cette façon, une relation de surclassement est définie dans A.\nDeux flux de surclassement sont définies : -Flux de surclassement positif, qui représente la puissance d\u0027une solution par rapport à toutes les autres. Plus ? + (a) est grand plus la solution a est mieux que les autres :\nb?A,b \u003da -Flux de surclassement négatif, qui représente la faiblesse d\u0027une solution par rapport à toutes les autres. Plus ? ? (a) est petit plus les autres solutions sont mieux que a :\nP pour \"préférable à\" et I pour \"indifférent à\".  \nUn exemple\nTAB. 2 -Exemple d\u0027un problème de décision multicritère.\nLe seuil de préférence p est fixé à 0.2 pour chaque critère i.e. si deux solutions ont une différence de plus de 20% pour un critère donné, l\u0027une des deux solutions est préférable à l\u0027autre pour le critère en question. En outre, le seuil d\u0027indifférence q est fixé à 0.05 pour chaque critère i.e. si deux solutions ont une différence de moins de 0.5% pour un critère donné, les deux solutions doivent être considérées comme égale pour le critère en question. Calculons le classement de ces voitures par la méthode PROMETHEE lorsque les poids de tous les critères sont fixés à 1 (Table 3).\nLes résultats montrent que la solution idéale est la voiture numéro 3 et que la voiture la moins inté-ressante est la numéro 4. La méthode ne peut classer les deux voitures numéros 1 et 2.\nCalculons à présent le classement des ces voitures par la méthode PROMETHEE lorsque le poids du critère Consommation est fixé à 4 (Table 4). \nTAB. 4 -Le classement par la méthode PROMETHEE lorsque le critère Consommation a un poids égal à 4.\nL\u0027effet de ce changement est clair, la voiture numéro 4 devient la solution idéale.\nPROMETHEE pour classer les fragments de documents XML\nDans cette section, nous présentons les différentes étapes de l\u0027adaptation de PROMETHEE à notre problématique de classement des fragments de documents XML, ces derniers étant le résultat d\u0027une recherche d\u0027information dans une collection de documents XML. D\u0027abord nous définissons l\u0027ensemble des critères que nous souhaitons prendre en compte dans le processus du classement. Ensuite, nous associons à chaque critère son ensemble des valeurs possibles ainsi que la valeur optimale souhaitée. Lorsqu\u0027une requête est soumise à notre système, le tableau des critères est rempli avec les fragments des documents de la collection en guise de solutions.\nLes critères de classement des fragments XML\n-La pertinence du contexte (Doc.) : Nous partons du principe que la pertinence d\u0027un élément est liée à la pertinence du document qui le contient. Ainsi le score global du document \"père\" constitue un critère pour la décision concernant le classement des éléments qu\u0027il contient. Pour l\u0027instant nous proposons que Doc. représente le score classique T F * IDF en ignorant la structure du document XML. Ainsi, pour une requête Q et un élément e du document D : (q1, q2, e), la distance entre les noeuds qui contiennent respectivement les deux mots clés q1 et q2 dans l\u0027élément e est représentée par la longueur du chemin le plus court entre les deux noeuds. Cette distance est nulle lorsque les deux mots clés appartiennent au même élément.\nCpc(q1, q2, e) représente le chemin le plus court entre les deux noeuds où apparaissent respectivement q1 et q2.\n-La fréquence d\u0027apparition des mots clés (F req.) : Comme dans la RI classique, nous supposons que les termes fréquents dans un élément contribuent fortement à la description de ce dernier et par conséquent les éléments qui contiennent plusieurs occurrences des mots clés ont plus de chance d\u0027être pertinents à la requête. | e | est le nombre de noeuds de l\u0027élément e.\n-Les liens structurels entre les mots clés (Rel.) : Nous faisons référence ici à la relation de parenté dans la structure d\u0027un élément XML. Nous supposons que la relation \"ancêtre-descendant\" entre deux noeuds contenant respectivement deux mots clés de la requête est un bon indicateur de pertinence. De ce fait, nous calculons Rel. comme suit : DisV (qi, qj , e) représente la distance verticale (en profondeur dans l\u0027arbre XML) dans l\u0027élément e entre les deux noeuds qui contiennent qi et qj respectivement. ID(q) étant l\u0027identifiant du noeuds où se trouve le terme q. Ansestors(ID(q), e) est l\u0027ensemble des noeuds ancêtres du terme q dans l\u0027élément e. \nLes valeurs optimales des critères de classement\nTAB. 6 -Les échelles des valeurs des critères de classement.\nnous envisageons d\u0027effectuer plusieurs tests afin de les déterminer.\nConclusion\nNous avons présenté dans cette article notre approche pour le classement des éléments de documents XML pertinents à une requête donnée. Nous avons montré l\u0027originalité de notre approche qui se distingue par la façon de combiner les différents critères qui rentrent en jeux dans le jugement de pertinence dans\n"
  },
  {
    "id": "918",
    "text": "Introduction\nEn analyse de données symbolique (voir Bock et Diday (2000)) une variable peut, entre autre être décrite par une distribution de probabilité continue. La classification en K groupes de ces données fonctionnelles peut être obtenue en utilisant une décomposition de mélange. Mais cette technique nécessite de pouvoir calculer la densité d\u0027une distribution de fonction. Or l\u0027espace des fonctions n\u0027est pas un espace de dimension finie, tels que ceux où sont défi-nies les distributions classiques. Projeter les fonctions dans un espace multidimensionnel par échantillonnage (voir Diday (2002)) permet de contourner ce problème, pour autant que l\u0027on choisisse des distributions conjointes adéquates. Dans la section 2 de cet article nous rappel-lerons brièvement la décomposition de mélange ainsi que l\u0027algorithme des nuées dynamiques. Ensuite nous préciserons le cadre des distributions de fonctions et la construction de lois de ce type via les distributions multivariées. Nous terminerons ensuite, avant les conclusions, par une utilisation des nouveaux objets mathématiques définis pour la classification de données symboliques synthétiques.\nDécomposition de mélange\nMélange de distributions\nLa décomposition de mélange est un outil important en classification. Elle consiste en l\u0027estimation de la densité de probabilité qui est supposée avoir gouverné la génération d\u0027un échantillon de données consitué de plusieurs groupes :\noù les p i représentent les proportions de chacun des groupes (leur somme étant égale à 1), et les fonctions f (., ?) les densités de ces groupes. Chaque composante du mélange correspondant en fait à un groupe. Pour trouver la partition P \u003d (P 1 , ..., P K ) la mieux adaptée aux données deux grands algorithmes ont été proposés : EM (Estimation,Maximisation) par Dempster et al. (1977) et l\u0027algorithme des nuées dynamiques par Diday et al. (1974). Nous avons choisi d\u0027utiliser ce dernier car il avait déjà été utilisé dans le cadre de l\u0027Analyse Symbolique par Diday (2002).\nAlgorithme des nuées dynamiques\nL\u0027algorithme utilisé est en fait une extension de la méthode des nuées dynamiques (Diday et al., 1974) dans le cas d\u0027un mélange. L\u0027idée principale est, alternativement, d\u0027estimer au mieux la distribution de chaque classe, et ensuite de vérifier que chaque objet symbolique appartient à la classe de densité maximale. L\u0027étape d\u0027estimation est réalisée en maximisant un critère de qualité, ici la log-vraisemblance :\ni u?Pi La classification commence avec une partition initiale aléatoire, et les deux étapes suivantes sont donc répétées jusqu\u0027à stabilisation de la partition : -Etape 1 : Estimation des paramètres Déterminer le vecteur (? 1 , ..., ? K ) qui maximise le critère de qualité. -Etape 2 : Distribution des objets symboliques dans les classes Les classes (P i ) i\u003d1,...,K , dont les paramètres ont été calculés à l\u0027étape 1, sont construites comme suit\nCet algorithme nécessite donc de pouvoir calculer la distribution, ou plus précisément la densité de probabilité, des objets à classer. Nous avons donc besoin de préciser la notion de distribution de fonctions. \nDistribution de fonctions\nDéfinitions\nSi la notion de distribution de fonction est facile à définir, il paraît, par contre, plus malaisé de donner immédiatement un moyen de la calculer. Considérons l\u0027exemple de la figure 1. Supposons que les lignes continues forment un échantillon fonctionnel homogène. Si v est une de ces fonctions, calculer F X,D (v) peut se faire empiriquement :\n#A Mais qu\u0027en est-il pour les fonctions w et u ? Pour w on peut supposer intuitivement que la valeur de F X,D (w) est proche de 90%. Et pour u, est-ce 50%, car u est toujours supérieure à 10 des 20 fonctions de l\u0027échantillon ? Et ce malgré le fait que u soit supérieure à 12 des 20 fonctions sur plus de la moitié du domaine ? Pour solutionner ce problème de calcul, nous allons projeter dans un espace multidimensionnel les fonctions, par nature définies dans un espace de dimension infinie. \nSi nous définissons ensuite les deux ensembles suivants :\nalors nous pouvons utiliser l\u0027approximation suivante :\noù H est une distribution multivariée de dimension q. Nous pouvons donc utiliser une distribution conjointe pour approximer notre distribution fonctionnelle. Le choix de la distribution, ou de la famille de distributions, à utiliser est évidemment important. Avant de préciser ce choix, remarquons que pour une valeur choisie x ? D, il est très facile d\u0027estimer la distribution des valeurs de X(x). \nIl est assez facile de calculer G et g à l\u0027aide des techniques univariées. Ainsi, si X est un processus Gaussien, alors ces deux fonctions peuvent être calculées pour une valeur donnée de x par la fonction de répartition et la densité de la loi N (µ(x), ?(x)). Dans les cas où l\u0027on ignore la loi suivie par X(x) on utilisera l\u0027estimation empirique pour G et l\u0027estimation à noyaux pour g :\nLa Fig. 2 montre ces deux surfaces avec l\u0027exemple de la Fig. 1, dans le cas Gaussien. Etant donné qu\u0027il est très facile de calculer les marges de la distribution H par :\nl\u0027idée de reconstruire cette distribution H à partir de ses marges a été proposée par Diday (2002) en utilisant les copules archimédiennes. \nCopules archimédiennes\nLes copules sont des outils précieux dans la modélisation des structures de dépendance grâce au théorème de Sklar (voir Nelsen (1999)).\nTAB. 1 -Générateurs archimédiens\nDe plus, si F 1 , ..., F n sont toutes continues, alors C est unique ; sinon C est unique seulement sur domF 1 × ... × domF n .\nDéfinition 3.7 Les copules archimédiennes sont définies par\nreprésente la dérivée d\u0027odre k de ?.\nLe tableau 1 montre trois familles de générateurs Archimédiens. Si nous utilisons conjointement les surfaces de distributions et les copules archimédiennes, alors notre approximation (8) peut directement se récrire :\nLa densité conjointe étant donnée par l\u0027expression suivante : \nCela signifie que la limite de l\u0027expression (13), lorsque q ? ? est presque toujours nulle ! Pour éviter ce problème, nous proposons d\u0027utiliser un nouveau type de distributions basée sur les moyennes quasi-arithmétiques.\nMoyennes quasi-arithmétiques discrètes\noù ? est une fonction continue strictement monotone, et ? \u003d ? ?1 .\nLe concept de moyenne quasi-arithmétique a été introduit par Kolmogorov (1930) et Nagumo (1930, et a été étudié dans le cadre des équations fonctionnelles par Aczel (1966).\nest une distribution conjointe de marges\nNous appelons cette distribution Moyenne Quasi-Arithmétique de Marges (en anglais :QuasiArithmetic Mean of Margins (QAMM)).\nDémonstration Il suffit de remarquer que si F i est une distribution univariée, alors F * i aussi, et d\u0027ensuite utiliser ces nouvelles distributions et la copule générée par ? pour construire la distribution multivariée.\nDistribution et densité définies dans un espace de dimension infinie\nMoyennes quasi-arithmétiques continues\nEn utilisant l\u0027expression (16) et en notant x n i+1 ? x n i \u003d ? x (cf. (7)) ?i on peut écrire :  (14) comme dans le cas fini. Nous proposons donc d\u0027utiliser une densité \"directionnelle\".\nDensité de Gâteaux\nRappelons ici un concept provenant de l\u0027analyse fonctionnelle : la dérivée de Gâteaux, qui est une dérivée directionnelle (cf. Atkinson et Han (2001)). Définition 3.10 Soient V et W deux espaces vectoriels normés, et F un opérateur de V vers W . La differentielle de Gâteaux DF (u; s) de F en u dans la direction s ? V est donnée par :\nL\u0027utilisation de ce type de différentiation nécessite donc de préciser dans quelle direction elle se fait. Nous proposons d\u0027utiliser comme fonction de direction toute fonction permettant de mesurer la dispersion des données pour toute valeur de x, avec comme exemple le plus immé-diat l\u0027écart-type ?. \nDéfinition 3.11 Soient\nLa dérivée de Gâteaux d\u0027une transformée intégrale étant un résultat classique d\u0027analyse fonctionnelle (cf. Lusternik et Sobolev (1974)), nous avons le résultat suivant.\nest une mesure fonctionnelle de la dispersion des valeurs de X(x), alors la densité de Gâteaux de F X,D calculée en u dans la direction de s est donnée par :\nSoulignons ici l\u0027intérêt de diviser la différentielle de Gâteaux dans l\u0027expression (21) par la norme de s. En effet, comme on peut le constater dans (22), sans cela, la densité de Gâteaux d\u0027une Moyenne Quasi-Arithmétique Continue de Marges calculée avec deux paramétrages différents s 1 ? D s 2 pourrait donner la même valeur, pour autant que g 1 [t, u(t)] s 1 (t) \u003d g 2 [t, u(t)] s 2 (t) pour toute valeur de t ? D. La division par la norme de la mesure de dispersion permet de réintroduire cette distinction.\nDomaines des modèles\nRemarquons maintenant que le calcul de l\u0027expression (22) nécessite de pouvoir calculer g (x, y) sur l\u0027ensemble des valeurs de D et que, ceci n\u0027est en général possible que si la mesure de dispersion s est non nulle. Nous dirons que le domaine du modèle est l\u0027ensemble des réels pour lesquels s(x) \u003e 0. Nous appellerons donc domaine du modèle tout intervalle D ? {x ? R : s(x) \u003e 0}. Ainsi, dans le cas des Moyennes Quasi-Arithmétiques de Marges utilisées conjointement avec la densité de Gâteaux, nous répondons aux deux questions évoquées plus avant concernant le nombre et les choix des points x n 1 , . . . , x n q : 1. quand à la valeur de q : on le choisit très grand (QAMM), voire on le fait tendre vers l\u0027infini (QAMML) pour minimiser l\u0027erreur due à l\u0027approximation, 2. quand au choix des x n 1 , . . . , x n q : ils doivent se situer dans le domaine du modèle, c\u0027est-à-dire pour les valeurs de x où il y a dispersion non nulle des valeurs de X(x).\nIl faut noter que ces deux règles peuvent aussi s\u0027appliquer dans le cas d\u0027utilisation des copules et de la densité multivariée. En effet le même problème de calculabilité se pose avec l\u0027expression (14) si la dispersion des valeurs X(x) est nulle pour au moins une des dimensions. Mais il est évidemment conceptuellement plus difficile de définir la notion de domaine de modèle dans le cas multivarié, car cela équivaut à ne pas toujours utiliser le même nombre de dimensions (et pas nécessairement les mêmes) pour calculer une même distribution ou sa densité en plusieurs endroits. Sauf, si l\u0027on se souvient que nous ne sommes pas confrontés à de vraies données multivariées, mais à la projection en dimension q de données définies dans un espace de dimension infinie.\nApplication\nNous avons donc utilisé les Moyennes Quasi-Arithmétiques Continues de Marges, conjointement avec la densité de Gâteaux, dans le cadre de l\u0027algorithme des nuées dynamiques sur des données de type symbolique : des densités de probabilités. Pour notre test nous avons utilisé un ensemble de 140 données synthétiques mixant des exponentielles, des normales et des bétas (Fig. 3). Pour constituer cet ensemble de données, pour chaque distribution nous avons généré 500 nombres aléatoires suivant la loi choisie et ensuite nous avons réalisé une estimation à noyaux à partir de ces nombres. Les résultat fonctionnel étant stocké à l\u0027aide des fonctions splines. Remarquons que les distributions de probabilités sont des données fonctionnelles qui sont définies sur R, même si la valeur de la fonction peut être nulle sur une partie du domaine (exemple :la loi exponentielle). Or, en classification, seule une partie du domaine de la fonction est intéressante : celle où l\u0027on peut distinguer cette fonction des autres, c\u0027est-à-dire là où la fonction est non nulle (ou supérieure à fixé). Nous restreignons donc, pour des raison classificatoires, le domaine sur lequel nous calculons la Moyenne Quasi-Arithmétique Continue de Marges (ou sa densité de Gâteaux) aux valeurs distinguables de la fonction considérée. Pour cela nous utilisons une fonction de \"confiance\" :\nAvec cette fonction l\u0027expression (18) devient : \nConclusions\nDans cet article nous proposons d\u0027utiliser deux outils mathématiques nouveaux, les Moyennes Quasi-Arithmétiques de Marges et la densité de Gâteaux, dans le cadre de la décomposi-tion de mélange classifiante. Ces outils nous permettent d\u0027apporter une réponse aux questions laissées sans réponse par les travaux précédents : à savoir le nombre et le choix des points d\u0027approximation. L\u0027utilisation de ces nouveaux objets mathématiques dans le cadre de la classification non supervisée sur des données symboliques synthétiques donne des résultats encourageants. D\u0027autres outils et méthodes de classification de données fonctionnelles existent, mais l\u0027utilisation de distributions adéquates permet d\u0027obtenir une modélisation probabiliste des données. D\u0027autre part un certain nombre de développements sont encore envisageables pour affiner cet outil mathématique : l\u0027utilisation dans (18) d\u0027une distribution autre que la distribution uniforme sur D, l\u0027utilisation conjointe de la distribution des dérivées successives d\u0027une fonction u ou encore l\u0027utilisation de directions autres que s, plus discriminantes (cf. Ramsay et Siverman (2005)).\nSummary\nIndividual data can be caracterized by continuous distributions and not by a single value. Those functional data can be used to classify individuals. In a elementary solution, we can reduce distribution to mean and variance. Another richer solution is proposed by Diday (2002) and implemented by Vrac et al. (2001) and Cuvelier et Noirhomme-Fraiture (2005). It uses cut points in the distributions and models those joint values by a multidimensional distribution built with copulas. We have shown in a previous work that even if this approach gives good results, classification quality depends on the number and the place of cutpoints. The questions of number and place of cuts remains open questions. We propose a solution to these questions, when the number of cuts tends to infinity. We suggest a new distribution adapted to the space of infinite dimension. We suggest also a density which uses the Gâteaux directional derivative. The chosen direction is dispersion of functions to be classified. Results are encouraging and offer multiples perspectives in all the domains where functional data distribution is necessary.\n"
  },
  {
    "id": "919",
    "text": "Introduction\nLe volume de données stocké double actuellement tous les 9 mois (Lyman et al, 2003) et donc le besoin d\u0027extraction de connaissances dans les grandes bases de données est de plus en plus important (Fayyad et al, 2004). La fouille de données (Fayyad et al, 1996) est confrontée au challenge de traiter de grands ensembles de données pour identifier des connaissances nouvelles, valides, potentiellement utilisables et compréhensibles. Elle utilise différents algorithmes pour la classification, la régression, le clustering ou les associations.\nNous nous intéressons plus particulièrement ici aux algorithmes de Séparateurs à Vaste Marge (SVM ou Support Vector Machine) proposé par (Vapnik, 1995) car ils se montrent particulièrement efficaces pour la classification, la régression ou la détection de nouveauté. On peut trouver de nombreuses applications des SVM comme la reconnaissance de visages, la catégorisation de textes ou la bioinformatique (Guyon, 1999). L\u0027approche est systématique et motivée par la théorie de l\u0027apprentissage statistique. Les SVM sont les plus connus parmi une classe d\u0027algorithmes utilisant les méthodes de noyau (Cristianini et al, 2000). Les SVM et les méthodes de noyaux permettent de construire des modèles précis et deviennent des outils de fouille de données de plus en plus populaires. Mais malgré ces qualités, les SVM ne peuvent pas traiter facilement des données volumineuses. Les solutions des SVM sont obtenues par résolution d\u0027un programme quadratique, le coût de calcul d\u0027une approche de SVM est au moins d\u0027une complexité égale au carré du nombre d\u0027individus de l\u0027ensemble d\u0027apprentissage et la quantité de mémoire nécessaire les rend impossible à utiliser sur de grands ensembles de données. Il y a donc besoin de permettre le passage à l\u0027échelle de ces algorithmes pour traiter de grands ensembles de données sur des machines standard. Une heuristique possible pour améliorer l\u0027apprentissage à l\u0027aide de SVM est de décomposer le programme quadratique original en une série de plus petits problèmes (Boser et al, 1992), (Chang et al, 2003), (Osuna et al, 1997), (Platt, 1999). Les méthodes d\u0027apprentissage incrémental (Cauwenberghs et al, 2001), (Do et Poulet, 2006), (Do et Poulet, 2003), (Fung et Mangasarian, 2002), , (Syed et al, 1999) permettent de traiter de grands ensembles de données par mise à jour des solutions partielles en augmentant l\u0027ensemble d\u0027apprentissage sans avoir à charger l\u0027ensemble de données total en mémoire. Les algorithmes parallèles et distribués (Do et Poulet, 2006), ) utilisent des machines connectées par internet pour améliorer le temps d\u0027exécution de l\u0027apprentissage de grands ensembles de données. Les algorithmes d\u0027apprentissage actif (Do et Poulet, 2005), (Tong et Koller, 2000) permettent de choisir un sous-ensemble d\u0027individus (ensemble actif) pour la construction du modèle. Nous présentons un nouvel algorithme de boosting de LS-SVM pour la classification de grands ensembles de données sur des machines standard. L\u0027algorithme de LS-SVM proposé par (Suykens et Vandewalle, 1999) effectue un changement de la contrainte d\u0027inégalité en égalité dans la résolution du problème d\u0027optimisation permettant d\u0027obtenir la solution par résolution d\u0027un système d\u0027équations linéaires au lieu du programme quadratique. Ce nouvel algorithme est donc beaucoup plus rapide en temps d\u0027exécution. Nous avons étendu cet algorithme pour construire un nouvel algorithme de SVM incrémental, parallèle et distribué permettant de traiter des ensembles de données ayant de très grands nombres d\u0027individus. Puis nous avons ajouté un terme de régularisation de Tikhonov (Tikhonov, 1943) et utilisé la formule de Sherman-MorrisonWoodbury (Golub et Van Loan, 1996) pour permettre au LS-SVM de traiter des ensembles de données ayant un très grand nombre de dimensions. Enfin nous avons appliqué la technique du boosting au LS-SVM pour obtenir un algorithme permettant la classification d\u0027ensembles de données ayant simultanément un grand nombre d\u0027individus et de dimensions. Les performances de l\u0027algorithme sont évaluées sur des ensembles de données de l\u0027UCI (Blake et Merz, 1998), Twonorm, Ringnorm (Delve, 1996), Reuters-21578 ( Lewis, 1997) et NDC (Musicant, 1998). Les résultats sont comparés avec ceux obtenus avec LibSVM (Chang et Lin, 2003).\nLe paragraphe 2 présente brièvement l\u0027algorithme de LS-SVM, le paragraphe 3 décrit l\u0027algorithme incrémental de LS-SVM. Dans le paragraphe 4 nous présentons l\u0027algorithme de boosting de LS-SVM puis les résultats des tests numériques dans le paragraphe 5 avant la conclusion et les travaux futurs.\nQuelques notations sont utilisées dans cet article. Tous les vecteurs sont représentés par des matrices colonne. Le produit scalaire de deux vecteurs x et y est noté x.y. La norme d\u0027un vecteur v est ||v||. La matrice A (de taille mxn) contient l\u0027ensemble des m individus en dimension n. La classe (+1 ou -1) est stockée dans la matrice diagonale D (de taille mxm). e est un vecteur colonne de 1. w et b sont les coefficients et le scalaire de l\u0027hyperplan, z est la variable de ressort et C est une constante positive. I représente la matrice identité. \nL\u0027algorithme de LS-SVM\nConsidérons une tâche de classification binaire linéaire comme représentée sur la figure 1 avec m points x i (i\u003d1..m) dans l\u0027espace de R n , representés par la matrice A avec les étiquettes de classe (+1 ou -1) stockées dans la matrice diagonale D. L\u0027algorithme de SVM cherche le meilleur hyperplan de séparation des données (meilleur au sens du plus éloigné possible des deux classes). Cela revient à maximiser la marge qui est la distance entre les plans supports des deux classes. Le plan support de la classe +1 [resp. -1] sépare tous les individus de la classe +1 [resp. -1] des autres. Ceci peut s\u0027écrire sous la forme suivante (1) :\n( 1 ) La marge entre les plans support est 2/||w|| (où ||w|| représente la norme du vecteur w). Dans le cas non linéairement séparable, les contraintes doivent être relaxées pour permettre à un point d\u0027être du mauvais côté du plan support de sa classe, une variable de ressort est alors ajoutée dans la partie gauche de l\u0027équation 1. Ensuite tout point du mauvais côté de son plan support est considéré comme une erreur et à une valeur de z positive (z i \u003e 0).\nEnsuite l\u0027algorithme de SVM doit simultanément maximiser la marge et minimiser les erreurs. La formulation standard de l\u0027algorithme de SVM avec un noyau linéaire est alors le programme quadratique (2):\n( 2 ) où z représente la variable de ressort et c est une constante positive pour régler les erreurs et la taille de la marge.\nL\u0027hyperplan (w,b) obtenu est la solution du programme quadratique (2). Ensuite la classification d\u0027un nouvel individu x se base sur sa position par rapport à l\u0027hyperplan obtenu classe(x) \u003d signe (w.x-b). Les algorithmes de SVM peuvent utiliser d\u0027autres types de fonctions pour la classification comme par exemple une fonction polynomiale de degré d, une fonction RBF (Radial Basis Function) ou une sigmoïde. Le passage de cas linéaire au cas non-linéaire se fait par l\u0027utilisation d\u0027une fonction de noyau à la place du produit scalaire dans RNTI -X -l\u0027équation (2). Plus de détails sur les SVM et les méthodes de noyaux peuvent être trouvés dans (Cristianini et Shawe-Taylor, 2000). La solution des SVM est obtenue par résolution d\u0027un programme quadratique donc le coût en temps d\u0027exécution est au moins proportionnel au carré du nombre d\u0027individus et la place mémoire nécessaire les rend incapables de traiter des ensembles de données très volumineux. L\u0027algorithme de LS-SVM proposé par (Suykens et Vandewalle, 1999) utilise une égalité au lieu de l\u0027inégalité dans le problème d\u0027optimisation (2) avec la fonction ? suivante :\nEn substituant z dans la fonction objectif ? du programme quadratique (2), nous obtenons alors (3) : \noù E\u003d[A -e] (juxtaposition de la matrice A avec une colonne de -1), I° est la matrice diagonale identité dont le dernier élément est 0.\nLa formulation du LS-SVM (6) nécessite la résolution d\u0027un système linéaire à n+1 inconnues au lieu du programme quadratique (2), donc si le nombre de dimensions de l\u0027ensemble de données est inférieur à 10 5 , l\u0027algorithme de LS-SVM (tableau 1) est capable de traiter un très grand nombre d\u0027individus en un temps restreint sur une machine standard. Les tests numériques ont montré des résultats satisfaisants en comparaison à des algorithmes comme libSVM mais en se montrant beaucoup plus rapide. Par exemple, la classification d\u0027un million de points en dimension 20 est effectuée en 1,3 seconde sur un PC (Pentium IV, 3GHz, 512Mo RAM). Pour traiter le cas de la classification non-linéaire, il faut remplacer la matrice A en entrée de l\u0027algorithme par la matrice de noyau non linéaire K, par exemple :\n2 ) L\u0027algorithme de LS-SVM utilisant une matrice de noyau nécessitera aussi un temps de calcul et une place mémoire importante. \nLS-SVM incrémental\nBien que l\u0027algorithme de LS-SVM soit efficace et rapide pour la classification de grands ensembles de données, il nécessite de charger l\u0027ensemble des données en mémoire. Avec de très grands ensembles de données, par exemple un milliard de points en dimension 20, l\u0027espace mémoire nécessaire est de 80Go. La plupart des algorithmes de classification actuels sont confrontés à ce problème. Nous allons nous intéresser à ce cas de figure, le traitement de très grands ensembles de données. Les algorithmes de classification incrémentaux (Do et Poulet, 2003, 2006, ) sont une méthode très efficace pour traiter de très grands ensembles de données car ils ne nécessitent pas le chargement de la totalité des données en mémoire : seul un petit bloc de données est considéré à un instant donné et le modèle est construit par modifications successives.\nLS-SVM incrémental en ligne\nSupposons que nous avons à traiter un ensemble de données ayant un très grand nombre de points et un nombre plus restreint de dimensions, nous pouvons décomposer cet ensemble de données en blocs de lignes A i , D i . La version incrémentale en ligne de LS-SVM va calculer la solution de l\u0027équation (6) de manière incrémentale. Considérons un exemple simple avec un ensemble de données décomposé en deux blocs de lignes \nA partir des équations (7), (9) et (10), on peut en déduire l\u0027équation (11) de l\u0027algorithme de LS-SVM incrémental en ligne, avec un ensemble de données décomposé en k blocs de\nL\u0027algorithme incrémental en ligne de LS-SVM du tableau 2 peut donc classifier des RNTI -X -données très volumineuses sur une machine standard. La précision de l\u0027algorithme est exactement la même que celle de l\u0027algorithme original. Si le nombre de dimensions de l\u0027ensemble de données est inférieur à 10,000 alors l\u0027algorithme est tout à fait capable de classifier des ensembles de données de plusieurs milliards d\u0027individus sur une machine standard. Entre deux étapes successives de l\u0027algorithme il n\u0027est nécessaire de conserver en mémoire qu\u0027une matrice de taille (n+1) -initialiser : \nLS-SVM incrémental en colonne\nCertaines applications comme la bioinformatique ou la fouille de textes nécessitent de traiter des données ayant un nombre très important de dimensions et un nombre d\u0027individus plus réduit. Dans ce cas la matrice de taille (n+1)x(n+1) est trop importante et la résolution du système à (n+1) inconnues nécessite un temps de calcul élevé. Pour adapter l\u0027algorithme à ce type de données nous avons appliqué la formule de Sherman-Morrison-Woodbury au système d\u0027équations (6). Mais ce faisant nous avions une matrice singulière à inverser (I°). Nous avons donc ajouté un terme de régularisation de Tikhonov, ce qui est la méthode la plus couramment utilisée pour résoudre ce genre de problème. Avec le terme de Tikhonov (?\u003e0) ajouté à (6) nous obtenons alors le système d\u0027équations (12) [ ]\nCe système peut être réécrit sous la forme suivante (13) :\noù H représente la matrice (n+1)x(n+1) diagonale dont le (n+1)ème terme est ? et les autres termes valent (1/c)+?. Ensuite nous appliquons la formule de Sherman-MorrisonWoodbury (14) dans la partie droite du système (13) : \nBoosting de LS-SVM\nPour pouvoir traiter des ensembles de données ayant simultanément un grand nombre d\u0027individus et de colonnes il y a au moins deux problèmes à résoudre : le temps d\u0027apprentissage devient rapidement déraisonnable et la quantité de mémoire nécessaire dépasse les capacités de mémoire des machines courantes. Bien que les algorithmes incrémentaux de LS-SVM puissent efficacement charger en mémoire des petits blocs de données successifs, ils nécessitent l\u0027inversion de matrice de taille (mxm) ou (n+1)x(n+1). La quantité de mémoire nécessaire et le coût de calcul deviennent trop importants. Pour pouvoir traiter des ensembles de données très volumineux, nous avons donc appliqué l\u0027approche du boosting au LS-SVM de manière analogue à . Cette solution présente deux avantages : résoudre le problème de passage à l\u0027échelle et conserver la précision de l\u0027algorithme original. Plus de détails sur le boosting peuvent être trouvés dans (Freund et Schapire, 1999) ou sur le site www.boosting.org. Nous décrivons brièvement ici le mécanisme de boosting de LS-SVM. Dans les années 1990, Freund et ses collègues ont introduit le boosting pour améliorer la précision des algorithmes d\u0027apprentissage. La méthode de boosting consiste à utiliser k fois un algorithme d\u0027apprentissage basique en se concentrant à chaque étape sur les erreurs commises à l\u0027étape précédente. Pour ce faire, il est nécessaire de tenir à jour une distribution de poids sur l\u0027ensemble des individus de l\u0027apprentissage. Initialement, tous les poids sont identiques et à chaque étape du boosting le poids des individus mal classifiés est augmenté pour obliger l\u0027algorithme à les prendre en compte de manière plus significative. Nous considérons l\u0027algorithme de LS-SVM comme l\u0027algorithme d\u0027apprentissage basique et à chaque étape du boosting nous échantillonnons un sousensemble d\u0027individus en tenant compte de la distribution des poids. Il faut remarquer que le LS-SVM n\u0027effectue l\u0027apprentissage que sur ce sous-ensemble d\u0027individus (de taille moindre que l\u0027ensemble original). La taille de l\u0027échantillon est inversement proportionnelle aux nombres d\u0027étapes du boosting. Les algorithmes de LS-SVM incrémentaux en ligne ou colonne peuvent ainsi être adaptés au traitement de très grands ensembles de données (à la RNTI -X -fois en nombre d\u0027individus et de dimensions), avec de bons résultats en précision et besoin en mémoire.\nQuelques résultats\nNous avons développé le programme en C/C++ sous Linux en utilisant la librairie Lapack++ (Dongarra et al, 1993) pour bénéficier de bonnes performances en calcul matriciel. Le programme peut donc classifier de très grands ensembles de données efficacement. Nous allons en présenter une évaluation prenant en compte les critères suivants : la précision, le temps d\u0027apprentissage et la place mémoire requise. Nous avons sélectionné 3 ensembles de données artificiels générés par Twonorm, Ringnorm et NDC et 8 ensembles de données de l\u0027UCI. Les caractéristiques de ces ensembles sont décrites dans le tableau 3 (les attributs catégoriques des ensembles Adult et Mushroom ont été convertis en binaire).\nNous avons utilisé le nouvel algorithme de boosting de LS-SVM (Boost-LS-SVM) et LibSVM (l\u0027un des algorithmes de SVM les plus efficaces) pour effectuer la classification sur un PC (Pentium IV, 3GHz et 512Mo RAM). Les 8 premiers petits ensembles de données sont utilisés pour comparer la précision et le temps d\u0027apprentissage (tableau 4). Boost-LS-SVM obtient de meilleures précisions dans tous les cas sauf un et un meilleur temps d\u0027apprentissage dans la moitié des cas. On peut remarquer que le temps d\u0027apprentissage de libSVM croit de manière très importante lorsque la taille des fichiers augmente. Par exemple sur l\u0027ensemble de données Adult, Boost-LS-SVM est 190 fois plus rapide que libSVM.\nReuters-21578 est un ensemble de données réputé pour la catégorisation de textes. Nous avons utilisé Bow (McCallum, 1998) en prétraitement de ces données. Chaque document est vu comme un vecteur de mots, nous avons obtenu 29406 mots (dimensions) sans sélection de dimensions. Nous avons effectué la classification des 10 classes les plus nombreuses. Cet ensemble de données ayant plus de deux classes nous avons utilisé l\u0027approche one-againstall. Les résultats sont présentés dans le tableau 5 avec la moyenne de la précision et du rappel (breakeven point) pour les 10 catégories. Boost-LS-SVM a obtenu une meilleure précision pour 9 des 10 catégories mais le temps d\u0027exécution du Boost-LS-SVM est deux fois plus long que celui de LibSVM car le nombre d\u0027itérations est important pour arriver à la même précision.\nTemps (secs)\nPrécision ( Deux grands ensembles de données sont utilisés pour évaluer le temps d\u0027exécution et la quantité de mémoire nécessaire aux algorithmes. LibSVM nécessite de charger la totalité de l\u0027ensemble de données en mémoire, nous avons donc étendu la capacité de la RAM. Pour l\u0027ensemble de données Forest Cover Type, nous avons effectué la classification des deux classes les plus nombreuses (Spruce-Fire : 211840 individus et Lorgepole-Pine : 283301 individus en dimension 24). Nous avons généré un ensemble de données de 55000 individus en dimension 20000 (2 classes) avec le programme NDC. Pour les deux ensembles de données, LibSVM n\u0027a pu donner de résultat : pour Forest Cover Type, le programme a tourné pendant 21 jours et pour le second, il n\u0027a pas pu être chargé en mémoire (4Go).\nBoost-LS-SVM n\u0027a utilisé que 512Mo de mémoire mais il a du relire les données à chaque étape de boosting (96% du temps est ainsi passé à charger les données en mémoire vive). Les résultats présentés dans le tableau 6 montrent que Boost-LS-SVM est capable d\u0027effectuer la classification d\u0027ensembles de données ayant simultanément un grand nombre d\u0027individus et de dimensions sur une machine standard dans un temps raisonnable.\nRAM (MB)\nPrécision ( \nConclusion et perspectives\nNous avons présenté un nouvel algorithme de boosting de LS-SVM capable d\u0027effectuer la classification de grands ensembles de données sur des machines standard. L\u0027idée principale est d\u0027étendre l\u0027algorithme récent de Suykens et Wandewalle pour en construire une version incrémentale et un boosting de LS-SVM. La précision des nouveaux algorithmes est exactement la même que celle de l\u0027algorithme original. La complexité de la version incrémentale en lignes est linéaire en nombre d\u0027individus. Si le nombre de dimensions est suffisamment restreint (inférieur à 10000), il permet de classifier plusieurs milliards de données sur un simple PC. Quelques applications comme la bioinformatique ou la fouille de texte utilisent des données dont le nombre de dimensions est très important et le nombre d\u0027individus plus faible, nous avons ajouté un terme de régularisation de Tikhonov et utilisé la formule de Sherman-Morrison-Woodbury pour construire la version incrémentale en colonne de l\u0027algorithme de LS-SVM et traiter les ensembles de données ayant un grand nombre de dimensions. Puis nous avons étendu ces algorithmes en utilisant la technique du boosting pour la classification d\u0027ensembles de données ayant simultanément un grand nombre de dimensions et d\u0027individus. Les résultats des tests numériques montrent que le nouvel algorithme de boosting de LS-SVM est rapide et de bonne précision. Il permet le passage à l\u0027échelle et obtient de bons taux de précision en comparaison à libSVM (l\u0027un des algorithmes de SVM les plus efficaces). Pour des petits ensembles de données il présente un bon taux de précision et une bonne rapidité d\u0027exécution. Pour des ensembles de données de très grandes tailles (à la fois en nombre de dimensions et d\u0027individus), il a montré ses possibilités avec un bon taux de précision.\nUne première extension de ces travaux va consister à étendre cet algorithme pour en faire une version parallèle et distribuée sur un ensemble de machines. Cette extension permettra d\u0027améliorer le temps de la tâche d\u0027apprentissage. Une seconde sera de proposer une nouvelle approche pour la classification non linéaire.\nRemerciements. Nous tenons à remercier vivement Jason Rennie du MIT pour son aide sur la préparation de l\u0027ensemble de données Reuters-21578.\n"
  },
  {
    "id": "922",
    "text": "Résumé. Cet article présente un modèle pour aborder les problèmes de classement difficiles, en particulier dans le domaine médical. Ces problèmes ont souvent la particularité d\u0027avoir des taux d\u0027erreurs en généralisations très élevés et ce quelles que soient les méthodes utilisées. Pour ce genre de problèmes, nous proposons d\u0027utiliser un modèle de classement combinant le modèle de partitionnement des cartes topologiques mixtes et les machines à vecteurs de support (SVM). Le modèle non supervisé est dédié à la visualisation et au partitionnement des données composées de variables quantitatives et/ou qualitatives. Le deuxième modèle supervisé, est dédié au classement. La combinaison de ces deux modèles permet non seulement d\u0027améliorer la visualisation des données mais aussi en les performances en généralisation. Ce modèle (CT-SVM) consiste à entraîner des cartes auto-organisatrices pour construire une partition organisée des données, constituée de plusieurs sous-ensembles qui vont servir à reformuler le problème de classement initial en sous-problème de classement. Pour chaque sous-ensemble, on entraîne un classeur SVM spécifique. Pour la validation expérimentale de notre modèle (CT-SVM), nous avons utilisé quatre jeux de données. La première base est un extrait d\u0027une grande base médicale sur l\u0027étude de l\u0027obésité réalisée à l\u0027Hôpital Hôtel-Dieu de Paris, et les trois dernières bases sont issues de la littérature.\nIntroduction\nEn apprentissage artificiel, on distingue deux grands thèmes, l\u0027apprentissage supervisé et l\u0027apprentissage non supervisé ; la plupart des problèmes d\u0027apprentissage sont traités par l\u0027une des deux approches. Selon les problèmes de classement, on a recours à de nombreuses mé-thodes telles que les machines à vecteurs de support (SVM) qui sont évaluées sur leur capacité à prédire correctement la classe des observations. Pour l\u0027apprentissage non supervisé, on utilise souvent le modèle des cartes topologiques où les critères de qualité sont plus difficiles à dé-finir ; ils s\u0027articulent autour de l\u0027interprétation des regroupements ou des partitions obtenues. Parmi les problèmes d\u0027apprentissage, il existe une catégorie de problèmes qui sont appelés dans la littérature : difficiles, complexes, et plus particulièrement le problème traité dans ce papier concernant des données mixtes avec des variables quantitatives et qualitatives.\nUne catégorie de modèles d\u0027apprentissage spécifiques, combinant l\u0027apprentissage non supervisé et supervisé, aussi bien dans le domaine de la classification hiérarchique et les arbres de décision que pour la recherche de partitions ont été développées, pour ce type de problème ; mais la majorité de ces modèles ne traitent que des données numériques, (Liu et al (2001); Rybnik et al (2003); Lebrun et al (2004); Sungmoon et al (2004); Shaoning et al (2005); Benabdeslem (2006)). Dans Wu et al (2004), les auteurs proposent d\u0027utiliser les cartes topologiques de Kohonen (Kohonen (1995)) pour filtrer les données. Les observations non étiquetées héritent de l\u0027étiquette de la classe du vote majoritaire de son sous-ensemble. A la fin de cette phase, un seul SVM est appris sur l\u0027ensemble d\u0027apprentissage initial réétiqueté, sans tenir en compte la partition des données. D\u0027autre méthodes sont aussi inspirées des méthodes de partitionnement et de classement comme la définition de cartes topologiques dans l\u0027espace de redescription (Sungmoon et al (2004)) ou l\u0027utilisation des vecteurs supports pour définir une partition (Ben-Hur et al (2001)).\nNotre approche est dédiée aux données numériques et/ou données mixtes, elle consiste à diviser le problème global de classement en sous-problème de classement guidé par la structure et l\u0027organisation des données de la base en utilisant les cartes topologiques mixtes, (Lebbah et al (2005)). Le partitionnement des données avec les cartes, en une partition constituée de plusieurs sous-ensembles organisés vont servir à définir un classifieur pour chacun en utilisant les SVMs, Vapnik (1995). Les cartes topologiques mixtes sont utilisées dans notre modèle parce qu\u0027elles sont de plus en plus utilisées comme outil de visualisation et de partitionnement non supervisé de différents types de données quantitatives et qualitatives codées en binaires. Elles permettent de projeter les données sur des espaces discrets qui sont généralement de dimensions deux et d\u0027avoir des prototypes (représentants) du même type que les données initiales (quantitatives et qualitatives). Le modèle de base, proposé par Kohonen (Kohonen (1995)), est uniquement dédié aux données numériques. Les machines à vecteurs de support ont été développées dans les années 90 par Vapnik (1995). Ces méthodes ont été utilisées dans notre modèle parce qu\u0027elles s\u0027avèrent particulièrement efficaces. Elles peuvent traiter des problèmes mettant en jeu un grand nombre de variables tout en assurant une solution unique (pas de problèmes de minimum local comme pour les réseaux de neurones). L\u0027algorithme sous sa forme initiale revient à chercher une frontière de décision linéaire entre deux classes, mais ce modèle peut considérablement être enrichi en se projetant dans un autre espace permettant ainsi d\u0027augmenter la séparabilité des données.\nPour la compréhension de notre modèle, nous présentons dans la section 2, les différentes notations utilisées. Pour simplifier la présentation du papier, le modèle SVM et le modèle des cartes topologiques ne seront pas présentés. Dans la section 2, nous présentons la combinaison des deux modèles que nous proposons d\u0027utiliser pour le classement. Dans la section 3.1, une validation du modèle sur des données issues de la littérature ainsi que des données médicales réelles. Cette validation pemet de démontrer que notre modèle peut être utilisé pour augmenter les performances en classement du SVM sur ce type de bases de données.\nMéthode hybride Cartes topologiques et SVM : CT-SVM\nOn suppose que l\u0027on dispose d\u0027une base d\u0027apprentissage A \u003d {(z i , y i ); i \u003d 1..N, z i ? D} où l\u0027observation est z i , y i l\u0027étiquette de sa classe utilisé pour l\u0027apprentissage du modèle SVM, et D représente l\u0027espace des observations de dimension d. Les observations z i sont composées de deux parties : la partie numérique z\nComme tout modèle de cartes topologiques, nous supposons que l\u0027on dispose d\u0027une carte discrète C ayant N cell . Cette structure de graphe permet de définir une partition de D en N cell sous-ensembles qui sera notée P \u003d {P 1 , ..., P N cell }. A chaque sous-ensemble P c , on associe un vecteur référent w c ? D qui sera le représentant ou le \"résumé\" de l\u0027ensemble des observations de P c . Par la suite nous notons W \u003d {w c \u003d (w m suivant le même codage binaire que les données initiales, ce qui simplifie l\u0027interprétation des référents. La partition P de D peut être définie d\u0027une manière équivalente avec la fonction d\u0027affectation de la carte ? qui est une application de D dans l\u0027ensemble fini des indices I \u003d {1, 2, ..., N cell }. Dans le cas où il y a eu regroupement des sous-ensembles, nous avons défini une application surjective ? de I dans l\u0027ensemble des indices J \u003d {1, 2, ..., S} où 1 ? S ? N cell . Si on utilise ces définitions, le sous-ensemble P c est alors représenté par P c \u003d {z ? D/?(z) \u003d c, ?(c) ? J }, (si ?(c) \u003d 1 alors P \u003d P c \u003d A). On notera par la suite, l\u0027ensemble des indices I p des sous-ensembles purs tel que I p \u003d {c/?z ? P c , ?(?(z)) \u003d c, vote(P c ) \u003d y c }. y c est l\u0027étiquette du vote majoritaire à 100% du sous-ensemble P c en utilisant la fonction vote. Par la suite, nous présentons un modèle de classement qui permet d\u0027augmenter les performances en classement du SVM en utilisant le partitionnement des observations par les cartes topologiques.\nDans (Kuncheva , 2004, chapitre 6), l\u0027auteur fournit une démonstration théorique pour ce type de modèles combinant partitionnement et classement. Si l\u0027on considère que l\u0027on dispose de S classeurs notés Cla associés à différents sous-ensembles P i et si on note par p(Cla i /P i ) la probabilité du classement correct avec le classifieur Cla i dans le sous-ensemble P i , alors la densité de probabilité du classement correct de notre système de partitionnement et de classement s\u0027écrit :\nNotre approche consiste à entrainer des SVMs (Cla i \u003d SV M ) différents avec des sousensembles d\u0027une partition P de la la base A. Ceci permet de redéfinir des espaces de redescription différents (ou les mêmes) pour chaque sous-ensemble P c ? P, L\u0027objectif de notre méthode CT-SVM est d\u0027améliorer la discrimination en entraînant un SVM pour chaque sous-ensemble P c ? P qui a plus d\u0027une classe (les sous-ensembles non purs). Pour les sous-ensembles qui sont composés d\u0027observation de la même classe, aucun un SVM ne sera entrainé. Afin de réduire la partition et par conséquent le nombre de SVMs entrainés, nous avons utilisé la classification hiérarchique (CAH), sur l\u0027ensemble des référents W de la carte pour réduire la partition ainsi le nombre de sous-ensembles, Yacoub et al (2001). Cette phase de réduction de la partition, qui consite à fusionner certains sous-ensembles, est optionnelle et elle peut être déterminée en interaction avec les experts et après visualisation des cartes topologiques ou avec un autre indice de regroupement,  .\nL\u0027algorithme de note modèle CT-SVM est le suivant : Pour un nombre de sous-ensembles S fixé faire : -Phase 1 : Construction d\u0027une partition P \u003d {P 1 , ..., P N cell } en utilisant les cartes topologiques mixtes constituées de N cell cellules. -Phase 2 (optionnelle) : Si S \u003c N cell appliquer l\u0027algorithme de regroupement pour construire la nouvelle partition P \u003d {P 1 , ..., P S /1 ? S ? N cell } -Phase 3 : Détecter l\u0027ensemble des indices I p des sous-ensembles pures tel que I p \u003d {c/?z ? P c , ?(?(z)) \u003d c, vote(P c ) \u003d y c }. y c est l\u0027étiquette du vote majoritaire à 100% du sous-ensemble P c (toutes les observation de P c portent la même étiquette y c ). -Phase 4 : Apprentissage d\u0027une SVM pour chaque sous-ensemble P i tel que i / ? I p .\nRemarque :\nPour l\u0027apprentissage des cartes topologiques mixtes, nous avons utilisé notre programme déve-loppé en C/C++. Pour le regroupement des sous ensembles nous avons utiliser la classification hiérarchique. Nous avons aussi utilisé les programmes et l\u0027heuristique développée par l\u0027équipe de Kohonen, , pour estimer la dimension de la carte. Pour l\u0027apprentissage du modèle SVM, nous avons utilisé la bibliothèque des programmes DAG-SVM (Directed Acyclic Graph SVM) dévellopé par Platt et al (2000); Cawley (2000). Avec ce modèle CT-SVM, la topologie des observations est préservée grâce aux cartes topologiques. Lorsqu\u0027on présente une nouvelle observation qui n\u0027a pas participé à la phase d\u0027apprentissage, elle sera projetée d\u0027abord sur la carte topologique avec la fonction d\u0027affectation associée ?. Puis, on utilisera la fonction d\u0027affectation ? (voir §2), pour selectionner le sous-ensemble qui va déterminer le classifieur SVM associé. Cette methode d\u0027affectation de notre classement permet de comprendre le comportement d\u0027une observation à travers son réfé-rent w c et/ou redéfinir une nouvelle partition en interaction avec l\u0027expert. Si on note par svm r la fonction de classement du modèle SVM du sous-ensemble P r alors la fonction d\u0027affectation globale de notre système s\u0027écrit comme suite :\noù I p est l\u0027ensemble des indice des sous-ensembles purs. ?(c) \u003d c si P \u003d {P 1 , ..., P c , ..., P N cell } et ?(c) \u003d 1 si P \u003d A .  (2006)). La base de données comporte des variables cliniques et biologiques, recueillies avant l\u0027intervention chirurgicale. Les patients sont classés en deux groupes (oui/non) suivant la médiane de perte de poids observée 3 mois après la chirurgie (gastroplastie par anneau ajustable ou bypass gastrique). Si la perte de poids est supérieure à la médiane, le patient est étiqueté \"oui\". Sinon il est étiqueté par \"non\". Chaque patient est caractérisé par 37 variables réelles (par exemple, le poids, le BMI, ALAT, ASAT, HDL, CRP...) et 13 variables qualitatives (exemple : diabète oui/non), caractérisant l\u0027obésité et ses aspects cliniques et métaboliques, ainsi que ses complications multiples.\nPour étudier le comportement de notre modèle en classement, nous avons procédé à une validation croisée en variant le nombre de sous-ensembles de la partition et par conséquent, le nombre d\u0027observations associées à chaque apprentissage d\u0027un SVM. Ainsi, nous avons dé-coupé la base complète en trois sous bases de même taille, B 1 , B 2 , B 3 . On apprend sur deux bases parmi les trois et on teste les performances en classement sur la troisième en utilisant l\u0027étiquette de perte de poids (oui/non) à trois mois. Ainsi, en utilisant le modèle CT-SVM ( §2), trois cartes topologiques sont construites de dimension 3 × 4, ce qui fournit une partition de 12 sous-ensembles (N cell \u003d 12). Pour montrer l\u0027importance de la taille de la partition, nous avons calculé les performances en classement en, variant le nombre de sous ensembles de 1 à 12. Dans le premier cas, l\u0027application de notre modèle CT-SVM sur une partition avec un seul sous-ensemble est équivalente à entraîner un SVM binaire classique sur toute la base.\nLa figure 1 montre les trois variations du taux de bon classement des trois bases de test en fonction du nombre de sous-ensembles de la même partition. Dans le cas où la partition contiendrait un seul sous-ensemble, un seul SVM est entraîné sur toute la base. Ainsi, dans ce cas particulier, la fonction d\u0027affectation des cartes topologiques ? n\u0027influe pas sur la fonction d\u0027affectation globale de notre modèle CT-SVM (formule 1). On observe aussi dans la figure 1, que l\u0027augmentation du nombre de sous-ensembles de la partition permet d\u0027augmenter les performances en classement sur les trois tests. En revanche, on constate que lorsque la taille de la partition est très grande, les performances diminuent. La partition contenant plusieurs sous-ensembles permet d\u0027apprendre autant de SVMs que de sous-ensembles. Ainsi, la fonction d\u0027affectation globale de notre modèle (formule 1) utilise d\u0027abord la fonction d\u0027affectation des cartes topologiques ? pour choisir le sous-ensemble, ainsi le SVM associé avec sa fonction d\u0027affectation svm. Avec le premier test, on obtient au maximum, un taux de bon  classement de 60.6% avec trois sous-ensembles ; avec le deuxième test, on obtient un taux de bon classement 70.6% avec trois sous-ensembles. Finalement, avec le troisième test, on obtient 55.9 avec quatre sous-ensembles. Dans l\u0027entraînement des SVMs avec notre modèle CT-SVM, nous avons utilisé la même fonction noyau linéaire. La validation croisée avec une variation du nombre de sous-ensembles montre l\u0027intérêt et la difficulté de choisir la bonne partition pour une bonne discrimination. Cette partition est déterminée dans notre cas par expérimentation et visualisation des cartes topologiques. Cette validation croisée montre aussi l\u0027intérêt de subdiviser le problème de classement global en sous-problèmes de classement pour améliorer les performances en classement.\nFIG. 1 -Taux de bon classement avec CT-SVM en fonction du\nDiscussion\nPuisque notre modèle utilise les cartes topologiques, on dispose d\u0027un pouvoir de visualisation de la partition. L\u0027application d\u0027abord des cartes topologiques mixtes, va nous permettre d\u0027analyser la répartition des observations et par conséquent les sous-ensembles qui ont servi au classement avec le SVM. Cette discussion va nous permettre de montrer l\u0027intérêt de projeter les patients sur la carte pour comprendre le comportement et le profil de perte de poids du patient après chaque classement. L\u0027apprentissage d\u0027une carte de dimension 3 × 4 cellules effectué sur la base entière des patients, fournit pour chaque cellule un référent w c composé de deux parties : la partie quantitative w r c et la partie qualitative w b c codée avec le codage disjonctif binaire. La figure 2.a présente la répartition des observations. On observe que la partition obtenue a permis de bien distribuer les observations sur 12 cellules de l\u0027ensemble de la partition P \u003d {P 1 , ..., P 12 }, mais pour cet exemple, aucun des ces sous ensembles n\u0027est pur. La figure 2.b présente la même répartition en distinguant ceux qui ont perdu ou non du poids à 3 mois par rapport à la médiane de l\u0027ensemble des patients. On constate que les sousensembles sont mélangés. A l\u0027aide de cette carte topologique 3 × 4, il est possible d\u0027effectuer un certain nombre d\u0027analyses de la base étudiée. Notre premier objectif est celui de partition-ner les données, en tenant compte de leurs spécificités (données mixtes) pour augmenter les performances en classement. Pour visualiser la carte topologique, nous nous sommes limités à analyser les effets dûs à quelques variables pour lesquels l\u0027exactitude des propriétés médi-cales retrouvées peuvent êtres vérifiées. En observant à la fois les deux figures 2.a et 2.b le médecin a détecté globalement trois grands groupes. Pour s\u0027approcher de la partition du mé-decin, nous avons appliqué la CAH avec les référents de la carte pour avoir 4 sous-ensembles, P \u003d {P 1 , P 2 , P 3 , P 4 }. La figure 7 présente la partition avec 4 sous-ensembles numéroté de 1 à 4. Cette répartition des données en quatre sous-ensembles et la répartition du médecin en trois sous-ensembles correspondent à la taille de la partition utilisée dans la phase de la validation croisée décrite ci-dessous.\nEn visualisant à la fois les figures 2,3, 4, 5, 6 et la figure 7, il est possible de demander au médecin de définir les profils des patients. Ces profils vont servir à décrire les paramètres (variables) liés à la perte de poids et fournir des hypothèses de travail sur la résistance à la perte de poids fournies par le classeur SVM. Trois grands profils de patients sont définis selon la perte de poids à 3 mois. Le profil 1 est plutôt un bon profil par rapport aux pertes de poids à trois mois (figure 2.b) et correspond aux deux sous-ensembles P 1 et P 2 de la CAH. Le profil 2 est caractérisé par une perte de poids moyenne à 3 mois et correspond approximativement au sous ensemble P 4 de la CAH. Enfin le profil 3 est caractérisé par une perte de poids médiocre à 3 mois, ce qui aboutit à dénommer ce profil comme un \"mauvais\" profil en terme de perte de poids. Ce profil correspond au sous-ensemble P 3 de la CAH. Nous détaillons par la suite les deux profils 1 et 3 par rapport aux différentes variables clinico-biologiques.\nLe profil 1 est caractérisé par un poids, un BMI (Body Mass Index) et une Dépense Energé-tique de Repos mesurée par calorimétrie (DERm) élevés. Les patients appartenants à ce profil ont une glycémie à jeûn et insulinémie élevées (figure 3) sans être diabétiques (figure 4). Il s\u0027agit donc de patients insulinorésistants avant le stade de diabète. Le reste du profil méta-bolique est caractérisé par des HDL plutôt bas, des triglycérides (TG) et enzymes hépatiques (ASAT, ALAT et GGT) élevées,(figure 3). Dans les classes qualitatives \"HTA\" (hypertension) ou \"SAS\" (Syndrome d\u0027apnées du sommeil) ces patients sont classés \"oui\" (figures 5 et 6). D\u0027un point de vue inflammatoire, la CRP, la férritinémie (FERR), la SAA et l\u0027orosomucoide (ORO), toutes des protéines de la phase aigue de l\u0027inflammation, sont modérément élevées. Sur le plan nutritionnel, la TSH est basse, le profil protéique (albumine, préalbumine, RBP) et vitaminique est favorable, sans déficit. En conclusion pour ce profil, il s\u0027agit de patients avec un poids très élevé mais dont le profil métabolique (figure 3) n\u0027est pas trop évolué (sans diabète), sans inflammation importante et un bon profil nutritionnel.\nLe profil 2 correspond à des patients ayant un BMI élevé et une leptine élevée (LEP, figure  3). Ils sont insulinorésistants mais pas diabétiques. Ils ont majoritairement une HTA et un SAS (figures 5 et 6). Les paramètres hépatiques et métaboliques sont normaux. L\u0027adiponectinémie (ADIPO) est plutôt basse. En revanche, les paramètres inflammatoires (SAA et CRP) sont très élevés. Sur le plan nutritionnel, la TSH est normale, haute et les marqueurs nutritionnels sont bas (bilan protéique avec albumine, préalbumine et RBP, fer, vitamines A, E, B1, B12). Le profil 3 est un profil intermédiaire en terme de paramètres clinicobiologiques.\nEn conclusion, les deux profils de patients 1 et 2 sont caractérisés par des paramètres clinico-biologiques différents, notamment en terme de marqueurs d\u0027inflammation et nutritionnels et sont aussi différents en termes de profil de perte de poids à 3 mois. Nous pouvons donc formuler l\u0027hypothèse que le statut nutritionnel et l\u0027état d\u0027inflammation des patients avant chirurgie pourraient être des éléments liés à la résistance à la perte de poids. \nBases issues de la littératures\nDans cet exemple, trois bases d\u0027apprentissage comportant un nombre variable d\u0027observations ont été utilisées, (table 1)  Puisque toutes les variables sont quantitatives, l\u0027utilisation des cartes topologiques mixtes se réduit pour ces bases à l\u0027application de cet algorithme avec l\u0027hyper-paramètre F \u003d 0 qui correspond à la version batch des cartes topologiques classiques de Kohonen. Afin de mesurer la robustesse de notre système, l\u0027apprentissage de notre modèle CT-SVM est réalisé sur les bases d\u0027apprentissage présentées dans la table 1. L\u0027affectation des observations de la base de test est réalisée à l\u0027aide de la fonction d\u0027affectation de notre modèle CT-SVM, présentée par la formule (1). La \nConclusion\nDans cet article, nous avons présenté un modèle de classement hybride, associant une mé-thode de partitionnement et une méthode de classement qui sont respectivement, les cartes topologiques et les SVMs. Ce modèle utilise l\u0027organisation des données fournis par les cartes topologiques mixtes pour subdiviser l\u0027espace des données afin d\u0027apprendre un SVM spécifique pour chaque sous-espace des données. Notre modèle CT-SVM utilise la partition résultat des cartes topologiques, pour associer un SVM à chaque sous-ensemble de la partition avec des hyper-paramètres différents si cela est nécessaire. Les expériences effectuées montrent la robustesse de celui-ci à traiter des bases classiques avec uniquement des données réelles ou des données mixtes. D\u0027autres parts, dans le cadre d\u0027une application médicale réelle, nous avons vu que la quantité d\u0027information fournie par ce modèle CT-SVM à travers les cartes topologiques mixtes est très importante et que le pouvoir de classement avec les SVMs est très performant. Nous avons aussi constaté, qu\u0027il est important de choisir la taille de la partition. Ceci nous amène à réfléchir sur des indices qui permettent d\u0027estimer la partition idéale et de faire une comparaison. Une comparaison avec d\u0027autres méthodes classiques de classement est envisagée dans nos futurs travaux.\nSummary\nThis paper introduces a classification model combining mixed topological map and support vector machines. The non supervised model is dedicated for clustering and visualizing mixed data. The supervised model is dedicated to classification task. In the present paper, we propose a combination of two models performing a data visualization and classification. The task of our model is to train topological map in order to cluster data set on organized subset. For each subset, we propose to train a SVM model. The global classification problem is devided into classification sub problem corresponding to the number of subset. The model is validated related to the obesity problem, which is provied by Nutrition team located in hospital Hôtel-Dieu in Paris. \nFIG. 7 -\nCarte topologiques 3×4 après le partitionnement de la CAH. P \u003d {P 1 , P 2 , P 3 , P 4 }.\n"
  },
  {
    "id": "923",
    "text": "Introduction\nNous nous intéressons dans cette contribution aux applications à forte composante d\u0027activité socio-sémantique -notion que nous définissons exemples à l\u0027appui. Nous avons analysé ce type d\u0027applications dans de précédents articles comme relevant du « Web sociosémantique » matérialisé en particulier par des cartes de thèmes co-construites au sein de groupes en s\u0027appuyant sur le modèle Hypertopic (Cahier et al., 2004).\nL\u0027approche proposée dans cet article vise à lever certaines difficultés qui subsistent dans la mise en oeuvre effective de ces cartes de thèmes co-construites au sein de communautés réelles. Le souci de mieux modéliser l\u0027activité socio-sémantique accompagne une série importante d\u0027expérimentations et de travaux menés au laboratoire Tech-CICO, pour mettre en oeuvre le modèle Hypertopic dans le cadre du Web socio-sémantique (applications utilisant les outils Agorae, Porphyry ou Cassandre) ou le comparer aux modèles sous-jacents à d\u0027autres applications (telles que l\u0027Open Directory Project, Del.icio.us ou Flickr, en partie basées sur les folksonomies et illustrant la tendance du Web2.0). Ces applications permettent à une communauté non seulement de partager des ressources, mais aussi de s\u0027organiser pour mettre en commun et rendre manipulable la description de ces ressources, et faciliter la recherche ou la navigation selon de multiples points de vue (Lejeune, 2002).\nNous proposons une approche basée sur des modèles génériques, s\u0027adressant non seulement aux professionnels de la modélisation (analystes, informaticiens, etc.) en termes de méthode de conception externe mais aussi -à terme -aux utilisateurs finaux en termes de conception participative. Ces modèles génériques visent la représentation des connaissances, mais aussi la représentation de l\u0027activité socio-sémantique qui la rend possible.\nPour cela nous présentons un cadre recourant conjointement au modèle Hypertopic pour la représentation des connaissances de domaine, et au modèle SeeMe (Hermann et al., 1999) pour la représentation des rôles et de l\u0027activité. Nous montrons comment ces deux modèles se complètent, pour mieux ancrer, sur le plan formel et méthodologique, les approches de cartographie collective des connaissances et d\u0027ontologie sémiotique (Zacklad, 2005).\nLa notation SeeMe offre des avantages pour représenter certains aspects de l\u0027activité collective. Elle autorise notamment des méta-relations et l\u0027expression de caractéristiques d\u0027incomplétude et de modularité, permettant de décomposer le modèle en plusieurs modules tout en assurant la cohérence formelle de l\u0027ensemble. Elle nous permet de comparer deux modèles d\u0027activités (parmi de nombreux autres) que nous avons expérimentés pour la coconstruction de cartes de thèmes multi-points de vue Hypertopic : l\u0027un de ces modèles d\u0027activités est construit avec le concours d\u0027un médiateur (modèle KBM ou Knowledge Based MarketPlace, (Cahier \u0026 Zacklad, 2002) ; l\u0027autre est construit sans le concours d\u0027un médiateur, dans une forme de construction dite « controversée » (Zaher et al., 2006), où plusieurs acteurs construisent concurremment chacun « leur » cartographie.\nConstatant qu\u0027il existe une complémentarité et de bonnes perspectives d\u0027intégration entre les modèles SeeMe et Hypertopic, nous argumentons pour construire en les associant des applications du Web socio-sémantique plus spécifiques en termes de rôles.\nDans la partie 2, nous définissons l\u0027activité socio-sémantique et nous posons le problème de la variété des modèles d\u0027activité susceptibles d\u0027être impliqués dans cette activité sociosémantique. Après un rappel de l\u0027état de l\u0027art sur la conception participative, en particulier pour ce qui concerne la conception participative des modes d\u0027organisation, nous justifions la nécessité de faire appel à de telles approches participatives pour l\u0027activité socio-sémantique. Nous introduisons alors les bases conceptuelles du modèle SeeMe de Thomas Herrmann (1999) qui est un modèle élaboré à des fins plus générales pour la conception participative de systèmes socio-techniques. Dans la partie 3, nous rappelons les principes du modèle Hypertopic, qui permet de représenter des cartographies de connaissances notamment pour classer des collections selon plusieurs points de vue portés par des acteurs de ces communautés. La partie 4 exprime et compare, selon la représentation SeeMe, deux modèles sociaux que nous avons expérimentés pour la co-construction de cartes de thèmes Hypertopic. La partie 5 trace quelques perspectives pour un programme à venir, dans le sens d\u0027une meilleure intégration au niveau des modèles, des outils et des méthodes, contribuant à l\u0027activité socio-sémantique sur le Web.\n2 Activité socio-sémantique et conception participative des modèles de cette activité\nWeb et activité socio-sémantiques\nAu sein du courant du Web sémantique, nous avons été amenés à mettre l\u0027accent sur les applications relevant d\u0027un courant que nous avons caractérisé comme « Web sociosémantique » (Cahier et al., 2004). Cette notion fait l\u0027objet de discussions dans la communauté d\u0027ingénierie des connaissances (Gandon, 2006). Simon Buckingham propose une notion, selon nous très proche, de « pragmatic Web » (Buckingham, 2006). Le Web socio-sémantique s\u0027adresse à des communautés d\u0027utilisateurs poursuivant des objectifs similaires. Social, il participe à la construction d\u0027une représentation structurée du domaine et du collectif. Il implique une structuration progressive des réseaux sémantiques gérés par le collectif, cette structuration représentant un enjeu pour le réseau social lui-même. Le Web socio-sémantique est adapté à la description collective des connaissances et à la recherche RNTI -X -ouverte d\u0027information, par des humains, dans des ressources complexes et évolutives (Zaher et al., 2006b). Il se veut ainsi complémentaire au Web sémantique « logique ».\nLors de la construction du Web socio-sémantique, les groupes mobilisent des méthodes et des outils qui relèvent d\u0027approches collaboratives pour la gestion des connaissances (Dieng et al., 2000). Les communautés considérées co-construisent de nombreux types de « structure sémantique au sens large » telles que des index, des cartes de thèmes (Park \u0026 Hunting, 2002), ou des ressources terminologiques et ontologiques (Aussenac et al., 2004 \nUne nécessaire conception participative\nNotre approche constructiviste de l\u0027activité et du social nous incite à tenir compte de la réalité singulière de chaque communauté. La modélisation nécessite donc d\u0027impliquer fortement des membres de la communauté. Cette description doit reposer sur des modes d\u0027expression souples afin qu\u0027à travers l\u0027autodescription de son organisation, la communauté se voie elle-même comme à la fois productrice et bénéficiaire de son activité sociosémantique.\nAinsi, l\u0027organisation sociale permettant à un groupe de co-construire une cartographie de thèmes va devoir prend des formes ad hoc dans chaque cas : il peut exister une division du travail entre plusieurs rôles fixes, comme dans le cas du modèle KBM (Knowledge-Based Marketplace) qui articule des rôles liés aux activités de contribution et de structuration sémantique des cycles de validations (Cahier et Zacklad, 2002). Nous avons cependant observé -notamment dans le cas d\u0027un « annuaire de compétences co-construit » pour l\u0027ingénierie d\u0027Airbus (Cahier et al., 2004) -que l\u0027activité socio-sémantique, si elle peut souvent s\u0027appuyer sur ce cadre général de rôles, a besoin de le raffiner.\nIl est donc nécessaire que l\u0027organisation soit comprise et « lisible » par les membres de la communauté dans leur activité quotidienne. Cette lisibilité peut, selon nous, être améliorée par une confrontation des utilisateurs à des diagrammes exprimant la représentation des rôles et de l\u0027activité. L\u0027apprentissage de ces diagrammes, la participation à leur critique et à leur amélioration, procurent aux membres du groupe une certaine conscience de l\u0027organisation en place. La maîtrise accrue de son organisation par le groupe est facilitée par l\u0027expression et l\u0027édition possible de ces diagrammes, au niveau même de l\u0027interface utilisateurs des «portails » qui supportent l\u0027implantation informatique de tout ou partie des règles de gestion prescrites dans le modèle. Etant donné les particularités de l\u0027activité socio-sémantique, il nous semble difficile de confier la conception à des spécialistes extérieurs au domaine comme des praticiens ontologistes, des organisateurs ou des modélisateurs du système d\u0027informations par des méthodes telles que UML. Selon notre approche, les membres de la communauté sont les plus aptes à décrire leur activité avec leur propre sémantique.\nRNTI -X -\nLe cadre des systèmes socio-techniques\nNous expliquons dans cette section l\u0027approche du système socio-technique qui fonde les bases conceptuelles du modèle SeeMe de Thomas Herrmann (1999); ce modèle, qui est utilisé depuis cinq ans en Allemagne et qui a fait l\u0027objet de plusieurs expérimentations de terrain (cf. Herrmann, 2005) et de discussions dans la communauté CSCW, a été élaboré pour la conception participative de l\u0027organisation, à des fins plus générales que celles que nous proposons d\u0027utiliser ici pour la modélisation de l\u0027activité socio-sémantique.\nUne manière simple de définir les systèmes socio-techniques est de les considérer comme des systèmes ayant un sous-système social et un sous-système technique (Herrmann et al., 2000). En CSCW, le terme renvoie : (1) à la prise en considération des aspects relatifs à l\u0027un et l\u0027autre des deux sous-systèmes quand une organisation introduit une nouvelle technologie ou un nouveau artéfact ; (2) à la relation complexe entre ces deux sous-systèmes.\nTrouvant ses origines dans l\u0027étude de l\u0027organisation du travail et des impacts de l\u0027introduction de dispositifs technologiques sur les aspects sociaux de l\u0027organisation et in fine sur la productivité du travail, l\u0027approche socio-technique a ensuite été développée ou critiquée par certains auteurs (Ehn, 2002). Cette approche a fait appel aux concepts d\u0027autonomie (le comportement du système dépend exclusivement de sa propre structure), d\u0027autopoièse (le système représente une unité qui est sans interruption reconstruite par ellemême), de contingence (rapport entre les stimuli de l\u0027environnement et les réactions du système), de sélectivité (sélection des informations à communiquer, de la manière utilisée pour la communication et des informations en cours de réception), d\u0027autoréférence (le système inclut sa propre description comme une partie à part entière de lui-même), l\u0027incomplétude des descriptions et l\u0027anticipation de l\u0027évolution du système (Herrmann 2005).\nUn système socio-technique est une unité : ses deux sous-systèmes social et technique doivent être étroitement intégrés. L\u0027interaction entre les deux sous-systèmes apparaît à travers les inscriptions qu\u0027elle laisse dans les structures de contrôles des dispositifs techniques et les processus de communication du sous-système social. Ces inscriptions peuvent être implicites et peuvent être partiellement explicitées au cours de réflexions.\nLes auteurs du modèle SeeMe (Herrmann et al., 2000) s\u0027inscrivent dans cette approche du système socio-technique. Ils souscrivent également à la problématique d\u0027un modèle spécifique du domaine face aux modèles universels. Ils insistent enfin sur l\u0027importance de modèles explicites permettant de soutenir l\u0027externalisation de la structure des activités qui facilitent leur compréhension et leur changement. Leur modèle graphique et formel basé sur des diagrammes effectue un certain nombre de choix, comme ceux de la description textuelle, de représentations diagrammatiques riches (Moody, 1996) qui nous semblent nécessaires à l\u0027expression de l\u0027activité socio-sémantique. \nLe modèle SeeMe\nTAB. 1 -Relations standard du modèle SeeMe (mais le modèle prévoit aussi des relations personnalisées, des méta-relations, etc.).\nLes relations sont représentées par des emboîtements ou des flèches. Si aucune autre indication n\u0027est fournie, les flèches ont une signification standard qui dépend des éléments qu\u0027elles relient. Une relation commence avec un point d\u0027ancrage à son « point de départ » ; elle aboutit à l\u0027autre extrémité à son « élément de fin ». Il y a neuf relations standard simples (Tab. 1). Dans l\u0027exemple d\u0027un système de gestion de base de données (Fig. 1b), introduire de nouvelles données ne modifie pas la structure. Dans la notation SeeMe, la relation alors indiquée par une flèche simple signifie « change » (« entrer des données change la base de données »). Au contraire, l\u0027activité « modifier la structure des données » (par exemple introduction d\u0027une nouvelle table par l\u0027informaticien) change la structure de la base. C\u0027est un changement profond justifiant de recourir à une méta-relation (flèche en ligne brisée).\nRNTI -X -\nModélisation du Web socio-sémantique avec SeeMe\nExpression du modèle Hypertopic\nAvec SeeMe comme avec Hypertopic, la structure est essentiellement déterminée par l\u0027emboîtement de sous-éléments et les relations entre eux. Dans le cas de Hypertopic (Fig.2b) des thèmes (topics) sont articulés hiérarchiquement au sein de points de vue multiples considérant une collection d\u0027entités 2 .\nFIG. 2 -a) L\u0027activité socio-sémantique relie une communauté avec les structures de sa sémantique et de son organisation ; b) Le Modèle Hypertopic ; (notation SeeMe)\nCe modèle est congruent avec la co-construction de cartographies de collections selon de multiples points de vue, débouchant sur ce que l\u0027on pourrait appeler des « représentations domaine » ou des « connaissances artefactuelles du domaine » (Fig. 2a). Pour représenter l\u0027organisation sociale, le modèle d\u0027activité, de rôles, d\u0027autorisations, de droits et de devoirs de chacun dans la (sous-)communauté concernée, nous proposons la modélisation SeeMe en 2 L\u0027appellation « d\u0027entité » que nous employons depuis l\u0027origine du modèle Hypertopic (Cahier et al . , 2004) se heurte à l\u0027emploi très chargé du terme « entité » tant en informatique des bases de données (modèle « entité-relation » de modélisation de structures de données) qu\u0027en philosophie où le terme connote un fort degré de réification. De plus le modèle SeeMe utilise aussi le terme « entité » en un sens bien précis. Pour ne pas provoquer trop de malentendus, nous avons cherché une meilleure appellation. De son côté l\u0027appellation « d\u0027objet » ne convient pas non plus, car elle est malheureusement très connotée chez les informaticiens à cause de la « conception orientée objet ». Dans un récent document de travail (Zacklad et al., 2007) réunissant la réflexion des chercheurs impliqués dans le modèle Hypertopic, nous proposons au lieu « d\u0027entité » l\u0027appellation « Stuff », bien rendue en français par « trucs » : soit une notion respécifiée pour chaque cas dans le langage de la communauté considérée, pas encore (forcément) substantifiée et restant en débat, mais que le groupe a déjà besoin de caractériser, d\u0027évaluer, d\u0027organiser, de documenter et de ranger au sein d\u0027une collection.\nRNTI -X -complément de Hypertopic. Comme l\u0027organisation sociale et le modèle d\u0027activité varient à chaque application du Web socio-sémantique, il est nécessaire de fournir aux acteurs des outils de conception participative et en particulier un langage de représentation à ce niveau.\nLa « méta-relation » proposée par le modèle SeeMe joue un rôle clé pour symboliser l\u0027émergence de la structure sémantique. Elle caractérise le rapport entre la structure sémantique (les points de vue, la carte de thèmes) et la collection considérée.\nL\u0027activité socio-sémantique avec Hypertopic\nLes expérimentations de terrain du modèle Hypertopic ont mis en évidence la nécessité d\u0027une certaine diversité des méthodes de co-construction, pour s\u0027adapter à une variété d\u0027objectifs d\u0027activité, de contextes humains, organisationnels, etc. Hypertopic structure la représentation de la connaissance mais ne répond pas à la question du modèle d\u0027activité à mettre en oeuvre pour cette co-construction. Hypertopic peut être considéré comme relativement neutre quant aux formes d\u0027organisation des rôles et des actions pour coconstruire une cartographie. Bien qu\u0027Hypertopic influence la forme de cette activité, il ne conduit pas à un modèle d\u0027activité unique ou optimal dans tous les cas. Le choix reste possible parmi plusieurs méthodes : ainsi, les deux méthodes que nous proposons dans la suite, mais aussi d\u0027autres qui restent à imaginer, en fonction des buts, des phases de la construction, ou des maturités des communautés. Nous soulignons la facilité de la création ou l\u0027adaptation de la méthode à chaque cas, grâce à un langage basé sur des diagrammes facilement compris par les acteurs eux-mêmes.\nComparaison de deux modèles d\u0027activité socio-sémantique\nComment les acteurs doivent-ils procéder concrètement pour déterminer les points de vue, et construire une carte Hypertopic ? Nous donnons ici deux exemples : (1) une méthode «mono-concepteur consensuelle» (Fig. 3), et (2) d\u0027une méthode de «conception controversée pluri-acteurs» (Fig.4). La première est une méthode de conception initiale par un unique médiateur enquêtant auprès du groupe et posant un jeu unique et cohérent de « dimensions d\u0027analyse » structurant la carte et reflétant le consensus où la sémantique majoritaire dans le groupe. La seconde permet la construction simultanée de plusieurs points de vue individuels et leur juxtaposition dans un même artéfact, permettant alors la comparaison et favorisant d\u0027éventuelles synthèses ultérieures. La première méthode a été appliquée à la phase initiale de conception, déjà évoquée, d\u0027un annuaire métier en ingénierie selon le modèle KBM (Cahier et al., 2002(Cahier et al., et 2004 ; la seconde est actuellement mise en oeuvre dans l\u0027application SeqXAM dans le cadre d\u0027un projet DKN soutenu par l\u0027UNESCO (Zaher, 2006a).\nLes contraintes d\u0027un modèle de co-construction\nA partir du moment où l\u0027on s\u0027est accordé sur la collection qu\u0027il s\u0027agit de considérer, la méthode de co-construction doit répondre aux besoins de deux grandes étapes. (1) L\u0027amorçage et la conception initiale du système : un ensemble de points de vue (ceux qu\u0027expriment des membres de la communauté, ou des dimensions d\u0027analyses assez consensuelles, etc.) ayant sens pour la communauté est construit sur la collection d\u0027entités (2) La construction sémantique par un cercle élargi d\u0027acteurs : une fois le système initialisé, RNTI -X -le dispositif doit permettre des formes d\u0027utilisation et de co-construction sémantique du système en « rythme de croisière », autour des différents types de rôles nécessaires à la communauté. Par exemple la méthode choisie pourra recommander que les éditeurs sémantiques ne modifient plus directement le schéma des points de vue. Les points de vue, jouant un rôle très structurant, ne pourront être modifiés qu\u0027après un niveau élevé de concertation et de consensus (mené sur un forum de discussions) et une décision collective. Ce sera alors l\u0027une des différences entre la phase d\u0027initialisation et la phase de croisière.\nLa méthode d\u0027amorçage « mono-concepteur »\nLa méthode «mono-concepteur» (Fig. 3) est une méthode rapide utilisable par un collectif où règne un certain consensus sur les dimensions d\u0027analyse et les catégories conceptuelles de la collection considérée. Dans le cas d\u0027une carte concernant la collection des projets de logiciels libres (Yeposs), l\u0027enquête a révélé un quasi-accord sur les dimensions d\u0027analyse pertinentes. Celles-ci correspondaient aux divers rôles et métiers principaux confrontés à cette entité « projet » (points de vue juridique, business model, fonctionnel, etc.). Le médiateur a pu poser un jeu consensuel de « points de vue » faisant sens.\nFIG. 3 -Amorçage de la construction avec médiateur\nLa carte est conçue par un seul analyste mandaté par le groupe. Dans la pratique, il mène l\u0027enquête auprès d\u0027un sous-groupe représentatif de membres de la communauté (de futurs contributeurs et éditeurs de la carte). Il centre l\u0027analyse sur certains éléments représentatifs.\nLe concepteur de la carte travaille seul, en tentant de résoudre les différents points de vue qu\u0027il rencontre dans son enquête en un jeu de dimensions d\u0027analyse pertinentes par rapport à l\u0027échantillon et consensuelles par rapport au groupe. Il réunit une analyse ascendante et inductive consistant à déterminer le jeu des dimensions d\u0027analyse à partir de la collection, et des éléments d\u0027analyse « descendante » qu\u0027il base sur sa propre expérience (sa vision du RNTI -X -domaine et de l\u0027activité enrichie par l\u0027enquête auprès des membres du groupe). Il ne s\u0027agit pas à proprement parler d\u0027une méthode de co-construction mais plutôt d\u0027intermédiation.\nLa méthode de « co-construction conflictuelle»\nFIG. 4 -« Co-construction controversée » sans médiateur\nCette méthode (Fig. 4) ne suppose pas un rôle spécialisé d\u0027enquêteur médiateur. Elle vise à permettre aux membres de la communauté de poser explicitement des opinions concurrentes, et à faciliter leur dialogue en se servant d\u0027autant de points de vue qu\u0027il existe d\u0027opinions entrant en controverse. Elle est davantage une forme co-constructive s\u0027appuyant dès le commencement sur l\u0027expression explicite de plusieurs points de vue portés par des acteurs du groupe. Chaque membre exprime son point de vue selon les concepts du modèle Hypertopic. Cette méthode aide à visualiser les différences de conceptions entre membres du groupe. La carte considérée est une carte de conceptions. Elle est utilisée pour construire, visualiser et comparer les propositions des acteurs. Les co-auteurs de la carte peuvent alors se contenter de juxtaposer ces conceptions différentes (en portant éventuellement les divergences à la connaissance de la communauté), car il n\u0027est pas toujours possible ni souhaitable de résoudre les différences. S\u0027ils entrevoient des consensus partiels ou globaux réalisables et intéressants à établir, ils peuvent élaborer une carte de synthèse conciliant progressivement certaines divergences, et imaginer le cas échéant les procédures de résolution nécessaires (attribution de pondérations, vote…).\nRNTI -X -\nPerspectives et conclusion\nNous pensons qu\u0027il existe une complémentarité et de bonnes perspectives d\u0027intégration entre les modèles SeeMe et Hypertopic, pour des applications socio-sémantiques plus finement adaptées en termes de rôles spécifiques et personnalisées ; i.e. cela donne une marge de liberté pour élargir le jeu de rôles du modèle KBM présenté au § 3.1 via des modèles d\u0027activité ad hoc adaptés selon des formes participatives à chaque application Agorae.\nDu point de vue technique, cela ouvre la possibilité d\u0027une transposition plus facile de nouveaux services, en prise directe sur les modèles dans une application spécifique (gestion plus simple de l\u0027affichage et du contrôle d\u0027autorisation pour accéder à certaines actions, par exemple). On peut imaginer une évolution de l\u0027outil où un membre connaît les rôles qui lui sont accessibles grâce au diagramme et accède dès lors à ces actions plus directement (par exemple par clic sur la partie correspondante, activable, du diagramme SeeMe).\nNous travaillons par exemple actuellement à l\u0027adjonction : (1) d\u0027un rôle modérateur pour la fonction de discussion des thèmes (chaque thème peut faire l\u0027objet d\u0027annotations éventuellement chaînables, etc. permettant des discussions ciblées exprimant les controverses dans la construction collective de la sémantique, que nous expérimentons dans l\u0027application Yeposs). Le processus modélisé en SeeMe a aidé à concevoir les détails de ce nouveau rôle, et permet à tous les membres de comprendre les règles de gestion partagée pour les remarques et controverses. (2) d\u0027un rôle facilitateur (user advocate), dans l\u0027application DKN, autorisé à rajouter par exemple « des bulles d\u0027aides spécifiques » pour aider à la compréhension des utilisateurs en rapport avec les objectifs métiers de la carte.\nIl est cependant prudent de considérer que l\u0027approche de conception participative et de modélisation engagée que nous mettons ici en avant n\u0027est pas (toujours) suffisante en ellemême, et qu\u0027on aura encore besoin de spécialistes (sous certains aspects qui resteraient à définir, et dans des processus et des rôles qui sont alors à considérer en profonde mutation) : organisateurs, sémanticiens, ingénieurs de la connaissance, spécialistes de ressources terminologiques et ontologiques (RTO, cf Aussenac et al., 2004). Même dans les domaines métiers qui évoluent rapidement, l\u0027activité socio-sémantique est un type d\u0027activité particulière, qui a aussi besoin dans une certaine mesure d\u0027une part d\u0027institutionnalisation. Les acteurs inventent et vont de plus en plus inventer, avec les évolutions à venir du Web à partir de sa version 2.0, des organisations pour l\u0027activité socio-sémantique, d\u0027où l\u0027enjeu de poursuivre la réflexion et les expériences sur les modèles et les notations soutenant la description de cette activité. \nRéférences\nRNTI -X -\n"
  },
  {
    "id": "924",
    "text": "Introduction\nUne ontologie est une structure formelle dans laquelle les concepts d\u0027un domaine et les relations entre ces concepts sont définis (Gruber (1993)). Notre ontologie porte sur l\u0027astronomie : dans leurs articles scientifiques, les astronomes identifient manuellement les caracté-ristiques des objets célestes, afin de les associer ensuite à une catégorie (galaxie, étoile, ...). Les catégories sont pré-définies et l\u0027astronome détermine la classe correspondant le mieux à l\u0027objet étudié. Cette classification a permis de catégoriser 3.751.128 objets célestes. Pourtant, il reste encore des milliards d\u0027objets à classifier et à caractériser de la manière la plus exhaustive possible. L\u0027utilisation des articles scientifiques, très facilement accessibles sous format électronique, permettent de répondre à ces attentes.\nNous proposons une méthode semi-automatique de construction d\u0027une ontologie sur le domaine de l\u0027astronomie. Les concepts de l\u0027ontologie sont des classes dont les instances sont les objets célestes. Les propriétés de chaque classe sont partagées par toutes ses instances. Ces propriétés sont extraites automatiquement des textes par un analyseur syntaxique partiel et robuste \"Enju\" de Miyao et Tsujii (2005). Objets et propriétés sont classés dans un treillis de Galois selon l\u0027analyse formelle des concepts : FCA présentée dans Ganter (1999). Le résultat de cette méthode est fourni aux astronomes afin d\u0027étiqueter chaque classe d\u0027après les propriétés partagées par les instances de la classe.\nNotre méthode présente plusieurs avantages : -elle peut être appliquée quelque soit le corpus de textes et le domaine spécifique sur lequel elle est utilisée, -elle est formalisée par la FCA, -elle est rapide comparée à une ontologie construite manuellement, -et elle permet d\u0027enrichir l\u0027ontologie résultante par la mise à jour du corpus de textes.\nNotre méthode de construction d\u0027ontologie s\u0027établit à partir d\u0027un corpus de textes. Nous choisissons de caractériser les objets célestes présents dans les textes par les verbes avec lesquels ils apparaissent en tant que sujet ou en tant que complément. Les verbes en effet, nous permettent de définir la nature des objets. Par exemple, tous les objets ne peuvent pas être sujet du verbe \"´ emettre\" : un objet émetteur peut être une étoile mais pas une planète.\nTout d\u0027abord, nous extrayons les paires (sujet,verbe) et (complément,verbe) qui repré-sentent l\u0027entrée de la FCA. Ensuite un treillis de Galois est construit avec le context formel K\u003d (G, M, I) tel que : G l\u0027ensemble des objets célestes, M l\u0027ensemble des verbes (propriétés), et il existe une relation I (g,m) ssi : l\u0027objet g est sujet ou complément du verbe m. De là, le treillis est transformé en une ontologie de concepts : les concepts sont représentés par les propriétés (intensions) du treillis et les instances par les objets célestes (extensions) du treillis. Le treillis définit ainsi l\u0027ordre partiel des concepts, d\u0027après l\u0027ensemble des propriétés qu\u0027ils partagent. Enfin, chaque classe d\u0027objets est étiquetée par les experts du domaine.\nCette méthode non supervisée propose aux astronomes une classification des objets cé-lestes pour éviter un \"goulot d\u0027étranglement\" dans l\u0027acquisition des connaissances. Elle ouvre deux perspectives. D\u0027une part, une analyse plus fine des résultats de l\u0027analyseur syntaxique permettrait d\u0027utiliser des patrons syntaxiques plus précis -au lieu du marqueur général \"complément\" préciser si c\u0027est un complément d\u0027objet direct, un complément de lieu, etc -ainsi que d\u0027autres types de marqueurs -non seulement les sujets et les compléments sont pris en compte mais aussi les adjectifs, les adverbes, etc. D\u0027autre part, afin de tenir compte des propriétés multivaluées pour obtenir de meilleures classes résultantes, une relation plus riche que la relation binaire dans la FCA devrait être envisagée.\nSummary\nThis paper presents a semi-automatic method of building ontology from a textual corpus on a specific domain. This method is based, on the first hand, on a robust and partial syntactic parser and, on the other hand, the use of formal concept analysis for the construction of object class with a Galois lattice. The construction of the ontology from concepts and instances hierarchization is effected in a formal transformation of the lattice structure. The application domain of this method is astronomy.\n"
  },
  {
    "id": "925",
    "text": "Introduction\nLes profils d\u0027accès à un site Web peuvent être influencés par certains paramètres de nature temporelle, comme par exemple : l\u0027heure et le jour de la semaine, des événements saisonniers, des événements externes dans le monde (guerres, crises économiques), etc. Dans ce contexte, la plupart des méthodes consacrées à la fouille de données d\u0027usage du Web (Web Usage Mining) (Cooley et al., 1999) prennent en compte dans leur analyse toute la période qui enregistre les traces d\u0027usage : les résultats obtenus sont donc naturellement ceux qui prédominent sur la totalité de la période. Ainsi, certains types de comportements, qui ont lieu pendant de courtes sous-périodes ne sont pas pris en compte, et restent donc ignorés par les méthodes classiques. Il est pourtant important d\u0027étudier ces comportements et donc de réaliser une analyse portant sur des sous-périodes significatives. Le volume des données considérées étant très élevé, il est en outre important de recourir à des résumés pour représenter les profils considérés. L\u0027analyse de l\u0027usage a commencé relativement récemment à tenir compte de la dépendance temporelle des profils de comportement. Dans (Roddick et Spiliopoulou, 2002), les auteurs examinent les travaux antérieurs. Ils résument les solutions proposées et les problèmes en suspens dans l\u0027exploitation de données temporelles, au travers d\u0027une discussion sur les règles temporelles et leur sémantique, mais aussi par l\u0027investigation de la convergence entre la fouille de données et la sémantique temporelle. Tout récemment, dans (Laxman et Sastry, 2006) les auteurs discutent en quelques lignes des méthodes pour découvrir les modèles séquentiels, les motifs fréquents et les modèles périodiques partiels dans les flux de données.\nLe présent article propose de suivre le changement de comportement à l\u0027aide des résu-més obtenus par une approche évolutive de la classification appliquée sur des sous-périodes de temps. L\u0027article est organisé comme suit : la section suivante présente l\u0027approche d\u0027analyse de l\u0027usage basée en sous-périodes temporelles. Nous présentons aussi dans cette section les expé-riences réalisées, en analysant les résultats et en les comparant à ceux des méthodes classiques. La dernière section présente les conclusions et les travaux futurs envisagés.\nApproche de classification par sous-périodes de temps\nLa caractérisation de groupes d\u0027utilisateurs consiste à identifier des traits d\u0027usage partagés par un nombre suffisant d\u0027utilisateurs d\u0027un site Web et ainsi fournir des indices permettant d\u0027inférer le profil de chaque groupe (Da Silva et al., 2006a,b). L\u0027approche proposée dans cet article consiste dans un premier temps à diviser la période analysée en sous périodes plus significatives (mois de l\u0027année). Ensuite, une classification est réalisée sur les données de chaque sous-période, aussi bien que sur la période complète. Les résultats fournis sont donc comparés les uns avec les autres.\nDans ce contexte, nous avons réalisé quatre classifications de la manière suivante : -Classification globale : cette classification est obtenue sur la totalité des individus ; -Classification locale indépendante : pour chaque zone temporelle a priori, on réalise une classification de l\u0027ensemble des navigations concernées. Comme chaque zone est distincte, chaque classification est donc indépendante des autres ; -Classification locale \"précédente\" : ici, on utilise la structure classificatoire de la pé-riode temporelle précédente pour obtenir une partition de la période courante ; -Classification locale dépendante : ici, on initialise l\u0027algorithme pour une période temporelle avec les résultats de cet algorithme appliqué sur la période précédente.\nAlgorithme et critères d\u0027évaluation\nPour la classification des navigations, nous utilisons un algorithme de type nuées dynamiques (cf Celeux et al. (1989)) applicable sur un tableau de données (voir tableau 1). L\u0027algorithme doit en particulier : (1) pouvoir affecter de nouvelles observations à une classification existante, et (2) pouvoir initialiser l\u0027algorithme avec les résultats d\u0027une autre réalisation de lui-même. Pour toutes les procédures de classification, nous avons demandé 10 classes avec un nombre d\u0027initialisations aléatoires égal à 100, sauf dans le cas de la classification locale dépendante.\nPour analyser les résultats, nous utilisons deux critères. Pour une analyse classe par classe, nous considérons la F-mesure de van Rijsbergen (1979). Pour une analyse plus globale, nous utilisons l\u0027indice de Rand corrigé (cf Hubert et Arabie (1985) \nApplication et résultats\nLes Nous avons réalisé un suivi des prototypes des classes (mois par mois) pour les classifications locales indépendante et dépendante, puis nous avons projeté ces prototypes dans le plan factoriel (voir figure 1). Sur cette représentation, chaque cercle représente un prototype. Dans la classification dépendante, les dix classes sont représentées par des couleurs différentes. On note une certaine stabilité malgré la diversité de mois analysés. Dans le cas de la classification indépendante, la trajectoire temporelle est simplement matérialisée par les lignes qui joignent un prototype à son plus proche voisin dans la période temporelle précédente. Cela ne donne pas des trajectoires parfaitement identifiées car certains prototypes partagent à un moment donné le même prédécesseur. On note en fait que seules quatre classes sont parfaitement identifiées et stables, les autres subissant des fusions et séparations au cours du temps. Par l\u0027analyse de la variance intra-classe, nous pouvons constater que les classes obtenues par la classification locale indépendante présentent plus de cohésion au sens de ce critère (voir figure 2).\nA partir des valeurs de l\u0027indice de Rand corrigé (cf figure 3), dans le cas de confrontation des classifications indépendante versus globale il y a presque systématiquement des valeurs faibles, c\u0027est-à-dire que certaines classes de la classification indépendante ne sont pas retrouvées dans la classification globale. On voit aussi que la classification \"précédente\" ne donne pas des résultats très différents de ceux obtenus par la classification dépendante, ce qui confirme l\u0027intuition acquise par l\u0027observation des prototypes dans le plan factoriel : ces derniers bougent \"peu\" au cours du temps.\nCes différences sont confirmées par la F-mesure (cf figure 4). Ce qui apparaît nettement, c\u0027est que les classes sont très stables dans le temps si on utilise la méthode de classification \nFIG. 1 -Classifications locales : indépendante (gauche) et dépendante (droite).\nFIG. 2 -Variance intra-classe des classifications : indépendante (trait noir), dépendante (trait rouge) et globale (trait bleu).\ndépendante. En fait, aucun indice ne descend au dessous de 0.877, ce qui représente une très bonne valeur. Par contre, dans le cas de la classification indépendante, on obtient au contraire des classes très différentes de celles obtenues globalement (avec des valeurs inférieures à 0.5).\nConclusions et perspectives futures\nDans cet article, nous avons abordé la problématique du traitement des données dynamiques dans le contexte de l\u0027analyse de l\u0027usage du Web. A travers nos expérimentations, nous pouvons dire que la méthode de classification locale dépendante montre que les classifications obtenues ne changent pas ou peu au cours du temps, alors que la méthode de classification locale indépendante est plus sensible aux changements qui peuvent se passer d\u0027une sous-période à l\u0027autre. Dans un plan secondaire, l\u0027approche de classification locale indépendante permet van Rijsbergen, C. J. (1979). Information Retrieval (second ed.). London : Butterworths.\nSummary\nThe way in which a Web site is visited can indeed evolve due to modifications of the structure and the contents of the site, or because of changes in the behaviour of certain user groups. Thus, the models associated with these behaviours in the Web Usage Mining domain must be updated continuously in order to reflect the current behaviour of the users. A solution to this problem, proposed in this article, is to update these models using the summaries obtained by an evolutionary approach of the classification methods.\n"
  },
  {
    "id": "926",
    "text": "Introduction\nDans cet article, nous nous intéressons au problème suivant : étant donné un ensemble de n données d 1 , ..., d n et une matrice de similarité M (d i , d j ) entre ces données, comment permettre à un expert d\u0027explorer cet ensemble de données de manière visuelle et avec une approche guidée par le contenu. Nous considérons que l\u0027expert souhaite avoir une vue globale des données mais également exploiter localement les données Shneiderman (1996), et en particulier passer de l\u0027une à l\u0027autre par une relation de voisinage tenant compte de la similarité. Notre problème se décompose en deux parties : établir un graphe de voisinage entre les données à partir de la similarité, et visualiser ce graphe afin de permettre à l\u0027utilisateur de l\u0027explorer.\nNous allons donc nous concentrer sur les méthodes de construction de graphes de voisinage (voir un état de l\u0027art dans Hacid et Zighed (2005)). Ce type de structure est également appelée graphe de proximité. L\u0027utilisation de ces graphes se retrouve aussi bien en fouille de données (classiques, spatiales) que dans l\u0027apprentissage ou la classification de données (Ester et al., 1997). Cependant, les algorithmes de construction de ces graphes sont d\u0027une grande complexité (par exemple O(n 3 ) pour l\u0027algorithme des Voisins Relatifs), ce qui les rend inefficaces face à de grands volumes de données.\nNous nous sommes intéressés dans cet article à une méthode biomimétique pour la construction incrémentale de ce type de graphe. Il s\u0027agit d\u0027une généralisation de l\u0027algorithme AntTree proposé par Azzag (2005) dans sa thèse. L\u0027auteur a introduit un algorithme de classification non supervisée hiérarchique capable de traiter n\u0027importe quel type de données. Il se base sur le principe d\u0027auto-assemblage observé chez une population de fourmis réelles présenté dans Lioni et al. (2001). Nous proposons ici une généralisation d\u0027AntTree en utilisant un graphe comme modèle de structure. Nous allons donc chercher à construire un graphe de voisinage qui soit représentatif de la similarité existante entre ces données avec une complexité inférieure à celle observée dans les méthodes classiques (Voisins Relatifs, graphes de Gabriel, triangulation de Delaunay).\nLa suite de notre article est organisée comme suit : dans la section 2, nous présentons les principes des graphes de voisinage et quelques modèles de référence dont celui des Voisins Relatifs auquel nous comparons notre algorithme. Dans la section 3, nous détaillons l\u0027algorithme de construction ainsi que l\u0027ensemble des règles locales de comportement des fourmis artificielles. Nous précisons également, dans cette section, la méthode de visualisation utilisée pour réaliser l\u0027affichage de graphes de voisinage. La section 4, quant à elle, est consacrée aux résultats et à l\u0027étude comparative sur des bases de données numériques. La dernière section rassemble les conclusions faites au cours de l\u0027article et présente des perspectives.\nGraphes de voisinage et visualisation\nPrincipes des graphes de voisinage\nConsidérons un graphe G(?, V) où ? est l\u0027ensemble des noeuds du graphe et V l\u0027ensemble des arêtes contenu dans le graphe. G est appelé graphe de voisinage si la propriété ci-contre est respectée : il existe une relation binaire entre deux points (a, b) ? ? 2 si et seulement le couple de points (a, b) ? V. En d\u0027autres termes, Pour un point p donné de ?, son voisinage ?(p) est l\u0027ensemble des points (sous-graphe) contenant p et tous les autres sommets directement connectés. Dans notre cas, chaque noeud (ou sommet) du graphe est une donnée. Les liens qui vont connecter les noeuds entre eux doivent représenter une information sur le voisinage des données. Cette information peut être une notion de distance entre les données.\nIl existe plusieurs méthodes pour établir ce type de graphe. La triangulation de Delaunay Preparata et Shamos (1985) va relier entre elles les données qui vérifient la propriété suivante : le cercle passant par les trois sommets de chaque triangle ne contient aucune autre donnée. Le graphe de Gabriel se construit selon Gabriel et Sokal (1969)   Toussaint (1991). Récemment, une extension a été proposée afin de construire de manière incrémentale un graphe de voisinage dans Hacid et Zighed (2005). Cette approche a pour but de compléter un graphe existant lors d\u0027une phase de mise à jour.\nVisualisation de graphes par forces et ressorts\nIl existe de nombreux algorithmes de visualisation de graphes Di Battista et al. (1998). Les travaux de recherche sur les algorithmes de ressorts commencent avec Tutte (1963) et se poursuivent avec Eades (1984). Ce dernier utilise l\u0027analogie suivante pour expliquer la visualisation dynamique de graphes : il compare les arêtes dans un graphe à des ressorts. Le système, ainsi considéré, engendre des forces entre les sommets. Ce qui provoque naturellement des dé-placements de sommets. Les sommets s\u0027attirent et se repoussent. La notion d\u0027attraction entre sommets se réalise grâce aux arêtes qui cherchent à atteindre une distance cible associée. Eades (1984) ajoute la notion de forces de répulsion aux sommets. La condition d\u0027arrêt initialement proposée pour un tel système est un nombre maximum d\u0027itérations (évolution du graphe dans le temps).\nPlusieurs recherches ont ensuite été consacrées au domaine. Nous pouvons citer entre autres Kamada et Kawai (1989), Frick et al. (1994) et Fruchterman et Reingold (1991. Ces différentes propositions ont amené à l\u0027établissement de plusieurs modèles de visualisation dynamique de graphes. Nous nous baserons sur Fruchterman et Reingold (1991) qui offre une méthode générique de visualisation.\nEtant donné un graphe de voisinage, on définit dans notre outil la longueur à atteindre entre chaque couple de noeuds voisins par la notion de similarité existante entre les deux données correspondantes. Ensuite, les noeuds sont placés initialement de manière aléatoire sur un plan 2D, et les forces et ressorts agissent jusqu\u0027à stabilisation du graphe qui offre alors une visualisation homogène et agréable à l\u0027utilisateur.\nAntGraph\nNotre modèle est une extension de l\u0027algorithme AntTree présenté dans Azzag et al. (2003). AntTree effectue une classification non supervisée hiérarchique pour regrouper des données de n\u0027importe quel type (numérique, symbolique, textuel, ...) sous forme d\u0027un arbre. Nous gé-néralisons ainsi ces principes dans le but de construire un graphe de fourmis avec comme connaissance de départ la mesure de similarité entre les données. Nous détaillons ci-dessous les règles de construction du graphe par des fourmis artificielles.\nNous considérons ici un ensemble de données triées de manière aléatoire. Chaque fourmi va représenter une donnée à regrouper. Nous choisissons ensuite aléatoirement une fourmi f 0 FIG. 1 -Etapes de construction du graphe. En 1., f 0 est le premier sommet et constitue le support fixe. En 2., la fourmi suivante f 1 se déplace sur f 0 et se fixe à cette seule fourmi qui lui est la plus similaire. En 3., la fourmi f 2 se déplace sur f 0 , se compare à elle et se fixe. f 1 est une fourmi fille de f 0 . f 2 doit vérifier si f 1 lui est similaire ou pas. En 4., nous avons le principe généralisé de construction de AntGraph.\nque nous considérons comme le point d\u0027entrée dans l\u0027étape de construction du graphe, et donc comme le premier noeud de ce graphe. Puis nous simulons les actions de chaque fourmi f i , qui entre dans le graphe par le noeud f 0 , se déplace de noeud en noeud, et ce jusqu\u0027à ce qu\u0027elle se connecte dans le graphe. On peut alors passer à la fourmi suivante. Lorsque f i est en dépla-cement, on note f pos la fourmi sur laquelle elle se trouve. Ensuite, le voisinage perçu par f i correspond à f pos ainsi qu\u0027aux fourmis connectées à f pos . Intuitivement, lorsque f i arrive dans le graphe, elle va suivre le chemin de similarité maximum indiqué dans le voisinage qu\u0027elle perçoit, puis elle va se connecter en établissant un ou plusieurs liens (voir 1). L\u0027algorithme est le suivant :\nLe choix de la fourmi la plus similaire pour f i implique plusieurs cas de décision par rapport à sa position : -f pos est la fourmi la plus similaire à f i et ne possède pas de fourmis voisines. f i se connecte alors directement à f pos . -f pos n\u0027est pas la fourmi la plus similaire à f i et possède des fourmis voisines. Dans ce cas, f i se déplace sur la fourmi voisine la plus similaire. -f pos est la fourmi la plus similaire et possède des fourmis voisines. Ce dernier cas se présente lorsque f pos , la fourmi la plus similaire, possède des fourmis voisines qui pourraient être également similaires à f i . Dans ce cas de figure, f i se connecte sur f pos et sur toutes les fourmis voisines de f pos suffisamment similaires à elle par rapport à un seuil de tolérance S t . L\u0027opération est répétée récursivement pour toutes les fourmis connectées aux voisines de f pos .\nAlgorithme 1 Vue d\u0027ensemble de l\u0027algorithme de construction incrémentale ENTRÉES: f 0 est le 1 er noeud du graphe. SORTIES: le graphe de voisinage G.\npour une fourmi f i non connectée faire f i se place en f 0 (i.e. f pos ? f 0 ) si f pos n\u0027est pas la fourmi la plus similaire à f i alors f i se déplace sur la fourmi voisine de f pos qui lui est la plus similaire sinon si f pos est la fourmi la plus similaire à f i alors f i se connecte à f pos si f pos possède des fourmis voisines alors calcul du seuil de tolérance S t f i interroge les fourmis voisines de f pos f i se connecte aux voisines de f pos les plus similaires (? S t ) et récursivement aux voisines de ces fourmis avec la même condition (similarité ? S t ) finsi finsi fin pour Nous précisons que la valeur du seuil de tolérance S t est calculé de la manière suivante :\nCet algorithme est bien incrémental : les fourmis/données sont ajoutées une à une dans le graphe.\nEtude comparative\nNous réalisons notre étude sur des bases numériques artificielles et réelles (voir la partie gauche de la table 1). Notre méthode peut traiter tout type de données, du moment que la similarité existe. Les bases de données artificielles {Art1, ..., Art6} nous sont fournies par Azzag (2005). Les bases réelles que nous utilisons proviennent du CE.R.I.E.S., Guinot et al. (2001), pour la base de même nom et du UCI Repository of Machine Learning, Blake et Merz (1998), pour les autres. Il est à noter que pour chaque test, la fourmi support f 0 est choisie aléatoirement. Il en est de même pour les fourmis ajoutées une à une dans le graphe. Nous pouvons remarquer sur la figure 2, que les visualisations obtenues pour AntGraph ont une ressemblance très forte avec les visualisations obtenues pour Voisins Relatifs. Nous distinguons bien les regroupements de données. Nous retrouvons visuellement les classes réelles : 3 classes pour Iris, 4 pour Art6 et 2 pour Art2.\nVisualisations\nLa figure 3 nous présente le rôle du seuil de tolérance S t dans la qualité de construction du graphe. Les visualisations correspondantes nous permettent de constater que la valeur de la constante ? a une influence sur la formation des graphes. Nous avons fait évoluer la constante de 0,1 à 0,99. Nous présentons trois visualisations pour les valeurs 0,8, 0,9 et 0,97. En dessous de 0,9, nous obtenons des graphes où les noeuds sont pratiquement tous connectés les uns aux autres : les fourmis ne se connectent pas aux fourmis les plus similaires mais à beaucoup d\u0027autres qui ne le sont pas du tout. Pour une valeur ? de 0,9 ou légèrement supérieure, nous visualisons de manière esthétique le regroupement des données en classes : les fourmis se connectent à leurs voisines les plus similaires. Enfin, plus la valeur de ? est proche de 1, plus le graphe perd de l\u0027information : les fourmis ne se connectent pratiquement plus qu\u0027à une seule voire deux fourmis similaires (le graphe se transforme en arbre). Les visualisations les plus probantes que nous obtenons s\u0027obtiennent pour des valeurs ? supérieures à 0,9.\nTemps d\u0027exécution\nNous avons représenté dans la partie droite de la table 1 les temps d\u0027exécution obtenus par les deux méthodes et uniquement pour le calcul du graphe. En comparaison avec l\u0027algorithme des Voisins Relatifs, notre algorithme obtient les meilleurs temps sur la totalité des bases testées. Par exemple pour les bases Art2 et Art5, AntGraph est respectivement 500 à 800 fois plus  1)) ou en désaccord ((0,1) et (1,0)) entre les graphes construits par chacune des méthodes.\nrapide. Nous pouvons remarquer que plus le nombre de données est important, plus notre algorithme est rapide par rapport à l\u0027algorithme des Voisins Relatifs. La complexité de ce dernier étant en O(n 3 ), le temps d\u0027exécution devient prohibitif pour les grands ensembles de données. A ce niveau, notre algorithme conserve des temps d\u0027exécution très compétitifs, ce qui présage de nouvelles applications dans le cadre des grandes bases de données, et représente un avantage certain.\nCette baisse de la complexité est du au fait que les nouvelles fourmis qui arrivent ne se comparent pas à l\u0027ensemble des données déjà présentes dans le graphe mais suivent les chemins de plus grande similarité et ne rencontrent ainsi qu\u0027un nombre très limité de données. A titre indicatif, dans un arbre équilibré, en suivant une branche de la racine vers les feuilles on ne rencontre que O(ln(n)) noeuds.\nNéanmoins, ces performances en terme de temps de calcul ne renseignent pas directement sur la qualité du graphe construit. Nous avons donc poursuivi nos expérimentations pour montrer que le graphe construit par AntGraph est représentatif de la similarité entre les données, au même titre que Voisins Relatifs.\nDonnées\nVoisins \nQualité des graphes construits\nNous rappelons qu\u0027un graphe peut se représenter sous la forme d\u0027une matrice binaire où 1 correspond à l\u0027existence d\u0027un lien entre deux sommets. Nous pouvons donc mesurer, avec une matrice de confusion, l\u0027accord en terme de liens entre deux graphes (voir table 2).\nNous pouvons remarquer dans un premier temps que pour l\u0027ensemble de nos matrices de confusion, le rapport du nombre total de non-liens (un 0 entre deux sommets) pour Voisins Relatifs d\u0027une part et pour AntGraph d\u0027autre part est relativement proche de 1. C\u0027est le cas par exemple pour la base Art4 où le graphe de Voisins Relatifs totalise 19658 liens à 0 pour un équivalent dans le graphe de AntGraph à 19454. Les deux méthodes sont donc en accord sur ce point. On remarque de même qu\u0027il y a un accord entre les liens qui sont créés dans les deux graphes : la moitié des liens de Voisins Relatifs se retrouvent dans AntGraph. Enfin, nous constatons que notre algorithme produit entre 2 à 4 fois plus de liens que Voisins Relatifs. C\u0027est le cas par exemple pour la base Iris (Voisins Relatifs contient 203 liens pour 495 liens dans AntGraph) avec un rapport de 2 et la base Art3 (Voisins Relatifs contient 1273 liens pour 5915 liens dans AntGraph) avec un rapport de 4.\nOn constate donc qu\u0027AntGraph construit plus de liens que Voisins Relatifs. Dans le cas de Voisins Relatifs, il est difficile de rencontrer la situation où un noeud est voisin d\u0027un grand nombre d\u0027autres noeuds. Concernant AntGraph, nous devons alors nous interroger sur la pertinence de ces liens par rapport à Voisins Relatifs. Pour cela, nous avons mesuré, dans un premier temps, pour un graphe donné, la similarité moyenne entre les voisins. Dans un second temps, nous avons effectué une mesure entre les données en considérant les k plus proches voisins (k \u003d 1, 2, ou 3). Et dans un troisième et dernier temps, la mesure a été faite sur l\u0027ensemble des données.\nLa table 3 donne les résultats obtenus. On peut constater alors qu\u0027AntGraph crée des liens supplémentaires mais pas de manière aberrante par rapport à la similarité. En effet, la similarité moyenne des liens de AntGraph est du même ordre de grandeur que celle des 2 plus proches voisins, et reste de plus très inférieure à la similarité moyenne du graphe complet.\nConclusion\nNous avons proposé dans cet article une méthode pour la construction de graphes de voisinage. Elle s\u0027inspire d\u0027un modèle de construction de structures chez les fourmis artificielles. Le graphe est construit de manière progressive et incrémentale à partir d\u0027un noeud initial. Chaque fourmi se déplace en suivant le chemin de plus grande similarité afin de trouver un noeud sur laquelle elle se connecte. Nous avons testé notre approche sur un ensemble de bases et nous avons montré que les temps d\u0027exécution sont compétitifs par rapport à l\u0027algorithme des Voisins Relatifs. Par ailleurs, nos tests sur la qualité des graphes ont montré également que notre approche propose des graphes représentatifs de la similarité entre les données.\nDans les perspectives en cours d\u0027étude, nous pouvons principalement indiquer les pistes suivantes. D\u0027une part le traitement de grands volumes de données paraît possible gràce à des temps très courts. Ainsi nous souhaitons faire des tests sur des flux de données afin de visualiser leur évolution au cours du temps. Ensuite, un défaut de notre approche (et des Voisins Relatifs) vient du fait que la construction du graphe est découplée de l\u0027algorithme d\u0027affichage. Autrement dit, pour un graphe très important, il est facile de le calculer grâce à la complexité faible d\u0027AntGraph, mais il devient difficile de l\u0027afficher car le temps de convergence des algorithmes à base de forces et de ressorts devient important. Donc nous allons combiner la construction incrémentale du graphe avec une version incrémentale de l\u0027algorithme de visualisation. Enfin, nous avons commencé à tester des opérations interactives permettant de parcourir le graphe, de le modifier en décrochant des noeuds qui vont ensuite être réinjectés dans le graphe. De cette manière nous allons rendre plus interactif cet outil d\u0027exploration de données. \n"
  },
  {
    "id": "927",
    "text": "Introduction\nLe problème de la découverte des modèles temporels caractérisant le comportement des systèmes dynamiques est un enjeu majeur pour les tâches de contrôle et de surveillance. La raison de base réside dans la difficulté des experts humains d\u0027apprendre et de formuler leurs connaissances sur la dynamique de ces processus. La surveillance est effectuée à partir d\u0027un ensemble d\u0027observations (séquences d\u0027occurrences d\u0027événements discret) produites par le système de pilotage. Les séquences d\u0027observations remontées par le système de supervision sont porteuses de connaissances temporelles sur les relations causales entre les différentes variables du processus.\nNotre approche est centrée sur la découverte des séquences particulières d\u0027événements signe d\u0027un comportement particulier. Nous proposons de représenter le comportement du systèmes sous la forme de chroniques (un formalisme graphique pour la représentation des motifs temporels où les noeuds sont les classes d\u0027événements et les arcs représentent les contraintes temporelles liant les classes d\u0027événements) (Dousson et Duong (1999), Ghallab (1996)). Ce choix de représentation s\u0027est révélé particulièrement adapté à la représentation des évolutions de systèmes dynamiques, tout en maintenant une complexité raisonnable pour le traitement en temps réel des occurrences d\u0027événements pour la supervision. Notre méthode de découverte des chroniques se déroule en deux phases : la première phase consiste à modéliser la séquence d\u0027événements discrets en intégrant l\u0027approche stochastique proposée par Le Goc et Bouché (2005). Cette approche est basée sur la représentation d\u0027une séquence d\u0027événements discrets sous les formes duales d\u0027une chaîne de Markov homogène et une superposition de processus de Poisson. Dans la deuxième phase, nous proposons un algorithme, appelé BJT4R (Backward Jump with Timed constraints For Roads), destiné à la découverte des chroniques à partir du modèle obtenu durant la première phase. L\u0027algorithme BJT4R est une extension de l\u0027algorithme Viterbi (Viterbi (1967)) aux chaînes de Markov, basé sur l\u0027application de la relation Chapmann-Kolmogorov comme fonctionnelle de coût.\nLa section suivante présente les principales approches de découverte des motifs séquentiels à partir de données datées. La section 3 introduit l\u0027approche de modélisation adoptée, dans la section 4 nous présentons l\u0027algorithme BJT4R. Les résultats préliminaires pour la découverte des processus de fabrication des wafers sont présentés dans la section 5. La conclusion de ce papier évoque les prochaines étapes de travail.\nContexte\nLa problématique générale est la suivante : étant donné un ensemble de comportements particuliers ou ordinaires, observés dans une série d\u0027expériences, quelles sont les modèles temporels qui caractérisent au mieux ces comportements ? Des questions similaires ont été traitées dans le domaine de Fouille de Données Temporel.\nIntroduits pour la première fois dans Agrawal et Srikant (1995), les motifs séquentiels peuvent être vus comme une extension de la notion des règles d\u0027associations (Agrawal et al. (1993)), intégrant la notion de temps. Dans Agrawal et Srikant (1995), les auteurs proposent une approche permettant la découverte des motifs séquentiels à partir de bases de données contenant des séquences de transaction d\u0027achat effectuées par des clients. Une séquence est constituée de plusieurs transactions, réalisées par un client. Une transaction est caractérisée par un identifiant, une date de transaction et l\u0027ensemble des produit achetés, appelés ItemSet. Le problème de la découverte de motifs séquentiels consiste à rechercher l\u0027ensemble des sé-quences ayant des supports supérieurs à un certain seuil minimal. Un ensemble d\u0027algorithmes ont été tirés de cette approche fréquentielle : AppioriAll, ApprioriSome et DynamicSome. Ces algorithmes de recherche des motifs séquentiels présentent quelques limites concernant la prise en compte des contraintes temporelles. Ce problème a conduit à la recherche des séquences généralisées définies dans (Srikant et Agrawal (1996)). Cette technique de recherche permet d\u0027obtenir des motifs séquentiels respectant certaines contraintes temporelles définies par l\u0027utilisateur (par exemple, regroupement des achats lorsque leurs dates sont assez proches, considération des Itemsets (achats) comme trop rapprochés pour apparaître dans le même motif fréquent).\nDans Mannila et al. (1997) . Lorsque les épisodes ont été découverts, des règles sont déduites afin de décrire ou de prédire toute ou partie d\u0027une séquence. Cette méthode proposée traite le temps d\u0027une manière implicite : la seule contrainte temporelle qu\u0027elle autorise est une borne maximale (la largeur de la fenêtre temporelle) sur la durée des épisodes, celle-ci devant être fixée par l\u0027utilisateur. Par contre, les contraintes temporelles liant les éléments des épisodes ainsi découverts sont ignorées. Ghallab (1996) propose une méthode permettant la découverte des modèles de chroniques à partir d\u0027un ensemble de séquences d\u0027alarmes, divisées en deux sous ensembles : un ensemble de séquences positives (exemples) et un ensemble de séquences néga-tives (contre exemples). L\u0027idée générale de la méthode est de considérer dans un premier temps les séquences d\u0027événements comme des suites ordonnées, sans tenir compte l\u0027aspect temporel. Ceci revient à conserver l\u0027ordonnancement des événements, et à supprimer toute notion de temps. Une fois les séquences dépourvues des écarts temporels, on détermine les chroniques les plus longues qui sont communes à tous les exemples, et qui ne sont pas reconnues par les contres exemples. Pour les motifs séquentiels découverts, les contraintes temporelles entre les éléments de modèles peuvent être déterminées par les experts ou par le calcul des écarts minimaux et maximaux entre chaque couple de deux alarmes afin d\u0027englober toutes les occurrences de ces deux types d\u0027alarmes. Une autre approche proposée par Dousson et Duong (1999) permettant la découverte des modèles de chroniques à partir d\u0027une séquence d\u0027alarmes, appelée journal d\u0027alarmes. Cette approche repose sur une analyse fréquentielle de la séquence d\u0027alarmes, visant à identifier les formes temporelles récurrentes. Cette méthode est une extension de l\u0027approche proposée par Mannila et al. (1997) (Algorithme Minepi) par l\u0027introduction des contraintes temporelles entre les alarmes.\nDans un article plus récent, Mannila (2002) dresse un bilan des principales limites des approches présentées et identifie le principal défaut : les relations entre les éléments des modèles que ces algorithmes permettent de découvrir sont trop locales pour constituer une véritable représentation de la séquence étudiée. Il invite donc à rechercher des algorithmes adoptant un point de vue plus global. Le Goc et Bouché (2005) proposent ainsi une approche stochastique globale permettant la modélisation des séquences d\u0027événements discrets générées par un système à base de connaissances, de surveillance et de diagnostic de processus dynamiques. Cette approche est basée sur la représentation d\u0027une séquence d\u0027événements discrets sous les formes duales d\u0027une chaîne de Markov homogène et d\u0027une superposition de processus de Poisson.\n3 Modélisation de séquences -Approche Stochastique-\nLe couple (o k , o k+1 ) de deux occurrences successives liées à une même variable x décrit l\u0027évolution temporelle de la fonction\n. Nous allons utiliser la notation \"e i :: C j \" pour noter que l\u0027événement discret e i appartient à la classe d\u0027événement C j . Par extension, nous notons \"o i :: C j \" une occurrence d\u0027un événe-ments discret appartenant à la classe C j . La relation binaire\ndécrit la relation orientée entre deux classes d\u0027événements discrets contraintes temporellement. \" [? ? , ? + ] \" est un intervalle de temps pour observer une occurrence de la classe de sortie C o après l\u0027occurrence de la classe d\u0027entrée C i .\nDans ce contexte, un modèle de chroniques est un ensemble de relations binaires temporellement contraintes entre des classes d\u0027événements discrets. Le modèle de chronique\n) \u003e définit deux relations binaires entre trois classes d\u0027événements discrets vérifiant la relation suivante :\n12 , ?\nLa figure 1 montre une représentation graphique de modèle de chronique M 123 représenté dans le \"Langage ELP\" ( Frydman et al. (2001) ; Le Goc et al. (2006) 23 , ? ?\nUn modèle de chroniques peut être utilisé dans une tâche de diagnostic pour la prédiction d\u0027une occurrence d\u0027une classe particulière dans une séquence. Comme la classe d\u0027événement C 3 dans le modèle M 123 . \nLorsque la chaîne de Markov est homogène, la probabilité de transition d\u0027un état i à la date t k?1 vers l\u0027état j à la date t k dépend uniquement des états i et j :\nLe processus de comptage des transitions d\u0027états dans une chaîne de Markov homogène est un processus de Poisson (N i j (t);t ? 0) où N i j (t) compte le nombre des transitions X(t k?1 ) \u003d i ? X(t k ) \u003d j de l\u0027état i vers l\u0027état j. C\u0027est le nombre de sous séquences ? \u003d (o k?1 :: C i , o k :: C j ) dans la séquence ?. Le processus de Poisson N i j (t) est entièrement défini par l\u0027unique paramètre ? i j , appelé le taux de Poisson, qui correspond au nombre de transitions (X(t k?1 ) \u003d i) ? (X(t k ) \u003d j) par unité de temps. Dans ce cas, la probabilité de transition dans la chaîne de Markov homogène est donnée par :\nLes contraintes temporelles sont évaluées à partir des délais d(o k :: \nLe délai moyen D i j entre deux occurrences de classe C i et C j dans ? est donné par : L\u0027algorithme BJT4R, basé sur l\u0027approche stochastique, est une extension de l\u0027algorithme Viterbi pour la recherche des chroniques dans un espace d\u0027états Markovien. La relation de Chapmann-Kolmogorov est utilisée pour définir la fonction de coût des chroniques, afin de sélectionner les chroniques les plus probables. Cet algorithme opère en deux étapes :\n1. Identification des chroniques sans contraintes temporelles, l\u0027idée est d\u0027identifier un ensemble des motifs séquentiels les plus probables liant une classe d\u0027événement d\u0027entrée à une classe d\u0027événement de sortie. Cette étape est basée sur l\u0027utilisation de la matrice de probabilité de transition construite à partir de la séquence ? et l\u0027utilisation de la relation de Chapmann-Kolmogorov comme fonction de coût dans l\u0027algorithme Viterbi.\n2. Établissement des contraintes temporelles en utilisant la superposition des processus de Poisson, l\u0027idée est d\u0027utiliser directement les contraintes temporelles estimées à partir des processus de Poisson composés.\nDécouverte des chroniques sans contraintes temporelles\nSoit X \u003d (X(t k )), k ? 0, une chaîne de Markov homogène correspondant à la séquence ?. Soit S M l\u0027ensemble des états de X correspond à l\u0027ensemble des classes d\u0027événements discrets ayant une occurrence dans ?,\nM , sa matrice de transition des occurrences binaires de sous séquences\nLa longueur d\u0027une chronique |? | \u003d k est le nombre de relations binaires contenues dans ? (i 0 , i 1 , . . . , i k ). Une chronique réduit à un seul état a une longueur de 0. Selon la propriété d\u0027absence de mémoire de chaîne de Markov, la probabilité\nn\u003d1 Comme la chaîne de Markov est homogène, la probabilité\n. . , i k ) est égale à la probabilité de transition d\u0027un état i 0 vers une autre état i k en passant par (k ? 1) états intermédiaires. Elle est donnée par le produit des probabilités des transitions d\u0027un état à l\u0027autre le long du chemin :\nLa probabilité p k i j d\u0027aller d\u0027un état i vers l\u0027état j en k transitions est égale à la somme de toutes les probabilités des chroniques de longueur k liant l\u0027état i à l\u0027état j, donnée par la relation de Chapmann-Komologrov :\nSi la chaîne de Markov est homogène, on a :\nLa relation 15 donne la probabilité totale de tous les chroniques possibles de longueur k menant de l\u0027état i vers l\u0027état j, y compris les chroniques de faible probabilité, qui correspondent à des comportements peu fréquents dans le processus modélisé par la chaîne de Markov. L\u0027objectif de l\u0027algorithme est d\u0027identifier les m chroniques les plus probables liant deux états i et j dans une chaîne de Markov X \u003d (X(t k ), k ? 0) construite à partir d\u0027une séquence ?. -Un état est ajouté s\u0027il n\u0027apparaît pas dans les chroniques sous la construction. Cela signifie que l\u0027algorithme interdit l\u0027apparition de plus q\u0027une classe d\u0027événement dans une chronique (ligne 10). -Un état est ajouté dans la chronique s\u0027il conduit à un nouveau chronique de probabilité supérieure à un seuil minimal (ligne 13-14).\nÉtablissement des contraintes temporelles\nLa seconde phase de l\u0027algorithme consiste à calculer les contraintes temporelles entre les classes d\u0027événements des chroniques identifiées. L\u0027idée est d\u0027utiliser directement les contraintes temporelles estimées à partir des processus de Poisson composés déduits de la superposition des processus de Poisson (paragraphe 3, équations 9 et 10).\nLa contrainte temporelle entre chaque couple de classes (C i ,C j ) est un intervalle de la forme 0, 2 ? i j où ? i j est le taux de Poisson, qui correspond au nombre de transitions (o k?1 ::\nL\u0027algorithme BJT4R est un des outils développés au sein du \" Laboratoire ELP \", un environnement Java dédié à l\u0027analyse des séquences d\u0027événements discrets ( Frydman et al. (2001)). La section suivante présente l\u0027application industrielle de l\u0027algorithme BJT4R pour la découverte des routes de fabrication des wafer dans la société STMicroelectronics.\nApplication\nL\u0027application proposée dans cette section concerne les routes de fabrication des wafers dans le site de production de STMicroelectronics. Un wafer est une galette de silicium sur laquelle sont gravées des puces électroniques pour la télécommunication. Le système de supervision de processus de fabrication génère une large quantité d\u0027informations (? 10.000 alarmes par jour). Ces informations décrivent les différentes étapes de processus de fabrication sous la forme d\u0027occurrences d\u0027événements discrets correspondant aux débuts et fins de chacun des traitements appliqués sur la plaque. Cette suite de traitements (appelée route) transforme les plaques de silicium en wafers contenant des puces électroniques. -Recette. A ce niveau de granularité le plus élevé, une occurrence est un couple (r,t) où t est la date de début de la recette r. Dans ce cas, une route est une suite d\u0027occurrences de recettes parmi les 5189 types (classes) de recettes. L\u0027application présentée dans cet article concerne une première approche du problème, nous nous sommes intéressées au niveau de granularité plus bas : le niveau Équipement. L\u0027objectif est de montrer la faisabilité de l\u0027approche aux processus de fabrication exploités sur le site Rousset de la société STMicroelectronics. Dans cette application, un modèle de chroniques est un ensemble de relations binaires les plus probables liant les équipements deux à deux en satisfaisant les contraintes temporelles. Une contrainte temporelle est le temps moyen entre deux traitements successifs effectués sur les équipements de la relation binaire.\nPour appliquer l\u0027approche stochastique, deux conditions doivent être satisfaites :\n1. Les occurrences des événements doivent être indépendantes. 2. Le processus de génération des occurrences doit se comporter comme une superposition de processus de Poisson.\nDans notre application, ces conditions sont vérifiées. La première est assurée par la dé-finition du système de supervision (les alarmes sont générées indépendamment les unes des autres). Selon les experts de STMicroelectronics, le taux d\u0027occurrences des événements discrets par jour est globalement stable durant toute la durée de fabrication, sauf incidents exceptionnels. La figure 3 montre six processus de Poisson correspondant à six équipements sélectionnés aléatoirement, cela garantie la deuxième condition.\nFIG. 3 -Superposition des processus de Poisson\nLa chaîne de Markov associée comprend 309 états(i.e 95481 transitions). La figure 4 pré-sente une partie de la matrice P de probabilité de transitions entre les classes d\u0027événements discrets. La séquence d\u0027événements analysée correspond à une superposition de 309×(309?1) \u003d 95172 processus de Poisson composés. Pour chaque processus de Poisson, le temps interoccurrences est constant, et correspond à la probabilité maximale de la loi exponentielle. L\u0027application de l\u0027algorithme BJT4R à la séquence ? produit un graphe (Fig 5) liant la classe d\u0027événement d\u0027entrée à la classe d\u0027événement de sortie. La figure 5 montre partiellement l\u0027ensemble des chroniques les plus probables menant de l\u0027équipement 1130 à l\u0027équipement 1206, la classe d\u0027événement spéciale 0 dénote le début de la route.\nL\u0027algorithme BJT4R est paramétré afin de produire les cinq chroniques les plus probables de longueur 15. Par souci de lisibilité, seulement les débuts et les fins des chemins sont montrés dans la figure 5.\n"
  },
  {
    "id": "928",
    "text": "Introduction\nDe nos jours, bien que les moyens de stockage soient de plus en plus performants et de moins en moins chers, les entrepôts de données arrivent vite à saturation et la question des données à conserver sous forme d\u0027historique va se poser rapidement. Il faut donc choisir quelles données doivent être archivées, et quelles données doivent être conservées actives dans les entrepôts de données. La solution qui est appliquée en général est d\u0027assurer un archivage périodique des données les plus anciennes. Cette solution n\u0027est pas satisfaisante car l\u0027archivage et la remise en ligne des données sont des opérations coûteuses au point que l\u0027on peut considérer que des données archivées sont des données perdues (en pratique inutilisables dans le futur) du point de vue de leur utilisation dans le cadre d\u0027une analyse des données.\nDans cette communication, nous proposons une solution pour éviter la saturation des entrepôts de données. Un langage de spécifications de fonctions d\u0027oubli des données anciennes est défini pour déterminer les données qui doivent être présentes dans l\u0027entrepôt de données à chaque instant. Ces spécifications de fonctions d\u0027oubli conduisent à supprimer de façon mécanique les données à \u0027oublier\u0027, tout en conservant un résumé de celles-ci par agrégation et par échantillonnage. L\u0027agrégation et l\u0027échantillonnage constituent deux techniques standard et complémentaires pour résumer des données. Considérons un entrepôt de données d\u0027analyse des click-stream sur les sites web. Avec le temps, les données détaillées anciennes deviennent de moins en moins \u0027utiles\u0027 et peuvent donc être agrégées par jour ou par mois par exemple. En plus d\u0027agréger des données, on peut conserver certaines données jugées intéres-santes ou choisies de façon aléatoire dans le but de pouvoir effectuer des analyses sur les données de l\u0027entrepôt.\nLe langage de spécifications est défini dans le cadre du modèle relationnel : sur chaque table, est défini au moyen de spécifications un ensemble de n-uplets à archiver. Pour des raisons applicatives, parmi les n-uplets à archiver, des échantillons peuvent être conservés dans le cadre de l\u0027utilisation de l\u0027entrepôt. De plus, des algorithmes pour mettre à jour le contenu de l\u0027entrepôt conformément aux spécifications de fonctions d\u0027oubli sont présentés. A noter que ces algorithmes gérés par l\u0027administrateur de bases de données ont été étudiés et programmés dans un prototype sous Oracle. Cette communication a pour but d\u0027apporter une réponse au problème de saturation des entrepôts mais l\u0027approche décrite ici s\u0027applique de façon générale aux bases de données relationnelles.\nLa suite de ce papier est structurée comme suit. La section 2 expose l\u0027état de l\u0027art des travaux en rapport avec les fonctions d\u0027oubli. Dans la section 3, sont présentées les spécifica-tions de fonctions d\u0027oubli, après avoir défini formellement l\u0027âge d\u0027une donnée et présenté un exemple de motivation. La section 4 propose des structures de données adaptées au stockage des données agrégées et présente les algorithmes pour mettre à jour le contenu de l\u0027entrepôt après application des fonctions d\u0027oubli. La section 5 traite de la conservation des échantil-lons. Enfin, dans la section 6, une conclusion et des perspectives associées à ce travail sont présentées.\nEtat de l\u0027art\nCe travail est à relier aux travaux sur le « vacuuming », une approche permettant de supprimer physiquement dans une base de données temporelle, les données plus anciennes qu\u0027une certaine date seuil. Ce concept a été développé par Jensen (Jensen, 1995): lorsqu\u0027une date particulière est spécifiée, les données antérieures à cette date sont considérées comme étant inaccessibles et doivent donc être supprimées physiquement de la base de données.\nDans (Skyt et al., 2001), une technique de réduction des données est décrite dans le cadre des bases de données multidimensionnelles. Un langage de spécifications est proposé pour réduire la granularité des données les plus anciennes en agrégeant les données anciennes à des niveaux supérieurs (plus grossiers). Ces travaux sont similaires aux nôtres. Cependant, les approches que nous proposons dans cette communication s\u0027appliquent aux entrepôts de données et plus largement aux bases de données relationnelles, où des contraintes référentiel-les peuvent exister entre les tables, à la différence de ces travaux, qui ne s\u0027appliquent que sur des données multidimensionnelles stockées dans un cube de données. De plus, les auteurs ne proposent pas de conserver le détail de certaines données dans une perspective d\u0027analyse des données. Plus récemment, beaucoup de travaux sont effectués sur les flux de données, où il existe un fort besoin de résumer les données par rapport au temps. Par exemple, dans (Chen et al., 2002), le problème du calcul des agrégats temporels pour les flux de données est étu-dié, et il est suggéré de maintenir les agrégats à différents niveaux de granularité par rapport au temps : les données les plus anciennes sont agrégées à des niveaux grossiers, alors que les données récentes sont agrégées avec un niveau plus fin. Dans cette communication, nous reprenons des concepts de ces travaux mais la façon dont sont agrégées les données est spéci-fiée par l\u0027administrateur de l\u0027entrepôt au lieu que cela soit contrôlé par le flux.\nCe travail est à relier aussi à l\u0027expiration des données (Garcia-Molina et al., 1998) dans le cadre des vues matérialisées (voir (Gupta et Mumick, 2005)) : l\u0027approche consiste à détecter les données dont la suppression n\u0027affecte pas la maintenance des vues matérialisées de l\u0027entrepôt.\nOn peut citer également les travaux de (Toman, 2001) sur l\u0027expiration des données : le problème est d\u0027étudier l\u0027expiration des données dans le contexte des bases de données historisées qui stockent l\u0027historique des différents états de la base de données. Cependant, dans nos travaux, nous ne nous intéressons pas à l\u0027historique des différents états de la base de données mais à l\u0027historique des données stockées explicitement dans la base de données ou dans l\u0027entrepôt de données.\nOn peut évoquer le principe du ramasse-miettes (Boehm, 2002) dans les programmes objet. Le ramasse-miettes constitue une forme de gestion automatique de la mémoire. Le principe est de déterminer les objets qui ne sont plus référencés et de récupérer le stockage utilisé par ces objets. Le ramasse-miettes doit analyser les liens entre les objets avant de les supprimer. Dans nos travaux aussi, lorsque les fonctions sont appliquées, des n-uplets peuvent être à archiver alors qu\u0027ils sont référencés par d\u0027autres n-uplets qui ne sont pas encore archivés. La section 4.3 traite un problème similaire, lorsque des contraintes référentielles (voir (Gardarin, 1999)) existent entre les tables.\nNotre contribution dans cette communication est l\u0027extension et la réalisation d\u0027un prototype de mise en oeuvre du langage de spécifications de fonctions d\u0027oubli présenté dans (Boly et al., 2004). Dans ce travail, les spécifications -définies par l\u0027administrateur de la base de données -permettent d\u0027appliquer de façon mécanique les fonctions d\u0027oubli, qui gèrent à la fois la suppression de données à archiver (avec une prise en compte des contraintes référen-tielles) et la mise à jour des résumés conservés qui sont des agrégats et des échantillons de données détaillées.\nDans (Chaudhuri et al., 2001), les auteurs montrent qu\u0027on peut répondre à des requêtes en utilisant seulement des échantillons et des données agrégées au lieu de la base totale. D\u0027une part, lorsque l\u0027on ne dispose que de données agrégées sans les données détaillées correspondantes, il est toujours possible de répondre à des requêtes d\u0027agrégation en fournissant des réponses approximatives. D\u0027autre part, dans la théorie de l\u0027échantillonnage (voir (Ardilly, 1994)), il est montré qu\u0027on peut inférer des propriétés sur un ensemble de données à partir des échantillons. De plus, dans la fouille de données comme les arbres de décisions par exemple, on peut seulement considérer des agrégats sur des données détaillées. Cela justifie notre approche de conservation de résumés de données à archiver.\nSpécification de fonctions d\u0027oubli\nNotion d\u0027âge d\u0027une donnée\nDans le cadre de ce travail, nous considérons que chaque donnée (n-uplet d\u0027une table dans notre étude) est associée à une date notée t s qui correspondra soit à la valeur d\u0027un attribut de type Date explicitement associé à la donnée ou soit au timestamp du système représentant la date de dernière mise à jour de la donnée. L\u0027âge d\u0027une donnée calculé à la date courante notée t c est défini comme étant la différence entre les dates t c et t s . Les dates t c et t s peuvent être exprimées dans les unités de temps suivantes : la seconde (SECOND), la minute (MINUTE), l\u0027heure (HOUR), le jour (DAY), le mois (MONTH), le trimestre ( Enfin, une relation d\u0027ordre peut être définie entre les âges : considérant deux âges age1 et age2, on dit que age1 \u003c age2 si et seulement si : en_secondes(age1) \u003c en_secondes(age2), où en_secondes est une fonction qui transforme en secondes une durée (un âge) qui est exprimée dans une autre unité de temps, en utilisant les conventions présentées ci-dessus. Par exemple: 30 DAY \u003c 3 MONTH \u003c 1 YEAR. Les spécifications LESS THAN définissent quand les données détaillées (les n-uplets de la table COMMANDE) doivent être archivées et le résumé à conserver après application de la fonction d\u0027oubli. Pour les données de moins de 30 jours (c\u0027est-à-dire les données qui ont un âge inférieur à 1 mois), on garde le détail dans la table COMMANDE (c\u0027est-à-dire les n- Enfin, la spécification « KEEP SAMPLE (1000) WHERE montant\u003e4000 » signifie la conservation d\u0027un échantillon aléatoire simple composé de 1000 n-uplets parmi les n-uplets archivés et qui ont un montant supérieur à 4000.\nExemple de motivation\nLangage de spécifications de fonctions d\u0027oubli\nUn langage a été défini pour spécifier la fonction d\u0027oubli associée à chaque table de la base de données. La grammaire du langage est présentée en annexe. Il comprend des caracté-ristiques supplémentaires telles que la discrétisation d\u0027attributs numériques dans le but de les utiliser comme attributs sur lesquels peut s\u0027effectuer une agrégation 1 . Lorsque les spécifications de fonctions d\u0027oubli sont définies, l\u0027application des fonctions d\u0027oubli est automatique : des algorithmes et des structures de stockage sont nécessaires pour assurer la gestion des fonctions d\u0027oubli. \nGestion des fonctions d\u0027oubli par agrégation\nStructure de stockage pour les données agrégées\nMise à jour des données agrégées\nLes algorithmes que nous proposons pour mettre à jour les données agrégées sont basés sur trois propriétés : (1) les mesures d\u0027agrégation sont additives, (2) les différents cubes sont disjoints, (3) la mise à jour des données agrégées et celle des données détaillées commutent. En conséquence, tous les cubes d\u0027une fonction d\u0027oubli sont disjoints : aucune cellule d\u0027un cube est incluse (fonctionnellement) dans une cellule d\u0027un cube plus agrégé. Les différents cubes ont des contenus exclusifs ; ils stockent des données de périodes différentes par rapport au temps.\nAdditivité des mesures d\u0027agrégation\nCommutativité des mises à jour : les mises à jour à effectuer sur les données agrégées du fait des fonctions d\u0027oubli sont de deux types : (1) un n-uplet de détail à archiver d\u0027une table doit être stocké dans un cube (un seul), (2) des données déjà agrégées d\u0027un cube peuvent être transférées dans un autre cube plus agrégé.\nPuisque dans la répercussion des n-uplets de détail vers les cubes, chaque n-uplet de dé-tail n\u0027est pris en compte que dans un seul cube, celui où il satisfait le critère 5 de la spécifica-tion correspondante, il est alors équivalent de commencer par répercuter le détail vers les cubes puis d\u0027effectuer le transfert des données agrégées entre les cubes, ou de commencer par le transfert entre les cubes pour ensuite répercuter les n-uplets de détail. A noter que les mises à jour à effectuer du fait des fonctions d\u0027oubli peuvent être appliquées à tout instant, soit de façon régulière (par exemple chaque jour), ou soit de façon irrégulière ou soit sporadiquement. A chaque application des fonctions d\u0027oubli, de nouveaux n-uplets de la table peuvent satisfaire les critères des spécifications d\u0027agrégation et doivent donc être agrégés et stockés dans les cubes correspondants. Egalement, lorsque des données agrégées d\u0027un cube C ne vérifient plus le critère de la spécification correspondante mais vérifient le critère correspondant à un cube plus agrégé C\u0027, ces données doivent alors être transférées dans ce cube C\u0027. A noter que l\u0027on fait l\u0027hypothèse que l\u0027espace de stockage nécessaire pour contenir les données des différents cubes est limité. Cela est rendu possible par le fait que les différents cubes sont disjoints (propriété présentée ci-dessus), et par un système d\u0027activation/désactivation pour les cellules des cubes. Lorsqu\u0027une donnée doit passer d\u0027un cube à un autre, la cellule qu\u0027elle occupait ne va pas être supprimée mais désactivée. Et donc, pour répercuter une donnée vers un cube, on commencera par vérifier si elle peut occuper la place d\u0027une cellule désactivée auquel cas, cette cellule est activée.\nEn conséquence de la propriété de commutativité des mises à jour, nous distinguons deux procédures qui peuvent être exécutées dans un ordre quelconque : (1) la procédure qui transfère les données agrégées entre les cubes, (2) la procédure qui répercute les n-uplets de détail archivés vers les cubes. Dans les algorithmes présentés ci-dessous, chaque cellule d\u0027un cube est caractérisée par deux champs : la position et la mesure. La position d\u0027une cellule est l\u0027ensemble des valeurs des niveaux de dimensions qui déterminent cette cellule. Par exemple, la position de la cellule (\u0027Paris\u0027, \u0027F\u0027, \u002715/05/06\u0027, 2000) est (\u0027Paris\u0027, \u0027F\u0027, \u002715/05/06\u0027) et la mesure correspondante est égale à 2000.\nTransfert des données agrégées entre les cubes\nPour chaque cube C j (j \u003d n, n -1, n -2, …, 2) Pour chaque cube C i (i \u003d j -1, j -2,…, 1) Pour chaque cellule c \u003d (x 1 ,…, x n , t, m) dans C i avec age(S j ) ? age(t) \u003e age(S i ) /* S j et S i sont les spécifications associées respectivement aux cubes C j et C i */ /* le parcours est accéléré par un index sur la dimension Temps */ soit c la cellule dans le cube C j couvrant c si ( c trouvé)\nsinon /*vérifier s\u0027il existe une cellule désactivée c trouvé dans C j qui peut remplacer Il est à noter que dans la mise à jour des cubes, il est possible que des cellules d\u0027un cube C i ne puissent pas satisfaire le critère de la spécification correspondant au cube suivant C i+1 mais qu\u0027elles vérifient le critère d\u0027une spécification correspondant à un cube C p tel que p\u003ei+1. Cette situation survient en général lorsque les fonctions d\u0027oubli n\u0027ont pas été appliquées pendant un certain temps : par exemple, considérant notre exemple présenté au paragraphe 3.2, lorsque les fonctions d\u0027oubli n\u0027ont pas été appliquées pendant cinq ans, les données du cube correspondant à la deuxième spécification (LESS THAN 3 MONTH : SUM (montant) BY Ville, Sexe, DAY) doivent être directement transférées au cube le plus agrégé. Ceci explique la présence dans l\u0027algorithme des deux premières « boucles Pour » imbriquées.\nRépercussion des n-uplets de détail vers les cubes\nOn note que l\u0027ordre de traitement des cubes n\u0027est pas important car chaque n-uplet de dé-tail ne va être stocké que dans un seul cube.\nPour chaque cube C j (j \u003d 1, 2,…, n)  er de chaque mois), il peut arriver que des n-uplets de la table COMMANDE soient spécifiés à archiver alors que les n-uplets de FACTURE correspondants ne sont pas encore archivés (puisque pour les factures, on les garde pendant 4 mois et que pour les commandes au delà d\u00271 mois, elles peuvent être archivées). Nous proposons la solution suivante : suspendre l\u0027archivage de t 1 jusqu\u0027à ce que t 2 soit archivé et le marquer à archiver. A chaque application des fonctions d\u0027oubli, il faut alors vérifier si les nuplets qui le référencent sont archivés, auquel cas le n-uplet devra être archivé. Et donc, dans notre exemple, on va marquer les n-uplets de la table COMMANDE spécifiés à archiver. Ils ne pourront être archivés que lorsque les n-uplets de la table FACTURE correspondants (qui les référencent) auront été archivés. A noter qu\u0027aucune mise à jour n\u0027est plus autorisée sur les n-uplets marqués à archiver.\nGestion de la conservation d\u0027échantillons\nComme présenté au paragraphe 3.2, le langage de spécifications permet de conserver des échantillons de données archivées. Il s\u0027agit de la spécification KEEP SAMPLE qui indique que l\u0027on doit assurer la maintenance d\u0027un échantillon aléatoire simple de taille fixe à chaque fois que des données détaillées sont archivées. Les n-uplets échantillonnés sont stockés dans une table séparée ayant la même structure que la table sur laquelle est définie la fonction d\u0027oubli.\nA chaque application des fonctions d\u0027oubli, de nouveaux n-uplets peuvent être à archiver et doivent être pris en compte dans la mise à jour de l\u0027échantillon. On suppose par exemple qu\u0027un échantillon aléatoire simple de 1000 individus est conservé parmi les n-uplets à archiver, et que 100 nouveaux n-uplets sont à archiver depuis la dernière mise à jour d\u0027oubli.\nLa maintenance de l\u0027échantillon est assurée de manière incrémentale : on utilise l\u0027algorithme réservoir de Vitter (Vitter, 1985). La propriété fondamentale de ce type d\u0027algorithme est qu\u0027après le traitement de chaque individu, le réservoir 7 constitue un échan-tillon aléatoire des individus visités. A noter que la taille des échantillons est fixée dans la définition de la spécification KEEP SAMPLE.\nComme indiqué à la fin de la section 2, des échantillons aléatoires sur une population peuvent être utilisés pour inférer de l\u0027information sur cette population totale à partir des données observées dans les échantillons (voir (Ardilly, 1994)). Reprenant notre exemple, on peut estimer la somme des montants de commandes à partir de l\u0027échantillon tiré sur les nuplets à archiver de la table COMMANDE. Pour effectuer de telles estimations, l\u0027effectif de la population de n-uplets à échantillonner est conservé.\nConclusion et perspectives\n"
  },
  {
    "id": "929",
    "text": "Contexte\nNotre travail s\u0027inscrit dans le contexte d\u0027un projet de fouille de données mis en oeuvre à Maroc Telecom et visant à mieux connaître la clientèle de la téléphonie mobile. Le niveau de consommation d\u0027un abonné est souvent calculé à partir de la durée facturée qui s\u0027avère insuffisante pour la plupart des cas. En effet, deux abonnés peuvent avoir la même durée d\u0027appel pour des services différents mais sans avoir le même degré de consommation. D\u0027où la nécessité d\u0027introduire d\u0027autres critères dans la détermination du niveau de consommation.\nProblématique et approche de résolution préconisée\nLa problématique à laquelle on s\u0027intéresse consiste à établir une échelle de mesure permettant de quantifier les niveaux de consommation afin discriminer entre les abonnés (Viertl, R. (2005)). L\u0027approche de résolution proposée comporte trois étapes principales. Son originalité réside dans l\u0027utilisation de la théorie des ensembles flous à travers la définition expérimentale d\u0027une fonction d\u0027appartenance (Mitaim, S. et B, Kosko. (2001)).\nDans une première étape, on attribue un score aux abonnés par rapport aux critères de type catégoriels (trafic, produits, services, plage horaire) caractérisant le niveau de consommation. La binarisation de chaque modalité de ces critères induit la création de plus de 60 variables indicatrices dans notre exemple qui traite 2 millions d\u0027enregistrements. Afin de réduire la taille de ces indicatrices, l\u0027Analyse des Correspondances Multiples (ACM) a été utilisée fournissant ainsi 10 facteurs expliquant 80,62% d\u0027inertie totale. L\u0027objectif de l\u0027étape 2 est la segmentation des abonnés par produits et services afin de discriminer entre les abonnés en se basant sur le comportement d\u0027utilisation des produits et services. Les facteurs obtenus par l\u0027ACM ont été utilisés comme variables d\u0027entrée des différents algorithmes non supervisés (K-means, Two Step, Réseau de Kohonen) qui ont été comparés. Le réseau de Kohonen a été plus concluant en terme d\u0027homogénéité entre les classes. Les inerties intra classes de chaque facteur ont ensuite été utilisées dans l\u0027étape 3 comme indicateur de la variation du niveau de consommation au sein de chaque classe. Un tel indicateur a permis d\u0027établir une mesure du niveau de consommation tenant compte de la durée facturée en appliquant la théorie des ensembles flous (Masson, M. H. (2003)).\nLa fonction d\u0027appartenance que nous proposons, dans l\u0027étape 3, pour quantifier l\u0027ensemble flou A \u003d « niveau de consommation en téléphonie mobile », représente une mise à l\u0027échelle exponentielle de la durée facturée. Cette fonction d\u0027appartenance donnée par l\u0027équation (1), présente non seulement l\u0027avantage de ne pas masquer les optima locaux de la distribution de la durée facturée mais aussi l\u0027avantage d\u0027être paramétrée par des critères liés aux produits et services.\noù d : la durée facturée, K \u003d {C 1 , C 2 , C 3 , C 4 } : l\u0027ensemble des classes issues du réseau de Kohonen, F j : l\u0027ensemble des facteurs contribuant à la construction de la classe j, , x K j ? ij : l\u0027inertie du facteur i appartenant à F j . I j : la fonction indicatrice de la classe j, p : une constante à ajuster (dans notre cas p : 0.01 suite aux différents tests numériques réalisés).\n"
  },
  {
    "id": "931",
    "text": "Introduction\nLes méthodes conceptuelles de classification imposent une description monothétique des classes, cette contrainte forte devant a priori engendrer une perte de la qualité des partitions au sens des critères optimisés par les méthodes de partitionnement. Pour étudier ce phéno-mène nous avons comparé une méthode de classification conceptuelle appelée DIVCLUS-T qui est une méthode de classification descendante hiérarchique monothétique avec la méthode ascendante hiérarchique WARD et la méthode de partitionnement des k-means. Cette méthode conceptuelle s\u0027applique à des données quantitatives et des données qualitatives. Cette comparaison a pu être effectuée car cette méthode conceptuelle optimise le même critère que les méthodes de WARD et des k-means. Ainsi comme WARD et les k-means, elle est basée sur la minimisation de l\u0027inertie des classes mais à la différence de WARD et des k-means elle fournit par construction une interprétation simple et naturelle des classes. La question à laquelle nous allons chercher à répondre est la suivante : quel est le prix payé, en terme d\u0027inertie, pour cette interprétation sous forme de règles des classes ?\nAvant de présenter un peu plus en détail la méthode DIVCLUS-T et de comparer à partir de 6 bases de l\u0027UCI les performances en terme d\u0027inertie de DIVCLUS-T avec WARD et les k-means, nous donnons un petit exemple introductif. Il s\u0027agit des données \"protéines\" de Hand et al. (1994)  \nFIG. 1 -Dendrogramme obtenu avec WARD pour les données proteines\nOn obtient bien grâce à ce dendrogramme des classes de pays proches en terme de consommation en protéines mais si l\u0027on souhaite avoir une interprétation facile de ces classes, une étape supplémentaire est alors nécessaire.\nLa méthode DIVCLUS-T a été appliquée au même jeu de données et le dendrogramme de la hiérarchie est représenté à la Figure 2 On note dans le Tableau 1 que les partitions de DIVCLUS-T sont meilleures que celles de WARD de 2 à 4 classes, puis WARD prend le dessus jusqu\u0027à 7 classes et enfin les pourcentages sont identiques à partir de 9 classes (car les partitions sont identiques). La perte d\u0027inertie due à la contrainte sur la description des classes est probablement compensée par le fait que DIVCLUS-T est un algorithme descendant qui trouve les partitions en peu de classes dans ses premières itérations tandis que WARD est ascendant est trouve donc ces mêmes partitions dans ses dernières itérations.\nLa méthode DIVCLUS-T\nLa méthode de classification monothetique DIVCLUS-T a d\u0027abord été proposée dans le cadre plus général de l\u0027Analyse des Données Symbolique (Chavent (1997)) et avait alors été implémentée sous le nom DIV dans le logiciel SODAS (Bock et Diday (2000)). Elle avait également été présentée de manière succincte dans Chavent (1998) pour des données quantitatives et dans Chavent et al. (1999) pour des données qualitatives. Dans Chavent et al. (1999) la méthode, appelée DIVOP à l\u0027époque, était présentée dans le cadre d\u0027une application en dermatologie conjointement avec une autre méthode divisive monothétique appelée DIVAF, basée sur l\u0027analyse des correspondances multiples. Une méthode divisive de type monothétique utilisant le processus de Poisson a également été proposée par Pircon (2004). Récemment, une méthode de classification divisive proche de DIVCLUS-T a été implémentée dans la dernière version du logiciel SPAD sous le nom ICT pour Interactive Clustering Tree (Rakotomalala et LeNouvel (2006)).\nIci, le nom DIVCLUS-T a été choisi comme acronyme de DIVisive CLUstering Tree. Cette méthode procède comme toute méthode descendante hiérarchique par divisions successives et s\u0027articule autour des 3 points suivants : -Les divisions s\u0027arrêtent après étapes. On obtient donc le \"haut\" du dendrogramme c\u0027est à dire les partitions de 2 à · ½ classes.\n-A chaque étape cette méthode choisit de diviser la classe telle que la nouvelle partition ainsi obtenue soit d\u0027inertie intra-classe minimum. Pour des données qualitatives, l\u0027inertie est calculée avec la distance du ¾ sur le tableau disjonctif complet. Le critère d\u0027inertie intra-classe étant additif cela revient à choisir la classe telle que la variation de l\u0027inertie obtenue en la divisant soit maximum. Dans WARD on agrège à chaque étape les deux classes minimisant ce même critère de variation de l\u0027inertie. Dans WARD et dans DIVCLUS-T on utilise donc le même critère pour indicer la hiérarchie et donc évaluer la hauteur des paliers dans le dendrogramme. Dans DIVCLUS-T le choix de la classe à diviser est nécessaire puisque l\u0027on ne continue pas nécessairement les divisions jusqu\u0027à l\u0027obtention des singletons. -L\u0027algorithme de bi-partitionnement d\u0027une classe à Ò éléments en deux sous-classes n\u0027éva-lue pas l\u0027inertie intra-classe des ¾ Ò ½ ½ bi-partitions possibles pour en retenir la meilleure, mais évalue ce critère sur l\u0027ensemble de toutes les bi-partitions induites par l\u0027ensemble de toutes les questions binaires. On utilise donc ici l\u0027approche monothétique des arbres de décisions et de régression (Morgan et Sonquist (1963), Breiman et al. (1984)) mais dans un cadre non supervisé. Les différences sont nombreuses. En particulier il n\u0027y a pas de variable à expliquer et pas d\u0027élagage. -construire une hiérarchie des modalités (en représentant chaque modalité par la descrption moyenne des objets qui la possèdent et en pondérant cet objet moyen par l\u0027effectif de la modalité). On retient alors une partition en Ñ classes, étant suffisamment petit pour que l\u0027on puisse parcourir toutes les partitions en deux classes de ces groupes de modalités -définir Õ ordres sur les Ñ modalités en utilisant l\u0027ordre des modalités sur les Õ composantes principales issues de l\u0027Analyse Factorielle des Correspondances Multiples.\nEvaluation sur six bases de l\u0027UCI\nNous avons voulu répondre à la question suivante : l\u0027aspect rigide et simpliste du processus monothétique de DIVCLUS-T implique-t-il des partitions beaucoup moins bonnes en terme d\u0027inertie intra-classe ? Pour donner un premier élément de réponse à cette question, nous avons comparé empiriquement le pourcentage d\u0027inertie expliquée des partitions de 2 à 15 classes obtenues avec DIVCLUS-T, WARD et les k-means, sur 6 jeux de données de l\u0027UCI Machine Learning repository (Hettich et al. (1998)). Ces six bases, trois quantitatives et trois qualitatives, sont décrites dans le tableau 2.\nLe tableau 3 donne les résultats pour les trois bases quantitatives et les trois méthodes (colonne DIV pour DIVCLUS-T, colonne WARD pour WARD, colonne W+km pour les centres mobiles sur la partition de WARD et la colonne km pour les centres mobiles en conservant le meilleure solution de 100 initialisations au hasard). Pour les données GLASS, on note que DIVCLUS-T est parfois meilleur que WARD (pour 4 classes), parfois moins bon (pour 2, 3 classes et de 12 à 15 classes) ou encore parfois équivalent (de 5 à 11 classes). Pour les don- \nTAB. 3 -Données quantitatives\nPour les trois bases qualitatives (tableau 4) on obtient le même type de résultats. Pour les données Solar Flare et CMC, DIVCLUS-T est meilleur que WARD jusqu\u0027à respectivement 10 et 8 classes. Pour les données Zoo, DIVCLUS-T reste toujours en dessous de WARD. C\u0027est peut-être du aux fait que les variables sont binaires et que pour des données qualitatives, le nombre de bi-partitions évaluées à chaque étape augmente avec le nombre de modalités.\nEchantillonages\nAfin de mieux évaluer ces résultats nous avons créé pour chacune des trois bases quantitatives d\u0027UCI (Glass, Pima et Abalone) 100 échantillons de taille 150 pour Glass, 500 pour \nTAB. 4 -Données qualitatives\nPima et pour Abalone. Sur chacun de ces échantillons nous avons appliqué les quatre mé-thodes de classification utilisées sur la base complète. Les premières colonnes des tableaux 5 et 6 donnent la moyenne des écarts entre la meilleure solution et la solution obtenue par la méthode. Par exemple dans la colonne DIV nous avons la moyenne des écarts entre DIV et la meilleure solution. Cette moyenne est souvent égale à zéro pour la méthode des centres mobiles (km) ce qui montre que cette méthode est presque toujours la meilleure. Ceci est aussi vrai pour la stratégie WARD+km sur la base Abalone mais sur la base Pima l\u0027écart est un peu différent de zéro. Avec la base Glass la stratégie km est la meilleure quand le nombre de classes est inférieur à 7, la stratégie WARD+km devient la meilleure stratégie quand le nombre de classes est supérieur à 7. Les dernières colonnes donnent, en pourcentage, le nombre de fois où la solution obtenue par DIV est meilleure que la solution obtenue par WARD. Pour les bases Abalone et Glass la méthode DIV est meilleure que la méthode WARD lorsque le nombre de classes est petit, on observe cela aussi pour la base Pima mais uniquement lorsque le nombre de classes est égal à 2. Cet indicateur montre assez clairement que pour un nombre de classes assez petit la méthode DIV est plus efficace que la méthode WARD. \nConclusion\nDIVCLUS-T est une méthode monothétique qui a l\u0027avantage par rapport aux méthodes polythétiques telles que WARD et les k-means, de donner une interprétation très simple et immédiate des classes et un arbre hiérarchique facile a lire et à comprendre par l\u0027utilisateurs. Il est en outre normal que cette contrainte sur l\u0027interprétation des classes, imposée dans le processus de classification, implique une perte de qualité au niveau du critère d\u0027inertie. Il n\u0027est donc pas surprenant que DIVCLUS-T soit généralement moins performant que WARD ou les k-means. En revanche, nous avons noté sur les exemples des 6 bases de l\u0027UCI que le comportement de DIVCLUS-T reste tout à fait raisonnable en terme d\u0027inertie, surtout pour les partitions en peu de classes.\nLorsqu\u0027un utilisateur veut obtenir une partition en un nombre de classes relativement important, par exemple pour réduire le nombre d\u0027objets, WARD et les centres mobiles sont certainement plus performants que DIVCLUS-T. Mais lorsque l\u0027utilisateur s\u0027intéresse aux partitions en peu de classes et à leur interprétation, alors DIVCLUS-T semble être une alternative inté-ressante aux méthodes classiques.\n"
  },
  {
    "id": "932",
    "text": "Préparation de données\nAvec l\u0027émergence des systèmes d\u0027information au tournant des années 90, la récolte des données brutes a été rendue complètement indépendante de toute finalité statistique. L\u0027analyse de ces données est un objectif qui intervient dans un second temps. La phase de préparation, dont le but est de construire à partir des données brutes une table de données pour modélisation, est donc devenue une partie critique et souvent coûteuse en temps du processus de fouille de données (Chapman et al., 2000).\nL\u0027analyste se trouve dans la situation suivante. D\u0027une part, il dispose d\u0027un entrepôt de données mis en place et alimenté dans un autre but que celui d\u0027une quelconque analyse statistique. D\u0027autre part, le propriétaire de l\u0027entrepôt envisage d\u0027exploiter ses données afin de compléter ses connaissances et pose une question à l\u0027analyste. Celui-ci doit alors tourner la question en un problème d\u0027analyse statistique, extraire de l\u0027entrepôt les données susceptibles d\u0027être pertinentes vis-à-vis de la question posée, les mettre sous forme d\u0027une table, procéder à la modélisation et interpréter les résultats afin de répondre à la question initiale.\nEntre autres, la préparation passe par la définition et la sélection des individus et variables constituant la table qui va servir à la modélisation. Les possibilités qui s\u0027offrent à l\u0027analyste lors de cette étape sont, virtuellement, limitées uniquement par son imagination. En pratique, l\u0027extraction et la mise en forme sont soumises à deux contraintes : celle sur les ressources et celle sur le passage à l\u0027échelle des méthodes de modélisation.\nD\u0027une part, le temps alloué à une étude est nécessairement limité, souvent très contraint. Les données brutes ne sont pas toujours accessibles facilement et rapidement. D\u0027autre part, les algorithmes de modélisation sont rarement linéaires en le nombre de lignes et colonnes de la table. De manière plus insidieuse, l\u0027analyste doit éviter de tomber dans le piège de la dimension, qui conduit à considérer un nombre de variables trop élevé pour le nombre d\u0027individus à disposition. L\u0027information portée par les individus est noyée dans un espace de représenta-tion de taille inadaptée et de nombreuses techniques de modélisation ont les pires difficultés à produire un modèle pertinent.\nNous laissons de côté la question de la définition des individus et nous intéressons à la définition des colonnes de la table. L\u0027analyste doit capturer à l\u0027aide d\u0027un ensemble de variables l\u0027information pertinente pour la question posée, sans autre aide que son intuition sur les variables \"a priori susceptibles\" d\u0027expliquer le phénomène étudié et sous contrainte de ressource et de passage à l\u0027échelle des procédés de modélisation. Dans cet article, nous nous proposons d\u0027aider l\u0027analyste dans cette tâche.\nL\u0027article est organisé comme suit. La section 2 présente le cadre de la sélection de variable, pour mieux circonscrire le lieu où se situe notre contribution. Notamment, nous montrons l\u0027intérêt de disposer d\u0027une méthode d\u0027évaluation automatique et fiable d\u0027une métrique. La section 3 déduit des travaux de Ferrandiz et Boullé (2006b) une telle méthode, dans le cadre de la classification supervisée. Pour la convenance du lecteur, la technique de modélisation introduite dans Ferrandiz et Boullé (2006b) est brièvement décrite dans la section 4. Enfin, la section 5 illustre notre propos par des expérimentations sur un problème de préparation de profils de consommation en téléphonie fixe dans un contexte supervisé. données brutes, une variable X : I ? X est construite en choisissant un espace de représen-tation X et en définissant un procédé de mesure X projetant chaque individu dans X. Une fois la variable X : I ? X obtenue, se pose la question de son intérêt relativement à la question étudiée. On entre alors dans le cadre de la sélection de variables.\nDe nombreux articles permettent de cerner les pratiques de la sélection, notamment Blum et Langley (1997), Kohavi et John (1997) et Guyon et Elisseeff (2003). Dans l\u0027article Kohavi et John (1997), une distinction est opérée entre approche enveloppe (de l\u0027anglais wrapper) et approche filtre (de l\u0027anglais filter).\nL\u0027approche enveloppe consiste à évaluer l\u0027impact d\u0027une modification de l\u0027ensemble de variables sur la performance d\u0027un modèle. Une technique de modélisation étant spécifiée, elle est appliquée à différents ensembles de variables et l\u0027ensemble de variables conduisant au modèle le plus performant est conservé. Chaque évaluation nécessite l\u0027ajustement d\u0027un modèle, ce qui se révèle coûteux en temps. Cette approche, en faisant intervenir le modèle dans l\u0027évaluation, est plus adaptée à la phase modélisation qu\u0027à la phase de préparation d\u0027une analyse. L\u0027approche filtre univariée de la sélection de variables repose sur l\u0027emploi d\u0027une méthode d\u0027évaluation de l\u0027intérêt d\u0027une variable quelconque X : I ? X. Non seulement en pratique mais aussi sur le plan formel, l\u0027espace de représentation X est souvent muni d\u0027une métrique (ou : distance) ? : X × X ? R + . C\u0027est notamment le cas lorsque X est multidimensionnelle numérique, mais aussi lorsque X est séquentielle.\nL\u0027information contenue dans une variable peut donc être observée à travers le prisme d\u0027une matrice de distance sur les individus. Une telle matrice est obtenue en calculant les distances au sens de ? entre tout couple d\u0027individus. Dans ce cas, l\u0027évaluation de la métrique induit une évaluation de la pertinence de la variable X.\nEvaluation probabiliste d\u0027une métrique\nNous proposons ici un protocole d\u0027évaluation de la pertinence d\u0027une métrique exploitant des travaux antérieurs. Le contexte est celui de la classification supervisée : une variable cible catégorielle unidimensionnelle est à expliquer.\nDans le cas d\u0027une variable statique numérique unidimensionnelle, Boullé (2006) aborde la question de l\u0027évaluation de la pertinence vis-à-vis d\u0027une variable cible catégorielle comme un problème de maximisation de la probabilité a posteriori. Les modèles considérés sont les partitions de la variable numérique en intervalles. Une approche bayésienne permet de définir\nFIG. 1 -Exemples de partitions de Voronoi pour la métrique euclidienne.\nun critère s\u0027interprétant comme la probabilité que le modèle explique la variable cible catégo-rielle. La sélection du modèle le plus probable conduit à une méthode de discrétisation d\u0027une variable statique numérique. La probabilité du modèle le plus probable s\u0027utilise alors comme un indicateur de pertinence de la variable descriptive relativement à la variable cible.\nDans Ferrandiz et Boullé (2006b), l\u0027approche est adaptée afin de traiter le cas où l\u0027espace de représentation est muni d\u0027une métrique. A l\u0027aide de la métrique, une partition de Voronoi est associée à tout sous-ensemble d\u0027individu (c.f. fig.1 pour des exemples). Le partitionnement d\u0027une variable en intervalles est ainsi généralisé en un partitionnement de l\u0027espace en cellules. L\u0027ensemble des partitions obtenues constitue l\u0027ensemble des modèles. La probabilité qu\u0027un modèle explique la variable cible catégorielle est explicitée et la sélection du modèle le plus probable conduit à une méthode de sélection d\u0027instances. Là encore, la probabilité associée au modèle sélectionné constitue un indicateur supervisé de pertinence de la métrique. La méthode est évaluée dans Ferrandiz et Boullé (2006a) en tant que méthode de sélection d\u0027instances pour la classification par le plus proche voisin.\nSoyons plus formels. L\u0027ensemble des individus est noté I. Pour une métrique ?, tout sousensemble H de I définit une partition de I, dite de Voronoi : chaque individu est associé au plus proche élément de H au sens de ?. Notons H ? (I) l\u0027ensemble des partitions de Voronoi.\nSi on note Y la variable cible catégorielle, nous disposons d\u0027un critère c ?,Y : H ? (I) ? R qui associe à chaque partition la probabilité que cette partition explique la cible Y . Nous proposons alors d\u0027évaluer la métrique ? par la probabilité du modèle le plus probable :\nLa fonction c * fournit une évaluation supervisée de la qualité de la métrique ? et permet ainsi de comparer différentes métriques et, à travers elles, différents espaces de représentation et différentes variables. Pour une métrique ? donnée, on applique un algorithme d\u0027optimisation combinatoire et on attribue la valeur rencontrée optimale du critère c ?,Y . Le critère c ?,Y et l\u0027heuristique d\u0027optimisation proposés dans Ferrandiz et Boullé (2006a) et Ferrandiz et Boullé (2006b sont décrits plus en détail dans la prochaine section.\nLe critère c ?,Y est non paramétrique et régularisé. Il quantifie le compromis entre le nombre de groupes de la partition et la discrimination de la cible, ce qui correspond à un compromis entre complexité du modèle et ajustement du modèle aux données de l\u0027échantillon. La régu-larisation est un moyen d\u0027endiguer le phénomène de sur-apprentissage et d\u0027assurer ainsi la fiabilité de la décision. Etant non paramétrique, l\u0027évaluation se passe de validation ou de validation croisée. On dispose ainsi de plus d\u0027individus pour ajuster le modèle, ce qui augmente sa qualité.\nAfin de travailler avec un indicateur normalisé, nous considérons la transformation suivante de c * :\noù c 0 (?) est la valeur du critère c ?,Y pour le modèle constitué par un seul groupe. D\u0027après les travaux de Shannon (1948), l\u0027opposé du logarithme d\u0027une probabilité s\u0027interprète comme une longueur de codage. L\u0027indicateur g * (?) s\u0027interprète alors comme un gain de compression. Il est supérieur à 0 et inférieur à 1. Si g * (?) \u003d 0, la métrique ? n\u0027apporte aucune information sur la variable cible. Plus la valeur de g * (?) est proche de 1, plus les classes cibles sont séparées, et plus la métrique ? est pertinente.\nSélection d\u0027instances : critère et algorithme\nL\u0027évaluation de la qualité d\u0027une métrique introduite ci-dessus repose sur la recherche de la meilleure partition de Voronoi induite par un sous-ensemble de l\u0027échantillon. Pour la convenance du lecteur, nous décrivons dans cette section le critère et l\u0027heuristique d\u0027optimisation utilisés, déjà proposés et étudiés dans Ferrandiz et Boullé (2006a)  et Ferrandiz et Boullé (2006b).\nEvaluation bayésienne d\u0027une partition\nPosons les notations. Soit I un ensemble de N individus. Soit Y : I ? L une variable cible catégorielle, L étant un alphabet de taille J. Soit X : I ? X une variable descriptive, l\u0027espace de représentation X étant muni d\u0027une métrique ?.\nSoit H un ensemble de K individus. La partition de Voronoi V (H) \u003d (V (k)) k?H associée à H est définie par :\nPour k ? H, la cellule de Voronoi V (k) contient les individus i dont k est l\u0027élément de H le plus proche, relativement à ?. L\u0027élément k est appelé prototype de la cellule V (k). La fig.1 donne des exemples de telles partitions. Si les éléments de H sont indexés de 1 à K, N k (1 ? k ? K) est le nombre d\u0027individus dans la cellule du k eme élément de H et N kj désigne le nombre de tels individus de la j\nL\u0027approche adoptée dans Ferrandiz et Boullé (2006b) conduit à évaluer H par :\nLe premier terme quantifie la probabilité d\u0027apparition du nombre K de cellules de V (H), le second terme quantifie la probabilité d\u0027apparition des K prototypes de H, et les derniers termes quantifient cellule par cellule la probabilité d\u0027apparition de la variable cible. Ils résultent de l\u0027adoption de l\u0027approche bayésienne de l\u0027évaluation : la somme des deux premiers termes correspond à l\u0027a priori sur les modèles et la somme des deux derniers termes à la vraisemblance de la cible. La dernière somme, d\u0027après la formule de Stirling log x! ? x log x ? x + O(log x), se comporte asymptotiquement comme N fois l\u0027entropie conditionnelle de la variable cible Y en connaissance de la partition :\nIntuitivement, le critère quantifie la discrimination des distributions par un terme entropique et la pondère par un coût structurel mesurant la complexité de la partition. Pour cela, il prend en compte diverses caractéristiques, comme le nombre de groupes, la répartition des instances dans les groupes (i.e les coefficients N k ), la répartition des instances dans les classes (i.e. les coefficients N kj ).\nHeuristique d\u0027optimisation\nNous disposons d\u0027un critère d\u0027évaluation des ensembles de prototypes H. Il reste à proposer un algorithme d\u0027optimisation. Nous reprenons celui introduit dans Ferrandiz et Boullé (2006a), qui consiste à encapsuler une optimisation gloutonne d\u0027un ensemble de prototypes dans une méta-heuristique. La complexité algorithmique de l\u0027optimisation gloutonne est ré-duite en exploitant les propriétés du critère et des partitions de Voronoi. La méta-heuristique permet de remettre en question le résultat de la recherche gloutonne afin d\u0027optimiser encore un peu plus le critère.\nL\u0027heuristique gloutonne Glouton(H) s\u0027applique à tout ensemble H de K prototypes. Elle comporte K étapes. A chaque étape, tout sous-ensemble obtenu par élimination d\u0027un prototype est évalué. Parmi ceux-ci, celui minimisant la valeur du critère est déclaré vainqueur. Cette étape gloutonne est itérée et appliquée à chaque vainqueur successif, jusqu\u0027à évaluation d\u0027un singleton. Le meilleur sous-ensemble rencontré est retourné.\nCette méthode considère O(K 2 ) sous-ensembles et chaque évaluation nécessite la recherche du plus proche prototype pour chaque instance. Une implantation directe de Glouton(H) possède une complexité en O(N K 3 ). L\u0027exploitation des propriétés du critère et des partitions de Voronoi conduit à une implantation de complexité un O(N K log K) nécessitant un espace mémoire en O(N K).\nL\u0027heuristique gloutonne effectue rapidement un grand nombre d\u0027évaluations. Il est naturel d\u0027envisager une application répétée mais limitée de cet algorithme. Pour cela, la méta-heuristique de recherche à voisinage variable est adoptée (Hansen et Mladenovic (2001)). Elle consiste à appliquer l\u0027heuristique de base (i.e. l\u0027algorithme Glouton) à un modèle proche de la solution considérée. Si la nouvelle solution n\u0027est pas meilleure, on considère un voisinage plus grand. Sinon, la méta-heuristique repart de la nouvelle meilleure solution avec une taille de voisinage minimale. Ce procédé est contrôlé par une taille maximale du voisinage à explorer.\nNous illustrons les apports de notre méthode par des expérimentations sur des données de consommation en téléphonie fixe. C\u0027est un problème de classification de profils de consommation suivant 4 classes cibles A, B, C et D. La distribution des classes cibles sur l\u0027échantillon est uniforme. On dispose de 168 variables descriptives numériques, chacune mesurant la consommation téléphonique sur une tranche horaire de la semaine. Nous répartissons uniformément les 3516 individus de l\u0027échantillon entre un ensemble d\u0027apprentissage (75% des individus) et un ensemble de test (25% des individus), de manière stratifiée (i.e. en respectant la distribution a priori des classes cibles).\nEvaluation d\u0027une variable séquentielle\nLa méthode d\u0027évaluation d\u0027une métrique proposée dans cet article, en plus de quantifier la pertinence d\u0027une métrique, fournit un support explicatif : la partition la plus probable. Ainsi, on dispose d\u0027une distribution des classes cibles et d\u0027un prototype pour chaque groupe, ce qui autorise une explication du résultat purement numérique.\nNous illustrons cet aspect en appliquant notre méthode à la variable séquentielle constituée par les 168 variables descriptives du problème de classification de profils de consommation, l\u0027espace de représentation étant muni de la métrique L 1 . Autrement dit, chaque individu se voit associer une suite de 168 mesures de consommation et la distance entre deux profils est mesurée à l\u0027aide de la métrique L 1 . L\u0027évaluation de la métrique fournit un gain de compression de 0.051, ce qui est très faible et caractérise un fort mélange des classes cibles. Mais il n\u0027est pas nul et la méthode partitionne les individus en 7 groupes. Les distributions relatives à chacun des groupes sont représentées par des histogrammes groupés sur cette la fig.2. En calculant la valeur moyenne de chacune des 168 variables dans chaque groupe, on obtient 7 profils de consommation caractéristiques. Trois de ces profils sont reportés sur la fig.2, ainsi que le profil moyen de consommation (i.e. celui calculé sur tout l\u0027ensemble d\u0027apprentissage).\nLe résultat étant visualisable, il est facilement interprétable. Par exemple, on voit que les individus du groupe 7 sont en grand nombre (35% de l\u0027échantillon d\u0027apprentissage), qu\u0027ils ont une consommation moyenne plus élevée que la moyenne globale, et que ce comportement est majoritairement caractéristique de la classe A (la répartition dans les classes cibles A, B, C, D est (41%, 26%, 15%, 17%)). Le groupe 1 est quant à lui plus discriminant (la répartition dans les classes cibles est (16%, 20%, 57%, 6%)) avec un profil de consommation atypique (pics de consommation élevés), mais est de taille réduite (4% des individus). Le groupe 4 discrimine lui aussi la classe C, moins fortement tout de même que le groupe 1, et se différentie par une consommation moyenne très faible.\nAfin de vérifier de visu la fiabilité du découpage effectué, nous calculons les distributions en test (reportées sur la fig.2). Bien que l\u0027ensemble de test soit trois fois plus petit que l\u0027ensemble d\u0027apprentissage, on constate que la distribution des individus dans les groupes est stable ((4%, 5%, 6%, 11%, 19%, 21%, 35%) en apprentissage et (3%, 4%, 6%, 12%, 17%, 24%, 34%) en test), que les classes majoritaires dans chaque groupe en test sont les mêmes que celles observées en apprentissage, etc.\nNotre méthode n\u0027est pas la première à fournir de telles informations. Ainsi, l\u0027analyse discriminante, linéaire ou quadratique (Hastie et al., 2001) \nFIG. 3 -Consommation moyenne sur l\u0027ensemble d\u0027apprentissage et consommations moyennes dans chacun des groupes 1, 4 et 7. Les individus du groupe 1 correspondent à de très fortes consommations, avec des pics très marqués. Ceux du groupe 7 correspondent aux faibles consommations.\ndes prototypes, comme les méthodes de quantification (Kohonen, 2001), conduisent à de telles visualisations. Mais c\u0027est la seule à évaluer strictement les informations qui sont visualisées : le calcul du gain de compression prend en compte la répartition des individus dans les groupes, les répartitions des individus dans les classes cibles groupe par groupe. La visualisation n\u0027en est que plus adaptée.\nL\u0027analyse discriminante suppose les individus d\u0027une classe toutes générées par une même gaussienne. Les modèles considérés sont tous de même capacité, ce qui se traduit en pratique par un nombre de groupes égal au nombre J de classes. L\u0027exemple étudié ici montre que contraindre la capacité revient à limiter la richesse de l\u0027information extraite. De toute façon, les paramètres des J gaussiennes sont ajustés en maximisant la vraisemblance complète et la vraisemblance ne tient pas compte des différences de capacité : on ne peut utiliser la mesure de vraisemblance pour comparer un modèle d\u0027analyse discriminante linéaire avec un modèle d\u0027analyse discriminante quadratique, et encore moins avec un modèle d\u0027analyse de mélange (qui autorise un nombre quelconque de gaussiennes par classe, c.f. Hastie et al. (2001)). On est ramené à appliquer un second critère. En pratique, c\u0027est le risque empirique qu\u0027on utilise, avec ses limites.\nLes techniques de quantification, hautement paramétriques, nécessitent entre autre de fixer le nombre de prototypes. Le choix d\u0027un \"bon\" nombre de prototypes repose donc sur un critère alternatif. En pratique, là encore, on estime le risque empirique. L\u0027idée sous-jacente aux techniques de quantification étant de repousser les prototypes en cas de mauvais étiquetage et de les rapprocher dans le cas contraire, la présence de prototypes \"morts\" à la fin de l\u0027optimisation constitue de plus un effet secondaire peu désirable. En effet, de nombreux prototypes sont dé-placés au point de ne plus être sollicités par la suite. En terme de partition de Voronoi associée, cela signifie que plusieurs cellules finales ne contiennent aucun élément de l\u0027ensemble d\u0027apprentissage. Le résultat perd de sa pertinence et la visualisation associée est rendue caduque.\nS\u0027il est usuel de mettre de côté un ensemble d\u0027individus, dit de validation, pour ajuster certains paramètres ou contrôler la fiabilité de l\u0027estimation, c\u0027est inutile lorsqu\u0027on utilise notre méthode. Elle est en effet non paramétrique et la fiabilité est intrinsèquement assurée par l\u0027usage d\u0027un critère régularisé. Tous les individus servent à la prise décision, ce qui profite nécessairement à la qualité de celle-ci.\nComparaison et sélection de variables séquentielles\nLa définition d\u0027une variable séquentielle nécessite la définition d\u0027un espace de représenta-tion. Celui-ci est souvent muni d\u0027une métrique. Nous avons proposé dans ce qui précède une méthode d\u0027évaluation supervisée de la pertinence d\u0027une métrique. A travers le prisme d\u0027une métrique, nous sommes donc en mesure d\u0027évaluer la pertinence d\u0027une variable séquentielle par le gain de compression que mesure notre méthode.\nNous illustrons son utilité par un problème de sélection de variables séquentielles. Pour cela, nous considérons les 24 variables séquentielles définies par les tranches horaires du problème de classification de profils de consommation. Chaque variable est composée de 7 mesures, correspondant à la consommation sur chaque jour de la semaine pour une tranche horaire fixée. Pour chacune, nous munissons l\u0027espace de représentation de la métrique L 1 et appliquons notre méthode d\u0027évaluation sur l\u0027ensemble d\u0027apprentissage. Le gain de compression et le taux de bonne prédiction en test sont reportés pour comparaison sur la fig.4. Utilisé pour de la sélection, le gain de compression conduit à choisir la tranche horaire 14, ou toute variable dont le gain de compression mesuré dépasse un certain seuil fixé a priori par l\u0027analyste. A l\u0027opposé, le gain de compression est nul pour les tranches horaires de fin de nuit. Ceci signifie qu\u0027un seul groupe est constitué et que les classes cibles sont mélangées. Considé-rées isolément, ces variables ne sont d\u0027aucun intérêt. C\u0027est la présence d\u0027une régularisation, qui consiste à contrôler la discrimination opérée par la capacité de la partition, couplée avec le fait que les partitions considérées fournissent des capacités allant d\u0027un minimum (un seul groupe) à un maximum (autant de groupes que d\u0027instances), qui rend possible une telle conclusion.\nFIG. 4 -\nSélection d\u0027une métrique\nDans l\u0027expérience précédente, nous avons utilisé la métrique L 1 pour mesurer la distance séparant deux profils. Nous reproduisons cette expérience et considérons deux métriques supplémentaires : la métrique euclidienne et un noyau gaussien (définissant une métrique euclidienne dans un espace implicite). Les courbes de gain de compression sont reportées sur la fig.5.\nCertains comportements des courbes sont analogues. Par exemple, quelle que soit la mé-trique ici considérée, les tranches horaires de fin de nuit sont déclarées non pertinentes relativement à la cible. Mais c\u0027est l\u0027utilisation de la métrique L 1 qui conduit aux meilleurs gains de compression, quasiment pour toutes les tranches horaires. Pour ces variables séquentielles et cette cible, l\u0027analyste est conduit automatiquement et de manière fiable à choisir cette métrique au détriment des deux autres. S\u0027il dispose de temps, il peut même s\u0027aider de la visualisation proposée précédemment pour expliquer les différences de comportement sur chaque tranche horaire. \nFIG. 5 -\nConclusion\nEn fouille de données, dès lors que la récolte des données n\u0027est pas orientée dans le sens de l\u0027analyse, un travail de préparation est à mener. Une table doit d\u0027abord être construite pour ensuite procéder à une modélisation statistique qui réponde à la question posée par le propriétaire des données. A priori, de nombreuses variables sont susceptibles d\u0027expliquer le phénomène étudié et il s\u0027agit d\u0027inclure dans la table les plus pertinentes d\u0027entre elles.\nL\u0027approche adoptée en préparation est l\u0027approche filtre univariée, indépendante d\u0027un modèle particulier et plus à même de faire face à un nombre élevé de variables. Dans ce cadre, la qualité de la méthode d\u0027évaluation utilisée pour juger de l\u0027intérêt d\u0027une variable est cruciale. En exploitant des travaux antérieurs, nous avons ici proposé une méthode automatique et fiable d\u0027évaluation d\u0027une métrique, dans le cas de la classification supervisée. Nous avons illustré son apport sur un problème réel de classification de profils de consommation téléphonique.\nCet exemple d\u0027application montre l\u0027apport de notre méthode en préparation de données. L\u0027analyste dispose grâce à elle d\u0027un outil pour mener à bien la sélection filtre univariée des variables qu\u0027il supposent a priori pertinentes. Cet outil permet d\u0027évaluer la pertinence a posteriori (après observation des données) de variables dont l\u0027espace de représentation est muni d\u0027une métrique. Cette évaluation est automatique, fiable et se passe d\u0027un ensemble de validation. Plus généralement, l\u0027outil permet de sélectionner la métrique la plus adaptée.\nRéférences Blum, A. et P. Langley (1997). Selection of relevant features and examples in machine learning.\nArtificial intelligence 97(1-2), 245-271.\n"
  },
  {
    "id": "934",
    "text": "Introduction\nDans le cadre de la modélisation des étapes du raisonnement à partir de cas pour la réalisation d\u0027un outil logiciel qui fera office d\u0027un tuteur d\u0027aide pour l\u0027évitement des circonstances de pollution domestique exprimées dans des plaintes (Z. Bellia, 2004), nous souhaitons améliorer la méthode de tri basée sur la contiguïté des termes de la requête dans le texte d\u0027un document source. À l\u0027évidence, il est dans l\u0027intérêt de l\u0027usager du système de retrouver les cas les plus pertinents parmi les plaintes déjà traitées. Généralement, lorsqu\u0027un utilisateur formule une requête au système, il compte retrouver les documents dont la signification du contenu se rapproche le plus de sa demande. Par exemple, pour la résolution d\u0027une nouvelle plainte comportant le terme «couverture », il sera judicieux de retrouver les anciens cas de la mémoire archive relatifs non seulement au terme « couverture » lui-même, mais aussi aux «couettes », aux « duvets », aux « édredons », etc. Les documents contenant ces termes sont sans doute pertinents pour la plainte courante, néanmoins, ils ne seront pas sélectionnés par un modèle de recherche basé uniquement sur les occurrences directes des termes. Une solution incontournable est l\u0027utilisation d\u0027un réseau sémantique pour gérer le vocabulaire très variés qui peut être employé dans les plaintes. Dans l\u0027étape de l\u0027« élaboration » des cas en RàPC nous avons opté pour un modèle semi-structuré pour la constitution de la base. L\u0027interface usager de notre système propose une série d\u0027indexes sous forme de questions, dont les réponses apportent de l\u0027information pour la description du problème. Nous avons proposé de traduire ces indexes sous forme de modèles de balise dans un document XML, et la partie renseignée par l\u0027utilisateur représente pour nous le contenu des balises.\nAprès avoir présenté les outils à l\u0027origine de notre approche, mesure de similarité conceptuelle et modèle de proximité, nous introduisons notre approche prenant en compte les deux aspects. Le développement d\u0027un exemple montre l\u0027intérêt de notre méthode.\nLes outils\nDans ce chapitre, nous rappelons brièvement la notion de mesure conceptuelle pour la gestion de la sémantique ainsi que la notion de cooccurrence floue entre les termes. Pour formaliser les relations entre les termes nous les rattachons aux concepts de WordNet (C. Fellbaum, 1998).\nAspect sémantique\nZarga et Salotti (H. Zargayouna et S. Salotti, 2004) définissent une métrique conceptuelle inspirée des travaux de Wu et Palmer (Z. Wu et M. Palmer, 1994). Elles privilégient toujours les liens père-fils par rapport aux autres liens de voisinage en adaptant la mesure de Wu et Palmer qui pénalise dans certains cas les fils d\u0027un concept par rapport à ses frères. Elles introduisent la fonction Spec pénalisant ainsi les concepts qui ne sont pas de la même lignée. Nous illustrons cela dans l\u0027exemple développé dans la figure 1.\nTel que prof(C 1 ) est le nombre d\u0027arcs entre la racine de la hiérarchie et le concept C 1 en passant par le plus petit généralisant (PPG) du couple C 1 , C 2 . La valeur de prof b (PPG) correspond au nombre maximum d\u0027arcs qui séparent le PPG du concept Bottom.\nMesure de proximité\nCette mesure entre termes doit être mise en contexte lorsque l\u0027on traite de documents. Le modèle vectoriel introduit par Salton (G. Salton et C. Buckley, 1998) exclut toute notion de position et de distance entre les mots. De surcroit, le modèle de Salton est mieux adapté à la codification des textes longs qu\u0027à la codification des textes courts (A. Singhal, 1996). Compte tenu de la nature hétérogène des textes en notre possession, il est primordial d\u0027élargir notre réflexion aux modèles de représentation adaptés à la nature de notre ressource (hétérogène). Nous avons étudié à cet effet le modèle de recherche basé sur la proximité des termes et inspiré du modèle booléen classique. L\u0027approche de Mercier (A. Mercier et M. Beigbeder, 2005) repose sur l\u0027hypothèse que plus les occurrences des termes d\u0027une requête se trouvent proches dans un document de la base plus ce document est pertinent par rapport à cette requête.\nRNTI -X -Z. Heddadji et al. \nAinsi, la similarité est obtenue en normalisant l\u0027ensemble des scores par la longueur du document.\nPertinence sémantique locale\nNous apportons une extension au modèle existant en le combinant avec la mesure de similarité conceptuelle de Zarga et Salotti de la manière suivante: RNTI -X -Par Syno(t) nous indiquons l\u0027ensemble des termes proches sémantiquement de t. Un seuil de similarité est nécessaire pour caractériser l\u0027ensemble de ses éléments. Nous fixons un seuil de similarité pour la valeur de Sim(t i ,t) qui correspond au degré de similarité entre t et le concept auquel est rattachée la balise où il apparaît. Dans l\u0027exemple suivant, nous comparons les deux approches. Les scores de pertinence sont calculés aux différentes positions pouvant être prises dans un intervalle d\u0027occurrence précis de taille fixe k\u003d10 (par exemple). Prenons en tant que exemple, la forme filtrée lemmatisée de la balise \u003cstate\u003e (balise décrivant l\u0027état du logement) d\u0027une requête combinant les mots-clés suivants: r\u003d{humidity, rampart, salon}. Nous pouvons imaginer un passage de la plainte initiale exprimée de la manière suivante: «There is humidity on the rampart in the salon». Supposons qu\u0027il existe un dossier stocké en mémoire dont la partie problème contient cet extrait:«Many moistures gleamed all on the wall of my bed-room». d\u003d{moisture, wall, bed-room}. Les termes de la requête courante, a priori, n\u0027appartiennent pas au texte du document source, néanmoins le sens de ces deux passages est résolument le même. Le Tableau  Pour l\u0027application de cet exemple, nous nous sommes assurés que les similarités autorisées pour l\u0027augmentation de la pertinence locale soient supérieures au degré de similarité entre le terme appartenant au document source et le terme associé à la balise où apparaît l\u0027extrait du document source. Dans notre cas il s\u0027agit d\u0027un extrait de la balise \u003cState\u003e. Sim ZS (moisture,state)\u003d0.62\u003c0.70, Sim ZS (wall,state)\u003d0\u003c0.78 et Sim ZS (bedroom,state)\u003d0\u003c0.52. La pertinence sémantique de la requête par rapport au document source dans l\u0027exemple correspond à Sim(r,d)\u003d0.46, alors que ce score est nul si on applique la méthode directe. Ces résultats montrent que l\u0027extension que nous proposons augmente de manière significative la qualité des résultats. Ceci tend à prouver que l\u0027usage des ressources sémantiques est très utile dans la phase de recherche que nous souhaitons fine.\n"
  },
  {
    "id": "935",
    "text": "Introduction\nLa reconnaissance et l\u0027extraction d\u0027entités nommées cherche à localiser et à classer les éléments atomiques d\u0027un texte en catégories prédéfinies telles que noms de personnes, organisations, localisation, dates, quantités, valeurs monétaires, pourcentages etc. Ce domaine de recherche est très actif, bien que des outils commerciaux existent déjà. Citons, par exemple, REX 1 (Rosette R Entity Extractor), Inxight SmartDiscovery 2 , Convera-RetrievalWare Entity Extraction 3 and Xerox-Research Entity Extraction systemDu coté recherche, certains systèmes de reconnaissance d\u0027entités nommées utilisent des techniques à base de grammaires linguistiques, d\u0027autres des modèles statistiques. Les systèmes à base de grammaires construits à la main obtiennent souvent de meilleurs résultats au prix d\u0027un travail très important par des linguistes chevronnés. Par ailleurs, les systèmes à base de modèles statistiques demandent beaucoup de données d\u0027apprentissage annotées, mais sont plus faciles à porter vers d\u0027autres langages, domaines ou genres de textes.\nNous proposons une approche dans laquelle, à partir d\u0027une liste connue d\u0027entités, le système génère automatiquement des schémas de phrases pouvant contenir ces entités. Une étape d\u0027apprentissages, à partir d\u0027un très petit nombre de documents permet de ne garder que les schémas les plus pertinents. Cette approche s\u0027inspire de celle utilisée pour l\u0027extraction de données dans des documents semi-structurés tels que des pages Web (wrappers), basée sur la génération de programmes d\u0027extraction à partir d\u0027un petit nombre d\u0027exemples (Kushmerick (2000); Adelberg (1998); Irmak et Suel (2006); Lerman et al. (2003); Liu et al. (2003)). Au lieu de s\u0027appuyer sur les balises HTML des documents, nos règles s\u0027appuient sur les syntagmes du langage (balises linguistiques). Cette approche ne nécessite pas de ressources linguistiques particulières (McNamee et Mayfield (2002); Cucerzan et Yarowsky (1999)) ni de larges collections de tests.\nNous avons testé cette approche sur le rapport d\u0027activité de l\u0027Inria. Il s\u0027agit d\u0027identifier, dans le rapport d\u0027activité annuel, et plus particulièrement dans les sections décrivant les contrats de recherche et les relations internationales, les organismes cités avec lesquels les équipes de recherche coopèrent. Dans ce contexte, identifier le plus possible de ces organismes (\"rappel\") est plus important qu\u0027une précision élevée puisque que la liste des entités extraite peut être revue manuellement, même si cette tâche de vérification s\u0027avère très lourde pratiquement. D\u0027autre part, ce genre de rapport étant répétitif d\u0027une année sur l\u0027autre, et les partenaires évoluant lentement, il est intéressant que le processus d\u0027extraction puisse s\u0027affiner avec le temps.\nDomaine applicatif\nLe rapport d\u0027activité scientifique annuel de l\u0027Inria est composé d\u0027environ 180 rapports en anglais décrivant différents aspects de l\u0027activité scientifique des équipe de recherche. Depuis quelques années l\u0027Inria est intéressé à exploiter cette source riche d\u0027information, disponible en XML. Nous nous intéressons ici à identifier les nombreux partenaires des équipes, en exploitant les sections spécifiques décrivant les collaborations et les contrats.\nCe travail se heurte à plusieurs difficultés inhérentes à la collection. Le style de ces sections est très peu homogène, parfois télégraphique ou peu rédigé, avec une représentation des noms de partenaires souvent approximative, voir avec des orthographes erronées. Ces noms eux-mêmes peuvent être très divers : sigles plus ou moins développés (FT R\u0026D), localisations intégrées au nom (Inria Rocquencourt), noms d\u0027organismes (EDF, MIT), de laboratoires (LRI, LSR), de réseaux ou de noms de projets souvent confondus avec des noms communs (Oasis, PARIS, Ondes). Le travail manuel d\u0027annotation des documents, utilisés pour la phase d\u0027apprentissage et pour l\u0027évaluation, est une activité très coûteuses en temps et intrinsèquement difficile. Il serait exclu d\u0027extraire les noms de ces organismes à la main dans toute la collection (et chaque année).\nDans un premier temps, nous avons essayé d\u0027utiliser un outil existant de bonne réputation, à savoir ANNIE, un des composants du système GATE (Cunningham et al. (2002)) développé par l\u0027université de Sheffield (UK). Ce premier essai a été très décevant. Le taux de rappel était seulement de 0,23 si nous cherchions la liste des organismes, et même de 0,17 si nous cherchions toutes les occurrences des noms. Une des raisons est sans doute le style elliptique de cette partie du rapport, très différent du type de collections standards sur lesquels ANNIE est généralement validé (journaux, etc.). Nous avons donc décidé de développer une approche différente, partant des données réelles plutôt que de collections standards pour l\u0027apprentissage.\nMéthode utilisée\nLes documents sont au préalable annotés à l\u0027aide de ANNIE qui détecte la fonction grammaticale (nom, verbe etc.) des mots utilisés. Ce sont ces fonction grammaticales (syntagmes) qui seront utilisés pour la construction de schémas tel que nous en parlerons plus loin.\nDe façon standard, nous travaillons sur un ensemble réduit de rapports (collection test), utilisés dans la phase d\u0027apprentissage et pour l\u0027évaluation des résultats. Dans ces rapports, les noms des organismes sont identifiés et annotés à la main. Cet ensemble est divisé en trois sous-ensembles : le premier sous-ensemble, noté L, sert à construire une   Ensuite, en partant de l\u0027ensemble A annoté avec les seules entités de la liste L, nous appliquons un à un les schémas classés précédemment et nous extrayons de nouveaux organismes qui sont ajoutés à L à l\u0027itération suivante. À chaque étape, la précision et le rappel sont évalués et l\u0027algorithme s\u0027arrête lorsque le rappel atteint un certain seuil, ou que la précision devient inférieure à un autre seuil. En principe, le rappel va augmenter à chaque étape puisque les schémas les plus performants sont ajoutés en premier. La précision, initialement égale à 1, puisque calculée à partir des seuls organismes de L présents dans A, ne peut que se dégrader.\nÀ cette étape du processus, nous avons donc sélectionné des schémas d\u0027extraction dont nous avons pu contrôler la performance sur l\u0027ensemble A.\nDans la phase d\u0027évaluation, nous appliquons les schémas précédemment sélectionnés pour extraire les noms d\u0027organisation de l\u0027ensemble de test B . Comme B a été lui aussi été annoté au préalable, nous pouvons calculer la précision et le rappel pour valider notre approche.\nExpériences et résultats\nComme il est extrêmement fastidieux et difficile d\u0027identifier les organismes cités dans les documents, nous ne voulions pas avoir à le faire pour plus de 20 documents. Afin de tester différents paramètres de l\u0027algorithme, nous avons effectué des permutations aléatoires des documents dans les ensembles L, A et B afin de créer 10 jeux de test différents. Le tableau 2 présente les résultats pour 3 de ces jeux de test, pour des seuils d\u0027apprentissage de 0,6 pour la précision et le rappel. Nous nous intéressons à la fois à l\u0027identification des noms d\u0027organismes (comptage simple), et à l\u0027identification des occurrences de ces noms (comptage multiple).\nOn peut tout d\u0027abord remarquer que le rappel de départ pour l\u0027ensemble A est faible (0,17 et 0,21), ce qui indique une grande diversité de partenaires selon les différentes équipes. Bien que la précision de départ devrait être égale à 1 nous voyons que ce n\u0027est pas tout à fait le cas. En effet par absence de normalisation si L contient \"FT\" et un document de A ou B contient \"FT R\u0026D\" identifié comme nom d\u0027organisme, FT sera identifié mais considéré comme non valide pour la calcul de la précision et du rappel.\nLe rappel à la fin de la période d\u0027apprentissage a été multiplié par plus de 2 en moyenne pour le comptage simple, mais reste malgré tout assez faible. Il faut rappeler que l\u0027algorithme d\u0027apprentissage s\u0027arrête lorsque le rappel est plus grand que 0,6 ou la précision inférieure à 0,6 (pour le comptage multiple). On ne peut donc pas espérer des valeurs très élevées à la fin de l\u0027apprentissage, en utilisant des schémas génériques et calculés automatiquement. Nous avons évalués les résultats sur 5 jeux de test identiques pour 3 couples de seuils différents. Le tableau 3 montre les résultats moyens sur les 5 jeux de test.\nOn peut voir que les seuils ont une influence non seulement sur la précision finale, mais aussi sur le nombre de schémas validés par apprentissage. Ce nombre est plus petit si la pré-cision demandée est élevée, ce qui avantage le temps d\u0027extraction. En contrepartie le nombre d\u0027entités extraites est inférieur ce qui est inconvénient pour notre application.\nIl se trouve que la partie \"contrats\" contient plus facilement des partenaires industriels et la partie \"collaboration\" plus souvent des partenaires académiques. Nous avons donc faits des expériences en effectuant un apprentissage séparément sur chacun de ces groupes. Les résultats sont meilleurs pour la partie \"collaborations\", les noms d\u0027universités étant plus facile à identifier, mais contrairement à notre attente, les résultats sont moins bons quand on traite les parties \"contrats\" et \"collaborations\" de façon séparée plutôt qu\u0027ensemble.\nFinalement, nous avons appliqué les schémas sélectionnés à l\u0027ensemble des 180 rapports. Selon les expérimentations (non reportées ici, faute de place), 1500 à 3000 noms ont été extraits. Un essai de validation d\u0027une liste de 1500 noms a montré la difficulté d\u0027une telle tâche.\nConclusion\nNous avons présenté une méthode pour extraire les noms d\u0027organismes dans des parties de documents assez peu rédigées. Notre approche s\u0027inspire des méthodes inductives des extracteurs pour des documents semi-structurés, et ne requière pas d\u0027importantes ressources linguistiques ni de mise au point manuelle. Les résultats, bien qu\u0027un peu décevants, montrent qu\u0027il est possible de découvrir un grand nombre d\u0027organismes non connus à l\u0027avance.\nD\u0027une année sur l\u0027autre, il y a une certaine continuité dans les partenaires avec lesquels les équipes Inria travaillent. Il est donc raisonnable d\u0027utiliser la liste des organisations d\u0027une année pour initialiser l\u0027extraction d\u0027entités pour l\u0027année N+1. Même si les listes produites demandent à être validées manuellement, c\u0027est certainement plus rapide que d\u0027extraire manuellement le nom des organismes à partir des 180 rapports d\u0027activité. \n"
  },
  {
    "id": "936",
    "text": "Introduction\nRaisonner à partir de cas consiste à résoudre un problème à l\u0027aide d\u0027une base de cas, dans laquelle un cas représente un problème déjà résolu accompagné de sa solution (Riesbeck et Schank (1989)). Un système de raisonnement à partir de cas (RÀPC) sélectionne un cas dans la base de cas, puis adapte la solution associée. L\u0027adaptation nécessite des connaissances spéci-fiques au domaine d\u0027application. L\u0027acquisition de connaissances d\u0027adaptation a pour but d\u0027extraire ces connaissances, ce qui peut être réalisé soit directement auprès d\u0027un expert du domaine (d\u0027Aquin et al. (2006)), ou encore par analyse de la base de cas (voir par exemple Hanney et Keane (1996), McSherry (1998), Craw et al. (2006)).\nUn cas est généralement représenté par un couple (pb, Sol(pb)) dans lequel pb repré-sente un énoncé de problème et Sol(pb) une solution de pb. L\u0027ensemble des cas sources (srce, Sol(srce)) d\u0027un système de RÀPC constitue la base de cas BC. Lors d\u0027une session particulière de RÀPC, le problème à résoudre est appelé problème cible, dénoté par cible. Une inférence à partir de cas associe à cible une solution Sol(cible), compte tenu de la base de cas BC et de bases de connaissances additionnelles, en particulier O, l\u0027ontologie du domaine, qui introduit les concepts et les termes utilisés pour représenter les cas.\nLe processus de RÀPC est principalement composé d\u0027une étape de remémoration et d\u0027une étape d\u0027adaptation. La remémoration sélectionne (srce, Sol(srce)) ? BC tel que srce est jugé similaire à cible. Le but de l\u0027étape d\u0027adaptation est ensuite de résoudre cible en modifiant Sol(srce) de façon adéquate. Un problème d\u0027adaptation est donné par un triplet (srce, Sol(srce), cible), et une solution d\u0027un problème d\u0027adaptation est une solution Sol(cible) du problème cible. Une étape de mémorisation d\u0027un cas peut venir compléter le processus.\nLe modèle d\u0027adaptation adopté est une forme d\u0027analogie transformationnelle (Carbonell (1983) \nL\u0027étape d\u0027adaptation est dépendante du domaine d\u0027application car elle nécessite des connaissances spécifiques au domaine. Ces connaissances doivent être acquises 1 . C\u0027est l\u0027objet de l\u0027acquisition de connaissances d\u0027adaptation (ACA).\nDans la section suivante nous rappelons les différentes étapes du processus d\u0027ECBD et détaillons la façon dont celles-ci sont effectuées dans notre système CABAMAKA. Puis, dans la section 3, nous nous intéressons à la définition d\u0027indices de qualité pour classer les règles d\u0027adaptation obtenues. Enfin, dans la section 4, nous montrons comment le système peut être amélioré pour extraire des dépendances qualitatives entre variables.\nCABAMAKA\nCABAMAKA (acronyme de case base mining for adaptation knowledge acquisition) reprend les idées principales présentées dans Hanney et Keane (1996). Dans ces travaux, les variations entre cas sources sont exploités pour apprendre des règles d\u0027adaptation. Tous les couples (cas-source i , cas-source j ) de cas sources similaires dans la base de cas sont formés. Puis, pour chacun de ces couples, les variations entre problèmes srce i et srce j et solutions Sol(srce i ) et Sol(srce j ) sont représentés (?pb et ?sol). Des heuristiques sont ensuite mises en oeuvre pour regrouper ces règles d\u0027adaptation et sélectionner la règle à appliquer lors d\u0027une session de RÀPC. Il a été montré expérimentalement que l\u0027utilisation de telles connaissances d\u0027adaptation augmente la performance du système.\nCABAMAKA se distingue néanmoins des ces travaux sur plusieurs points : -Les connaissances d\u0027adaptation obtenues doivent être validées par un expert et des explications doivent y être associées pour qu\u0027elles soient compréhensibles par l\u0027utilisateur. En ce sens, CABAMAKA peut être considéré comme un système d\u0027apprentissage semiautomatique. -Tous les couples de cas sources distincts de la base de cas sont pris en compte, pas seulement les couples de cas similaires. En conséquence, si n est la taille de la base de cas (n \u003d |BC|) le volume de cas examinés s\u0027élève à n(n ? 1). Dans notre application, n ? 650, ce qui amène à examiner un assez grand nombre de couples (n(n?1) ? 5·10 5 ).\nC\u0027est pourquoi des techniques efficaces d\u0027extraction de connaissances (Dunham (2003)) ont été choisies pour ce système.\nPrincipes\nPrincipes de l\u0027ECBD\nLe but de l\u0027ECBD est d\u0027obtenir des connaissances à partir de données. Le processus d\u0027ECBD se fait sous la supervision d\u0027un analyste, qui est un expert du domaine. Une fois l\u0027acquisition des données réalisée, il se déroule en trois étapes : la préparation des données, la fouille de données et la validation des connaissances extraites.\nLa préparation des données est une étape de mise en forme et de sélection des données. L\u0027opération de mise en forme met les données dans un format acceptable pour l\u0027algorithme de fouille choisi. La sélection des données permet de concentrer la fouille sur un sous-ensemble pertinent d\u0027objets et/ou d\u0027attributs, et d\u0027éliminer les données bruitées.\nLa fouille de données extrait des éléments d\u0027information à partir des données. Par exemple, CHARM (Zaki et Hsiao (2002)) est un algorithme de fouille de données qui réalise efficacement l\u0027extraction de motifs fermés fréquents (MFF). CHARM prend en entrée un ensemble d\u0027objets, chaque objet x étant un ensemble de propriétés booléennes. Un motif m est un ensemble de propriétés et son extension est l\u0027ensemble des objets qui le contiennent. Le support de m, Supp (m), est la proportion d\u0027objets x contenant m (m ? x). Autrement dit, pour une variable aléatoire X parcourant l\u0027ensemble des objets avec une distribution uniforme de probabilités, on a Supp\nLa validation des connaissances extraites se fait avec l\u0027aide de l\u0027analyste, qui interprète les résultats. Cette étape d\u0027interprétation produit des unités de connaissances.\nPréparation des données\nL\u0027étape de préparation des données génère un ensemble d\u0027objets à partir de la base de cas BC, en appliquant successivement deux transformations.\nLa première transformation ? formate chaque cas source (srce, Sol(srce)) en deux ensembles de propriétés booléennes : ?(srce) et ?(Sol(srce)). L\u0027implantation de cette transformation dépend beaucoup du formalisme utilisé pour représenter les cas. Cette transformation entraîne en général une perte d\u0027information, qui doit être minimisée. Le vocabulaire utilisé pour décrire les cas étant celui de l\u0027ontologie du domaine O, si ?(srce) \u003d {p 1 , . . . , p n }, on ajoute à ?(srce) toute propriété q qui peut se déduire de l\u0027ensemble {p 1 , . . . , p n } en fonction de l\u0027ontologie O.\nLa deuxième transformation produit un objet à partir de chaque couple de cas sources (?(cas-source 1 ), ?(cas-source 2 )). Suivant le modèle d\u0027adaptation présenté en introduction, x doit encoder les propriétés de ?pb et de ?sol. ?pb encode les similarités et dissimila-\n-Les propriétés communes à srce 1 et srce 2 (marquées par \"\u003d\"), -Les propriétés de srce 1 que srce 2 ne partage pas (\"-\") et -Les propriétés de srce 2 que srce 1 ne partage pas (\"+\").\nToutes ces propriétés sont reliées à des problèmes et sont marquées par pb. ?sol est calculé de façon similaire et x \u003d ?pb ? ?sol. Par exemple, \nInterprétation\nL\u0027étape d\u0027interprétation est supervisée par un analyste. Le système CABAMAKA fournit à l\u0027analyste les MFF extraits et lui permet de naviguer parmi eux. L\u0027analyste peut sélectionner un MFF, l\u0027interpréter en règle d\u0027adaptation, puis valider, corriger, voire généraliser la règle.\nChaque motif obtenu par CABAMAKA à la suite de l\u0027étape de fouille peut se lire comme une règle d\u0027adaptation qui exprime une relation entre :\n-La présence ou non de certaines propriétés booléennes dans ?(srce), ?(cible) et ?(Sol(srce)), -La présence ou non de certaines propriétés booléennes dans ?(Sol(cible)). Par exemple, le motif m ex correspond à une règle d\u0027adaptation qui peut se lire de la manière suivante :\nsi a est une propriété de srce mais n\u0027est pas une propriété de cible, c est une propriété à la fois de srce et cible, d n\u0027est pas une propriété de srce mais est une propriété de cible, A et B sont des propriétés de Sol(srce) et C n\u0027est pas une propriété de Sol(srce) alors les propriétés de Sol(cible) sont ?(Sol(cible)) \u003d (?(Sol(srce)) \\ {A}) ? {C}.\nApplication\nLe domaine d\u0027application pour lequel cette étude a été réalisée est celui du traitement du cancer du sein. Dans cette application, un problème décrit une classe de patients par un ensemble d\u0027attributs (comme l\u0027âge ou la taille de la tumeur) et de contraintes sur les valeurs prises par ces attributs. Une solution est un ensemble de traitements (radiothérapie, chimiothé-rapie, etc.) recommandés pour ces patients. Ce motif peut être interprété ainsi : si srce et cible représentent tous deux des classes de patients de moins de 70 ans, si la différence entre srce et cible réside dans la taille de la tumeur -moins de 4 cm pour srce et plus de 4 cm pour cible -et si une mastectomie partielle avec curage axillaire est proposée pour srce, alors Sol(cible) est obtenue en remplaçant dans Sol(srce) la mastectomie partielle par une mastectomie totale.\nRésultats\nCette règle traduit le fait que le type d\u0027intervention chirurgicale proposé dépend de la taille de la tumeur du patient : plus la taille de la tumeur est grande, plus on augmente le geste chirurgical.\nL\u0027obtention de telles règles d\u0027adaptation nécessite pour l\u0027instant l\u0027intervention d\u0027un ingé-nieur de la connaissance qui sélectionne les motifs intéressants parmi les résultats, les interprète comme des règles d\u0027adaptation puis les présente à l\u0027analyste pour validation. Deux obstacles subsistent à un pilotage par l\u0027analyste du processus complet d\u0027extraction de connaissances :\n-Les règles obtenues ne sont pas formulées dans un format intelligible par l\u0027analyste : l\u0027analyste n\u0027est pas capable d\u0027interpréter un motif. Pour devenir compréhensibles les règles obtenues doivent être exprimées en langue naturelle. -Les règles obtenues sont trop nombreuses pour être toutes présentées à l\u0027analyste pour validation. La figure 1 présente les résultats expérimentaux -temps d\u0027exécution de CHARM, implanté dans la plateforme CORON (Szathmary et Napoli (2005) \nFIG. 1 -Résultats de l\u0027étape de fouille de données pour une base de cas test de 59 cas.\nA cause du grand nombre de motifs que l\u0027ingénieur de la connaissance doit examiner avant de pouvoir en proposer à l\u0027analyste pour validation, la mise en oeuvre expérimentale pour l\u0027évaluation du système prend également du temps. Il est donc nécessaire de doter l\u0027analyste de moyens de naviguer dans l\u0027ensemble des règles obtenues et de sélectionner les règles inté-ressantes.\nVers la définition d\u0027indices de qualité pour les règles d\u0027adaptation\nUn moyen de sélectionner les règles intéressantes parmi l\u0027ensemble des résultats est de les classer selon un indice de qualité. Cet indice pourra être adapté des indices utilisés pour les règles d\u0027association.\nOn rappelle qu\u0027une règle d\u0027association est la donnée de deux motifs A et B disjoints et dénotée par A ? B. A est appelé l\u0027antécédent de la règle et B le conséquent. Elle traduit, dans un ensemble d\u0027objets, le fait que si le motif A est présent dans un objet, alors il est probable que le motif B soit également présent. Cette probabilité est appelée confiance de A ? B et est la probabilité conditionnelle d\u0027avoir le motif B, sachant qu\u0027on a le motif A :\n(X est une variable aléatoire de distribution uniforme sur l\u0027ensemble des objets). À l\u0027issue de l\u0027étape de fouille de CABAMAKA, un motif m peut être décomposé en deux sous-motifs ?pb et ?sol et lu comme la règle d\u0027association ?pb ? ?sol : ?pb est l\u0027ensemble des propriétés de m indicées par pb et ?sol est m\\?pb. Dans ce cas, la confiance de la règle ?pb ? ?sol vaut :\nNéanmoins, la règle produite par une telle décomposition du motif m ne correspond pas à une règle d\u0027adaptation. En effet, la règle ?pb ? ?sol exprime ce que doit être le couple (Sol(srce), Sol(cible)) étant donné un certain couple (srce, cible). Une règle d\u0027adaptation exprime quant à elle ce que doit être Sol(cible) étant donné un problème d\u0027adaptation (srce, Sol(srce), cible). Le même motif doit donc se lire :\nOr une telle règle ne correspond pas à une décomposition d\u0027un motif en deux sous-motifs, donc les indices définis pour les règles d\u0027association ne s\u0027appliquent pas directement.\nPour définir une mesure de confiance qui soit plus adaptée aux règles d\u0027adaptation, il convient de considérer non seulement l\u0027ensemble des objets constituant l\u0027extension d\u0027un motif, mais également les couples de cas sources que ces objets représentent. Prenons par exemple un objet faisant partie de l\u0027extension d\u0027un motif m et le couple (cas-source 1 , cas-source 2 ) de cas sources représenté par cet objet. Si le motif contient une propriété p \u003d pb , alors le couple de cas sources est tel que les deux problèmes sources partagent la propriété p, soit p ? ?(srce 1 ) ? ?(srce 2 ). Les propriétés constitutives du motif et leur marquage expriment ainsi un ensemble de conditions portant sur la présence ou non de certaines propriétés dans les ensembles ?(srce), ?(Sol(srce)), ?(cible) et ?(Sol(cible)) d\u0027un couple de cas. La\nsol ?(Sol(srce 1 )) ? ?(Sol(srce 2 )) ?(Sol(srce 1 ))\\?(Sol(srce 2 )) ?(Sol(srce 2 ))\\?(Sol(srce 1 ))\nFIG. 2 -Appartenance des propriétés booléennes aux cas sources selon leur marquage dans un motif.\nfigure 2 résume à quel ensemble de propriétés d\u0027un couple de cas sources doit appartenir une propriété suivant la façon dont elle est marquée dans un motif. Une règle d\u0027adaptation décrit alors les propriétés que doit ou non contenir ?(Sol(cible)) compte tenu de la présence ou non de certaines propriétés dans ?(srce), ?(Sol(srce)) et ?(cible). La règle d\u0027adaptation correspondant à un motif m peut être lue de la façon suivante :\n? m}. On peut dès lors définir une mesure de confiance pour les règles d\u0027adaptation, en considérant une variable aléatoire non plus sur l\u0027univers des objets, mais sur l\u0027univers des couples (?(cas-source 1 ), ?(cas-source 2 )) d\u0027images par ? de cas sources distincts (avec une distribution uniforme sur cet ensemble). La confiance d\u0027une règle d\u0027adaptation mesure alors la probabilité conditionnelle que le conséquent d\u0027une règle d\u0027adaptation soit vérifié, sachant que l\u0027antécédent l\u0027est :\nLe classement des règles selon cet indice n\u0027a pour l\u0027instant pas été mis en place. Des travaux sont en cours pour tenter de ramener le calcul de ce nouvel indice à des calculs de supports, ce qui faciliterait son opérationnalisation.\nEn dehors  \nExhiber des dépendances qualitatives entre variables\nLors de l\u0027étape de préparation des données, les cas sources sont décrits par des ensembles de propriétés booléennes et un objet encode les variations de présence et d\u0027absence de ces propriétés lorsqu\u0027on passe d\u0027un cas source à un autre. Lorsque ces propriétés booléennes représentent différentes modalités d\u0027une même variable, comme par exemple l\u0027âge, la taille ou le sexe, il peut être intéressant d\u0027encoder également dans les objets les variations de modalité subies par ces variables. Les motifs obtenus expriment alors des dépendances qualitatives entre variables.\nPrenons par exemple les deux propriétés booléennes âge \u003d 30 et âge \u003d 45, qui correspondent à deux modalités de la variable âge, l\u0027une caractérisant les problèmes pour lesquels la valeur de l\u0027âge est 30 ans et l\u0027autre les problèmes pour lesquels la valeur de l\u0027âge est 45 ans. Soient deux cas sources cas-source 1 et cas-source 2 , tels qu\u0027à l\u0027issue de la première transformation ?(srce 1 ) contient la propriété âge \u003d 30 et ?(srce 2 ) contient la propriété âge \u003d 45. L\u0027objet produit lors de la deuxième transformation pour ces deux cas sources contient donc le motif {(âge \u003d 30)\npb }. Ce motif correspond à une augmentation de la valeur de la variable âge lorsqu\u0027on passe du cas source cas-source 1 au cas source cas-source 2 . Pour encoder également dans cet objet la variation qualitative que subit la variable âge, on peut enrichir l\u0027objet d\u0027une nouvelle propriété âge:varie, qui représente une variation de la variable âge, et d\u0027une propriété âge:augmente, qui représente son sens de variation.\nSi le même objet contient la propriété chimiothérapie:varie, qui représente une variation de dose prescrite pour la chimiothérapie, et la propriété chimiothérapie:diminue, qui représente une diminution de cette dose, un motif obtenu à l\u0027issu de l\u0027étape de fouille peut être le motif {âge:varie, chimiothérapie:varie}. Ce motif exprime une dépendance fonctionnelle entre les variables âge et chimiothérapie. De la même façon, le motif plus spé-cifique {âge:augmente, chimiothérapie:diminue} peut être obtenu. Ce motif exprime la dépendance qualitative âge ? ? ?? chimiothérapie, selon laquelle la dose de chimiothérapie diminue avec l\u0027âge du patient 2 .\nDe telles règles abstraites ont l\u0027avantage d\u0027être plus facilement interprétables par l\u0027analyste que les règles d\u0027adaptation. De plus, chacune d\u0027elle étant partagée par plusieurs motifs, elles constituent un moyen efficace de les regrouper hiérarchiquement.\nConclusion\nDans cet article, nous avons présenté CABAMAKA, un système qui met en oeuvre une technique d\u0027ECBD, l\u0027extraction de motifs fermés fréquents, pour extraire des connaissances d\u0027adaptation à partir des variations qui existent au sein d\u0027une base de cas. Ce système est assez unique en son genre car il extrait des connaissances à partir de connaissances.\nNous avons montré en quoi les indices de qualité existant pour les règles d\u0027association sont inadaptés pour mesurer la qualité d\u0027une règle d\u0027adaptation obtenue par ce système et comment l\u0027on peut s\u0027en inspirer pour créer des indices plus appropriés. Des travaux sont actuellement en cours sur la définition de tels indices.\nNous avons également proposé une amélioration du système de façon à découvrir des règles plus abstraites qui expriment des dépendances qualitatives entre les variables entrant en jeu dans la description des cas. La validation de telles dépendances doit être plus facile car ces règles sont moins nombreuses et plus intelligibles par l\u0027analyste. Cette méthode est actuellement en cours d\u0027implantation.\nPar ailleurs, les connaissances d\u0027adaptation obtenues par CABAMAKA ont vocation à venir alimenter un portail sémantique (d\u0027Aquin (2005)) développé dans le cadre du projet KASIMIR (Lieber et al. (2002)), dont l\u0027objet est la gestion de connaissances et l\u0027aide à la décision en cancérologie. En particulier, les travaux actuels portent sur l\u0027intégration de ces connaissances au moteur de raisonnement à partir de cas de KASIMIR.\n"
  },
  {
    "id": "937",
    "text": "Classification des pages\nL\u0027objectif de cette phase est d\u0027identifier les principaux types de pages composant le site analysé. Un type de pages est un ensemble de pages relativement similaires tant sur le plan syntaxique (code HTML) que sémantique (concept représenté par la page).\nPour atteindre cet objectif, un taux de similarité est calculé entre les pages du site sur la base d\u0027un ensemble de critères tels que ceux décrits dans Ricca et Tonella (2003).\nAnalyse sémantique des pages\nLors de cette étape, l\u0027utilisateur définit les composants qu\u0027il souhaite extraire à partir d\u0027un échantillon représentatif de pages d\u0027un même type. Un composant est un concept présent au sein des pages d\u0027un même type. Il peut être absent de certaines pages et/ou y apparaître plusieurs fois. De plus, on lui associe une indication de format (i.e. texte simple ou balisé) et de localisation. Dans Retroweb, cette dernière propriété est exprimée sous la forme d\u0027un chemin (XPath) dans l\u0027arborescence formée par les balises HTML.\nLa figure 1 illustre le scénario de construction d\u0027une règle d\u0027extraction.\n(1) L\u0027utilisateur sélectionne une instance du composant à définir et lui assigne un nom représentatif tandis que l\u0027outil calcule son chemin d\u0027accès XPath. (2) La règle est appliquée à chacune des pages de l\u0027échantillon afin d\u0027en vérifier la validité. (3) Si la valeur attendue pour chacune des pages n\u0027a pu être extraite, la règle doit être raffinée. Pour ce faire plusieurs solutions sont proposées :\n(1) Construction de la règle \nExtraction des données et de leur schéma\nLe module d\u0027extraction des données applique un ensemble de règles d\u0027extraction à un ensemble de pages afin d\u0027en extraire les instances de composants ainsi que leur structure (XML).\nConclusion\nRetroweb est un outil d\u0027extraction de données ciblées à partir de sources Internet. Ses avantages principaux sont sa facilité d\u0027utilisation et la possibilité de se concentrer uniquement sur les types de données utiles pour un usage spécifique.\nRéférences\nEstiévenart, F., J.-R. Meurisse, J.-L. Hainaut, et P. Thiran (2006 \nSummary\nThe Retroweb tool is dedicated to the extraction of web data. The proposed approach is user-oriented and semi-automated, since it requires minimal user input in order to focus only on those pieces of information that are of particular interest to them.\n"
  },
  {
    "id": "938",
    "text": "Introduction\nLes motifs séquentiels sont étudiés depuis plus de dix ans (Agrawal et Srikant (1995)), ils permettent de mettre en exergue des corrélations entre événements suivant leur chronologie d\u0027apparition. Les motifs séquentiels ont été récemment étendus dans un contexte multidimensionnel par Pinto et al. (2001), Plantevit et al. (2005) et Yu et Chen (2005). Ils permettent ainsi de découvrir des motifs définis sur plusieurs dimensions et ordonnés par une relation d\u0027ordre (e.g. temporelle). Par exemple, dans Plantevit et al. (2005), des motifs de la forme \"La plupart des consommateurs achètent une planche de surf et un sac à N.Y., puis ensuite une combinaison à SF\" sont découverts. Les motifs séquentiels multidimensionnels sont bien adaptés aux contextes de stockage et de gestion des données actuels (entrepôts de données). En effet, les motifs ou règles obtenus permettent une autre appréhension des données sources. Cependant leur découverte nécessite certains paramètres dont en particulier le support minimal. Celui-ci correspond à la fréquence minimale d\u0027apparition des motifs au sein de la base considérée. Si le support minimal choisi est trop élevé, le nombre de règles découvertes est faible mais si le support est trop bas, le nombre de règles obtenues est très important et rend difficile l\u0027analyse de celles-ci. Un autre problème est la longueur des motifs extraits. Comment ajuster au mieux le support afin d\u0027obtenir des séquences suffisamment longues pour être réellement utilisables ? L\u0027utilisateur est alors confronté au problème suivant : comment baisser le support minimal sans générer la découverte de règles non pertinentes ? Ou comment augmenter le support minimal sans perdre les règles utiles ? Est-il alors nécessaire de faire un compromis entre qualité des connaissances extraites et support ?\nL\u0027utilisation des hiérarchies dans l\u0027extraction de connaissances représente un excellent moyen de résoudre ce dilemme. Elle permet de découvrir des règles au sein de plusieurs niveaux de hiérarchies. Ainsi, même si un support élevé est utilisé, les connaissances importantes dont le support est faible dans les données sources peuvent être \"subsumées\" par des connaissances plus générales qui, elles, seront comptabilisées comme fréquentes.\nLa prise en compte des hiérarchies dans l\u0027extraction de motifs séquentiels multidimensionnels a été proposée par Plantevit et al. (2006b)  Dans la suite de cet article, nous décrivons les différentes propositions prenant en compte les hiérarchies dans un contexte multidimensionnel. Après avoir rappelé les concepts associés aux motifs séquentiels multidimensionnels, notre contribution est détaillée en définissant les concepts de séquences convergentes et divergentes. Nous décrivons ensuite les algorithmes et les fonctions permettant l\u0027extraction de telles séquences. Des expérimentations, sur des jeux de données synthétiques et réelles, montrent l\u0027intérêt de notre approche M 2S_CD aussi bien en terme de robustesse des algorithmes que de pertinence des motifs extraits.\nTravaux antérieurs : motifs multidimensionnels et hiérar-chies\nCombiner plusieurs dimensions d\u0027analyse permet d\u0027extraire des connaissances qui dé-crivent mieux les données. Dans Pinto et al. (2001), les auteurs sont les premiers à rechercher des motifs séquentiels multidimensionnels. Ainsi les achats ne sont plus simplement décrits en fonction des seuls date et identifiant du client comme dans contexte classique, mais en fonction d\u0027un ensemble de dimensions telles que Type de consommateur, Ville, Age. Cette approche ne permet que l\u0027extraction de séquences définies sur une seule dimension (e.g. produit) caractéri-sées par une motif multidimensionnel. Ainsi, il est impossible d\u0027extraire des combinaisons de motifs multidimensionnels suivant le temps.\nL\u0027approche proposée par Yu et Chen (2005) est très singulière puisqu\u0027il existe un très fort lien hiérarchique entre les dimensions d\u0027analyse. Les pages web sont visitées durant une ses-sion au cours d\u0027une journée. Cette approche multidimensionnelle permet donc une gestion plus fine du temps mais ne permet pas de se situer réellement dans un contexte multidimensionnel.\nDans Plantevit et al. (2005), les règles extraites ne combinent pas seulement plusieurs dimensions d\u0027analyse. Ces dimensions sont également combinées au cours du temps. Par exemple, dans la règle \"Les ventes de pepsi augmentent à NY puis les ventes de coca augmentent à LA\", N Y apparaît avant LA et pepsi avant coca.\nIl existe très peu de travaux conciliant hiérarchies et multidimensionnalité lors de l\u0027extraction de motifs séquentiels. Les travaux de Yu et Chen (2005) permettent une représentation plus fine du temps, mais ne répondent pas à notre problématique générale d\u0027un nombre quelconque de dimensions. Seule l\u0027approche HYPE, (Plantevit et al. (2006a), Plantevit et al. (2006b) Mais cette proposition ne permet pas d\u0027extraire des séquences où des items de même dimension mais de granularité différente cohabitent tels que (N ice, Coca) et (F rance, Soda). En effet, pour assurer un passage à l\u0027échelle dans un contexte d\u0027explosion du nombre de motifs possibles, le choix de ne conserver que les items maximalement spécifiques a été fait.\nA notre connaissance, il n\u0027existe donc aucune approche proposant de prendre en compte les hiérarchies dans un contexte multidimensionnel tel qu\u0027il existe des items comparables dans les séquences extraites. Nous proposons donc les nouveaux concepts de séquences multidimensionnelles convergentes et divergentes afin de permettre une extraction de connaissances plus complète et adaptée aux spécificités des contextes multidimensionnels. Nous dirigeons ainsi la génération des motifs soit du général au particulier soit du particulier au général afin de limiter le nombre de motifs candidats mais nous ouvrons ainsi la voie à des motifs composés de séquences plus longues.\nM2S_CD : motifs séquentiels multidimensionnels convergents ou divergents\nDans cette section, nous introduisons un concept original. En effet, l\u0027esprit humain raisonne souvent de deux façons différentes et symétriques. La réflexion s\u0027exécute de l\u0027exemple vers la théorie ou de la théorie vers l\u0027exemple. Nous essayons donc de reproduire ce type de raisonnement dans les connaissances que nous souhaitons extraire. Nous introduisons donc le concept de séquence multidimensionnelle convergente ou divergente. Nous présentons les différentes définitions préliminaires associées aux motifs séquentiels multidimensionnels avec prise en compte des hiérarchies pour ensuite détailler les concepts de motifs convergents et divergents ainsi que les algorithmes associés.\nBase Exemple\nPour illustrer les différents concepts et définitions, nous proposons la base exemple du tableau Tab. 1 qui décrit les ventes réalisées dans différentes villes du monde par différentes  \nDéfinitions préliminaires\nSoit une base de données DB où les données sont définies suivant n dimensions, nous considérons une tri-partition de l\u0027ensemble des dimensions :\n-L\u0027ensemble des dimensions sur lesquelles sont extraites les règles \nAinsi, la séquence in, N ice)}{(P errier, N ice)} est une sous-séquence de la séquence in, * ), ( * , M oscou)}{( * , N ice)(P errier, N ˆ imes)} Nous considérons que chaque bloc défini sur D R contient une séquence de données multidimensionnelles qui est identifiée par ce bloc. Un bloc supporte une séquence ? si ? est une sous-séquence de la séquence de données identifiée par ce bloc. Le support d\u0027une séquence multidimensionnelle correspond donc au nombre de blocs définis sur D R qui contiennent cette séquence.\nDans le contexte dans lequel nous nous situons, nous considérons qu\u0027il existe des relations hiérarchiques sur chaque dimension d\u0027analyse matérialisées sous la forme d\u0027arbres. Une hiérarchie est donc représentée par un arbre orienté dans lequel les arcs sont de type isa. La relation de généralisation/spécialisation s\u0027effectue ainsi de la racine vers les feuilles. Chaque dimension d\u0027analyse possède donc une hiérarchie qui permet de représenter les relations entre les éléments de son domaine. Soit T DA \u003d {T 1 , . . . , T m } l\u0027ensemble des hiérar-chies associées aux dimensions d\u0027analyse où : (i) T i est la hiérarchie représentant les relations entre les éléments de la dimension d\u0027analyse\nOn notê x un ancêtre de x dans la hiérarchie et?xet? et?x un de ses descendants. Par exemple, boisson \u003d soda signifie que boisson est un ancêtre de soda dans la relation Généralisation/Spécialisation. Plus précisément, boisson est une instance plus générale que soda. Seuls les éléments qui sont feuilles dans l\u0027arbre adéquat sont présents dans la base de données. Plantevit et al. (2006b) proposent une définition des concepts d\u0027item, itemset et séquence multidimensionnels h-généralisés. Ainsi un item multidimensionnel h-généralisé e \u003d (d 1 , . . .-, d m ) est un m-uplet défini sur les dimensions d\u0027analyse D A tel que d i ? {label(T i )} (d i existe dans la hiérarchie adéquate). Contrairement aux données de DB, un item multidimensionnel h-généralisé peut être défini avec n\u0027importe quelle valeur d i dont le noeud associé dans l\u0027arbre hiérarchique n\u0027est pas nécessairement une feuille.\nPuisque les items multidimensionnels h-généralisés peuvent être définis sur différents niveaux de hiérarchies, il est nécessaire de définir une relation hiérarchique entre ces items. \nDéfinition 2 (Inclusion hiérarchique) Soient deux items multidimensionnels h-généralisés\nEn d\u0027autres mots, pour tout item de la séquence, il n\u0027existe pas un item plus général déjà présent à une date antérieure. La séquence M ontpellier)}, {(Coca, LR)(P epsi, P ACA)}, {(Soda, F rance), (Soda, Allemagne)} est une séquence divergente.\nDéfinition 4 (Séquence convergente) Une séquence ? \u003d 11 , . . . , e ij , . . . , e nk est conver-\nEn d\u0027autres mots, pour chaque item de la séquence, il n\u0027existe pas d\u0027item plus spécifique déjà présent dans la séquence à une date antérieure. La séquence Eurasie)}, {(Soda, Europe)(B.A, Asie)}, {(Coca, F rance)(P epsi, Russie)} est une séquence convergente.\nMise en oeuvre\nOrdre dans les séquences\nOrdonner les séquences est une étape fondamentale afin d\u0027améliorer l\u0027implémentation et éviter les cas déjà examinés. Les méthodes existantes, basées sur les différentes philosophies (pattern growth (Pei et al. (2004)), générer/élaguer (Agrawal et Srikant (1995)  (2002))), ne sont pas directement applicables dans un contexte multidimensionnel. En effet, les items h-généralisés ne sont pas explicités dans la base de données. De tels items sont extraits par inférence puisqu\u0027ils ne sont pas directement associés à un n-uplet dans la base de données.\nMunich)(Coca, N ice)} N ice)(P epsi, Munich)}\nTAB. 2 -Contre-exemple\nLe tableau Tab. 2 montre un exemple de séquences de données qui ne peut pas être traité avec les approches existantes dans un contexte classique puisque les items h-généralisés ne sont pas \"explicitement\" présents dans la base. En effet, aucun ordre lexical total, prenant en compte les items h-généralisés, ne peut être directement utilisé. Ainsi, ces méthodes ne peuvent pas extraire la séquence N ice)(Soda, M unich)} (où Soda est un ancêtre de coca et pepsi). En effet, les méthodes basées sur le paradigme pattern growth trouvent l\u0027item (Coca, N ice) avec un support de 2. Ensuite, elles construisent la base projetée pré-fixée par la séquence N ice)} Cette base projetée contient les séquences et epsi, M unich)} L\u0027item h-généralisé (Soda, M unich) n\u0027apparaît pas dans cette base projetée alors qu\u0027il est fréquent dans la base initiale. Dans les approches de type générer-élaguer, le problème est similaire. Par exemple, dans Masseglia et al. (1998), la projection de la base de données dans l\u0027arbre préfixé des séquences candidates est biaisée.\nIl est impossible d\u0027étendre l\u0027ensemble de la base avec tous les item h-généralisés possibles avant le processus d\u0027extraction. Par exemple, considérons une base de données contenant m dimensions d\u0027analyse et n i items (feuilles) dans un itemset i, la profondeur moyenne des hié-rarchies est d. La transformation d\u0027un itemset va produire d m ×n i items au lieu des n i initiaux, multipliant donc la taille de la base initiale par d m . Il est donc nécessaire de prendre en compte les items h-généralisés durant le processus d\u0027extraction et non après un pré-traitement. Nous allons donc introduire un ordre lexical et matérialiser localement les items h-généralisés.\nDéfinitions\nIl est primordial de disposer d\u0027un ordre lexicographique lors de l\u0027extraction de motifs fré-quents puisque c\u0027est la clef de la non-duplication des items durant le processus.\nOn dit qu\u0027un itemset est étendu s\u0027il est égal à sa fermeture transitive par rapport à la relation de spécialisation (\u003c h ). La notion d\u0027itemset étendu permet de prendre en compte tous les items h-généralisés qui peuvent être inférés à partir d\u0027une séquence de données. Afin d\u0027optimiser le traitement des données, nous introduisons un ordre lexicographico-spécifique (lgs), qui est un ordre alpha-numérique selon le degré de précision d\u0027un item. Ainsi, les items les plus spécifiques sont prioritaires. Nous devons définir une fonction LGS-Closure qui transforme un itemset (transaction) en un itemset étendu contenant tous les items h-généralisés. L\u0027extraction des items fréquents peut donc être effectuée sur chaque itemset étendu. Dans les approches pattern growth, les séquences sont extraites en ajoutant un item fréquent à une sé-quence fréquente de manière gloutonne. Il est nécessaire de définir un moyen efficace d\u0027étendre les séquences à partir du dernier itemset de la séquence. Dans ce but, nous définissons une restriction de la fonction LGS-Closure de la façon suivante :\nDéfinition 5 (Fonction LGS-Closure) La fonction LGS-Closure est une application d\u0027un itemset i vers la fermeture de i avec l\u0027ordre LGS (\u003c lgs\nAlgorithmes\nLes séquences divergentes sont extraites en utilisant l\u0027algorithme M 2S_CD (Algorithme 1) suivant une exploration gloutonne en profondeur (paradigme pattern growth). Au lieu de parcourir l\u0027intégralité de la base de données, niveau par niveau, comme le font les méthodes de type générer-élaguer, la base de données est projetée en fonction de la séquence actuellement explorée. La projection est différente de celle proposée par Pei et al. (2004). En effet, comme nous devons gérer les items h-généralisés, la projection doit prendre en compte la transaction (itemset) où l\u0027item a été trouvé, et pas seulement l\u0027item lui même comme dans Pei et al. (2004). Pour prendre en compte cette transaction, nous utilisons la fonction LGS-Closure en filtrant les items déjà trouvés.\nL\u0027utilisation des bases projetées permet d\u0027éviter des passes inutiles sur des données déjà parcourues. En effet, considérons une séquence fréquente ? et la séquence actuellement explorée ? tel que ? ? ? or ? ? ?, si ces deux séquences partagent la même base projetée alors il est inutile de continuer l\u0027exploration de la séquence ?. Nous avons seulement besoin de copier le sous-arbre (déjà extrait) de la séquence ? à la séquence ?.\nL\u0027algorithme 3 permet l\u0027extraction des items localement fréquents sur la base projetée. Il est basé sur la fonction LGS-Closure. La base projetée est parcourue une seule fois pour extraire tous les items fréquents. Deux types d\u0027items peuvent être extraits :\n1. Les items qui ne peuvent pas être inclus dans le dernier itemset de la séquence courante ?. Ces items sont donc inclus dans un nouvel itemset de ?. Pour extraire ces items et prendre en compte les items h-généralisés, nous devons étendre les transactions de la base projetée (pas à pas) avec la fonction LGS-CLosure.\n2. Les items qui peuvent être inclus dans le dernier itemset de la séquence courante ?. Dans ce cas, nous utilisons la fonction LGS-Closure X où X représente le dernier itemset de ?.\nAlgorithme 1: M 2S_CD Data : Base de données DB, support minimum minsup Result : Ensemble des séquences divergentes L begin /*-Initialisation -*/ Set L ? {}; Sequence ? ? /*-Extraction des séquences en profondeur-*/ SequenceGrowing(?, DB, L, minsup); return L; end Ces différents algorithmes permettent l\u0027extraction de séquences divergentes. Pour extraire des séquences convergentes, il est nécessaire d\u0027utiliser les mêmes algorithmes mais sur une base de données inversée. En effet, il suffit d\u0027inverser la relation d\u0027ordre (commencer par la fin) au sein de la séquence de données pour permettre un résultat du général au particulier.\nExpérimentations\nDans cette section, nous reportons les expérimentations effectuées sur des jeux de données synthétiques et réels.\nAlgorithme 2: SequenceGrowing : Algorithme d\u0027extraction\nData : Séquence ?,base projetée DB|?, ensemble des fréquents L , support minimum minsup Result : L\u0027ensemble des séquences divergentes fréquentes et préfixées par ? begin insérer(?, L); /*-Vérifier si la séquence a déjà été parcourue-*/ if ?? | (? ? ? or ? ? ?) ? ? et ? partagent la même base projetée then Copie des descendants de ? dans ?; return Set F l ? getF requentItems(DB|?, minsup); foreach itemset is in other do /*-Recherche des items qui peuvent être insérés dans un nouvel itemset de ?-*/ SearchOtherTransFrequentItem e in LGS-Closure(is); /*-Recherche des items qui peuvent être insérés dans le dernier itemset de ?-*/ if is supports lastItemset(?) then SearchSameTransFrequentItem _e in LGS-Closure lastItemset(?) (is);\nreturn (F l \u003d {e|support(e) ? suppmin ? e est maximalement spécif ique}) end\nDonnées synthétiques\nLes expérimentations ont été effectuées sur une base de données synthétiques composée de 10, 000 n-uplets définies sur 5 dimensions d\u0027analyse. Des hiérarchies sont définies sur les dimensions d\u0027analyse. Les expérimentations reportent le nombre de fréquents obtenus et le temps d\u0027exécution en fonction du support, du nombre de dimensions d\u0027analyse, des spécificités des hiérarchies (degré et profondeur). Les figures 2(c) et 2(d) montrent le nombre de fréquents extraits et le temps d\u0027exécution en fonction de la profondeur des hiérarchies pour un seuil de support fixé. Etendre la hiérarchie d\u0027un niveau engendre une spécialisation supplémentaire des données (Soda devient pepsi ou coca). Il y a ainsi plus de valeurs différentes dans la base de données. M 2S_CD apporte une certaine robustesse face à ce phénomène de spécialisation. En effet, même si les données deviennent très détaillées (5 niveaux dans la hiérarchie), notre approche permet d\u0027extraire des séquences définies sur plusieurs niveaux de hiérarchies. On remarque cependant que le temps de traitement est plus long quand le nombre de niveaux augmente. Ceci est dû au nombre d\u0027items h-généralisés potentiellement fréquents qui augmente.\nLes figures 2(e) et 2(f) montrent le nombre de séquences extraites et le temps d\u0027exécution en fonction du degré des hiérarchies. Augmenter le degré d\u0027une hiérarchie équivaut à spécia-liser les données (ajout de fils à une instance). Notre approche permet de continuer à extraire des connaissances lorsque la hiérarchie se spécialise. Le temps de traitement devient cependant plus coûteux.\nLes figures 2(g) et 2(h) montrent le nombre de séquences extraites et le temps d\u0027exécution en fonction du nombre de dimension d\u0027analyse. Augmenter le nombre de dimensions d\u0027analyse engendre une augmentation du nombre de fréquents et du coût de leur extraction.\nCes expérimentations menées sur des données synthétiques montrent la robustesse de M2S_-CD pour l\u0027extraction des connaissances face à la diversité des données (nombre de dimensions, degré et profondeur des hiérarchies, etc.). Diversifier les données sources engendre un coût de traitement plus important qui reste cependant acceptable.\nDonnées réelles\nNous avons étudié plusieurs parties du jeu Eleusis. Eleusis est un jeu de cartes dont le but est de trouver une règle secrète. Les règles secrètes sont des séquences de cartes contenant une partie droite et une partie gauche. Chaque partie peut contenir plusieurs cartes. Ce jeu permet de simuler la découverte scientifique qui est formée de tests, publications et réfutations. Nous avons donc analysé différentes parties du jeu développé par Dartnell et Sallantin (2005 \nConclusion\nDans cet article, nous proposons une méthode originale pour extraire des connaissances multidimensionnelles définies sur plusieurs niveaux de hiérarchies mais selon un certain point de vue : du général au particulier ou vice et versa. Nous définissons ainsi le concept de sé-quences multidimensionnelles convergentes ou divergentes ainsi que les algorithmes associés basés sur le paradigme \"pattern growth\". Des expérimentations, sur des jeux de données synthétiques et réelles, montrent l\u0027intérêt de notre approche M 2S_CD.\nCe travail offre de nombreuses perspectives. L\u0027efficacité de l\u0027extraction peut être amélio-rée en s\u0027appuyant sur des représentations condensées des connaissances extraites (clos, libres).\n"
  },
  {
    "id": "939",
    "text": "Introduction\nL\u0027extraction de motifs contraints est un champ significatif de l\u0027Extraction de Connaissances dans les Bases de Données, notamment pour dériver des règles d\u0027association. L\u0027intérêt des motifs extraits est garanti par le point de vue de l\u0027analyste exprimé à travers la sémantique de la contrainte. Par ailleurs, la complétude de l\u0027extraction assure qu\u0027aucun motif jugé pertinent par l\u0027utilisateur ne sera manqué. La contrainte la plus populaire est certainement celle de fréquence minimale (Agrawal et al., 1993) qui permet de rechercher des régularités au sein d\u0027une base de données. Malheureusement, le nombre de motifs fréquents est souvent prohibitif. Les motifs les plus pertinents sont alors noyés au milieu d\u0027informations triviales ou redondantes que même d\u0027autres contraintes d\u0027agrégats (Ng et al., 1998) n\u0027arrivent pas davantage à isoler.\nDans ces conditions, plusieurs approches proposent de comparer les motifs entre eux pour ne sélectionner que les meilleurs (Fu et al., 2000) ou une couverture (Mannila et Toivonen, 1997;Pasquier et al., 1999). De tels motifs révèlent alors une structure globale au sein des données. Le critère d\u0027appartenance ou non à cette structure s\u0027apparente à une contrainte globale. L\u0027extraction de motifs satisfaisant une contrainte globale présente donc une finalité importante pour les utilisateurs. Cependant, leur extraction s\u0027avère souvent ardue car leur localisation dans l\u0027espace de recherche est loin d\u0027être triviale. En particulier, trouver les k motifs maximisant une mesure d\u0027agrégat (e.g., la fréquence (Fu et al., 2000)) devient problématique dès que la mesure ne satisfait aucune propriété particulière comme l\u0027anti-monotonie.\nDans cet article, nous proposons d\u0027extraire des motifs satisfaisant une contrainte globale. Notre première contribution est une méthode générale d\u0027extraction appelée Approximer-etPousser. L\u0027idée fondamentale est de déduire une contrainte locale qui est affinée au cours de l\u0027extraction. Cette contrainte locale évolutive, exploitée par un algorithme indépendant, réduit alors l\u0027espace de recherche. Ensuite, nous appliquons cette méthode pour rechercher les top-k motifs selon une mesure d\u0027intérêt spécifiée par l\u0027utilisateur. Nous expliquons comment constituer une approximation des top-k motifs à extraire. Puis, nous montrons que cette approximation peut être poussée pour réduire considérablement l\u0027espace de recherche en se fondant sur une méthode de relaxation exposée dans (Soulet et Crémilleux, 2005). L\u0027une des originalités de notre approche est d\u0027autoriser des mesures sans bonne propriété de monotonie et ainsi, de ne pas se cantonner aux seuls top-k motifs fréquents.\nCet article est organisé de la manière suivante. Dans le contexte des contraintes globales, la section 2 définit la notion de top-k motifs selon une mesure et en présente la problématique de l\u0027extraction. La section 3 décrit l\u0027approche Approximer-et-Pousser dédiée à l\u0027extraction des contraintes globales. La section 4 applique cette méthode pour la recherche des top-k motifs selon une mesure en détaillant les deux étapes. Enfin, cette approche est évaluée à la section 5. \nContexte et travaux relatifs\nContexte et définitions\nD\nTrans.\nItems\nL\u0027extraction de motifs cherche la collection de tous les motifs de L I satisfaisant un pré-dicat q, appelé contrainte, et présents dans le contexte transactionnel D. Un motif X est pré-sent dans D s\u0027il apparaît dans au moins une de ses transactions. Introduite dans (Agrawal et Srikant, 1994), l\u0027une des contraintes les plus utilisées est celle de fréquence minimale. La fré-quence d\u0027un motif X, dénotée par freq(X), donne le nombre de transactions contenant X. La contrainte de fréquence minimale (i.e., freq(X) ? ?) sélectionne les motifs dont la fréquence excède un seuil ? fixé par l\u0027utilisateur. De nombreuses contraintes remplacent la fréquence par une autre mesure d\u0027intérêt pour juger au mieux la pertinence d\u0027un motif (Ng et al., 1998). Parmi ces mesures, l\u0027aire d\u0027un motif X, notée area(X), correspond au produit de sa fréquence par sa longueur (i.e., freq(X) × count(X) où count(X) dénote la cardinalité de X). La vé-rification de certaines mesures d\u0027intérêts m(X) ? ? où m : L I ? nécessite de complèter D avec des informations supplémentaires (e.g., une table associant des valeurs à chaque item pour sum(X.val)).\nCes mesures d\u0027intérêt ne suffisent pas toujours à focaliser directement sur les motifs les plus significatifs. Il s\u0027avère alors nécessaire de comparer les motifs entre eux pour n\u0027en conserver que les meilleurs ou une couverture. De tels motifs révèlent alors une structure globale au sein des données. L\u0027appartenance ou non à cette structure se formalise avec la notion de contrainte globale que nous définissons maintenant :\nDéfinition 1 (Contrainte globale) Une contrainte est globale si sa vérification nécessite de comparer plusieurs motifs entre eux.\nTypiquement, les contraintes \"être-maximal\" (Mannila et Toivonen, 1997), \"être-fermé\" (Pasquier et al., 1999) ou \"être-libre\" (Boulicaut et al., 2003) qui dégagent une représentation des motifs, sont des contraintes globales. Par exemple, la contrainte \"être-fermé\" extrait une couverture de la base de données en sélectionnant uniquement les motifs X dont toutes les spécialisations Y ? X ont une fréquence strictement inférieure à celle de X. La vérification de \"être-fermé\" nécessite donc la comparaison de X avec d\u0027autres motifs. De même, tester si X est un motif fermé en le comparant avec sa fermeture est une comparaison entre deux motifs. Les contraintes globales mettent intrinsèquement en relation plusieurs motifs contrairement aux contraintes usuelles, dites locales, qui peuvent se vérifier isolément sur chacun des motifs.\nDans cet article, nous nous intéressons plus particulièrement à la contrainte globale correspondant aux top-k motifs selon une mesure d\u0027intérêt. Le choix du seuil pour la contrainte de fréquence ou d\u0027aire minimale (et plus généralement de m(X) ? ?) se révèle souvent difficile pour l\u0027utilisateur. En effet, si ce seuil est trop élevé, trop peu de motifs sont extraits (au risque de n\u0027obtenir que des informations triviales). A l\u0027inverse, si ? est trop bas, le nombre de motifs explose et les motifs les plus intéressants sont noyés dans la masse. Comme plusieurs tentatives d\u0027extraction sont nécessaires pour estimer ?, l\u0027utilisateur préfère souvent fixer ce dernier relativement bas rendant parfois les extractions infaisables. Puis, parmi tous les motifs obtenus, il focalise son intérêt sur les premiers motifs maximisant sa mesure d\u0027intérêt. \nLa contrainte top k,m est clairement globale puisque X et Y sont présents conjointement dans la définition. Cette contrainte compare les motifs entre eux pour conserver ceux dont la mesure fait partie des k meilleures. Par exemple, les 3 motifs de plus grande aire correspondent exactement aux motifs satisfaisants top 3,area 1 : AB (3 × 2 \u003d 6), AC (3 × 2 \u003d 6) et ABC (2 × 3 \u003d 6). Les motifs associés à la contrainte top k,m sont nommés les top-k motifs selon la mesure m. En fait, leur nombre est parfois supérieur à k (tous les motifs au-delà du k ème ont alors la même mesure). Typiquement les top-3 motifs fréquents sont 4 à savoir A (5), C (4), B (3) et E (3), car la fréquence ne permet pas de distinguer les motifs B et E. Notons que les k motifs minimisant une mesure m satisfont la contrainte top k,?m . Naïvement l\u0027extraction des top-k motifs peut s\u0027effectuer avec un post-traitement. Après l\u0027extraction de tous les motifs dont la mesure m excède un seuil ?, il suffit de sélectionner les k motifs maximisant m. Outre l\u0027inefficacité algorithmique, la difficulté du choix du seuil minimal persiste. Si celui-ci est fixé trop haut, moins de k motifs peuvent être extraits. En revanche, si ce seuil est trop bas, des motifs inutiles sont extraits et ce processus ne profitant pas du paramètre k devient très lent voire infaisable. Pour résoudre ce problème, il est préférable de pousser la contrainte top k,m au sein de l\u0027extraction de motifs. Cette tâche est peu aisée car se pose une double problématique à travers la localisation des motifs dont la mesure est potentiellement élevée et la comparaison de ces mesures pour garantir la maximalité des mesures correspondant aux motifs finalement retenus. En fait, cette contrainte recouvre des problèmes inhérents à la vérification des contraintes globales.\nTravaux relatifs\nÀ notre connaissance, aucune méthode générale d\u0027extraction des contraintes globales n\u0027a été proposée dans la littérature auparavant. Individuellement certaines contraintes globales comme \"être-maximal\" (Mannila et Toivonen, 1997), \"être-fermé\" (Pasquier et al., 1999) ou \"être-libre\" (Boulicaut et al., 2003) ont des algorithmes spécifiques reposant principalement sur des élagages anti-monotones. Par exemple, la contrainte de liberté est anti-monotone : si un motif n\u0027est pas libre aucune de ses spécialisations ne sera libre et on peut alors élaguer cette partie de l\u0027espace de recherche. L\u0027extraction des top-k motifs fréquents a été introduite dans (Fu et al., 2000). Les auteurs adaptent APRIORI (Agrawal et Srikant, 1994) pour ajuster le seuil de fréquence minimale au fur et à mesure de l\u0027extraction en bénéficiant à nouveau de l\u0027anti-monotonie de la fréquence. Dans (Hirate et al., 2004), la structure FP-tree permet d\u0027optimiser l\u0027extraction des top-k motifs fréquents. Plus récemment, la structure COFI-tree a aussi été utilisée (Ngan et al., 2005). Dans (Tzvetkov et al., 2003), les auteurs remplacent le langage L I par celui des motifs séquentiels pour extraire les top-k séquences fréquentes. Plusieurs travaux extraient aussi les top-k motifs fréquents satisfaisant un critère additionnel. Par exemple, les k motifs fermés les plus fréquents et de longueur minimale sont recherchés dans  en utilisant la structure FP-tree. D\u0027autres recherchent les motifs les plus fréquents, fermés ou non, et de longueur minimale (Cong, 2001). Tous ces travaux sont restreints à la mesure de fréquence comme mesure d\u0027intérêt car la contrainte de fréquence minimale est anti-monotone (i.e., les motifs satisfaisant top k,freq sont les plus généraux). Remarquons aussi qu\u0027ils se focalisent principalement sur les motifs d\u0027items.\nNotre démarche se distingue donc en proposant une méthode adaptée à n\u0027importe quelle mesure d\u0027intérêt basée sur les primitives. Même si cet article est dédié aux motifs d\u0027items, l\u0027approche Approximer-et-Pousser est adaptable à d\u0027autres langages (séquences, arbres, etc).\nPar ailleurs, l\u0027approche Approximer-et-Pousser constitue une première proposition générique pour l\u0027extraction des contraintes globales.\n3 Approximer-et-Pousser : une approche générique d\u0027extraction pour les contraintes globales L\u0027approche Approximer-et-Pousser est une méthode générique d\u0027extraction des motifs satisfaisant une contrainte globale. Brièvement, l\u0027idée est de restreindre l\u0027espace de recherche lors du parcours en affinant la localisation des motifs susceptibles de vérifier la contrainte globale. Pour cela, cette approche s\u0027appuie sur la répétition de deux étapes majeures (et qui forment son nom) : (1) approximer la collection finale à extraire, (2) pousser des informations issues de cette approximation pour diminuer l\u0027espace de recherche. Plutôt que d\u0027algorithme Approximer-et-Pousser, nous préférons parler d\u0027approche Approximer-et-Pousser car par la suite, cette approche délègue l\u0027élagage de l\u0027espace de recherche à un algorithme indépendant. La condition d\u0027élagage lui est donnée sous forme d\u0027une contrainte locale d\u0027extraction qui est dynamiquement affinée à chaque itération. Une telle approche Approximer-et-Pousser peut être alors vue comme une relaxation évolutive de la contrainte globale en une contrainte locale. Plusieurs illustrations de cette approche sont proposées dans (Soulet, 2006) pour extraire les contraintes \"être-maximal\" et \"être-libre\". La section suivante instancie cette aproche pour l\u0027extraction des top-k motifs selon une mesure d\u0027intérêt. Auparavant, nous développons, de manière générale, les deux étapes majeures de l\u0027approche Approximer-et-Pousser :\nApproximer La mise à jour de la collection de motifs candidats se décline en trois opé-rations : l\u0027initialisation, l\u0027ajout et la suppression. L\u0027initialisation de la collection des motifs candidats doit être choisie avec attention afin de ne pas manquer de motifs. Lorsque l\u0027espace de recherche est parcouru dans son ensemble, la collection est initialisée à vide ce qui assure la complétude. Ensuite, l\u0027ajout et la suppression des motifs interviennent à chaque nouvelle étape d\u0027approximation i.e., un nouveau motif postule pour entrer dans la collection. Ce dernier est ajouté à celle-ci si et seulement si au vu des motifs candidats déjà présents dans la collection, il peut éventuellement satisfaire la contrainte globale. Enfin, un motif est supprimé de la collection s\u0027il est exclu par un motif postulant. Un motif peut être supprimé soit positivement (i.e., il est conservé car il satisfait la contrainte globale), soit négativement (sinon). Lorsqu\u0027un motif est exclu par le motif postulant, cela n\u0027implique pas toujours l\u0027entrée de ce dernier.\nPousser Par l\u0027intermédiaire de la collection de motifs candidats, cette étape doit permettre de pousser la contrainte globale au coeur de l\u0027extraction et ainsi, réduire l\u0027espace de recherche. Dans un premier temps, cette étape déduit certaines informations de l\u0027approximation (e.g., un calcul effectué sur l\u0027ensemble des motifs candidats). Ces informations évoluent au gré de l\u0027ajout et de la suppression des motifs. Ensuite, celles-ci sont converties en une condition d\u0027éla-gage afin d\u0027éliminer des motifs de l\u0027espace de recherche. Cette condition d\u0027élagage peut par exemple être une contrainte locale adaptée à un algorithme d\u0027extraction.\nExtraction des top-k motifs selon une mesure\nAperçu de l\u0027approche\nCette section donne un aperçu général de notre approche d\u0027extraction des top-k motifs selon une mesure m en exploitant la méthode Approximer-et-Pousser.\nL\u0027extraction des top-k motifs selon une mesure m est épineuse car en général, on ne sait pas où se situeront dans l\u0027espace de recherche les motifs vérifiant la contrainte. Par ailleurs, la définition 2 ne permet pas directement d\u0027obtenir une contrainte locale qui pourrait être exploitée par un algorithme usuel. Afin de pallier en partie ce dernier point, nous introduisons une définition alternative des top-k motifs avec la propriété 1 : 2. Pousser : cette étape poussera la contrainte m(X) ? ? pour réduire l\u0027espace de recherche.\nChacune de ces deux étapes est difficile. La première doit permettre de fixer le seuil temporaire ? de façon à ne pas éliminer de motifs satisfaisant top k,m (section 4.2.1). La contrainte à pousser m(X) ? ? n\u0027est pas forcément anti-monotone. Nous utiliserons alors le principe de la relaxation anti-monotone (section 4.2.2). Ainsi, avec un algorithme d\u0027extraction de contrainte anti-monotone (comme l\u0027algorithme par niveaux (Mannila et Toivonen, 1997)), notre approche permet de traiter un large ensemble de mesures.\nDescription des deux étapes 4.2.1 Approximer les top-k motifs\nL\u0027étape d\u0027approximation conserve les k motifs maximisant la mesure m parmi les motifs déjà extraits. De cette façon, lorsque l\u0027algorithme d\u0027extraction aura parcouru l\u0027intégralité de l\u0027espace de recherche, les k motifs candidats retenus seront exactement les top-k motifs selon la mesure m.\nÀ l\u0027initialisation de l\u0027extraction, la collection des motifs candidats Cand ne contient aucun motif. La maintenance de cette collection commence alors par une phase de remplissage. Tous les motifs extraits sont ajoutés sans condition jusqu\u0027à obtenir une collection de k motifs candidats. Durant cette phase, aucun motif de Cand n\u0027est supprimé. Ensuite, l\u0027évolution de Cand entre dans une phase sélective guidée par la propriété suivante : Dans notre approche, la collection C de cette propriété correspond aux motifs candidats Cand (ou à un de ses sous-ensembles). Dès que Cand a atteint k éléments, la propriété peut être appliquée sur un motif postulant pour savoir s\u0027il est bien nécessaire de l\u0027ajouter à la collection des motifs candidats. Plus précisément, un motif postulant X est ajouté à la collection si la mesure de X est supérieure à celle d\u0027au moins un des motifs candidats. Dans le cas contraire, la propriété 2 nous garantit que le motif postulant ne pourra pas faire partie des top-k motifs selon m. En outre, un motif est supprimé de la collection dès que k autres motifs de Cand ont une mesure supérieure à la sienne. En effet, la propriété 2 assure à nouveau que ce motif ne sera jamais parmi les k motifs de plus forte mesure m et donc, ne satisfera pas la contrainte top k,m . L\u0027introduction du seuil d\u0027ajout permet d\u0027unifier ces deux phases distinctes de l\u0027étape approximer :\nDéfinition 3 (Seuil d\u0027ajout) Le seuil d\u0027ajout, noté ?, est défini de la manière suivante :\nsinon L\u0027intérêt de cette approche est que ce seuil évolue au fur et à mesure des modifications de la collection des motifs candidats Cand. Basiquement, un motif postulant est ajouté à la collection si et seulement si sa mesure m est supérieure à celle du seuil d\u0027ajout ?. Ainsi, durant la phase de remplissage, la collection accepte tous les motifs car leur mesure est toujours supérieure au seuil d\u0027ajout alors égal à ??. Ensuite, les valeurs de la mesure de chacun des motifs de Cand, synthétisées par le seuil d\u0027ajout, conditionne l\u0027introduction ou non du motif postulant au sein de la collection.\nLe tableau 2 décrit l\u0027évolution des motifs candidats Cand au cours du processus d\u0027extraction de la contrainte top 3,area (cf. section 2.1) avec l\u0027algorithme APRIORI pour le contexte donné au tableau 1. L\u0027algorithme par niveaux génère trois vagues successives de motifs postulants. La section 4.2.2 explique quels sont les motifs extraits par APRIORI. Pour chaque niveau, les motifs dont l\u0027aire est supérieure à ? (motifs en gras) entrent dans la collection des motifs candidats. La valeur de l\u0027aire est donnée par le chiffre entre parenthèses dans la colonne de\nTAB. 2 -Les top-3 motifs selon l\u0027aire avec APRIORI.\ngauche et les motifs candidats sont rassemblés dans la colonne centrale. Le seuil ? (colonne de droite) est ajusté au fur et à mesure. Tant que le nombre de motifs de Cand est inférieur à k, le seuil ? a pour valeur ??. Ensuite, ? correspond à l\u0027aire minimale satisfaite par un des motifs de Cand. Le motif E n\u0027est pas exclu par l\u0027entrée de B car son aire est égale ?. En revanche, B et E sont supprimés à l\u0027arrivée du motif AB. À la fin du dernier niveau, Cand correspond aux 3 motifs de plus forte mesure d\u0027aire.\nPousser l\u0027approximation\nCette étape bénéficie de la collection obtenue des motifs candidats afin de réduire l\u0027espace de recherche. Nous montrons maintenant comment il est possible de déduire de cette collection une contrainte anti-monotone afin de réutiliser des algorithmes efficaces bénéficiant de l\u0027antimonotonie.\nLes seuls motifs pouvant satisfaire la contrainte top k,m sont ceux qui peuvent être ajoutés à la collection des motifs candidats (car les autres sont immédiatement rejetés, cf. la propriété 2). Ces motifs doivent donc avoir une mesure supérieure au seuil d\u0027ajout i.e., ils satisfont la contrainte locale m(X) ? ?. De façon générale, cette contrainte n\u0027est pas antimonotone. Typiquement, la contrainte area(X) ? ? n\u0027est pas anti-monotone. Par exemple, dans le contexte D, le motif ABC satisfait la contrainte area(X) ? 6, mais pas sa généra-lisation BC dont l\u0027aire est seulement de 4. Afin d\u0027obtenir dans le cas général une contrainte anti-monotone, nous proposons d\u0027approximer la contrainte m(X) ? ? par une relaxation m (X) ? ? vérifiant les deux conditions suivantes :\n, cette dernière assurant la complétude du processus. L\u0027obtention de la contrainte relaxée m (X) ? ? n\u0027est pas une tâche triviale. Une méthode automatique et générale pour toute contrainte fondée sur des primitives est donnée dans (Soulet et Crémilleux, 2005). À partir de la contrainte d\u0027aire, nous allons montrer comment procéder.\nTout d\u0027abord, on peut remarquer que la mesure freq(X) × l est décroissante lorsque X est croissant et celle-ci satisfait la condition C1. D\u0027autre part, il est possible de fixer l pour être certain que freq(X) × l sera plus grande que freq(X) × count(X) pour tous les motifs X du jeu de données D (satisfaction de C2). Pour cela, il suffit que le seuil l soit supérieur à la longueur de chacun des motifs présents dans D. Or, la taille du plus grand motif correspond exactement à la taille de la plus grande transaction. De cette manière, l est fixé à 4 avec le jeu de données D. Ainsi, la relaxation anti-monotone freq(X) × 4 ? ? pourra être exploitée comme contrainte locale pour extraire les top-k motifs selon la mesure d\u0027aire dans le jeu de données D.\nReprenons le déroulement de l\u0027extraction des top-3 motifs selon l\u0027aire présenté au tableau 2. La relaxation anti-monotone utilisée est freq(X) ? ?/4. À la fin du premier niveau, ? \u003d 3 et la relaxation freq(X) ? 3/4 n\u0027élimine aucun motif. Tous les motifs de longueur 2 et présents dans le contexte D sont donc générés. En revanche, à la fin du niveau 2, ? \u003d 5 et la relaxation devient freq(X) ? 5/4. Pour le niveau 3, seuls les motifs de fréquence supé-rieure ou égale à 2 sont donc générés (il y a uniquement ABC dans cet exemple). Le processus d\u0027extraction s\u0027arrête alors car plus aucun motif ne peut être généré. Au final, l\u0027approche Approximer-et-Pousser a économisé la génération de 8 motifs de longueur 3 et de 2 motifs de longueur 4. L\u0027efficacité de cette approche Approximer-et-Pousser réside dans l\u0027ajustement dynamique de la contrainte au cours de l\u0027extraction. Plus précisément, la relaxation anti-monotone m (X) ? ? devient de plus en plus sélective car le seuil d\u0027ajout ? croît pour tendre vers ? k,m . Cette approche Approximer-et-Pousser diminue donc significativement l\u0027espace de recherche pour donner un processus d\u0027extraction rapide comme le montre la section expérimentale suivante.\nExpérimentations\nL\u0027objectif de ces expérimentations est de montrer l\u0027efficacité de l\u0027approche Approximeret-Pousser pour différentes mesures et différents jeux de données. Au-delà de la rapidité, nous souhaitons montrer la faisabilité de notre approche générique. Aussi, nous ne nous comparons pas aux algorithmes de la littérature limités à la seule mesure de fréquence, mais nous confrontons trois stratégies différentes d\u0027extraction des top-k motifs basées sur l\u0027algorithme APRIORI (Agrawal et Srikant, 1994) :\n-Approximer-et-Pousser : cette stratégie extrait les top-k motifs en s\u0027appuyant sur l\u0027approche Approximer-et-Pousser. -Optimale à 50% : cette stratégie exploite la relaxation anti-monotone de m(X) ? ? en fixant le seuil ? à 50% du seuil idéal ? k,m . Ce seuil idéal est le seuil permettant d\u0027obtenir exactement et directement les top-k motifs. Bien sûr, dans la réalité, ce seuil n\u0027est pas connu et l\u0027utilisateur procède plutôt par tâtonnement à partir de son intuition. -Post-traitement : les motifs sont extraits avec un seuil de fréquence minimale de 10%.\nPuis, les k motifs maximisant la mesure sont conservés. Le seuil de 10% est un compromis entre faisabilité et exhaustivité (i.e., ne manquer aucun des top-k motifs). Pour toutes ces expériences, nous utilisons la même implémentation d\u0027APRIORI. Les temps d\u0027extractions sont donc comparables. Toutes les expériences sont effectuées sur un ordinateur doté d\u0027un processeur Xeon 2.2 GHz et de 3GB de mémoire RAM avec le système d\u0027exploitation Linux.\nLa figure 1 reporte les temps des extractions en fonction du nombre de motifs désirés k pour les jeux de données mushroom et letter (D.J. Newman et Merz, 1998) (www. ics.uci.edu/~mlearn/MLRepository.html). Sur chaque base, deux mesures ont alors été utilisées, à savoir la fréquence et l\u0027aire. En plus, des trois stratégies exposées cidessus, nous ajoutons le temps d\u0027extraction optimal comme courbe de référence. Cette valeur de référence consiste à donner directement la relaxation anti-monotone de m(X) ? ? k,m pour obtenir exactement les k meilleurs motifs. \nFIG. 1 -Temps d\u0027extraction des top-k motifs.\nLa stratégie Post-traitement se distingue des deux autres car, quelque soit la valeur de k, le temps d\u0027extraction est le même. Le plus souvent cette stratégie est la moins bonne (surtout lorsque k est peu élevé). Dans de rares situations où k est de valeur moyenne, cette stratégie dépasse les deux autres. En revanche, pour des valeurs de k trop grandes, il arrive que cette approche manque des top-k motifs. Cela se traduit par un arrêt des courbes sur les graphiques de la figure 1 car le processus n\u0027effectue plus la tâche demandée. Par exemple, avec le jeu de données letter, quelque soit la valeur de k, cette stratégie manque des motifs. Elle ne fournit donc pas toujours le résultat souhaité et échoue parfois en temps.\nIl est intéressant de remarquer que, globalement, les deux stratégies Approximer-et-Pousser et Optimale-50% ont le même comportement. Plus le nombre de k motifs à extraire est grand, plus le temps d\u0027extraction augmente. Par ailleurs, lorsqu\u0027une mesure est plus difficile à traiter qu\u0027une autre, elle l\u0027est pour les deux stratégies. Comme attendu, dans tous les cas, la courbe de référence est en deçà des deux stratégies. Un résultat important est que pour toutes les expériences, la stratégie Optimale-50% (pourtant optimiste) a de plus mauvais résultats que\n"
  },
  {
    "id": "940",
    "text": "Introduction\nL\u0027Internet représente un extraordinaire outil d\u0027accès à un ensemble quasi infini de ressources et un puissant outil de communication. Elle prend une place grandissante dans la vie quotidienne et dans le monde professionnel. Le public qui y a accès est de plus en plus large, mais aussi de plus en plus jeune. Les enfants trouvent chaque jour un accès plus facile à la toile. Cet accès de plus en plus large ne va pas sans inconvénients, les sites à caractère adulte, violent, raciste exposent les enfants à des contenus qui peuvent heurter leur sensibilité, voire les choquer. En effet, ces sites sont souvent en accès libre, ce qui pose un problème évident vis à vis des enfants. Ces utilisations litigieuses de l\u0027Internet, par des individus mal intentionnés, n\u0027ont pas occulté les énormes possibilités de progrès personnel et social, d\u0027enrichissement culturel et éducatif offertes par ce réseau. Ainsi, un ensemble de produits commerciaux sur le marché proposent des solutions de filtrage de sites Web. La majorité de ces produits traitent principalement le caractère adulte, alors que les autres caractères, comme le caractère néonazie, raciste et violent, ont été marginalisé. C\u0027est ce dernier caractère qui sera traité dans cet article. La section suivante présente une revue de littérature sur les travaux qui ont porté sur le filtrage de sites web. Nous décrivons dans la section 3 notre approche de classification des sites Web à caractère violent par une analyse du contenu textuel et structurel des pages Web. Les résultats de l\u0027expérimentation de l\u0027approche proposée seront détaillés dans la section 4. La section 5 décrit l\u0027architecture et le principe de fonctionnement de notre solution « WebAngels Filter » ainsi que la comparaison des résultats de ce dernier avec les logiciels les plus connus sur le marché. Enfin une conclusion et quelques perspectives feront l\u0027objet de la dernière section.\nFiltrage Web\nPlusieurs techniques de filtrage Web ont été proposées pour bloquer les pages Web à caractère litigieux. Parmi ces techniques, on peut citer :\n1. La technologie de l\u0027étiquetage PICS 1 (Platform for Internet Content Selection) : c\u0027est un standard de programmation permettant de véhiculer des informations concernant le genre de contenus qui sont représentés sur les sites. En se basant sur ce codage, le navigateur prendra la décision d\u0027afficher ou non une page. Il est à noter que l\u0027efficacité du PICS est relative, en effet, elle dépend fortement de l\u0027engagement des concepteurs des sites à étiqueter leurs pages, en absence d\u0027organisme qui les oblige à le faire.\n2. La liste noire : représente un ensemble de sites, motifs génériques, ou domaines à exclure de la navigation. On garde donc la possibilité de naviguer librement d\u0027un site à un autre, ce qui permet de conserver la spécificité de l\u0027Internet, tout en restreignant les risques d\u0027accéder à un site inapproprié. Cependant il est difficile de regrouper tous les sites inappropriés puisque de nouveaux sites apparaissent chaque jour. De ce fait, une liste noire ne peut jamais être exhaustive.\n3. La liste blanche : contient l\u0027ensemble de sites sur lesquels la navigation peut avoir lieu. C\u0027est donc un ensemble de sites autorisés. Toute tentative d\u0027accès à n\u0027importe quel site ne figurant pas sur cette liste blanche sera automatiquement refusée. Les éditeurs de logiciels constituent rarement de telles listes blanches, dont l\u0027élaboration est le plus souvent laissée aux parents. Cette solution, qui restreint strictement la navigation à un « jardin d\u0027enfants », peut servir à sécuriser la navigation de très jeunes enfants. Ces listes demandent une vérification régulière par un administrateur, en effet, il arrive souvent que certains sites à contenu tout a fait licite disparaissent en laissant leurs adresses récupérées par des sites inappropriés. Les travaux proposés pour la classification et le filtrage des pages Web à caractère litigieux sont assez nombreux et variés, Cependant la majorité de ces travaux ne traitent que le caractère adulte. Nous proposons dans la section suivante notre approche pour la classification des sites à caractère violent.\nApproche proposée\nOn se place dans le cadre des systèmes automatiques de classification et de catégorisation de sites Web pour proposer une approche de classification des sites Web à caractère violent. En effet, pour classifier les sites à caractère violent et les sites normaux, nous nous sommes basés sur le processus d\u0027extraction des connaissances à partir des données. Le principe général de l\u0027approche de classification est le suivant : Soit S une population de sites concernés par le problème d\u0027apprentissage. A cette population est associé un attribut particulier appelé « attribut classe » noté C. Cet attribut peut avoir deux valeurs, la valeur 0 si le site est violent et 1 si le site est normal. A chaque site s peut être associée sa classe C(s)\nDans notre étude nous cherchons un moyen pour prédire la classe C. La détermination de ce modèle de prédiction est liée à un vecteur de caractéristiques X \u003d (X i ) 1?i?p que nous avons établi a priori. Ce modèle de prédiction permet, pour un site s issu de S, pour lequel nous ne connaissons pas la classe C(s) mais nous connaissons son vecteur de caractéristiques, de prédire sa classe. La figure 1 illustre le schéma général de l\u0027approche de classification proposée. Deux parties peuvent être distinguées : la première est celle de l\u0027apprentissage consacrée à la FIG. 1 -Schéma général de l\u0027approche proposée préparation du modèle de prédiction, la deuxième est celle de la classification des sites Web. Nous signalons que les modèles construits sont sensibles à la qualité des données qui leur sont fournies et nous avons été obligé de faire plusieurs itérations qui ont conduit à affiner la recherche et à élaborer de nouvelles variables ce qui nous a permis d\u0027améliorer les résultats obtenus au fur et à mesure des différentes étapes.\nDans ce qui suit, nous détaillerons les différentes étapes de l\u0027élaboration du modèle de prédiction des sites Web\nPréparation des données\nLa préparation des données pour la phase d\u0027apprentissage consiste à identifier les informations exploitables et vérifier leur qualité et leur efficacité afin de construire une table bidimensionnelle, à partir de notre corpus d\u0027apprentissage. La recherche des attributs les plus informatifs est le point central de cette phase puisque c\u0027est elle qui va conditionner la qualité des modèles établis lors de l\u0027apprentissage, par conséquent cette étape a une influence directe sur la performance du classifieur.\nConstruction de la base d\u0027apprentissage\nPour que l\u0027apprentissage soit efficace, il faut que notre base d\u0027apprentissage soit repré-sentative de la population et que le nombre des éléments de la base sur lequel il est fait soit important. Cette phase de collecte et de sélection des sites Web constitue une charge de travail considérable vu la diversité et le nombre énorme de sites Web sur Internet. Dans la collecte des sites violents, nous avons essayé d\u0027avoir une base diversifiée en terme de :\n-Contenu des sites : la recherche de ces sites s\u0027est focalisée sur la guerre, l\u0027attentat, la torture de prisonniers, l\u0027assassinat, la violence explicite, la création de bombe, les films d\u0027horreur, et les articles qui parlent des effets de la violence. Elle a été effectuée avec le moteur de recherche « Google » en se basant sur un ensemble de mots clés violents. Dans un premier temps on s\u0027est intéressé sur le contenu textuel et ensuite sur les images qui représentent un contenu violent. -Langues traitées : nous avons traité deux langues à savoir la langue anglaise et la langue française. -Structure : certains sites collectés ne contiennent que du texte, d\u0027autres ne contiennent que des images, et la majorité des sites contient les deux. Pour la sélection des sites non violents, nous avons inclus ceux qui peuvent prêter à confusion, en particulier des sites qui luttent contre la violence et des sites de loi, etc. Le reste des sites a été choisi au hasard, on trouve alors des sites de téléchargement de jeu, des sites de codes sources, des sites de bandes dessinées, des sites éducatifs, des sites d\u0027enfance, etc.\nNotre base d\u0027apprentissage se compose de 700 sites dont 350 sont violents, et 350 sites non violents.\nAnalyse du contenu textuel et structurel\nL\u0027analyse du contenu textuel et structurel d\u0027une page vise à extraire des variables structurelles et textuelles permettant de mieux discriminer les pages Web violentes de celles inoffensives. Dans cette phase nous nous sommes basé sur les connaissances acquises suite à notre étude des travaux de recherche existants et sur la sélection manuelle des pages Web lors de la construction de notre base d\u0027apprentissage.\nLa fréquence des mots interdits dans une page Web nous semble la variable la plus discriminante. C\u0027est pourquoi nous proposons d\u0027utiliser deux variables textuelles qui sont n_v_mots, et pourcentage v_mots, qui présentent respectivement le nombre de mots violents qui figurent dans la page et leur pourcentage.\nLa structure d\u0027une page Web est fondée sur un système de balises (chaînes de caractères délimitées par les symboles \u003c et \u003e) qui décrit leur type (liens hypertexte, images, mots clés, etc.). Glover et al. (2002) ont prouvé que l\u0027analyse de cette structure combinée à une analyse textuelle ne peut qu\u0027améliorer la classification et la description de la page Web. L\u0027analyse de différentes balises nous a permis également d\u0027extraire et de calculer d\u0027autres variables dites structurelles comme n_v_url qui représente le nombre de mots violents dans l\u0027URL, et le n_v_meta qui décrit le nombre de mots violents dans les balises meta. Pour récapituler ce qui précède, le vecteur de caractéristiques que nous avons utilisé pour classifier les pages Web est représenté par le tableau 1.\nL\u0027extraction des différentes caractéristiques précédentes nécessite l\u0027analyse du code HTML d\u0027une page Web. Il nous a donc fallu nous doter de : (1) un client HTTP, qui prend en paramètre une URL et renvoie une page de code HTML ; (2) un analyseur syntaxique (parser HTML), qui lit le code de la page, calcule les valeurs associées aux différents critères et stocke ces valeurs dans un fichier, qui sera utilisé par la suite dans une phase d\u0027apprentissage. Il est à noter que le calcul de la majorité des variables a été effectué en se basant sur un vocabulaire de mots violents rassemblés dans un dictionnaire. Ce dernier a été construit manuellement et il comprend des mots clés français et anglais.\nLa phase suivante de notre démarche est celle de la sélection de variable. Dans cette phase, nous avons déterminé les variables qui ont une influence sur notre problème. La sélection des variables contribue à réduire la taille du problème en isolant les variables exogènes les plus pertinentes. L\u0027élimination des variables inutiles et redondantes permet d\u0027accélérer le processus d\u0027apprentissage et d\u0027augmenter la fiabilité du classifieur obtenu. Afin de sélectionner les variables les plus pertinentes nous avons utilisé une approche de type filtre et plus précisé-ment l\u0027algorithme Relief (Kira et l. Rendel, 1992) vue qu\u0027il est capable de travailler avec des variables bruitées et corrélées et de traiter des données nominales et continues.\nApprentissage supervisé\nIl s\u0027agit de trouver une fonction de classement efficace pour prédire les valeurs d\u0027une variable catégorielle, dite à prédire, en fonction des valeurs d\u0027une série de variables continues et/ou catégorielles, dites prédictives. Dans la littérature, il existe plusieurs techniques d\u0027apprentissage supervisé comme les réseaux de neurones (Herault et Jutten, 1994), les graphes d\u0027induction (Zighed et Rakotomalala, 2000), les réseaux bayesiens (Naïm et al., 2004), les machines à vecteurs supports (Schölkopf et al., 1998  (Quinlan, 1986), C4.5 (Quinlan, 1993), IMPROVED C4.5 (Rakotomalala et Lallich, 1998), SIPINA (avec ?\u003d1 et ?\u003d0.2) (Zighed, 1996).\nValidation\nAprès la phase d\u0027apprentissage, nous avons évalué la qualité et la stabilité des modèles obtenus à partir des quatre algorithmes de data mining par le biais de la méthode des taux d\u0027erreur. En effet, il est délicat de formuler des indicateurs généraux pour valider les modèles, et dans la plupart des cas, les chercheurs travaillent sur le taux d\u0027erreur parce qu\u0027il est l\u0027un des meilleurs indicateurs qui soit véritablement comparable d\u0027un algorithme à un autre.\nLes \nExpérimentations\nCette section présente les différentes expérimentations réalisées afin de trouver le modèle de prédiction le plus pertinent pour notre application. L\u0027étape de recherche du modèle consiste à extraire la connaissance utile de l\u0027ensemble de données que nous avons collecté dans les phases décrites auparavant. Avant de présenter les séries d\u0027expérimentations et afin de clarifier leurs conditions, nous allons décrire brièvement les conditions d\u0027expérimentation.\nConditions d\u0027expérimentations et techniques de validation\nDans nos expérimentations, nous présentons deux séries de tests : La première est le résultat de notre système de classification sur les 700 sites qui constituent notre base d\u0027apprentissage. Après une première phase d\u0027apprentissage, nous avons évalué la qualité et la stabilité des modèles obtenus à partir des quatre algorithmes de data mining par le biais de la méthode des taux d\u0027erreur. La deuxième série de test est le résultat de notre système de classification sur une base de test composée de 300 sites : 150 à caractère violent et 150 à caractère non violent. Cette série d\u0027expérimentation a été réalisée afin d\u0027éviter le phénomène de « surapprentissage » (overfitting). En effet, il est fréquent que certains classifieurs « apprennent » les données plutôt que le modèle.\nRésultats\nLa figure 2 illustre les différents taux d\u0027erreur correspondants à l\u0027utilisation des quatre algorithmes d\u0027apprentissage. Le meilleur algorithme étant SIPINA que ce soit avec ?\u003d1 ou ?\u003d0.2. Ces résultats peuvent s\u0027expliquer par le fait que cet algorithme tente de réduire les inconvénients des méthodes arborescentes d\u0027une part par l\u0027introduction de l\u0027opération de fusion et d\u0027autre part par l\u0027utilisation d\u0027une mesure sensible aux effectifs. Nous pouvons signaler que, pour la majorité des algorithmes, le taux d\u0027erreur a priori et a posteriori violent sont faibles par rapport à ceux non violent. Cela signifie que ces algorithmes fournissent une décision plus fiable en ce qui concerne la classification des sites violents.\nFIG. 2 -Classification des sites Web à caractère violent\nEncouragés par les résultats précédents, nous avons alors testé les différents modèles de prédiction, obtenus lors de la phase d\u0027apprentissage, sur notre base de test. Les résultats des différentes expérimentations sont décrits par la figure 3. Un filtre efficace détermine les sites à filtrer et les sites à ne pas filtrer. En d\u0027autres termes, le logiciel identifie tous les sites à caractère violent, disponibles sur le web. Ceci constitue le rappel. Ce qui différencie les bons filtres des moins bons filtres est leur capacité à correctement distinguer les sites trouvés. Les sites contenant le mot « violent » ne doivent pas tous être filtrés. L\u0027accès aux sites violents doit être bloqué, mais les sites qui luttent contre la violence doivent rester accessibles. Cette capacité à distinguer les différents sites constitue la précision. Le rappel et la précision sont inversement proportionnels. Ainsi, si un filtre est capable d\u0027identifier tous les sites à contenus inappropriés, l\u0027accès à certains sites inoffensifs sera également bloqué. Mais, si le logiciel est très spécialisé et capable de trouver uniquement des contenus préjudiciables sur un sujet spécifique, de nombreux contenus inadéquats pourront toujours être consultés. La stratégie que nous avons choisie consiste à utiliser le modèle qui assure un meilleur compromis entre le rappel et la précision. Compte tenu des résultats obtenus dans les phases d\u0027apprentissage et de test, nous avons opté pour l\u0027utilisation du modèle de prédiction produit par l\u0027algorithme SPINA (?\u003d1). -récupérer le code source HTML de la page demandée ; -vérifier si l\u0027URL appartient à une liste noire, et sinon analyser le code ; -déclarer cette page autorisée ou interdite ; -mettre à jour la liste noire ; -mettre à jour l\u0027historique de navigation ; -afficher ou non la page. La figure 4 résume le fonctionnement de notre logiciel.\nComparaison avec quelques produits commerciaux\nAfin de mieux évaluer notre modèle de prédiction produit par l\u0027algorithme SIPINA (?\u003d1), nous avons mené une étude comparative de notre solution avec quatre produits commerciaux, à Ces logiciels sont censés être capable de filtrer des sites à caractère litigieux, et ils ont été paramétrés de façon à filtrer que le caractère violent. Notre objectif, ici, est d\u0027évaluer si nos résultats théoriques avaient un sens en les comparant aux résultats réels du logiciel. Cette étude a été effectuée sur notre base de test, totalement indépendante de celle utilisée dans la phase d\u0027apprentissage. La figure 5 montre la performance de « WebAngels Filter » par rapport aux logiciels existants sur le marché avec un taux de classification égale à 81%. Ceci peut être expliqué par le fait que la majorité de ces logiciels traitent principalement le caractère pornographique des sites Web, alors que le caractère violent à été omis , et donc échappe à leurs filtres.\nConclusion et perspectives\nCe papier présente une solution de classification et de filtrage des sites Web à caractère violent par un apprentissage qui s\u0027appuie sur plusieurs algorithmes de data mining avec non seulement une analyse du contenu textuel mais aussi du contenu structurel. L\u0027étude compa-FIG. 5 -Etude comparative de l\u0027approche proposée avec quelques produits du marché.\nrative de notre solution avec les logiciels du marché les plus connus, sur notre base de test, montre la performance de notre système.\nCes résultats encourageants nous incitent à approfondir nos travaux de classification et filtrage de sites Web par une analyse conjointe de plusieurs modalités et à les appliquer à d\u0027autres problèmes comme par exemple le filtrage de sites adultes, racistes, etc. D\u0027autres pistes d\u0027amé-lioration concernent l\u0027élaboration du dictionnaire des mots clés qui a joué un rôle central dans les performances de « WebAngels Filter ». Or, l\u0027élaboration de ce dictionnaire a été très laborieuse car manuellement améliorée étape après étape, et elle n\u0027a vraisemblablement possible que grâce à la compréhensibilité des modèles obtenus par les techniques d\u0027extraction de connaissance. Il serait donc intéressant d\u0027automatiser par l\u0027apprentissage à partir d\u0027un corpus la construction d\u0027un tel dictionnaire. Enfin, penser à une manière d\u0027intégrer le traitement de l\u0027aspect visuel dans les modèles de prédictions afin de remédier aux difficultés de classifier les sites Web violentes qui ne comprennent que des images.\n"
  },
  {
    "id": "942",
    "text": "Introduction\nA l\u0027heure actuelle, l\u0027Internet est devenu une des sources d\u0027information les plus importantes dans des nombreux domaines, comme celui de la santé. Afin de faciliter l\u0027accès aux informations médicales disponibles en ligne, l\u0027élaboration de nouveaux instruments et méthodes de recherche s\u0027avère nécessaire. Le projet CISMeF 1 (Catalogue et Index des Sites Médicaux Francophones) Darmoni et al. (2000) est le catalogue de santé lancé par le CHU de Rouen en 1995. L\u0027objectif du catalogue est de décrire et de classer les principales ressources (documents sur le Web) de santé en français pour aider les utilisateurs dans leur recherche d\u0027information médicale de qualité disponibles en ligne.\nDes efforts considérables ont été engagés par l\u0027équipe CISMeF afin de développer des architectures d\u0027indexation automatique, et des avancements significatifs ont été présentées Né-véol et al. (2006). Cependant, l\u0027indexation automatique a des limites et un des principaux problèmes reste la difficulté d\u0027indexation des médias non textuels, comme les images.\nLes travaux précédents\nIl existe deux approches principales pour rechercher des images : en utilisant le contenu ou en utilisant le contexte de l\u0027image (les régions textuelles associées aux images). Initialement, les images étaient indexées à l\u0027aide des index reposant sur des mots clés Frankewitsch et Prokosch (2001). Au cours des années, des méthodes basées sur le contenu visuel des images ont été proposées pour l\u0027annotation, l\u0027indexation et la recherche des images médicales non annotées Lehmann et al. (2003)  Müller et al. (2003). Dernièrement, nous avons remarqué un intérêt croissant pour les architectures qui proposent des descriptions d\u0027images combinant les deux approches Deselaers et al. (2005)  Besancon et Millet (2006).\nDans cet article, nous proposons une architecture combinant le contenu et le contexte pour l\u0027annotation des images médicales, le but étant d\u0027extraire au mieux des informations bien défi-nies. Etant placés dans le contexte réel d\u0027un catalogue en ligne, nous avons créé une application capable de traiter tous les aspects techniques de l\u0027extraction d\u0027information à partir de différents formats (cryptées, non structurées). En même temps, nous avons cherché à minimiser la sensibilité de notre système aux variations de contenu et de qualité spécifiques à l\u0027Internet.\nL\u0027architecture du module MedIC\nFIG. 1 -Le module MedIC\nDans le contexte du projet d\u0027indexation automatique des documents de santé développé par l\u0027équipe CISMeF, le module MedIC (Medical Image Categorization) a comme tâche l\u0027annotation des images médicales. Le but du module est de localiser, d\u0027extraire et d\u0027annoter les images médicales à partir d\u0027un document donné. Le MedIC a été conçu pour rechercher plusieurs types d\u0027informations médicales : la modalité médicale, la région anatomique, l\u0027angle de vue d\u0027acquisition et la pathologie. Ces informations permettront aux utilisateurs du catalogue CISMeF, de formuler des requêtes orientées vers l\u0027image, telles que \u003cTrouve-moi les documents contenant des images ANGIOGRAPHIQUES (modalité) présentant un EMBOLISME (pathologie) PULMONAIRE (région anatomique)\u003e. L\u0027architecture de module MedIC est présentée dans la figure 1. Pour extraire l\u0027information médicale, nous traitons : le contenu visuel des images Florea et al. (2006), les annotations marquées directement sur l\u0027image Florea et al. (2005), et les régions textuelles associées aux images.\nMême si la source a été prouvée comme étant précise, elle est rarement utilisable en ligne à cause du manque d\u0027annotations marquées sur la majorité des images publiées sur l\u0027Internet. Dans cet article, nous évaluons la pertinence de l\u0027information extraite à partir des sources et et le gain de performance obtenu en combinant les deux approches.\nLes images et les régions textuelles associées\nLes ressources médicales (i.e. documents) contiennent des quantités considérables d\u0027informations relatives aux images. Habituellement, il y a deux régions textuelles associées aux images : la légende -courte et placée près de l\u0027image et le paragraphe plus long et plus détaillé.\nPour les expériences d\u0027extraction/annotation des images que nous présentons dans cet article, nous avons créé une base de 657 enregistrements, extraits automatiquement à partir des documents classés par CISMeF. Les images représentant les six principales modalités médi-cales : l\u0027angiographie, l\u0027échographie, l\u0027imagerie à résonance magnétique, la radiographie standard, la tomographie par l\u0027ordinateur (scanner) et la scintigraphie. Chaque modalité est liée à une hiérarchie de régions anatomiques et sous anatomiques. Pour nous permettre l\u0027évaluation automatique des performances d\u0027annotation, chaque enregistrement est manuellement annoté avec la modalité médicale et la région anatomique capturée par l\u0027image.\nL\u0027annotation basée sur le contenu visuel (V)\nLa catégorisation des images médicales basée sur le contenu d\u0027image, peut être un outil d\u0027annotation très performant, dans le contexte de la recherche d\u0027images dans des bases non annotées. Notre approche de catégorisation est basée sur la classification supervisée des représentations numériques des images, reposant sur des attributs de texture (e.g. matrices de co-occurrence, dimension fractale, les réponses aux filtres de Gabor, et autres) et statistiques. Une analyse en composantes principales (PCA) est employée pour réduire la dimensionnalité de l\u0027espace des attributs. Finalement, un classifieur SVM (Support Vector Machines) est employé par MedIC, pour la projection des données de test dans les catégories correspondantes. Plus de détails sur l\u0027approche « visuelle », sont disponibles dans Florea et al. (2006).\nLes performances d\u0027annotation de la modalité sur les 657 enregistrements de notre base sont présentées dans le tableau 1. Pour chaque information extraite (modalité et région anatomique), nous présentons les résultats dans la forme des précision (p), rappel (r) et f-mesure (f m ), des mesures souvent utilisées en « recherche d\u0027informations ». \nTAB. 1 -Décision visuelle (V)\nLes résultats d\u0027annotation que nous avons obtenus en utilisant le contenu des images sont plus faibles que prévu. En utilisant la même architecture, nous avons obtenu des résultats bien meilleurs au cours de la campagne d\u0027évaluation CLEF Florea et al. (2006. Les résultats inferieurs sont dus principalement au fait que, pour les expérimentations présentées dans cet article, nous avons fait l\u0027apprentissage SVM sur une base d\u0027images de qualité sensiblement différente (meilleure) que notre base de 657 images de test. Généralement, avec des images extraites à partir des documents en ligne nous obtenons des résultats inferieurs, à cause de la faible résolution et compression élevée de ces images.\nL\u0027annotation basée sur des informations contextuelles (T)\nCette deuxième approche vise à traiter l\u0027information portée par les régions textuelles associées aux images (légendes et paragraphes), pour extraire les mêmes annotations que pour l\u0027approche visuelle : les modalités médicales et les régions anatomiques.\nLa première étape est de modéliser l\u0027information à extraire sous forme de dictionnaires. Pour chaque information, des dictionnaires DELA ont été créés, basés sur la terminologie MeSH 2 et des termes et synonymes CISMeF. Les dictionnaires devraient nous permettre de localiser les termes MeSH, sous les diverses formes qu\u0027ils peuvent prendre en langage naturel. Les dictionnaires ont été manipulés en utilisant l\u0027environnement linguistique INTEX/NOOJ 3 . Les termes obtenus suite à l\u0027application des dictionnaires, sont utilisés pour l\u0027annotation des images, en employant une décision par vote majoritaire. Nous avons implémenté plusieurs stratégies de traitement des deux zones textuelles que nous disposons :\nLégendes et paragraphes traitées ensemble T(L+P) -les légendes et les paragraphes sont considérés comme ayant la même importance.\nPriorité aux légendes T(L en priorité) -nous traitons la légende en priorité, car souvent, la légende contient des informations plus précises et succinctes que le paragraphe.\nApproche voisinage (pour les régions anatomiques) T(L en priorité+voisins) -Les confusions les plus courantes concernent les régions anatomiques voisines (e.g. bras/coude, avant-bras/main). Par conséquent, nous avons ajouté des conditions supplémentaires, en défi-nissant une table de voisinage. Même si cette stratégie de voisinage semble adaptée, les résul-tats concrets sont moins satisfaisants. \nTAB. 2 -Résultats de décisions modalité et régions anatomiques (T)\nLes résultats pour l\u0027annotation des modalités et des régions anatomiques sont présents dans le tableau 2. Une analyse détaillée nous a montré que les principaux responsables des erreurs sont : les légendes communes aux plusieurs images, les extractions erronées des couples imagetexte ou les erreurs grammaticales dans le texte original.\nIl est important de remarquer que même si cette approche est incapable de proposer une décision pour toutes les images (faible rappel), elle est très précise (surtout pour les modalités).\nFusion des décisions (F)\nDans cette section, nous allons évaluer le gain de performance obtenu après la combinaison des sources et Les résultats obtenus sont présentés dans le tableau 3.\nApproche prioritaire texte F(T en priorité) -la décision textuelle est traité en priorité (avec aussi des légendes prioritaires), pour exploiter la bonne précision de cette approche. Pour pondérer les informations, nous avons utilisé les rangs cumulés de chaque déci-sion pour les deux sources.\nApproche équilibré F(T+V) -nous utilisons les mêmes critères de rangs cumulés. Par rapport à l\u0027approche précédente, nous notons des faibles améliorations.\nApproche équilibrée, avec la modalité décidée F(T+V)mod. Motivées par les bons ré-sultats de la décision sur la modalité, nous avons proposé une troisième approche où nous extrayons les annotations sur les régions anatomiques en utilisant la décision sur la modalité. Cependant, dans la pratique, cette approche est moins efficace.\nLes approches de fusion (F) que nous avons essayées présentent toutes des améliorations de performance comparées aux méthodes reposant sur le contenu visuel (V) et sur le texte associé aux images (T), considérées séparément. Même avec les résultats plus faibles de la décision visuelle, la f-mesure globale, après la fusion, affiche une amélioration significative. Combinant les deux décisions, nous perdons généralement un petit pourcentage sur la préci-sion moyenne (comparée à la décision textuelle), mais nous obtenons des taux de rappel et f-measure bien plus élevés. \nConclusion\nLe but du module MedIC est de permettre l\u0027extraction et l\u0027annotation automatique des images médicales extraites à partir des documents de santé complexes. L\u0027objectif est de fournir des annotations médicales précises pour l\u0027indexation des documents et de leurs images attachées.\nLa catégorisation des représentations visuelles peut fournir des annotations précises pour des images médicales. En même temps, elle est dépendante de l\u0027existence des images d\u0027apprentissage déjà annotées et très sensible aux variations de qualité des images. L\u0027approche textuelle que nous présentons dans cet article peut fournir des annotations médicales précises pour les images, mais, à son tour, elle est dépendante des dictionnaires linguistiques.\nEn perspective, nous allons développer cette architecture pour traiter des informations autres que les modalités et les régions anatomiques. L\u0027architecture que nous avons présen-tée dans cet article est extensible en définissant des dictionnaires additionnels ou des classes d\u0027apprentissage visuelles supplémentaires.\nL\u0027architecture présentée a été conçue pour être intégrée dans le module d\u0027indexation automatique des documents du catalogue CISMeF, pour offrir aux utilisateurs un meilleur (et plus complet) outil de recherche d\u0027information médicale sur Internet.\n"
  },
  {
    "id": "944",
    "text": "Introduction\nLes entrepôts de données centralisent des données provenant de différentes sources pour répondre aux besoins d\u0027analyse des utilisateurs. Le schéma de l\u0027entrepôt est défini avec l\u0027objectif d\u0027analyser des mesures qui caractérisent des faits, en fonction de dimensions qui peuvent être organisées sous forme de hiérarchies, composées de différents niveaux de granularité, déterminant la manière selon laquelle sont agrégées les données.\nPour concevoir le schéma d\u0027un entrepôt, nous distinguons dans la littérature différents types d\u0027approches : celles guidées par les sources de données (Golfarelli et al., 1998), celles guidées par les besoins d\u0027analyse (Kimball, 1996) et les approches mixtes qui combinent les deux approches précédentes, mettant en adéquation des schémas candidats générés à partir des sources de données avec les besoins d\u0027analyse exprimés par les utilisateurs (Nabli et al., 2005).\nCependant, en pratique, les sources de données, tout comme les besoins d\u0027analyse sont amenés à évoluer. Dans la littérature, il existe deux alternatives qui permettent l\u0027évolution de schéma nécessaire suite à ces modifications. D\u0027une part la mise à jour de schéma qui est réalisée grâce à des opérateurs qui font évoluer un schéma donné (Hurtado et al., 1999). D\u0027autre part, la modélisation temporelle qui consiste à garder la trace de ces évolutions en utilisant des labels de validité temporelle. Ces labels sont apposés soit au niveau des instances (Bliujute et al., 1998), soit au niveau des liens d\u0027agrégation (Mendelzon et Vaisman, 2000), ou encore au niveau des versions du schéma (Morzy et Wrembel, 2004). L\u0027inconvénient de ce type de solutions est la nécessité d\u0027une réimplémentation des outils d\u0027analyse, de chargement, ... afin de gérer les particularités de ces modèles.\nLes deux alternatives sont intéressantes pour répondre au problème de l\u0027évolution de schéma suite à une modification dans les sources de données, puisque ce sont des solutions techniques devant être mises en oeuvre par l\u0027administrateur. Cependant, elles n\u0027impliquent pas directement les utilisateurs dans le processus d\u0027évolution. De ce fait, elles n\u0027apportent pas de solution au problème posé par l\u0027émergence de nouveaux besoins d\u0027analyse exprimés par les utilisateurs. Or, au cours de l\u0027utilisation de l\u0027entrepôt, de nouveaux besoins apparaissent étant donné que (1) il est difficile de déterminer de façon exhaustive les besoins d\u0027analyse pour l\u0027ensemble des utilisateurs lors de la conception de l\u0027entrepôt, (2) ces besoins dépendent également des propres connaissances des utilisateurs, (3) il est impossible de déterminer les besoins futurs.\nDans cet article, nous proposons alors une approche originale, qui s\u0027inscrit dans l\u0027approche de mise à jour de schéma, impliquant les utilisateurs dans le processus d\u0027évolution de l\u0027entrepôt, dans le but de leur fournir des analyses personnalisées en fonction de leurs propres connaissances du domaine et de leurs besoins. Les connaissances utilisateurs concernent plus précisément la définition de nouvelles données agrégées et sont représentées sous la forme de règles de type «si-alors». Ces règles, dites d\u0027agrégation, sont ensuite utilisées pour générer de nouveaux axes d\u0027analyse en créant dans les hiérarchies de dimension de nouveaux niveaux de granularité. Notre approche est fondée sur un modèle d\u0027entrepôt de données évolutif à base de règles nommé R-DW (Rule-based Data Warehouse), dont nous proposons ici une formalisation. Pour valider notre approche, nous avons développé une plateforme baptisée WEDriK 1 (data Warehouse Evolution Driven by Knowledge) et avons appliqué notre approche aux données bancaires de LCL 2 . Néanmoins, cette partie n\u0027est pas développée ici en raison du manque de place et peut être consultée dans Favre et al. (2006). Dans la suite de cet article, nous introduisons tout d\u0027abord dans la Section 2 un exemple simplifié motivant notre approche orientée utilisateurs. Puis, nous présentons dans la Section 3 notre approche ainsi que le principe du modèle R-DW sur lequel elle se base. Nous proposons ensuite la formalisation de ce modèle dans la Section 4. Enfin, nous concluons et indiquons les perspectives de ce travail dans la Section 5.\nExemple introductif\nPour illustrer notre approche de modélisation d\u0027entrepôt de données évolutif à base de règles, nous utilisons le cas réel de la banque LCL. Le PNB annuel (Produit Net Bancaire) correspond à ce que rapporte un client à l\u0027établissement bancaire. Cette mesure est analysée selon les dimensions CLIENT, AGENCE et ANNEE ( Supposons qu\u0027un utilisateur veuille analyser les données selon le type d\u0027agence ; il sait qu\u0027il en existe trois : type «étudiant» pour les agences ne comportant que des étudiants, type «non résident» lorsque les clients ne résident pas en France, et le type «classique» pour les agences ne présentant pas de particularité. Ces informations n\u0027étant pas présentes dans l\u0027entrepôt, il est impossible pour lui d\u0027obtenir une telle analyse. Notre objectif est donc de proposer à l\u0027utilisateur d\u0027intégrer dans le schéma de l\u0027entrepôt sa connaissance sur les types d\u0027agence pour créer le niveau de granularité TYPE_AGENCE (Figure 2b).  \nDéfinition 1. Univers de l\u0027entrepôt\nest l\u0027ensemble des   . Remarque : pour des raisons de simplification, nous supposons ici que chaque niveau de granularité est caractérisé par un seul attribut généré. Exemple 4. Dans l\u0027exemple de la section 2, \nDéfinition 3. Hiérarchie de dimension et niveau de granularité\n\u0027non résident\u0027 , tel que : Une règle d\u0027agrégation est une règle de type «si-alors». La conclusion de la règle (clause «alors») définit la valeur de l\u0027attribut généré. La prémisse de la règle (clause «si») est basée sur une composition de (conjonctions V disjonctions) des termes de règles :  \nConclusion\nDans cet article, nous avons proposé une approche originale qui exploite les connaissances utilisateurs pour faire évoluer le schéma de l\u0027entrepôt, afin d\u0027obtenir des analyses personna-\n"
  },
  {
    "id": "946",
    "text": "Introduction\nroduction de con ion de connaissances est en partie une retombée des échanges au sein de la communauté.\nDans une économie de plus en plus fondée sur la connaissance, au point que l\u0027importance du facteur de production « connaissance » augmente par rapport aux facteurs de production traditionnels, on a assisté à l\u0027émergence de nombreux concepts afin de mieux gérer ces connaissances (en tant que ressource centrale) existant au niveau de l\u0027entreprise.\nA l\u0027heure actuelle, on s\u0027intéresse fortement au concept de communautés de pratique car, par définition, elles constituent un lieu d\u0027échange et de partage de connaissances de plus en plus utiles voire indispensables pour les entreprises. Wenger (1998)  Brown et Duguid (1991 voient dans ces communautés un lieu privilégié pour la création, la maintenance et la rep naissances. Cependant, notre revue préliminaire de la littérature a montré qu\u0027il existe très peu d\u0027articles qui se consacrent explicitement au processus de création de connaissances au sein des communautés de pratique. Et pourtant, la capacité de créer de nouvelles connaissances et de les transférer au sein d\u0027une organisation est considérée comme étant la base d\u0027un avantage concurrentiel (Inkpen, 1996). Dès lors, dans notre travail de recherche, les communautés de pratique seront analysées sous l\u0027angle d\u0027une théorie qui nous a semblé constructive pour traiter ce processus : la théorie de l\u0027émergence, dans la mesure où le processus de créat Dans un premier temps, une revue de la littérature sur les communautés de pratique sera faite et nous proposerons une définition de travail pour les communautés de pratique. Dans un deuxième temps, nous considérerons les fondements principaux de la théorie de l\u0027émergence. Ensuite, nous analyserons l\u0027application aux communautés de pratique. La dernière partie décrit une première proposition de modélisation pour un support de l\u0027émergence de connaissances. Nous conclurons en faisant le bilan de notre travail et indiquant les limites de notre approche ainsi que les perspectives de recherche.\nconsidère que les communautés de pratique sont les « ressources en connaissances les plus versatiles et dynamiques des entreprises et qu\u0027elles forment la base de la capacité cognitive et d\u0027apprentissage des organisations ».\nLes communautés de pratique\nPremières définitions\nLa notion de communauté de pratique s\u0027est développée à partir des travaux sur l\u0027apprentissage en situation (Situated Learning) de John Seely Brown, Paul Duguid, Jean Lave, Lucy Suchman et leurs collègues du Palo Alto Institute for Research on Learning dans les années 1980. Selon la théorie de l\u0027action située, la connaissance est étroitement dépen-dante du contexte dans lequel l\u0027action se déroule, l\u0027environnement de l\u0027action constituant donc une ressource déterminante pour les processus cognitifs. Pour tout novice, l\u0027apprentissage d\u0027un métier passe par la participation aux pratiques socioculturelles d\u0027une communauté de praticiens. C\u0027est de ces pratiques concrètes que les capacités de résolution de problème des individus émergent. Dès lors, la connaissance reste pour une large part tacite et contextualisée (Zacklad, 2003).\nPlusieurs définitions ont été proposées pour rendre compte du phénomène mais toutes ne correspondent pas à notre idée du concept. La première définition a été avancée par Lave et Wenger dans leur ouvrage fondateur en 1991 (Lave et Wenger, 1991  (Wenger et al, 2002) ou même plus court comme « groups of people informally bound together by shared expertise and passion for a joint enterprise » (Wenger et Snyder, 2000). Deux points complètent la première définition : le fait que les personnes sont reliées de façon informelle entre elles et le partage de pratiques communes.\nAlors que les premiers travaux concernant les communautés de pratique se focalisaient sur des relations face-à-face, par la suite les études tiennent souvent aussi compte des relaier (2004). tions virtuelles, c\u0027est-à-dire des échanges à distance en mode synchrone ou asynchrone, supportés par les technologies de l\u0027information et de la communication.\nMcDermott définit : « A community of practice is a group that shares knowledge, learns together and creates common practices. » (McDermott, 1999a(McDermott, , 1999b \nD\u0027autres types de communautés en sciences de gestion\nAprès son apparition dans la littérature au début des années 1990, la notion de communautés de pratique a vite rencontré un vif intérêt tant du côté académique que managérial. Par la suite, beaucoup de travaux liés à ce concept ou des concepts similaires ont été ou sont encore publiés. Certains auteurs, appliquent le concept « communauté de pratique », à des groupes dont les caractéristiques sont éloignées de celles des communautés de pratique dans le sens de Lave et Wenger, ou proposent même des extensions du concept dans des directions opposées à celles-ci (Vaast, 2002, Guérin, 2004.\nEn plus de la notion de communauté de pratique, on trouve actuellement une multitude d\u0027autres termes sans que ceux-ci constituent pour autant des concepts scientifiques indépen-dants.\n2 Bien que ces appellations sous-entendent des distinctions a priori claires entre les notions, leurs applications empiriques montrent des proximités et distances entre elles encore peu exploitées par les auteurs ; ceci les rend presque substituables entre elles et ne favorise pas la compréhension de leurs contributions propres (Vaast, 2002). Parmi toutes ces notions, les communautés de pratique représentent le concept de loin le plus développé, concept qui est maintenant généralement reconnu dans la littérature académique ainsi que managériale.\nDimensions retenues\nLes définitions ont fait ressortir les dimensions qui composent une communauté de pratique mbres aux activités de r la base d\u0027une particip . Ces dimensions nous serviront à proposer une définition de travail. La première dimension retenue est la participation volontaire des me la communauté. Contrairement aux entités d\u0027organisation formelle telle que l\u0027équipe de travail où c\u0027est la tâche ou le projet qui construit le groupe, les membres d\u0027une communauté de pratique sont tenus par un intérêt commun dans un champ de savoir.\nLa deuxième dimension mise en avant est le concept de partage. Su ation volontaire, les membres d\u0027une communauté de pratique échangent et partagent des connaissances non seulement explicites, mais aussi et plus particulièrement implicites : des 2 sition… Ces autres « notions voisines » sont par exemple : communauté de métier, communauté épistémique, communauté virtuelle, communauté en ligne, communauté distribuée, communauté d\u0027intérêt, communauté d\u0027apprentissage, communauté d\u0027action, communauté de po expériences, des pratiques des outils, des modèles, etc. A travers cet échange, les membres développent un langage commun et une compréhension partagée de leur environnement professionnel.\nLa troisième dimension évoquée fait référence à la place des communautés de pratique par ération est le mode de communicat différents points théoriques abordés nous amènent à formuler une proposition de dé-fini de personnes liées entre elles par le par rapport à l\u0027organisation formelle. En effet, les membres d\u0027une communauté de pratique peuvent être issus du même service d\u0027une organisation, de différents services au sein de la même organisation, mais aussi de différentes organisations.\nEnfin, la dernière dimension que nous prenons en consid ion. Les membres d\u0027une communauté de pratique communiquent entre eux face-à-face ou virtuellement, c\u0027est-à-dire à distance en mode synchrone ou asynchrone, en utilisant les technologies de l\u0027information et de la communication. A l\u0027heure de la mondialisation et des communautés distribuées, il importe de tenir compte des modes de communication à distance.\nLes tion de travail pour les communautés de pratique :\n« Les communautés de pratique sont des groupes tage de pratiques communes. Sur la base d\u0027un échange volontaire et motivées par un intérêt commun dans un champ de savoir, les personnes appartenant à une communauté partagent des connaissances avant tout implicites, développant peu à peu un langage commun et une identité communautaire. Dans ce but, les membres de la communauté de pratique issus de la même organisation ou bien d\u0027organisations différentes utilisent des modes de communication face-à-face ainsi qu\u0027à distance. »\nréductibilité et l\u0027imprédictibilité des propriétés émergentes.\n3 Selon cette définition, un phé-nomène est émergent si : -Il y a un système d\u0027entités (ensemble d\u0027agents) en interaction dont la description des états et de la dynamique se fait dans un vocabulaire ou une théorie D et n\u0027est pas exprimée dans les termes du phénomène émergent à produire ; -La dynamique des agents en interaction produit un phénomène global qui peut être un processus, une structure stable, une trace d\u0027exécution ou n\u0027importe quel invariant statique ou dynamique ; -Ce phénomène global peut être observé et décrit soit par un observateur extérieur soit par les agents eux-mêmes dans un vocabulaire ou une théorie distincte, c\u0027est-à-dire en des termes distincts de la dynamique sous-jacente.\nL\u0027application de la théorie de l\u0027émergence aux communautés de pratique\nDans le domaine de la gestion de connaissances, on distingue souvent les connaissances individuelles, les connaissances organisationnelles et la relation entre les deux. Lors de la résolution d\u0027un problème, les capacités de tous les individus sont utilisées pour agir ensemble et on constate que cette interaction fait que la connaissance organisationnelle n\u0027est pas que le rassemblement des connaissances individuelles (Probst et al, 2003). La totalité des connaissances individuelles et organisationnelles, y compris les données et les informations sur lesquelles ces connaissances sont fondées, constituent la mémoire organisationnelle (Corporate Memory). L\u0027organisation peut recourir au contenu de cette mémoire lors de la résolution d\u0027une tâche.\nDans le cas des communautés de pratique, analogue à la situation décrite dans l\u0027organisation, les connaissances existant au niveau de la communauté en tant que collectif font bien plus que la somme (dans le sens d\u0027une simple juxtaposition) des connaissances de tous les membres (Brown et Duguid, 1991). Les communautés de pratique possèdent une méthode particulière de résolution de problème. En plus de l\u0027échange de connaissances explicites, les membres de la communauté confrontés à un problème commencent à faire des récits plus ou moins circonstanciés de leurs expériences du passé relatives au problème actuel (« narration » ou « storytelling »). En transmettant des connaissances sur la base de récits, la communauté essaie d\u0027arriver à une solution en accumulant les expériences de ses membres. Il s\u0027agit d\u0027un mode de communication tout à fait humain. Comme le soulignent Boland et Tenkasi (1995), « human cognition operates almost continuously in narrative, storytelling mode ». Par la confrontation de plusieurs points de vue, de différents angles d\u0027attaque, tout en partageant des pratiques communes, les membres de la communauté mettent des connaissances existantes dans de nouveaux contextes et créent ainsi de nouvelles connaissances. Il y a donc ici émergence de connaissance grâce à la communication et l\u0027interaction entre les membres de la communauté de pratique.\nEn quelque sorte, l\u0027entreprise commune 4 et le répertoire partagé 5 constituent des résultats émergents des communautés de pratique. L\u0027entreprise commune émerge d\u0027un processus collectif permanent de négociation qui reflète la complexité de la dynamique de l\u0027engagement mutuel des membres de la communauté. Elle désigne l\u0027identité commune : « What the community is about ».\nLe répertoire partagé émerge également grâce à la communication et l\u0027interaction entre les membres de la communauté de pratique. Il s\u0027agit de l\u0027ensemble de ressources accumulées propres à une communauté de pratique particulière. Par la suite, ces ressources regroupant des supports physiques tels que des prototypes, des routines, des mots, des gestes, des symboles, des outils et des concepts etc. sont mises en commun pour favoriser la poursuite des buts à atteindre. De cette façon, les individus peuvent se servir du répertoire partagé déjà acquis pour ensuite créer de nouvelles connaissances.\nCes phénomènes émergent donc au niveau macro (la communauté comme ensemble) à partir d\u0027une interaction entres les membres de la communauté de pratique (niveau micro).\nD\u0027ailleurs, il y a ici un parallèle entre le phénomène de l\u0027émergence et les communautés de pratique : les deux ne se pilotent pas, mais on peut tenter de créer des conditions qui sont favorables au développement des interactions et par là favorables à l\u0027émergence d\u0027éléments nouveaux. Cette notion de faire-émerger nous amène à l\u0027énaction (Varela 1989) qui par le poids qu\u0027elle donne à l\u0027action pourra nous offrir un éclairage complémentaire que nous pensons étudier dans une prochaine étape.\nProposition de modélisation\nAvant de procéder à une proposition de modélisation pour un support de la création et de l\u0027émergence de connaissances, il convient de considérer les caractéristiques des communautés de pratique pour en déduire les différents éléments à prendre en compte.\nPremièrement, la communication et les interactions personnelles entre les membres d\u0027une communauté sont essentielles pour le partage de connaissances. Il importe de bien connaître les autres membres avec lesquels on interagit, d\u0027avoir des informations sur eux : l\u0027échange de connaissances dans une communauté de pratique n\u0027est pas seulement le ramassage et la mise à disposition anonyme de connaissances ; au contraire, l\u0027échange se situe entre un producteur et un consommateur de connaissances, entre un auteur et un lecteur, en d\u0027autres termes, comme Zacklad (2004) le formule, entre une situation source et une situation cible. Ainsi, le lien avec l\u0027auteur est surtout important pour l\u0027interprétation des énoncés souvent subjectifs. Un support des communautés de pratique devrait donc permettre de gérer les profils des membres ce qui permet de déceler « qui sait quoi » dans une communauté de pratique.\nIl est à noter qu\u0027en instituant ces profils, notre modélisation part de l\u0027hypothèse que les connaissances ne sont pas distribuées uniformément parmi les membres 6 et que tous les acteurs d\u0027une communauté de pratique ne sont donc pas égaux. En effet, il existe selon  Dans ce cadre, l\u0027entreprise commune peut dépasser les frontières d\u0027une société et être entendue comme un projet partagé par les membres de la communauté.\n5\nCes notions sont issues des travaux de Wenger (1998). Selon lui, les communautés de pratiques sont caractérisées par trois dimensions : un engagement mutuel, une entreprise commune et un répertoire partagé.\n« A community\u0027s knowledge is not held equally by all, but shared differentially across the community as a whole, though it is made available to all. », cf. Brown et Duguid (2001).\nger plusieurs niveaux de participation au sein d\u0027une communauté. L\u0027idée centrale du concept de « Legitimate Peripheral Participation » est que les personnes acquièrent les connaissances nécessaires pour avoir une performance compétente en devenant « insiders » ou membres légitimes de la communauté en question (Lave et Wenger, 1991, Lorenz, 2001). Au début, les nouveaux entrants se trouvent à la périphérie de la communauté, observant le comportement des membres anciens. En participant aux discussions, ils acquièrent peu à peu le langage et la conception du monde de la communauté. Avec le temps, ils deviennent des membres expérimentés capables de transférer des connaissances aux débutants. Le but est donc que beaucoup de membres profitent des contributions des experts de la communauté.\nUne deuxième caractéristique concerne les relations entre différentes communautés de pratique. Il peut y avoir des problèmes si une communauté se concentre trop sur elle-même et s\u0027isole ainsi. Dans ce cas, il faut encourager l\u0027échange inter-communautaire dans lequel les communautés de pratique présentent leurs idées dans un contexte social plus vaste. Il est possible de surmonter les barrières entre des communautés à travers des « Knowledge Brokers », c\u0027est-à-dire des individus qui appartiennent à plusieurs communautés, ou bien à travers des « objets frontière » (Boundary Objects), des objets qui sont intéressants pour chaque communauté impliquée, mais qui sont utilisés différemment. Dans une modélisation, il faut dès lors tenir compte du fait qu\u0027un individu ou bien un objet puisse appartenir à plusieurs communautés de pratiques.\nEnfin, le modèle doit permettre l\u0027échange des connaissances implicites ainsi qu\u0027explicites. Dès lors, des composants sont à prévoir pour sauvegarder la partie formalisable du répertoire partagé et pour faciliter la communication des connaissances implicites. Vu sous un autre angle d\u0027attaque, il faut supporter la communication directe entre les membres, mais aussi la communication indirecte, c\u0027est-à-dire proposer des possibilités pour la publication et la recherche d\u0027informations.\nBasé sur ces réflexions préliminaires, nous proposons ci-dessous un modèle générique de système pour un soutien des communautés de pratique. Dans la partie suivante seront décrits les différents composants et les interactions entre eux.\nInterface A travers l\u0027interface, le membre utilise les différents services et fonctionnalités proposés par les autres composants du système. Il a non seulement accès d\u0027une manière passive au système, mais il peut aussi contribuer activement, par exemple en mettant à jour des informations, en ajoutant de nouvelles informations ou en communiquant avec d\u0027autres membres.\nProfils des membres\nCe composant contient une liste de tous les membres de la communauté de pratique qui sont enregistrés comme utilisateurs. Surtout dans des communautés de pratique virtuelles, en l\u0027absence de contacts face-à-face, la mise à disposition des profils est une mesure pour établir de la confiance entre les membres.\nIl y a trois possibilités pour l\u0027établissement et la modification (la mise à jour) de ces profils. Premièrement, c\u0027est l\u0027utilisateur lui-même qui entre des informations dans le système. Cela concerne surtout des informations personnelles (nom, prénom, adresse etc.). De surcroît, l\u0027utilisateur pourra donner des informations concernant sa qualification et ses intérêts. En ce qui concerne les intérêts du membre, on pourra cependant envisager de les compléter automatiquement.\nDeuxièmement, le profil peut être modifié et complété automatiquement, c\u0027est-à-dire de façon assistée par le système. Dans ce cas, le système surveille le comportement et les activités du membre au sein de la communauté : quels autres membres il contacte le plus souvent, RNTI -X -quels items de la base de connaissances il consulte ou même ajoute et comment il les évalue (soit explicitement en donnant une « note », soit implicitement en le regardant plus ou moins souvent ou plus ou moins longtemps), s\u0027il participe vivement aux discussions du forum etc.\nEnfin, le profil d\u0027un individu peut -de façon limitée -être modifié par les autres membres si ceux-ci évaluent les contributions de l\u0027individu aux activités de la communauté, par exemple relatives aux articles qu\u0027un membre particulier a rédigés.\nEn principe, le composant gérant les profils des membres possède trois fonctions.  \nFIG. 1 -\nModèle du système\ne système proposé permet une « personnal à-dire donne à chaque individu le sys L isation », c\u0027estla possibilité de modifier des paramètres traduisant ses intérêts ; par la suite, ce membre recevra une offre adaptée à ses besoins au niveau de messages et du contenu de la base de connaissances. Il s\u0027agit d\u0027une personnalisation explicite, initiée par le membre lui-même.\nEn plus, on pourrait envisager une personnalisation automatique et implicite où c\u0027est tème qui fouille et exploite le profil et les activités d\u0027un membre pour en déduire des informations qui complètent le profil. Ce principe est connu du domaine des systèmes de filtrage adaptatif (ou système de recommandation), dont l\u0027objectif principal est d\u0027envoyer des informations pertinentes aux utilisateurs tout en s\u0027adaptant en permanence à leur besoin d\u0027information.\nLa fonction « Matching » consiste à proposer des partenaires potentiellement intéressants pour un membre concerné, et ce, soit sur demande du membre, soit automatiquement par le système. Pour faire cela, le système compare les profils des membres et cherche en fonction de la situation particulière des similitudes ou bien des profils « complémentaires » qui enrichissent et comblent en quelque sorte les lacunes d\u0027autres profils.\nDans les profils des membres, le système gére également les droits de chaque individu quant à l\u0027accès aux informations et aux modifications. Alors que la plupart des membres ne sont autorisés qu\u0027à modifier ou effacer les items qu\u0027ils ont générés eux-mêmes et uniquement ceux-ci, un ou plusieurs membres prennent le rôle d\u0027administrateur, doté de droits plus étendus, s\u0027il est probable que ces membres soient experts, tous les experts ne seront pas administrateurs.\nitems.\nBase de connaissances\nCe composant sert à sauvegarder la partie formalisable du répertoire partagé. Il s\u0027agit de l\u0027ensemble de ressources accumulées propres à une communauté de pratique particulière. Ce sont des items tels que des documents, des fichiers audio ou vidéo, des outils et des concepts… dont les individus peuvent se servir pour ensuite créer de nouvelles connaissances. Il importe que le lien entre l\u0027auteur qui publie et l\u0027item enregistré ou publié soit maintenu et facile à suivre puisqu\u0027il s\u0027agit souvent d\u0027éléments subjectifs au niveau de la communauté qui ont un fort rapport avec une certaine personne, une situation ou un contexte bien particulier. Les fonctionnalités à prévoir concernent la création, la requête, la mise à jour (la modification) et l\u0027effacement des Par ailleurs, deux modes d\u0027accès semblent souhaitables : un mode « Pull » où l\u0027utilisateur prend l\u0027initiative en cherchant des informations ; un mode « Push » (couplé avec le composant messagerie) où le système met pro activement des informations ciblées ou des contacts potentiellement intéressants à la disposition de l\u0027utilisateur sans que celui-ci les ait sollicités explicitement.\nMessagerie Ce composant réalise la communication entre les membres et entre le système et les membres à travers différents canaux : d\u0027abord, il y a la communication directe asynchrone entre des membres qui envoient des messages directement aux destinataires qu\u0027il souhaitent contacter. Outre le support du processus de l\u0027envoi du message, le système ne constitue aucune aide. En revanche, le système peut offrir une assistance lors du choix des destinataires : en fonction du contenu, le système propose à l\u0027utilisateur de rajouter automatiquement des destinataires. Enfin, le système peut aussi créer des messages automatiquement en fonction de ce qui se passe au niveau de la communauté de pratique (nouveaux items dans la base de connaissances, modification dans les profils). Dans ce cas, il s\u0027agit du mode « Push » cité cidessus : le système propose des informations à l\u0027utilisateur qui pourraient être intéressantes pour lui selon son profil et selon les attributs du message.\nForum d\u0027échange\nIl s\u0027agit d\u0027un ou plusieurs forums plus ou moins formels dans lesquels les membres de la communauté de pratique peuvent discuter de différents sujets. C\u0027est surtout dans ce cadre que les personnes échangent des expériences à travers des récits (« storytelling »). Pour instrumentaliser ce phénomène particulier, le concept des systèmes de raisonnement à base de cas nous semble particulièrement adapté aux communautés de pratique. En effet, ce travail fondé sur les échanges de récits présente des analogies avec le raisonnement à base de cas qui peuvent se révéler très constructives. La base de cas d\u0027un tel système (comme instrumentalisation du composant base de connaissances) pourrait constituer le lieu de conservation et d\u0027archivage des résolutions de problèmes déjà traités qui sont emmagasinées au sein d\u0027une mémoire accessible à tous les membres de la communauté.\nComposant d\u0027échange\nCe composant permet des échanges entre les systèmes de différentes communautés de pratique. Il s\u0027agit d\u0027items de la base de connaissances ou bien de profils de membres qui appartiennent à plusieurs communautés.\nEnfin, il est à noter qu\u0027il existe également une communication entre les membres de la communauté de pratique hors du système, c\u0027est-à-dire un échange de connaissances qui a lieu sans passer par les composants du système informatique. Il s\u0027agit par exemple de rencontres face à face, de conversations téléphoniques ou -même si probablement moins souvent de nos jours -de correspondance. Le problème du point de vue informatique, c\u0027est que les connaissances échangées par cette voie ne laissent aucune trace dans le système informatique et qu\u0027on ne peut pas les exploiter.\nConclusion\nCe travail vise à considérer le processus de l\u0027émergence de connaissances dans les communautés de pratique. La considération des caractéristiques de ces communautés nous a conduit à une définition de travail qui tient compte du caractère de plus en plus virtuel des communautés de pratique dont les membres peuvent être géographiquement distants. Ce fait influence aussi la manière d\u0027utiliser les technologies de l\u0027information et de la communication. En effet, surtout pour les échanges à distance, le support par des TIC semble indispensable. Ensuite, la création de nouvelles connaissances dans les communautés de pratique a été analysée sous l\u0027angle de la théorie de l\u0027émergence.\nSur cette base, nous avons proposé une modélisation pour un support de l\u0027émergence de connaissances. Cette modélisation part de l\u0027hypothèse que tous les acteurs d\u0027une communauté de pratique ne sont pas égaux car les connaissances ne sont pas distribuées uniformément parmi les membres : un composant qui sauvegarde les profils des membres permet de localiser des experts pour le sujet abordé dans le but de profiter des connaissances de ces membres expérimentés.\nNéanmoins, notre approche possède certaines limites. Tout d\u0027abord, en proposant une modélisation pour un support de l\u0027émergence et de la création de connaissances dans les communautés de pratique, notre approche suppose qu\u0027il est possible d\u0027influencer les activités au sein d\u0027une communauté. Cette démarche s\u0027inscrit donc dans une logique d\u0027encourager et favoriser le développement de processus d\u0027échange entre des acteurs volontaires. Toutefois, dans le cadre du présent travail, nous ne considérons pas encore des aspects liés à l\u0027acceptation des outils des technologies de l\u0027information et de la communication par les utilisateurs.\nDeuxièmement, la modélisation considère la base de connaissances comme un lieu central de sauvegarde de connaissance. Cette approche se heurte à des limites dès que la base de connaissances devient trop volumineuse. A l\u0027heure des communautés de pratique virtuelles, il est tout à fait possible que les communautés arrivent à des centaines de membres, voire plus. Dans ce cas, il faudrait envisager une sauvegarde décentralisée/distribuée des connaissances. Les profils des membres pourraient eux aussi être sauvegardés de façon distribuée. Cette configuration semble suggérer l\u0027utilisation des systèmes multi-agents. Par exemple, un agent pourrait détenir le profil de l\u0027individu auquel il est attaché. Lorsqu\u0027un membre de la communauté de pratique fait une recherche d\u0027expertise, c\u0027est son agent qui questionne les autres agents de la communauté pour ensuite échanger des informations sur les profils de différents utilisateurs.\nLes perspectives de recherche consistent donc à approfondir notre modélisation et à mettre en oeuvre un prototype de gestion de connaissances au sein des communautés de pratique qui permettra une validation de nos propositions.\n"
  },
  {
    "id": "947",
    "text": "Aperçu du processus d\u0027enrichissement\nLe processus d\u0027enrichissement que nous avons proposé (Faïz et Mahmoudi, 2005, Mahmoudi et Faïz, 2006 ) émane d\u0027un besoin informationnel réclamé par les utilisateurs des SIG. Pour extraire les connaissances incarnées dans les documents dans des temps raisonnables, nous procédons d\u0027une manière distribuée en adoptant le paradigme multi-agents (Ferber, 1997).\nL\u0027approche que nous proposons est modulaire, elle peut être décomposée en trois grandes phases. Il s\u0027agit de la segmentation et de l\u0027identification des thèmes abordés dans les documents initiaux. Suite à cette phase, un nouveau document est généré pour chaque thème regroupant les segments de textes distribués entre les différents agents et traitant le même thème. La seconde phase consiste à affecter pour chaque thème un délégué responsable de l\u0027extraction de l\u0027essentiel d\u0027information de son document généré. Enfin, un filtrage textuel s\u0027opère, il consiste à éliminer toute portion de texte qui s\u0027avère inutile à la compréhension du thème (Mahmoudi et Faïz, 2006 a ).\nSDET : Un outil pour l\u0027enrichissement des données\nNotre approche a été mise en oeuvre pour permettre un support informationnel pour les utilisateurs de SIG. L\u0027implémentation de notre approche a été réalisée en utilisant le langage Java. Ce choix est fondé sur le fait que Java supporte le multi-threading qui favorise le traitement parallèle entre les différents agents du système. Les fonctionnalités que nous proposons ont été intégrées à un SIG OpenSource : Open Jump 1 (Java Unified Mapping Platform). Ce SIG supporte les principaux standards industriels comme GML ainsi que le modèle objet spatial de l\u0027OpenGIS Consortium. Open Jump offre une grande modularité et de nombreuses extensions.\nSDET est une suite de fonctionnalités facilitant l\u0027extraction de nouvelles données qui viennent enrichir celles déjà existantes dans la Base de Données Géographiques (BDG). Nous procédons comme suit : L\u0027utilisateur du SIG à la quête d\u0027informations, va solliciter en premier lieu la BDG du système. En cas d\u0027insatisfaction (information non stockée dans la BDG, détails insuffisants…), l\u0027utilisateur peut faire appel à notre outil d\u0027enrichissement. Ceci est rendu possible en ajoutant une nouvelle option au SIG libre : Open Jump. L\u0027enrichissement est initié par une collecte de corpus de documents. A partir de ces documents bruts, nous extrayons la liste des thèmes traités et leurs documents générés associés.\nL\u0027outil propose trois manières pour condenser les documents générés : Le résumé d\u0027un segment sélectionné par l\u0027utilisateur, le résumé de l\u0027intégralité du document généré ou le résumé de l\u0027intégralité du document généré, mais, avec un filtrage appliqué aux segments afin d\u0027éliminer toute redondance d\u0027information. Les résumés résultants peuvent être affichés et stockés pour une consultation ultérieure.\nRéférences Faïz, S. et K. Mahmoudi (2005) \nSummary\nDatabase enrichment is a mean to provide complementary data to end users. In a geograhic context, this is essential to make accurate decisions. SDET is our tool built to perform the data enrichment that we integrated to an open GIS.\n"
  },
  {
    "id": "949",
    "text": "Evaluer les catégorisations\nLa validation manuelle n\u0027est pas forcément toujours faisable ou souhaitable. C\u0027est pourquoi il convient de prendre en considération des méthodes automatiques quantitatives afin de donner une idée de la qualité des catégorisations. Nous nous basons sur la distinction entre critères \"externes\" et \"internes\" faite par Halkidi et al. (2002). Alors que les premiers reposent sur l\u0027hypothèse d\u0027une partition idéale des données (étiquettes données par l\u0027utilisateur, par exemple), les seconds n\u0027utilisent aucune information a priori pour juger de la qualité des catégorisations. C\u0027est cette seconde approche que nous avons choisi d\u0027adopter dans notre logiciel.\nContrairement à l\u0027approche externe, aucun étiquetage préalable des données ne permet ici de comparer le résultat du clustering à un quelconque modèle idéal. De nombreux indices de validité ont été proposés et des travaux récents attestent de la vitalité de cette perspective de recherche. Ils se basent sur la recherche, thème classique en apprentissage non supervisé, d\u0027un compromis entre les principes de similarité intra-classe et de dissimilarité inter-classes. Des indices caractéristiques de cette approche interne sont les indices de Dünn, Davies-Bouldin et Hubert modifié, qui ont été implémentés dans notre logiciel.\nLogiciel et expérimentations\nL\u0027objectif du logiciel que nous proposons est d\u0027aider l\u0027utilisateur à comparer différentes partitions d\u0027un même jeu de données sur la base de critères internes. Ces partitions peuvent être les résultats obtenus à l\u0027aide d\u0027un ou de plusieurs algorithmes de classification automatique, tels les k-means ou EM. Les données d\u0027entrée sont, d\u0027une part, la définition du langage de description et des exemples d\u0027apprentissage décrits à l\u0027aide de ce langage, et, d\u0027autre part, les partitions qui feront l\u0027objet de la comparaison. L\u0027évaluation repose sur trois composantes : l\u0027indice utilisé, la mesure de distance (ou de similarité) choisie, ainsi que la normalisation effectuée sur les attributs numériques. Le logiciel permet de lancer plusieurs évaluations en même temps et propose, en sortie, une visualisation des résultats obtenus. La visualisation est différente suivant que l\u0027on traite un ou plusieurs critères. De plus, le caractère évolutif de notre logiciel donne l\u0027opportunité d\u0027ajouter très facilement de nouveaux indices ou de nouvelles distances.\nLa figure ci-dessus présente les résultats obtenus avec quatre algorithmes (k-means, EM, Farthest-first et PRESS) sur la célèbre base \"vote\" du répertoire UCI. Elle permet de constater la supériorité de l\u0027un des algorithmes dans le cas mono-critère (indice de Davies-Bouldin), ici celui qui a obtenu la plus petite des aires. Le cas multi-critères, par contre, semble indiquer deux types de résultats distincts. L\u0027utilisation de notre logiciel peut ainsi suggérer à l\u0027utilisateur d\u0027étudier plus attentivement les raisons de cette différence.\nConclusion et perspectives\nNous présentons un logiciel pour aider l\u0027utilisateur à comparer les résultats obtenus par des algorithmes de classification. La caractéristique principale de ce travail est son caractère évolutif : ajout de nouveaux indices, de nouvelles distances, etc. Dans les perspectives à court terme, nous souhaitons étendre le logiciel aux indices externes, tels la F-mesure ou les fonctions entropiques. A plus long terme, cet outil devrait nous permettre de comparer, non plus les partitions ou les algorithmes, mais directement les critères de pertinence. Ces derniers pourraient alors être regroupées et mis en relation avec la nature des données traitées (données clairsemées, bruitées, à grande dimension, etc.). Ceci devrait mener à une contribution concernant l\u0027évaluation des techniques d\u0027apprentissage non supervisé, évaluation qui présente encore de réelles difficultés au jour d\u0027aujourd\u0027hui.\nSummary\nThis paper details a software that can assist the user for clustering comparison. It gives a clear visualization of different criteria (Dunn, Silhouette, etc.) calculated on one or more partitions of the data. The main feature is its modularity in three components: a quality criterion, a comparison measure and a normalization on numerical attributes. Furthermore, it allows the user to add its own items into those components.\n"
  },
  {
    "id": "950",
    "text": "Introduction\nDans les méthodes qui génèrent des règles de décision du type Si condition Alors Conclusion comme les arbres de décision (Breiman et al., 1984;Quinlan, 1993), les graphes d\u0027induction (Zighed et Rakotomalala, 2000),... les mesures d\u0027entropie sont fréquemment utilisées. Or celles-ci reposent sur de nombreuses hypothèses implicites qui ne sont pas toujours justifiées.\nLes mesures d\u0027entropie ont été définies mathématiquement par un ensemble d\u0027axiomes en dehors du contexte de l\u0027apprentissage machine. On peut trouver des travaux détaillés dans Rényi (1960), et Aczél et Daróczy (1975). Leur transfert vers l\u0027apprentissage s\u0027est fait de manière peut-être hâtive et mérite d\u0027être revu en détail.\nLe présent travail examine et discute des propriétés des entropies dans le cadre des arbres d\u0027induction.\nDans la section suivante, nous fixons quelques notations et rappelons le contexte d\u0027utilisation des mesures d\u0027entropie. Dans la section 3, nous présentons les mesures d\u0027entropie et discutons leurs propriétés et leurs conséquences dans les processus d\u0027induction. Dans la section 4, nous proposons une axiomatique conduisant à une nouvelle mesure d\u0027entropie.\nNotations, définitions et concepts de base\nNous nous plaçons dans le cadre des arbres de décision qui font explicitement appel aux entropies pour mesurer la qualité de la partition induite en apprentissage.\nSoit ? la population concernée par le problème d\u0027apprentissage. Le profil de tout individu ? de ? est décrit par p variables, X 1 , . . . , X p , dites variables exogènes ou variables explicatives. Ces variables peuvent être qualitatives ou quantitatives.\nNous considérons également la variable à prédire C, parfois appelée variable endogène ou variable classe ou encore variable réponse. L\u0027ensemble des valeurs prises par cette variable sur la population est un ensemble discret et fini noté C. On note par m j le nombre de valeurs différentes prises par X j et par n le nombre de modalités de C. Ainsi, C \u003d {c 1 , . . . , c n }. Et s\u0027il n\u0027y a pas d\u0027ambiguïté, on notera la classe c i simplement par i.\nL\u0027objectif d\u0027un algorithme d\u0027induction d\u0027arbre est de générer un modèle ?(X 1 , . . . , X p ) de prédiction de C que l\u0027on représente par un arbre de décision. Chaque branche de l\u0027arbre repré-sente une règle. L\u0027ensemble des règles forme le modèle de prédiction qui permet de calculer, pour un nouvel individu dont on ne connaît que les variables exogènes, l\u0027état de la variable endogène. Le développement de l\u0027arbre s\u0027effectue selon un schéma simple : l\u0027ensemble d\u0027apprentissage ? a est segmenté itérativement, à chaque fois selon une des variables exogènes X j ; j \u003d 1, ...p de sorte à engendrer la partition de plus faible entropie sur la distribution de C. Les sommets obtenus à chaque itération définissent une partition sur ? a . Plus l\u0027arbre grandit, plus la partition devient fine. Le sommet à la racine de l\u0027arbre représente la partition grossière.\nChaque sommet s d\u0027une partition S est caractérisé par une distribution de probabilités des modalités de la variable endogène C : p(i/s); i \u003d 1, . . . , n.\nDans les arbres d\u0027induction, l\u0027entropie H sur la partition S à minimiser est généralement une entropie moyenne calculée comme suit :\n. . , p(n/s)) est par exemple l\u0027entropie de Shannon dont l\u0027expression est donnée plus loin, et p(s) la proportion de cas dans le sommet s.  Khinchin (1957), Forte (1973) et Aczél (1973 ont fondé une axiomatique.\nEntropie de Shannon\nSoit une expérience E avec les événements possibles e 1 , e 2 , . . . , e n de probabilités respectives p 1 , p 2 , . . . , p n . On suppose que\nShannon de la distribution de probabilité est donnée par la formule :\nPar continuité on pose 0 log 2 0 \u003d 0. D\u0027autres mesures d\u0027entropie existent (Zighed et Rakotomalala, 2000).\nPropriétés théoriques des mesures d\u0027entropie\nOn considère que (p 1 , p 2 , . . . , p n ) pour n ? 2 est pris dans un ensemble fini de distributions de probabilités et on considère le simplexe d\u0027ordre n n ? n \u003d {(p 1 , p 2 , . . . , p n ) :\nUne mesure d\u0027entropie est définie comme suit :\navec les propriétés suivantes :\noù ? est une permutation quelconque sur (p 1 , p 2 , . . . , p n ).\nStricte concavité La fonction h(p 1 , p 2 , . . . , p n ) est strictement concave.\nAinsi, l\u0027évaluation de l\u0027entropie h d\u0027une partition S nécessite la connaissance de p(i/s); i \u003d 1, . . . , n; ?s ? S.\nMesure d\u0027entropie pour l\u0027apprentissage inductif\nLes propriétés des mesures d\u0027entropie que l\u0027on vient de lister ne nous paraissent pas adaptées à l\u0027apprentissage inductif. En effet, d\u0027une part l\u0027incertitude maximale ne correspond pas nécessairement à la distribution uniforme, ainsi dans le cadre de la détection de transactions frauduleuses peu fréquentes il peut par exemple être opportun de conclure à une fraude dès que la probabilité de celle-ci dépasse un seuil de disons 10%, voire moins. D\u0027autre part, dans la pratique, le calcul de l\u0027entropie repose sur des probabilités estimées et devrait donc tenir compte de leur précision et donc de la taille de l\u0027échantillon. C\u0027est pourquoi nous proposons une nouvelle axiomatique que nous justifions très brièvement et dont l\u0027objectif est d\u0027aboutir à une entropie, que l\u0027on pourrait qualifier d\u0027empirique, qui tient mieux compte de ces considé-rations pratiques.\nPropriétés requises\nSoit la nouvelle fonction d\u0027entropie que nous voulons bâtir. Nous voulons qu\u0027elle soit empirique, c\u0027est-à-dire fonction des fréquences f (i/.), sensible à la taille N de l\u0027échantillon sur lequel elles sont calculées et qu\u0027elle soit également paramétrée par une distribution W \u003d (w 1 , . . . , w j , . . . , w p ) où elle sera maximale.\n:\nOn notera, pour une distribution W fixée, W (N, f 1 , . . . , f i , . . . , f n ). Nous souhaitons que possède les propriétés suivantes :\nP1 : Non négativité La fonction doit être à valeur non négative\nP2 : Maximalité Soit W \u003d (w 1 , w 2 , . . . , w n ) une distribution fixée par l\u0027utilisateur comme étant la moins souhaitée et donc d\u0027entropie maximale. Ainsi, pour N fixé,\npour toute distribution (f 1 , . . . , f n ) de taille n. P3 : Asymétrie La nouvelle propriété de maximalité remet en cause l\u0027axiome de symétrie requis par les entropies classiques. Par conséquent, certaines permutations ? pourraient affecter la valeur de l\u0027entropie : 1 , . . . , f n ) \u003d ?1 , . . . , f ?n ). On peut facilement identifier les conditions dans lesquelles la symétrie serait conservée comme par exemple les cas où certains w i seraient identiques et a fortiori dans le cas de la distribution uniforme. P4 : Minimalité Dans le contexte classique, l\u0027entropie est nulle dans le cas où la distribution est concentrée en une seule classe, les autres étant vides, c\u0027est-à-dire lorsqu\u0027il existe j tel que p j \u003d 1 et que p i \u003d 0 pour tout i \u003d j. Cette propriété doit en effet demeurer valide sur le plan théorique. Seulement, en apprentissage ces probabilités sont inconnues. Il serait quand même gênant de dire que l\u0027entropie est nulle dès lors que la distribution est concentrée en une classe. Il faut prendre en considération la taille de l\u0027échantillon qui sert à estimer les p j . On exige simplement que l\u0027entropie d\u0027une distribution empirique pour laquelle il existe j tel que f j \u003d 1, tende vers 0 quand N devient grand, soit\nP5 : Consistance Pour un W donné et à distribution fixée, l\u0027entropie devrait être plus faible sur un effectif plus grand. \nest une mesure d\u0027entropie pour processus d\u0027apprentissage inductif qui vérifie les propriétés P1 à P5.\nLe graphique 1 visualise la forme de cette entropie pour deux classes avec un vecteur de maximalité W \u003d (0.3; 0.7) et différentes valeurs de N \u003d 5, 10, 50, ...\nConclusion\nDans ce travail nous avons défini une nouvelle fonction d\u0027entropie qui possède, sur un plan théorique et algorithmique de bonnes propriétés. Cette fonction repose sur le choix du paramètre W . L\u0027idée la plus simple pour fixer W est de prendre la distribution a priori observée sur l\u0027échantillon d\u0027apprentissage. En effet, si l\u0027utilisateur met en oeuvre des techniques d\u0027apprentissage c\u0027est pour s\u0027éloigner le plus possible de la distribution a priori. Il est donc naturel qu\u0027elle soit associée à la plus forte entropie. Faute de place, nous n\u0027avons pas pu reporter toutes les critiques que nous pouvons formuler sur l\u0027utilisation des entropies classiques en apprentissage. L\u0027intérêt de notre approche est qu\u0027elle permet de répondre de façon assez simple et sans trop de\n"
  },
  {
    "id": "951",
    "text": "Introduction\nL\u0027évaluation des performances d\u0027un modèle constitue l\u0027étape finale de tout processus d\u0027apprentissage supervisé. Elle est le retour nécessaire à l\u0027utilisateur pour le guider dans la poursuite de sa fouille de données. Ces mesures, comme celles utilisées pour bâtir des arbres de décisions, sont généralement symétriques. De façon pratique, on entend par symétrique le fait que les erreurs sur chaque modalité de la variable endogène se voient attribuer une importance similaire. Or de nombreux exemples industriels nous montrent que cela n\u0027est pas toujours le cas, en particulier lorsqu\u0027on se trouve en présence de jeux de données fortement déséquilibrés : aide au diagnostic (Grzymala-Busse, 2000), identification de phénomènes inhabituels comme les fraudes lors des transactions par cartes bancaires (Chan, 2001) ou les pannes d\u0027équipements de télécommunications (Weiss, 1998), et bien d\u0027autres encore. Dans ce type de cas l\u0027objectif principal est d\u0027identifier les instances représentatifs de la classe minoritaire. Il est pour cela nécessaire d\u0027utiliser des méthodologies d\u0027apprentissage adaptées (Weiss, 2004) (Japkowicz, 2000), comme notamment les méthodologies sensibles au coût (Domingos, 1999) ou celles basées sur des techniques d\u0027échantillonnage (Chawla, 2002), mais l\u0027évaluation des performances des modèles résultant doit également prendre en considération cet aspect non symétrique de l\u0027importance des modalités, sans se limiter à un simple taux de correction global. Une évaluation locale, c\u0027est-à-dire par modalité, doit alors être conduite. Le taux de rappel et le taux de précision sont les deux indicateurs de base des performances d\u0027un modèle vis-à-vis d\u0027une modalité. Il est également possible de fusionner ces deux critères en utilisant par exemple la f-measure (Van Rijsbergen, 1979) ou de créer d\u0027autres critères utilisant le dénom-brement de chaque type d\u0027erreurs (insertion ou omission) (Makhoul, 1999). Nous proposons dans cet article un critère appelé PRAGMA (Precision and RecAll rates Guided Model Assessment), pouvant servir à la fois pour l\u0027évaluation de modèles d\u0027apprentissage supervisé ou encore de critère utilisé pour bâtir des arbres de décision, prenant en compte l\u0027ensemble de ces aspects. Nous proposons ensuite une évolution des modèles de type forêts aléatoires utilisant ce critère pour les jeux de données fortement déséquilibrés.\nPRAGMA : Precision and RecAll rates Guided Model Assessment\nPRAGMA utilise deux principes : la notion d\u0027importance d\u0027une classe et la notion de pré-férence entre taux de rappel et taux de précision pour chaque classe. Tout d\u0027abord l\u0027importance d\u0027une classe est représentée par un coefficient ? i fixé par l\u0027utilisateur et utilisé en fin d\u0027évaluation. Ensuite pour chaque classe, nous évaluons le modèle en fonction de son taux de rappel (r i ) et de son taux de précision (p i ). Cette fonction f (r i , p i ), que nous cherchons à minimaliser par analogie avec le nombre d\u0027erreurs d\u0027un modèle, doit avoir les propriétés suivantes :\n(1) f (0, 0) \u003d 1 , on fixe la valeur de la pire situation (r i \u003d 0 et p i \u003d 0).\n(2) f (1, 1) \u003d 0 , on fixe la valeur de la meilleure situation (r i \u003d 1 et p i \u003d 1).\n(3)\ndf (r,p) dr \u003c 0, r ? [0; 1] , à taux de précision égal, la mesure doit diminuer lorsque le taux de rappel augmente. (4) df (r,p) dp \u003c 0, p ? [0; 1] , à taux de rappel égal, la mesure doit diminuer lorsque le taux de précision augmente.\nUne telle fonction peut avoir la forme suivante :\nPour prendre en compte les souhaits de l\u0027utilisateur en terme de préférence entre le taux de rappel et le taux de précision, nous décidons de pondérérer à la fois r i et p i :\nLe ratio ?/? détermine la préference entre le rappel et la précision (plus celui-ci est grand (supérieur à 1), plus le rappel est préféré ; plus celui-ci est petit (inférieur à 1), plus la précision est préférée ; s\u0027il est égal à 1, cela signifie qu\u0027aucune distinction n\u0027est faite entre le rappel et la précision). Pour déterminer, de manière instinctive et compréhensible, ces deux paramètres, l\u0027utilisateur doit définir deux situations extrêmes qu\u0027il juge de qualité équivalente. En pratique, ces deux situations sont : (a) celle où le taux de rappel est parfait (r i \u003d 1) et (b) celle où le taux de précision est parfait (p i \u003d 1). Il implique donc à l\u0027utilisateur de définir deux valeurs x et y tel que f (1, x) \u003d f (y, 1). Choisir ces deux valeurs peut être considéré comme répondre aux deux questions suivantes : Quel compromis êtes-vous prêt à faire vis-à-vis de la précision pour avoir un taux de rappel de 1 ? (répondre à cette question permet de définir x, avec 0 ? x \u003c 1)(a) Quel compromis êtes-vous prêt à faire vis-à-vis du rappel pour avoir un taux de précision de 1 ? (répondre à cette question permet de définir y, avec 0 ? y \u003c 1)(b) Avec cette dernière contrainte (5) f (1, x) \u003d f (y, 1), nous pouvons déterminer les paramètres ? et ? :\nLa fonction f utilisée pour évaluer localement un modèle selon son rappel et sa précision sur une modalité est la suivante :\nCette fonction correspond à l\u0027équation d\u0027un plan où l\u0027axe défini par les points (0,0,1) et (1,1,0) est fixe, et où le ratio ?/? détermine l\u0027orientation du plan autour de cet axe (la préférence entre le rappel et la précision)( fig. 1). Ces évaluations locales (propres à chaque modalité) sont ensuite combinées lors de l\u0027évalua-tion finale à l\u0027aide d\u0027une moyenne pondérée où les coefficients d\u0027importance de chaque classe constituent les pondérations :\nEvolution des modèles de type forêts aléatoires\nUne forêt aléatoire (ou Random Forest RF) (Breiman, 2001) est un ensemble d\u0027arbres de classification de profondeur maximale, chacun construit à partir d\u0027un échantillon bootstrap du jeu d\u0027apprentissage. De plus, pour l\u0027obtention de chaque noeud on limite la recherche de la meilleure discrimination à k variables tirées au sort. La prédiction pour un objet est obtenue en comptabilisant les prédictions de chaque arbre pour l\u0027objet (chaque arbre vote pour une modalité) puis en choississant la modalité ayant reçu le plus de voix parmi tous les arbres de la forêt (vote à la majorité). Les performances d\u0027une forêt sont sensiblement supérieures à celles d\u0027un arbre seul tel que C4.5 (Leon, 2004). Elle est également plus robuste au bruit et présente de meilleures facultés de généralisation (Breiman, 2001). Cependant, celle-ci n\u0027est pas spécifiquement adaptée aux jeux de données déséquilibrés, et ses deux paramètres (le nombre d\u0027arbres et le nombre k de variables à tirer au sort) ne permettent pas à l\u0027utilisateur de spécifier ses préférences en termes de taux de rappel et de précision selon chaque modalité. L\u0027évolution que nous proposons ici consiste à remplacer l\u0027étape du vote classique à la majorité par une nouvelle stratégie de vote pondéré où la recherche automatique des poids optimaux se fait à l\u0027aide de PRAGMA.\nStratégie de vote. Notre stratégie de vote consiste à donner plus ou moins d\u0027importance aux voix attribuées par les arbres ( fig. 2). Une pondération par classe est déterminée (soit par l\u0027utilisateur, soit automatiquement), laquelle multiplie le nombre de voix reçues par l\u0027individu pour cette classe. Ainsi la modalité assignée à un objet n\u0027est pas toujours celle dont il a reçu le plus de voix, mais celle dont le nombre de voix multiplié par son poids est le plus grand. Ceci permet d\u0027augmenter les taux de rappel des classes minoritaires en leur affectant des pondérations fortes, ou plus généralement de jouer sur les taux de rappel et de précision de chaque classe en modifiant leur pondération.\nRecherche automatique. Il peut être assez difficile de trouver manuellement les pondéra-tions ajustant au mieux les résultats du modèle aux besoins de l\u0027utilisateur. Si pour un problème à deux modalités tout peut se ramener à un déplacement de la frontière, en terme de nombre de votes, entre les deux classes, dès qu\u0027il y a plus de trois modalités le nombre de possibilités de paramétrage, c\u0027est-à-dire de ratios entre chaque couple de pondérations, devient bien plus conséquent, et il apparaît nécessaire de rendre automatique la recherche des pondérations. L\u0027algorithme utilisé pour automatiser la recherche des pondérations est construit autour d\u0027un recuit simulé (Kirkpatrick, 1983) cherchant à optimiser la mesure PRAGMA paramétrée selon les souhaits de l\u0027utilisateur. Ce procédé est parfaitement adapté et efficace pour ce type d\u0027optimisation. Le surcoût calculatoire (comparé à une forêt aléatoire classique) est extrêmement (Hettich, 1999) faible. En effet, la forêt n\u0027est construite qu\u0027une fois, seul le résultat du vote après pondéra-tion est mis à jour pour en permettre l\u0027évaluation par PRAGMA. De plus, en conservant pour chaque individu le nombre de votes non pondérés qu\u0027il a reçu pour chaque modalité, il suffit de mettre à jour uniquement la matrice de confusion après pondération du vote pour évaluer le modèle et passer à l\u0027itération suivante.\nFIG. 2 -Exemples de distribution de votes pour : (A) le jeu de données Letters\nExpérimentations\nNous présentons dans cette section les résultats obtenus par pondération automatique des votes d\u0027une forêt aléatoire. Deux types de situations ont été envisagés : (1) Utilisation de l\u0027optimisation sur des jeux de données équilibrés. Notre but ici, en tant qu\u0027utilisateur, est de favoriser un maximum le taux de rappel de certaines classes jugées \u0027prioritaires\u0027. Les tests sont réalisés sur les jeux de données de référence Autos et Letters (Hettich, 1999) dont les variables endogènes possèdent respectivement 6 et 26 modalités.\n(2) Utilisation de l\u0027optimisation des jeux de données déséquilibrés à 2 modalités. Notre but dans cette situation est de favoriser un maximum le taux de rappel de la classe minoritaire. Les tests sont réalisés sur les jeux Hypothyroïd et Satimage (Hettich, 1999) réduits à 2 classes (minoritaire ; fusion des autres classes), ainsi que sur le jeu Mammo, issu de la mise au point d\u0027un système d\u0027aide au diagnostic du cancer du sein.\nJeux de données équilibrés\nNous supposerons ici que l\u0027utilisateur cherche à maximiser les taux de rappel des classes \u0027_3\u0027 et \u0027_2\u0027 pour le jeu Autos, et les taux de rappel des voyelles pour le jeu Letters. Ceci se traduit par le paramétrage de la fonction PRAGMA suivant : coefficient d\u0027importance 10 et couple (x; y) \u003d (10; 90) pour les classes prioritaires, coefficient d\u0027importance 1 et couple (x; y) \u003d (80; 80) pour les autres classes. Nous utilisons des forêts aléatoires de 20 arbres, avec respectivement 5 et 4 variables pour la randomisation. Les résultats présentés dans les tables 1 et 2 sont issus d\u0027une 10-CrossValidation. La figure 3 montre les résultats détaillés sur le jeu Letters. Notez que la classe \u0027__2\u0027 du jeu Autos ne contient que 3 objets, les résultats propres à cette modalité sont peu significatifs. Les différentes moyennes réalisées sont toujours pondé-rées par les effectifs des différentes classes. Ces différents résultats montrent la capacité de l\u0027optimisation à retranscrire les volontés de l\u0027utilisateur. Pour les deux jeux de données, les taux de rappel des classes ciblées ont augmenté. Il en résulte également (de manière logique) : (1) une baisse du taux de précision pour ces mêmes classes ; (2) une baisse du taux de rappel et une augmentation du taux de précision (en moyenne) pour les classes où aucune préférence n\u0027avait été spécifiée. Notons également que ces changements n\u0027entraînent pas forcément une diminution du taux de correction global. Celui-ci peut augmenter ou diminuer selon les jeux de données et le paramétrage de la mesure PRAGMA (ici augmentation du taux correction global pour Autos et diminution sur Letters).\nFIG. 3 -Résultats détaillés pour Letters : (A) RF Classique ; (B) RF Optimisée, le rappel et la précision \"s\u0027organisent\" selon les préférences de l\u0027utilisateur.\nJeux de données déséquilibrés\nCe type de jeux de données assez courant dans le milieu industriel (détection de phéno-mènes anormaux, fraudes, pannes, aide au diagnostic...) constitue un réel challenge pour l\u0027apprentissage automatique. L\u0027objectif principal est de détecter un maximum d\u0027objets de la classe minoritaire (taux de rappel élevé) sans présenter trop de faux positifs (taux de précision correct) ce qui aurait pour effet de rendre de tels systèmes inutilisables. Nos tests sont réalisés sur 3 jeux de données (table 3) : Hypothyroïd et Satimage (Hettich, 1999) réduits à deux classes en fusionnant les classes non minoritaires et Mammo issu de la mise au point d\u0027un système d\u0027aide au diagnostic du cancer du sein. Notons que ce dernier a été réduit en terme de variables et d\u0027objets pour le rendre plus difficile et ne pas dévoiler des résultats industriels confidentiels. La volonté de maximiser le taux de rappel de la classe minoritaire se traduit par le paramétrage de PRAGMA suivant : coefficient d\u0027importance 10 et couple (x; y) \u003d (10; 90) pour la classe minoritaire, coefficient d\u0027importance 1 et couple (x; y) \u003d (80; 80) pour la classe majoritaire. Nous présentons les résultats obtenus en 10-CrossValidation avec C4.5 (témoin de référence des difficultés pouvant présenter les jeux de données), une forêt aléatoire classique, et une forêt aléatoire optimisée par pondération des votes à l\u0027aide de la mesure PRAGMA (table 4). Les forêts sont composées de 20 arbres, avec respectivement 5, 6 et 15 variables utilisées lors de la randomisation pour les 3 jeux de données.\nLes taux du rappel des classes minoritaires les plus élevés sont systématiquement obtenus par la forêt aléatoire optimisée, ceci sans provoquer de fortes baisses des taux de précision. La mesure PRAGMA guide en cela parfaitement le modèle vers les performances souhaitées par l\u0027utilisateur. On remarque également que selon les cas l\u0027optimisation permet également parfois d\u0027améliorer le taux de précision de la classe minoritaire ou le taux de correction globale.\nDes résultats détaillés obtenus en 10-CrossValidation sur le jeu Mammo sont présentés en figure 4. Ils permettent une meilleure description de l\u0027effet de la pondération des votes et de l\u0027utilisation de la mesure PRAGMA sur les performances du modèle. Quatre indices (taux de rappel, taux de précision, nombre d\u0027erreurs, mesure PRAGMA) sont évalués pour différentes valeurs du ratio R : pondération de la classe \u0027Cancer\u0027 / pondération de la classe \u0027Non Cancer\u0027. On remarque que les plus fortes variations pour les taux de rappel et de précision se produisent pour la classe \u0027Cancer\u0027 de par son effectif faible. Le graphe du nombre d\u0027erreurs présente deux caractéristiques notables : (1) celui-ci est asymétrique, car une baisse légère du taux de rappel sur la classe majoritaire due à une forte pondération de la classe minoritaire crée logiquement plus d\u0027erreurs qu\u0027une faible variation du taux de rappel de la classe minoritaire ; (2) pour les ratios 2 ? R ? 5 le nombre d\u0027erreurs total varie très peu alors que la nature des erreurs change (voir les graphes des taux de rappel et de précision). Une sorte de transfert d\u0027erreurs se produit : R ? 2 : les objets mal classés appartiennent majoritairement à la classe \u0027Cancer\u0027 3 ? R ? 4 : les proportions d\u0027objets mal classés pour chacune des 2 classes sont similaires 5 ? R : les objets mal classés appartiennent majoritairement à la classe \u0027Non Cancer\u0027 La mesure PRAGMA permet de faire différentes observations : (1) l\u0027asymétrie est inversée, montrant ainsi que les variations du taux de rappel de la classe \u0027Cancer\u0027 constituent la principale influence de la mesure (ceci s\u0027expliquant par le fort coefficient d\u0027importance et le paramétrage orienté vers le taux de rappel pour la classe \u0027Cancer\u0027) ; (2) pour les ratio 2 ? R ? 5 \nConclusion\nNous proposons dans cet article une nouvelle mesure de qualité des performances des modèles d\u0027apprentissage supervisé appelée PRAGMA (Precision and RecAll rates Guided Model Assessment). Ce critère permet à l\u0027utilisateur d\u0027évaluer ses modèles vis-à-vis de ses attentes en termes de taux de rappel, de taux de précision et d\u0027importance de chaque classe sous la forme d\u0027une mesure unique. Nous montrons ensuite comment il est possible d\u0027utiliser cette mesure comme critère à optimiser pour orienter les performances d\u0027un modèle. L\u0027exemple présenté ici est l\u0027optimisation des forêts aléatoires par pondération (selon les différentes modalités) des votes. Les résultats montrent que cette adaptation permet à l\u0027utilisateur d\u0027orienter de manière effective les performances des forêts aléatoires selon ses souhaits. Dans nos travaux futurs nous projetons de tester d\u0027autres types de fonctions que celle d\u0027un plan pour l\u0027évaluation locale d\u0027une modalité vis-à-vis de son taux de rappel et de son taux de précision. Nous travaillons d\u0027ores et déjà sur l\u0027utilisation de la mesure PRAGMA comme critère de construction des arbres de décision en remplacement des différentes mesures d\u0027entropie classiquement utilisées, mais nos différents tests ne sont pas encore finalisés.\nRemerciements Nous tenons à remercier les membres de la société Fenics Sas (France), en particulier Simon Marcellin, Jérémy Clech, et Anne-Sophie Darnand, ainsi qu\u0027Elie Prudhomme de l\u0027université Lumière Lyon2 pour leur aide et leurs nombreux conseils qui ont permis à cet article de voir le jour. Ce travail a été réalisé dans le cadre d\u0027une thèse cofinancée par le Ministère de la Recherche et de l\u0027Industrie.\n"
  },
  {
    "id": "952",
    "text": "Résumé. Dans le domaine thermique, la plupart des études reposent sur des modèles à éléments finis. Cependant, le coût en calcul et donc en temps de ces mé-thodes ont renforcé le besoin de modèles plus compacts. Le réseau RC équivalent est la solution la plus souvent utilisée. Toutefois, ses paramètres doivent souvent être ajustés à l\u0027aide de mesures ou de simulation. Dans ce contexte d\u0027identification de système, les méthodes statistiques seront comparées aux méthodes classiquement utilisées pour la prédiction thermique.\nLe contrôle de la température de jonction des composants est l\u0027un des enjeux majeurs de l\u0027évolution actuelle de l\u0027électronique du fait qu\u0027elle influe sur leur fiabilité et leurs caracté-ristiques. L\u0027analyse par éléments finis apporte une solution numérique à ce problème mais ne peut pas être utilisée concrètement du fait d\u0027un nombre de calculs trop important. C\u0027est dans ce contexte que les CTM (Compact Thermal Model) ont été developpés (Lasance (2003)). Toutefois, en se rapprochant de l\u0027identification de système, ces modèles ont ouvert la voie aux méthodes statistiques, et notamment à celles pouvant être utilisées dans des cas non-linéaires.\nLe problème de la prédiction thermique en trois dimensions peut se résumer à trouver la fonction u(x, y, z, t), représentant la température du système à un instant donné. En discré-tisant le système, via un maillage, l\u0027équation de diffusion thermique peut être ré-écrite sous forme matricielle (Bergheau et Fortunier (2004)) :\noù u(t) est un vecteur représentant la température aux différents points du maillage, C la matrice élémentaire de masse et K la matrice élémentaire de rigidité. F (t) représente toujours la puissance dissipée mais discrétisée. Le système est alors représenté sous la forme de plusieurs blocs de matériaux homogènes mis bout à bout pour obtenir une structure réaliste. Si le flux de chaleur est supposé être unidirectionnel, alors un bloc peut être remplacé par un circuit électrique équivalent de type RC. Le modèle se trouve donc mis sous la forme d\u0027un réseau RC correspondant aux différents \"étages\" du système. Toutefois, les conditions de cette simplification étant rarement respectées, les paramètres doivent souvent être ajustés à l\u0027aide de simulations ou de mesures. \ndont les paramètres sont plus simples à estimer.\nLa précision obtenue par les modèles linéaires est très bonne. Toutefois, elle peut être améliorée car la linéarité des problèmes thermiques n\u0027est pas toujours garantie. En effet, les effets de la convection mais aussi la conductivité thermique sont légèrement dépendants de la température. Comme pour le cas linéaire, l\u0027identification de système peut être utilisée pour construire un modèle, notamment à partir de réseaux de neurones. Pour conserver le même type d\u0027architecture que précédemment, un réseau de type NNOE (Norgard et al. (2000)) a été choisi.\nSi les méthodes fondées sur les éléments finis sont inutilisables en temps réel, l\u0027analyse classique à base de réseaux RC montre aussi ses limites. Les méthodes statistiques peuvent ainsi aider à simplifier les calculs et à améliorer la précision. Les meilleurs résultats pour les modèles linéaire et non-linéaire sont comparés dans la table 1. Ce travail a donc permis de montrer l\u0027intérêt que pouvait encore avoir une méthode éprouvée telle que les réseaux de neurones, dans un domaine où l\u0027apprentissage statistique est resté très peu utilisé. Les résultats obtenus pourront servir de référence pour l\u0027utilisation de méthodes plus originales comme les réseaux bayésiens dynamiques ou les SVM. \nSummary\nIn the thermal field, most of the studies are based on finite elements model. However, the calculation -and thus time -cost of these methods have inlighted the need of more compact models. The equivalent RC network is so the most used solution. But, the parameters should often be tuned thanks to measurements or simulations. In this context of system identification, statistical methods will be compared with the classical ones for thermal predictions.\n"
  },
  {
    "id": "953",
    "text": "Introduction\nDans le cadre de l\u0027ACI Masse de Données, le projet FoDoMuSt \nFIG. 1 -Extrait de l\u0027ontologie d\u0027objets géographiques.\nbas niveau. On tente alors d\u0027identifier et valider ces polygones en tant qu\u0027objets géographiques par une classification. Lorsque cette dernière échoue, les concepts de l\u0027ontologie d\u0027objets géo-graphiques sont étudiés pour affecter une sémantique aux polygones. Cette étape nécessite un processus de navigation efficace dans l\u0027ontologie, ainsi que l\u0027élaboration d\u0027une métrique adaptée à la comparaison des polygones aux concepts. L\u0027objectif de cet article est donc de proposer un mécanisme de navigation, de comparaison et d\u0027appariement de polygones avec des concepts d\u0027une ontologie issue du domaine de la géographie et dédiée à l\u0027analyse d\u0027une banque de données images (aériennes et/ou satellites).\nDans la première section, nous abordons les approches existantes au niveau de la comparaison sémantique et des mesures de similarité. La deuxième section expose la méthode de navigation et de comparaison mise en place pour répondre à notre objectif. Ensuite, nous pré-sentons une utilisation de notre approche avec la plate-forme FoDoMuSt, avant de conclure et évoquer des perspectives à notre travail.\nÉtat de l\u0027art\nL\u0027identification d\u0027un polygone en utilisant l\u0027ontologie implique une comparaison des propriétés du polygone avec celles des concepts de l\u0027ontologie. Les propriétés (attributs et contraintes sur les attributs) d\u0027un concept sont des conditions individuellement nécessaires et collectivement suffisantes pour établir la relation d\u0027appartenance. Autrement dit, un objet doit toutes les satisfaire pour être membre de la catégorie (dans notre cas un concept) et tout objet les satisfaisant en est un membre. En terme fonctionnel, une catégorie peut être définie par un prédicat d\u0027appartenance sous la forme d\u0027une fonction qui retourne une valeur booléenne selon l\u0027appartenance ou non de l\u0027objet à la catégorie (Mariño Drews, 1993). Ce raisonnement s\u0027appuie néanmoins sur deux hypothèses fortes. La première concerne les propriétés utilisées qui doivent être suffisamment discriminantes. La deuxième est l\u0027hypothèse du monde clos qui présuppose que tout objet peut être parfaitement représenté dans un mode donné de représen-tation et qu\u0027il est possible de déterminer s\u0027il appartient ou non à une catégorie donnée. Les différentes approches présentes dans la littérature articulent la comparaison sémantique entre une stratégie et une fonction de comparaison (ou mesure de similarité).\nD\u0027une manière générale, les stratégies de comparaison admettent, explicitement ou non, une distinction entre structure locale (interne, intrinsèque) et globale (externe, extrinsèque) des concepts. La première correspond aux propriétés définies au niveau du concept lui-même, la deuxième correspond à la place du concept dans la structure globale et aux relations auxquelles il participe (héritage, relation de composition, etc.). Pour plus d\u0027informations sur les approches existantes, nous reportons le lecteur à (Schvaiko et Euzenat, 2005).\nEn ce qui concerne l\u0027évaluation de la similarité, le passage à une représentation \"attributvaleur\" permet le calcul de distances euclidiennes dans un espace multidimensionnel. Schwering et Raubal (2005)  Ces approches abordent donc simultanément les structures internes et externes des concepts. Le souci de définir des mesures asymétriques où la relation de généralisation/spécialisation influe sur la mesure, traduit l\u0027idée d\u0027appréhender les concepts dans toute leur complexité. Si de nombreuses mesures évaluent la similarité selon la profondeur des concepts, le plus court chemin entre eux, etc., certaines utilisent ces caractéristiques (explicitement ou non) comme facteur d\u0027asymétrie ou de spécialisation.\nMéthode d\u0027appariement proposée\nUn objet géographique potentiel est dans un premier temps un polygone sur lequel un certain nombre de caractéristiques (dimensions, indices divers tels que l\u0027indice de Miller, etc.) sont calculées dans une phase de vectorisation. Un polygone est alors donné sous la forme d\u0027attributs-valeurs. Notre approche d\u0027appariement d\u0027un polygone aux concepts de l\u0027ontologie, est orientée attributs (\"feature-based\"). Elle consiste à vérifier la validité des valeurs selon les propriétés et les contraintes définies au niveau des concepts. Cependant, un polygone n\u0027ayant pas de structure sémantique, nous ne pouvons pas nous baser directement sur les formules évo-quées en section 2, comme par exemple MDSM. Un polygone peut être a priori apparié à n\u0027importe lequel des concepts. Les attributs d\u0027un polygone permettant l\u0027appariement à un concept ne sont donc pas identiques selon le concept étudié. Par exemple, le concept \"Bâtiment\" est défini par de nombreux indices (élongation, Miller, . . .) et des informations radiométriques, alors que le concept \"Ombre\" est défini uniquement à partir d\u0027attributs radiométriques. Sans connaissance a priori, il est alors nécessaire pour chaque polygone, de calculer tous les attributs possibles, même si la majorité s\u0027avéreront inutiles pour le traitement d\u0027un concept.\nAfin de tenir compte de toutes ces spécificités nous avons élaboré notre propre méthode d\u0027appariement. Nous inspirant de la littérature pour affecter une sémantique aux polygones, nous avons plaçé au coeur du mécanisme d\u0027appariement une mesure différenciant les composantes locales et globales des concepts. Nous avons défini une nouvelle mesure de similarité (locale) et un score d\u0027appariement (global) pour évaluer la pertinence dans une hiérarchie de concepts.\nScore d\u0027appariement\nLa mesure de similarité (locale) compare les attributs d\u0027un polygone P avec les attributs spécifiques à un concept C et est définie par la formule suivante :\noù v i sont les valeurs dans P pour les attributs A i , et ? i les pondérations rattachées aux A i du concept C (traduisant des rôles plus ou moins discriminants). Notons que la mesure locale ne compare que les attributs communs entre ceux de P et ceux qui sont spécifiques à C (i.e. attributs surchargés ou définis dans ce concept).\nLe score d\u0027appariement (global) évalue la pertinence de l\u0027appariement dans la hiérarchie de concepts. Il est défini par l\u0027équation ci-dessous, où les C j sont les concepts constituant le chemin de la racine à C m .\nS(P, C m ) \u003d m j\u003d1 ? j Sim(P, C j ) m j\u003d1 ? j Ce score d\u0027appariement est une combinaison linéaire des similarités locales obtenues avec les C j . Les similarités locales sont \"propagées\" par héritage aux concepts plus spécifiques. Nous intégrons dans ce calcul, un facteur de spécialisation ? basé sur la profondeur des concepts. Nous privilégions ainsi a priori la spécialisation, en considérant que toute nouvelle information apporte une nouvelle sémantique.\nNotons qu\u0027un polygone n\u0027est pas comparé aux différents concepts avec les mêmes proprié-tés (car les concepts sont décris différemment). On ne peut donc pas assurer que les valeurs respectent les caractéristiques des distances (symétrie, séparation et inégalité triangulaire). Il semble alors logique que des mesures spécifiques aux différents concepts puissent être inté-grées par la suite au module d\u0027appariement. On pourrait ainsi traiter spécifiquement les relations entre concepts (autre que l\u0027héritage) dans une approche plus spécialement adaptée aux relations de composition par exemple.\nNavigation dans l\u0027ontologie\nLe score entre un polygone et un concept étant défini, il faut à présent parcourir l\u0027ontologie pour déterminer le ou les concepts les plus proches. Pour cela nous opérons un parcours en largeur en utilisant deux heuristiques pour diminuer l\u0027espace de recherche et ainsi accélérer les traitements. Un seuil minimum pour le score d\u0027appariement permet d\u0027élaguer les branches dont le concept de départ ne donnerait pas un score suffisant. Cette stratégie est basée sur le fait qu\u0027un concept ayant peu de propriétés vérifiées par le polygone ne sera pas pertinent au niveau des concepts spécialisés. Remarquons que dans le cas où la similarité locale ne serait pas calculable (aucun attribut à comparer), alors le seuil n\u0027est pas pris en compte et le parcours se poursuit. La deuxième possibilité offerte à l\u0027utilisateur consiste à fixer une profondeur d\u0027exploration. Cela permet ainsi de choisir le degré de détail que l\u0027on souhaite utiliser. Par exemple, sur la figure 1, si on fixe la profondeur à 3, seules les catégories usuelles seront visitées (végétation, eau, etc.). La navigation est bien sûr parametrable de façon à utiliser ou non ces stratégies.\nImplantation\nL\u0027ontologie de FoDoMuSt a été créée avec Protégé2000 2 . Nous avons développé le module d\u0027appariement sous JAVA en utilisant l\u0027API de développement Protégé2000 qui permet d\u0027aborder la base de connaissances sous forme de frames. Ces dernières sont liées au paradigme objet et sont particulièrement adaptées au traitement informatique et à la manipulation des concepts et de leurs relations. Nous travaillons avec notre propre mécanisme de raisonnement, sans passer par un raisonneur qui nécessiterait un formatage de la base de connaissance.\nL\u0027accent a été mis sur une implantation la plus générique possible, aboutissant à un environnement de développement pour l\u0027appariement. Le module a été ensuite intégré à la plate-forme FoDoMuSt.\nUtilisation dans la plate-forme FoDoMuSt\nLa plate-forme FoDoMuSt permet de manipuler et d\u0027effectuer des traitements sur des images aériennes ou satellitaires. Il est entre autre possible d\u0027effectuer une classification et/ou une segmentation de l\u0027image afin de construire des polygones en fonction des informations radiométriques des pixels. A l\u0027issue de cette étape, l\u0027utilisateur a la possibilité de lancer l\u0027identification d\u0027un polygone avec notre méthode. Il est aussi possible de lancer le processus sur tous les polygones de l\u0027image. Tout ceci se réalise par l\u0027intermédiaire d\u0027une interface graphique (affichage de l\u0027image segmentée, sélection du polygone, choix des paramètres, restitution des résultats, etc.). Rappelons que les différents paramètres possibles sont principalement le seuil minimum du score d\u0027appariement et le niveau de profondeur souhaité. Dans le contexte d\u0027expérimentation, il faut aussi évoqué les valeurs de pondération des attributs de chaque concept. Cela a été réalisé en concertation avec les géographes. Le ou les concepts ayant obtenus le meilleur score sont proposés à l\u0027utilisateur pour attribuer un label au polygone étudié. Dans le cas d\u0027un traitement sur un seul polygone, une arborescence représentant un sous-ensemble d\u0027intérêt de l\u0027ontologie est aussi affichée avec pour chaque concept les informations concernant les calculs de l\u0027appariement.\nDes exprimentations ont été réalisées sur une image satellite (Quickbird MS) montrant une partie de la ville de Strasbourg. Dans un premier temps, le seuil minimum du score n\u0027a pas était utilisé, et le niveau de profondeur a été fixé à 3. Nous avons alors pu identifier de manière très significative la végétation, l\u0027eau, l\u0027ombre et le minéral. De mauvaises identifications ont concerné principalement l\u0027ombre. Dans une seconde expérience, nous avons laissé la navigation aller jusqu\u0027aux concepts feuille de l\u0027ontologie. L\u0027évaluation est ici difficile étant donné le nombre de concepts. Nous allons prochainement travailler sur ce point. Cependant, les premières observations sont très encourageantes.\nConclusion\nNous avons présenté une méthode d\u0027identification d\u0027objets géographiques, utilisant des connaissances sous forme d\u0027une ontologie. Notre approche est basée sur le parcours et l\u0027appariement d\u0027objets avec des concepts de cette ontologie. Nous avons défini pour cela une mesure\n"
  },
  {
    "id": "955",
    "text": "Introduction\nLa classification automatique (ou clustering) est un domaine d\u0027étude situé à l\u0027intersection de deux thématiques de recherches majeures que sont l\u0027analyse de données et l\u0027apprentissage automatique. Ce domaine est en perpétuelle évolution du fait de l\u0027apparition constante de nouveaux besoins portant à la fois sur la quantité ou la nature des données à traiter (numériques, symboliques, spatiales, histogrammes, etc.) que sur le type de classification attendue (partition, hiérarchie, schéma flou, etc.).\nNombreuses sont les approches proposées afin d\u0027organiser, de résumer ou de simplifier un ensemble de données à l\u0027aide d\u0027une structure de laquelle il est possible de faire émerger des classes d\u0027objets similaires au sens d\u0027un critère de proximité défini ou plus généralement au regard des propriétés que ces objets partagent. Il est de coutume de structurer ces approches en différentes catégories mutuellement non-exclusives (voir Jain et al. (1999)) comme par exemple, pour ne citer que les principales, les approches hiérarchiques, par partitionnement ou encore les modèles de mélanges.\nLes approches par partitionnement, dont l\u0027algorithme des k-moyennes (MacQueen, 1967) en est l\u0027un des plus célèbre représentant, consiste le plus souvent à construire une collection de classes disjointes formant une partition des données par optimisation d\u0027un critère objectif.\nCe critère étant généralement choisi de façon à minimiser la variance intra-classe (les objets à l\u0027intérieur d\u0027une classe doivent tous être assez similaires) et/ou à maximiser la variance interclasses (les classes doivent être séparées les unes des autres).\nLes approches hiérarchiques aboutissent en revanche à une collection de classes emboîtées, que l\u0027on peut représenter par un arbre ou plus généralement un graphe dont les arêtes modé-lisent une relation d\u0027inclusion. Généralement agglomératifs (parfois divisifs) les algorithmes proposés procèdent par fusions successives de classes similaires et les plus utilisés restent sans nul doutes les méthodes agglomératives hiérarchiques (liens simple, complet, moyen ou critère de Ward) présentées dans Sneath et Sokal (1973).\nEnfin, pour les approches de classification par mélange de lois, le problème est posé de façon à maximiser la vraisemblance d\u0027un modèle faisant l\u0027hypothèse que les données sont des observations d\u0027un mélange de densités. Le modèle est caractérisé par le nombre de lois supposées et leurs paramètres. La méthode EM par exemple, proposée par Dempster et al. (1977) constitue une solution algorithmique incontournable à ce genre de problème d\u0027optimisation.\nSi la plupart des domaines d\u0027application trouvent dans cette pluralité d\u0027approches des ré-ponses satisfaisantes aux besoins exprimés, des domaines récents nécessitent d\u0027adapter voire de reposer la problématique de la classification et d\u0027y adjoindre une solution algorithmique efficace. Nous nous intéressons ici au problème de la classification de données en classes nondisjointes (également dites empiétantes ou recouvrantes)\n1\n. Ce type d\u0027approche vise à structurer les données en une collection de classes telle que chaque objet puisse appartenir à plusieurs classes, correspondant alors à une organisation naturelle pour des données par exemple multimédia (texte, image et/ou vidéo) ou encore biologiques (gènes) (Banerjee et al., 2005).\nNous présenterons tout d\u0027abord en Section 2 un ensemble de pistes proposées (approches pyramidales, classification floue, etc.) pour répondre plus ou moins directement aux besoins exprimés par les domaines d\u0027application mentionnés ci-dessus. Nous montrerons alors qu\u0027aucune de ces approches ne constitue une solution globale pour le problème posé. Nous tenterons en Section 3 d\u0027en proposer une nouvelle formalisation et d\u0027y adjoindre une première solution algorithmique en nous inspirant de l\u0027algorithme simple et efficace des k-moyennes. L\u0027approche ainsi présentée sera ensuite observée dans son fonctionnement puis évaluée sur deux jeux de données réelles dont la collection de textes Reuters (Section 4). Enfin nous proposerons en guise de conclusion, un ensemble de perspectives à cette étude, visant à positionner le problème de classification avec recouvrements comme sous-domaine à part entière des recherches menées en classification automatique.\nProblématique de la classification avec recouvrements\nDes solutions partielles\nUne première voie de recherches conduisant à une structuration des données en classes empiétantes réside dans les techniques de classification pyramidales initiées par Diday (1984). Cependant, l\u0027ensemble des recouvrements envisageables par une telle structure se limite aux collections de classes telles que chaque classe s\u0027intersecte avec au plus deux autres classes.\nPlusieurs autres structures hiérarchiques ont été proposées par la suite afin d\u0027étendre les schémas atteignables de façon à approcher l\u0027ensemble de tous les recouvrements possibles ; en particulier les hiérarchies faibles puis les k-hiérarchies faibles (Bertrand et Janowitz, 2003). Cependant deux points restent à déplorer : d\u0027une part il n\u0027existe pas aujourd\u0027hui de méthode algorithmique permettant de construire de telles structures hiérarchiques, et d\u0027autre part l\u0027ensemble des recouvrements atteints -bien que très largement étendu -reste limité aux collections de classes vérifiant la propriété suivante : \"l\u0027intersection de (k + 1) classes arbitraires peut être réduite à l\u0027intersection de k de ces classes\". Un second axe de recherches a été assez fortement étudié ces dernières années et consiste soit à adapter des algorithmes existants (k-moyennes et sa variante floue ou encore EM) ou à développer de nouvelles méthodologies spécifiées pour la recherche d\u0027un \"bon\" recouvrement des données en classes d\u0027objets similaires. Dans cette dernière classe de méthodes on peut citer les algorithmes des k-moyennes axiales (Lelu, 1994) et CBC (Clustering By Committee) développé par Pantel (2003) tous deux motivés par l\u0027application aux données textuelles (mots ou documents) ou encore l\u0027algorithme plus général POBOC (Pole-Based Overlapping Clustering) proposé par Cleuziou et al. (2004).\nDe façon globale, qu\u0027il s\u0027agisse d\u0027algorithmes nouveaux ou simplement adaptés, toutes ces méthodes consistent, en une ou plusieurs itérations, à rechercher des centres auxquels sont affectés les objets. Ces centres peuvent être des points de l\u0027espace (k-moyennes mais aussi EM 2 ), des axes (k-moyennes axiales) ou encore des petits ensembles d\u0027objets appelés committee dans CBC et Pole dans POBOC. Quelque soit la forme prise par ces centres, l\u0027algorithme permettant de les obtenir ne prend pas en considération le fait que les classes finales formeront un recouvrement et pourront ainsi contenir des objets communs. Par exemple les méthodes d\u0027agrégation autour des centres mobiles (k-moyennes et k-moyennes axiales) déterminent un centre après affectation de chaque objet à un seul de ces centres ; à l\u0027inverse les variantes floues de ce type de méthodes considèrent systématiquement que tous les objets doivent participer à la définition de chaque centre ; l\u0027une des hypothèses utilisées dans l\u0027algorithme EM vise à considérer que chaque objet est une observation de l\u0027une (et une seule) des lois du mélange ; enfin les algorithmes CBC et POBOC définissent les centres indépendamment de tout critère objectif de qualité du recouvrement induit par ces centres.\nAinsi définis, les centres sont déterminants pour la dernière étape d\u0027affectation qui conduira au schéma final de classification. L\u0027affectation est le plus souvent réalisée au moyen d\u0027un seuil 3 , difficile à déterminer, quitte à violer les fondements théoriques sur lesquels l\u0027algorithme repose, par exemple la minimisation d\u0027un critère objectif. Finalement, l\u0027hypothèse sous-jacente formulée par ces approches vise à considérer qu\u0027\"un recouvrement de qualité correspond né-cessairement à l\u0027extension d\u0027une \"bonne\" partition\".\nRecouvrements et partitions étendues\nL\u0027hypothèse précédemment formulée ne semble pas incohérente de prime abord. En théo-rie tout recouvrement R \u003d {R 1 , . . . , R k } d\u0027un ensemble d\u0027objets X peut être obtenu par l\u0027extension d\u0027au moins une partition P \u003d {P 1 , . . . , P k } telle que ?i ? 1, . . . , k , P i ? R i . En revanche, partant d\u0027une partition P, l\u0027ensemble des recouvrements possibles par extension correspond à une classe de recouvrements (notée C P ), qui n\u0027est qu\u0027un sous ensemble de tous les recouvrements possibles. Ainsi, considérer qu\u0027un \"bon\" recouvrement (selon un critère W (.)) correspond nécessairement à l\u0027extension d\u0027une \"bonne\" partition (selon un critère V (.)) supposerait que : si P optimise le critère V (.) et R optimise le critère W (.) alors R ? C P Cette dernière propriété dépend bien sûr des critères V (.) et W (.) choisis. Nous montrons alors sur un exemple que l\u0027on peut choisir des critères cohérents pour lesquels cette propriété n\u0027est pas vérifiée.\nSoit X \u003d {x 1 , . . . , x 6 } un ensemble d\u0027objets définis dans R 2 , présentés en figure 1 et que l\u0027on souhaite organiser en deux classes (k\u003d2). On pose V (.) et W (.) les critères objectifs pour l\u0027évaluation respectivement d\u0027une partition et d\u0027un recouvrement et on les défini de la manière suivante :\nFIG. 1 -Partition (à gauche) et recouvrement (à droite) optimaux selon les critères\nOn peut calculer que la partition P \u003d {{x 1 , x 2 , x 3 }, {x 4 , x 5 , x 6 }} minimise le critère V (.) (V (P) \u003d 12.0) et que R \u003d {{x 1 , x 2 , x 4 , x 5 }, {x 2 , x 3 , x 5 , x 6 }} minimise le critère W (.) (W (R) \u003d 12.0 également). Pourtant R n\u0027est pas une extension de la partition P et n\u0027appartient donc pas à la classe des recouvrements C P .\nNous venons donc de montrer par un exemple simple que la classification avec recouvrements ne se résume pas à étendre une partition par des affectations supplémentaires. Faire cette hypothèse consisterait à ne considérer qu\u0027un sous-espace de l\u0027espace de recherche d\u0027une solution, ce dernier étant défini par l\u0027ensemble des recouvrements possibles.\nÀ la recherche d\u0027un \"bon\" recouvrement\nDéfinition du problème\nRechercher une partition P d\u0027un ensemble X \u003d {x 1 , . . . , x n } en k classes P 1 , . . . , P k selon un critère V (.) défini sur l\u0027ensemble des partitions possibles est un problème NP-difficile dans la mesure où l\u0027espace de recherche est de taille exponentielle (k n partitions possibles). L\u0027algorithme bien connu des k-moyennes (MacQueen, 1967) propose une solution partielle au problème d\u0027optimisation du critère des moindres carrés (aussi appelé critère de variance intra-classe) :\n. Ce critère favorise les partitions dont les classes présentent une faible variance, autrement dit telles que les objets à l\u0027intérieur d\u0027une même classe sont faiblement dispersés. Cet algorithme procède par itérations de deux étapes (calcul des centres de classes puis affectation de chaque objet à son centre le plus proche) assurant la décroissance du critère et par la même, la convergence de la méthode vers une partition stable. La solution ainsi obtenue correspond à un minimum seulement local du critère et dépend de l\u0027initialisation (tirage aléatoires de k centres) de l\u0027algorithme.\nLe problème de recherche d\u0027un recouvrement minimisant un critère W (.) ne peut pas être considéré comme plus facile que le précédent puisque l\u0027espace de recherche est de taille beaucoup plus importante (2 k.n recouvrements possibles). Par ailleurs le critère V (.) permettant d\u0027évaluer la qualité d\u0027une partition n\u0027est plus adapté dans le cas des recouvrements car si R est un recouvrement de X en k classes on peut montrer que ?P, R ? C P ? V (P) ? V (R) ; un bon recouvrement selon V (.) ne pouvant alors être qu\u0027une partition.\nDans cette étude, notre proposition porte ainsi sur la définition d\u0027un nouveau critère de qualité d\u0027un recouvrement d\u0027une part et d\u0027une solution algorithmique permettant d\u0027approcher un recouvrement optimal selon ce critère d\u0027autre part. Pour y parvenir nous nous inspirons de l\u0027algorithme simple et efficace des k-moyennes.\nCritère objectif pour les recouvrements\nPour définir un critère de qualité d\u0027un recouvrement il est indispensable de se reporter aux motivations premières qui nous conduisent à rechercher ce type d\u0027organisation. Dans le cas d\u0027un document par exemple, choisir une classe thématique et une seule pour ce document peut réduire considérablement la représentation que l\u0027on conservera de ce document dans la classification. En revanche, autoriser ce document à s\u0027afficher selon plusieurs thèmes rendra une image certainement plus juste de son contenu. La qualité d\u0027un recouvrement pourra alors être mesurée relativement à l\u0027écart entre le contenu réel des objets et l\u0027\"image\" que la classification (ici le recouvrement) établie renvoie d\u0027eux. Nous formalisons cette intuition dans le critère suivant :\nL\u0027image d\u0027un objet dans un recouvrement R est notée x i dans ce critère et correspond à un compromis entre les différentes classes auxquelles cet objet appartient. Ainsi pour un recouvrement R en k classes {R 1 , . . . , R k } de centres respectifs {c 1 , . . . , c k }, x i est défini par le centre de gravité de l\u0027ensemble {c j |x i ? R j }.\nL\u0027algorithme OKM\nL\u0027algorithme OKM (Overlapping k-means) que nous détaillons dans cette section présente un squelette (figure 2) similaire à l\u0027algorithme des k-moyennes. L\u0027initialisation qui consiste à tirer aléatoirement k centres puis à dériver un premier recouvrement est suivie par l\u0027itération de deux étapes : (1) la mise à jour des centres de classes puis (2) l\u0027affectation des objets à ces centres.\nL\u0027intérêt de l\u0027algorithme OKM réside dans la méthode employée pour Mettre_à_jour les centres et pour Affecter chaque objet à un ou plusieurs centres. Ces deux opérations doivent d\u0027une part assurer la cohérence des classes en regroupant ensemble des objets similaires et d\u0027autre part permettre la convergence de la méthode par décroissance du critère W (.).\nÉtant donné un ensemble C \u003d {c 1 , c 2 , . . . , c k } correspondant aux centres des k classes respectives R 1 , R 2 , . . . , R k d\u0027un recouvrement R, la méthode d\u0027affectation d\u0027un objet x i , pré-sentée en figure 3 consiste à parcourir l\u0027ensemble des centres de classes du plus proche au plus éloigné (suivant une métrique d) et à affecter x i tant que son image est améliorée (d(x i , x i ) diminue). La nouvelle affectation de l\u0027objet x i ne sera finalement conservée que si l\u0027image de x i s\u0027en trouve améliorée par rapport à l\u0027ancienne affectation. Cette dernière précaution permet d\u0027assurer la décroissance du critère W (.) lors de l\u0027étape d\u0027affectation.\nAffecter(x i ,C) :\nInitialisation : Soit c * le centre de C le plus proche de\nSoit c * le centre de C le plus proche de x i et x i A le centre de gravité des éléments de A,\nalors affecter x i aux centres de A, Sinon conserver l\u0027ancienne affectation A .\nFIG. 3 -Méthode d\u0027affectation utilisée dans l\u0027algorithme OKM.\nEnfin, la mise à jour du centre c j de la classe R j est définie dans l\u0027algorithme OKM par :\n(1)\nDans cette expression, c j,v désigne la v ième composante du vecteur c j , ? i correspond au nombre de classes de R auxquelles x i appartient etˆxetˆ etˆx i j v symbolise la v ième composante du centre c j \"idéal\" pour l\u0027objet x i , c\u0027est à dire le centre c j tel que d(x i , x i ) \u003d 0. De façon plus précise on a ˆ\noù A désigne l\u0027ensemble des centres des classes auxquelles x i appartient. Il découle de ce qui précède une définition plus intuitive du nouveau centre c j qui correspond finalement au centre de gravité du nuage de points {(\n. On montre que chaque mise à jour d\u0027un centre dans OKM permet d\u0027assurer la décroissance du critère W (.) mais également que le nouveau centre calculé est celui qui minimise ce critère.\nPreuve Soient X un ensemble d\u0027objets définis dans (R p , d) où d est la distance euclidienne, et R un recouvrement de X en k classes de centres c 1 , . . . , c k . L\u0027étape de mise à jour dans OKM consistant à recalculer chaque centre un par un, il suffit alors de montrer que le recalcul d\u0027un nouveau centre c j quelconque minimise le critère\nPar réécriture du terme x i et décomposition de la somme sur les objets de X on obtient :\nPour les objets n\u0027appartenant pas à la classe R j , leur image x i est indépendante de c j , le premier terme est donc constant relativement à c j . Le second terme constitue une fonction quadratique de c j qui sera alors minimisée pour une dérivée égale à 0.\n. Notons pour conclure sur la présentation de l\u0027algorithme, que la méthode des k-moyennes peut être considérée comme un cas particulier de OKM. En effet si on restreint dans OKM chaque objet à n\u0027appartenir qu\u0027à une seule classe (? i \u003d1) on retrouve exactement le processus de classification utilisé dans l\u0027algorithme k-moyennes. Il s\u0027agit donc d\u0027un algorithme nondéterministe puisque le résultat dépendra de l\u0027initialisation ; de plus, chaque classe n\u0027étant plus indépendante l\u0027une de l\u0027autre dans un recouvrement, l\u0027algorithme OKM dépendra également de l\u0027ordre de parcours des classes lors de l\u0027étape de mise à jour des centres.\nÉvaluations et applications\nL\u0027évaluation des méthodes de classification non-supervisée reste un problème entier dans ce domaine de recherches. Une piste possible pour évaluer (au moins partiellement) une telle méthode est de mesurer sa capacité à retrouver un schéma de classification préétabli ; nous l\u0027utiliserons pour évaluer l\u0027algorithme OKM en insistant toutefois sur les précautions qu\u0027il s\u0027impose de prendre lors de l\u0027interprétation des résultats quantitatifs, notamment du fait de l\u0027hypothèse non vérifiée que l\u0027ensemble de descripteurs est pertinent pour établir la classification attendue. Les courbes de la figure 4 mettent en évidence un phénomène prévisible : la convergence plus lente pour l\u0027algorithme OKM vers un recouvrement stable des données (19 itérations), par rapport à k-moyennes qui obtient une partition stable en seulement 12 itérations. On retiendra cependant que dès la cinquième itération, les deux méthodes ont généré un résultat de qualité qui évoluera peu par la suite.\nObservation du fonctionnement de l\u0027algorithme\nX X X X X X X X X X La figure 6 présente la matrice de confusion et nous révèle que les deux méthodes identifient correctement les trois catégories d\u0027iris puisque chacune des classes contient majoritairement l\u0027une de ces trois catégories. Il est reconnu que, sur cette base de données Iris, la catégorie des \"Iris Setosa\" est plutôt facile à identifier tandis que les deux autres catégories sont répu-tées difficilement séparables, ce que l\u0027on observe sur la figure 5. Ces phénomènes se vérifient également sur notre expérimentation :\nÉtiquettes\n-les 50 individus de la catégorie \"Iris Setosa\" se retrouvent exclusivement dans la classe n?3 avec les deux méthodes. Dans le recouvrement obtenu avec OKM, cette classe contient 9 individus de la catégorie \"Iris Versicolour\" en supplément, qu\u0027elle partage avec les autres classes\n5\n(faible intersection) ; -la séparation difficile des deux autres classes se manifeste par des erreurs de classification si l\u0027on cherche à partitionner les données (k-moyennes) et par une intersection importante entre les deux classes lorsque ces données sont organisées en classes recouvrantes (OKM). Sur cette première expérimentation, nous avons d\u0027une part observé un comportement satisfaisant de l\u0027algorithme OKM et d\u0027autre part noté que la structuration en classes recouvrantes fournit un résultat informationnel plus riche qu\u0027une simple partition, notamment en ce qui concerne l\u0027organisation des classes entre elles.\nClassification de documents multi-thématiques\nComme nous l\u0027avons mentionné en introduction, les recherches menées autour de la classification avec recouvrements des classes sont motivées par des besoins apparaissant dans des domaines d\u0027application où des données peuvent appartenir à plusieurs catégories prédéfinies. Dans cette seconde expérimentation, nous évaluons l\u0027impact de notre contribution dans le domaine de la Recherche d\u0027Information et plus précisément pour la classification de documents multi-thématiques.\nL\u0027expérimentation est conduite sur la collection de documents Reuters 6 initialement composée de 21578 articles journalistiques en langue anglaise. Chaque document peut être étiqueté par une ou plusieurs étiquettes parmi un ensemble de 114 catégories. Après filtrage, nous avons retenu 2739 documents, en ne choisissant que ceux pour lesquels au moins une catégorie est proposée, dont le corps de l\u0027article n\u0027est pas vide et appartenant au sous-ensemble \"TEST\" selon la répartition suggérée dans Apté et al. (1994).\nFIG. 7 -Classification de documents issus de la collection Reuters.\nLa représentation des documents est l\u0027aboutissement d\u0027une chaîne de traitements usuelle en Recherche d\u0027Information : chaque document est représenté par un vecteur de dimension m dans lequel chaque composante x i,v correspond à l\u0027information mutuelle du mot 7 w v pour le document x i . Une étape préalable de filtrage des descripteurs consiste à ne sélectionner que les m mots d\u0027information mutuelle supérieure à un seuil ? fixé. Enfin, la similarité entre deux documents est évaluée au moyen du cosinus de Salton (Salton et McGill, 1983) :\nL\u0027évaluation que nous proposons consiste en des séries de 10 exécutions des algorithmes OKM et k-moyennes dans des conditions initiales identiques, sur un sous-ensemble de 300 documents pour un nombre de classes variant de 5 à 30. On appelle association issue de R, une paire d\u0027objets appartenant à une même classe de R ; on dira de plus que cette association est correcte si ces deux objets contiennent au moins une étiquette de catégorie en commun dans la classification préétablie. Chaque partition ou recouvrement R est alors évalué relativement au nombre d\u0027associations de R (noté n a ), qui sont correctes (n b ) par rapport au nombre total d\u0027associations correctes attendues (n c ). Nous recourons aux indicateurs traditionnels en Recherche d\u0027Information : la précision, le rappel et l\u0027indice de F score (avec ?\u003d1).\nNous présentons en figure 7 les moyennes comparatives des mesures de F score obtenues sur les recouvrements générés par OKM d\u0027une part et les partitions obtenues avec k-moyennes d\u0027autre part. La diminution observée du F score s\u0027explique dans les deux méthodes par la ré-duction logique du nombre d\u0027associations (et donc du rappel) lorsque le nombre de classes augmente. Pourtant, si dans le cas des partitions, l\u0027augmentation en précision ne permet pas de compenser la diminution importante du rappel, cette compensation est possible lorsqu\u0027il s\u0027agit de recouvrements du fait d\u0027une perte de rappel atténuée sous l\u0027effet des intersections.\nConclusion et Perspectives\nCette étude part du constat suivant : les méthodes de classification actuelles ne sont pas adaptées à la recherche d\u0027une organisation des données en classes recouvrantes ; ce type de schéma de classification devient pourtant indispensable pour appréhender les domaines d\u0027application actuels tels que les documents multimédia ou les données biologiques.\nNous avons alors proposé une première solution visant à rechercher dans l\u0027ensemble des recouvrements possibles des données, un schéma correspondant au mieux à l\u0027organisation de ces données. Cette proposition s\u0027appuie d\u0027une part sur la définition d\u0027un critère objectif permettant d\u0027évaluer les recouvrements, et d\u0027autre part sur une méthode d\u0027exploration de cet espace des possibilités (l\u0027algorithme OKM).\nDes expérimentations menées sur deux ensembles de données ont mis en évidence la cohé-rence globale de la méthode proposée (sur les données Iris) et justifié de l\u0027intérêt d\u0027organiser les données en classes recouvrantes afin d\u0027en conserver une synthèse riche en informations (sur les données Reuters). Cependant cette première contribution suggère plusieurs améliorations et perspectives importantes à mener.\nTout d\u0027abord on peut noter que, afin d\u0027assurer la convergence du critère objectif, la méthode d\u0027affectation proposée dans OKM favorise mais ne garantie pas que chaque objet soit affecté uniquement à ses centres les plus proches (cf. figure 3). Si cette situation est en pratique suffisamment rare pour ne pas remettre en cause la cohérence globale du schéma, il conviendra de proposer une solution théorique à ce problème.\nNous serons également amené à confirmer la justification de cette approche en montrant sur des études comparatives plus larges son intérêt par rapport à d\u0027autres méthodes mentionnées dans ce papier, en particulier les algorithmes CBC, POBOC ou encore des algorithmes de classification floue complétés par une étape supplémentaire d\u0027affectation.\nEnfin, nous envisageons d\u0027étudier l\u0027intégration d\u0027une pondération différente des descripteurs pour chaque classe en construction (Modha et Spangler, 2003). Cette perspective s\u0027appuie sur l\u0027hypothèse qu\u0027un objet multi-classé doit l\u0027être sur la base de critères différents.\nRéférences\nApté, C., F. Damerau, et S. M. Weiss (1994) \n"
  },
  {
    "id": "957",
    "text": "Introduction\nLes réseaux sociaux sont des systèmes complexes dont certains ont des structures maintenant bien identifiées : graphes de petits mondes et graphes sans échelle typique. Un graphe sans échelle typique est un graphe dont la distribution des degrés n\u0027est pas groupée autour d\u0027une valeur moyenne ; c\u0027est le cas lorsque celle-ci suit une loi de puissance. Les études menées sur le world wide web, des réseaux de courrier électronique ou des réseaux P2P , le réseau des collaborations scientifiques , le réseau des relations sexuelles en sont des exemples (Bornholdt et Schuster, 2003). Les graphes sans échelle typique ont peu de sommets de degrés très élevés et beaucoup de faible degré, ces graphes ont la propriété de présenter des fluctuations locales des degrés d\u0027autant plus importantes que la distribution des degrés est proche d\u0027une loi de puissance. Si les sommets de forts degrés sont connectés entre eux on parle alors de phénomène de \"club huppé\"\nAlors que les études ont en général été effectuées sur des réseaux sociaux contemporains nous analysons ici un réseau relatif à la paysannerie médiévale. Nous travaillons sur une base de contrats agraires signés d\u0027une part entre 1240 et 1350 et d\u0027autre part entre 1450 et 1520 dans une petite région du Sud-Ouest de la France. Cette base pour l\u0027instant réduite à environ 700 actes sera amenée à plus de 8000 actes lorsque le travail de saisie et de désambiguïsation sera terminé. Les sommets du graphes sont les paysans et ils sont liés s\u0027ils apparaissent dans un même contrat, nous définissons ainsi deux graphes G av et G ap ; nous avons éclairci la base en enlevant les seigneurs de notre étude. Nous ne possédons pas de données entre 1350 et 1450, intervalle temporel correspondant à la guerre de Cent Ans.\nLa notion de communauté varie en fonction du réseau que l\u0027on étudie (Palla et al., 2005;Newman, 2006). Nous supposerons dans notre étude que les communautés sont constitués d\u0027individus qui ont à la fois les mêmes liens à l\u0027intérieur de la communauté (clique) et à l\u0027exté-rieur de la communauté. Nous verrons dans la section 3 que cette définition assez contraignante permet pourtant de révéler une structuration très particulière de notre réseau.\nCet article est constitué de deux parties : nous allons tout d\u0027abord commencer par vérifier si notre graphe partage les propriétés rencontrées dans les grands réseaux d\u0027interaction (l\u0027effet petit monde, la distribution des degrés et le phénomène de club huppé). Ensuite nous nous attarderons sur la détection et l\u0027organisation des communautés grâce à des méthodes spectrales. Enfin en conclusion, nous ébaucherons une comparaison des graphes avant et après la guerre de Cent Ans.\nLes indices des deux réseaux d\u0027interaction\nL\u0027effet petit monde\nL\u0027effet petit monde regroupe deux propriétés : la première énoncant que la distance entre deux sommets quelconques est faible (ceci est relatif à la connectivité globale) et la deuxième que la connectivité locale est forte. Pour quantifier ces notions nous utiliserons dans le premier cas soit la moyenne des plus courts chemins \u003c l \u003e soit la longueur caractéristique L (médiane des moyennes des plus courts chemins de chaque sommet (Watts, 2003)) et dans le deuxième cas la moyenne C 1 des densités du graphe des voisins de chaque sommet (Watts, 2003).\nLe tableau TAB. 1 résume les résultats obtenus sur les graphes des liens de sociabilités paysans avant et après la guerre de Cent Ans et les met en perspective avec d\u0027autres exemples de réseaux dont les densités d\u0027arêtes sont voisines. Dans le cas de nos deux réseaux signalons qu\u0027ils ont respectivement un diamètre de 5 et de 6 et que 90% des paires de sommets sont à une distance inférieure ou égale à 3.  Iamnitchi et al. (2004), (b)  Montoya et Solé (2002), (c)  Watts et Strogatz (1998).\nLes coefficients de clustering de nos réseaux sont sensiblement plus forts que ceux rencontrés habituellement.\nLa distribution des degrés\nL\u0027objectif de cette partie est de modéliser la distribution des degrés de nos graphes. Afin de lisser les fluctuations nous étudions la distribution cumulative des degrés P c (k) \u003d ? j\u003dk P (j) où P (j) est la probabilité d\u0027avoir un sommet de degré j. Ce choix permet aussi de repérer plus aisément un degré de coupure éventuel au delà duquel la distribution décroit plus vite (Pastor-Satorras et Vespignani, 2004). Si beaucoup de réseaux récemment étudiés montrent une distribution cumulative des degrés qui suit une loi de puissance, la présence d\u0027un degré de coupure est le signe d\u0027un écart à cette loi. L\u0027 ajustement de la distribution par une loi de puissance tronquée par une coupure exponentielle (TPL) peut alors permettre de mieux expliquer l\u0027ensemble de la distribution ; citons par exemple (Amaral et al., 2000;Achard et al., 2006).\nNous testons trois lois pour ajuster la distribution : loi de puissance figure FIG. 1  \nTAB. 2 -Coefficients et erreurs quadratiques moyennes (10 ?3 ) des différents modèles d\u0027ajustement de la distribution cumulative des degrés.\nLe meilleur ajustement de nos données est obtenu pour une TPL. Le graphe G av échappe à une distribution des degrés en loi de puissance. Cette distribution est assez bien ajustée par une loi exponentielle même si on améliore l\u0027erreur quadratique avec une TPL. Pour G ap une loi de faible puissance donne de meilleurs résultats que pour G av mais une TPL reste la meilleure modélisation.\nL\u0027effet « club huppé »\nNos deux graphes G av et G ap possèdent un club-huppé (Zhou et Mondragón, 2004) c\u0027est-à-dire que les sommets de forts degrés (« les riches ») forment ensemble un sous-graphe dense. Ces individus jouant un rôle important dans l\u0027organisation du réseau, les indices de centralité de proximité et de centralité d\u0027intermédiarité (Degenne et Forsé, 1994)  \nRecherche des communautés\nL\u0027effet petit monde avec un coefficient de clustering élevé associé à une faible densité du graphe nous indique la présence de communautés ; afin de les déceler nous étudions le spectre du laplacien (non normalisé). Nous définissons nos communautés ainsi : \nLa démonstration consiste à étudier la matrice binaire A + I qui est de rang 2 dans le cas (i) et de rang 3 dans le cas (ii). On procède par épuisement des cas.\nEn utilisant le théorème énoncé dans van den Heuvel et Pejic (2000) et le théorème 1 précédent, nous extrayons pour le graphe G av 28 communautés de taille supérieure ou égale à 3 dont la plus importante est de taille 15 et pour le graphe G ap 31 communautés dont la plus grande est de taille 7.\nEn supprimant la partie du graphe ne contenant aucune communautés (cette partie contient le club-huppé) nous obtenons un graphe à plusieurs composantes connexes. Les communautés trouvées via l\u0027étude du spectre ne sont donc guère liées entre elles, elles sont préférentielle-ment liées à la partie que nous avons otée et notamment au club huppé. Nous avons donc une structure inter-communautaire proche de celle d\u0027une étoile. Le centre de cette étoile contient le club-huppé dont on visualise bien à présent le rôle central qu\u0027il joue dans l\u0027organisation du ré-seau social. Ce partitionnement en club-huppé et communautés permet une bonne visualisation des graphes (FIG. 2) \nConclusion\nMalgré le fait d\u0027avoir enlevé les seigneurs de notre étude, nous constatons dans chacun des deux graphes G av et G ap la présence d\u0027un groupe d\u0027individus (le club huppé) possédant un rôle central. Ces deux graphes apparaissent sous forme d\u0027une étoile de communautés. Dans G av , un fort nombre de communautés de petite taille cohabitent avec un nombre significatif de communautés plus importantes, ce qui explique le bon ajustement des distributions des degrés avec une TPL. Concernant G ap , la structuration est moins claire : le résidu est sensiblement plus important et la taille des communautés moins variable.\nSi certaines de ces communautés correspondent à des zones géographiques comme on peut le voir dans (Hautefeuille, 2001), d\u0027autres n\u0027ont pour l\u0027instant pas trouvé d\u0027explications.\nOn remarquera un renouvellement quasi-complet des noms du club-huppé entre G av et G ap : la famille Combelcau très influente avant la guerre disparaît complètement après la guerre laissant la place à la famille Limairac, nouvelle famille qui paraît très influente.\nFIG. 2 -Graphe des communautés de G av (à gauche) et G ap (à droite). Les disques repré-sentent les k-communautés extraites et le rectangle le reste des sommets (dont le club-huppé).\n"
  },
  {
    "id": "960",
    "text": "Introduction\nRAS\n1 , Reference Annotation System est un outil semi-automatique d\u0027annotation de documents basé sur le contexte de citation, l\u0027expert du domaine reste décideur de la fiabilité de l\u0027annotation. L\u0027approche d\u0027annotation permet d\u0027annoter un document sans connaissance préa-lable de son contenu, en se basant sur les références. Cet outil a été réalisé dans le contexte d\u0027un besoin réel, celui d\u0027une communauté souhaitant partager l\u0027information existante et ceci sous certaines contraintes, la plus importante étant celle de l\u0027absence de contenu des documents à partager. Afin de tester les résultats de l\u0027annotation, nous avons utilisé une base avec un nombre important de documents qui s\u0027inter-référencent. L\u0027outil utilise les technologies suivantes :\n-Python 2 comme langage de script ; -la base documentaire Citeseer 3 ; -L\u0027ontologie dmoz 4 (informatique) ; -l\u0027algorithme de classification fuzzy C-means Dunn (1973  Summary RAS (Reference System Annotation) is a documents annotation tool. This tool is the result of the implementation of our approach of annotation based on the context of citation. The approach is independent of the content and uses a regrouping set of themes of the references builds starting from a not-supervised fuzzy classification. The tool presented in this article was tested and evaluated with the base of scientific documents Citeseer.\n"
  },
  {
    "id": "961",
    "text": "Introduction\nLe Web 2.0 a pour objectif de faciliter l\u0027accès à l\u0027information en représentant les documents Web par une structure sémantiquement riche et non par un traditionnel « sac de mots ». Cette structure est généralement définie par la représentation des documents sous forme d\u0027arbres : des éléments de contenu, identifiés par la séquence étiquetée des feuilles de l\u0027arbre, sont organisés selon une structure prédéfinie par un ensemble de noeuds internes représentant les relations entre éléments. Cette structure traduit les relations sémantiques ou logiques entre éléments de contenu. Les comparateurs de prix, le Web Sémantique sont des exemples de services fournis par le Web 2.0.\nLa plupart des documents du web utilisent des formats semi structurés comme le HTML, le XML, le PDF ou encore le WikiText. Ces formats permettent d\u0027enrichir le texte à l\u0027aide de balises et une interprétation directe de celles-ci permet de décrire les documents par un arbre, l\u0027arbre DOM. Nous appellerons structure syntaxique cette structure directement liée à la manière dont l\u0027information est codée. Les applications du Web 2.0 ne peuvent toutefois pas tirer directement profit de cette structure : elles ont toutes besoin de connaître à priori la structure utilisée et ne sont donc capables de ne traiter que les documents respectant strictement un schéma qui leur est spécifique. Ce schéma définit les structures que peuvent avoir les documents. Il est peu probable que les documents Web respectent un schéma donné à priori : en général, ceux-ci proviennent de plusieurs sources hétérogènes, utilisant chacune des structures différentes. De plus, dans le cas du HTML leur format ne contient que des informations de mise en page peu informatif pour ces applications. L\u0027utilisation directe de la structure syntaxique par les application du Web 2.0 est donc impossible. Pour les rendre utilisables, il faut transformer ces sources hétérogènes en un format médiateur spécifique de l\u0027application. C\u0027est l\u0027objet du travail que nous présentons. Nous nous intéressons plus spécifiquement à la conversion automatique de documents HTML vers un format XML prédéfini. Cette problématique spécifique tire son intéret de la masse d\u0027information présente sur le web sous un format HTML.\nBien que moins riche que celle de nombreux formats semi-structurés, l\u0027information de mise en page présente dans le HTML fournit une information qui est exploitée quotidiennement par de nombreux utilisateurs, notamment pour faciliter leur navigation ou la recherche d\u0027information. En effet, avec le développement des sites basés sur des systèmes de gestion de contenu (blogs, site de nouvelles, ...), de plus en plus de pages sont générées automatiquement à partir de bases de données. Par leurs régularités, la mise en page des documents reflète leur structure logique et permet d\u0027identifier des éléments (un titre, un commentaire, ...) ainsi que des relations entre ceux-ci (on peut par exemple préciser l\u0027auteur d\u0027une sous-partie du document). Ce nouveau type d\u0027information, directement lié à la présentation des documents, peut, par exemple, être utilisé pour organiser les commentaires des visiteurs d\u0027un site en threads (Figure 1)  Pour exploiter cette information additionnelle nous proposons de transformer les documents Web vers un format médiateur. Cette structure sera spécifiée par un schéma cible dépen-dant de l\u0027application considérée. L\u0027écriture manuelle de convertisseurs spécifiques à chaque source de documents et à chaque application est un travail présentant un grand risque d\u0027erreur et qui est peu adapté à la richesse et à la nature dynamique du Web. Plusieurs solutions (Doan et al., 2003;Chung et al., 2002)  \nde documents exprimés à la fois dans leur structure d\u0027origine et dans la structure cible. Une des approches les plus prometteuses, (Chidlovskii et Fuselier, 2005), a formulé cette tâche comme une généralisation de l\u0027analyse syntaxique : le document d\u0027entrée est représenté par une séquence d\u0027observations correspondant aux feuilles du document HTML ; la structure du document de sortie est reconstruite à partir de cette représentation grâce à une grammaire hors-contexte décrivant le schéma de sortie. Dans cette approche, la structure du document d\u0027entrée est cependant ignorée, même si, comme nous le montrerons, elle fournit une information indispensable pour déterminer la bonne structure de sortie.\nDans ce travail, nous proposons une approche plus générale qui permet de considérer des caractéristiques arbitraires décrivant à la fois la structure de sortie et la structure d\u0027entrée des documents. Nous commencerons par décrire un cadre général de transformation de documents fondé sur des techniques d\u0027apprentissage structuré (paragraphe 2), puis nous détaillerons notre modèle (paragraphe 3). Nous présentons enfin un ensemble d\u0027expériences prospectives sur plusieurs corpus de documents réels (paragraphe 4). \nLa problématique de restructuration\nDans l\u0027Équation 1, l\u0027argmax traduit le parcours de l\u0027espace de toutes les restructurations potentielles D(d in ) pour rechercher la meilleure solution. Cet ensemble doit inclure l\u0027ensemble des segmentations et des réorganisations des noeuds de contenu (notamment les permutations, fusions et séparations de noeuds) et tous les arbres compatibles avec cette nouvelle organisation. La taille de l\u0027espace de recherche est donc exponentielle par rapport au nombre de noeuds de contenu : en se limitant aux permutations de n noeuds, l\u0027espace de recherche contient déjà n! éléments.\nPour trouver la meilleure solution en un temps raisonnable, plusieurs sources d\u0027informations doivent être considérées pour élaguer l\u0027espace de recherche : le contenu du document source, sa structure et la définition du schéma cible. Ainsi, sur l\u0027exemple de la Figure 2, on peut dire que la restructuration est guidée par la structure du document d\u0027entrée -les deux premières feuilles décrivent le même personnage, puisqu\u0027ils ont le même parent -et contrainte par la structure cible -chaque personnage est composé d\u0027un acteur et d\u0027un nom : l\u0027information apportée par les structures d\u0027entrée et de sortie sont essentielles pour la tâche de restructuration. Dans la suite, nous supposerons que F est linéaire par rapport à ? :\n. Avec cette hypothèse, la tâche de restructuration peut être vue comme un problème d\u0027apprentissage structuré (Tsochantaridis et al., 2004). L\u0027apprentissage structuré est une généralisation de l\u0027apprentissage multi-classe permettant de traiter des problèmes dans lesquels les entrées et les sorties peuvent être décomposées en un ensemble de sous-parties inter-dépendantes. Utiliser des techniques d\u0027apprentissage structuré est intéressant puisque ce formalisme nous fournit plusieurs méthodes pour apprendre à partir de caractéristiques arbitraires décrivant les structures d\u0027entrée et de sortie. Les algorithmes de l\u0027apprentissage structuré souffrent en général d\u0027une complexité élevée. Cela prohibe leur utilisation pour des tâches complexes comme la tâche de restructuration et pour traiter des corpus de grande taille. Nous allons maintenant présenter une méthode d\u0027apprentissage structuré qui présente l\u0027avantage d\u0027avoir une complexité suffisamment faible pour pouvoir être appliquée au problème de la restructuration de corpus de grande taille.\nModèle d\u0027apprentissage\nLa méthode que nous proposons repose sur l\u0027observation suivante : l\u0027apprentissage structuré, tel qu\u0027il est décrit par l\u0027Équation 1, nécessite de construire un ensemble très grand de structures combinatoires et de retrouver celle de plus grand score. Nous proposons de considérer ces deux étapes séquentiellement. Plus précisément, l\u0027algorithme proposé enchaîne deux étapes :\nune étape de génération qui va construire GEN (d in ) un ensemble de N solutions candidates. Ce processus repose sur des hypothèses d\u0027indépendance fortes entre les éléments des documents et sur l\u0027utilisation d\u0027un ensemble restreint de caractéristiques pour permettre une construction efficace des solutions candidates à l\u0027aide d\u0027un algorithme basé sur la programmation dynamique. une étape d\u0027ordonnancement qui va trouver la meilleure restructuration parmi les solutions candidates GEN (d in ) générées à l\u0027étape précédente en considérant des caractéristiques arbitraires aussi bien du document d\u0027entrée que de la solution candidate. En particulier, puisqu\u0027on travaille l\u0027univers restreint GEN (d in ), on pourra considérer des caractéris-tiques globales des arbres d\u0027entrée et de sortie, ce que ne permet pas la programmation dynamique.\nTraiter ces deux étapes de manière séquentielle permet de conserver les avantages de la programmation dynamique (construction efficace d\u0027une sortie structurée, mais qui ne considére que des caractéristiques locales) tout en considérant des caractéristiques globales lors de la seconde étape pour sélectionner la meilleure restructuration. Cette approche ne fournit qu\u0027une solution approchée de l\u0027Équation 1, puisque seules les solutions les plus prometteuses de la première étape sont évaluées. Toutefois, elle a déjà montré son efficacité dans plusieurs tâches de langue naturelle (Collins et Koo, 2005). Nous allons maintenant détailler ces deux étapes.\nGénération des solutions candidates\nLa première étape de notre modèle a pour objectif de construire la structure du document de sortie à partir de la séquence des noeuds de contenu c \u003d (c 1 , ..., c #c ). Cette construction peut se faire facilement avec des algorithmes d\u0027analyse syntaxique (Jurafsky et Martin, 2000). L\u0027utilisation de ces algorithmes est d\u0027autant plus intéressante que les structures arborescentes des documents semi structurés se modélisent naturellement par des grammaires horscontexte (Chidlovskii et Fuselier, 2005). En effet, celles-ci permettent de décrire simplement des structures récursives à la fois horizontale (une section comporte plusieurs sous-sections) et verticale (il est possible de faire des listes de listes). De plus, elles permettent aussi de formaliser l\u0027observation suivante : on peut identifier un élément soit par son contenu (une taille est définie par un nombre et une unité), soit par les éléments qui le constituent (une date est composée d\u0027un jour, d\u0027un mois et d\u0027une année, même si la représentation et l\u0027ordre de ces éléments peuvent varier).\nL\u0027utilisation d\u0027une version probabilisée de cette grammaire (PCFG) permet, en outre, de caractériser les régularités et la variabilité du schéma cible : il est possible, par exemple, de modéliser le fait qu\u0027une section regroupe un ensemble de sous-sections, et que, le nombre de sous sections suit une distribution donnée.\nFormellement, une PCFG décrivant un document semi structuré est définie par le quintuplet G \u003d ?, R, S, P c où :\n-? est un ensemble de non-terminaux décrivant les étiquettes des noeuds internes (qui correspondent aux relations) ; -? est un ensemble de terminaux définissant les étiquettes des feuilles (qui correspondent aux noeuds de contenu) ; -S, un élément de ?, est le symbole initial décrivant la racine de l\u0027arbre ; -R est un ensemble de productions dont chaque élément est associé à une probabilité.\nChaque production est une relation de ?×(???) * qui décrit les règles de composition : la règle html ? head body indique qu\u0027un élément html regroupe (dans cet ordre) un élément head et un élément body ; -P c est un modèle de contenu qui défini une distribution de probabilité sur ? pour chaque noeud de contenu. Intuitivement, cette distribution permet de rajouter un ensemble de productions du type ? ? c i pour tous les éléments ? de ? et tous les éléments de contenu c i . Ces productions permettent de déterminer l\u0027étiquette des feuilles. La Figure 3 montre un exemple de schéma cible et de la PCFG qui lui est associée.\nUne PCFG permet d\u0027associer une probabilité à chaque structure de sortie t. Cette probabilité mesure la compatibilité entre la structure arborescente et la séquence d\u0027observations. Elle est définie par :\noù r est l\u0027ensemble des productions utilisées pour construire l\u0027arbre t, c est la séquence des noeuds de contenu du document. L\u0027estimation des probabilités p(r) sera détaillée dans le prochain paragraphe. La Figure 4 donne un exemple du calcul de cette probabilité.\nUn algorithme d\u0027analyse syntaxique permet de reconstruire efficacement les N -meilleures restructuration associées à une séquence de noeuds de contenu. Nous avons utilisé une exten-   (Jimnez et Marzal, 2000) pour construire les N meilleures solutions avec une complexité O(n 3 + N · #R #? · n · log n 3 ). Généralement, dans nos expériences N \u003c n, cette complexité est donc du même ordre de grandeur que celle de l\u0027algorithme reconstruisant la meilleure solution, qui est en O(n 3 ).\n\u003c!ELEMENT NEWS (HEADER BODY) \u003e \u003c!ELEMENT HEADER ( a u t h o r t i t l e | a u t h o r t i t l e d a t e ) \u003e \u003c!ELEMENT BODY\nApprentissage des paramètres L\u0027apprentissage se fait à partir d\u0027un ensemble de documents exprimés dans le schéma cible. Les paramètres décrivant les éléments de R sont estimés par maximum de vraisemblance (Jurafsky et Martin, 2000). La probabilité d\u0027une production A ? ? est donnée par :\np(A ? ?) \u003d #{A ? ?} A???R #{A ? ?} où #{A ? ?} correspond au nombre d\u0027apparition de cette production dans le corpus d\u0027apprentissage, A est un élément de ? et ? de ? ? ? * . Les probabilités de contenu sont estimées dans notre modèle par un classifieur maximisant l\u0027entropie. Ce type de classifieur nous permet de prendre en compte facilement toutes les caractéristiques que nous jugeons pertinentes, sans nécessiter d\u0027hypothèses d\u0027indépendance entre celles-ci. Ces caractéristiques peuvent dépendre à la fois du noeud de contenu c et de l\u0027étiquette ?. Nous pouvons donc définir aussi bien des caractéristiques décrivant le contenu des noeuds (nombre de majuscules, présence de chiffre, ...), leur contexte (le nombre de frères, la profondeur dans l\u0027arbre, ...) et le type de données spécifié par le schéma. La Table 1 détaille une partie des caractéristiques utilisées.\nPlus précisément, on a : \nFIG. 4 -Exemple de deux restructurations potentielles (les arbres (b) et (c)) de l\u0027arbre (a). Avec la PCFG de la Figure 3, le score du document (b) est\noù f est le vecteur de caractéristiques décrit au paragraphe précédent,? une étiquette, Z ? (c) est un coefficient de normalisation, ? le vecteur des paramètres à estimer et\nLe vecteur des paramètres ? a été estimé en utilisant le principe de maximisation de l\u0027entropie : l\u0027algorithme d\u0027apprentissage détermine, parmi toutes les distributions compatibles avec les observations, celle qui fait le moins d\u0027hypothèse sur les valeurs non observées.\ncaractéristiques de contenu caractéristiques du contexte caractéristiques sur le type de données contains-http is-only-child is-xs_string begins-with-capitals has-1-to-3-siblings is-xs_duration contains-number is-descendant-of-title is-xs_time contains-1-to-5-spaces ...\nTAB. 1 -Exemples de caractéristiques utilisées pour décrire les noeuds de contenu. La figure représente trois types de caractéristiques : contenu (e.g. une séquence contient \"\u0027http\"\u0027), contexte (e.g. ce noeud n\u0027a pas de frère), type de données (e.g. type string selon la définition de la norme XML Schema).\nÉtape d\u0027ordonnancement\nin ; w), qui permet d\u0027ordonner l\u0027ensemble des solutions candidates. Ce score va notamment nous permettre de prendre en compte des caractéristiques globales sur d, et d in . Le vecteur de paramètres w est estimé à partir d\u0027un ensemble d\u0027apprentissage constitué par : -un ensemble de n documents, chaque document étant exprimé à la fois dans sa structure d\u0027origine et dans la structure cible. Nous noterons T \u003d d in cet ensemble ;\n-pour chaque élément de T les N solutions candidates :\nj\u003d1 qui sont construites par la première étape. Nous supposerons, sans perte de généralité, que d i 1 est la meilleure restructuration pour d in 1 . L\u0027apprentissage est effectué par un perceptron à noyaux (Collins et Duffy, 2002;Collins et Koo, 2005). C\u0027est une méthode simple mais efficace, qui permet, grâce à l\u0027utilisation d\u0027un noyau, de considérer des espaces de grandes dimensions. Le score calculé par le perceptron à noyau pour une solution candidate d associée au document d\u0027entrée d in est :\navec k une fonction noyau qui sera explicitée dans le paragraphe suivant, ? ij les paramètres appris, d \nCaractéristiques utilisées\nDeux types de caractéristiques sont envisageables pour discriminer la meilleure solution candidate. Un premier type de caractéristiques décrit les dépendances à longue distance entre les noeuds de l\u0027arbre, afin de décrire chaque noeud par un contexte plus riche que celui utilisé dans l\u0027étape de génération. Un deuxième type de caractéristiques permet de mesurer une similarité entre le document d\u0027entrée et la solution candidate. En effet, dans le cas de documents Web majoritairement textuel, certains groupes d\u0027éléments doivent être conservés lors de la transformation, pour garder un sens.\nC\u0027est pourquoi nous allons utiliser une combinaison de deux noyaux :\nL\u0027addition de deux noyaux traduit la concaténation des deux espaces de caractéristiques. Le premier noyau, k tree est le noyau d\u0027arbre de (Collins et Duffy, 2001) qui capture les dépendances à long terme entre les noeuds d\u0027un arbre. Contrairement à une PCFG qui ne considère que les dépendances entre un noeud et ses fils, le noyau d\u0027arbre considère l\u0027ensemble de l\u0027arbre pour modéliser le contexte d\u0027un noeud : un arbre sera représenté par l\u0027ensemble de ses sous-arbres. Le nombre de sous-arbres d\u0027un arbre est exponentiel par rapport à la taille d\u0027un arbre, mais, une solution basée sur la programmation dynamique permet de réaliser le comptage sans avoir à énumérer tous les sous-arbres.\nLe second noyau utilisé, k f eatures est un noyau RBF (Radial Basis Function). Il utilise des caractéristiques globales à la fois sur le document d\u0027entrée et sur la solution candidate. Ces caractéristiques incluent :\n-une comparaison entre le nombre de noeuds du document d\u0027entrée et du document de sortie -les couvertures communes au document d\u0027entrée et au document de sortie. La couverture est définie, pour chaque noeud d\u0027un arbre par la paire constituée de la position dans la séquence des feuilles de la première et de la dernière feuille du sous-arbre ayant ce noeud comme racine. De manière intuitive, les couvertures permettent de résoudre des problème comme ceux présentés Figure 4 ; -des contraintes imposées par le schéma cible (par exemple un film ne peut avoir qu\u0027un titre mais plusieurs réalisateurs). Ces contraintes sont déduites automatiquement du schéma cible. -le score de la première étape Les valeurs de ces caractéristiques sont combinées à l\u0027aide d\u0027un noyau puis incorporées dans le noyau global.\nExpériences\nNous avons testé notre modèle sur deux corpus différents. Le premier corpus est un ensemble de nouvelles publiées sur un site traitant d\u0027actualité informatique LinuxFr (http: //linuxfr.org). Les pages du site ont été téléchargées et converties, à la main, en XML pour un schéma prédéfini. Chaque page correspond à une nouvelle et comporte un en tête regroupant les méta-informations (auteur, titre, date, ...), le corps de la nouvelle et plusieurs threads de commentaires des visiteurs du site. Le corps de la nouvelle comme les commentaires peuvent utiliser la plupart des tags HTML. La partie décrivant les commentaires des utilisateurs est très fortement structurée et présente un défi à la transformation du document : la structure logique des commentaires (i.e. : de quel commentaire un commentaire donné est-il la réponse) doit être reconstruite à partir de la structure du document d\u0027entrée. Le corpus comporte 200 nouvelles, chaque nouvelle ayant, en moyenne, 70 noeuds de contenu et 50 noeuds internes. Le plus grand document a 165 noeuds de contenu et 114 noeuds internes. Le schéma cible définit 13 étiquettes possibles pour les noeuds de contenu et 11 pour les noeuds internes.\nLe second corpus est basé sur les données d\u0027IMDb (http://imdb.com). 710 descriptions de films ont été téléchargées et converties manuellement en XML suivant un schéma donné. Chaque description comporte, en moyenne, 35 noeuds de contenu et 35 noeuds internes ; la plus grande en a, respectivement, 212 et 211. C\u0027est un corpus de type « base de données » : comme dans les bases de données relationnelles, les descriptions de film ont une structure attribut-valeur et seuls quelques noeuds ont des données textuelles (généralement les commentaires des utilisateurs et les résumés). En conséquence, la structure des documents est plus régulières que dans le premier corpus.\nChaque corpus a été séparé aléatoirement en un ensemble d\u0027apprentissage et un ensemble de test. Tous deux comportent les documents d\u0027entrée en HTML et les documents cible correspondant en XML, toutefois, pour le test seuls les documents d\u0027entrée sont utilisés. Ces deux ensembles ont la même taille. Notre modèle a alors été utilisé pour retrouver la structure cible XML des documents du corpus de test. Différentes valeurs de N (le nombre de solutions candidates générées par la première étape) ont été testées. Notre modèle de base (N \u003d 1) correspond au cas où seule l\u0027étape de génération entre en jeu : la reconstruction a alors lieu sans tenir compte de la structure du document d\u0027entrée et est proche du modèle proposé par (Chidlovskii et Fuselier, 2005). Nous proposons d\u0027évaluer la qualité de la restructuration en mesurant la similitude entre le document reconstruit et le document convertit la main. Comme mesure de similarité, nous avons utilisé le pourcentage de constituants correctement reconstruit. Un constituant correspond à une paire (´ etiquette, couverture) et permet de décrire à la fois l\u0027étiquette d\u0027un noeud et sa position dans l\u0027arbre reconstruit. Nous avons mesuré sépa-rément les résultats de la reconstruction sur les feuilles de l\u0027arbre et sur les noeuds internes de celui-ci afin de pouvoir évaluer la capacité de notre modèle à identifier des éléments et à identifier des relations entre ces éléments.\nLa Table 2 rassemble les résultats de nos expériences. Dans tous les cas, l\u0027utilisation de l\u0027information sur le document d\u0027entrée lors de l\u0027étape de ré-ordonnancement améliore les résultats du modèle de base, pour une complexité globale sensiblement identique. Cette amélioration est significative lorsque l\u0027on ne considère qu\u0027un petit nombre de solutions candidates. Cependant, le fait d\u0027augmenter le nombre de solutions candidates au delà d\u0027un certain seuil lors de l\u0027étape de ré-ordonnancement diminue les performances. Cette baisse est certainement due aux limites de l\u0027algorithme d\u0027apprentissage notamment par rapport au petit nombre de données fournies en apprentissage. Une analyse plus détaillée des résultats montre que les performances de l\u0027approche sont très bonnes sur les parties les plus régulières des documents, mais que la qualité de la reconstruction chute dans les parties récursives des documents. Dans tous les cas, les résultats de l\u0027étape de ré-ordonnancement dépendent de la qualité des solutions candidates. \nÉtat de l\u0027art\nDes problèmes semblables (schema matching, intégration de données, ...) sont traités depuis plusieurs années par la communauté base de données et sont apparus, plus récemment, pour la recherche documentaire, la conversion (Chidlovskii et Fuselier, 2005) et l\u0027intégration de documents (Chung et al., 2002), et l\u0027alignement d\u0027ontologies. Plusieurs techniques d\u0027apprentissage ont été employées : classifieur multi-classes, spectre de graphe, ... Plusieurs travaux récents abordent la problématique de la transformation de documents, notamment avec des grammaires formelles (Chidlovskii et Fuselier, 2005;Wisniewski et al., 2006).\nUne comparaison des différentes approches proposées en base de données est faite dans (Doan et Halevy., 2005). (Doan et al., 2003) présente une des approches les plus abouties pour travailler sur différents types de données (SQL, XML, ontologies). La tâche y est présentée comme un problème de classification supervisée multi-étiquettes. Toutefois les corpus considérés sont très différents des corpus auxquels nous nous intéressons : l\u0027évaluation des méthodes développées a, généralement, été faite sur des corpus de petite taille, ayant une structure très stricte présentant peu de récursion et ne contenant que très rarement des données textuelles.\nConclusion\nNous avons proposé un cadre général pour la transformation de document semi structuré vers un schéma arbitraire. Ce cadre nous permet de considérer des caractéristiques du document d\u0027entrée et de la structure de sortie, améliorant ainsi plusieurs approches existantes.\n"
  },
  {
    "id": "962",
    "text": "Introduction\nDe nos jours, le stockage de grands volumes de données est devenu possible et abordable. Ainsi, à des problématiques aussi diverses que l\u0027analyse statistique de la fréquentation d\u0027un lieu, la sécurisation de l\u0027accès à des bâtiments, la surveillance de malades épileptiques dans des hôpitaux, ou encore la facturation des véhicules aux péages des autoroutes, les industriels proposent de plus en plus de solutions techniques basées sur l\u0027acquisition numérique de sé-quences vidéo. Ces données vidéo sont tridimensionnelles (deux dimensions spatiales, et une dimension temporelle). Il s\u0027agit donc d\u0027un volume 2D+T tel que représenté sur la Figure 1 (b) 1 . Quelle que soit l\u0027application, la première tâche d\u0027un système d\u0027analyse de séquences vidéo est toujours la détection de mouvement, et si possible, la détection (segmentation) des objets mobiles. La difficulté de cette tâche est très variable selon les conditions d\u0027acquisition, la pré-cision et la rapidité du traitement escomptées. Une liste relativement exhaustive des difficultés liées à l\u0027acquisition et au contenu de la scène peut être trouvée dans Toyama et al. (1999). Dans cet article, nous ne nous intéresserons qu\u0027au cas d\u0027une acquisition par caméra fixe. \nFIG. 1 -Différentes manières de représenter une séquence vidéo.\nLa plupart des algorithmes de détection de mouvement présents dans la littérature consistent à bâtir un modèle de l\u0027arrière-plan de la scène filmée (c\u0027est-à-dire, ce que l\u0027on verrait s\u0027il n\u0027y avait aucun objet mobile), puis à comparer ce modèle à l\u0027image visible à un instant t. Autrement dit, les données vidéo ne sont pas traitées comme un volume 2D+T (Figure 1(b)), mais comme une succession de couples d\u0027images à deux dimensions (Figure 1(a)). Cette méthode est appelée soustraction de l\u0027arrière-plan (background subtraction).\nLe modèle le plus simple consiste à considérer à chaque instant t que l\u0027image au temps t ? 1 représente l\u0027arrière-plan, et que les zones en mouvement sont celles qui ont changé d\u0027apparence entre t ? 1 et t. En d\u0027autres termes, la détection de mouvement est obtenue par dérivation temporelle de la séquence. La dérivée ainsi obtenue est très rapide à calculer, mais elle est également très instable du fait de sa sensibilité à tout type de bruit. Par ailleurs, seul le passé immédiat est pris en compte donc les mouvements lents ou saccadés sont mal détectés. Ainsi, les auteurs qui utilisent la dérivée temporelle sont obligés d\u0027ajouter des post-traitements afin de corriger le résultat. Dans Tian et Hampapur (2005), la dérivée est lissée par un opérateur de moyenne mobile, puis les points dont la direction du flot optique a beaucoup varié dans un passé proche sont éliminés du résultat. Le flot optique est défini comme la projection dans le plan de l\u0027image du vecteur mouvement (3D) réel. Un panorama des différentes méthodes pour calculer le flot optique est présenté dans Barron et al. (1994). Les méthodes utilisant le flot optique prennent davantage en considération la dimension temporelle des séquences vidéo, mais se restreignent à un intervalle de temps de taille 2.\nUne alternative à l\u0027utilisation de l\u0027image au temps t ? 1 comme modèle de l\u0027arrière-plan est l\u0027utilisation d\u0027une image de référence. Malheureusement, une telle image n\u0027est pas toujours disponible, et même lorsqu\u0027elle l\u0027est, elle devient vite obsolète, particulièrement en environnement extérieur (changement de luminosité, intempéries, etc.) C\u0027est pourquoi les auteurs utilisant cette méthode proposent toujours une fonction de mise à jour de l\u0027image de référence (background maintenance), comme dans Yang et al. (2004), où l\u0027image de référence est continuellement mise à jour aux points où la dérivée temporelle est négligeable.\nSouvent, le modèle de l\u0027arrière-plan est un modèle statistique permettant d\u0027évaluer la probabilité d\u0027apparition d\u0027un niveau de gris ou d\u0027une couleur en un certain point. Si celle-ci est élevée, on considère que le pixel appartient à l\u0027arrière-plan, sinon c\u0027est qu\u0027il appartient à un objet. Parfois, il s\u0027agit de l\u0027ensemble des paramètres d\u0027une loi dont la forme est supposée connue. Par exemple, dans McKenna et al. (2000) le modèle de l\u0027arrière-plan comporte la moyenne et l\u0027écart-type des valeurs observées sur chaque canal R, G et B, car on suppose que la distribution de chaque composante couleur est gaussienne. Dans d\u0027autres cas, on ne fait pas d\u0027hypothèse a priori sur la forme de la loi à estimer, et on pratique alors une estimation non paramétrique comme dans Elgammal et al. (2000). Le modèle de l\u0027arrière-plan est alors un ensemble d\u0027observations passées qui permettront d\u0027estimer les densités ponctuellement à l\u0027aide d\u0027une fenêtre de Parzen ou d\u0027une fonction noyau.\nLa soustraction de l\u0027arrière-plan peut également être vue comme un problème de prédic-tion. Les méthodes les plus utilisées sont le filtrage de Wiener et celui de Kalman. Dans Toyama et al. (1999), le modèle de l\u0027arrière-plan est l\u0027ensemble des dernières images et des coefficients pondérateurs d\u0027un filtre de Wiener associés à chacune de ces images. Les coefficients sont mis à jour de manière à minimiser l\u0027erreur quadratique entre l\u0027image observée au temps t et sa pré-diction qui est la somme des images précédentes pondérée par les coefficients du filtre. Dans Koller et al. (1993), c\u0027est un filtre de Kalman qui sert à prédire le vecteur des paramètres, en l\u0027occurrence l\u0027image observée ainsi que ses dérivées spatiales et sa dérivée temporelle.\nAinsi, rares sont les méthodes qui ne cherchent pas à estimer l\u0027arrière-plan de la scène pour détecter les objets en mouvement. On notera néanmoins les travaux de Ma et Zhang (2001) où l\u0027on considère que les zones en mouvement sont celles où l\u0027entropie spatio-temporelle de la séquence est maximale. Contrairement aux méthodes citées précédemment, la dimension temporelle est pleinement prise en considération par un algorithme d\u0027analyse semi-locale dans le volume 2D+T que constitue la séquence vidéo. Dans Guo et al. (2004), cette approche est légè-rement modifiée pour calculer l\u0027entropie de la dérivée temporelle plutôt que celle des images d\u0027entrée afin d\u0027éviter de détecter les contours spatiaux comme étant des zones en mouvement. Les résultats obtenus sont assez proches d\u0027une dérivée temporelle lissée par un opérateur de moyenne mobile. Notre étude se situe dans ce cadre : nous cherchons à détecter les objets mobiles en analysant le volume 2D+T de manière globale puis locale. Dans un premier temps nous préciserons l\u0027espace de représentation choisi pour l\u0027étude des séquences, puis dans une seconde partie nous expliquerons les critères utilisés pour la sélection des objets en mouve-ment. Enfin nous présenterons les résultats et les évaluerons.\nEspace de représentation des données\nLes données vidéo sont initialement représentées par une fonction définie dans un espace à trois dimensions : deux dimensions spatiales (x, y) et une temporelle (t). A chaque point de cet espace est associé un niveau de gris (ou un vecteur de composantes couleur) en un point (x, y) à l\u0027instant t. Les différentes entités sémantiques (arrière-plan, objets mobiles) sont donc des sous-ensembles de points de cet espace. Afin de les identifier, il convient de les agréger en classes de points présentant des caractéristiques communes. Il va sans dire que le nombre de points à considérer est très important, surtout si l\u0027on veut prendre en compte plus de deux trames pour détecter les objets en mouvement. C\u0027est pourquoi l\u0027approche consistant à bâtir un modèle de l\u0027arrière-plan est si usuelle : les seuls points à considérer sont ceux de la trame courante, tandis que le modèle de l\u0027arrière-plan est censé résumer toutes les observations passées. Nous pensons qu\u0027il est préférable de conserver une connaissance moins synthétique du passé car l\u0027information pertinente à en extraire n\u0027est pas toujours la même. Nous envisageons donc de choisir un espace de représentation adapté davantage à la séquence elle-même plus qu\u0027à chacune des trames et qui permette de prendre en compte le mouvement sans modifier l\u0027information initiale. Comme représenté sur la Figure 1(c), à chaque point de l\u0027espace image (x, y) est associé un vecteur contenant les niveaux de gris en ce point le long de l\u0027intervalle de temps considéré. De plus, dans la perspective de l\u0027utilisation des techniques d\u0027analyse des données, la séquence n\u0027est plus considérée comme une fonction mais comme un ensemble d\u0027individus : les pixels que nous observons quand nous regardons la séquence. Dans cette phase, les relations spatiales entre les pixels sont donc ignorées. Pour éviter de devoir faire une analyse fine, ce ne sont pas les objets que l\u0027on suit mais c\u0027est une position fixe que l\u0027on considère sur la surface de l\u0027image. De chaque pixel on va retenir plusieurs valeurs de niveau de gris au cours du temps. On peut retenir une dizaine de valeurs, soit p et chaque pixel devient un individu caractérisé par un ensemble de paramètres. Les individus sont repérés dans un espace de dimension p. Comme notre méthode traite p trames à la fois, nous pouvons nous permettre d\u0027être p fois plus lents que si nous traitions chaque trame individuellement, et donc d\u0027utiliser des techniques plus coû-teuses en temps de calcul. Néanmoins, pour rester dans des temps de traitement raisonnables, presque temps réel, nous devons faire une réduction de la masse des informations. Il existe de nombreuses méthodes de réduction de dimension, telles que l\u0027analyse en composantes principales (ACP), l\u0027analyse factorielle des correspondances (AFC), toute la famille des méthodes d\u0027analyse en composantes indépendantes (ACI), ou encore les algorithmes à base de réseaux neuronaux tels que les cartes de Kohonen. Le lecteur intéressé pourra se référer à Lebart et al. (2006) pour obtenir un panorama détaillé. L\u0027ACP étant connue pour être la meilleure technique linéaire de réduction de dimension au sens des moindres carrés, nous avons choisi cette mé-thode dans le but de ne préserver que les informations qui permettront au mieux de discriminer les points et de construire des classes.\nL\u0027ACP a été développée au début du XIXème siècle pour analyser des données issues des sciences humaines. C\u0027est une technique statistique qui vise à simplifier un ensemble de données en l\u0027exprimant dans un nouveau système de coordonnées de manière à ce que les plus grandes variances soient observées sur les premières coordonnées. Cela permet de réduire la dimensionnalité de l\u0027espace de recherche en ne conservant que les premières dimensions de l\u0027espace de projection obtenu. Une base de cet espace est composée des vecteurs propres de la matrice de covariance des données, ordonnés par valeurs propres associées décroissantes.\nOn peut penser que les pixels correspondant au fond ont des composantes à peu près toutes égales alors que les pixels correspondant au passage d\u0027un objet mobile comportent un changement. C\u0027est ce changement que l\u0027on veut mettre en évidence. Pour cela il est intéressant de trouver l\u0027axe, c\u0027est-à-dire la bonne base dans l\u0027espace de dimension p où la variance du facteur est la plus grande.\nDans le cas d\u0027une séquence vidéo, la matrice des données X contient donc l\u0027ensemble des caractéristiques des points à considérer. Par la suite, nous noterons n le nombre de lignes de X, c\u0027est le nombre de pixels de l\u0027image, et p son nombre de colonnes, c\u0027est le nombre de caractéristiques retenues pour chaque pixel. Les deux premières coordonnées peuvent prendre un nombre fini de valeurs (domaine de définition D P des pixels). En revanche, le domaine de définition de la troisième coordonnée (le temps) est a priori infini. Il convient donc de choisir une plage de valeurs qui devra contenir toute l\u0027information pertinente. Nous proposons d\u0027utiliser le domaine D t \u003d {t ? ?t, . . . , t} où t est le temps courant. Nous choisissons de remplir la matrice X en considérant qu\u0027une donnée (une ligne) est un point (x, y), et qu\u0027une variable (une colonne) est un ensemble de niveaux de gris observés à chaque instant de D t . La nouvelle base de l\u0027espace de représentation est alors associée aux vecteurs propres de la matrice de covariance C des données :\noù ¯ X est la matrice des données centrées, et D p \u003d 1 p I p (I p étant la matrice identité d\u0027ordre p.) Comme nous n\u0027avons aucune information supplémentaire, on suppose que chaque variable devrait présenter une variance comparable, et nous utilisons une ACP dite « simple » (données centrées) plutôt qu\u0027une ACP « standard » (données centrées-réduites).\nLa méthode doit être le plus possible insensible aux diverses conditions dans lesquelles se font les acquisitions : nous nous intéressons plus aux variations de niveau de gris qu\u0027au niveau de gris lui-même du pixel. Nous pouvons, dès le départ, supprimer une dimension de l\u0027espace de représentation des données en choisissant de remplir la matrice de données avec les dérivées temporelles en tout point, on est alors dans un espace de dimension p ? 1. (Nous appellerons Y la matrice de données représentée dans cet espace.) Considérons une séquence de 10 trames de 288 lignes par 720 colonnes dont la première est représentée sur la Figure 2(a). La matrice X a donc 288 × 720 lignes et 10 colonnes, tandis que la matrice Y sur laquelle nous allons appliquer l\u0027ACP a 288 × 720 lignes et 9 colonnes. La Figure 2 montre les neuf projections de Y sur les axes principaux issus de l\u0027ACP. Plus précisément nous considérons le domaine de l\u0027image et nous construisons une image dont le niveau de gris correspond à la valeur de la composante du vecteur de caractéristiques sur l\u0027un des facteurs. D\u0027après la Figure 2, les zones en mouvement apparaissent clairement lorsqu\u0027on projette la matrice Y sur les deux premiers axes principaux. La différence entre une zone statique et une zone en mouvement est accentuée sur ces axes. Cette observation est confirmée par l\u0027histogramme de la variance expliquée par les facteurs (Figure 3). La variance expliquée par un axe est définie par le rapport entre la valeur propre associée à cet axe, et la somme des valeurs propres de la matrice de covariance.\nAinsi, si l\u0027on choisit de ne retenir que les deux premiers axes principaux, 20% de l\u0027information initialement contenue dans notre matrice de données suffisent à préserver 80% de\nFIG. 2 -(a) Séquence de travail. (b)-(j) Projections de Y sur chacun des axes mis en évi-dence par l\u0027ACP. Les sous-figures sont ordonnées selon le rang du facteur correspondant.\nFIG. 3 -Variance expliquée par les axes principaux.\nla variance observée. Cette première expérimentation confirme donc notre approche qui reste une approche très globale. Dans la section suivante, nous utiliserons donc cet espace de représentation des données. Cela revient à calculer une ACP pour tout ensemble de 10 trames consécutives. De manière à gagner en robustesse nous allons maintenant considérer une approche plus locale qui repose sur cette première étude globale.\nDétection de zones de mouvement cohérent\nLa représentation des données telle que dans la Figure 2(b) permet de facilement détecter les mouvements au niveau local (au niveau des pixels). En effet, il suffit de sélectionner les pixels dont la valeur absolue est élevée (les plus sombres et les plus clairs) pour obtenir une segmentation objets mobiles/arrière-plan. La Figure 4(b) représente la segmentation automatique de la projection de Y sur le premier axe principal.\nNous nous retrouvons alors dans le cas de la plupart des méthodes présentes dans la littérature, une telle image binaire serait étiquetée en composantes connexes pour obtenir une détection par objet mobile. Comme dans la plupart des cas, dans l\u0027exemple de la \nFIG. 4 -(a) Projection de Ysur le premier axe principal, (b) segmentation de l\u0027arrière-plan obtenue à partir de (a), (c) segmentation améliorée par des opérations morphologiques.\nprétraitement de l\u0027image serait nécessaire pour supprimer les faux positifs et pour rétablir la connexité des objets (c). Une telle approche fournit une segmentation précise, mais le choix des opérations morphologiques à effectuer est souvent délicat. Une erreur dans le choix d\u0027un élément structurant pourrait effacer un objet intéressant, connecter deux objets différents, valider un faux positif, etc. Une phase d\u0027apprentissage est nécessaire pour adapter la méthode générale au cas particulier de la séquence étudiée. Nous préférons donc éviter d\u0027avoir à effectuer une telle étape, mais nous avons tout de même besoin de définir des zones connexes associées à un unique objet mobile. Pour gagner en cohérence nous allons perdre en précision. A partir de la représentation globale considérée précédemment, dans la population des pixels de l\u0027image, des sous-populations de tailles égales sont isolées et seront comparées. Pour cela, nous commençons par fractionner les données (Y) en plusieurs sous-ensembles. Chaque sousensemble correspond à un bloc de b × b pixels maintenant caractérisés par les neuf valeurs de la matrice de données Y qui constituent les valeurs des facteurs mis en évidence dans l\u0027étude globale. Sur la séquence initiale ce sont donc des blocs tridimensionnels de taille b × b × 10 qui sont étudiés au travers de 9 nouvelles caractéristiques. Les blocs que nous avons choisis, de manière à obtenir des résultats plus continus et sans augmenter trop les temps de calcul, se recouvrent par moitié le long des dimensions spatiales. Les individus des sous-ensembles ainsi obtenus sont représentés dans un espace de dimension p ? 1. Nous allons étudier les positions relatives de ces ensembles de points. Pour simplifier les calculs, nous représentons chaque ensemble de points par son ellipsoïde d\u0027inertie. De plus nous comparons les projections des ellipsoïdes dans le plan formé par les deux premiers facteurs de la représentation globale.\nComparaison des zones détectées\nLa Figure 5 montre un ensemble d\u0027ellipsoïdes d\u0027inertie projetés sur le premier plan factoriel de l\u0027image globale. Ils correspondent chacun à un bloc spatio-temporel tel que décrits dans la section 3.\nLes ellipses observées se différencient par leur position dans le plan, leur surface, et leur orientation. Dans le cadre de cette étude, nous ne nous intéresserons pas à l\u0027orientation des ellipses. Les données, étant centrées, le repère de la Figure 5 a pour origine la moyenne de Y (ou plus exactement la projection de la moyenne). Par conséquent, une ellipse qui se trouve loin de l\u0027origine représente un bloc dont beaucoup de points sont en mouvement. La surface des ellipses donne une indication sur la variabilité des points du bloc qu\u0027elle représente. On peut ainsi distinguer plusieurs cas :\nFIG. 5 -Chaque bloc tridimensionnel est modélisé par l\u0027ellipsoïde d\u0027inertie des points qui le composent, et chaque ellipsoïde est projeté dans le plan formé par les deux premiers facteurs issus de l\u0027étude globale (ACP).\n1. Une petite ellipse proche de l\u0027origine représente un bloc dans lequel aucun mouvement n\u0027est présent.\n2. Une grande ellipse proche de l\u0027origine représente un bloc dans lequel les différents points ont des mouvements dissemblables, mais où la moyenne des mouvements est quasinulle. Autrement dit, il s\u0027agit de bruit.\n3. Une petite ellipse éloignée de l\u0027origine représente un bloc dans lequel le mouvement moyen est important, et dont les points ont quasiment tous le même mouvement. Ce sont les blocs intégralement inclus dans un objet en mouvement.\n4. Une grande ellipse éloignée de l\u0027origine représente un bloc dans lequel le mouvement moyen est important, et dont les points présentent des mouvements assez variés. Ce sont les blocs qui peuvent par exemple se trouver à la frontière d\u0027un objet en mouvement.\nPour détecter les objets en mouvement, les blocs les plus intéressants sont donc ceux qui correspondent aux cas 3 et 4, autrement dit, les ellipses éloignées de l\u0027origine. Il faut donc effectuer un seuillage par rapport à la distance à l\u0027origine des centres des ellipses, c\u0027est-à-dire la moyenne (ou la somme) des points appartenant aux blocs correspondants.\nExpérimentation\nLa taille des blocs spatio-temporels introduits à la section 3 reste à définir. Des blocs trop larges nuiraient à la précision des contours des objets détectés, tandis que des blocs trop petits impliqueraient des temps de calcul plus élevés, et la connexité des régions pourrait en souffrir. La  Nous constatons que le temps de calcul dépend peu de la taille des blocs (et donc de leur nombre). On peut donc choisir celle-ci uniquement en fonction de la séquence à analyser, sans se soucier du temps de calcul nécessaire. Dans notre cas, les images ont pour dimension 720 × 288 pixels, et la taille de bloc produisant le moins d\u0027erreurs est 32 × 32.\nPour évaluer notre algorithme, nous utilisons cinq séquences vidéo qui se différencient par la problématique demandée par l\u0027application et/ou les difficultés intrinsèques de la séquence. La première séquence illustre un problème de comptage de personnes passant par le sas situé en bas de l\u0027image. La difficulté est liée au fait que plusieurs personnes restent longtemps plus ou moins immobiles dans le champ de vision avant de franchir (ou non) le sas. Il faut donc que l\u0027algorithme ne détecte pas les mouvements insignifiants. La seconde vidéo représente égale-ment un problème de comptage de personnes, mais là, les personnes ont tendance à se déplacer en groupes connexes. Il faut donc un algorithme suffisamment précis pour pouvoir discerner les différents membres de chaque groupe. La troisième séquence représente une application de détection de passage de véhicules dans le premier plan. La difficulté provient du fait que l\u0027image est très bruitée par le soleil passant à travers les arbres sur la gauche de l\u0027image, et par des véhicules circulant dans l\u0027arrière-plan. Les deux dernières séquences sont des séquences de test classiques utilisées dans de nombreux articles 2 . Elles sont utilisées dans le but de faciliter la comparaison des résultats présentés dans cet article avec d\u0027autres méthodes. Sur la Figure 7 sont représentées les segmentations entre objets mobiles et arrière-plan obtenues sur ces cinq séquences vidéo, avec cinq algorithmes différents. La ligne 1 montre les résultats obtenus avec l\u0027algorithme présenté dans cet article ; la ligne 2, la dérivée temporelle lissée de la séquence ; la ligne 3, la soustraction de l\u0027arrière-plan en modélisant celui-ci par une loi gaussienne (McKenna et al., 2000) ; la ligne 4, la soustraction de l\u0027arrière-plan quand celui-ci est modélisé de manière non paramétrique (Elgammal et al., 2000) ; la ligne 5, l\u0027entropie spatio-temporelle de la différence entre images consécutives (Guo et al., 2004). Dans la littérature, les méthodes concurrentes que nous avons testées sont toujours suivies d\u0027une phase de post-traitement pour faciliter l\u0027extraction des composantes connexes. Pour les lignes 2 à 5, nous avons donc appliqué aux résultats obtenus une fermeture morphologique par un disque de diamètre 5 suivie d\u0027une ouverture morphologique par le même élément structurant.\nOn constate que les algorithmes utilisant une modélisation statistique de l\u0027arrière-plan (lignes 3 et 4) font apparaître les contours des objets mobiles de manière plus précise. En revanche, ces méthodes sont très sensibles au bruit, donc à moins de choisir très précisément le post-traitement en fonction de la séquence traitée, les résultats obtenus ne constituent pas une bonne segmentation des objets mobiles.\nLes contours des objets sont également assez précis avec la méthode de dérivation temporelle (ligne 2). Cela dit, cette méthode a tendance à ne révéler que les contours des objets et à en ignorer l\u0027intérieur. Cet inconvénient peut être compensé en augmentant le coefficient de lissage, mais l\u0027on risque alors de créer un effet « fantôme », et de perdre la précision obtenue.\nNotre méthode ainsi que celle de l\u0027entropie spatio-temporelle ont en commun le fait de sacrifier la précision des contours au profit d\u0027une plus grande robustesse. Le nombre de composantes connexes est néanmoins plus exact avec la méthode ici présentée (ligne 1).\n"
  },
  {
    "id": "963",
    "text": "Résumé. La classification des images sonar est d\u0027une grande importance par exemple pour la navigation sous-marine ou pour la cartographie des fonds marins. En effet, le sonar offre des capacités d\u0027imagerie plus performantes que les capteurs optiques en milieu sous-marin. La classification de ce type de données rencontre plusieurs difficultés en raison des imprécisions et incertitudes liées au capteur et au milieu. De nombreuses approches ont été proposées sans donner de bons résultats, celles-ci ne tenant pas compte des imperfections des données. Pour modéliser ce type de données, il est judicieux d\u0027utiliser les théories de l\u0027incertain comme la théorie des sous-ensembles flous ou la théorie des fonctions de croyance. Les machines à vecteurs de supports sont de plus en plus utilisées pour la classification automatique aux vues leur simplicité et leurs capacités de généralisation. Il est ainsi possible de proposer une approche qui tient compte de ces imprécisions et de ces incertitudes au coeur même de l\u0027algorithme de classification. L\u0027approche de la régression par SVM que nous avons introduite permet cette modélisation des imperfections. Nous proposons ici une application de cette nouvelle approche sur des données réelles particulièrement complexes, dans le cadre de la classification des images sonar. la flore. Ces imperfections rendent la tache difficile pour la caractérisation des fonds marins à partir de ce type de données. Il est donc nécessaire de proposer des algorithmes, robustes aux imperfections, pour la classification automatiques des images sonar.\nPlusieurs choix sont envisageable pour remédier aux problèmes d\u0027imperfections : soit nous tentons de supprimer ces imperfections, ce qui nécessite une compréhension, souvent difficile, de la physique qui a conduit à ces imperfections ; soit nous cherchons à développer des processus de traitement robustes à ces imperfections ; soit nous cherchons à les modéliser.\nLe cadre théorique des théories de l\u0027incertain offre la possibilité de modéliser finement ces imperfections. Parmi elles, la théorie des ensembles flous et la théorie des fonctions de croyance permettent de tenir compte des incertitudes et imprécisions.\nDe nombreuses approches ont été proposées pour la classification des images sonar par exemple dans Laanaya et al. (2005b) et Leblond et al. (2005). Ces approches ne tiennent pas compte de l\u0027incertitude de l\u0027expert lors de la segmentation de ces images. Nous adopterons dans ce papier l\u0027approche que nous avons proposée dans Laanaya et al. (2006) avec une ré-solution du problème d\u0027optimisation adaptée à la classification automatique des images sonar. Cette approche a donné des résultats intéressants sur des données générées, nous montrerons ici son intérêt sur les données complexes que sont les images sonar.\nAinsi, nous présenterons une description rapide des fonctions d\u0027appartenance et des fonctions de croyance utilisées par l\u0027approche de la régression par SVM. Nous rappelons ensuite, l\u0027approche de la régression par SVM après une brève introduction du principe des SVM. Cette approche est comparée au SVM classique et discutée dans une dernière partie à partir d\u0027images sonar.\nThéories de l\u0027incertain\nNous avons vu dans Martin (2005) que les théories de l\u0027incertain telles que la théorie des sous-ensembles flous introduite par Zadeh (1965), la théorie des possibilités de Dubois et Prade (1987) ou encore la théorie des fonctions de croyance de Dempster (1967) et Shafer (1976 permettent la modélisation de données incertaines et imprécises dans le cadre de la classification d\u0027images sonar.\nCes théories sont fondées sur les fonctions d\u0027appartenance pour les premières et sur les fonctions de croyance pour la dernière. Afin d\u0027intégrer directement les contraintes liées à ces fonctions dans un algorithme de classification, nous rappelons ici les caractéristiques des fonctions d\u0027appartenance de la théorie des sous-ensembles flous et des fonctions de croyance de Dempster et Shafer.\nLes fonctions d\u0027appartenance\nLes fonctions d\u0027appartenance permettent de décrire une appartenance floue à une classe. Ainsi l\u0027appartenance d\u0027une observation x à une classe C i parmi N c classes, est donnée par une fonction µ i (x) telle que :\nDans ce cas, nous considérons les classes floues. Dans le cas de classes nettes, il est possible de considérer les distributions de possibilité. Typiquement x peut représenter une partie du fond marin et C i le type de sédiment présent sur l\u0027image x. Nous verrons au paragraphe 4.1.2 comment ces fonctions µ i peuvent être choisie dans notre application.\nLes fonctions de croyance\nLa théorie des fonctions de croyance est fondée sur la manipulation des fonctions de masse. Les fonctions de masse sont définies sur l\u0027ensemble de toutes les disjonctions du cadre de discernement ? \u003d {C 1 , . . . , C Nc } et à valeurs dans [0, 1], où C i représente l\u0027hypothèse \"l\u0027observation appartient à la classe i\". La contrainte de normalité couramment employée est ici donnée par :\nA?2 ? où m(.) représente la fonction de masse. La première difficulté est donc de définir ces fonctions de masse selon le problème. Nous verrons comment il est possible de le faire pour notre application dans la section 4.1.2. A partir de ces fonctions de masse, d\u0027autres fonctions de croyance peuvent être définies, telles que les fonctions de crédibilité, représentant l\u0027intensité que toutes les sources croient en un élément, et telles que les fonctions de plausibilité repré-sentant l\u0027intensité avec laquelle on ne doute pas en un élément. Afin de conserver un maximum d\u0027informations, il est préférable de rester à un niveau cré-dal (i.e. de manipuler des fonctions de croyance) pendant l\u0027étape de manipulation des informations pour prendre la décision sur les fonctions de croyance à l\u0027issue de la manipulation de ces fonctions. Si la décision prise par le maximum de crédibilité peut être trop pessimiste, la décision issue du maximum de plausibilité est bien souvent trop optimiste. Le maximum de la probabilité pignistique, introduite par Smets (1990), reste le compromis le plus employé. La probabilité pignistique est donnée pour tout X ? 2 ? , avec X \u003d ? par :\nSimilitudes\nAinsi les fonctions d\u0027appartenance et les fonctions de masse permettent une modélisation de l\u0027incertitude et de l\u0027imprécision à partir de points de vue différents.\nCes fonctions ont toutes deux la particularité d\u0027être à valeurs dans [0,1] et d\u0027avoir une contrainte de normalité équivalente. Nous allons voir dans la section suivante comment intégrer ces contraintes dans une régression linéaire multiple. montré des performances remarquables sur des données générées. Afin d\u0027assoir les notations utiles pour la suite, nous rappelons le principe des SVM sur laquelle s\u0027appuie la régression floue et crédibiliste présentée ensuite.\nPrincipe du classifieur SVM\nLes machines à vecteurs de support initiées par Vapnik (1998), sont avant tout une approche de classification linéaire à deux classes. Elles tentent de séparer des individus issus de deux classes (+1 et -1) en cherchant l\u0027hyperplan optimal qui sépare les deux ensembles, en garantissant une grande marge entre les deux classes. Un nombre réduit d\u0027exemples pour la recherche de l\u0027hyperplan est suffisant pour la description de cet hyperplan.\nDans le cas où les exemples sont linéairement séparables, on cherche l\u0027hyperplan y \u003d w.x + b qui maximise la marge entre les deux ensembles où w.x est le produit scalaire de w et x. Ainsi w est la solution du problème d\u0027optimisation convexe :\nsous les contraintes :\noù les x t ? IR d représentent les l données d\u0027apprentissage , et y t ? {?1, +1} la classe. Ce problème d\u0027optimisation se résout par la méthode du lagrangien.\nDans le cas où les données ne sont pas linéairement séparables, les contraintes (5) sont relachées par l\u0027introduction de termes positifs ? t . Nous cherchons alors à minimiser :\nt\u003d1 sous les contraintes données pour tout t :\noù C est une constante choisie par l\u0027utilisateur. Le problème se résout alors de manière similaire au cas linéairement séparable. Afin de classer un nouvel élément x il suffit d\u0027étudier la fonction de décision donnée par :\nt?SV où SV \u003d {t ; ? 0 t \u003e 0} pour le cas séparable et SV \u003d {t ; 0 \u003c ? 0 t \u003c C} pour le cas non séparable, est l\u0027ensemble des vecteurs de support, et ? t ? 0 sont les multiplicateurs de Lagrange.\nDans les cas non linéaire, le principe des SVM est de projeter, par une fonction noyau, les données de départ dans un espace de grande dimension (éventuellement infinie). Ainsi la classification d\u0027un nouvel élément x est donnée par la fonction de décision :\nt?SV où K est la fonction noyau, dont les plus utilisées sont le noyau polynomial\n2 , ? ? IR + . Le choix du noyau et l\u0027optimisation des paramètres de celui-ci reste délicat selon l\u0027application.\nRégression floue et crédibiliste par SVM\nNous avons situé cette approche dans la littérature Laanaya et al. (2006). Ainsi elle est novatrice par la prise en compte des contraintes similaires de normalisation des fonctions de croyance et d\u0027appartenance dans le problème de régression multiple. De plus nous proposons ici d\u0027employer une résolution du problème d\u0027optimisation pouvant gérer de grande quantité de données.\nSoient les vecteurs d\u0027apprentissage x t ? IR d et les fonctions associées y t ? IR N , où N \u003d N c le nombre de classes dans le cas des fonctions d\u0027appartenance et N \u003d 2\nNc dans le cas des fonctions de masse. Ainsi par la régression multiple linéaire, nous cherchons une fonctionnelle f \u003d (f 1 , . . . , f N ) où les f n sont linéaires, de forme f n (x) \u003d w n .x + b n . Nous cherchons à déterminer cette fonctionnelle telle que pour les (x t , y t ) de la base d\u0027apprentissage |y tn ? w n .x t + b n | ne dépasse pas un certain fixé pour tout n. Nous supposons ainsi que tous les points sont à l\u0027intérieur du cylindre défini par Afin de généraliser, nous associons un facteur C pour les points qui sont à l\u0027extérieur du cylindre défini par Le problème d\u0027optimisation convexe revient donc à celui exposé dans la section 3.1, et le critère à minimiser est : sous les contraintes données pour tout t et tout n :\nLe lagrangien est donc donné par :\noù les ?, ?, ? et ? sont les multiplicateurs de Lagrange et sont positifs.\nAu point selle du lagrangien L, on a pour tout t et tout n, ?L/?b n \u003d 0, ?L/?w n \u003d 0, ?L/?? tn \u003d 0 et ?L/?? * tn \u003d 0. Ainsi :\nEn intégrant ces équations (13) dans le lagrangien (équation (12)), le problème revient à maximiser :\nsous les contraintes :\nEnfin, pour prédire la n ème sortie˜ysortie˜ sortie˜y n , d\u0027un nouvel élément x, on calcule :\noù b n est déduite des conditions de Kuhn, Karush et Tucker :\nSi pour un t 0 , ? t0n ?]0, C[ alors, ? t0n \u003d 0, ainsi b n \u003d y t0n ? w n .x t0 ? un raisonnement identique sur ? * donne b n \u003d y t0n ? w n .x t0 + La résolution du système d\u0027optimisation de la régression par SVM pour des problèmes de grande dimension nécessite des mémoires de stockage de grande taille. Ainsi, l\u0027application des algorithmes d\u0027optimisation classiques est difficile. Ces limites ont été constatées dans Laanaya et al. (2006). Une solution est d\u0027utiliser des méthodes d\u0027optimisation itératives, où on essaye de résoudre des sous-problèmes du problème principale. Nous avons adapté la résolution par SMO (Sequential Minimal Optimization) développée par Platt (1998) pour les machines à vecteurs de support, pour notre problème d\u0027optimisation. Il résout des sous-problèmes de dimension deux d\u0027une manière analytique. Nous pouvons ainsi résoudre des problèmes de grande taille avec une vitesse remarquable.\nSi on suppose que la relation entre les x t et les sorties˜ysorties˜ sorties˜y t est non-linéaire, nous pouvons représenter les données de départ en utilisant un noyau. Ainsi le produit scalaire entre les données de la base d\u0027apprentissage peut être donc substitué par un noyau : le produit scalaire x.\n). Une régression linéaire peut alors s\u0027appliquer dans l\u0027espace de représentation. Pour une observation x, la sortie˜ysortie˜ sortie˜y se prédit en considérant les N valeurs :\nA partir de cette approche de régression sur les fonctions d\u0027appartenance ou les fonctions de croyance, nous obtenons un classifieur en prenant la décision via le maximum des fonctions d\u0027appartenance ou le maximum de la probabilité pignistique.\nExpérimentations\nNous présentons ici l\u0027application de notre approche pour la classification des images sonar. En effet, l\u0027environnement sous-marin lui même est très incertain et les systèmes de mesure sont complexes et imprécis. Il est particulièrement important de classifier les fond marins pour de nombreuses applications telles que la navigation et la cartographie sous-marine. Nous trouverons plusieurs études sur la classification de images sonar, citons par exemple Martin et al. (2004), Laanaya et al. (2005a), Laanaya et al. (2005b) et Leblond et al. (2005).\nLes données à classifier sont ainsi entachées de nombreuses imperfections dues aux bruits de mesure, aux interférences des signaux utilisés pour l\u0027acquisition, aux bruits de chatoiement et à la faune et la flore. importantes pour la navigation sous-marine et les sédimentologues. Ainsi la première classe regroupe roche et cailloutis, la deuxième classe les rides et la troisième le sable et les vases. L\u0027unité de classification retenue est l\u0027imagette de taille 32×32 pixels (soit environ 640×640 cm.\nBase de données\nExtraction de paramètres\nAfin de réduire les problèmes de représentativité des imagettes qui comportent plus d\u0027un sédiment et les problèmes liés à l\u0027évaluation (cf. Martin et al. (2006)), nous ne considérons ici que les imagettes homogènes (imagettes avec un seul type de sédiment). Nous avons ainsi 31957 imagettes.\nNous avons calculé sur ces imagettes six paramètres extraits à partir des matrices de cooccurrence calculés sur les imagettes Martin et al. (2004). Les matrices de cooccurrence C d sont calculées en comptant les occurrences identiques de niveaux de gris entre deux pixels contigus dans une direction d donnée. Quatre directions sont considérées : 0, 45, 90 et 135 degrés. Dans ces quatre directions six paramètres d\u0027Haralick sont calculés : l\u0027homogénéité, le contraste, l\u0027entropie, la corrélation et l\u0027uniformité. L\u0027homogénéité qui a une valeur élevée pour des images uniformes ou possédant une texture périodique dans la direction d est donnée par :\noù N G est le niveau de gris des imagettes. L\u0027estimation du contrast est donnée par :\nL\u0027entropie qui a de faibles valeurs s\u0027il y a peu de probabilités de transition élevées dans C d , est définie par :\nLa corrélation entre les lignes et les colonnes de la matrice est donnée par :\noù µ x , ? x , µ y , ? y représentent respectivement les moyennes et écart-types des distributions marginales des éléments de la matrice de cooccurrence. La directivité qui définie l\u0027existence d\u0027une direction privilégiée de la texture est calculée par :\nL\u0027uniformité qui caractérise la proportion d\u0027un même niveau de gris est donnée par :\nNous avons moyenné ces paramètres selon les quatre directions d, ainsi chaque imagette est représentée uniquement par six parammètres.\nModélisation des fonctions floues et crédibilistes\nNous avons utilisé l\u0027approche de Keller et al. (1985) pour calculer la fonction d\u0027appartenance des vecteurs d\u0027apprentissage que nous utiliserons pour l\u0027apprentissage du SVM flou, et l\u0027approche de Denoeux (1995) pour estimer les fonctions de masses que nous utiliserons pour l\u0027apprentissage du SVM crédibiliste. L\u0027approche de Keller et al. (1985) est celle d\u0027un k-plus proches voisins flou. Les fonctions d\u0027appartenance d\u0027un vecteur d\u0027apprentissage x t sont estimées dans un premier temps par :\noù k f est le nombre de plus proches voisins choisi pour le voisinage flou\nDans un second temps, nous calculons la fonction d\u0027appartenance pour un vecteur x à classifier :\nLa norme employée est ici la norme euclidienne. La classe d\u0027appartenance de x est ensuite décidée de manière classique comme la classe donnant le maximum des fonctions d\u0027appartenance prédites par notre régression. L\u0027approche de Denoeux (1995) calcule une estimation des fonctions de masses à partir d\u0027un modèle de distance :\noù C i est la classe associée à x (t,k) , qui sont les k vecteurs d\u0027apprentissage les plus proches de la valeur x et la distance employée est la distance euclidienne. ? i et ? i sont des coefficients d\u0027affaiblissement, et de normalisation. Les k fonctions de masse ainsi calculées pour chaque x sont combinées par la règle orthogonale normalisée de Dempster-Shafer. Cette règle est donnée pour deux experts et pour tout A ? 2 ? , A \u003d ? par :\nLa décision est ensuite prise par le maximum sur les fonctions de masse prédites par notre régression. Dans ce cas, il est équivalent au maximum de probabilité pignistique car les seuls éléments focaux sont les singletons et l\u0027ignorance.\nRésultats\nNous avons effectué un tirage aléatoire de 3000 imagettes homogènes sur toute la base de données (31957 imagettes), ainsi la base d\u0027apprentissage contient des effectifs différents pour les trois classes : 15.53% des imagettes contiennent du roche et cailloutis, 11.85% sont des imagettes rides et les 72.62% restants sont du sable et de la vase. La base de test est constituée de 1000 imagettes choisies de façon aléatoire. Nous avons répété cette opération 10 fois afin d\u0027obtenir des estimations plus fiables des taux de classification.\nNous avons comparé la classification fondée sur les machines à vecteurs de support donnée par le logiciel libSVM de Chang et Lin (2001) et une version modifiée de ce dernier qu\u0027on a développée pour intégrer notre approche.\nLes matrices de confusion normalisées obtenues par le SVM classique (avec les paramètres par défaut de libSVM : noyau gaussien avec ? \u003d 1 et C\u003d1), SVM crédibiliste et SVM flou avec un noyau gaussien, avec ? \u003d 1, C \u003d 1 et \u003d 0. \nConclusion\nNous avons proposé dans ce papier une nouvelle résolution de l\u0027approche de régression floue et crédibiliste à partir de machines à vecteurs de support pour la classification précé-dement introduite. Les résultats obtenus sur les images sonar ont montré l\u0027intérêt de cette approche. En particulier l\u0027approche crédibiliste donne de très bons résultats sur des données faiblement apprises, alors que l\u0027approche floue permet d\u0027avoir une meilleure classification pour chaque classe considérée.\nNous n\u0027avons donné ici que des résultats en utilisant des valeurs empiriques pour les paramètres (C, et ?). Le réglage de ces paramètres peut se faire en utilisant les algorithmes génétiques par exemple, il est possible aussi d\u0027intégrer l\u0027optimisation de ces constantes dans le problème d\u0027optimisation générale des SVM pour la régression.\n"
  },
  {
    "id": "964",
    "text": "Introduction\nLe volume toujours plus important de textes rend l\u0027exploitation de ces derniers par des méthodes automatiques de plus en plus complexes. Face à ce problème, la segmentation thé-matique offre la possibilité d\u0027isoler dans un texte, des segments cohérents du point de vue de leur contenu informationnel. Ainsi, d\u0027autres tâches telles que le résumé automatique ou la recherche d\u0027information par exemple s\u0027en trouve simplifiées. Mais l\u0027on peut imaginer des tâches plus spécifiques telles que la création automatique de table des matières ou de plans à partir d\u0027un gros volume de données non structurées. Nous présentons ici une approche originale de la segmentation thématique en nous appuyant sur les données du défi DEFT\u002706, Azé et al. (2006). Pour son édition 2006, DEFT a fixé comme tâche de retrouver les différents segments théma-tiques d\u0027un grand volume de textes. Trois catégories de textes nous ont été soumises : -un ensemble de discours politiques.\n-un ensemble d\u0027articles de loi.\n-un extrait d\u0027un livre à teneur scientifique. Chacune de ces catégories a été divisées en deux corpus distincts : -Un corpus d\u0027apprentissage, fourni au début du défi avec les segments thématiques éti-quetés, afin d\u0027entraîner nos méthodes. -Un corpus de test, fourni à la fin du défi, sur lequel nous avons été évalués. Un calcul de F score sur les phrases frontières rapportées par les méthodes a permis l\u0027éva-luation des résultats. Les modalités du calcul du F score, et du couple rappel / précision qui lui est lié, dans le cadre de ce défi sont explicités par Azé et al. (2006). La tâche de segmentation thématique peut être assimilée à la détection de frontières. Retrouver les segments thématiques au sein d\u0027un texte, revient à retrouver la première phrase (ou la dernière) de chacun de ces segments. Cette phrase jouerait ce rôle de frontière, si toutefois l\u0027épaisseur de la frontière se limite à la phrase (hypothèse fondamentale de l\u0027évaluation). Les trois catégories de texte sont grandement différentes, et posent donc des problèmes diffé-rents. Dans le cas du corpus de la catégorie scientifique (que nous appellerons corpus « scientifique » par la suite) la segmentation thématique consiste à retrouver les différents paragraphes / chapitres du livre. Il nous faut regrouper les articles appartenant au même texte de loi dans le cas du corpus de la catégorie juridique (que nous appellerons corpus « juridique » par la suite). Enfin le corpus de la catégorie discours politiques (que nous appellerons corpus « discours » par la suite) pose lui un double problème :\n-Il faut séparer entre eux les différents discours du corpus.\n-Au sein même des discours, il faut retrouver les frontières entre les thèmes abordés par l\u0027orateur. Nous sommes donc devant une triple tâche (voire quadruple si l\u0027on considère la double tâche imposée par le corpus « discours »). Dans cet article, nous avons toutefois cherché à aborder cet ensemble de tâches complexes sous un angle unique et original, celui de la cohésion sémantique au sein d\u0027un même thème, cohésion que nous chercherons à caractériser. Après avoir brièvement décrit quelques-unes des méthodes non supervisées les plus courantes à l\u0027heure actuelle dans le domaine de la segmentation thématique, nous présenterons les diffé-rentes étapes de notre démarche, depuis la génération des vecteurs sémantiques jusqu\u0027à l\u0027identification des phrases frontières. Nous finirons sur une analyse de nos résultats dans le cadre de la campagne DEFT\u002706.\nMéthodes de segmentation thématique non supervisées\nLes méthodes de segmentation thématique non supervisées qui ne nécessitent donc ni apprentissage, ni règles, se basent principalement sur la notion de cohésion lexicale observée au travers de la répétition de termes. Par terme, on entend l\u0027unité lexical minimum porteuse de sens, à savoir un mot la plupart du temps, mais parfois un groupe de mots (une collocation), tel que « cul de sac » par exemple. On peut regrouper ces méthodes en trois grandes familles que nous allons présenter ici.\nSegmentation à partir de mesure de similarité entre segments de texte\nLes méthodes de segmentation à base de similarité considèrent les différentes portions de texte du document à traiter comme autant de vecteurs. Les composantes des vecteurs étant, dans la plupart des cas, les fréquences d\u0027apparition des termes au sein de la portion de texte, après que l\u0027on ait retiré les termes inutiles (termes jugés comme peu porteurs de sens) de celleci. Parfois, cette fréquence des termes est pondérée par un IDF (Inverse Document Frequency), pour renforcer l\u0027importance des termes supposés thématiquement saillants. L\u0027objectif de ces méthodes est donc de mesurer la proximité ou l\u0027éloignement des portions de texte étudiées grâce à l\u0027angle que forment leurs vecteurs représentatifs. Elles s\u0027appuient donc en général sur le cosinus de cet angle, qu\u0027elles considèrent comme un indice de similarité. La similarité est ensuite exploitée de diverses manières. Choi (2000), par exemple, utilise la similarité pour effectuer un classement local et cette approche a retenu notre attention. Ces méthodes, notamment l\u0027algorithme c99 de Choi (2000), sont, à l\u0027heure actuelle, parmi les plus performantes. Ces méthodes bien qu\u0027efficaces deviennent rapidement inutilisables à mesure que le volume de données augmente. En effet, ces méthodes s\u0027appuient sur des matrices de similarité entre phrases. Cette approche est donc difficile à mettre en ¡uvre dans le cas de masses de données volumineuses telles que celles issues du défi DEFT\u002706 (pour un volume de 400000 phrases on obtient : 400000 * 400000 \u003d 1.6 * 10 11 entrées dans la matrice ; même en utilisant la symétrie de la matrice pour diviser par deux le nombre d\u0027entrées, ce dernier reste trop élevé).\nSegmentation à partir de représentation graphique de répétition de termes\nEn passant par une représentation graphique des termes, il est plus facile de visualiser leur répartition le long du document étudié. Ainsi, la méthode du nuage de points, présentée par Helfman (1994), emploie cette représentation pour la recherche d\u0027informations. Le principe est de positionner sur un graphique chaque occurrence des termes du document (les termes vides de sens ayant bien entendu été retirés au préalable). Ainsi, un terme apparaissant à une position i et une position j du texte, sera représenté par les 4 couples\nLes portions du document où les répétitions de termes sont nombreuses apparaîtront alors sur le graphique comme les zones de forte concentration de points. Cette approche visuelle de la représentation d\u0027un texte a été reprise et adaptée à la segmentation thématique par Reynar (1998) dans son algorithme DotPlotting. L\u0027idée est d\u0027identifier les segments thématiquement cohérents sur le graphique en cherchant les limites des zones les plus denses. La densité d\u0027une région du graphique est calculée en divisant le nombre de points présents dans la région par l\u0027aire de cette dernière. L\u0027objectif de DotPlotting est d\u0027isoler les segments thématiques soit en maximisant leur densité, soit en minimisant la taille des zones « vides » entre les segments. On notera que, dans son principe, cette méthode est très proche de l\u0027algorithme c99 de Choi (2000) et donne des résultats proches même si elle est un peu moins efficace. Cette approche a même inspiré des méthodes originales, comme celle proposée par Ji et Zha (2003), qui consiste à remplacer le problème de segmentation thématique par un problème de segmentation d\u0027image. Cette méthode utilise une technique de diffusion anisotropique (détec-tion des contours par lissage d\u0027une image) sur la représentation graphique de la matrice de distance afin de renforcer les contrastes entre les zones denses et les frontières. Comme les précédentes approches, ces méthodes montrent vite leur limite face à de gros volume de texte.\nSegmentation à partir de chaînes lexicales\nLa segmentation à base de chaînes lexicales relie les occurrences multiples des termes dans un document et estime qu\u0027une chaîne est rompue si la distance entre deux occurrences du même terme est trop importante. Cette distance est généralement exprimée en nombre de phrases. Ainsi, la méthode Segmenter présentée par Kan et al. (1998), procède selon ce principe pour effectuer une segmentation thématique du document étudié. On notera tout de même une subtilité. La distance à partir de laquelle l\u0027algorithme considère qu\u0027il y a rupture dépend de la catégorie syntaxique du terme impliqué dans le lien. Une autre approche fondée sur les chaînes lexicales est proposée par Hearst (1997) avec son algorithme T ext T illing. Un score de cohésion est attribué à chacun des blocs de texte en fonction du bloc qui le suit. Il est quant à lui calculé sur la base d\u0027un premier score dit « lexical » attribué à chaque paire de phrases en fonction de la paire de phrases qui la suit. Ce score lexical est lui même calculé à partir des paramètres que sont le nombre de termes en commun, de termes nouveaux et de chaînes lexicales actives dans les phrases considérées. Le score de chaque segment de texte est alors le produit scalaire normalisé des scores de chacune des paires de phrases qu\u0027il contient. Si un segment présente un score très différent des segments précédents et suivants, alors la rupture thématique se situe au sein de ce segment. Ces méthodes ne résolvent pas le problème de la taille variable des frontières et / ou de la localisation précise de ces dernières.\nOutre les remarques exprimées sur les limites de ces méthodes par rapport à la tâche demandée, toutes ces approches ont ceci en commun qu\u0027elles s\u0027appuient sur la cohésion lexicaleSYGFRAN (Chauché 1984). Le principe est de projeter un terme ou un groupe de termes dans un espace de concepts de dimension finie. Les concepts de cet espace sont issus d\u0027un thésau-rus « à la Roget ». Dans notre cas, il s\u0027agit du thésaurus Larousse (1992) qui comprend 873 concepts organisés sur 4 niveaux. L\u0027analyseur SYGFRAN construit, pour une phrase donnée, un arbre syntaxique. Les feuilles de cet arbre sont les différents termes de la phrase, les n¡uds de l\u0027arbre sont le regroupement des termes de la phrase en groupe ou proposition. La racine de l\u0027arbre représente donc la phrase elle-même. Chaque n¡ud de l\u0027arbre se voit attribuer un vecteur sémantique qui est une combinaison linéaire des vecteurs sémantiques de ses fils. Ainsi prenons par exemple la phrase « Le calcul du sens, qui dépend de la structure syntaxique, utilise une forme vectorielle. » (figure 1). Dans cette phrase le terme « calcul » peut ramener à plusieurs concepts : les mathématiques, la médecine (calcul biliaire), le comportement (quelqu\u0027un de calculateur), entre autres. Toutefois la présence du terme « vectorielle » dans le groupe verbal va confirmer que nous parlons bien de mathématiques ici. La structure syntaxique entre aussi en ligne de compte dans le calcul du vecteur sémantique, principalement comme élément pondérateur des combinaisons linéaires. Toujours dans la même phrase, le groupe nominal prépositionnel rattaché à « calcul », avec les termes « sens » et « syntaxique » véhicule une forte notion de linguistique. Mais comme ce n\u0027est qu\u0027un groupe nominal prépositionnel son importance est jugée moindre et donc le concept de linguistique sera au final présent dans le vecteur sémantique de la phrase, mais dans une moindre mesure par rapport à celui de mathématiques.\nLe calcul du sens qui dépend de la structure syntaxique utilise une forme vectorelle. \nPostulat sur l\u0027organisation thématique d\u0027un texte\nEn langue française, comme dans toutes les langues, la rédaction d\u0027un texte suit un certain nombre de règles, souvent explicites, mais parfois implicites. Nous sommes partis de la constatation selon laquelle lorsqu\u0027une portion de texte quelconque (paragraphe, chapitre, etc.) traite d\u0027un thème particulier, les premières phrases exposent le sujet abordé, lorsque l\u0027on avance dans le texte, on fait de plus en plus face à des exemples ou des illustrations, pour finir par une ou plusieurs 2 phrases de transitions, qui introduisent le thème suivant. Cette structure, relativement classique, est enseignée dès les premières années d\u0027enseignement secondaire et influence donc la rédaction d\u0027une grande majorité de textes, tant elle est « intégrée » dans notre approche de l\u0027écriture. On peut donc considérer qu\u0027un texte écrit « selon les règles » aura une structure analogue à celle représenté en 2. Ce postulat rejoint les constatations de Chauché et al. (2003).\nFIG. 2 -Structure thématique d\u0027un texte\nCentroïde d\u0027un segment\nEn partant du postulat, précédent nous avons décidé de représenter un segment thématique non pas par l\u0027ensemble des vecteurs sémantiques (et donc des phrases) qui le composent, mais par un centroïde dont le calcul accordera plus d\u0027importance aux premières phrases qu\u0027aux dernières. Le vecteur centroïde est un barycentre dont les composantes sont calculées selon la méthode de Leibniz. Les dimensions de l\u0027espace étant connues (les vecteurs sémantiques comprennent 873 composantes), nous avons pour j \u003d 1 à j \u003d 873, n nombre de vecteurs composant le segment thématique, A l\u0027ensemble de ces vecteurs (A i étant le ième vecteur du segment dans l\u0027ordre d\u0027apparition et x j,A i la jème composante du vecteur A i ) :\navec C le vecteur centroïde du segment thématique, x j,C la jème composante du vecteur C et a i \u003d n + 1 ? i. Ainsi la pondération a i qui détermine l\u0027importance que l\u0027on accorde au vecteur courant dans le calcul du barycentre sera égale à n pour le premier vecteur et à 1 pour le dernier, ce qui va dans le sens du postulat que nous avons énoncé plus haut.\n2 mais généralement un petit nombre\nLa distance thématique\nAfin de pouvoir mesurer la différence thématique entre deux phrases, deux centroïdes ou encore entre une phrase et un centroïde il nous faut disposer d\u0027une fonction similarité ou d\u0027une distance. Nous avons choisi d\u0027adopter la distance thématique présentée par Lafourcade et Prince (2001). Ainsi, si X et Y sont deux vecteurs, D A étant la distance thématique recherchée, on a :\nLa distance D A est donc la distance angulaire entre les deux vecteurs X et Y exprimée en radians. Classiquement, le cosinus fait office de mesure de similarité en TALN. Le choix d\u0027appliquer l\u0027arc cosinus pour retrouver la distance angulaire s\u0027est fait pour deux raisons :\n-Elle correspond à une distance et présente donc l\u0027avantage d\u0027être réflexive, symétrique et de respecter l\u0027inégalité triangulaire. -L\u0027arccosinus est une fonction décroissante par rapport à la similarité, mais surtout fortement non linéaire pour des valeurs d\u0027angles faibles (inférieur à ? 4 ), alors qu\u0027elle se comporte de manière quasi linéaire pour les valeurs d\u0027angles élevées. Ainsi on obtient une plus grande finesse d\u0027analyse lorsque deux phrases sont sémantiquement proches. En nous appuyant sur ces différents outils, nous pouvons maintenant nous attacher à détec-ter les zones de transitions au sein d\u0027un texte.\nDétection des zones de transition\nAfin de détecter les zones de transition abordées plus haut, nous faisons glisser une fenêtre le long du texte et attribuons à la phrase centrale de la fenêtre une valeur qui correspond à la distance thématique entre le centroïde calculé à partir des phrases précédant la phrase centrale dans la fenêtre (la phrase centrale exclue), et le centroïde calculé à partir de toutes les phrases suivant la phrase centrale (cette dernière étant cette fois incluse dans le centroïde comme le montre la figure 3).\nNous nous sommes servi du corpus d\u0027apprentissage fourni par DEFT\u002706 plus comme un corpus de calibrage que d\u0027apprentissage. Ainsi la taille de la fenêtre est de deux fois la taille moyenne d\u0027un segment calculée sur le corpus d\u0027apprentissage, une taille est calculée par type de texte. On suppose donc que le corpus de test, sensé être jumeau du corpus d\u0027apprentissage, présentera les même caractéristique. Une fois la distance thématique estimée, on la compare avec un seuil à partir duquel on considère qu\u0027il y a de fortes chances pour que la phrase fasse partie d\u0027une zone de transition. Ce seuil est calculé à partir de chacun des corpus d\u0027apprentissage comme suit :\n1 Sur chaque corpus nous avons calculé les distances qui séparent chaque segments successif. 2 Nous avons calculé la moyenne et l\u0027écart type de ces distances. 3 Le seuil est égal à la moyenne moins une fois l\u0027écart type. L\u0027usage de la plus petite distance observée sur le corpus comme valeur seuil a été envisagé, mais sur un tel volume de données traité il y a forcement des accidents et des valeurs particulières. Utiliser la moyenne n\u0027aurait pas forcément été judicieux (éliminant trop de solutions et\nFIG. 3 -Attribution d\u0027une valeur de distance thématique à chaque phrase\nfaisant ainsi chuter le rappel). En utilisant un seuil égal à la moyenne moins l\u0027écart type, on se prémunit des valeurs aberrantes qui pourraient survenir tout en étant moins restrictif que si l\u0027on utilisait la moyenne seulement. Bien entendu, cela implique que nous supposons que le phénomène suit une loi normale. On notera que sur chacun des corpus ce seuil est proche de ? 4 , malgré la différence de structure et de discours qu\u0027il existe entre les corpus. Au final, on obtient deux tableaux, l\u0027un contenant des distances thématiques, l\u0027autre des valeurs booléennes indiquant pour chaque phrase si elle fait partie d\u0027une zone de transition ou non. Toujours dans un souci d\u0027éviter les valeurs singulières, on élimine d\u0027office toutes les phrases marquées comme zone de transition potentielle qui seraient isolées. Il nous reste à déterminer au sein de cette zone de transition quelles sont les phrases qui constituent vraiment une amorce de segment thématique. Pour ce faire, nous procédons de manière différente selon les corpus.\nLes corpus scientifique et discours et la notion de phrase charnière\nToujours en nous appuyant sur la conception « classique » de la rédaction en langue française, nous avons émis l\u0027hypothèse que pour qu\u0027un texte soit bien construit, il doit comporter des phrases de transition ou phrases charnières (à ne pas confondre avec la zone de transition qui englobe la phrase de transition et les phrases adjacentes) entre chaque portion de texte thématiquement cohérente. Elles ont la particularité d\u0027être la plupart du temps peu porteuses de thème, servant avant tout de lien logique entre deux parties d\u0027un texte. Se trouvant à la frontière entre deux segments thématiques sans avoir de véritable importance thématique, la distance thématique d\u0027une phrase de ce type au centroïde du segment thématique qui la pré-cède doit être proche de la distance au centroïde du segment suivant. Nous avons donc attribué à chacune des phrases traitées un score de transition. Si on désigne par St i le score de transition de la phrase i alors on a :\nOù D p est la distance de la phrase examinée au centroïde du segment thématique précédent et D s la distance au centroïde du segment thématique suivant. Cette valeur est comprise entre 0 et 1 et se rapproche de 1 à mesure que les distances entre la phrase examinée et les centroïdes des segments thématiques adjacents se rapprochent. Si les deux distances sont égales (et donc que la phrase centrale est équidistante des deux segments thématiques) elle vaut 1. Si, au contraire, une des distances vaut ? 2 et l\u0027autre 0 (et donc que la phrase centrale est complè-tement intégrée thématiquement à l\u0027un des segments et pas du tout à l\u0027autre) alors cette valeur vaut 0. Du fait de ces propriétés, nous pouvons utiliser ce score pour pondérer les distances thématiques des phrases suivantes.\nA l\u0027étape précédente, nous avons isolé de petites portions de texte susceptibles de contenir la phrase d\u0027amorce d\u0027un segment thématique. Ici, nous allons déterminer quelle phrase au sein de cette portion est la plus susceptible d\u0027être la première phrase d\u0027un segment thématique. Pour ce faire, nous partons du principe qu\u0027une phrase est la première phrase d\u0027un nouveau segment thématique si :\n-La distance thématique qui lui a été attribuée est la plus élevée.\n-La phrase qui la précède a de fortes chances d\u0027être une phrase de transition. Comme il est peu probable que ces 2 conditions soient réunies simultanément, nous associons à chaque phrase de la zone de transition, une nouvelle valeur qui est le produit de la distance thématique attribuée à la phrase avec le score de transition de la phrase qui la précède. Il ne nous reste plus qu\u0027à sélectionner le maximum.\nLe corpus juridique et son traitement simplifié\nLa structure même d\u0027un texte juridique exclut les phrases de transitions entre les articles. Il n\u0027aurait donc pas été pertinent de rechercher ces dernières dans le cadre du traitement du corpus juridique. Toutefois, le corpus « loi » se présente sous la forme d\u0027une succession d\u0027articles tous précédés de la mention « Article X » (le X remplaçant le numéro de l\u0027article afin que ce dernier ne soit pas immédiatement identifiable comme le premier d\u0027une loi). Nous avons choisi de continuer à chercher les zones de transition selon la méthode présen-tée plus haut, mais pour déterminer quelle était la phrase d\u0027amorce au sein de ces groupes de phrases nous recherchions simplement la phrase « Article X ».\nRésultats expérimentaux\nPour évaluer les résultats, l\u0027équipe organisatrice s\u0027est appuyée sur trois calculs de F score différents. Si tous donnent la même importance à la précision et au rappel, les trois F scores se différencient par un certain degré de tolérance à l\u0027erreur. Si le F score strict ne se calcul qu\u0027en considérant les phrases ramenées, les F scores souples de taille 1 et 2 considèrent également les phrases autour de la phrase ramenée (immédiatement adjacentes pour la taille 1 et éloignées d\u0027une phrase pour la taille 2).\nLes résultats obtenus sont malheureusement partiels du fait d\u0027un problème de temps lié au pré-traitement du corpus (le corpus « loi » n\u0027étant traité qu\u0027au quart dans la deuxième exécu-tion). La première exécution se base sur une méthode proche de la méthode décrite dans cet Toutefois, même partiels, ces résultats nous permettent de faire un certain nombre de constatations : -Ce qui ressort avant tout de ces résultats, c\u0027est la faible précision de la méthode par rapport à la moyenne des participants. Ce manque de précision peut aisément s\u0027expliquer. En effet, l\u0027objectif de cette méthode est de détecter les zones au sein du texte où le thème change, pas la phrase exacte qui marque ce changement. -Le recour à un calcul de F score souple pour l\u0027évaluation de cette tâche se voit totalement justifié. En effet, si les résultats avec un calcul de F score strict sont décevants, dès que l\u0027on prend un tant soit peu de marge, ces derniers accusent une hausse significative. Cette remarque renforce l\u0027idée qu\u0027une frontière thématique est plus une zone floue, qu\u0027une unité bien définie. -Le très fort rappel obtenu sur le corpus discours laisse supposer que les textes politiques obéissent probablement à un schéma d\u0027organisation thématique. Il doit être possible d\u0027extraire ou d\u0027approximer ce dernier de manière algorithmique. On notera, qu\u0027à l\u0027exception d\u0027une autre équipe que la nôtre, tous les participants au défi se sont appuyés sur des méthodes numériques dérivées de celle présentées au début de cet articles. Les équipes ayant utilisé des approches sémantiques et structurelles (dont la notre) ce situe dans le milieu de tableau (nous sommes 4ème sur le défi, l\u0027autre équipe étant 5ème). Ces approches, étant encore jeunes comparées à des approches numériques, doivent être perfectionnées et ont donc une grande marge de progression devant elles.\nConclusion\nDans cet article, nous avons présenté une méthode originale de segmentation thématique qui s\u0027appuie sur une approche sémantique. Cette dernière a déjà été testée sur différents domaines. D\u0027abord en catégorisation de texte, où elle a donné de bons résultats, puis lors de la précédente édition de DEFT, pour identifier des auteurs, où elle a été moins performante. Autour de cette représentation plus sémantique du texte, nous avons étudié une méthode intégrant des contraintes stylistiques pour segmenter thématiquement le texte. Nous regrettons de n\u0027avoir pu être évalués sur un jeu de données complet, toutefois les résul-tats obtenus, même partiels, nous laissent entrevoir des possibilités que nous allons explorer en dehors du cadre parfois restrictif d\u0027une situation d\u0027évaluation. Notre classement en milieu de tableau lors de l\u0027atelier DEFT\u002706 (4ème sur 7) prouve que si notre approche n\u0027est pas aussi efficace que les approches à base de méthodes numériques, elle reste viable, et mérite d\u0027être approfondie. Même si l\u0027utilisation d\u0027un F score souple permet d\u0027avoir une meilleure vision de l\u0027efficacité des méthodes, le découpage même des textes peut être sujet à contestation. La notion de thème telle qu\u0027elle est abordée dans le défi DEFT\u002706, à savoir l\u0027idée directrice d\u0027un segment de texte, est très subjective. Peut-on affirmer que les différents paragraphes du corpus scientifique forment bien des segments thématiques distincts ? Le découpage des discours politiques est-il approprié ? Sans mettre en doute la compétence des experts qui ont préparé ce corpus, d\u0027autres experts auraient-ils découpé les corpus de la même manière ? Il pourrait être instructif de procéder à l\u0027évaluation autrement, en proposant par exemple à des\n"
  },
  {
    "id": "965",
    "text": "Introduction\nCet article présente un outil de recherche d\u0027information associant sémantique et contexte conceptuel. Notre objectif est d\u0027utiliser conjointement l\u0027analyse conceptuelle et la sémantique afin de fournir des réponses contextuelles aux requêtes des utilisateurs sur le web.\nDans cet article, nous présentons notre méthodologie et nous l\u0027illustrons par une recherche d\u0027information effectuée sur un ensemble de pages web relatives au domaine du tourisme. Le processus de recherche d\u0027information est divisé en deux étapes :\n-traitement hors ligne de pages web ; -traitement contextuel en ligne de requêtes utilisateurs. Le prétraitement consiste à construire un treillis conceptuel à partir de pages web, par exemple dans le domaine du tourisme, de manière à obtenir un contexte conceptuel global ; cette notion est définie dans la section 3.2. Chaque concept du treillis correspond à un groupe de pages web ayant des propriétés communes. Un appariement sémantique est effectué entre les termes décrivant chaque page et un thésaurus du domaine du tourisme (thésaurus de l\u0027Organisation Mondiale du Tourisme), permettant de labelliser chaque concept de façon standardisée.\nTandis que le traitement des pages web est effectué hors ligne, la recherche d\u0027information se fait en temps réel : les utilisateurs formulent leurs requêtes à l\u0027aide des termes du thesaurus. Cette classe de termes est alors comparée avec les labels des concepts et les concepts les plus pertinents sont délivrés à l\u0027utilisateur. Celui-ci peut alors naviguer à travers le treillis de manière à généraliser ou, au contraire, à spécialiser sa requête.\nCette méthode présente plusieurs avantages : -les résultats sont fournis à la fois en fonction du contexte de la requête et du contexte des données disponibles. Par exemple, seuls les raffinements de requêtes correspondant à des pages touristiques existantes sont proposés ; -l\u0027ajout de sémantique peut dépendre de l\u0027utilisateur cible ; -une sémantique plus puissante, comme les ontologies, peut être ajoutée. Ceci permet d\u0027améliorer la formulation des requêtes et la pertinence des résultats. Cet article est organisé de la manière suivante : la section 2 introduit la notion de contexte, dans le sens général et dans le domaine de l\u0027informatique. La section 3 décrit brièvement l\u0027analyse formelle de concepts et les treillis de Galois, puis définit notre notion de contexte conceptuel global et instantané. Enfin, nous concluons et donnons quelques perspectives concernant la poursuite et l\u0027application de ces travaux.\nNotion de contexte\nUn contexte est une notion abstraite et ne peut pas être défini de manière précise puisqu\u0027il est lié à une situation particulière. Nous avons tendance à associer un contexte de manière implicite à un ensemble d\u0027actions, une attitude, etc. dans des situations courantes. Des définitions de la notion de contexte ont émergé en psychologie cognitive, philosophie, ainsi que dans des domaines de l\u0027informatique comme le traitement du langage naturel.\nLe concept de contexte formel a été introduit par McCarthy dans (McCarthy 1968(McCarthy , 1987. Selon Giunchiglia, qui a également effectué des travaux de recherche sur la formalisation de contexte, « un contexte est une théorie sur le monde qui englobe les perspectives subjectives des individus ». Cette théorie est partielle -incomplète -et approximative du fait que le monde n\u0027est jamais décrit dans tous ses détails (Giunchglia, 1993).\nLa notion de contexte est importante pour beaucoup de communautés de recherche comme l\u0027intelligence artificielle, la résolution de problèmes, etc. (Brezillon 1999ade problèmes, etc. (Brezillon , 1999b, (Theodorakis et Spyratos, 2002). En ce qui concerne l\u0027intelligence artificielle, l\u0027interaction entre contextes se fait au moyen de règles, qui permettent de naviguer d\u0027un contexte à un autre (Guha et McCarthy, 2003). Les contextes peuvent être représentés par des graphes contextuels, des topic maps, les logiques de description avec notamment les extensions OWL, etc.\nComme dans le cas du web sémantique, le contexte est utilisé soit en tant que filtre dans un but de désambiguïsation pour la recherche d\u0027information (Dolog et al, 2006), soit pour définir des services web contextuels (Mrissa et al, 2006), ou enfin comme un moyen d\u0027intégrer ou de fusionner des ontologies, (Bouquet et al, 2004), (Doan et al, 2002). Un contexte peut être spécifié à différents niveaux de granularité (document, page web, etc.). Cette information additionnelle peut être liée à chaque ressource.\nRNTI -X -\nContextes conceptuels -Relation avec les ontologies\nDans la section précédente, nous avons présenté diverses définitions de la notion de contexte. Dans cet article, nous définissons des contextes conceptuels, basés sur l\u0027analyse formelle de concepts, en particulier les treillis de Galois. Beaucoup de travaux de recherche ont appliqué les treillis de concepts à la recherche d\u0027information (Priss, 2000). Les concepts formels peuvent être vus comme des documents pertinents pour une requête donnée. L\u0027introduction d\u0027une ontologie de domaine, combinée avec les treillis de concepts pour améliorer la recherche d\u0027information est plus récente. (Messai et al, 2005) proposent une approche basée sur l\u0027analyse formelle de concepts pour classifier et rechercher des sources de données pertinentes pour une requête donnée. Ces travaux ont été appliqués à des données de bioinformatique. Un treillis de concepts est construit en fonction des métadonnées associées aux sources de données. Puis, un concept construit à partir d\u0027une requête donnée est fusionné à ce treillis de concepts. Dans cette approche, le raffinement de requête s\u0027effectue en utilisant une ontologie de domaine. Le processus de raffinement d\u0027OntoRefiner, outil dédié aux portails web sémantiques (Safar et al, 2004), est basé sur l\u0027utilisation d\u0027une ontologie de domaine pour construire un treillis de Galois pour le processus de raffinement de requête. L\u0027ontologie de domaine évite de construire complètement le treillis ; ce travail vise à améliorer la construction du treillis, ce qui n\u0027est pas l\u0027objectif de notre travail. Enfin, le système CREDO (Carpineto et Romano, 2004) permet à l\u0027utilisateur d\u0027interroger des documents web et de voir les résultats à travers la navigation dans un treillis de concepts (http://credo.fub.it). (Dolog et al, 2006) ont proposé une méthode pour relâcher automatiquement des requêtes trop contraintes en se basant sur la connaissance du domaine et les préférences utilisateur. Leur approche combine raffinement et relaxation de manière à permettre un accès personnalisé à des données RDF hétérogènes. Contrairement à cette approche, notre méthode est dédiée à des requêtes imprécises et centrées utilisateurs.\nDans notre proposition, les treillis de Galois sont construits pour représenter le contenu de pages web. L\u0027utilisateur peut alors naviguer dans ces treillis de manière à raffiner ou généraliser sa requête. Comparativement aux approches décrites ci-dessus, notre méthode n\u0027est pas seulement dédiée à la recherche d\u0027information mais peut être utilisée pour d\u0027autres objectifs comme le peuplement d\u0027ontologies, la comparaison de sites web à travers leurs treillis respectifs, une aide au concepteur de site web pour vérifier que le contenu du site reflète bien le message qu\u0027il a voulu faire passer, etc.\nCette section est organisée de la manière suivante : après une brève introduction aux treillis de Galois, nous proposons notre définition de contextes conceptuels global et instantané.\nIntroduction à l\u0027Analyse Formelle de Concepts et aux Treillis de Galois\nL\u0027Analyse Formelle de Concepts est une approche mathématique de l\u0027analyse de données qui permet de fournir une structure à l\u0027information. Cette approche peut être utilisée pour le clustering conceptuel, comme montré dans (Carpineto et Romano, 1993) et dans (Wille, 1984).\nRNTI -X -La notion de treillis de Galois établissant une relation entre deux ensembles est à la base d\u0027un ensemble de méthodes de classification conceptuelles. Cette notion fut introduite par (Birkoff, 1940) et (Barbut et Monjardet, 1970). Les treillis de Galois consistent à regrouper des objets en classes qui vont matérialiser les concepts du domaine d\u0027étude. Les objets individuels sont discriminés en fonction de leurs propriétés communes, ce qui permet d\u0027effectuer une classification sémantique. L\u0027algorithme que nous avons implémenté est basé sur celui proposé dans (Godin, 1998).\nNous introduisons tout d\u0027abord les principaux concepts des treillis de Galois. Soient deux ensembles finis E et E\u0027 (E est un ensemble d\u0027objets et E\u0027 est l\u0027ensemble de leurs propriétés), et une relation binaire R ? E x E\u0027 entre ces deux ensembles. La figure 1 montre un exemple de relation binaire entre deux ensembles. Selon la terminologie de Wille (Wille, 1992), le triplet (E, E\u0027, R) est un contexte formel correspondant à un unique treillis de Galois. Il représente des regroupements naturels d\u0027éléments de E et E\u0027.\nSoient P(E) une partition de E et P(E\u0027) une partition de E\u0027. Chaque élément du treillis est un couple, appelé aussi concept, noté (X, X\u0027). Un concept est composé de deux ensembles X ? P(E) et X\u0027 ? P(E\u0027) satisfaisant les deux propriétés suivantes :\nUn ordre partiel sur les concepts est défini de la manière suivante :\nCet ordre partiel est utilisé pour dessiner un graphe appelé diagramme de Hasse, illustré par la figure 1. Il existe un arc entre deux concepts C 1 et C 2 si C 1 \u003cC 2 et s\u0027il n\u0027y a pas d\u0027autre élément C 3 dans le treillis tel que C 1 \u003cC 3 \u003cC 2 . Dans un diagramme de Hasse, la direction des arcs est par convention toujours dans le même sens, soit vers le haut. Le graphe peut être interprété comme une représentation de la relation de généralisation / spécialisation entre les couples, où C 1 \u003cC 2 signifie que C 1 est plus général que C 2 (et C 1 est situé au-dessus de C 2 dans le diagramme).\nFig. 1. Relation binaire et treillis de Galois associé (Diagramme de Hasse).\nRNTI -X -Le treillis de concepts montre les points communs entre les objets du domaine. La première partie d\u0027un concept est l\u0027ensemble des objets et est appelée l\u0027extension. La seconde partie -l\u0027intension -révèle les propriétés communes des objets de l\u0027extension. La partie droite de la figure 1 montre un exemple de treillis de concepts. Par exemple, dans ce treillis, le concept {(1, 4), (c, f, h)} contient dans son extension les objets 1 et 4, qui ont les propriétés c, f et h en commun. 1. Du fait des aspects fortement exploratoires et itératifs du processus de recherche d\u0027information, la principale propriété de ce contexte conceptuel instantané est qu\u0027il évolue à chaque fois que l\u0027utilisateur modifie, raffine ou généralise sa requête. 2. La variation de ce contexte conceptuel instantané est bornée par le contexte conceptuel global, comme illustré dans la figure 3. 3. Une page web donnée peut appartenir à différents contextes conceptuels instantanés du fait des relations de généralisation/spécialisation inhérentes aux treillis de Galois.\nDéfinition d\u0027un contexte conceptuel\nEnfin, l\u0027information fournie par le contexte conceptuel (global ou instantané) est complémentaire à l\u0027information intrinsèque des pages web (les propriétés, dans notre cas, représentent les termes les plus significatifs de la page).  \nRNTI -X -\nMéthodologie pour une coordination sémantique de contextes conceptuels et d\u0027ontologies\nNotre méthodologie de recherche d\u0027information sémantique et contextuelle peut être divisée en deux étapes : un pré traitement hors ligne et un traitement contextuel en ligne des requêtes des utilisateurs. Dans cette section, nous décrivons ces opérations et les illustrons avec un exemple simple dans le domaine du tourisme.\nEtape 1: prétraitement hors ligne\nSélection de sites web de tourisme\nLa première étape consiste à construire un contexte conceptuel global dans un domaine donné et sur un ensemble de pages web pertinentes. Ces pages peuvent appartenir à plusieurs sites web. Pour des besoins d\u0027illustration, nous avons sélectionné 5 pages web à partir d\u0027un site touristique français (le site de la mairie de Metz) : http://tourisme.mairie-metz.fr/. Cet ensemble de pages web constitue l\u0027information de base à partir de laquelle la recherche d\u0027information sémantique et contextuelle est réalisée.\nAnalyse des pages Web : génération des données d\u0027entrée pour le treillis de Galois (objets et propriétés)\nLe contexte conceptuel global est représenté par le treillis de Galois construit à partir de l\u0027ensemble des pages web sélectionnées ; nous devons générer des données en entrée appropriées pour le calcul du treillis conceptuel :\n-Chaque page web correspond à un objet;\nRNTI -X --Les propriétés d\u0027une page web sont les noms significatifs les plus fréquents de la page (ils sont extraits à l\u0027aide de notre outil basé sur Tree-Tagger comme l\u0027illustre la figure 4). La liste des objets et de leurs propriétés est stockée dans une base de données Mysql, comme le montre la figure 4. Par exemple, l\u0027objet 3 (correspondant à une page web spécifique) est décrit par les propriétés spectacle et réservation.\nFig. 4. Extraction d\u0027objets/propriétés et génération de la base de données.\nConstruction du treillis de Galois\nLa base de données décrite ci-dessus -contenant des objets et propriétés -est utilisée comme entrée pour la construction d\u0027un treillis de Galois. Nous utilisons un algorithme existant pour la construction incrémentale du treillis de Galois (Godin, 1998), car notre contribution concerne l\u0027exploitation et l\u0027interprétation de ces treillis et non l\u0027optimisation de leur construction. La construction du treillis consistant à identifier et structurer tous les regroupements d\u0027objets en fonction de leurs propriétés communes, le nombre de concepts du treillis devient rapidement élevé si le nombre d\u0027objets et/ou de propriétés est important. Dans le cas de sites possédant de nombreuses pages, cela aura donc un impact sur le temps de calcul. Ceci n\u0027est pas trop pénalisant dans notre approche puisque cette étape est effectuée lors d\u0027une phase de prétraitement.\nEn sortie, nous obtenons un treillis de concepts où chaque concept consiste en un ensemble d\u0027objets ayant des propriétés communes. La liste des objets d\u0027un concept est appelée l\u0027extension d\u0027un concept et les propriétés partagées correspondantes constituent l\u0027intension du concept. Le treillis généré à partir de la base de données de la figure 4 est illustré sur la figure 5.\nRNTI -X -\nFig. 5. Le contexte conceptuel global : le treillis de Galois.\nLe treillis de la figure 5 contient 12 noeuds (concepts). Chaque concept est caractérisé par son extension -la liste des objets qu\u0027il contient -et son intension -la liste des propriétés communes des objets de l\u0027extension. Le noeud en haut du treillis est le plus général et contient tous les objets, qui n\u0027ont pas de propriétés communes. Plus nous descendons dans le treillis, plus les noeuds deviennent spécifiques. Lors de l\u0027interprétation du treillis, il est intéressant d\u0027étudier quels objets de l\u0027extension sont similaires à d\u0027autres.\nCe treillis nous montre que l\u0027objet 3 apparaît seulement dans deux concepts car il possède seulement deux propriétés et ne partage que la propriété réservation avec les objets 1, 2 et 4. Nous pouvons également remarquer qu\u0027aucun objet ne partage la propriété spectacle avec l\u0027objet 3. Au contraire, l\u0027objet 2 apparaît dans cinq concepts car il possède plus de propriétés (4) que l\u0027objet 3 et aussi parce que ses propriétés sont partagées par plus d\u0027objets (4) que l\u0027objet 3.\nL\u0027interprétation que nous pouvons faire de ce treillis est que : -Le contexte conceptuel global du site touristique de la ville de Metz indique que celuici est moins ciblé vers le spectacle et plus dirigé vers les centres d\u0027intérêts de la ville elle-même. -Les requêtes des utilisateurs sur les spectacles ne pourront pas être raffinées mais généralisées sur le concept réservation, et de ce fait, sont fortement bornées par le contexte conceptuel global. En d\u0027autres termes, si un utilisateur est plus intéressé par un tourisme basé sur le spectacle, la ville de Metz n\u0027est pas l\u0027endroit le mieux adapté (au vu des informations disponibles sur le Web). L\u0027analyse des requêtes des utilisateurs va permettre de déterminer si l\u0027utilisateur est plus intéressé par les spectacles ou par une autre forme de tourisme. Dans ce RNTI -X -cas, il sera possible de lui proposer un autre contexte conceptuel global plus orienté \"spectacles\". Ce nouveau contexte peut être relié au contexte conceptuel global du tourisme de la ville de Metz en réalisant un appariement avec des ontologies comme nous le proposons dans notre conclusion.\nLabellisation des concepts du treillis\nLa dernière étape du traitement hors ligne consiste à affecter un label aux concepts du treillis de manière normalisée, c\u0027est à dire en utilisant les termes d\u0027un thesaurus ; nous avons utilisé le thesaurus de l\u0027Organisation Mondiale du Tourisme qui décrit le tourisme et les activités de loisirs. La même opération peut être réalisée avec une ontologie plutôt qu\u0027un thesaurus.\nL\u0027analyse des pages web consiste à extraire les mots les plus fréquents d\u0027une page (voir section 4.1.2). Notre analyse va plus loin en réalisant un appariement syntaxique entre ces mots fréquents et les termes du thesaurus de l\u0027OMT. Par exemple, la propriété spectacle est liée à l\u0027entrée loisirs du thesaurus. Le label normalisé d\u0027un concept est constitué de l\u0027ensemble des labels normalisés associés aux objets de son extension.\nLe lien entre les données textuelles -les pages web de tourisme -et une structure sémantique -le thesaurus de l\u0027OMT -nous permet d\u0027obtenir un contexte conceptuel global plus riche. En effet, ce contexte reflète aussi bien l\u0027information issue des données d\u0027origine que les connaissances générales du domaine.\nEtape 2: traitement en ligne des requêtes des utilisateurs\nUne fois le contexte conceptuel global construit, le contexte conceptuel instantané est calculé pour chaque requête utilisateur.\nFormulation d\u0027une requête avec des mots-clés\nLes utilisateurs formulent leurs requêtes avec des mots-clés (nous pouvons restreindre ces mots-clés aux entrées du thesaurus ou de l\u0027ontologie). L\u0027intérêt d\u0027avoir normalisé les labels des objets et concepts est la possibilité d\u0027utiliser un vocabulaire contrôlé.\nIdentification des concepts les plus pertinents du treillis\nLa réponse à une requête correspond au concept dans le treillis dont les propriétés de l\u0027intension s\u0027apparient le mieux avec les mots-clés de cette requête. S\u0027il n\u0027existe pas de concept correspondant parfaitement, les concepts les plus pertinents sont proposés à l\u0027utilisateur à travers un raffinement ou une généralisation, qui s\u0027effectue en fonction des données disponibles (dans les pages web de tourisme), c\u0027est-à-dire correspondant au contexte des données.\nLa figure 6 montre une autre représentation visuelle du treillis de Galois. Cette interface est plus riche du fait qu\u0027elle montre également les liens avec les données sémantiques -les entrées du thesaurus de l\u0027OMT. Supposons que l\u0027utilisateur entre une requête avec le mot-clé réservation. Le contexte conceptuel instantané correspondant à cette requête est illustré dans la fenêtre de gauche avec une focalisation sur le concept du treillis (concept numéro 5), dont l\u0027extension est {page1, page2, page3, page4} et la propriété commune entre ces objets est RNTI -X -{réservation}. Nous pouvons également noter que ce concept est labellisé avec trois concepts du thesaurus: monument, loisirs, europe.\nFig. 6. Navigation à travers le contexte global enrichi avec de l\u0027information sémantique\nA partir de ce contexte instantané (noeud numéro 5), l\u0027utilisateur est libre de naviguer soit dans d\u0027autres noeuds du treillis -à travers des concepts plus généraux et plus spécifiques -soit dans le thesaurus.\nLa partie en haut à droite de la figure 6 donne un aperçu de l\u0027entrée monument du thesaurus, et la partie gauche de la fenêtre montre la hiérarchie du thesaurus -monument est une sous-classe de tourisme et une superclasse de architecture, musée et patrimoine-de même que les autres concepts du treillis labellisés avec l\u0027entrée monument-soient les concepts 1, 3,5,9,10,7,4 et 11. Encore une fois, la navigation peut se poursuivre soit à travers le thesaurus ou en revenant sur le treillis conceptuel. L\u0027avantage de cette interface de navigation est que l\u0027utilisateur peut explorer facilement les données via le thesaurus -ou une ontologie -et le contexte conceptuel global, ceci de manière totalement transparente. L\u0027élargissement du contexte des données avec la connaissance du domaine véhiculée par un thesaurus ou une ontologie permet de fournir des réponses plus riches et plus pertinentes aux requêtes des utilisateurs. Cette interface n\u0027est qu\u0027un exemple de représentation et de navigation illustrant l\u0027utilisation des contextes conceptuels. Dans l\u0027avenir, nous nous pencherons plus en détail sur la question des interfaces afin qu\u0027elles permettent d\u0027optimiser l\u0027utilisation de ces contextes globaux et instantanés.\nConclusion et perspectives\nDans ce papier, nous avons présenté une méthodologie combinant l\u0027analyse conceptuelle avec la sémantique. Notre méthodologie est divisée en deux étapes ; la première consiste en RNTI -X -un prétraitement hors ligne d\u0027un ensemble de pages web à partir desquelles un treillis conceptuel est construit. Chaque concept correspond à un cluster de pages web partageant un ensemble de propriétés (un ensemble de termes communs). Puis, les termes pertinents des pages web sont mis en correspondance avec un thesaurus du domaine du tourisme, permettant ainsi d\u0027affecter des étiquettes (labels) à chaque concept du treillis de manière normalisée. La deuxième étape est une phase de traitement contextuel des requêtes des utilisateurs. Les termes constituant la requête sont alors comparés avec les labels des concepts. L\u0027utilisateur peut ensuite naviguer comme il le souhaite dans le treillis pour raffiner ou bien généraliser sa requête ; il peut également naviguer à travers la structure sémantique -thesaurus ou ontologie -s\u0027il a besoin de la connaissance du domaine pour affiner sa recherche.\nNous avons illustré cette méthodologie dans le domaine du tourisme. Un thesaurus a été utilisé, mais les travaux futurs prévoient d\u0027étendre cette méthodologie aux ontologies. L\u0027avantage des ontologies ou des thesaurus pour les contextes est qu\u0027il est possible de lier divers contextes à travers ces structures sémantiques. Le contexte peut être élargi par l\u0027ontologie. Inversement, l\u0027avantage des contextes pour les ontologies est que le fait de les relier à différents contextes revient à les instancier avec ces différents contextes. Nous pouvons donc voir ces contextes comme un moyen de peupler les ontologies.\nDans ce papier, nous avons montré que l\u0027utilisation conjointe de treillis conceptuels et de la sémantique pouvait permettre d\u0027obtenir des résultats intéressants pour la recherche d\u0027information sémantique et contextuelle. Mais cette combinaison de l\u0027analyse formelle de concepts et de la sémantique peut également être utilisée dans le cadre d\u0027objectifs différents, comme par exemple le peuplement d\u0027ontologies. Cette méthodologie peut également permettre de comparer différents sites web à travers leurs treillis respectifs. Un autre aspect intéressant est que, les termes correspondants à des points d\u0027entrée d\u0027un ou plusieurs treillis, les utilisateurs vont pouvoir naviguer d\u0027un treillis à l\u0027autre pour reformuler leurs requêtes. Enfin, une autre application possible est d\u0027aider les concepteurs d\u0027un site web du fait que le treillis va refléter le contenu du site. Il sera alors aisé de comparer le site web/treillis résultant avec les objectifs initiaux du concepteur de site, et ainsi de vérifier si le site est bien conforme à ces objectifs.\n"
  },
  {
    "id": "966",
    "text": "Introduction\nL\u0027un des problèmes majeurs rencontrés dans la fouille des règles d\u0027association valides au sens de la confiance est le nombre souvent très élevé de ces règles. Plusieurs solutions à ce problème ont été proposées ou considérées dans la littérature. Parmi ces solutions figurent les bases, c\u0027est-à-dire, des familles génératrices minimales (Zaki et Ogihara, 1998;Pasquier et al., 1999). La plupart de ces bases se caractérisent en terme d\u0027un opérateur de fermeture de Galois sur l\u0027ensemble des motifs du contexte considéré. Or, cet opérateur de fermeture correspond à une famille de Moore m-faiblement hiérarchique, où m ? 2 est un entier (Diatta, 2004). Plus précisément, les fermés de cet opérateur de fermeture coïncident avec les classes faibles associées à une certaine mesure de dissimilarité m-voies et forment donc, de ce fait, la hiérarchie m-faible associée à cette mesure de dissimilarité.\nDans cet article, nous considérons la caractérisation de ces bases pour les règles d\u0027association, en remplaçant l\u0027opérateur de fermeture de Galois par un opérateur de fermeture correspondant à la hiérarchie k-faible associée à une mesure de dissimilarité k-voies donnée, où 2 ? k ? m. Pour chaque valeur de k, l\u0027ensemble de règles ainsi caractérisé sera appelé sousbase k-faible. Ces sous-bases k-faibles offrent une approximation de l\u0027ensemble des règles valides, relativement à des ensembles d\u0027items (classes k-faibles) ayant un certain degré d\u0027homogénéité exprimé par le biais d\u0027un indice d\u0027isolation. Par ailleurs, la possibilité d\u0027associer une sous-base (k?) faible à une mesure de dissimilarité (k-voies) permet d\u0027intégrer la sémantique de cette mesure de dissimilarité dans le choix des règles à générer.\nRègles d\u0027association\nDéfinition générale\nÉtant donné un contexte binaire K \u003d (E, V), où E désigne un ensemble fini d\u0027entités et V un ensemble fini de variables booléennes (ou attributs) définies sur E. On appelle motifs les sous-ensembles de V et on dit qu\u0027une entité possède un attribut \"x\" si x(e) \u003d 1.\nUne règle d\u0027association de (E, V) est un couple (X, Y ) de motifs, notée X?Y , où Y est non vide. X et Y sont respectivement appelés \"prémisse\" et \"conséquent\".\nÉtant donné un motif X, X désignera l\u0027ensemble des entités qui possèdent tous les attributs de X, i.e., X \u003d {e ? E : ?x ? X, x(e) \u003d 1}. Un contexte binaire (E, V) contient 2 |V| (2 |V| ? 1) règles d\u0027association parmi lesquelles beaucoup ne sont pas pertinentes. On utilise des mesures de qualité pour sélectionner uniquement les règles qui vérifient des contraintes données.\nRègles d\u0027association confiance-valides\nUne mesure de qualité pour les règles d\u0027association d\u0027un contexte K est une application à valeurs réelles, définie sur l\u0027ensemble des règles d\u0027association de K. Beaucoup de mesures de qualité on été proposées dans la littérature, les plus utilisées d\u0027entre elles étant le support et la confiance (Agrawal et al., 1993).\nLe support d\u0027un motif X est la proportion des entités de E qui possèdent tous les attributs de X ; on le définit par Supp(X) \u003d |X | |E| , où, pour un ensemble fini S, |S| désigne le cardinal de S. Si on note p la mesure de probabilité intuitive définie sur (E, P(E)) par p(E) \u003d |E| |E| pour E ? E, alors le support de X peut s\u0027écrire en termes de p par :\n. La confiance d\u0027une règle d\u0027association X?Y est la proportion des entités qui possèdent tous les attributs de Y , parmi celles qui possèdent tous les attributs de X ; elle est définie par Conf(X?Y ) \u003d\nconditionnelle de Y sachant X . Une règle d\u0027association est confiance-valide si sa confiance est au moins égale à un seuil minimum (de validité) fixé. Une règle est dite confiance-exacte si sa confiance est égale à 1 ; elle est dite confiance-approximative si sa confiance est strictement inférieure à 1.\nBases pour les règles confiance-valides\nL\u0027un des problèmes majeurs de l\u0027extraction de règles d\u0027association est le nombre très élevé de règles générées. En effet, pour une mesure de qualité donnée µ, l\u0027ensemble des règles d\u0027association µ-valides contient fréquemment de nombreuses règles redondantes, i.e qui peuvent être déduites d\u0027autres règles µ-valides. Pour palier ce problème on cherche à calculer une base de l\u0027ensemble des règles valides, c\u0027est à dire un ensemble minimal (au sens de l\u0027inclusion) de règles d\u0027association valides, à partir duquel toute règle valide peut être dérivée. Dans ce papier, nous considérons des bases qui se caratérisent en termes d\u0027opérateurs de fermeture de Galois.\nOpérateurs de fermeture de Galois\nLe contexte binaire K induit une correspondance de Galois entre les ensembles ordonnés Barbut et Monjardet, 1970). Par ailleurs, la correspondance de Galois (f, g) induit un opérateur de fermeture ? :\u003d f • g sur (P(V), ?) (Birkhoff, 1967). Cet opérateur de fermeture sera dit de Galois, et un motif X sera dit fermé de Galois du contexte K, ou ?-fermé, si ?(X) \u003d X. Pour un motif X, ?(X) sera appelé sa ?-fermeture.\nBases de Guigues-Duquenne et de Luxenburger\nL\u0027ensemble des règles d\u0027association confiance-exactes est un système complet d\u0027implications, i.e. il satisfait les axiomes d\u0027inférence de Armstrong (1974). Il en découle que la base de Guigues et Duquenne (1986) pour les systèmes complets d\u0027implications est aussi une base pour les règles d\u0027association confiance-exactes.\nLa base de Guigues-Duquenne pour les règles d\u0027association confiance-exactes est l\u0027ensemble GD \u003d {X??(X) \\ X : X est ?-critique}, où un motif X est ?-critique s\u0027il n\u0027est pas ?-fermé et contient strictement la ?-fermeture de tout motif ?-critique Y tel Y ? X. Pour tout motif X, ?(X) est applelé la ?-fermeture de X.\nLa base de Luxenburger (1991) pour les règles d\u0027association confiance-approximatives est l\u0027ensemble LB \u003d {X?Y :\n4 Sous-bases k-faibles pour les règles confiance-valides\nOpérateurs de fermeture et familles de Moore\nSoit E un ensemble. Une famille de Moore sur E est une partie F de l\u0027ensemble P(E) des parties de E telle que E ? F et F ? F impliquent ?F ? F. Si F est finie, donc si E est fini, alors la seconde condition peut être remplacée par :\nPar ailleurs, étant donnée un famille de Moore F sur E, l\u0027application ? F définie sur P(E) par ? F (X) \u003d ?{Y ? F : X ? Y } est un opérateur de fermeture sur P(E).\nRéciproquement, étant donné un opérateur de fermeture ? sur E, la collection F ? de sousensembles de E, définie par F ? \u003d {X ? E : ?(X) \u003d X} est une famille de Moore sur E.\nFermés de Galois et classes faibles\nUne mesure de dissimilarité (mutuelle) sur E est une fonction\n. Dans ce papier, nous remplaçons les deux premières conditions par la condition moins forte d 2 (x, y) ? d 2 (x, x). Une justification de cet affaiblissement peut être trouvée dans (Diatta, 2006).\nUn sous-ensemble X de E est une classe faible associée à une mesure de dissimilarité\nx,y?X,z / ?X est strictement positif. En d\u0027autres termes, pour tous x, y dans la classe et tout z extérieur à la classe, au moins l\u0027une des dissimilarités d 2 (x, z) et d 2 (y, z) est strictement supérieure à la dissimilarité d 2 (x, y). Les mesures de dissimilarité (mutuelle) se généralisent naturellement en mesures de dissimilarité dites multivoies permettant d\u0027évaluer le degré de dissemblance globale entre entités d\u0027un ensemble de plus de deux éléments.\nEtant donnés un ensemble S et un entier k ? 1, désignons par S * ?k l\u0027ensemble des sousensembles non vides de S ayant au plus k éléments. Alors, en adoptant la définition ensembliste proposée dans (Diatta, 2006), une mesure de dissimilarité k-voies sur E est une fonction monotone croissante définie sur E * ?k à valeurs réelles positives ou nulles, i.e., une fonction\nOn notera que les mesures de dissimilarité (mutuelle) correspondent au cas où k \u003d 2. Par ailleurs, on parlera de mesure de dissimilarité multivoies pour signifier une mesure de dissimilarité k-voies quelconque, pour k ? 2. De plus, telle qu\u0027observée dans (Bandelt et Dress, 1994), la notion de classe faible s\u0027adapte tout aussi naturellement aux mesures de dissimilarité multivoies.\nAinsi, un sous-ensemble X de E est une classe faible associée à une mesure de dissimilarité\nest strictment positif. Par ailleurs, il a été montré dans (Diatta, 2004) qu\u0027il existe un entier m ? 2 tel qu\u0027un sous-ensemble X de V est ?-fermé si et seulement si X est une classe faible associée à une certaine mesure de dissimilarité m-voies d m sur V. Il découle de ce résultat que les motifs ?-fermés forment une famille de Moore m-faiblement hiérarchique appelée la hiérarchie mfaible associée à la mesure de dissimilarité d m . Ainsi, ? est tout simplement l\u0027opérateur de fermeture correspondant à cette famille de Moore au sens de la section 4.1.\nSous-bases k-faibles\nNous avons vu dans la section 3 qu\u0027aussi bien la base de Guigues-Duquenne que celle de Luxenburger pour les règles d\u0027association confiance-valides se caractérisent en terme de l\u0027opé-rateur de fermeture ?. Par ailleurs, nous venons de voir dans la section 4.2 que l\u0027opérateur de fermeture ? correspond à une famille de Moore m-faiblement hiérarchique, pour un certain entier m ? 2. Ces deux observations nous conduisent à considérer les analogues respectifs des bases de Guigues-Duquenne et de Luxenburger, en remplaçant l\u0027opérateur de fermeture ? par un opérateur de fermeture correspondant à une famille de Moore k-faiblement hiérarchique quelconque, où 2 ? k ? m. Pour k \u003d m, on peut retrouver ces bases sous certaines conditions mais, dans le cas g´néralg´néral, les ensembles de règles obtenus ne sont pas des bases au sens algébrique.\nAinsi, nous appellerons sous-base k-faible de Guigues-Duquenne pour les règles d\u0027association confiance-valides un ensemble de règles GD ? k défini par :\noù ? k est un opérateur de fermeture correspondant à une famille de Moore k-faiblement hié-rarchique.\nOn notera que les règles de GD ? k ne sont pas nécessairement exactes. En effet, certaines de ces règles peuvent avoir des exceptions pour la simple raison que la ?-fermeture de la prémisse d\u0027une telle règle peut être strictement contenue dans sa ? k -fermeture, i.e., X ? k -critique et ?(X) ? ? k (X).\nOn notera également qu\u0027il a été montré dans (Diatta, 2005) que si F une famille de Moore k-faiblement hiérarchique sur E, contenant toutes les parties de E d\u0027au plus k ? 1 éléments, alors X ? E est ? F -critique si et seulement si X / ? F et |X| \u003d k. Ce résultat généralise en fait son analogue obtenu par Domenach et Leclerc (2004) dans le cas particulier des familles de Moore faiblement hiérarchiques (k \u003d 2).\nDe même, nous appellerons sous-base k-faible de Luxenburger pour les règles d\u0027association confiance-valides un ensemble de règles LB ? k défini par :\nOn notera que les règles de LB ? k ne sont pas nécessairement approximatives. En effet, certaines de ces règles peuvent être exactes si par exemple leur conséquent est contenu dans la ?-fermeture de leur prémisse, i.e.,\nConclusion et discussion\nNous avons introduit la notion de sous-base k-faible relative à l\u0027opérateur de fermeture correspondant à une famille de Moore k-faiblement hiérarchique. Ces sous-bases k-faibles peuvent permettre de réduire le nombre de règles générées tout en assurant que les règles ainsi générées soient relatives à des ensembles d\u0027items (classes k-faibles) ayant un certain degré d\u0027homogénéité exprimé par le biais d\u0027un indice d\u0027isolation. Le degré d\u0027homogénéité de ces classes peut, dans une certaine mesure, garantir la cohérence du lien entre la prémisse et le conséquent d\u0027une règle. En effet, on sait que des règles générées dans l\u0027approche classique peuvent lier des motifs qui sont en réalité très peu liés voire statistiquement indépendants. Par ailleurs, la possibilité d\u0027associer une sous-base (k?) faible à une mesure de dissimilarité (kvoies) permet d\u0027intégrer la sémantique de la mesure de dissimilarité dans le choix des règles à générer. On notera qu\u0027une sous-base k-faible n\u0027est pas nécessairement une famille génératrice, donc pas une base au sens algébrique du terme. Toutefois, cela ne nous semble pas être un handicap. En effet, l\u0027intérêt d\u0027un ensemble de règles générées nous parait être plus dans la pertinence des règles qu\u0027il comporte que dans sa capacité à permettre de reconstruire toutes les règles valides. Cela étant, les idées présentées dans ce travail posent, entre autres, les deux questions suivantes : (a) comment générer efficacement une sous-base k-faible ? (b) quel est le degré d\u0027approximation des bases usuelles par les sous-bases k-faibles ?\nRéférences\nAgrawal, R., T. Imielinski, et A. Swami (1993). Mining association rules between sets of items in large databases. In P. \n"
  },
  {
    "id": "967",
    "text": "Introduction\nLes données issues du monde réel sont souvent entâchées d\u0027imperfections. En particulier, il est très courant de disposer de nombreuses données incomplètes (pannes, erreur de format, oubli humain, ...). Or la présence de valeurs manquantes induit de très sérieux problèmes, les données contenant des valeurs manquantes étant souvent éliminées lors du processus de fouille de données. C\u0027est notamment le cas pour l\u0027extraction de motifs séquentiels. Cette technique de fouille de données, présentée comme une extension des règles d\u0027association prenant en compte l\u0027information temporelle des bases de données historisées, ne permet en effet que l\u0027analyse des données complètes, sans tenir compte des enregistrements incomplets, ce qui constitue une grande perte d\u0027information. Par ailleurs, les solutions de remplacement des valeurs manquantes sont souvent soit trop simplistes pour produire des résultats intéressants, soit trop coûteuses pour être mises en oeuvre sur de gros volumes de données.\nOr, s\u0027il existe à ce jour des techniques robustes aux valeurs manquantes pour l\u0027extraction de règles d\u0027association, il n\u0027existe aucune méthode générique pour l\u0027extraction de motifs séquentiels. En effet, dans le contexte de la recherche de motifs séquentiels, les valeurs manquantes n\u0027ont pas été considérées jusqu\u0027ici, l\u0027application principale, les bases de données de supermarchés, n\u0027en comportant quasiment jamais. Désormais, les motifs séquentiels sont utilisés afin d\u0027extraire des connaissances d\u0027applications industrielles (analyse de processus, web access logs, ...) qui contiennent inévitablement des données incomplètes.\nNous proposons donc ici une extension des principes décrits dans (Agrawal et Srikant, 1995) afin d\u0027extraire des séquences fréquentes en présence de valeurs manquantes réparties aléatoirement dans une base de données. Cette approche a été inspirée par une méthode d\u0027extraction de règles d\u0027association sur des bases de données incomplètes (Ragel et Cremilleux, 1998) et sur une technique couramment utilisée en apprentissage (Liu et al., 1997) : ignorer les valeurs manquantes sans ignorer tout l\u0027enregistrement associé. Le principe consiste à utiliser seulement l\u0027information disponible (i.e. les attributs renseignés) et à ignorer les informations manquantes. Ainsi, seules des bases partielles complètes servent à l\u0027extraction de chaque schéma fréquent et l\u0027ensemble de la base est utilisée pour trouver tous les schémas. Nous transposons ici ce principe afin d\u0027extraire des motifs séquentiels, des séquences fréquentes maximales sur des bases de données historisées incomplètes. Pour cela, nous avons adapté les notions utilisées pour l\u0027extraction de séquences fréquentes. Puis, nous avons implémenté l\u0027algorithme SPoID (Sequential Patterns over Incomplete Database) sur la base d\u0027un algorithme d\u0027extraction de motifs séquentiels, PSP (Masseglia et al., 1998). Cet algorithme a été testé sur des jeux de données synthétiques afin de montrer la validité de notre approche.\nDans la suite de cet article, section 2, nous présentons les méthodes qui permettent d\u0027extraire des règles d\u0027association en présence de valeurs manquantes, ainsi que les concepts liés à la découverte de motifs séquentiels. Dans la section 3, nous développons notre approche pour l\u0027extraction de ces motifs à partir de bases de données incomplètes, puis nous présentons notre algorithme dans la section 4. La section 5 est ensuite consacrée aux expérimentations qui montrent la validité de notre approche. Enfin, nous concluons dans la section 6 par les perspectives qu\u0027ouvrent ce travail.\nDes données incomplètes aux motifs séquentiels\nLes motifs séquentiels sont souvent présentés comme une extension des règles d\u0027association, initialement proposées dans (Agrawal et al., 1993). Ils mettent en évidence des corré-lations entre des enregistrements d\u0027une base de données, ainsi que la relation temporelle qui existe entre eux. Toutefois ces algorithmes ne prennent pas en compte les enregistrements incomplets contenus dans une base de données. Afin de diminuer le prétraitement lié à la pré-sence de valeurs manquantes et d\u0027améliorer la qualité des motifs séquentiels extraits, nous proposons une technique d\u0027extraction de motifs séquentiels dans des bases de données incomplètes, inspirées des méthodes existantes pour les règles d\u0027association. Nous présentons dans cette section les méthodes qui permettent d\u0027extraire des règles d\u0027association en présence de valeurs manquantes ainsi que les notions liées à la découverte de motifs séquentiels.\nRègles d\u0027association et valeurs manquantes\nDes travaux ont été proposés pour la recherche de règles d\u0027association dans des bases de données incomplètes. Notamment, (Nayak et Cook, 2001;Ng et Lee, 1998), mettent en oeuvre un système d\u0027approximation probabiliste dans lequel une valeur manquante peut prendre plusieurs valeurs lors de la découverte des règles. Ces méthodes sont particulièrement adaptées aux bases de données relationnelles, mais ne sont pas facilement transposables au format spé-cifique des données dont on extrait les motifs séquentiels, détaillé section 3.\nNous avons donc choisi de nous inspirer de l\u0027algorithme RAR (Robust Association Rules), proposé dans (Ragel et Cremilleux, 1998). Cette méthode, complètement compatible avec la méthode originelle (Agrawal et al., 1993), permet la prise en compte des données incomplètes lors de l\u0027extraction de règles dans des bases de données relationnelles incomplètes, par omission partielle et temporaire de ces enregistrements. Le principe consiste à ne prendre en compte que les attributs renseignés pour les enregistrements incomplets. La base de données entière n\u0027est pas utilisée pour chaque règle mais pour générer l\u0027ensemble des règles. Cette technique repose sur la définition de bases de données valides, complètes pour un ensemble d\u0027items données, le reste de la base étant momentanément ignoré. Afin de prendre en compte ce partitionnement de la base, les concepts de support (pourcentage des enregistrements de la base qui contiennent tous les items de la règle) et de confiance (la probabilité qu\u0027un enregistrement qui contient la partie gauche de la règle contienne également la partie droite) ont été redéfinis. Par ailleurs, une nouvelle notion est introduite afin de tenir compte de la taille de l\u0027échantillon complet considéré pour déterminer le support de la règle. Cette mesure de représentativité permet ainsi d\u0027éliminer de la liste des règles celles trouvées sur une base peu significative par rapport à la base initiale.\nLa recherche de motifs séquentiels consiste à extraire des ensembles d\u0027items couramment associés sur une période de temps bien spécifiée. Elle permet de mettre en évidence des associations inter-enregistrement, par rapport à celle de règles d\u0027association qui extrait des combinaisons intra-enregistrement. L\u0027identification des individus ou objets est alors indispensable afin de pouvoir suivre leur comportement au cours du temps.\nMotifs séquentiels\nLes motifs séquentiels ont initialement été proposés par (Agrawal et Srikant, 1995) et reposent sur la notion de séquence fréquente maximale.\nConsidérons une base de données DB d\u0027achats pour un ensemble O d\u0027objets o. Chaque enregistrement R correspond à un triplet (id-objet, id-date, itemset) qui caractérise l\u0027objet auquel est rattaché l\u0027enregistrement, ainsi que la date et les items correspondants. Soit I \u003d {i 1 , i 2 , · · · , i m } l\u0027ensemble des items de la base. Un itemset est un ensemble non vide et non ordonné d\u0027items, noté (i 1 , i 2 , . . . , i k ), où i j est un item. Une séquence s se définit alors comme une liste ordonnée non vide d\u0027itemsets qui sera notée \u003c s 1 s 2 · · · s p \u003e, où s j est un itemset. Une n-séquence est une séquence de taille n, c\u0027est-à-dire composée de n items. \nLes enregistrements de la base sont regroupés par objet et ordonnés chronologiquement, définissant ainsi des séquences de données. Un objet o supporte une séquence S si elle est incluse dans la séquence de données o. Le support (ou fréquence) d\u0027une séquence est alors défini comme le pourcentage d\u0027objets de la base DB qui supporte S. Une séquence est dite fréquente si son support est au moins égal à une valeur minimale minSup spécifiée par l\u0027utilisateur. Une séquence candidate est une séquence potentiellement fréquente.\nLa recherche de motifs séquentiels dans une base de séquences telle que DB consiste alors à trouver toutes les séquences maximales (non incluses dans d\u0027autres) dont le support est supérieur à minSup. Chacune de ces séquences fréquentes maximales est un motif séquentiel.\nDes extensions ont été proposées pour prendre en compte la recherche incrémentale de motifs séquentiels (Masseglia et al., 2000), la gestion de valeurs numériques associées aux items (Hong et al., 2001;Chen et al., 2001;Fiot et al., 2005) ou encore la généralisation des motifs séquentiels pour différents paramètres temporels (espacement des différents évènements d\u0027une séquence, rapprochement d\u0027évènements proches en une même date...) (Srikant et Agrawal, 1996;Masseglia et al., 1999;Fiot et al., 2006). Toutefois, il n\u0027existe pas, à notre connaissance, de techniques permettant de gérer les valeurs manquantes lors de la découverte de motifs sé-quentiels. C\u0027est pourquoi nous proposons ici une approche permettant d\u0027extraire des séquences fréquentes maximales dans une base de séquences incomplètes.\n3 SPoID : une nouvelle approche du traitement des données incomplètes\nMotivations\nNous souhaitons extraire les motifs séquentiels contenus dans la base de données TAB. 1.\nTAB. 1 -Base de données complète.\nTAB. 2 -Base de données incomplète.\nTAB. 3 -Après suppression des valeurs manquantes.\nAvec un support de 50%, les motifs obtenus sont :\nConsidérons maintenant la même base, mais incomplète, TAB. 2. Pour certaines séquences de données, les informations n\u0027ont pas été transmises et des valeurs sont manquantes. On considère que ces valeurs sont identifiées comme des attributs de valeur indéterminée, contrairement à des attributs inexistants où la valeur sera identifiée comme non-renseignée. Afin de pouvoir extraire les motifs, les méthodes classiques requièrent une élimination des valeurs manquantes. La base sur laquelle s\u0027effectue la fouille de données est alors la base TAB. 3 et les motifs obtenus pour minSup \u003d 50% sont :\nOn constate que seule une petite partie de la base est utilisée pour extraire l\u0027information et on ne retrouve qu\u0027une partie des schémas fréquents extraits de la base complète : des sousséquences des motifs que l\u0027on devrait extraire. C\u0027est pourquoi il paraît nécessaire d\u0027utiliser l\u0027intégralité de la base lors de la fouille et non pas d\u0027en supprimer une partie.\nSequential Patterns over Incomplete Database (SPoID)\nL\u0027élimination des enregistrements incomplets conduisant à une perte d\u0027information, nous avons donc envisagé d\u0027adapter une méthode d\u0027extraction de règles d\u0027association robuste aux valeurs manquantes pour extraire des motifs séquentiels. Nous présentons ici la méthode SPoID (Sequential Patterns over Incomplete Database), inspirée de l\u0027algorithme RAR présenté dans (Ragel et Cremilleux, 1998).\nLe principe général de notre méthode, comme de la méthode RAR, repose sur la désacti-vation des éléments incomplets, dans notre cas, des séquences. Alors que pour les règles d\u0027association, l\u0027algorithme RAR ne considère que les enregistrements complets, nous proposons ici de ne prendre en compte que les séquences de données complètes pour la séquence candidate recherchée. Autrement dit, on ne considèrera que les dates et attributs renseignés pour les séquences incomplètes. Ainsi pour chaque séquence on déterminera si elle est fréquente sur une base partielle, mais la totalité de la base sera utilisée pour l\u0027ensemble des séquences fréquentes.\nConsidérant une séquence candidate S, l\u0027ensemble O des objets de la base peut être divisé en trois sous-ensembles disjoints (FIG. 1 \nNous avons montré, sous certaines conditions peu restrictives, que cette définition respecte la propriété d\u0027antimonotonie du support énoncée dans (Agrawal et Srikant, 1995).\nLa nouvelle définition du support étant antimonotone, on peut utiliser les différentes propriétés énoncées dans (Agrawal et Srikant, 1995) afin de réaliser l\u0027extraction de motifs séquen-tiels sur base de données incomplètes. Toutefois, la notion de fréquence dépend du calcul du support et de la taille de la base de données valide utilisée pour le calculer. On définit un critère de représentativité minimale : une base de données valide doit être un échantillon significatif de la base de départ pour qu\u0027on considère la séquence comme fréquente si elle valide le support minimal minSup.\nDéfinition 4. La représentativité Rep(S) d\u0027une séquence S est définie comme le ratio du nombre de fois où cette séquence apparaît complète ou n\u0027apparaît pas avec certitude dans la base, par rapport au nombre total de séquences de données dans la base. Elle est donnée par :\nRep ( Pour être considérée comme fréquente, une séquence doit donc être trouvée fréquente sur une base de données valide et représentative, c\u0027est-à-dire si sa représentativité est supérieure au seuil minimal de représentativité minRep et si son support est supérieur au support minimum spécifié par l\u0027utilisateur minSup.\nSeuil de représentativité et marge d\u0027erreur\nLes statisticiens disposent d\u0027outils d\u0027échantillonnage permettant de considérer un sousensemble d\u0027une population afin d\u0027estimer une proportion, à un pourcentage d\u0027erreur près, avec un niveau de confiance suffisant. Ces outils permettent de déterminer la taille optimale d\u0027un échantillon en fonction de la distribution des données. Ainsi, dans le cas d\u0027une distribution au hasard des données, (Toivonen, 1996) utilise les bornes de Chernoff pour déterminer la taille minimale d\u0027un échantillon tiré au hasard pour l\u0027extraction de règles d\u0027association. Ce résultat est également démontré théoriquement et empiriquement dans (Zaki et al., 1996).\nNous proposons donc d\u0027utiliser deux formes de représentativité selon le souhait de l\u0027utilisateur : soit celle-ci sera définie comme une proportion de la base, soit elle sera absolue et calculée à partir de formules statistiques dépendant de la distribution des données, en respectant un pourcentage d\u0027erreur et un niveau de confiance spécifiés par l\u0027utilisateur. Les expérimenta-tions montrent toutefois que le seuil de représentativité optimale n\u0027est pas absolu mais dépend en fait du taux de valeurs manquantes contenues dans la base de données.\nMise en oeuvre 4.1 Illustration\nReprenons la base incomplète présentée TAB. 2. On pose minSup\u003d50%, puis on calcule le support et la représentativité de chacun des items de la base pour déterminer les items fré-quents. L\u0027item a est supporté de manière sûre par les trois objets son support est supp(a) \u003d 3/3 \u003d 100% et sa représentativité vaut 1. Il en est de même pour les items b et c. Même si ces motifs ne sont pas exactement ceux extraits sur la base complète, on constate qu\u0027ils sont plus proches des motifs qui devraient être extraits des motifs obtenus de la base après prétraitement. Les expérimentations présentées dans la section 5 montrent qu\u0027il existe une valeur de la repré-sentativité à partir de laquelle l\u0027algorithme SPoID extrait d\u0027une base incomplète l\u0027intégralité des motifs extraits sur la base complète.\nAlgorithme\nLe principe de l\u0027algorithme SPoID est similaire aux algorithmes d\u0027extraction de motifs séquentiels de type générer-élaguer. Il consiste à générer toutes les séquences candidates de longueur k à partir des séquences fréquentes de longueur k-1, puis à scanner l\u0027ensemble de la base pour compter le nombre de séquence de données qui supporte chacune des séquences candidates. La différence réside dans le dénombrement des séquences de données incomplètes pour la séquence candidate considérée. La phase de comptage est décrite par l\u0027algorithme ALG. 1 : pour chaque séquence candidate, pour chaque objet, -si on trouve la séquence candidate, on incrémente les valeurs absolues du support, -si on ne trouve pas la séquence candidate ni de séquence dans laquelle des valeurs manquantes pourraient être remplacées par les items correspondant dans la séquence candidate, alors l\u0027objet ne supporte pas la séquence candidate. On n\u0027incrémente pas le support. -on trouve une séquence incomplète dans laquelle des valeurs manquantes pourraient être remplacées par les items correspondant dans la séquence candidate. Dans ce cas, on ajoute cet objet à l\u0027ensemble des objets désactivés. Une fois l\u0027ensemble de la base parcouru, on divise la valeur absolue du support par la différence entre le nombre total d\u0027objets et le nombre d\u0027objets désactivés, on calcule la représentativité. Puis on procède à la phase d\u0027élagage en supprimant toutes les séquences candidates qui ne sont pas fréquentes puis celles qui ne sont pas représentatives.\nSPoID -Input : |O|, une base de séquences de données, minSup, support minimum spécifié par l\u0027utilisateur minRep, représentativité minimum, spécifiée ou calculée Ouput : SP List, liste des séquences fréquente La complexité temporelle de cet algorithme est, dans le pire des cas, la même que celle de l\u0027algorithme TOTALLYFUZZY présenté dans (Fiot et al., 2005). On utilise le même type d\u0027optimisations afin de réduire le nombre de passes sur la base. Par contre, la complexité en mémoire est nettement moindre puisqu\u0027elle est du même ordre que celle de PSP.\nExpérimentations\nCes expérimentations ont été réalisées sur un PC équipé d\u0027un processeur 2,8GHz et de 512Mo de mémoire DDR, sous système Linux, noyau 2.6. Nous utilisons un jeu de données synthétiques générés aléatoirement par une loi normale dans lequel nous remplaçons certains items par des valeurs manquantes, réparties de maniére aléatoire. On extrait les motifs séquen-tiels sur la base complète ainsi que sur la base prétraitée (dont les itemsets incomplets ont été supprimés). Puis on compare ces motifs extraits par les méthodes existantes aux motifs extraits par notre algorithme SPoID. Les résultats présentés ici ont été obtenus à partir du traitement de plusieurs jeux de données synthétiques comportant environ 2000 séquences de 20 transactions en moyenne. Chacune de ces transactions comporte en moyenne 10 items choisis parmi 100.\nNos analyses sont basées sur le calcul du nombre de bons motifs séquentiels trouvés par SPoID et du nombre de motifs différents extraits par SPoID, ces derniers regroupant les motifs extraits, qui n\u0027existe pas dans la base complète et les motifs non trouvés, mais contenus dans la base complète. Le tableau 4 récapitule l\u0027ensemble de ces notations.\n? Nombre de motifs extraits par SPoID, contenus dans la base complète ? Nombre de motifs différents ? Nombre de motifs extraits par SPoID sur la base incomplète ? Nombre de motifs extraits sur la base de données complète TAB. 4 -Notations pour les différentes catégories de motifs séquentiels extraits.\nLa FIG. 2(a)\n, tout d\u0027abord, montre l\u0027évolution du rapport ?/?, en fonction de la représenta-tivité minimale. On constate que ce taux croît à mesure que minRep augmente, ce qui signifie que parmi les motifs extraits par SPoID, la proportion des motifs également trouvés dans la base complète augmente avec la représentativité minimale.\nOn peut compléter cette observation par l\u0027analyse de la FIG. 2(b), qui présente l\u0027évolution du rapport ?/? (nombre de motifs extraits par rapport aux motifs à extraire) en fonction de la représentativité minimale. On constate que ce ratio diminue à mesure que minRep augmente, ce qui signifie qu\u0027il est nécessaire de choisir une représentativité suffisamment faible pour permettre l\u0027extraction de l\u0027intégralité des motifs présents dans la base complète.\nNous avons donc mis en évidence l\u0027existence d\u0027une valeur optimale du seuil de représen-tativité, pour laquelle les ratios ?/? (bons motifs/motifs extraits par SPoID) et ?/? sont les plus proches possibles de 1. Cette valeur correspond au seuil pour lequel le nombre de bons motifs extraits sur la base incomplète est le plus élevé possible par rapport au nombre de motifs diffé-rents. La FIG. 2(c) met en évidence l\u0027existence de cette valeur optimale de minRep. Ce graphe montre l\u0027évolution du rapport (?/?), rapport du nombre de bons motifs extraits par SPoID et du nombre de motifs qui diffèrent des motifs extraits sur la base complète (manquants + supplémentaires). On constate qu\u0027il n\u0027existe pas de valeur absolue de la repésentativité minimale, commune à toutes les bases et dépendant d\u0027une marge d\u0027erreur donnée. D\u0027après ces résultats, la représentativité minimale dépend uniquement du taux d\u0027incomplétude de la base.\nQuelle que soit la proportion de valeurs manquantes dans la base incomplète, l\u0027allure géné-rale de la courbe est la même : le ratio (?/?) augmente avant d\u0027atteindre un maximum puis de décroître. Ce point maximal correspond à la représentativité optimale, pour laquelle le nombre de bons motifs extraits par SPoID est le plus élevé et le nombre de motifs différents le plus faible. Le tableau TAB. 5 donne la valeur de la représentativité optimale trouvée expérimenta-lement pour chaque proportion de données incomplètes dans notre jeu de test. La FIG. 2(c) met aussi en évidence l\u0027évolution de comportement de l\u0027algorithme SPoID selon le taux de valeurs manquantes dans la base. On constate une différence entre l\u0027allure de la courbe et la valeur du ratio ?/? pour les bases incomplètes contenant 40% de valeurs man-  \nPerspectives\nLa découverte de motifs séquentiels est une méthode de fouille de données intéressante lorsqu\u0027il s\u0027agit d\u0027extraire des connaissances dans une base de données historisée, telle que des relevés de processus industriel ou de fonctionnement de machines. Or, dans ce type de bases de séquences, la présence de valeurs manquantes est inévitable. Pourtant il n\u0027existe aucune technique permettant de découvrir des séquences fréquentes à partir de bases de données incomplètes. Nous avons donc proposé dans cet article une adaptation des définitions originales liées à l\u0027extraction de motifs séquentiels afin de pouvoir traiter les informations incomplètes distribuées au hasard, directement pendant la fouille plutôt que de supprimer ces enregistrements, comme cela était le cas avec les algorithmes existants. Notre méthode, SPoID (Sequential Patterns over Incomplete Database) a été implémentée et testée sur des jeux de données synthétiques, ce qui nous a permis de montrer sa robustesse aux valeurs manquantes jusqu\u0027à un taux d\u0027incomplétude d\u0027environ 40%, alors que les méthodes classiques, après prétraitement, donnent de mauvais résultats dès 10% de valeurs manquantes.\nNous envisageons maintenant d\u0027étendre cette méthode afin de pouvoir prendre en compte d\u0027autres types de valeurs manquantes (non distribuées au hasard, par exemple), après avoir détecté les différents types d\u0027informations incomplètes contenues dans la base. Il apparaît éga-lement nécessaire de pouvoir distinguer, quand cela ne l\u0027a pas été fait au préalable, les valeurs d\u0027attribut inexistant, qui n\u0027ont donc pas à être considérées comme manquantes. Enfin, le bruit est également une imperfection courante dans les bases de données du monde réel. Il pourra donc être intéressant de le prendre en compte dans une version ultérieure de notre algorithme.\n"
  },
  {
    "id": "968",
    "text": "La plateforme SyRQuS\nMême si cela fait plusieurs années que RDF est devenu un standard recommandé par W3C, le développement des langages de requête RDF a été plus long. Après l\u0027apparition de RDF, des langages permettant d\u0027accéder aux triplets RDF ont émergé, comme TRIPLE (Sintek et al., 2002) ou encore Squish (SquishQL, 2002). De ces premiers sont inspirés d\u0027autre langages comme RQL, RDQL -langage d\u0027origine de la plateforme Jena (Jen) -ou encore SeRQL langage de base de Sesame (Kampman et Broekstra). Tous ces efforts convergent aujourd\u0027hui vers un langage SQL-like qui est en train de devenir la future recommandation W3C : SPARQL (Seaborne et Prud\u0027hommeaux, 2006 \nConclusion et perspectives\nSyRQuS est une application destinée à la recherche d\u0027informations dans les documents RDF. Son implémentation permet d\u0027obtenir des réponses plus complète par la combinaison de plusieurs graphes RDF fournissant des réponses partielles. Développée dans un souci de compatibilité avec le langage de requête SPARQL, elle permet le traitement des requêtes de type SELECT, ce type de requête étant le seul à pouvoir profiter de notre algorithme de recherche par combinaison de graphes.\nActuellement, nous travaillons sur l\u0027intégration des ontologies référencées dans les documents RDF, ce qui nous permettra de procéder à des aproximations de réponses au moment de l\u0027intérrogation de la base de documents. \nRéférences\nSummary\nNowadays, RDF is a W3C recommended standard for describing knowledge about resources over the Web. In this context, we explore the RDF query answering field and take a closer look at the possibility of combining several RDF graphs while searching for query answers. We propose a prototype tool that generates virtual complete answers by combining partially answering RDF graphs. The generation of such answers is performed under a noncontradiction condition calculated through a similarity measure between the RDF graphes to be combined.\n"
  },
  {
    "id": "969",
    "text": "Introduction\nIl est envisagé de déployer en France dans les prochaines années des compteurs communicants chez un grand nombre de clients des compagnies d\u0027électricité, à l\u0027image de ce qui a été déjà réalisé en Italie par la société ENEL. Ces nouveaux compteurs, reliés aux Systèmes d\u0027Information (SI) des fournisseurs d\u0027électricité, permettront de relever à distance les consommations d\u0027électricité afin d\u0027effectuer des opérations tels que la facturation, l\u0027agréga-tion, le contrôle,... L\u0027approche standard pour le traitement de ce type de données dans les SI est d\u0027utiliser des Systèmes de Gestion de Bases de Données (SGBD) qui assurent leur stockage et permettent de les consulter par des requêtes. Cette approche trouve ses limites lorsque les flux de données sont importants (en débit et/ou en nombre). C\u0027est pour cela que de nouvelles approches ont été développées dans le domaine des bases de données pour traiter de façon performante des flux de données, sans mémoriser l\u0027ensemble des informations : il s\u0027agit des Systèmes de Gestion de Flux de Données -SGFD -ou encore Data Stream Management Systems -DSMS -en anglais. Plusieurs prototypes de SGFD ont été développés ces dernières années pour répondre à ces besoins. Le rôle de ces systèmes est de traiter en temps réel un ou plusieurs flux de données par des requêtes dites continues. Le travail présenté ici décrit l\u0027expérimentation de deux prototypes de SGFD du domaine public (STREAM (Arasu et al., 2004) et TelegraphCQ (Chandrasekaran et al., 2003)) pour la réali-sation de fonctions de traitement de données de consommation électrique, disponibles sous forme de flux à partir de compteurs communicants. La communication est organisée comme suit : la section 2 présente en général les SGFD et en particulier les systèmes STREAM et TelegraphCQ. La section 3 présente notre expérimenta-tion, la synthèse de celle-ci est exposée à la section 4. Enfin, nous concluons à la section 5 et donnons nos perspectives de recherche.\nSystèmes de Gestion de Flux de Données\nLes Systèmes de Gestion de Bases de Données (SGBD) sont des logiciels permettant de définir des structures de données, de stocker des données dans ces structures, de modifier ces données, et de les consulter à l\u0027aide de requêtes. Les SGBD les plus utilisés sont ceux dits relationnels, où les données sont structurées sous forme de tables (aussi appelées relations). Le langage standard pour spécifier les requêtes aux bases de données relationnelles est le langage SQL. Les Systèmes de Gestion de Flux de Données (SGFD) préfigurent des prochaines générations de SGBD. En effet, devant la volumétrie sans cesse plus importante des données produites par les systèmes de mesure et les systèmes informatiques, certaines applications né-cessitent de lever l\u0027hypothèse que toutes les données produites sont stockées dans la base de données avant d\u0027être consultées par des requêtes. Pour les SGFD, la structure de données est celle d\u0027une collection de flux de données entrants, qui sont traités au fil de l\u0027eau pour ré-pondre aux requêtes des utilisateurs, elles-mêmes définies pour produire des flux sortants. A titre d\u0027exemple, si les flux entrants de données correspondent à des mesures de consommation d\u0027électricité à différents points du réseau électrique, une requête utilisateur peut consister en la restitution des consommations moyennes et maximales pour chaque département dans les trois dernières heures. Les travaux de recherche menés aux Etats-Unis sur le thème des flux de données se divisent principalement en deux grandes directions (voir (Babcock et al., 2002) et (Golab et al., 2003)) : (1) la conception de systèmes de gestion de flux de données, (2) la conception d\u0027algorithmes de fouille de données (data mining) pouvant fonctionner sur des flux de données, c\u0027est-à-dire sans mémoriser l\u0027ensemble des informations apparaissant dans les flux. Nous nous intéressons ici uniquement à la gestion des flux de données. Les SGFD permettent de formuler des requêtes dites « continues » qui s\u0027évaluent au fur et à mesure que les différents flux sur lesquels elles portent sont alimentés. Afin que ces requêtes aient un sens et ne nécessitent pas le stockage de l\u0027ensemble du flux, des techniques de fenê-trage sont utilisées : une fenêtre définit un intervalle temporel exprimé soit en terme de durée (par exemple les 5 dernières minutes), ou soit sous forme logique exprimé en nombre de tuples (par exemple les 20 derniers éléments). Ces fenêtres peuvent être délimitées par des bornes fixes ou glissantes dans le temps. La figure 1 présente une architecture abstraite de SGFD, extraite de (Golab et al., 2003). Un moniteur d\u0027entrée sert à réguler le débit d\u0027arrivée des données et peut être amené à supprimer quelques tuples en cas de surcharge du système. Les données des flux sont stockées temporairement dans des buffers (working storage) pour permettre le traitement des requêtes fenêtrées. Des résumés des flux sont éventuellement constitués (summary storage) lorsque la capacité de stockage ne permet pas de gérer tous les flux entrants : le résultat des requêtes est alors ap-proché. Un stockage statique (static storage) assure la mémorisation des meta-données (structure et provenance des flux) ainsi que des données présentes de façon permanente dans la base. Lorsque les requêtes des utilisateurs sont définies, celles-ci sont stockées dans le système (query repository) et traitées par le processeur de requêtes (query processor). Ce dernier a la charge de définir un plan optimisé d\u0027exécution de l\u0027ensemble de requêtes (en mutualisant les ressources entre plusieurs requêtes) et de produire les résultats des requêtes soit directement sous forme de flux, soit sous forme de données stockées temporairement (output buffer) et remises à jour périodiquement. Il faut noter que les plans d\u0027exécution des requêtes ne doivent pas être figés car ils peuvent être remis en question soit par l\u0027ajout de nouvelles requêtes, soit par des variations dans le contenu et le débit des flux.\nFIG. 1 -Architecture globale d\u0027un SGFD\nSi les applications qui ont motivé le développement des SGFD portaient sur la supervision des réseaux informatiques, de nombreuses autres applications se sont développées : le traitement des logs de sites web, des tickets de communications fixes ou mobiles, des données de capteurs (trafic routier, météorologie par exemple), mais aussi de données boursières. Plusieurs systèmes prototypes ont été développés, certains avec une vocation généraliste (tels que STREAM (Arasu et al., 2004), TelegraphCQ (Chandrasekaran et al., 2003), Borealis (Abadi et al., 2005), Aurora/Medusa (Zdonik et al., 2003)), alors que d\u0027autres sont destinés à un type particulier d\u0027application (tels que Gigascope (Cranor et al., 2003), Hancock (Cortes et al., 2000), NiagaraCQ (Chen et al., 2000)).\nSTREAM\nSTREAM (pour STanford stREam datA Management) est un système prototype de gestion de flux de données généraliste développé à l\u0027Université de Stanford (Arasu et al., 2004). Ce système permet d\u0027appliquer un grand nombre de requêtes déclaratives et continues à des données statiques (tables) et dynamiques (flux de données). Un langage déclaratif CQL (Continuous Query Language) (Arasu et al., 2003), dérivé de SQL, a été implémenté pour pouvoir traiter des requêtes continues. STREAM considère deux structures de données : -Flux : Un flux S est un ensemble d\u0027éléments (s,t), où s est un tuple appartenant au flux, et t ?T est l\u0027estampille temporelle (timestamp) du tuple, t croissant de façon monotone. -Relation : Une relation R(t) est un ensemble de tuples qui dépend du temps. A chaque instant une relation est susceptible d\u0027avoir un nouveau contenu. Le language CQL spécifie des opérations de sélection, d\u0027agrégation, de jointure, de fenêtrage sur les flux, ainsi que trois opérateurs transformant des relations en flux :\n-Istream(R(t)) : envoie sous forme de flux les nouveaux tuples apparus dans R à la période t ; -Dstream(R(t)) : envoie sous forme de flux les tuples supprimés de R à la période t ; -Rstream(R(t)) : envoie sous forme de flux tous les tuples appartenant à R à la période t. CQL supporte des fenêtres glissantes sur un flux S, qu\u0027on peut décliner en 3 groupes : (1)  \nTelegraphCQ\nTelegraphCQ est un Système de Gestion de Flux de Données réalisé à l\u0027Université de Berkeley (Chandrasekaran et al., 2003). Ce système est construit comme une extension du SGBD relationnel PostGreSQL pour supporter des requêtes continues sur des flux de données. Le format d\u0027un flux de données est défini comme toute table PostGreSQL dans le langage de dé-finition de données de PostGreSQL, et est créé à l\u0027aide de la clause CREATE STREAM, avant d\u0027être utilisé dans des requêtes continues. A chaque source de flux est affecté un traducteur (wrapper) qui transforme les données en un format compatible avec les types de données PostGreSQL. TelegraphCQ est un mode d\u0027exécution de PostGreSQL, l\u0027utilisateur pouvant aussi lancer un serveur PostGreSQL en mode normal. Les requêtes continues peuvent inclure une clause de fenêtrage sur les flux. Une fenêtre est définie sur un intervalle de temps, le timestamp de chaque n-uplet du flux étant assigné par le wrapper si celui-ci ne possède pas un attribut déclaré comme timestamp. La sémantique de fenêtrage a évolué au cours des différentes versions de TelegraphCQ. La version la plus ré-cente définit la clause de fenêtrage par [RANGE BY \u0027interval\u0027 SLIDE BY \u0027interval\u0027 START AT \u0027date et heure de début\u0027] :\n-RANGE BY : définit la taille de la fenêtre en terme d\u0027intervalle de temps ; -SLIDE BY : définit l\u0027intervalle après lequel la fenêtre est recalculée, spécifié en intervalle de temps ; -START AT : définit l\u0027instant (date et heure) auquel la première fenêtre commence, dans un format de date standard. Les flux interrogés par les requêtes peuvent être archivés dans la base de données PostgreSQL à la demande de l\u0027utilisateur (clause ARCHIVED). Dans le cas contraire, ils sont supprimés du système lorsque les requêtes n\u0027ont plus à les conserver dans des espaces temporaires pour leur exécution.\nExpérimentation du traitement de flux de consommations électriques\nNous avons installé les deux systèmes STREAM et TelegraphCQ version 2.1 et les avons testés sur un jeu de données composé d\u0027index de consommations relevés à pas de 2 secondes pendant 150 jours. Ces relevés correspondent à trois compteurs électriques se trouvant dans des villes différentes.\nSpécification des traitements\nL\u0027analyse des consommations électriques peut être utile pour le fournisseur d\u0027électricité comme pour le client. Ce dernier pourra s\u0027en servir, par exemple, pour détecter les causes d\u0027une consommation excessive ou anormale.\nUn compteur « communicant » est capable de transmettre toutes les 2 secondes un enregistrement (tuple) de données contenant le numéro de compteur, l\u0027heure du relevé, la date, l\u0027index de consommation, et d\u0027autres informations annexes. La différence entre deux index à deux instants différents représente la consommation d\u0027électricité en energie entre ces deux instants. Les tuples provenant de plusieurs compteurs forment un flux de données de consommation.\nPour notre expérimentation, le jeu de données que nous avons utilisé est composé de tuples dont la structure (simplifiée) est la suivante : Flux_index est le nom identifiant le flux de données. La date d\u0027un relevé est spécifiée à l\u0027aide de trois attributs : \u0027mois\u0027, \u0027jour\u0027, \u0027année\u0027 ; l\u0027heure à l\u0027aide de trois autres attributs : \u0027h\u0027, \u0027m\u0027, \u0027sec\u0027 (heure, minute, seconde). L\u0027attribut \u0027compteur\u0027 désigne l\u0027identifiant du compteur et \u0027index\u0027 désigne le relevé de l\u0027index du compteur (en kWh).\nL\u0027analyse des consommations s\u0027appuie sur des requêtes continues sur le flux. Parmi les requêtes possibles nous avons choisi trois requêtes qui nous semblent assez représentatives de ce type d\u0027application.\n1. Consommation des 5 dernières minutes, minute par minute, par compteur, ou par ville ; 2. Consommation historique minute par minute, par compteur, à partir d\u0027une date de dé-part ; 3. Alarme de dépassement de consommation heure par heure par rapport à une consommation dite « normale » dépendant de la température.\nNous avons essayé d\u0027implanter ces requêtes sur les deux SGFD TelegraphCQ et STREAM. Les sections suivantes, 3.2 et 3.3 présentent le résultat de ce travail. Elles précisent pour chaque SGFD les requêtes qui ont pu être implantées, et expliquent comment cette implantation a pu se faire.\nImplantation sur Stream\nUne des spécificités de STREAM est qu\u0027il ne supporte pas les expressions de requêtes imbriquées. Cependant, il offre la possibilité d\u0027associer un nom à une expression de requête (comme une vue) et de s\u0027en servir dans l\u0027expression d\u0027une requête plus complexe.\nQ1 -Consommation des cinq dernières minutes, minute par minute, par compteur ou par ville. Le flux est une suite infinie de tuples qui associent des relevés d\u0027index à des dates de prélèvements (année, mois, jour, heure, minute, seconde). Dans ce cas, le calcul de la consomation sur une durée donnée (dans notre exemple une minute) peut se faire en calculant la différence entre les valeurs minimales de l\u0027index observées pendant deux périodes (minutes) successives. L\u0027implantation de la requête Q1 peut donc se faire sur STREAM en suivant les étapes suivantes : Pour afficher les consommations par ville, nous transformons la dernière requête en flux que nous appelons Elec.fluxconso. Les flux Elec.minflux1 et Elec.minflux2 sont calculés sur des fenêtres de 1 minute. Ensuite, nous réalisons une jointure entre la table Tableville et Elec.fluxconso fenêtré à 5 minutes.\nQ2 -Consommation historique minute par minute, par compteur, à partir d\u0027un point de départ. La réalisation de cette requête est facilitée par la clause START AT permettant de filtrer les tuples à partir d\u0027un point de départ fixé. Q3 -Alarme de dépassement de consommation heure par heure de minuit à l\u0027heure courante. La consommation normale est fournie dans une table consoNormale, pour chaque heure et pour une température de 20°C. Nous disposons d\u0027un flux Elec.temperature qui donne la température relevée chaque heure pendant 150 jours. Ce flux est affecté à un deuxième wrapper csvwrapper2. Pour s\u0027approcher des conditions réelles, nous construisons un flux de consommation normale Elec.fluxconsonormale qui dépend de la température. L\u0027implantation de la requête Q3 a été faite en suivant les étapes suivantes :\n1. Nous calculons les consommations cumulées de 24 heures heure par heure, et nous sé-lectionnons celles au-delà de 12h. Le flux Elec.fluxconsoParHeure a été construit en suivant la même démarche que pour la consommation minute par minute en prenant une fenêtre de 1 heure. wtime(*) donne le timestamp du dernier tuple de la fenêtre en cours. \nSynthèse des expérimentations Adéquation des langages\nAu vue des implantations des requêtes à la section 3, nous remarquons que la logique de résolution des requêtes par SGFD est assez éloignée de la logique SQL. Dans le cas des bases de données, les requêtes manipulent des données persistentes qui sont sélectionnées selon leurs valeurs dans le contenu d\u0027une ou de plusieurs relations. Dans le cas des flux, les données sont volatiles et le contenu est non borné ce qui rend plus complexe la sélection des données et leur traitement. Il faut (1) spécifier par le mécanisme des fenêtres temporelles un réservoir de tuples à constituer et sur lequel s\u0027appliquera la requête, et (2)  Performances L\u0027équipe STREAM s\u0027est intéressée à améliorer les performances de son système. Pour une bonne utilisation des ressources, STREAM propose des mécanismes permettant un partage des requêtes entre les flux et un partage des flux entre les requêtes. Les développeurs ont implanté une interface graphique pour visualiser les plans d\u0027exécution des requêtes ainsi que le taux d\u0027utilisation du système grâce à des jauges disponibles sur les différents composants du plan de la requête. L\u0027utilisateur peut surveiller l\u0027état du système et ré-optimiser l\u0027exécution des requêtes dynamiquement en manipulant les tailles des buffers pour mieux s\u0027adapter aux conditions du système ou des flux. Cependant, STREAM tel qu\u0027il est offert au public demeure un prototype utile pour effectuer des démonstrations sur les possibilités des SGFD, mais n\u0027est pas adapté à un besoin réel tel que le nôtre pour y répondre de manière complète et robuste. En effet, nous avons été confrontés à des arrêts intempestifs du serveur à plusieurs reprises lors de l\u0027exécution des requêtes. TelegraphCQ est un système robuste, en terme de charge et de complexité des requêtes acceptées. Il est possible d\u0027ajouter des requêtes même si d\u0027autres requêtes sont en cours d\u0027exé-cution. Nous avons implanté toutes les requêtes présentées dans cet article sans le moindre souci. Néanmoins, nous ne nous sommes intéressés dans le cadre de cette étude qu\u0027à l\u0027as-pect fonctionnel de ces deux SGFD. Nous envisageons dans nos prochaines expérimentations d\u0027augmenter le nombre de compteurs électriques de notre jeu de données pour tester la charge de TelegraphCQ.\nDivers\nLes données d\u0027un flux de consommation électrique arrivent constamment et une requête continue ne connait pas à priori l\u0027extrémité d\u0027un flux (contrairement à une table dans les SGBD). Les fenêtres nécessaires à des opérations sur les flux telle que l\u0027agrégation sont implémentées dans les deux SGFD testés. Toutefois, il était impossible d\u0027utiliser la fonction min avec un opérateur de fenêtrage lors de l\u0027exécution de la requête Q1 dans STREAM, pour une raison inconnue. Ceci nous aurait permis d\u0027améliorer les performances en ne gardant que les données dont nous avons besoin pendant une période donnée. TelegraphCQ possède une syntaxe claire, une simplicité d\u0027utilisation et de formulation des requêtes bien meilleure que STREAM. Ceci est dû aux fonctionnalités fournies par PostgreSQL telle que la gestion des formats de date et les méthodes associées. De plus, TelegraphCQ offre la possibilité de réutiliser les résultats des requêtes sous forme de flux, ou de les stocker dans un fichier résultat en spécifiant le format de sortie. Quant à STREAM, il se distingue par son interface graphique permettant de se connecter au serveur, de lui envoyer des requêtes et de visualiser le plan des requêtes. Par exemple, il a été possible de voir dans STREAM que les opérations d\u0027agrégation (sans fenêtrage) de la requête Q2 s\u0027effectuent de façon incrémentale sans garder en mémoire tout le flux pour recalculer le résultat de l\u0027agrégation. Une autre caractéristique de STREAM est de pouvoir spécifier des fenêtres logiques exprimées en nombre de tuples, ces fenêtres peuvent être utiles dans certaines applications.\nConclusion et perspectives\nNous avons montré que deux Systèmes de Gestion de Flux de Données (SGFD) permettent de traiter des données de consommation électrique arrivant en rythme continu et rapide. D\u0027après nos expérimentations, il apparaît que TelegraphCQ est mieux adapté à nos besoins que STREAM. En effet, il offre une simplicité d\u0027utilisation, une syntaxe claire et des fonctionnalités intéressantes pour analyser les résultats. De plus, l\u0027équipe de Berkeley travaillant sur ce projet est toujours active dans ce domaine alors que l\u0027équipe STREAM a suspendu ses travaux depuis janvier 2006. Cependant, la comparaison entre ces deux SGFD est principalement fonctionnelle et n\u0027a pas pris en compte les performances en temps d\u0027exécution et en consommation de ressources système. Ce sera un objectif de nos prochaines expérimentations. Une des perspectives des nombreuses équipes qui ont implémenté des SGFD est de proposer une architecture distribuée de leur système. Une première proposition est faite par l\u0027équipe de Aurora qui propose le système Boréalis (Abadi et al., 2005). Boréalis distribue le traitement des flux de données sur plusieurs sites d\u0027Aurora pour des raisons de répartition de charge et de tolérance aux pannes. Nous testerons dans nos prochains travaux ce système qui a le mérite d\u0027être en libre téléchargement. Nous surveillerons aussi les éventuelles versions futures de TelegraphCQ.\n"
  },
  {
    "id": "970",
    "text": "Summary\nThe goal of this work consists in designing and producing a Software Tool, by using the concepts of the Web Usage Mining, to offer to the Webmasters complete knowledge in order to make appropriate decisions. It works as follows: it extracts information from log files and it makes decisions to discover the behaviours of the users. It adapts to the users behaviour by adapting the contents and general aspects of the Web pages.\n"
  },
  {
    "id": "971",
    "text": "Introduction\nL\u0027analyse syntaxique et terminologique de textes organisés en corpus est envisagée depuis une quinzaine d\u0027années comme une issue possible au problème de la construction d\u0027ontologies ou de terminologies. Non seulement la référence à l\u0027utilisation de la langue est le moyen d\u0027assurer une certaine pertinence aux connaissances ainsi identifiées, mais surtout, la disponibilité de textes sous format électronique autorise le recours à leur analyse par des logiciels de Traitement Automatique des Langues (TAL). Enfin, dans le cas particulier où l\u0027ontologie à construire doit servir à indexer ou annoter des documents par des méta-données, le recours aux textes fournit une terminologie riche et au plus près de celle utilisée dans ces documents.\nPlusieurs méthodes ont été définies pour guider ce processus et le rendre systématique, comme Archonte (Bachimont, 2004) ou Terminae . Toutefois, ces méthodes se heurtent à la lourdeur du dépouillement des résultats des logiciels d\u0027analyse syntaxique, lexicale ou sémantique des textes auxquels elles ont recours. De plus, toutes s\u0027accordent à souligner le rôle majeur de l\u0027analyste (que nous appelerons aussi par la suite ontologue) dans l\u0027interprétation de ces résultats. Ainsi, lorsqu\u0027un outil d\u0027aide au repérage de relations selon une approche par patron présente des phrases contenant des termes en relation, il n\u0027est pas sûr que cette relation doive être intégrée dans l\u0027ontologie, ou encore qu\u0027elle mette en relation exactement les concepts auxquels renvoient ces termes (Séguéla, 2001).\nAussi, depuis quelques années, plusieurs tentatives sont effectuées pour réduire la charge de l\u0027analyste. Elles profitent des avancées de l\u0027apprentissage automatique ou de méthodes de clas-sification pour exploiter plus automatiquement les données linguistiques extraites des textes. On parle d\u0027Ontology Learning (Maedche, 2002). À titre d\u0027exemple, l\u0027approche préconisée par Maedche et Staab (2000) utilise des heuristiques et un algorithme de fouille de données appliqués aux textes pour « découvrir » des règles d\u0027association entre termes, et définir ainsi à la fois des concepts et des relations entre concepts.\nNotre système, Dynamo 1 , vise ce même objectif : réduire la part manuelle dans le dé-pouillement des résultats d\u0027analyse de textes, et suggérer une amorce de réseau conceptuel en vue de construire une ontologie plus efficacement. L\u0027approche retenue est tout à fait originale, et fait appel à un système multi-agent adaptatif. Ces systèmes sont capable de répondre aux changements induits par l\u0027utilisateur en modifiant leur structure. Ce choix est motivé par les qualités qu\u0027offrent les systèmes multi-agents adaptatifs : ils peuvent faciliter la mise au point interactive d\u0027un système (Georgé et al., 2003) (dans notre cas, un réseau conceptuel), permettent sa construction incrémentale automatique par la prise en compte progressive de nouvelles données (issues d\u0027analyses de textes) et enfin, ils peuvent être aisément implémentés de manière répartie. Ainsi le comportement de Dynamo a trois étapes : le système propose une taxonomie, l\u0027utilisateur fait des modifications locales et le système se réorganise pour faire une nouvelle proposition. Contrairement à ASIUM (Faure, 2000), l\u0027utilisateur invervient sur le résultat entier et non sur le contrôle de chaque étape du processus de classification.\nDynamo prend en entrée les résultats d\u0027une analyse syntaxique et terminologique de textes. Il s\u0027appuie sur un algorithme de classification hiérarchique distribué, basé sur un calcul de similarité entre termes qui exploite leurs contextes linguistiques. En sortie, Dynamo présente à l\u0027analyste une organisation hiérarchique de concepts qu\u0027il peut valider, affiner ou corriger, jusqu\u0027à parvenir à un état satisfaisant du réseau sémantique.\nCet article présente l\u0027architecture et l\u0027approche retenue pour le système Dynamo, et particulièrement l\u0027algorithme de classification hiérarchique distribuée implémenté dans les agents (section 2.2). Après une étude quantitative et qualitative des performances de cet algorithme (section 3), nous soulignons l\u0027apport d\u0027une approche multi-agent pour la classification. Enfin, nous discutons de son utilisation dans la perspective de construire des ontologies (section 4).\nPrésentation du système Dynamo\nArchitecture proposée\nDans cette section, nous présentons l\u0027architecture de Dynamo. Elle se situe dans la lignée d\u0027une réflexion portée dans un contexte plus global de gestion et de maintenance d\u0027ontologies dynamiques en lien avec des collections de documents (Ottens et al., 2004). Contrairement à la réflexion précédente qui cherchait, entre autre, à aborder les besoins du Traitement Automatique des Langues (TAL), ici, nous nous attachons uniquement à combler les besoins de l\u0027Ingénierie des Connaissances (IC).\nLe système Dynamo se compose de trois grandes parties (cf. figure 1  (Bourigault et Aussenac-Gilles, 2003). Nous l\u0027avons sélectionné principalement pour sa robustesse et la grande quantité d\u0027informations extraites. En particulier, le réseau « Tête-Expansion », créé par cet outil, a déjà prouvé être une structure intéressante pour un système de classification (Assadi, 1998). Dans un tel réseau, chaque terme est relié, d\u0027une part à sa tête 2 et son expansion 3 , et d\u0027autre part à tous les termes dont il est lui-même tête ou expansion. Par exemple, « algorithme centralisé de classification » a comme tête « algorithme centralisé » et comme expansion « classification ». De même, « algorithme centralisé » est composé de « algorithme » et « centralisé ».\nLe réseau de termes récupéré en sortie de l\u0027extracteur est stocké dans une base de données. Pour chaque paire de termes, on suppose qu\u0027il est possible de calculer son indice de similitude (ou similarité), en vue d\u0027effectuer une classification (Faure, 2000)  (Assadi, 1998). De par la nature des données, nous nous intéressons uniquement à des indices de similarité entre objets décrits par des variables binaires, c\u0027est-à-dire qu\u0027un individu est décrit par la présence ou l\u0027absence d\u0027un ensemble de caractéristiques (Saporta, 1990). Dans le cas de termes, il s\u0027agit, en général, des contextes d\u0027utilisation. Avec Syntex, ces contextes sont identifiés par des termes et des relations syntaxiques caractérisées.\nLe système multi-agent implémente l\u0027algorithme de classification distribué décrit en détail dans la section 2.2. Il est conçu pour être à la fois le système produisant la structure résultante et la structure elle-même. En ce sens, chaque agent est une entité informatique représentant un concept, dont le comportement autonome lui permet de trouver sa place dans l\u0027organisation, à savoir l\u0027ontologie. Ils disposent tous de capacités de communication et d\u0027algorithmes leur permettant de s\u0027approcher ou s\u0027éloigner selon différents critères. La sortie du système est donc l\u0027organisation obtenue par l\u0027interaction des agents, tout en tenant compte des pressions exté-rieures exercées par l\u0027ontologue lorsqu\u0027il effectue des modifications de la taxonomie selon ses besoins. Dans cet article, nous nous intéressons principalement au processus de classification distribué dans les agents, bien que la création d\u0027une ontologie ne se réduise pas à cet aspect.\nUn algorithme de classification distribué\nCette section présente l\u0027algorithme de classification distribué, utilisé dans le système. Afin d\u0027en améliorer la compréhension, et en vue de son évaluation dans la section 3, nous rappelons ci-dessous l\u0027algorithme de base utilisé pour une classification hiérarchique ascendante dans un espace non métrique, mais dont la mesure de similitude utilisée est symétrique (Saporta, 1990) (ce qui est le cas avec les mesures utilisées dans notre système).\nAlgorithme 1 : Algorithme centralisé de classification hiérarchique ascendante\nEntrées : La liste L des individus à hiérarchiser\nDans l\u0027algorithme 1, à chaque étape de la classification, la paire des éléments les plus similaires est déterminée. Ces deux éléments sont regroupés, et la classe résultante est insérée dans la liste des éléments restants. L\u0027algorithme s\u0027arrête quand cette liste ne contient plus qu\u0027un élément.\nLa structure hiérarchique résultante de l\u0027algorithme 1 est nécessairement un arbre binaire de par le regroupement deux à deux. Le regroupement des éléments les plus similaires revient aussi à les éloigner des éléments dont ils sont le plus dissimilaires. L\u0027algorithme distribué multi-agent présenté est conçu autour de ces deux constats. Cet algorithme est exécuté de manière concurrente par chacun des agents du système.\nNotons aussi que dans la suite de l\u0027article nous utilisons lors de l\u0027évaluation des deux algorithmes un stratégie d\u0027agrégation en moyenne (Saporta, 1990) et l\u0027indice de similarité utilisé sur des termes dans Assadi (1998) ayant la formule suivante pour deux termes i et j :\nOù, a, b, c et d sont respectivement le nombre de contextes partagés par i et j, présents uniquement chez i, présents uniquement chez j, et présents ni chez i ni chez j. Les contextes sont déterminés en explorant le réseau « Tête-Expansion » (Assadi, 1998). Pour nos tests, nous avons fixé ? à 0, 75. Tous ces choix conditionnent l\u0027arbre résultat, mais n\u0027influencent ni le déroulement général de l\u0027algorithme ni sa complexité.\nLe système est initialisé de la manière suivante : -un agent TOP n\u0027ayant aucun parent est créé, il sera la racine de la taxonomie résultante ; -un agent est créé pour chacun des concepts à positionner dans la taxonomie, ils ont tous TOP comme parent ; ces concepts initiaux ont tous un terme du réseau comme référent. Une fois cette structure de base en place, l\u0027algorithme se déroule en parallèle au sein de chaque agent, jusqu\u0027à obtenir une position d\u0027équilibre global et donc une première version de la taxonomie résultante est présentée à l\u0027utilisateur. Par la suite, les modifications effectuées par l\u0027utilisateur dans la taxonomie auront pour effet de réactiver les agents concernés, l\u0027agorithme sera donc relancé localement. La première étape du procédé (figure 2) se déroule en parallèle dans les agents (ici on s\u0027intéressera uniquement à A k ) ayant plus d\u0027un frère (puisque nous cherchons à obtenir un arbre binaire). L\u0027agent A k envoie alors un message à son père P indiquant le frère dont il est le plus dissimilaire (ici A 1 ). P reçoit donc un message du même type de chacun de ses fils. Par la suite, ce type de message sera appelé un « vote ».\nEnsuite, lorsque P a reçu les messages de tous ses fils, il exécute la deuxième étape (figure 3). Grâce aux messages reçus indiquant les préférences de ses fils, P peut constituer trois sous-groupes parmi ses fils :\n-le fils qui a reçu le plus de « votes » par ses frères, c\u0027est-à-dire le fils étant le plus dissimilaire du plus grand nombre de ses frères. En cas d\u0027égalité, un des ex aequo est choisi au hasard (ici A 1 ) ;\n-les fils ayant permis « l\u0027élection » du premier groupe, c\u0027est-à-dire les agents ayant choisi leur frère du premier groupe comme étant celui qui leur est le plus dissimilaire (ici A k à A n ) ; -les fils restants (ici A 2 à A k?1 ). P crée alors un nouvel agent P (de père P ) et demande aux agents du second groupe (ici les agents A k à A n ) d\u0027en faire leur nouveau père. \nFIG. 4 -Classification distribuée : Etape 3\nEnfin, l\u0027étape 3 (figure 4) est assez évidente. Les fils repoussés par P (ici les agents A k à A n ) tiennent compte de son message et choisissent P comme nouveau père. La hiérarchie se constitue ainsi de proche en proche.\nRemarquons que cet algorithme converge nécessairement, puisque le nombre de frères d\u0027un agent va en diminuant. Lorsqu\u0027un agent n\u0027a plus qu\u0027un seul frère, son activité est stoppée (hors traitement des messages de ses fils). Il s\u0027agit donc bien d\u0027un algorithme de classification hié-rarchique distribué, les décisions étant prises par des négociations au sein de groupes d\u0027agents autonomes. Les traitements ainsi que les connaissances étant locales aux agents, et la communication se faisant par envoi de message, un tel algorithme peut tout à fait être réparti sur un réseau de machines.\nÉvaluation des performances\n3.1 Approche quantitative À présent, nous allons évaluer les propriétés de notre algorithme distribué. Pour cela, il convient de commencer par une évaluation quantitative, basée sur sa complexité, tout en le comparant à l\u0027algorithme 1 de la section précédente.\nLa complexité théorique sera calculée pour le pire cas, en considérant l\u0027opération de calcul de la similitude entre deux éléments comme élémentaire. Pour l\u0027algorithme distribué, le pire cas signifie qu\u0027à chaque étape, on ne peut constituer qu\u0027un seul groupe de deux individus. Dans ces conditions, pour un jeu de données initial de n éléments, les deux algorithmes donnent le nombre de calculs de similitude suivant 4 :\n4 Les calculs menant à ces équations ne sont pas donnés par souci de concision Ici, t 1 (n) et t dist (n) sont le nombre de calculs de similitudes effectués respectivement par l\u0027algorithme centralisé et par l\u0027algorithme distribué sur l\u0027ensemble du système. Les deux algorithmes ont donc une complexité en O(n 3 ). Dans le pire cas, l\u0027algorithme distribué effectue deux fois plus d\u0027opérations élémentaires que l\u0027algorithme centralisé. Cette différence de facteur 2 provient de la localité des prises de décision au sein des agents. Ainsi, les calculs de similitudes sont faits deux fois pour chaque paire d\u0027agents. On pourrait envisager qu\u0027un agent, après avoir effectué un tel calcul, envoie son résultat à l\u0027autre partie. Toutefois, cela reviendrait simplement à déplacer le problème en générant plus de communications au sein du système. \nFIG. 5 -Résultats expérimentaux\nLa complexité en moyenne de l\u0027algorithme a été déterminée expérimentalement. Pour cela, le système multi-agent a été exécuté avec des jeux de données en entrée allant de dix à cent termes. La valeur retenue est la moyenne du nombre de comparaisons effectuées pour une centaine d\u0027exécutions sans intervention de l\u0027utilisateur. Ceci donne les courbes de la figure 5. L\u0027algorithme distribué est donc en moyenne plus efficace que l\u0027algorithme centralisé, et sa complexité en moyenne plus réduite que pour le pire cas. Cela s\u0027explique simplement par la faible probabilité qu\u0027un jeu de données pousse le système à ne constituer que des groupes minimaux (deux éléments) ou maximaux (n ? 1 éléments) à chaque étape du raisonnement. La courbe 2 représente le polynôme logarithmique minimisant l\u0027erreur avec la courbe 1. Le terme de plus fort degré de ce polynôme est en n 2 log(n), donc notre algorithme distribué a en moyenne une complexité en O(n 2 log(n)). Enfin, on remarque l\u0027écart réduit entre les performances en moyenne, au maximum et au minimum. Dans le pire cas, pour 100 termes, l\u0027écart type est de 1960,75 pour une moyenne de 40550,70 (soit environ 5%) ce qui souligne la stabilité du système.\nConstat qualitatif\nBien que les résultats quantitatifs constatés soient intéressants, le réel intérêt de l\u0027approche relève de traits plus qualitatifs que nous présentons dans cette section. Tous sont des avantages obtenus grâce à l\u0027utilisation d\u0027une approche multi-agent.\nLe principal avantage de l\u0027utilisation d\u0027un système multi-agent adaptatif pour une tâche de classification est le côté dynamique introduit par un tel système. L\u0027utilisateur peut intervenir pour effectuer des corrections et la hiérarchie s\u0027adapte en fonction de cette demande. C\u0027est particulièrement intéressant dans un contexte d\u0027IC. En effet, la hiérarchisation fournie par le système est amenée à être modifiée par l\u0027utilisateur puisqu\u0027elle est le résultat d\u0027un traitement statistique. Lors de retours nécessaires au texte pour examiner les contextes d\u0027usage des termes (Aussenac-Gilles et Condamines, 2004), l\u0027ontologue pourra interpréter le contenu réel et ainsi réviser la proposition du système. Ceci est très difficile à réaliser avec une approche centralisée. Dans la plupart des cas, il faut trouver l\u0027étape du raisonnement qui a engendré le résultat erroné et modifier la classe correspondante manuellement. Malheureusement, dans ce cas, toutes les étapes de raisonnement subséquentes à la création de la classe modifiée sont perdues et doivent être recalculées en tenant compte de la modification. C\u0027est pourquoi un système comme ASIUM (Faure, 2000) présente à l\u0027utilisateur les classes créées à chaque étape du raisonnement, pour essayer de gommer ce problème grâce à une collaboration système-utilisateur. De plus, la hierarchie complète n\u0027est visible qu\u0027à la fin du processus. Ainsi, l\u0027utilisateur peut ne constater l\u0027introduction d\u0027une erreur que trop tard. Dans Dynamo, on laisse l\u0027algorithme se dérouler, bien que l\u0027utilisateur ait l\u0027opportunité de l\u0027interrompre et le relancer quand il le souhaite. Il dispose à tout instant d\u0027une hiérarchie complète qu\u0027il peut modifier. Ses interventions déclencheront l\u0027algorithme localement jusqu\u0027à obtenir un nouvel état stable. Ce couplage système-ontologue est indispensable pour une création d\u0027ontologie. Pour le mettre en place, aucun ajustement particulier sur le principe de l\u0027algorithme distribué n\u0027a été nécessaire car chaque agent effectue un traitement local autonome.\nDe plus, cet algorithme peut être réparti sur un réseau de facto. Il est tout à fait envisageable d\u0027exécuter les agents sur des machines différentes. La communication entre les agents se fait par envoi de messages et chacun d\u0027entre eux conserve son autonomie de décision. Donc, une telle modification du système, pour lui permettre de tourner sur plusieurs machines, ne néces-siterait aucun ajustement de l\u0027algorithme. En revanche, cela demanderait de revoir la couche de communication et la gestion de la création des agents dans notre implémentation actuelle.\ntralisé. En effet, le travail de synchronisation s\u0027effectue grâce à des envois de message, un tel système est donc plus coûteux en temps machine qu\u0027une implémentation centralisée.\nLe principal point faible actuel de notre algorithme est que le résultat est dépendant de l\u0027ordre d\u0027ajout des données. Lorsque le système travaille de lui-même sur un jeu de données fixe, fourni à l\u0027initialisation, le résultat final est équivalent à ce qui peut être obtenu avec un algorithme centralisé. En revanche, l\u0027ajout d\u0027un nouvel élément après une première stabilitisation a un impact sur le résultat final. L\u0027utilisateur intervient alors et ajoute un nouveau concept représenté par le terme « hé-patite » sous la racine. Le système réagit et se stabilise, on obtient en résultat l\u0027organisation présentée en figure 9. « hépatite » est bien situé dans la bonne branche, mais on ne retrouve pas la configuration désirée de la figure 6 du précédent exemple. Pour cela, il faut compléter notre algorithme distribué pour permettre à un concept de se déplacer le long d\u0027une branche. Nous travaillons actuellement sur ces règles, mais la comparaison avec des algorithmes centralisés deviendra alors plus difficile.\nVers la construction d\u0027ontologies\nNotre projet n\u0027a pas pour finalité de proposer un algorithme distribué de classification hié-rarchique, auquel cas celui présenté, qui crée un arbre binaire, suffirait, mais d\u0027obtenir une taxonomie dynamique. Cela signifie que le système présenté dans cet article doit encore évo-luer afin d\u0027être utilisable pour cette tâche.\nTout d\u0027abord, dans le cas général, faute d\u0027un voisinage utilisable dans le réseau « Tête-Expansion », certains termes extraits par Syntex ne sont pas traitables en utilisant notre algorithme de classification hiérarchique. Pourtant ils seraient intéressants pour la taxonomie. Il est donc nécessaire d\u0027ajouter des capacités à nos agents pour tenir compte d\u0027autres critères pour se positionner. En particulier, les éléments de plus bas niveau de la taxonomie pourraient être positionnés en examinant aussi la relation « Tête » des termes qui les représentent. Il a été constaté qu\u0027il s\u0027agissait d\u0027un travail effectué par les utilisateurs de Syntex pour la structuration des niveaux les plus bas de la taxonomie . Cela permettrait de rapprocher par exemple « infection » et « infection viral » qui, dans notre corpus de test, ne pourraient être rapprochés par le système actuel que si leurs contextes d\u0027utilisation étaient identiques.\nUne ontologie n\u0027est que rarement un arbre binaire, il faudra donc ajouter des critères supplémentaires afin d\u0027effectuer des regroupements et ainsi obtenir des noeuds n-aires. De la même manière, les interventions de l\u0027utilisateur poussent le système à introduire des niveaux dans la hiérarchie qui ne sont pas forcément nécessaires. On obtient parfois des situations dé-gradées où un noeud n\u0027a qu\u0027un seul fils. Il s\u0027agit donc des deux axes de simplification à explorer pour obtenir une structure plus raffinée.\nConclusion\nAprès avoir été présentée comme une solution prometteuse, assurant la qualité des modèles et leur richesse terminologique, la construction d\u0027ontologies à partir de l\u0027analyse de corpus s\u0027avère longue et coûteuse. Elle requiert la supervision de l\u0027analyste et la prise en compte de l\u0027objectif d\u0027utilisation de l\u0027ontologie. L\u0027utilisation de logiciels de TAL facilite l\u0027étude des connaissances repérables dans les textes à travers l\u0027utilisation du langage. Cependant, ces logiciels produisent de gros volumes d\u0027informations lexicales ou grammaticales qu\u0027il n\u0027est pas simple d\u0027exploiter pour définir des éléments conceptuels. Notre contribution se situe à cette étape du processus de modélisation à partir de textes, avant toute forme de normalisation ou différenciation.\nNous proposons une approche de classification hiérarchique distribuée, implémentée à l\u0027aide d\u0027un système multi-agent adaptatif, pour suggérer à l\u0027analyste une première structure taxonomique organisant des concepts. Notre système exploite en entrée le réseau terminologique résultant d\u0027une analyse syntaxique de corpus faite par Syntex. L\u0027état courant du déve-loppement permet de produire des structures simples, de les soumettre à l\u0027analyste et de les faire évoluer en fonction des corrections qu\u0027il a apportées. Les performances de cet algorithme distribué sont comparables à celle de la version centralisée. Ses points forts sont essentiellement qualitatifs, car il autorise des interactions avec l\u0027utilisateur et une adaptation progressive à l\u0027ajout de nouveaux éléments linguistiques. La décentralisation permet également de répartir les calculs effectués.\nDans la perspective de construire une ontologie, ce travail n\u0027est qu\u0027une ébauche. Il doit être poursuivi, à la fois pour assurer une meilleure robustesse à la classification, et pour parvenir à des structures plus riches que de simples arbres. Parmi ces enrichissements, un point difficile sera certainement l\u0027étiquetage des relations et la gestion différenciée des types de relation.\n"
  },
  {
    "id": "973",
    "text": "Introduction\nSuite au succès rencontré par les techniques de puces à ADN pour mesurer l\u0027expression des gènes à grande échelle, la reconstruction de réseaux de gènes à partir de ces données d\u0027expression a suscité depuis quelques années un intérêt croissant. Dans des travaux antérieurs [1,2], nous avons proposé une approche ayant pour but de découvrir différents types de règles entre gènes. Pour faciliter l\u0027interprétation des règles par les experts, nous proposons dans ce papier une visualisation conviviale des règles générées. Nous montrons comment les règles peuvent être visualisées sous forme de graphe orienté présentant les diverses relations découvertes dans les données. L\u0027originalité de notre proposition est de superposer différents types de règles dans un même suppport visuel. Nous proposons également aux utilisateurs de spécifier plusieurs gènes dits centraux, à partir desquels seront présentées uniquement les règles impliquant ces gènes centraux et limitant ainsi le coût de la génération des règles.\nApproche proposée\nNous souhaitons avant tout réaliser un outil convivial et proposer ainsi une méthode de visualisation intuitive pour les experts. D\u0027autre part, nous proposons d\u0027appliquer un filtre sur les règles générées en fonction de cinq indices de qualité (support, confiance, lift, leverage et conviction). Ne seront donc visualisées que les règles les plus intéressantes pour les experts, il est donc suffisant de pouvoir visualiser les indices pour la règle ou l\u0027attribut sélectionnés par un simple clic. L\u0027interprétation des règles est une étape particulièrement délicate et très difficile, puisqu\u0027une règle entre deux gènes impliquent également divers produits associés (protéines, facteurs de transcription...). C\u0027est pourquoi les biologistes sont rarement intéressés par les règles avec plus de 3 ou 4 attributs en partie gauche, l\u0027interprétation devenant vite très difficile. Enfin, nous souhaitons offrir la possibilité aux utilisateurs de choisir avant la généra-tion des règles, quelques gènes d\u0027intérêts que nous appelons gènes centraux, qu\u0027ils souhaitent voir apparaître en partie gauches ou droites des règles. Les biologistes pourront ainsi visualiser l\u0027ensemble des règles associées à ces quelques gènes qui les intéressent plus particulièrement. Nous avons donc opté pour une visualisation par graphe qui nous semble être la méthode la plus facile à interpréter et la plus intuitive pour les experts, notre objectif étant aussi de nous positionner dans le cadre de la reconstruction de réseaux de gènes. L\u0027originalité de notre approche réside dans le fait que nous proposons des graphes avec plusieurs sémantiques, que nous appelons réseaux globaux. Chaque sémantique est caractérisée par une couleur particulière. De plus, nous avons choisi de représenter les différents gènes composant les parties droites et gauches des règles, sous forme d\u0027un cercle pour une meilleure lisibilité. Enfin, les gènes centraux sont colorés en rouge, permettant ainsi de les repérer plus facilement.\nImplémentation\nL\u0027approche proposée a été développée spécifiquement pour les données d\u0027expression de gènes. Un nouveau module appelé RG (Rule Generation) a été intégré à un logiciel gratuit et open-source consacré à l\u0027analyse de données d\u0027expression de gènes, le logiciel MeV. Les points-clés de ce module sont tout d\u0027abord une interface conviviale permettant de choisir parmi plusieurs sémantiques et de spécifier les gènes centraux, le calcul de 5 indices de qualité pour chaque règle générée, la visualisation de réseaux globaux incluant diverses sémantiques, enfin la possibilité de visualiser sous forme textuelle les règles impliquant un gène en particulier avec les différents indices de qualité associés. La version étendue du logiciel MeV avec le module RG, proposant la visualisation de différents types de règles entre gènes, est disponible sur le site : http ://www.isima.fr/agier/GeneRules. \nRéférences\nSummary\nReverse engineering of gene regulatory networks is one of the major challenges of the functional genomics. From gene expression data, various techniques exist to infer these networks. We propose in this paper an approach for the visualization of between-genes interaction networks, from gene expression data. The originality of our approach is to superimpose rules with different semantics within the same visual support and to generate only the rules which imply central genes. Those are specified by the experts and make it possible to limit the generation of the rules to the only genes which interest the experts. An implementation was carried out in the free software MeV of the TIGR institute.\n"
  },
  {
    "id": "975",
    "text": "Introduction\nL\u0027extraction d\u0027informations à partir de séries temporelles est un domaine de plus en plus important dans la fouille de données. En effet de nombreuses applications utilisant des signaux réels nécessitent l\u0027utilisation des outils de ce champ d\u0027étude. Ainsi, la fouille de données dans des ensembles de séries temporelles utilise divers outils pour extraire des informations intrinsèques ou d\u0027interdépendance entre ces séries temporelles : recherche de similarités entre séries Marteau et al. (2006), corrélation entre séries Pelletier (2005), ou classification Nunez et al. (2002) par exemple. Les domaines d\u0027application pour ces outils sont très variés : finance Takada et Bass (1998), données météorologiques Harms et Deogun (2004), données médicales Summa et al. (2006), biotechnologies, etc. Par ailleurs les biotechnologies et plus particulièrement les bioprocédés offrent aujourd\u0027hui des défis importants concernant l\u0027extraction et la gestion des connaissances. L\u0027analyse des états physiologiques survenant durant ces bioprocédés est un point essentiel pour leur contrôle et leur optimisation. Ces bioprocédés ont longtemps utilisé des approches à base de modèles mathématiques. En effet les bioprocédés forment un système complexe de réactions biologiques qui peuvent être décrites par un système d\u0027équations dynamiques non-linéaires. Mais bien que ces modèles aient montré leur utilité, ils ne peuvent gérer des situations inattendues, car ils sont basés sur des simulations et ne tiennent pas toujours compte de tous les variables réelles mesurées lors des bioprocédés. Les méthodes qui ne sont pas basées sur des modèles sont de plus en plus utilisées. Parmi ces méthodes, beaucoup s\u0027appuient sur l\u0027analyse de signaux biochimiques (qui sont des séries temporelles) mesurés durant le bioprocédé. Ces méthodes se basent notamment sur des techniques de classification Regis et al. (2003), d\u0027apprentissage ou de règles d\u0027expert de type \"if-then\" Steyer (1991), Steyer et al. (1991). Cependant, si ces approches fournissent de bons résultats dans le cas où les états du bioprocédé sont bien connus (comme par exemple pour des bioprocédés de type \"batch\" 1 , voir Régis (2004), Roels (1983)), elles sont moins performantes dans le cas des bioprocédés pour lesquels on ne connaît pas parfaitement tous les états qui surviennent (c\u0027est le cas des bioprocédés de type \"fed-batch\" voir Régis (2004), Roels (1983)). L\u0027approche que nous proposons cherche à répondre à la problématique suivante :\nPeut-on caractériser les états d\u0027un système en s\u0027appuyant sur l\u0027analyse dynamique et statistique de séries temporelles ? Dans le cadre de l\u0027application sur les bioprocédés, est-il possible d\u0027extraire de l\u0027information concernant les états physiologiques d\u0027un bioprocédé de type fed-batch, à partir des séries temporelles mesurées pendant l\u0027expérience, et en utilisant peu ou pas de connaissances a priori des experts du domaine ?\nNous proposons d\u0027utiliser une méthode de clustering qui combine l\u0027analyse des propriétés dynamiques et des propriétés statistiques pour détecter et caractériser les états physiologiques d\u0027un bioprocédé fed-batch. L\u0027analyse dynamique consiste à détecter et sélectionner les singularités présentes dans les séries temporelles en utilisant la méthode du maximum du module de la transformée en ondelettes Mallat et Zhong (1992); Mallat et Hwang (1992) et l\u0027évalua-tion du coefficient de Hölder (aussi appelé exposant de Hölder ou exposant de Lipschitz). Ces singularités correspondent aux frontières des différents états. L\u0027analyse statistique consiste à calculer les corrélations des différentes séries temporelles entre le début et la fin d\u0027un état afin de caractériser chaque état. On rappelle que ces séries temporelles représentent pour cette application des variables biochimiques mesurées durant l\u0027expérience. Il faut noter que l\u0027approche que nous proposons est suffisamment générique pour être utilisée sur n\u0027importe quel ensemble de séries temporelles ou n\u0027importe quel système de flux de données, quelque soit le domaine d\u0027application concerné. Cependant, pour une utilisation pertinente de cette approche, des corrélations implicites ou explicites doivent exister entre les séries temporelles, faute de quoi les résultats obtenus auront peu de sens et ne seront pas interprétables. Le plan de cet article est le suivant.\nNous présentons, dans le paragraphe 2 l\u0027application biotechnologique, et dans le paragraphe 3, les travaux existants liés à cette application qui ont motivé la mise en place de la méthode proposée. Dans la section 4, nous présentons en détails l\u0027approche que nous proposons. Enfin nous présentons des résultats expérimentaux dans le paragraphe 5 avant de conclure dans le paragraphe 6.\nLes bioprocédés\nLes bioprocédés sont des procédés industriels ou expérimentaux utilisant des micro-organismes dans le but de fabriquer des produits biochimiques (produits pharmaceutiques, biocarburants) ou de produire de la biomasse. Durant ces bioprocédés, de nombreuses variables biochimiques sont mesurées. Certaines peuvent être contrôlées tandis que d\u0027autres expriment la biologie du système. Ces variables représentent essentiellement des gaz (dioxygène, dioxyde de carbone, etc.), des éléments rajoutés ou produits dans le milieu (substrat, base, biomasse, etc.) ou des variables physiques régulées (par exemple la variable agitation : c\u0027est la vitesse de rotation des pales d\u0027un moteur assurant l\u0027homogénéité du milieu). Pour des détails sur la fonction biochimique des variables on peut se référer à Régis (2004) ou dans une moindre mesure à Ré-gis et al. (2004a). Les mesures sont effectuées de manière régulière avec des capteurs en ligne ayant la même fréquence pour tous les variables. Ces variables biochimiques sont représentées sous forme de séries temporelles qui évoluent au cours du temps et expriment la dynamique du système pendant le procédé. Ces séries temporelles sont cruciales car elles permettent de comprendre les phénomènes biochimiques et physico-chimiques et par conséquent d\u0027optimiser le procédé. Ainsi l\u0027utilisation d\u0027outils de classification et d\u0027extraction d\u0027informations permet de caractériser de façon automatique les états du système. La classification consiste à segmenter les séries temporelles de telle sorte qu\u0027une classe corresponde à un état physiologique donné. Pour effectuer l\u0027analyse de ces séries temporelles, il convient de faire quelques remarques sur leurs propriétés (voir Régis (2004)). Ainsi il faut noter que :\n-ce ne sont pas des signaux périodiques. Ces signaux traduisent des phénomènes biologiques faisant intervenir des micro-organismes. Ces micro-organismes n\u0027ont pas de comportement physiologique périodique. -ce ne sont pas des signaux pairs ou impairs. Il n\u0027y pas de symétrie ou d\u0027antisymétrie dans ces phénomènes biologiques. -ce sont des signaux d\u0027énergie finie. Ils n\u0027ont pas de pics qui tendent vers l\u0027infini. Ce sont des séries temporelles basse fréquence, elles n\u0027ont pas des singularités oscillantes indéfiniment. -ce ne sont pas -pour la plupart en tout cas-des signaux déterministes. Certains signaux physiques régulés peuvent néanmoins être décrits par des formules mathématiques mais la plupart dépend de la biologie et il est impossible à l\u0027heure actuelle de trouver une formule mathématique décrivant parfaitement l\u0027activité liée à ces variables biologiques. -ce ne sont pas des signaux stationnaires. -aucune hypothèse n\u0027est faite concernant un quelconque bruit présent dans ces signaux.\nEn effet, on suppose que les signaux ne sont pas bruités ou le sont très peu, en raison du dispositif matériel utilisé pour les capteurs. Un exemple de ces variables biochimiques est donné sur la figure 1.  \nTravaux existants et motivation\nPlusieurs travaux sur la détection des états ont montré que les singularités des signaux mesurés durant un procédé (que ce procédé soit biochimique ou physico-chimique) correspondent au début et à la fin d\u0027un état du système. Ainsi plusieurs auteurs utilisant des méthodes très différentes les unes des autres sont arrivés à la conclusion que ces singularités représentaient les limites des états : c\u0027est le cas de Steyer et al. (1991) (en utilisant la logique floue et un système expert), Bakshi et Stephanopoulos (1994) (en utilisant les ondelettes et un système expert) et Doncescu et al. (2002) (en utilisant la logique inductive). De plus, Jiang et al. (2003) se basent aussi sur cette assertion pour détecter le début et la fin d\u0027un état. Cette hypothèse n\u0027est pas seulement valable pour les procédés chimiques ou biochimiques. En effet dans la plupart des applications réelles dans lesquelles on utilise des séries temporelles décrivant des phénomènes non stationnaires, les singularités représentent des informations significatives. Ainsi par exemple, Struzick montre que les singularités sont significatives dans des séries temporelles issues de la finance Struzik (2000) ou de données médicales Struzik (2003).\nPar ailleurs, la détection des états peut être suivie d\u0027une phase de caractérisation automatique de ces états. Une des particularités des bioprocédés de type fed-batch est qu\u0027un état peut apparaître plusieurs fois et à des moments différents de l\u0027expérience. Il est donc nécessaire de caractériser ces états pour savoir s\u0027ils réapparaissent au cours du temps. Plusieurs méthodes statistiques ont été proposées pour caractériser ces états. Par exemple, des méthodes de clas-sification basées sur l\u0027Analyse en Composantes Principales (ACP) Ruiz et al. (2004), l\u0027ACP adaptative Lennox et Rosen (2002), ou la kernel ACP Lee et al. (2004) permettent de distinguer et de caractériser les différents états d\u0027un procédé. Nous proposons d\u0027étudier la corrélation des variables biochimiques entre les intervalles temporels définis par les singularités, afin de caractériser les états du système. On tient compte ainsi de l\u0027évolution dynamique des corrélations à l\u0027image de ce qui est proposé dans d\u0027autres approches Nunez et al. (2002), Pelletier (2005) utilisées pour d\u0027autres applications du data mining. La méthode du maximum du module de la transformée en ondelettes Mallat et Zhong (1992); Mallat et Hwang (1992) est utilisée dans cet article pour détecter les singularités des signaux (afin de définir les limites d\u0027un état) et l\u0027étude des coefficients de corrélation entre ces signaux permet de caractériser ces différents états.\nDescription de la méthode proposée\nLa méthode que nous présentons se compose de deux étapes présentées dans les sous paragraphes suivants :\n1. la détection et la sélection des singularités des séries temporelles, par les ondelettes et le coefficient de Hölder. Ces singularités correspondent aux frontières des différents états. 2. la caractérisation et la classification des états par la corrélation. Les corrélations sont calculées sur chaque intervalle temporel défini par les singularités.\nDétection et sélection des singularités par les ondelettes et l\u0027exposant de Hölder\nLes singularités de séries temporelles (ici des signaux biochimiques) correspondent aux limites d\u0027un état. Plusieurs méthodes utilisent les ondelettes pour trouver ces singularités afin de détecter les états : par exemple, Bakshi et Stephanopoulos (1994)   Jaffard (1989Jaffard ( , 1997. Le coefficient de Hölder permet de caractériser le type de singularité comme cela est illustré sur les exemples de la figure 2 et dans le tableau 1.\nAinsi il est possible de sélectionner les singularités les plus significatives à partir de leur exposant de Hölder. Plusieurs méthodes de calcul du coefficient de Hölder existent : ces mé-thodes sont soit basées sur des régressions linéaires Mallat et Hwang (1992), soit basées sur  l\u0027optimisation d\u0027une fonction de coût Mallat et Zhong (1992). Une méthode utilisant les algorithmes génétiques a été proposée pour optimiser la fonction de coût, et semble fournir des résultats plus précis que les méthodes classiques Manyri et al. (2003). La sélection des singularités est réalisée en fonction des attentes des experts en microbiologie. Ainsi pour les bioprocédés de type fed-batch, les experts privilégient les singularités brusques (pic, saut, palier, etc.) qui correspondent à des singularités dont les coefficients de Hölder sont inférieurs à 1. La détection des singularités par les ondelettes et l\u0027évaluation du coefficient de Hölder ont déjà été testées dans un bioprocédé de type fed-batch pour une application proche de celle présentée dans cet article (voir Régis et al. (2004b)).\nFIG. 2 -Sur ce signal, les valeurs des coefficients de\nCaractérisation et classification des états par corrélation\nLes singularités détectées et sélectionnées définissent les bornes d\u0027intervalles temporels : on obtient ainsi une segmentation temporelle du bio-procédé en plusieurs intervalles de temps (un intervalle de temps représentant a priori un état). Nous proposons de caractériser chaque intervalle par les signes des différents coefficients de corrélations linéaires calculés deux à deux entre tous les variables. En effet, les règles de la logique floue du type \"if-then\" décrivent les relations entre les va-riables biochimiques du point de vue de l\u0027expert en microbiologie Steyer et al. (1991);Steyer (1991). Nous avons déjà signalé que ces règles s\u0027appliquaient parfaitement aux bioprocédés de type batch mais qu\u0027elles rencontraient certaines difficultés face aux bioprocédés de type fed-batch. Nous faisons l\u0027hypothèse dans l\u0027approche que nous proposons, que les règles de type \"if-then\" peuvent être implicitement remplacées par l\u0027analyse des corrélations entre les variables biochimiques. En fait, les corrélations décrivent les relations entre les variables mais d\u0027un point de vue statistique. De plus ces corrélations décrivent les relations entre variables en fonction du contexte et de manière plus exhaustive que des règles d\u0027expert. Par ailleurs cette approche permet de tenir compte implicitement de l\u0027évolution dynamique des corrélations en fonction du temps, car ces corrélations sont modifiées du fait des réactions biochimiques complexes réalisées et régulées par les micro-organismes au cours du temps. Cette hypothèse qui consiste à utiliser l\u0027évolution des corrélations de séries temporelles multiples au cours du temps comme élément discriminant de classification n\u0027est pas propre aux séries temporelles de cette application biotechnologique ; elle peut être étendue à d\u0027autres applications utilisant des séries temporelles multiples. Ainsi par exemple dans Pelletier (2005) il est indiqué que l\u0027évolution des corrélations entre séries temporelles financières caractérisent les différentes phases du système ; l\u0027auteur de cet article s\u0027appuie donc sur une hypothèse similaire à celle que nous proposons dans cette approche. Les états physiologiques sont donc caractérisés par l\u0027analyse des corrélations entre les signaux biochimiques. Sur chaque intervalle temporel défini à partir des singularités, le coefficient de corrélation est calculé entre les signaux deux à deux. Ce coefficient de corrélation (aussi appelé coefficient de Bravais-Pearson voir Saporta (1990)) est donné par l\u0027équation suivante :\noù Ü représente les valeurs de la première variable biochimique (sur un intervalle temporel donné), Ý les valeurs de la deuxième variable (sur le même intervalle temporel), Ò le nombre d\u0027éléments, Ü la valeur moyenne des éléments Ü , Ý la valeur moyenne des éléments Ý , et Ü et Ý les écarts type pour chacune des deux variables. Les valeurs mesurées Ü et Ý des deux variables sont les mesures comprises entre les singularités détectées par les ondelettes (on rappelle que ces singularités représentent les bornes de l\u0027intervalle temporel analysé). Ce coefficient de corrélation est en fait équivalent au cosinus du produit scalaire de deux variables projetées sur le cercle de corrélation pour une Analyse en Composantes Principales (ACP) effectuée entre ces deux variables. Sur chaque intervalle on garde le signe des coefficients de corrélation entre deux signaux. Chaque intervalle est ainsi caractérisé par un ensemble de signes positifs et négatifs. Les intervalles ayant la même série de signes sont regroupés dans la même classe et représente donc le même état (voir figure 3). Ruiz et al. (2004) proposent également une méthode basée sur l\u0027ACP pour une application voisine, concernant le traitement des eaux usées : la méthode consiste à classer les données projetées préalablement dans l\u0027espace défini par les deux premières composantes principales. Cette méthode réduit la dimension de l\u0027espace analysé mais l\u0027ACP ne tient pas compte du temps : l\u0027évolution des signaux n\u0027est pas prise en compte. Pour pallier ce problème, Ruiz et al. proposent d\u0027utiliser  ne tient pas réellement compte des changements survenant dans le procédé. Ainsi, la méthode basée sur la segmentation temporelle à partir de l\u0027exposant de Hölder des singularités, semble mieux adaptée si l\u0027on veut tenir compte de la dynamique du système. Bien qu\u0027elle repose sur une idée relativement simple (corrélation sur des intervalles temporels), l\u0027approche que nous proposons peut être utilisée dans d\u0027autres applications pour caractériser les divers états, car il existe plusieurs domaines dans lesquels les corrélations entre séries temporelles évoluent dynamiquement. On citera le cas de l\u0027économétrie pour laquelle il existe des modèles qui tiennent compte de cette évolution dynamique des corrélations Pelletier (2005).\nRésultats expérimentaux\nLes tests ont été effectués sur un bioprocédé fermentaire de type fed-batch. Ce bioprocédé utilisant des micro-organismes (levures) appelés Saccharomyces Cerevisiae a duré environs 34 heures. 11 signaux biochimiques ont été mesurés durant l\u0027expérience et ont chacun 2448 points de mesure. Les 11 séries temporelles ont été utilisées pour la classification. Ces signaux ont été normalisés (voir Régis (2004)). Pour la transformée en ondelettes, l\u0027échelle maximum utilisée qui était égale à ¾ ½¼ , a été choisi empiriquement après discussion avec un expert en microbiologie. La méthode a permis de détecter et de caractériser une action externe réalisée durant l\u0027expérience. La classification est composée au total de ¾¾ classes mais c\u0027est surtout la classe numéro qui nous intéressait (voir figure 4). En effet cette classe correspond à l\u0027ajout d\u0027un acide dans le milieu. Toutes les apparitions de la classe correspondent exactement à l\u0027ajout de cet acide. Autant que nous le sachions, c\u0027est la première fois qu\u0027une méthode (qui n\u0027est pas basée sur un modèle) permet de trouver automatiquement l\u0027addition d\u0027un acide dans un bioprocédé fed-batch. Les résultats sont donc encourageants et une analyse biologique plus approfondie doit être réalisée. De même cette approche pourrait mettre en lumière des phénomènes récurrents dans d\u0027autres applications utilisant des ensembles de séries temporelles. L\u0027approche est générique pour les séries temporelles mulitples de quelque origine que ce soit, car : -la méthode n\u0027utilise pas de modèle, ni de connaissance spécifique à la microbiologie.\nElle s\u0027appuie uniquement sur les propriétés analytiques et statistiques des séries temporelles. La classification dépend donc uniquement du contexte et non de connaissance exogène. -même sur des séries temporelles bruitées (comme celles issues des domaines écono-mique ou physique par exemple) il est possible de détecter les singularités les plus significatives (qui ont souvent une amplitude supérieure à celle du bruit) en choisissant le maximum de l\u0027échelle pour la transformée en ondelettes de façon adéquate (en ce qui concerne la détection des singularités par les ondelettes) et en sélectionnant les singularités en fonction de leur coefficient de Hölder (en ce qui concerne la caractérisation de ces singularités) \nConclusion\nNous avons présenté une méthode de classification non supervisée basée sur l\u0027analyse statistique et dynamique des variables mesurées pendant l\u0027expérience. Elle s\u0027appuie sur la détec-tion de singularités par le maximum du module de la transformée en ondelettes et des valeurs de leurs différents coefficients de Hölder pour tenir compte de la dynamique du système. Elle utilise les corrélations entre variables pour analyser les propriétés statistiques du système et caractériser les états de ce système. La méthode a été testée sur une expérience réelle et les résultats sont prometteurs. Elle a permis de détecter des informations pertinentes sans utiliser les informations a priori des experts de la microbiologie. Les perspectives et les approfondissement sont nombreux. D\u0027une part, les résultats obtenus ont été validés par l\u0027expertise humaine, mais ils peuvent être évalués avec des critères d\u0027évaluation numérique ou être comparés aux résultats d\u0027autres mé-thodes de classification. D\u0027autre part, des travaux supplémentaires consisteront à utiliser les classes trouvées dans une méthode supervisée à tester en temps réel. Une voie à explorer est d\u0027utiliser des plages de valeurs définies avec l\u0027expert au lieu des signes de corrélations afin d\u0027obtenir une certaine souplesse dans la caractérisation des états. Une autre possibilité serait de regrouper les vecteurs de signe des corrélations légèrement différents les uns des autres mais suffisement semblables dans une même classe. Cette méthode pourrait aussi être utilisée dans d\u0027autres applications faisant intervenir des ensembles de séries temporelles. On rappelle en effet que l\u0027approche n\u0027est pas spécifique aux bioprocédés mais elle est utilisable sur tout système de séries temporelles dans lequel il existe un lien explicite ou implicite entre les séries. Ainsi tout système de flux de données pourrait être analysé par cette approche.\n"
  },
  {
    "id": "976",
    "text": "Introduction\nDans cette introduction, nous décrivons tout d\u0027abord une situation particulière de l\u0027apprentissage supervisé où l\u0027on s\u0027intéresse à prédire le rang d\u0027une cible plutôt que sa valeur. Nous exposons ensuite deux approches qui permettent de passer d\u0027une prédiction ponctuelle en régression à une description plus fine de la loi prédictive. Nous présentons ensuite notre contribution qui vise à fournir une estimation de la densité conditionnelle complète du rang d\u0027une cible par une approche Bayesienne non paramétrique.\nRégression de valeur et régression de rang\nEn apprentissage supervisé on distingue généralement deux grands problèmes : la classification supervisée lorsque la variable à prédire est symbolique et la régression lorsqu\u0027elle prend des valeurs numériques. Dans certains domaines tels que la recherche d\u0027informations, l\u0027intérêt réside cependant plus dans le rang d\u0027un individu par rapport à une variable plutôt que dans la valeur de cette variable. Par exemple, la problématique initiale des moteurs de recherche est de classer les pages associées à une requête et la valeur intrinsèque du score n\u0027est qu\u0027un outil pour produire ce classement. Indépendamment de la nature du problème à traiter, utiliser les rangs plutôt que les valeurs est une pratique classique pour rendre les modèles plus robustes aux valeurs atypiques et à l\u0027hétéroscédasticité. En régression linéaire par exemple, un estimateur utilisant les rangs centrés dans l\u0027équation des moindres carrés à minimiser est proposé dans Hettmansperger et McKean (1998). L\u0027apprentissage supervisé dédié aux variables ordinales est connu sous le terme de régression ordinale (cf Chou et Ghahramani (2005) pour un état de l\u0027art). Dans la communauté statistique les approches utilisent généralement le modèle linéaire généralisé et notamment le modèle cumulatif (McCullagh, 1980) qui fait l\u0027hypothèse d\u0027une relation d\u0027ordre stochastique sur l\u0027espace des prédicteurs. En apprentissage automatique, plusieurs techniques employées en classification supervisée ou en régression métrique ont été appliquées à la régression ordinale : le principe de minimisation structurelle du risque dans Herbrich et al. (2000), un algorithme utilisant un perceptron appelé PRanking dans Crammer et Singer (2001) ou l\u0027utilisation de machines à vecteurs de support dans Shashua et Levin (2002)  Chu et Keerthi (2005). Les problèmes considérés par ces auteurs comprennent cependant une échelle de rangs fixée au préalable et relativement restreinte (de l\u0027ordre de 5 ou 10). Autrement dit, le problème se ramène à prédire dans quel partile se trouve la cible, en ayant défini les partiles avant le processus d\u0027apprentissage. On se rapproche alors plus d\u0027un problème de classification et les algorithmes sont évalués selon leur taux de bonne classification ou sur l\u0027erreur de prédiction entre le vrai partile et le partile prédit.\nVers une description plus complète d\u0027une loi prédictive\nQu\u0027il s\u0027agisse de classification ou de régression, le prédicteur recherché est généralement ponctuel. On retient alors uniquement la classe majoritaire en classification ou l\u0027espérance conditionnelle en régression métrique. Ces indicateurs peuvent se révèler insuffisants, notamment pour prédire des intervalles de confiance mais également pour la prédiction de valeurs extrêmes. Dans ce contexte, la régression quantile ou l\u0027estimation de densité permettent de décrire plus finement la loi prédictive.\nLa régression quantile vise à estimer plusieurs quantiles de la loi conditionnelle. Pour ? réél dans [0, 1], le quantile conditionnel q ? (x) est défini comme le réél le plus petit tel que la fonction de répartition conditionnelle soit supérieure à ?. Reformulé comme la minimisation d\u0027une fonction de coût adéquate, l\u0027estimation des quantiles peut par exemple être obtenue par l\u0027utilisation de splines (Koenker, 2005) ou de fonctions à noyaux (Takeuchi et al., 2006). Les travaux proposés dans Chaudhuri et al. (1994); Chaudhuri et Loh (2002) combinent un partitionnement de l\u0027espace des prédicteurs selon un arbre et une approche polynômiale locale. La technique récente des forêts aléatoires est étendue à l\u0027estimation des quantiles conditionnels dans Meinshausen (2006). En régression quantile, les quantiles que l\u0027on souhaite estimer sont fixés à l\u0027avance et les performances sont évaluées pour chaque quantile.\nLes techniques d\u0027estimation de densité visent à fournir un estimateur de la densité conditionnelle p(y|x). L\u0027approche paramétrique présuppose l\u0027appartenance de la loi conditionnelle à une famille de densités fixée à l\u0027avance et ramène l\u0027estimation de la loi à l\u0027estimation des paramètres de la densité choisie. Les approches non paramétriques, qui s\u0027affranchissent de cette hypothèse, utilisent généralement deux principes : d\u0027une part, l\u0027estimateur de la densité est obtenu en chaque point en utilisant les données contenues dans un voisinage autour de ce point ; d\u0027autre part, une hypothèse est émise sur la forme recherchée localement pour cet estimateur. Très répandues, les méthodes dites à noyau définissent le voisinage de chaque point en convoluant la loi empirique des données par une densité à noyau centrée en ce point. La forme du noyau et la largeur de la fenêtre sont des paramètres à régler. Une fois la notion de voisinage définie, les techniques diffèrent selon la famille d\u0027estimateurs visée : l\u0027approche polynômiale locale (Fan et al., 1996) regroupe les estimateurs constants, linéaires ou d\u0027ordre supérieur. On peut également chercher à approximer la densité par une base de fonctions splines. Cette dé-marche d\u0027estimation de la loi complète a déjà été adoptée en régression ordinale dans Chu et Keerthi (2005) en utilisant des processus Gaussiens dans un cadre Bayésien .\nNotre contribution\nNous proposons ici une approche Bayésienne non paramétrique pour l\u0027estimation de la loi conditionnelle du rang d\u0027une cible numérique. Notre approche utilise la statistique d\u0027ordre en amont du processus d\u0027apprentissage. La manipulation exclusive des rangs au détriment des valeurs rend notre estimateur invariant par toute transformation monotone des données et peu sensible aux valeurs atypiques. Si le problème étudié ne s\u0027intéresse pas à des variables ordinales mais à des variables numériques on peut bien entendu s\u0027y ramener en calculant les rangs des exemples à partir de leurs valeurs. Contrairement aux problèmes habituellement traités en régression ordinale, on considère en amont de l\u0027apprentissage l\u0027échelle globale des rangs de 1 au nombre d\u0027exemples dans la base. Notre méthode effectue un partitionnement 2D optimal qui utilise l\u0027information d\u0027un prédicteur pour mettre en évidence des plages de rangs de la cible dont le nombre n\u0027est pas fixé à l\u0027avance. Disposant d\u0027un échantillon de données de taille finie N et ne souhaitant pas émettre d\u0027hypothèse supplémentaire sur la forme de la densité prédictive, nous nous restreignons à des densités conditionnelles sur les rangs constantes sur chaque cellule. Notre estimateur se ramène donc à un vecteur d\u0027estimateurs de quantiles de la loi conditionnelle sur les rangs. A la différence de la régression quantile, le choix des quantiles n\u0027est pas décidé au préalable mais est guidé par les partitions obtenues.\nSuite à ce positionnement, la seconde partie est consacrée à la description de l\u0027approche MODL pour le partitionnement 1D en classification supervisée puis pour le partitionnement 2D en régression. Nous détaillons dans la troisième partie comment obtenir un estimateur univarié et un estimateur multivarié Bayesien naïf de la densité prédictive à partir des effectifs de ces partitionnements. Dans la quatrième partie, les estimateurs obtenus sont testés sur quatre jeux de données proposés lors d\u0027un challenge récent et sont comparés avec les autres méthodes en compétition. La dernière partie est consacrée à la conclusion.\net Merz, 1998) est tracé en fonction de la largeur des sépales, à gauche de la Fig. 1. La discré-tisation consiste à trouver la partition de [2.0, 4.4] qui donne le maximum d\u0027informations sur la répartition des trois classes connaissant l\u0027intervalle de discrétisation.  [ [2.95, 3.35[ [3.35, 4.4 \nFIG. 1 -Discrétisation MODL de la variable Largeur de sépale pour la classification du jeu de données Iris en trois classes.\nL\u0027approche MODL (Boullé, 2006) considère la discrétisation comme un problème de sé-lection de modèle. Ainsi, une discrétisation est considérée comme un modèle paramétré par le nombre d\u0027intervalles, leurs bornes et les effectifs des classes cible sur chaque intervalle. La famille de modèles considérée est l\u0027ensemble des discrétisations possibles. On dote cette famille d\u0027une distribution a priori hiérarchique et uniforme à chaque niveau selon laquelle : -le nombre d\u0027intervalles de la discrétisation est distribuée de manière équiprobable entre 1 et le nombre d\u0027exemples ; -étant donné un nombre d\u0027intervalles, les distributions des effectifs sur chaque intervalle sont équiprobables ; -étant donné un intervalle, les distributions des effectifs par classe cible sont équiprobables ; -les distributions des effectifs par classe cible pour chaque intervalle sont indépendantes les unes des autres. Adoptant une approche Bayesienne, on recherche alors le modèle le plus vraisemblable connaissant les données. En utilisant la formule de Bayes et le fait que la probabilité du jeu de données soit constante quel que soit le modèle, le modèle visé est celui qui maximise le produit p(modèle) × p(données|modèle). Formellement, on note N le nombre d\u0027exemples, J le nombre de classes cible, I le nombre d\u0027intervalles d\u0027une discrétisation, N i. le nombre d\u0027exemples dans l\u0027intervalle i et N ij le nombre d\u0027exemples dans l\u0027intervalle i appartenant à la jème classe cible. En classification supervisée, les nombres d\u0027exemples N et de classe cible J sont connus. On caractérise donc un modèle de discrétisation par les paramètres\n. On remarque que, dans la mesure où une discrétisation est caractérisée par les effectifs des intervalles, un tel modèle est invariant par toute transformation monotone des données. En considérant l\u0027ensemble des discrétisations possibles sur lequel on adopte la distribution a priori hiérarchique uniforme décrite précédemment, on obtient que le log négatif du produit p(modèle) × p(données|modèle) peut s\u0027écrire sous la forme du critère d\u0027évaluation suivant (1) :\nLes trois premiers termes évaluent le log négatif de la probabilité a priori : le premier terme correspond au choix du nombre d\u0027intervalles, le second au choix de leurs bornes et le troisième terme décrit la répartition des classes cibles sur chaque intervalle. Le dernier terme est le log négatif de la vraisemblance des données conditionellement au modèle. On trouvera tous les détails nécessaires à l\u0027obtention de ce critère dans Boullé (2006  Pour illustrer le problème du partitionnement 2D lorsque le prédicteur et la cible sont des variables numériques, nous présentons en Fig. 2 le diagramme de dispersion des variables longueur de pétale et longueur de sépale du jeu de données Iris. La figure montre que les iris dont la longueur de pétale est inférieure à deux cm ont toujours des sépales de longueur inférieure à six cm. Si l\u0027on sépare les valeurs de la variable cible longueur de sépale en deux intervalles (inférieur et supérieur à six cm), on peut décrire la loi de répartition de cette variable conditionnellement au prédicteur longueur de pétale à l\u0027aide d\u0027une discrétisation du prédicteur comme en classification supervisée. L\u0027objectif d\u0027une méthode de partitionnement 2D est de décrire la distribution des rangs d\u0027une variable cible numérique étant donné le rang d\u0027un prédicteur. La discrétisation du prédicteur et de la cible comme illustrée en Fig. 3  En utilisant le modèle de discrétisation 2D et la loi a priori définis précedemment, on peut écrire le logarithme négatif du produit p(M) × p(données|M) sous la forme du critère (2) pour un modèle de discrétisation M :\nPar rapport au critère obtenu dans le cas de la classification supervisée, il y a un terme supplémentaire égal à log(N ) pour la prise en compte du nombre d\u0027intervalles cible selon la loi a priori et un terme additif en log(N .j !) qui évalue la vraisemblance de la distribution des rangs des exemples dans chaque intervalle cible. Nous présentons succintement l\u0027algorithme adopté pour minimiser ce critère. Nous débutons avec une partition initiale aléatoire de la cible puis, tant que le critère décroît, nous optimisons alternativement la partition 1D du prédicteur pour la partition fixée de la cible et la partition 1D de la cible pour la partition fixée du prédicteur. Nous répétons le processus pour plusieurs partitions initiale aléatoire de la cible et nous retournons la partition qui minimise le critère. En pratique, la converge s\u0027effectue très rapidement, en deux ou trois itérations. Les discrétisations 1D sont effectuées selon l\u0027algorithme d\u0027optimisation utilisé pour la classification supervisée. La valeur du critère (2) pour un modèle de discrétisation donné est relié à la probabilité que ce modèle explique la cible. C\u0027est donc un bon indicateur pour évaluer les prédicteurs dans un problème de régression. Les prédicteurs peuvent être classés par probabilité décroissante de 3 De la discrétisation 2D à l\u0027estimation de densité conditionnelle sur les rangs\nCas univarié\nLe passage du partitionnement 2D à l\u0027estimation de densité conditionnelle univariée est illustrée sur un jeu de données synthétique proposé lors du récent Challenge Predictive Uncertainty in Environmental Modelling (Cawley et al., 2006) qui comporte N \u003d 384 exemples et un seul prédicteur. Le diagramme de dispersion ainsi que la partition MODL obtenue sont représentés en Fig. 4. Les intervalles de rangs 1 sont notés P i pour i \u003d 1, . . . , 7 pour le pré-dicteur et C j , j \u003d 1, . . . , 5 pour la cible. Comme pour le partitionnement 1D, les bornes des intervalles notées x 1 , . . . , x 6 et y 1 , . . . , y 4 sont obtenues par moyennage des valeurs du dernier individu d\u0027un intervalle et du premier individu de l\u0027intervalle suivant. Soit x la valeur du prédicteur pour un nouvel individu et P x i la plage de rangs à laquelle appartient le rang de x. Les effectifs de la grille nous permettent de calculer directement la probabilité que le rang de la cible de ce nouvel individu soit dans un intervalle donné C j :\nEn supposant la densité conditionnelle sur les rangs constante sur chaque plage de rangs cible que délimite la grille, on obtient une expression pour les probabilités élémentaires\nrg ( \nFIG. 5 -Estimations MODL de la fonction de répartition conditionnelle univariée sur les rangs pour les sept intervalles de discrétisation du prédicteur\n. On obtient un estimateur de la fonction de répartition conditionnelle sur les rangs en cumulant ces probabilités élémentaires :\nN .l . Les estimateurs MODL de la fonction de répartition conditionnelle au prédicteur sont tracés pour chacun des sept intervalles du prédicteur en Fig 5. \nCas multivarié\nDans le cas de P (P \u003e 1) prédicteurs, on peut en première approche construire un estimateur sous l\u0027hypothèse Bayesienne naïve que les prédicteurs soient indépendants conditionnellement à la cible. Soient (x 1 , . . . , x P ) les coordonnées d\u0027un nouvel individu dans l\u0027espace des prédicteurs et P x i l\u0027intervalle de discrétisation auquel appartient chaque composante x i . Sous l\u0027hypothèse Bayesienne naïve, la probabilité élémentaire multivarié s\u0027écrit alors :\nCette dernière expression peut être estimée grâce aux effectifs des partitionnements 2D. On peut en effet directement estimer le premier facteur P (k ? rg(y) \u003c k + 1) par la probabilité empirique 1/N . En ce qui concerne le produit, le premier facteur du numérateur s\u0027obtient selon le même principe que les probabilités univariées en (4) en considérant la plage de rangs à laquelle appartient y après fusion des partitions de la cible induites par chaque prédicteur. Soit J le nombre d\u0027intervalles cible pour la partition cible résultant de cette fusion et N M .j l\u0027effectif de chaque plage de rangs C M j pour j \u003d 1, . . . , J . Par construction, le partitionnement de la cible associé à chaque prédicteur est inclus dans ce partitionnement \"multivarié\" et l\u0027on note N lM ij l\u0027effectif de la cellule associée au ième intervalle du lème prédicteur et au jème intervalle cible du partitionnement multivarié. Chaque fraction du produit précédent peut alors s\u0027estimer\n.j\nEvaluation d\u0027un estimateur de la densité conditionnelle sur les rangs\nEn apprentissage supervisé, les fonctions de score les plus couramment utilisés pour éva-luer un prédicteur sont le score logarithmique et le score quadratique qui prennent différentes formes selon la tâche visée (classification, régression métrique ou ordinale) et l\u0027approche adoptée (déterministe ou probabiliste). En régression ordinale, les approches déterministes citées en introduction sont évaluées sur l\u0027erreur quadratique moyenne entre le rang prédit et le rang vrai en considérant les rangs comme des entiers consécutifs. Pour une prédiction probabiliste, le score logarithmique du NLPD est également utilisé dans Chu et Keerthi (2005) \nl\u003d1 pour le jeu de données D \u003d (x l , y l ) l\u003d1,...,L . Nous utilisons cette fonction de score pour notre estimateur de la densité conditionnelle sur les rangs. Pour un nouvel individu (x l , y l ), on calcule son rang d\u0027insertion k dans l\u0027échantillon d\u0027apprentissage et on estimê p(rg(y)|rg(x)) par la probabilité élémentairê P M odl (k ? rg(y) \u003c k + 1|rg(x)) décrite en (5) et en (6) .\nEvaluation expérimentale\nComme exposé en introduction, notre approche se distingue des problèmes habituellement traités en régression ordinale du fait qu\u0027on estime la distribution pour l\u0027échelle globale des rangs et non pour une plage de rangs restreinte à 5 ou 10 rangs distincts. D\u0027autre part, peu de méthodes fournissent un estimateur de la loi complète. Afin de positionner la méthode, nous avons donc choisi de la comparer en premier lieu à d\u0027autres estimateurs de densité conditionnelle sur les valeurs. Connaissant les valeurs associées aux rangs, chaque estimateur sur les rangs nous fournit un estimateur de la fonction de répartition conditionnelle en les N valeurs cibles de l\u0027échantillon. Si l\u0027on note y (k) la valeur cible de l\u0027individu de rang k, on a en effet :\nPour calculer la densité prédictive en tout point à partir de ces N quantiles conditionnels, on a adopté les hypothèses utilisées lors du Challenge à savoir l\u0027hypothèse que la densité conditionnelle soit uniforme entre deux valeurs successives de la cible et que les queues de distribution soit exponentielles 3 . Les différents estimateurs obtenus sont comparées sur le critère du NLPD sur le jeu de données test. Nous avons tout d\u0027abord constaté les mauvaises performances de l\u0027estimateur Bayesien naïf utilisant l\u0027ensemble des prédicteurs. Sur les trois jeux de données réels, le NLPD est en effet supérieur au NLPD pour la méthode dite de référence qui calcule à partir des données d\u0027apprentissage l\u0027estimateur empirique de la loi marginale p(y). Lorsque l\u0027hypothèse d\u0027indé-pendance est trop forte, il est connu qu\u0027elle dégrade fortement l\u0027estimation des probabilités a posteriori (Frank et al., 1998). Ces mauvaises performances sont donc certainement dues à des corrélations importantes entre les prédicteurs. Le tableau 2 indique le NLPD sur le jeu de données test pour les estimateurs MODL univarié et bivarié ainsi que pour la meilleure méthode du Challenge 4 et pour la méthode dite de réfé-rence. On observe tout d\u0027abord que pour tous les jeux de données, les estimateurs MODL sont meilleurs que la méthode de référence, ce qui est loin d\u0027être le cas pour toutes les méthodes soumises. On observe ensuite de bonnes performances des estimateurs MODL, notamment sur les jeux de données SO2 et Precip où les estimateurs univarié et bivarié se placent en tête. Les bonnes performances du prédicteur univarié montrent la qualité du partitionnement 2D obtenu malgré la manipulation exclusive des rangs et non des valeurs durant cette étape. D\u0027autre part, l\u0027estimateur bivarié est toujours meilleur que l\u0027estimateur univarié. Cela indique la présence \nConclusion\nNous avons proposée une approche Bayésienne non paramétrique pour l\u0027estimation de la loi conditionnelle du rang d\u0027une cible numérique. Notre méthode se base sur un partitionnement 2D optimal de chaque couple (cible, prédicteur). Les effectifs de chaque partitionnement nous permettent d\u0027obtenir des estimateurs univariés et un estimateur multivarié sous l\u0027hypothèse Bayesienne naïve d\u0027indépendance des prédicteurs. Une mise en oeuvre de ces estimateurs sur des données proposés lors d\u0027un récent challenge démontrent la qualité des partitionnements 2D. Les très bonnes performances du prédicteur univarié et du Bayesien naïf utilisant le meilleur couple de prédicteur nous encourage à travailler à l\u0027amélioration du Bayesien naïf utilisant l\u0027ensemble des prédicteurs.\n"
  },
  {
    "id": "977",
    "text": "Introduction\nL\u0027hypothèse que le KM peut être observé comme un système sociotechnique 1 (Coakes et al., 2002) traduit un chemin de pensée plus proche du constructivisme que du positivisme. Plus précisément afin d\u0027intégrer dans une même démarche l\u0027interrelation permanente de deux composants, on peut dire que l\u0027un est social (il incarne les besoins des individus) et l\u0027autre est technique (il représente l\u0027impératif technologique que l\u0027on veut) du système vu en termes entrées/transformation/sorties. Si l\u0027on détaille plus, on peut dire que la composante sociale est formée par deux sous composants interrelationés : domaine (activité ou business) et acteurs (sujet et autres). De même, la composante technique est formée aussi par deux sous groupes appelés : tâches (processus et données) et technologie 2 (méthodes et outils). La richesse de l\u0027approche réside alors dans sa complexité 3 . En effet l\u0027ensemble (social et technique) coexiste de façon harmonieuse avec les autres, mais dans différents niveaux systémiques en générant ainsi plusieurs niveaux explicatifs d\u0027une même réalité. Dans cet article, nous verrons tout d\u0027abord l\u0027aspect social du KM, puis nous présenterons brièvement l\u0027aspect technique du KM. Nous conclurons en analysant les perspectives de l\u0027ingénierie des systèmes de connaissances pour le KM dans un cadre sociotechnique.\nAspect social du KM\nApproche organisationnelle du KM\nHistoriquement, la connaissance dans le travail a été considérée comme un facteur décisif pour les entreprises (Ballay, 1997), c\u0027est ainsi qu\u0027au cours du temps l\u0027on trouve comme un levier d\u0027avantage productif dans une économie de production (1930), concurrentiel ou compétitif dans une économie de service (1960), coopératif dans une économie globalisée (1990), et collective dans une économie du savoir (2000). D\u0027où l\u0027émergence de nouveaux systèmes d\u0027organisation du travail et mode de management, autour de la connaissance, tels que : knowledge worker (1967), knowledge society (1969), learning organization (1990), systems thinking (1990), actionable knowledge (1996), knowledge-creating company (1995), information ecology (1997), information age (1997), knowledge-based economy (1997), corporate knowledge (2000), corporate longitude (2000), knowledge-based assets (2000), social responsibility in the information age (2005), etc.\nL\u0027approche organisationnelle du KM trouve ses racines dans le concept knowledgecreating company (connaissance créatrice organisationnelle) proposé par Nonaka et Takeuchi (1995) comme un facteur critique d\u0027innovation et un levier d\u0027avantage compétitif de l\u0027entreprise. Ces auteurs indiquent que « l\u0027entreprise ne \"traite\" pas seulement de la connaissance mais la \"créée\" aussi ». Dans ce même esprit non simonien Varela (1989) affirme « la méta-phore populaire désignant le cerveau comme une machine de traitement de l\u0027information n\u0027est pas seulement ambiguë, elle est totalement fausse ». Dans ce contexte, une entreprise ne peut pas créer de la connaissance sans êtres vivants, car ils sont le moteur de la connaissance créatrice organisationnelle 4 .\nParadigme du ballon de rugby\nL\u0027équipe est à la base de la connaissance créatrice organisationnelle de l\u0027entreprise (Nonaka et Takeuchi, 1995). Dans l\u0027entreprise la connaissance existe dans un domaine individuel et collectif au travers de la convergence de deux dualités. Premièrement, la dualité sujet/objet traduit le fait que le sujet conscient (individu, groupe, entreprise) créée de la connaissance en s\u0027impliquant lui-même dans l\u0027objet (espace de travail, environnement, le travail et ses outils de production,…). Ceci forme la dimension épistémologique de la connaissance. Cette dimension a été visualisée à travers des travaux de Polanyi, développée dans ses livres, Personal Knowledge (publié en 1958) et The Tacit Dimension (publié en 1966), sur le champ phi-losophique de la \"connaissance tacite\" (savoir-faire). Pour Polanyi « les êtres humains acquièrent la connaissance en créant et organisant activement leurs propres expériences » (Nonaka et Takeuchi, 1995). Deuxièmement, la dualité sujet/autres traduit le fait que le sujet conscient créée de la connaissance dans la relation avec les autres (individu, groupe, entreprise). Ceci forme la dimension ontologique de la connaissance. Les travaux de Barnard sur la \"connaissance comportementale\" (savoir-être) développé dans The Functions of the Executive (publié en 1938), ont contribué à la formation de cette dimension. Nonaka et Takeuchi (1995) distinguent la connaissance tacite et explicite. La connaissance tacite est attachée à la conscience du sujet (niveau individuel) et du groupe (niveau collectif) à travers des idées, métaphores, créances, concepts, hypothèses, modèles mentaux, analogies, etc. La connaissance explicite est attachée à un objet (rapport, document, email, modèle, maquette, plan, etc.). Ainsi, la connaissance individuelle (tacite ou explicite) est au niveau individuel, la connaissance collective (tacite ou explicite) est au niveau du groupe, et la connaissance organisationnelle (tacite ou explicite) est au niveau de l\u0027entreprise.\nUn \"ballon de rugby\" permet de symboliser la capacité de l\u0027entreprise de créer de nouvelles connaissances et de la valoriser sous forme de nouveaux produits ou services. Nonaka et Takeuchi (1995) affirment « la capacité d\u0027une entreprise considérée dans son ensemble, de créer de nouvelles connaissances, de les disséminer au sein de l\u0027organisation et de leur faire prendre corps dans les différents produits, services du système ». La création de connaissances n\u0027est pas un fait isolé sinon qu\u0027elle est liée à l\u0027innovation continue et à l\u0027avantage compéti-tif durable pour l\u0027entreprise.\nLe ballon (partie gauche figure 1) prend forme grâce à un processus de conversion de connaissances. Il s\u0027agit d\u0027un processus de causalité circulaire entre la dimension épistémolo-gique (tacite, explicite) et ontologique (individu, groupe, entreprise) de la connaissance, à partir de quatre mécanismes génériques. De gauche à droite : socialisation : conversion de connaissance individuelle tacite à connaissance collective et organisationnelle tacite, la connaissance est créée à travers le partage de l\u0027expérience par apprentissage organisationnel (observation, réflexion, construction de sens, implication personnelle, engagement individuel, etc.) ; extériorisation : conversion de connaissance individuelle tacite à connaissance collective et organisationnelle explicite, la connaissance est crée à travers l\u0027explicitation (formalisation, énonciation) de la connaissance individuelle tacite. Recours au langage ou support écrit pour communiquer idées, concepts, analogies, métaphores, hypothèses, modèles mentaux, etc. ; combinaison : conversion de connaissance individuelle explicite à connaissance collective et organisationnelle explicite, la connaissance est créée à travers la mise en commun de la connaissance explicite à travers des réunions, de changements d\u0027informations, de données, etc. ; internalisation : conversion de connaissance collective et organisationnelle explicite à connaissance collective et organisationnelle tacite, la connaissance dans l\u0027entreprise est créée à travers la réflexion à partir des modèles mentaux partage, savoir-faire technique, etc. entre ses membres. Plus loin nous caractérisons ces mécanismes en termes d\u0027apprentissage organisationnel.\nLe ballon avance (partie droite figure 1) entre la dimension épistémologique et ontologique sous forme de spirale ascendante 5 . La connaissance tacite et explicite est disséminée entre les acteurs (individu, groupe, entreprise) à tous les niveaux de l\u0027entreprise (stratégique, tactique, opérationnel).\nFIG. 1 -Paradigme du ballon de rugby.\nL\u0027apprentissage organisationnel prend forme dans la dimension épistémologique par une sorte de savoir-faire qui transforme la connaissance tacite en connaissance explicite (et vice versa), et d\u0027autre part, dans la dimension ontologique comme un savoir-être entre l\u0027individu, le groupe, et l\u0027entreprise. Le KM est vu comme une dynamique de l\u0027entreprise apprenante. Ingham commente dans l\u0027introduction de l\u0027édition française du livre de Nonaka et Takeuchi (1995), que « les processus d\u0027apprentissage concernent les \"savoir quoi faire\" et les \"savoir pourquoi faire\" il s\u0027agira d\u0027apprendre \"comment faire\" et le résultat prendra généralement la forme d\u0027un \"savoir-faire\". Mais ils pourront aussi entraîner une modification d\u0027un comportement et avoir trait alors à un \"savoir être\" ».\nL\u0027approche organisationnelle du KM relie alors la connaissance à l\u0027innovation (produits ou services), à l\u0027avantage (productive, concurrentiel ou compétitif, coopératif, collectif) et à l\u0027apprentissage organisationnel (dynamique de l\u0027entreprise apprenante).\nGérer les connaissances de l\u0027entreprise, caractérise les mécanismes de création de connaissances nouvelles et d\u0027apprentissage organisationnel par la mise en place d\u0027un processus de causalité circulaire parmi quatre états, dans le but de créer (connaissance créatrice organisationnelle), disséminer (individu, groupe, entreprise) et valoriser (nouveaux produits ou services) les connaissances de l\u0027entreprise, ces états sont : Socialisation : savoir ou savoir technique pour exprimer la connaissance (tacite ou explicite) créée par le raisonnement de l\u0027acteur (individu, groupe, entreprise), liés aux phénomènes d\u0027intelligence humaine (individu, groupe), d\u0027intelligence économique (entreprise), veille, etc. ; Extériorisation : savoir-faire collectif pour exprimer la connaissance organisationnelle ou collective, organisée comme un tout dans un système de connaissances et matérialisée par l\u0027innovation (produits ou services) capable de produire un avantage compétitif durable pour l\u0027entreprise ; Combinaison : savoirfaire ou savoir-faire technique pour exprimer la connaissance (tacite ou explicite) créée dans l\u0027action (l\u0027apprentissage) pour l\u0027acteur (individu, groupe, entreprise) ; Internalisation : savoircomportemental relatif d\u0027une part à la compétence (transformation de la connaissance en action) des ressources humaines qui matérialisent les qualités professionnelles de l\u0027individu dans son espace de travail (l\u0027environnement : le travail et ses outils de production), et au savoir-être (les qualités personnelles de l\u0027individu) exprimé au travers du phénomène de l\u0027intelligence émotionnelle.\nApproche managériale du KM\nLe fondement théorique de l\u0027approche managériale du KM est la conception de l\u0027entreprise comme un système ouvert (entrées/transformation/sorties) vis-à-vis de son environnement sur la base du concept d\u0027enaction de Weick (1979). L\u0027enaction décrit la relation entre entreprise et environnement comme deux sous-systèmes distincts, mais en interaction forte (confrontation), afin de maintenir une certaine stabilité dans la relation du système. Weick (1979) distingue un processus d\u0027interaction par inclusion (l\u0027un des deux systèmes doit dicter sa loi à l\u0027autre) et un processus d\u0027interaction par parallélisme (les systèmes négocient). Or, le concept d\u0027enaction de Weick est important car il permet l\u0027existence et l\u0027opérabilité d\u0027un système ouvert (entrées, sorties) mais aussi d\u0027un système fermé (ni entrées, ni sorties) vis-à-vis de son environnement, et comme nous verrons plus loin l\u0027approche de l\u0027enaction de Maturana et Varela (1987) permet l\u0027existence et l\u0027opérabilité d\u0027un système clos (organisation, structure, intelligence).\nLes travaux de Ermine (1996), Tounkara (2002), Tounkara et al. (2002), Ermine (2003), Boughzala et Ermine (2004) sur le \"patrimoine de connaissances\" de l\u0027entreprise (la connaissance est associée au métier) comme un système ouvert et les travaux de Grundstein (1996), Pachulsky et al. (2000), Pachulsky (2001), Grundstein et Rosenthal (2003), Ermine et al. (2006) sur les \"connaissances cruciales\" de l\u0027entreprise (la connaissance est associée à l\u0027action managériale) ont contribué à la caractérisation de l\u0027approche managériale du KM sur la base du concept d\u0027enaction de Weick (1979).\nParadigme de la marguerite\nSi l\u0027on fait l\u0027hypothèse que l\u0027entreprise (système ouvert) peut être décrite à travers un patrimoine de connaissances (approche de Ermine) ou un ensemble de connaissances cruciales (approche de Grundstein). Une \"marguerite\" (figure 2) permet de symboliser l\u0027enaction environnement-patrimoine de connaissances (ou connaissances cruciales) à travers des processus internes (interaction par inclusion) et processus externes (interaction par parallélisme) (Ermine, 1996). La marguerite est composée d\u0027un coeur pour abriter le patrimoine de connaissances de l\u0027entreprise, autour de celui-ci on a quatre pétales. Le coeur et les pétales sont définis dans une dynamique circulante permanente entre cinq processus (internes et externes) pour créer et gérer le patrimoine de connaissances. De gauche à droite, le processus externe de sélection par l\u0027environnement (phase de projection) permet de repérer les connaissances métiers (cruciales) du business par sélection d\u0027information (formulation des requêtes vers l\u0027environnement externe) afin d\u0027élaborer le corpus d\u0027information. Le processus interne de capitalisation et de partage des connaissances (phase de renseignement ou d\u0027intelligence) permet de préser-ver les connaissances métiers (cruciales) du business, et de le mettre à la disposition de tous les acteurs (individu, groupe, …) de l\u0027entreprise. Le processus externe d\u0027interaction avec l\u0027environnement (phase de confrontation) permet de valoriser les connaissances métiers (cruciales) du business qu\u0027on met en correspondance avec l\u0027extérieur pour la détection du besoin. Le processus interne d\u0027apprentissage et de création de connaissances (phase de création de sens) permet de faire évoluer le patrimoine de connaissances (connaissances cruciales). Le processus interne d\u0027évaluation du patrimoine de connaissances (phase de développement) permet de mesurer la valeur du patrimoine de connaissances ou les connaissances cruciales RNTI -X -de l\u0027entreprise, par un système de mesure traduisant la rentabilité de leurs investissements en matière d\u0027actifs immatériels.\nFIG. 2 -Paradigme de la marguerite.\nLe maintient de la marguerite en vie se fait par une confrontation permanente avec son environnement au travers de la sélection d\u0027information.\nL\u0027approche managériale du KM relie la connaissance au sujet, c\u0027est-à-dire la connaissance est liée à l\u0027action managériale (Grundstein parle de connaissances cruciales), ou bien la connaissance est reliée à l\u0027objet, c\u0027est-à-dire que la connaissance est liée au métier (Ermine parle de patrimoine de connaissances).\nGérer les connaissances de l\u0027entreprise, selon l\u0027enaction de Weick (1979), implique d\u0027une part que l\u0027entreprise et son environnement sont en confrontation permanente, et d\u0027autre part, la mis en place des processus de sélection, de capitalisation et de partage, d\u0027interaction, d\u0027apprentissage, de création, et d\u0027évaluation dans le but de repérer, préserver, valoriser, faire évoluer, et mesurer les connaissances métiers (cruciales) de l\u0027entreprise, où la connaissance dans l\u0027entreprise est décrite à travers un système ouvert (entrées, sorties).\nApproche biologique du KM\nLe fondement théorique de l\u0027approche biologique du KM prend ses racines dans la conception de l\u0027entreprise par rapport à une opération de distinction. Cette opération permet d\u0027indiquer que les causes et les effets sont distinguables dans des espaces forts différents. L\u0027un est le domaine conceptuel (l\u0027organisation) du processus organisationnel (c\u0027est un processus conceptuel de description abstrait de l\u0027organisation). L\u0027autre est le domaine physique (la structure) du processus structurel (c\u0027est un processus physique de description matérielle de la structure, il s\u0027agit bien ici de la description des propriétés des composants de la structure de l\u0027organisation). L\u0027opération de distinction, est formulée par rapport à l\u0027approche de l\u0027enaction de Maturana et Varela (1987). L\u0027enaction décrit la relation entre organisation et structure à partir de la spontanéité de trois processus : détermination structurelle, couplage structurel, et clôture opérationnelle. La détermination structurelle veut dire que l\u0027entreprise afin de définir l\u0027identité de l\u0027organisation doit maintenir l\u0027unité de la structure (l\u0027entreprise assure une transformation définie dans et par l\u0027organisation). Le couplage structurel signifie que l\u0027entreprise pour maintenir son organisation doit modifier sa structure. La clôture opérationnelle se réfère au fait que l\u0027entreprise est un système clos 6 au niveau de l\u0027organisation (le résultat de la transformation se situe à l\u0027intérieur des frontières du système lui-même), mais ouvert au niveau de la structure (le résultat de la transformation se situe à l\u0027extérieur des frontières du système lui-même). Autrement dit, une fois définie l\u0027organisation, il faut trouver la structure qui maintient l\u0027identité dans et par l\u0027organisation de l\u0027unité. Comme Maturana et Varela (1987) soulignent « la clôture opérationnelle engendre une unité, qui à son tour spécifie un domaine phénoménal ». Et donc, il y a une spontanéité dans leur relation.\nPartant de l\u0027hypothèse que l\u0027entreprise peut être approchée comme un système clos, c\u0027est-à-dire, d\u0027une part comme un système vivant caractérisé par l\u0027identité (organisation) et l\u0027unité (structure), et d\u0027autre part comme un système viable caractérisé par l\u0027autonomie (émergence d\u0027un comportement intelligent 7 ) nous formulons le paradigme de l\u0027arbre.\nParadigme de l\u0027arbre\nUn \"arbre\" (figure 3) permet de symboliser l\u0027enaction organisation-structure d\u0027un organisme vivant et viable. L\u0027arbre des connaissances 8 établi un rapport essentiel entre les processus de détermination structurelle, de couplage structurel, et de clôture opérationnelle qui ont lieu à l\u0027intérieur de l\u0027organisme de façon spontané afin de garantir l\u0027auto-maintient de l\u0027identité de l\u0027organisation, l\u0027auto-organisation de l\u0027unité de la structure, et l\u0027autogestion de l\u0027autonomie au cours du temps, comme l\u0027a dit Varela (1989) « tout système autonome est opération-nellement clos ».\nLa connaissance d\u0027un point de vue biologique est « ce qui nous unit à tous les hommes de tous les temps et la manière par laquelle nous faisons apparaître en nous nos significations existentielles, la manière par laquelle celles-ci sont créées, stabilisées, transformées. C\u0027est justement, dans ce processus d\u0027apprentissage social qu\u0027émerge en nous mêmes la signification du monde dans lequel nous vivons … la connaissance n\u0027est pas armée comme un arbre avec un point de départ solide qui croît progressivement jusqu\u0027à épuiser tout ce qu\u0027il faut connaître, car la connaissance est un mécanisme \"circulant\" et \"d\u0027émergence de signification\" … la connaissance est propre de l\u0027être vivant, la connaissance est un grain que l\u0027on sème dans le plus profond de nous mêmes » (Maturana et Varela, 1987). Autrement dit, la connaissance est une conduite (mécanismes et moyens d\u0027agir) qui permet à l\u0027organisme de faire seulement des choses qui n\u0027affectent pas sa survie. La connaissance correspond au fait de faire émerger chez l\u0027organisme un comportement intelligent. Et donc, l\u0027enaction organisation-structure est un mécanisme \"circulant\" et \"d\u0027émergence de signification\". Par conséquent, la connaissance ne doit pas obligatoirement impliquer des représentations vraies de la réalité objective.\nL\u0027approche biologique du KM relie alors la connaissance à la survie (processus d\u0027organisation et de structuration du vivant) et à l\u0027intelligence (processus d\u0027émergence d\u0027un comportement, une action). Le KM est un mécanisme de survie et d\u0027intelligence plutôt que de construction de sens (problème de la signification, vraies représentations de la réalité).\nGérer les connaissances de l\u0027entreprise, selon l\u0027enaction de Maturana et Varela (1987), consiste à mettre en place des processus de détermination structurelle, de couplage structurel, et de clôture opérationnelle dans le but de créer, stabiliser et transformer les connaissances de l\u0027entreprise, où la connaissance est décrite à travers un système clos (organisation, structure, intelligence) et non pas comme un système ouvert.\nFIG. 3 -Paradigme de l\u0027arbre.\nLe KM symbolise la capacité de l\u0027entreprise à capitaliser, partager, et créer de la connaissance afin de maintenir l\u0027identité de l\u0027organisation et l\u0027unité de la structure comme un tout dans le temps.\nSynthèses des approches\nLe tableau 1 montre une synthèse des approches organisationnelle, managériale et biologique du KM selon l\u0027aspect social.\nLes approches organisationnelle, managériale et biologique du KM font apparaître de vrais domaines de recherche pour aboutir aux concepts, méthodes et outils autour de méca-nismes génériques et processus du KM que nous avons synthétisés dans le tableau 1 : -Comment créer des connaissances nouvelles et faire de l\u0027apprentissage organisationnel dans l\u0027entreprise ? En effet, selon l\u0027approche organisationnelle du KM, le KM n\u0027est pas un système de traitement de l\u0027information mais bien un système de création des connaissances nouvelles et d\u0027apprentissage organisationnel.\n-Comment évaluer (mesurer) le savoir de l\u0027entreprise ? En effet, selon l\u0027approche managé-riale du KM, le KM permet d\u0027évaluer les connaissances du business (patrimoine de connaissances ou connaissances cruciales) de l\u0027entreprise par des méthodes et outils issus de l\u0027expé-rience en terrain.\n-Comment \"faire-évoluer\" (c\u0027est-à-dire créer de nouvelles connaissances là, où il n\u0027y a pas de savoir) et \"faire-émerger\" (c\u0027est-à-dire créer des connaissances nouvelles à partir d\u0027une représentation non symbolique de la réalité) les connaissances de l\u0027entreprise ? En effet, le KM selon l\u0027approche de l\u0027enaction de Weick (mis en évidence par Grundstein (1996), Pachulsky (2001), Ermine (1996), Tounkara (2002) permet la mise un place d\u0027un système de connaissance bâti sur l\u0027idée que la connaissance peut être définie au travers de l\u0027information (données, traitements) qui prend une certaine signification (concepts, tâches) dans un contexte (domaine, activité) donné. En revanche, le KM selon l\u0027approche de l\u0027enaction de Maturana et Varela (mis en évidence par Limone et Bastias (2006), Jiménez (2005 \nAspect technique du KM\nEn se plaçant maintenant dans l\u0027aspect technique, le KM est vu à travers de tâches (processus et données) et de la technologie (méthodes et outils).\nApproche ingénierie des connaissances et compétences du KM\nLa connaissance dans l\u0027entreprise peut être considérée d\u0027une part comme un objet (données) que l\u0027on peut approcher à partir d\u0027un projet d\u0027ingénierie des connaissances (méthodes et outils du KM 9 ), par exemple Ermine propose MASK (Method for Analyzing and Structuring Knowledge) et MKSM (Method for Knowledge System Management), permettant une analyse et une structuration d\u0027un patrimoine de connaissance liée à la connaissance métier (une tâche en particulier) . Ermine (1996) affirme « MKSM est l\u0027équivalent de MERISE pour les systèmes d\u0027information, à savoir une méthode d\u0027analyse de systèmes de connaissances pour aboutir à la conception d\u0027un système opérationnel de gestion des connaissances », et d\u0027autre part comme une action managériale (processus), par exemple Grundstein propose GAMETH (Global Analysis Methodology), une méthode pour le KM qui permet le repérage des connaissances cruciales de l\u0027entreprise (Pachulsky et al. (2000). Grundstein (1996) dit « le management des connaissances … couvre toutes les actions managériales visant à actionner le cycle de capitalisation des connaissances afin de repérer, préserver, valoriser, transférer et partager les connaissances cruciales de l\u0027entreprise ». La connaissance peut aussi être décrite à partir de cartes routières de compétences des acteurs de l\u0027entreprise, par exemple Authier et Lévy (1992) proposent une méthode de repérage des savoirs et des savoir-faire, afin d\u0027établir un arbre de compétences collectives pour l\u0027entreprise. La méthode se trouve implémentée dans un progiciel appelé GINGO et développé par www.trivium.fr. Ces méthodes et autres 10 du KM symbolisent un macroscope (en empruntant le terme de Joël de Rosnay) pour observer l\u0027acteur (individu, groupe, entreprise) dans son poste de travail (connaissance métier) en tant que facteur clé pour améliorer la capacité de l\u0027entreprise pour maintenir l\u0027organisation comme un tout dans le temps et non pas pour ajouter une autre application au parc informatique de l\u0027entreprise, sinon nous risquons dans le futur d\u0027enfermer le KM dans une problématique des systèmes à base de connaissances ou des systèmes experts.\nConclusion\nNous avons présenté dans cet article, une approche sociotechnique du KM. Cette approche caractérise davantage l\u0027aspect humain de la connaissance et son support technologique. C\u0027est ainsi qu\u0027ont été mises en évidence quatre perspectives, à savoir : l\u0027approche organisationnelle de Nonaka et Takeuchi (fondée sur le concept de knowledge creating-company) ; l\u0027approche biologique de Maturana et Varela (fondée sur le concept de l\u0027enaction) ; l\u0027approche managériale de Ermine (fondée sur le concept de la marguerite) ; et l\u0027approche ingénierie des connaissances et compétences du KM.\nAu carrefour de ces quatre approches, deux concepts sont le fondement de la problémati-que essentielle du KM. L\u0027un est le concept de \"faire-évoluer\" les connaissances, c\u0027est-à-dire de créer de nouvelles connaissances là, où il n\u0027y a pas de savoir. L\u0027autre est le concept de \"faire-émerger\" la connaissance, c\u0027est-à-dire de créer des connaissances nouvelles à partir d\u0027une représentation non symbolique de la réalité. En effet, tous les modèles de KM de nos jours sont gérés à partir du passé (les bonnes pratiques, le retour d\u0027expérience, la communauté de pratiques, etc.) et non pas à partir de l\u0027avenir (l\u0027inconnu, le chaos, le désordre, etc.). Nous pensons que les concepts \"faire-évoluer\" et \"faire-émerger\" la connaissance sont fort intéressants pour réfléchir sur la question.\nRemerciments\nL\u0027auteur tient à remercier Christine Deville pour son aide à la correction grammaticale du texte et tout particulièrement Germain Lacoste (Directeur de l\u0027ENI de Tarbes) pour son amitié, y finalmente, agradezco a algo tan vivo como es el canto siempre alegre de un picaflorcito entre flores… Références Authier M. et P. Lévy (1992). Les Arbres de Connaissances. La Découverte.\n"
  },
  {
    "id": "978",
    "text": "Introduction\nUn réseau de neurones est un ensemble de neurones interconnectés qui communiquent entre eux et avec l\u0027extérieur. Un réseau de neurones se présente comme un graphe où les noeuds sont les différentes unités de réseau et les arcs représentent les connexions entre ces unités. Le nombre de couches, le nombre de neurones par couche et les interconnexions entre les différentes unités du réseau définissent l\u0027architecture (encore appelée topologie) de celui-ci. Un neurone peut être appelé unité ou cellule. Comme tout système d\u0027apprentissage supervisé, les systèmes d\u0027apprentissage supervisé à base des réseaux de neurones fonctionnent en deux phases : la phase d\u0027apprentissage qui consiste à construire à partir des observations (exemples présentés sous forme (x, y) où y représente l\u0027observation de la fonction f en x) un système capable d\u0027approximer la fonction f dont l\u0027expression analytique n\u0027est pas facile à trouver ; la phase de classement qui utilise le modèle construit en phase d\u0027apprentissage pour produire des décisions (prédire un nouvel exemple qui ne faisait pas partie des observations de la base d\u0027apprentissage). Définir la structure du réseau pour de tel système n\u0027est pas une tâche évidente (J. Han et Hamber, 2001;A.Cornuéjols et Miclet, 2002). En effet, il n\u0027existe aucune méthode permettant de définir et de justifier la structure d\u0027un réseau de neurones (J. Han et Hamber, 2001).\nLa définition de l\u0027architecture du réseau de neurones multicouches pour la résolution d\u0027un problème donné reste un problème ouvert. Outre les méthodes génétiques (D.Curran et O\u0027Riordan, 2002), ce problème est souvent résolu en utilisant deux approches : la première consiste à ajouter successivement des neurones et des connexions à une petite architecture, la deuxième quant à elle consiste à supprimer des neurones et des connexions d\u0027une architecture initiale maximale. Ces deux approches ont souvent comme inconvénient le temps d\u0027apprentissage élevé et imprévisible.\nLes domaines d\u0027application des réseaux de neurones sont multiples (Dreyfus et al., 2002) : la biologie moléculaire (analyse des séquences d\u0027ADN (Shavlik et Towell., 1994)), prédiction, classification, traitement d\u0027images, le génie logiciel (estimation des coûts de logiciel (S. Mbarki et al., 2004)), etc. Aucune explication ne justifie à notre connaissance la définition des architectures utilisées. Pour les problèmes de classification en particulier, plusieurs méthodes ont été développées et sont proposées dans la littérature (J. Yang et al., 1999;Yang et al., 1996;Parekh et al., 1997b). On peut classer ces méthodes en deux catégories : celles qui construisent l\u0027architecture en utilisant un ensemble de connaissances de domaine (exemple de KBANN (Shavlik et Towell., 1994)) et les autres qui définissent cette architecture sans aucune connaissance (J. Yang et al., 1999;Parekh et al., 1997b;Yang et al., 1996;Parekh et al., 1995). Les algorithmes de construction des réseaux de neurones artificiels que nous avons rencontrés dans la littérature produisent des réseaux ayant les caractéristiques suivantes (Parekh et al., 1995(Parekh et al., , 1997a(Parekh et al., , 2000 : architecture minimale, habile à trouver le compromis entre les mesures de performances telles que le temps d\u0027apprentissage, habilité à généraliser, . . .etc. Ces méthodes constructives de ré-seau de neurones diffèrent par les facteurs suivants (Parekh et al., 1997a(Parekh et al., , 2000 : restriction des entrées (type de données en entrée), circonstances d\u0027ajout d\u0027une nouvelle unité, initialisation des poids de connexion de cette unité et son apprentissage.\nDans ce travail, notre intérêt porte sur les méthodes de recherche d\u0027architecture des ré-seaux de neurones multicouches feed-forward (les informations circulent des entrées vers les sorties, sans retour) pour la résolution des problèmes de classification. Les principaux paramètres de mesure de performance traités sont : la taille du réseau (nombre de neurones, nombre de couches...), la complexité en temps et la capacité de généralisation. Certaines méthodes de recherche d\u0027architecture de réseaux de neurones ont été évaluées sur des données de taille relativement petite et la qualité des résultats varie d\u0027un ensemble de données à l\u0027autre (Parekh et al., 1997a). D\u0027autre part, une comparaison théorique des ces algorithmes n\u0027a pas à notre connaissance été faite. Notre étude portera essentiellement sur la comparaison de ces algorithmes d\u0027après les mesures de performances citées ci-dessus et des résultats expérimentaux sur les données tirées de la base UCI (Newmann et al., 1998). Les opérations supplémentaires de prétraitement de données telles que projection, binarisation, la normalisation et autres ne seront pas abordées dans cette étude.\nLe reste du papier est organisé comme suit : la section suivante présente les réseaux de neurones multicouches, et quelques notions (définitions et apprentissage) liées aux réseaux de neurones multicouches ; la troisième section recense les algorithmes de construction d\u0027architecture neuronale. Les analyses expérimentales et théoriques feront l\u0027objet de la quatrième section.  (Rumelhart et al., 1986a,b). Cet algorithme produit des bons résultats lorsque l\u0027architecture est appropriée, il est également utilisé lorsque l\u0027architecture du réseau reste statique (D.Curran et O\u0027Riordan, 2002). Des outils tels que WEKA (Witten et Frank, 2005) et SNNS (Stuttgart Neural Network Simulator) 2 offrent aux utilisateurs la possibilité de définir la structure de leur réseau, ces outils apprennent ces réseaux par retropropagation. La difficulté majeure est de trouver cette architecture (A.Cornuéjols et Miclet, 2002).\nAlgorithme d\u0027apprentissage des poids de connexion\nL\u0027apprentissage dans les systèmes neuronaux peut se faire par unité ou par couche. Le principal algorithme d\u0027apprentissage des unités neuronales est le perceptron ; lorsque les données ne sont pas linéairement séparables, il est remplacé par l\u0027une de ses variantes : pocket with racket, barycentric,...etc. L\u0027apprentissage d\u0027une couche peut être généralisé à toutes ses unités ou se faire suivant le principe du Winner Take All (WTA). Afin de montrer l\u0027influence de l\u0027algorithme d\u0027apprentissage sur la complexité du système, nous présentons (voir tableau 1) sans entrer dans les détails les complexités en temps de ces algorithmes. Les détails sur ces algorithmes d\u0027apprentissage sont présentés dans (Parekh et al., , 2000, (Gallant, 1990) et (Frean, 1992a). Théoriquement, ces algorithmes utilisent de manière similaire l\u0027espace mé-moire. Le tableau 1 présente les complexités en temps de ces algorithmes d\u0027apprentissage dans lequel les variables désignent : n le nombre d\u0027objets, M le nombre de classes, m le nombre d\u0027attributs et N it le nombre d\u0027itérations dans l\u0027algorithme d\u0027apprentissage.\nAlgorithmes de construction des RNA\nCette section présente un résumé des algorithmes MTiling (Yang et al., 1996), MTower (Gallant, 1990;Parekh et al., 1995Parekh et al., , 1997a, MUpstart (Parekh et al., 1997b), Distal (J.Yang\nAlgorithmes\nComplexité en temps Perceptron et al., 1999). La convergence de ces algorithmes est démontrée dans (Parekh et al., 1995) et dans (Parekh et al., 1997a).\nTAB. 1 -Complexité en temps des algorithmes d\u0027apprentissage\n3.1 L\u0027algorithme Mtiling (Yang et al., 1996) Mtiling est une adaptation de l\u0027algorithme Tiling (Mezard et Nadal., 1989) à la classification multiclasses. Cette méthode construit un réseau de neurones multicouches dans lequel les unités d\u0027un niveau (couche) reçoivent des unités du niveau inférieur (immédiat). En cas de mauvais classement du réseau courant, la procédure détermine le neurone ayant fait le maximum d\u0027erreurs, augmente un certain nombre de neurones (auxillaires) à la couche de sortie courante, elle augmente également une nouvelle couche de M neurones au réseau et connecte les entrées de cette couche aux sorties des unités de la couche de sortie ; cette couche (ajoutée) devient la nouvelle couche de sortie. Les neurones ajoutés sont appris individuellement par les algorithmes pocket ou une variante. Le nombre maximal de couches cachées H est spécifié par l\u0027utilisateur. La couche de sortie fonctionne suivant le principe de WTA afin d\u0027assurer qu\u0027une seule classe soit active pour un exemple donné.\n3.2 L\u0027algorithme Mtower (Parekh et al., 1997a) Cette méthode construit le réseau sous forme de tour comme la méthode originale Tower (Gallant, 1990). L\u0027architecture finale du réseau est telle que : les neurones au sommet (sortie) sont connectés à tous les neurones d\u0027entrée et un neurone de la couche k reçoit de l\u0027information de tous les neurones de la couche k ? 1 (immédiatement connectés à la couche k). La tour est construite en ajoutant successivement les couches de M unités au réseau. Ces unités sont apprises par l\u0027une des variantes du perceptron. La couche ajoutée est complètement connectée à la couche de sortie et à la couche d\u0027entrée ; cette couche devient ainsi la nouvelle couche de sortie. La modification du réseau est répétée jusqu\u0027à l\u0027obtention de la précision (de classement) fournie par l\u0027utilisateur, l\u0027algorithme peut aussi d\u0027arrêter le nombre maximal de couche est atteint. La couche de sortie fonctionne aussi suivant le principe du WTA. La méthode Mpyramid (Parekh et al., 1997a) est semblable à Mtower à la seule différence que la couche ajoutée reçoit l\u0027information de toutes les couches précédentes.\n3.3 L\u0027algorithme MUpstart (Parekh et al., 1997b) Mupstart est une version de l\u0027algorithme Upstart (Frean, 1992b) pour la classification multiclasses. Comme Upstart, cet algorithme est basé sur une correction d\u0027erreur produite par le réseau courant. Ce réseau a une couche d\u0027entrée de N + 1 unités et une couche de sortie de M unités. Le fils gauche, X est augmenté au noeud en cas de \" wrongly on \" et le fils droit Y en cas de \" wrongly off \". On parle de \"Wrongly on\" (resp. \"Wrongly off\") lorsque la sortie obtenue est \"1\" (resp. \"0\") alors que l\u0027on désirait obtenir plutôt \"0\" (resp. \"1\"). Le neurone ajouté pour corriger l\u0027erreur produite à cette étape est appris par l\u0027algorithme du perceptron ou une de ses variantes. Ces neurones ajoutés sont appris avec pour sorties attendues (T x et T y) définies dans le tableau 2 ; dans ce tableau, y (resp. o) représente la sortie désirée (resp. sortie obtenue), T x (resp. T y) est la sortie attendue pour l\u0027apprentissage des unités x (resp. y) pour la correction de la différence entre y et o. C\u0027est un algorithme qui est basé sur le calcul de distance entre exemple (ou entre attribut). La distance peut être euclidienne, ou autre. Cette procédure calcule, trie d\u0027abord les distances entre exemples (ou attributs) et les stocke dans une matrice D. La procédure Distal construit à partir de la matrice des distances la couche cachée du réseau. Cette couche cachée est construite de la manière suivante : un neurone est ajouté pour apprendre k exemples tel que k soit l\u0027indice de la classe ayant la plus grande plage d\u0027exemples consécutifs dans D appartenant à la même classe. Les neurones de la couche cachée dans Distal sont à seuil sphérique (neurone actif si ? low ? W X ? ? high et inactif sinon, où W X est le résultat en sortie) et les poids de connexion entre l\u0027unité ajoutée et les unités d\u0027entrée sont initialisés par les attributs de l\u0027exemple i à partir duquel k a été trouvé. A l\u0027étape suivante, tous les poids du réseau entre la couche cachée et celle de sortie sont multipliés par deux. Les unités de la couche fonctionnent suivant le principe de WTA.\nComplexités -Evaluations théoriques\nLes algorithmes de recherche d\u0027architectures de réseau de neurones peuvent être classés suivant l\u0027approche de construction du réseau en deux grands groupes :\n1. l\u0027approche descendante, le réseau final est obtenu par élimination des neurones et de connexions. L\u0027architecture initiale comporte un nombre suffisant de neurones et de connexions capables de bien classer tous les exemples.\n2. L\u0027approche ascendante, le réseau est obtenu par ajout de neurones et de connexions. Les ajouts se font lorsque le réseau considéré produit des erreurs. L\u0027architecture initiale comporte un nombre assez réduit de neurones, généralement deux couches (la couche d\u0027entrée et la couche de sortie).\nTous les algorithmes présentés précédemment construisent le réseau par l\u0027approche ascendante. Seul Mtiling (Yang et al., 1996) permet de combiner les deux approches en recherchant par l\u0027approche ascendante une architecture maximale (architecture ayant le maximum H) de couches et ensuite élaguant cette architecture. Asymptotiquement, les algorithmes que nous avons présentés ont un comportement semblable. La complexité de ces algorithmes est calculée et présentée (tableau 3) au pire des cas : pour Distal par exemple, en général le nombre d\u0027exemples est toujours supérieur au nombre d\u0027attributs, c\u0027est ce qui justifie que le temps maximal est obtenu lorsque la distance entre exemples est considérée. Le tableau 3 présente les complexités en temps et en espace mémoire des algorithmes tandis que le tableau 4 présente d\u0027autres aspects non négligeables de ces algorithmes ; la projection consiste à ajouter un attribut supplémentaire ayant pour valeur la somme des carrés des autres attributs de l\u0027exemple ( i x 2 i ). Les particularités de ces algorithmes sont\nTAB. 3 -Complexité en temps et en espace mémoire\nAlgorithmes Nombre de Nombre de neurones opération couches cachées dans la couche cachée suplémentaire\nTAB. 4 -Recapitulatif de certains aspects.\nles suivantes : La particularité de l\u0027algorithme Mtiling, la correction d\u0027erreur se fait par ajout d\u0027une couche de M neurones (\"maitres\"), cette couche devient la nouvelle couche de sortie ; ajout de k neurones à la couche cachée immédiatement connectée à la couche de sortie. Les connexions sont complètes entre couches adjacentes.\nMtower ajoute tout simplement une nouvelle couche de M neurones. Cette couche devient également la nouvelle couche de sortie et est complètement connectée à la couche d\u0027entrée.\nDistal est plus spécifique que les autres méthodes ; elle construit un réseau ayant une seule couche cachée, chaque neurone de la couche cachée permet de séparer un sous ensemble d\u0027exemples. le réseau obtenu garantit le bon classement des exemples sans être appris.\nLa méthode MUpstart quant à elle ajoute à chaque correction un seul neurone ; ce neurone est appris individuellement et connectée au neurone ayant produit le plus grand nombre d\u0027erreurs.\nExpérimentations\nCes algorithmes ont été testés sur les données de la base UCI (Newmann et al., 1998), les attributs de ces ensembles sont tous numériques et sans valeur manquante. Les expérimentations précédentes sont présentées dans (Parekh et al., 1997a;J.Yang et al., 1999) Nos expériences ont porté sur les algorithmes Distal, Mupstart, Mtiling, Perceptron-Cascade, Mpyramid, Mtower. Ces expériences étaient validées par validation croisée d\u0027ordre 10 (10-Cross-Validation). Dans nos expériences, la distance choisie (sans aucune justification) entre exemples pour Distal est la distance euclidienne. Les unités ajoutées au cours de la construction du réseau avec les autres ont été appris par \"Perceptron with racket modification\". Les résultats obtenus après le classement que nous avons obtenus ne sont pas aussi meilleurs que ceux présentés dans (Newmann et al., 1998 \nTAB. 5 -Description des données utilisées pour les expérimentations\nAu cours de ces expériences, l\u0027attention s\u0027est surtout portée pour chaque algorithme sur les aspects suivants : le temps nécessaire pour la construction du réseau, le nombre total de neurones du réseau construit et la capacité de généralisation.\nNos expérimentations sont classées en deux groupes : Distal est la seule méthode qui construit son réseau sans faire d\u0027apprentissage ; pour cette raison, nous l\u0027avons évalué séparément des autres. Cette méthode présente des très bons résultats expérimentaux en présence des données de petite taille. Mais lorsque les données atteignent une certaine taille, Distal ne produit plus de résultat. Cela est due à un besoin excessif de l\u0027espace mémoire. La figure 5 montre l\u0027évaluation de Distal en fonction de la taille de données.\nLes autres algorithmes construisent un réseau de neurones en modifiant simultanément la structure de celui-ci et les poids de connexion. Leurs évaluations expérimentales sont très coû-teuses en temps. L\u0027apprentissage des unités ajoutées lors de la construction du réseau s\u0027est fait avec l\u0027algorithme \"Perceptron with racket modification\" avec 500 itérations, un maximum de 350 (choix arbitraire) neurones par couches et une précision d\u0027apprentissage et de test souhaitée à 100%. Le temps affiché ici est en secondes, mais représente le temps obtenu pour le meilleur système en validation croisée. \nFIG. 1 -Evaluation expérimentale de l\u0027algorithme Distal\nLes tableaux 6 et 7 présentent les résultats expérimentaux de ces algorithmes sur la base \"spambase\" avec respectivement comme nombre maximum de couches cachées 2 et 24. Ces tableaux montrent l\u0027influence du nombre de couches sur le taux de généralisation. En effet malgré un temps plus élevé, nous obtenons une bonne capacité de généralisation lorsque le nombre de couches est élevé.\nLes tableaux 8 et 10 présentent les résultats obtenus de l\u0027expérimentation sur les données présentées plus haut. Le nombre maximal de couches cachées choisies au cours de ces expériences est 2 (ce choix est arbitraire et nous a permis d\u0027avoir les résultats avec un temps relativement raisonnable). Le symbole \"-\" signifie que soit l\u0027expérience a échoué (générale-ment un débordement de la pile), soit le nombre maximal de couches a été atteint avec une précision inférieure à celle souhaitée. Notons que les données \"pendigits\" et \"optdigits\" ont pour particularité par rapport à \"spambase\" le fait que les exemples appartiennent à 10. Le comportement des algorithmes sur les données ci-dessus n\u0027est pas très appréciable, mais ne saurait être généralisé à toute la classification multiclasses. Les tableaux 9 et 11 montrent une amélioration de la capacité de généralisation lorsque le nombre maximum de couches passe de 2 à 24 ; ce qui explique l\u0027influence du nombre de couches sur la capacité de généralisation.\nDiscussion -Conclusion\nCes méthodes de construction des réseaux de neurones sont presque semblables ; ces algorithmes initialisent le réseau à une structure minimale et modifient ce réseau par ajout des unités au fur et à mesure que les erreurs surviennent. La question qui se pose est de savoir comment diriger le choix d\u0027un concepteur face au problème d\u0027architecture du réseau de neurones. Le choix de l\u0027architecture de réseau de neurones devra être orienté vers la satisfaction de l\u0027utilisateur final du système c\u0027est-à-dire que ce choix devrait aboutir à la production d\u0027un réseau de neurones ayant une bonne capacité de généralisation. D\u0027après notre étude, nous pouvons séparer ces algorithmes en deux catégories : Les algorithmes qui construisent les réseaux ayant une seule couche cachée ; dans cette catégorie, peut être classé Distal. Cet algorithme a aussi pour avantage la faible intervention de l\u0027utilisateur ; mais il y a un besoin énorme en espace mémoire. Les autres algorithmes construisent des réseaux pouvant avoir H couches cachées ; l\u0027utilisateur doit fournir à l\u0027algorithme en plus du nombre maximal (H) de couches cachées, la précision souhaitée, l\u0027algorithme d\u0027apprentissage. A partir des tables 6 et 7, on remarque une variation de taux généralisation en fonction du nombre maximal de couches. On peut également noter le mauvais comportement de ces algorithmes sur les données \"pendigits\" et \"optdigits\" mais nous ne pouvons pas généraliser sur toutes les grandes bases de données. Pour généraliser, nous devons tenir compte de l\u0027influence de la taille du réseau, de la répartition des données... Quelque soit la méthode de construction des réseaux de neurones, le choix des paramètres (modèle du réseau, nombre de couches, nombre de neurones par couches, définition des connexions, taux d\u0027apprentissage,...) reste toujours très problématique : pour la méthode \n"
  },
  {
    "id": "979",
    "text": "Introduction\nIl y a deux visions d\u0027un document XML : une vision « centrée données » et une vision « centrée document ». Les documents XML « centrés données » sont constitués d\u0027un ensemble d\u0027éléments ayant une structure régulière : un ensemble de fiches bibliographiques, par exemple. Les documents XML « centrés document » décrivent des textes plus ou moins structurés : des livres scientifiques, par exemple. Pour interroger des documents XML « centrés données », le langage de requêtes XQuery (le SQL de XML), défini par le W3C (W3C, 2006b), est tout à fait bien adapté. Par contre, pour interroger des documents XML « centrés document » XQuery n\u0027est pas suffisant lorsque l\u0027interrogation est de nature sémantique, comme par exemple, la recherche des chapitres de livres qui concernent un certain sujet. De telles requêtes sont traitées traditionnellement par les systèmes de recherche d\u0027information (Baeza-Yates et Ribeiro-Neto, 1999). Ce constat a conduit le W3C à proposer une extension de XQuery, XQuery Full-Text (W3C, 2006a), pourvue de fonctionnalités de recherche plein-texte. Le coeur de XQuery Full-Text est une fonction nommée ftcontains qui permet de tester si le contenu textuel d\u0027un élément est conforme à une requête exprimée à l\u0027aide d\u0027opérateurs spécifiques : troncatures, connecteurs logiques, calcul de distance entre mots, etc. La fonction ftcontains retourne un degré de similarité entre l\u0027élément et la requête qui peut être strict (appartient à {0, 1}) ou flou (appartient à [0,1]). Cependant, XQuery FullText n\u0027impose pas la façon de calculer ce degré de similarité, qui dépend donc de l\u0027implémentation.\nParallèlement, dans le cadre de l\u0027initiative INEX pour l\u0027évaluation de la recherche d\u0027information XML 1 , un langage de recherche d\u0027information spécifique nommé NEXI (Trotman et Sigurbjörnsson, 2005), a été proposé. Ce langage est une version simplifiée de XPath (W3C, 1999), qui intègre une fonction about permettant de calculer le degré de similarité d\u0027un élément XML avec une requête textuelle restreinte à un sac de mots éventuellement marqués d\u0027un indicateur de présence ou d\u0027absence. Pour ce langage aussi, la façon de calculer ce degré de similarité n\u0027est pas imposée.\nDans (Le Maitre, 2005), nous avons proposé d\u0027étendre le langage XQuery en y intégrant le langage NEXI muni d\u0027une sémantique floue basée sur le modèle de recherche d\u0027information vectoriel (Baeza-Yates et Ribeiro-Neto, 1999). Cet article améliore cette proposition sur le plan formel et décrit le prototype que nous avons construit au-dessus d\u0027un moteur XQuery classique : Galax (Fernandez et al., 2003) et Saxon (Saxon, 2006), à l\u0027heure actuelle.\nCet article est organisé de la façon suivante. Dans la section 2, nous décrivons le langage NEXI. Dans la section 3, nous proposons une sémantique pour ce langage, basée sur le modèle vectoriel et la logique floue. Dans la section 4, nous montrons comment intégrer une requête NEXI dans XQuery. Dans la section 5, nous présentons le prototype que nous avons réalisé afin de valider notre proposition. Enfin, dans la section 6, nous concluons et dressons quelques perspectives.\nNEXI\nLe langage NEXI (Trotman et Sigurbjörnsson, 2005), a été défini dans le cadre de l\u0027initiative INEX dans le but d\u0027effectuer de la recherche d\u0027information dans des documents XML. Il permet d\u0027exprimer deux types de requêtes : les requêtes « Content Only (CO) » qui ne portent que sur le texte d\u0027un document et les requêtes « Content and Structure (CAS) » qui portent à la fois sur le texte d\u0027un document et sur sa structure.\nUne requête CO est une liste de termes, éventuellement affectés d\u0027un indicateur imposant une présence ou une absence de ce terme dans le contenu textuel du fragment de document interrogé. Voici un exemple de requête CO : +XML XQuery -SGML Une requête CO s\u0027applique à un document XML et a pour réponse la conformité du contenu textuel de ce document avec cette requête. La forme de la réponse à une requête CO et la façon de calculer cette réponse ne font pas partie de la spécification du langage NEXI. Cette réponse pourrait être le degré de similarité du document interrogé avec la requête.\nUne requête CAS est une expression XPath simplifiée qui a la forme suivante dans sa version la plus générale :\nCette requête s\u0027applique à un arbre de document XML et retourne les noeuds de cet arbre de type nt 2 conformes au prédicat p 2 qui sont des descendants des noeuds de type nt 1 conformes au prédicat p 1 . Les prédicats p 1 et p 2 sont construits à partir de prédicats textuels ou numériques connectés par des opérateurs de conjonction ou de disjonction. Un prédicat textuel a la forme suivante : about(chemin de localisation, requête CO) et un prédicat numérique a la forme suivante :\nchemin de localisation connecteur littéral numérique\nIci encore, la forme de la réponse à une requête CAS et la façon de la calculer ne font pas partie de la spécification du langage NEXI. La réponse pourra être une séquence d\u0027éléments de type nt 2 affectés de leur degré de similitude avec la requête.\nConsidérons par exemple un document XML, enregistré dans le fichier « actes.xml », conforme à la DTD suivante : Dans le langage NEXI original, une requête NEXI est adressée à un document XML dans son entier. Mais dans le but d\u0027intégrer NEXI à XQuery, nous considérons qu\u0027une requête NEXI est adressée à une séquence de noeuds éléments de l\u0027arbre d\u0027un document XML dont chaque noeud texte est supposé être annoté par un sac de termes obtenu en extrayant les mots du texte contenu dans ce noeud, et en les normalisant par lemmatisation ou par racinisation. Notons que le fait de ne considérer qu\u0027un seul document n\u0027est pas restrictif, car lorsque l\u0027on souhaite interroger plusieurs documents, il est toujours possible de les rassembler dans un document unique.\nSoit d l\u0027arbre du document interrogé et M le nombre de termes indexant les textes contenus dans les noeuds textes de d. Conformément au modèle vectoriel, on associe à chaque noeud texte ou élément n j de d un vecteur à M dimensions v j \u003d (w 1j , …, w Mj ) où w ij est le poids du terme t i dans le noeud n j (w ij ? [0, 1]). Nous plaçant dans le cadre de la logique floue, nous interprétons le vecteur associé au n j comme étant la conjonction floue p 1j ? … ? p Mj où chaque p ij est un prédicat flou dont la valeur de vérité est le poids w ij du terme t i dans le noeud n j . Ce poids est calculé de la façon suivante :\n-Si n j est un noeud texte et lt j est le sac de termes annotant ce noeud, w ij est calculé selon la formule classique du modèle vectoriel : des k noeuds enfants (texte ou élément) du noeud n j . Cette fusion est interprétée comme la disjonction floue des conjonctions de prédicats associées à chacun de ces noeuds. Par exemple, si l\u0027on choisit la fonction max comme opérateur de s-norme, on aura w ij \u003d max(w\u0027 i1 ,…, w\u0027 ik ), ce qui correspond à la disjonction des valeurs de vérité des noeuds enfants pour le terme t i . Un vecteur v q \u003d (w 1q , …, w Mq ) est associé à chaque requête CO q où w iq est égal à ief i si le terme t i est présent dans la requête q, ou à 0, s\u0027il ne l\u0027est pas. La valeur d\u0027une requête CO q adressée à un noeud n j auquel est associé un vecteur v j est égale au cosinus de l\u0027angle entre les vecteurs v j et v q .\nDans le langage XQuery + NEXI, une requête CAS a la forme suivante :\noù e est une expression XQuery qui doit avoir pour valeur une séquence de noeuds élément. La valeur d\u0027une requête CAS est une séquence floue de noeuds de l\u0027arbre du document interrogé. Nous appelons séquence floue de noeuds une séquence S de la forme : \nIntégration de NEXI dans XQuery\nL\u0027intégration de NEXI dans XQuery est faite au moyen de la métafonction nexi et d\u0027une extension de la clause for de l\u0027opérateur FLWOR, similaire à celle de XQuery FullText (W3C, 2006a). L\u0027appel de la métafonction nexi a la forme suivante :\noù q est une requête CAS ayant la forme définie au paragraphe 3 et t est une expression XQuery dont la valeur appartient à [0, 1] et qui spécifie un seuil de pertinence. Cet appel retourne la séquence floue de noeuds résultant de l\u0027évaluation de q suivie du filtrage des noeuds dont le degré d\u0027appartenance est supérieur ou égal à t. Pour accéder aux noeuds de cette séquence et à leur degré d\u0027appartenance, il faut utiliser la clause for étendue suivante :\nfor $n score $s in nexi(q, t) qui lie successivement chaque noeud de la séquence floue de noeuds résultant de l\u0027appel nexi(q, t) à la variable $n et le degré d\u0027appartenance de ce noeud à la variable $s.\nPar exemple, la requête: « Titres et année des articles concernant SGML avec un seuil de pertinence de 0,5, triés par pertinence décroissante », peut s\u0027exprimer de la façon suivante :\nfor $a score $s in nexi(fn:doc(\"actes.xml\")//article[about(.,SGML)], 0.5) order by $s descending return \u003carticle\u003e{$a/titre, $a/année}\u003c/article\u003e Montrons maintenant que l\u0027on peut traduire en XQuery standard l\u0027appel de la métafonction nexi et la clause for étendue. Le principe consiste 1. à traduire l\u0027appel nexi(q, t) en une expression XQuery qui retourne la séquence  [e, p]. où nexi:about est la fonction qui calcule la valeur d\u0027une requête CO appliquée à un noeud élément ou texte.\nUne clause for $n score $s in nexi(q, t) peut être traduite en la suite de clauses for et let suivante :\nlet $fs :\u003d traduction de l\u0027appel nexi(q, t) for $k in 1 to fn:count($fs) div 2 let $n :\u003d $fs[2 * $k -1] let $s :\u003d $fs[2 * $k] 5 Description du prototype Le prototype que nous avons réalisé est composé :\n-d\u0027un préprocesseur qui reçoit en entrée une requête XQuery + NEXI et la traduit en une requête XQuery pure, en appliquant le processus de traduction défini au paragraphe 4 ci-dessus, -d\u0027un moteur XQuery, -d\u0027une interface utilisateur.\nLe préprocesseur\nLe processus d\u0027évaluation d\u0027une requête XQuery + NEXI est schématisé sur la figure 1. Le préprocesseur a été programmé en Java. L\u0027analyse lexicale et syntaxique de l\u0027appel à la métafonction nexi et de l\u0027extension de la clause for, est réalisée au moyen des parseurs JFlex et Cup.\nFIG. 1 -Evaluateur de requête XQuery + NEXI\nL\u0027évaluation d\u0027une requête fait appel à un module XQuery de préfixe nexi qui contient les définitions des fonctions assurant les opérations liées à la recherche d\u0027information textuelle : construction du sac de noeuds annotant un noeud texte, construction des vecteurs associés aux noeuds texte, aux éléments et aux requêtes CO, fonction about, etc.\nMoteurs XQuery\nLes caractéristiques des deux moteurs XQuery utilisables dans notre prototype, Galax et Saxon, sont :\n-Galax traite des requêtes XQuery conformes aux recommandations du W3C. Il a été développé en Objective Caml, et fournit des APIs en Objective Caml, C et Java, ainsi que des exécutables binaires pré-compilés pour les différents systèmes d\u0027exploitation. Le prototype fait appel au fichier binaire « galax-run.exe » pour évaluer la requête. L\u0027avantage de ce type d\u0027utilisation est que le traitement de la requête s\u0027effectue sur les ressources système de la machine. L\u0027inconvénient est que la portabilité du logiciel est dépendante des fichiers binaires dont on dispose. Un autre inconvénient réside dans le fait que l\u0027implémentation de NEXI en XQuery nécessite l\u0027utilisation de certaines fonctions mathématiques, comme par exemple le logarithme, qui ne sont pas fournies par Galax. -Saxon est un processeur XQuery/XSLT conforme aux recommandations du W3C.\nIl a été développé en Java. Saxon est, entre autres, utilisé dans l\u0027éditeur XML Oxygen. Le prototype utilise les APIs Java fournies par Saxon pour l\u0027exécution des requêtes XQuery. L\u0027avantage majeur de ce processeur est qu\u0027il permet de lier des modules Java aux modules XQuery. Ainsi, la déclaration : -Les chemins d\u0027accès aux fichiers nécessaires à l\u0027exécution de la requête.\n-Le moteur XQuery sur lequel sera exécuté la requête : Galax ou Saxon, dans la version actuelle. -La pondération globale d\u0027un terme dans le vecteur associé à un noeud texte. Si la réponse est Non : le facteur ief a la valeur 1 et donc le poids d\u0027un terme dans un noeud texte est égal à la fréquence de ce terme dans le sac de termes annotant ce noeud. Si la réponse est Oui : le facteur ief est calculé selon la formule indiquée au paragraphe 2 ci-dessus. \n"
  },
  {
    "id": "980",
    "text": "Introduction\nDans les applications de gestion de la relation clients, les scores permettent d\u0027identifier les clients les plus susceptibles de réagir positivement à une campagne marketing. L\u0027interprétation du score apporte alors une information supplémentaire pour améliorer l\u0027efficacité des campagnes marketing. L\u0027utilisation de la méthode présentée 1 ici doit se faire après une étape de sélection de variable qui aura supprimer les variables redondantes pour ne pas risquer de diluer l\u0027interprétation. L\u0027interprétation d\u0027un score est constituée de l\u0027association de l\u0027importance à l\u0027instance (I) d\u0027une variable d\u0027entrée et de l\u0027influence à l\u0027instance d\u0027une variable d\u0027entrée (I v ) présentées ci-dessous.\nNotations -Soit V j : la variable explicative j, X : un vecteur de dimension J, K : le nombre d\u0027instances, X n : le vecteur représentant l\u0027instance n, X nj : la composante j du vecteur n, F : le modèle, p : la sortie p du modèle, F p (X) : la valeur de la sortie p du modèle pour le vecteur X et F p j (X n ; X k ) désigne la sortie p du modèle étant donné le remplacement de la composante j de l\u0027instance X n par celle de l\u0027instance X k .\nImportance à l\u0027instance d\u0027une variable d\u0027entrée\nEtant donné 2 le modèle F , l\u0027instance considérée X n , la variable explicative V j du modèle et la variable à expliquer p du modèle, on définit la sensibilité du modèle S(V j /F, X n , p) par : la moyenne des variations mesurées en sortie du modèle lorsqu\u0027on perturbe l\u0027instance considérée X n en fonction de la distribution de probabilité de la variable V j . La variation mesurée, pour l\u0027instance X n est la différence entre la \"vraie sortie\" du modèle F j (X n ) et la \"sortie perturbée\" du modèle F j (X n , X k ).\nLa sensibilité du modèle pour l\u0027exemple X n à la variable V j est alors la moyenne des\n2 sur la distribution de probabilité (distribution empirique observée sur K exemples) de la variable V j . On a alors :\nEn réalisant cette mesure de sensibilité pour la sortie p mais quelque soit la variable d\u0027entrée 3 j on possède une distribution des sensibilités. On définit alors l\u0027importance de la variable V j à l\u0027instance X n , I(V j |F, X n , p), comme étant le rang, o, de la sensibilité du modèle S(V j |F, X n , p) parmi l\u0027ensemble des sensibilités S(V j |F, X i , p) ?i, j. Cette mesure fournit l\u0027importance d\u0027une variable d\u0027entrée pour l\u0027instance X n relativement à toutes les autres instances et toutes les autres variables. Cette mesure relative permet de se concentrer sur les seules informations pertinentes pour chaque instance. Cette mesure a été testée avec succès pour des problèmes de classification dans (Lemaire et Clérot, 2004) elle est notamment reliée aux travaux de (Breiman, 2001;Féraud et Clérot, 2002) \nInfluence à l\u0027instance d\u0027une variable d\u0027entrée\nUne variable peut \"tirer vers le haut\" (valeur forte) ou \"tirer vers le bas\" (valeur faible) la sortie du modèle. Pour l\u0027exemple X n la valeur \"naturelle\" de la sortie p \u0027 du modèle est par définition F (X n ). La valeur \"perturbée\" de la sortie du modèle pour l\u0027exemple et en perturbant la variable d\u0027entrée V j est F j (X n , X k ). La distribution des F j (X n , X k ) représente ce qu\u0027aurait pu être la valeur de la sortie du modèle pour l\u0027instance X n si sa variable V j avait été différente. La position de sa sortie \"naturelle\" au sein de cette distribution renseigne sur la nature de la valeur de sa variable V j . On définit alors l\u0027influence de la variable V j à l\u0027instance X n , I v (V j |F, X n , p), comme étant le rang, r, de la sortie \"naturelle\" parmi l\u0027ensemble de ses sorties potentielles. Cette mesure fournit l\u0027influence d\u0027une variable d\u0027entrée pour une instance relativement à toutes les autres valeurs \"potentielles\" de la variable.\nExemple d\u0027utilisation pour un problème de classification\nDans le cas d\u0027un problème de classification à deux classes (?1 ;+1) un rang important de I v dénotera une influence positive par rapport à la classe +1 et négative par rapport à la classe ?1 (et réciproquement pour un très faible rang de I v ). On obtiendra alors une interprétation de la forme (l\u0027interprétation sera réalisée variable explicative, j, par variable explicative en entrée du modèle) : \"Pour l\u0027instance X n la variable j qui est I importante indique qu\u0027elle est I v fortement de la classe +1\".\n"
  },
  {
    "id": "981",
    "text": "Introduction\nDans un projet de fouille de données, la phase de préparation des données vise à extraire une table de données pour la phase de modélisation (Pyle, 1999;Chapman et al, 2000). La préparation des données est non seulement coûteuse en temps d\u0027étude, mais également critique pour la qualité des résultats escomptés. La préparation repose essentiellement sur la recherche d\u0027une représentation pertinente pour le problème à modéliser, recherche qui se base sur une sélection de variables. L\u0027objectif de la sélection de variable est triple: améliorer la performance prédictive des classifieurs, le temps d\u0027apprentissage et de déploiement des modèles, et leur interprétabilité (Guyon et Elisseeff, 2003). Deux approches principales, filtre et enveloppe (Kohavi et John, 1997), ont été proposées dans la littérature. Les méthodes filtres évaluent la corrélation entre les variables exogènes et la variable endogène, indépendamment de la méthode de classification utilisée. Les méthodes enveloppes recherchent pour un modèle donné le meilleur sous-ensemble de variables. Les méthodes enveloppes, très coûteuses en temps de calcul, sont plutôt adaptées à la phase de modélisation. Parmi les méthodes filtres, les méthodes procédant par analyse univariée permettent d\u0027ordonner les variables exogènes par importance prédictive décroissante. Elles sont classiquement utilisées en phase de préparation des données pour rapidement extraire un sous-ensemble de variables pertinent pour la modélisation à partir d\u0027un ensemble de variables candidates potentiellement de grande taille. Dans cet article, nous nous focalisons sur l\u0027approche filtre.\nL\u0027approche filtre la plus fréquemment utilisée repose sur la mise en oeuvre de tests statistiques (Saporta, 1990), comme par exemple le test du Khi2 pour les variables exogènes caté-gorielles, ou les tests de Student ou de Fisher-Snedecor pour les variables exogènes numéri-ques. Ces tests d\u0027indépendance sont simples à mettre en oeuvre, mais présentent de nombreux inconvénients. Ils se limitent à une discrimination entre variables dépendantes et indépendan-tes, sans permettre un ordonnancement précis des variables exogènes, et sont contraints par des hypothèses d\u0027applicabilité fortes (effectifs minimaux, hypothèse de distribution gaussienne dans le cas numérique…). De nombreux autres critères d\u0027évaluation de la dépendance entre deux variables ont été étudiés dans le contexte des arbres de décision (Zighed et Rakotomalala, 2000). Ces critères sont basés sur une partition de la variable exogène, en intervalles dans le cas numérique et en groupe de valeurs dans le cas catégoriel. En recherchant de façon non paramétrique un modèle de dépendance entre variables exogènes et endogène, ils permettent une évaluation fine de l\u0027importance prédictive des variables exogènes. Dans le cas où tous les modèles de partitionnement de la variable exogène sont envisagés, un compromis doit être trouvé entre finesse de la partition et fiabilité statistique. Ce compromis est réalisé dans l\u0027approche MODL (Boullé, 2005(Boullé, , 2006a en formulant le problème comme un problème de sélection de modèle et en adoptant une approche Bayesienne.\nLes méthodes filtres univariées restent néanmoins limitées, en étant aveugles aux interactions entre variables exogènes. Ainsi, les variables redondantes, apportant la même information, ne peuvent être détectées. De même, les variables exogènes qui seules sont non informatives et simultanément le sont (cas du XOR par exemple) ne sont pas détectables par les méthodes filtres. L\u0027évaluation supervisée de l\u0027importance d\u0027une paire de variables exogènes, qui fait donc intervenir trois variables, a été peu étudiée dans la littérature. Les diagrammes de dispersion catégorisés par valeur endogène permettent une visualisation des paires de variables exogènes numériques, mais cela ne permet pas de quantifier l\u0027information prédic-tive. Le regroupement simultané des lignes et des colonnes d\u0027une table de contingence a été étudié dans un cadre général (Govaert et Nadif, 2006), ou dans le cadre des arbres de déci-sion (Zighed et al, 2005)  Nous proposons dans cet article une extension des méthodes de partitionnement univariées MODL au cas de l\u0027analyse bivariée, pour tout type de paires de variables, numériques, catégorielles ou mixtes. Chaque variable est partitionnée, en intervalles ou groupe de valeurs selon son type, ce qui permet de distribuer les individus sur les cellules d\u0027une grille bidimensionnelle. La corrélation entre la grille et la variable endogène est alors évaluée pour mesurer l\u0027importance prédictive jointe de la paire de variables. Le compromis entre information et fiabilité, lié à la finesse de la grille, est établi au moyen d\u0027une approche Bayesienne de la sélection de modèle, qui aboutit à un critère d\u0027évaluation des partitionnements joints de variables exogènes. Ce critère d\u0027évaluation est optimisé au moyen d\u0027une heuristique réutili-sant les techniques d\u0027optimisation univariée MODL, en optimisant alternativement le partitionnement de chaque variable de la paire, le partitionnement de l\u0027autre variable étant fixé.\nL\u0027article est organisé de la façon suivante. La section 2 rappelle l\u0027approche MODL dans le cas univarié. La section 3 décrit l\u0027extension de cette approche à l\u0027analyse bivariée. La section 4 présente l\u0027évaluation de la méthode. Enfin, la section 5 conclut cet article.\nAnalyse univariée supervisée\nCette section résume l\u0027approche MODL de la discrétisation supervisée (Boullé, 2006a) et du groupement de valeurs supervisé (Boullé, 2005). La discrétisation supervisée traite des variables exogènes numériques. Elle consiste à partitionner la variable exogène en intervalles, en conservant le maximum d\u0027information relative à la variable endogène. A titre illustratif, la figure 1 présente le nombre d\u0027individus par classe du jeu de données Iris (Blake et Merz, 1996), pour chaque valeur de la variable largeur de sépale. Un compromis doit être trouvé entre la finesse de l\u0027information prédictive, qui permet une discrimination efficace des valeurs endogènes, et la fiabilité statistique, qui permet une généralisation du modèle de discrétisation.\nDiscrétisation MODL\nDans l\u0027approche MODL, la discrétisation supervisée est formulée en un problème de sé-lection de modèle. Une approche Bayesienne est appliquée pour choisir le meilleur modèle de discrétisation, qui est recherché en maximisant la probabilité p(Model|Data) du modèle sachant les données. En utilisant la règle de Bayes, et puisque la quantité p(Data) ne dépend pas du jeu de données, il s\u0027agit alors de maximiser p(Model)p(Data|Model), c\u0027est-à-dire un terme d\u0027a priori sur les modèles et un terme de vraisemblance des données connaissant le modèle.\nDans un premier temps, une famille de modèles de discrétisation est explicitement défi-nie. Les paramètres d\u0027une discrétisation particulière sont le nombre d\u0027intervalles, les bornes des intervalles et les effectifs des classes endogènes par intervalle. Dans un second temps, une distribution a priori est proposée pour cette famille de modèles. Cette distribution a priori exploite la hiérarchie des paramètres: le nombre d\u0027intervalles est d\u0027abord choisi, puis les bornes des intervalles et enfin les effectifs par classe endogène. Le choix est uniforme à chaque RNTI -X - , ,\n}. En utilisant la définition de la famille de modèles de discrétisation et de sa distribution a priori, la formule de Bayes permet de calculer explicitement les probabilités a posteriori des modèles connaissant les données. En prenant le log négatif de ces probabilités, cela conduit au critère d\u0027évaluation fourni dans la formule (1).\nLes trois premiers termes représentent la probabilité a priori du modèle: choix du nombre d\u0027intervalles, des bornes des intervalles, et de la distribution des valeurs endogènes dans chaque intervalle. Le dernier terme représente la vraisemblance, c\u0027est à dire la probabilité d\u0027observer les valeurs de la variable endogène connaissant le modèle de discrétisation.\nLa discrétisation optimale est recherchée en optimisant le critère d\u0027évaluation, au moyen de l\u0027heuristique gloutonne ascendante décrite dans (Boullé, 2006a). A l\u0027issue de cet algorithme d\u0027optimisation, des post-optimisations sont effectuées au voisinage de la meilleure solution, en évaluant des combinaisons de coupures et de fusions d\u0027intervalles. L\u0027algorithme exploite la décomposabilité du critère sur les intervalles pour permettre après optimisations de se ramener à une complexité algorithmique en O (JN log (N)). \nGroupement de valeurs MODL\nFIG. 2 -Groupement de valeurs MODL de la variable couleur de chapeau pour la classification de la base Mushroom en deux classes.\nLe cas des variables exogènes catégorielles est traité au moyen d\u0027une approche similaire, en évaluant les modèles de groupement de valeurs. Dans le cas numérique, il s\u0027agit de partitionner les valeurs exogènes, avec une contrainte d\u0027adjacence entre valeurs (partitionnement en intervalles). Dans le cas catégoriel, il s\u0027agit toujours de partitionner les valeurs exogènes, cette fois sans aucune contrainte (partitionnement en groupes de valeurs). La figure 2 illustre le groupement des valeurs de la variable couleur de chapeau pour la classification de la base Mushroom (Blake et Merz, 1996).\nM. Boullé\nSoient N le nombre d\u0027individus, V le nombre de valeurs exogènes; J le nombre de classes endogènes, I le nombre de groupes de valeurs, N i. le nombre d\u0027individus dans le groupe de valeur i et N ij le nombre d\u0027individus de la classe j dans le groupe i. L\u0027application de l\u0027approche Bayesienne de la sélection de modèle conduit ici à un critère d\u0027évaluation d\u0027un groupement de valeurs, fourni dans la formule (2). Cette formule possède une structure similaire à celle de la formule (1), en remplaçant dans les deux premiers termes la probabilité a priori d\u0027une partition en intervalles par celle d\u0027une partition en groupes de valeurs.\n( )\nB(V,I) est le nombre de répartitions des V valeurs exogènes en I groupes (éventuellement vides). Pour I\u003dV, B(V,I) correspond au nombre de Bell. Pour I \u003eV, B(V,I) s\u0027écrit comme une somme de nombres de Stirling de deuxième espèce.\nLe critère d\u0027évaluation des groupements de valeurs est optimisé au moyen d\u0027une heuristique gloutonne ascendante décrite dans (Boullé, 2005). Des étapes de pré-optimisation et post-optimisation sont utilisées, de façon à garantir une complexité algorithmique en O(JN log(N)) sans sacrifier aux performances de la méthode.\nLes méthodes de discrétisation et groupement de valeurs MODL produisent le partitionnement des valeurs exogènes le plus probable connaissant les données. Des évaluations comparatives intensives (Boullé, 2005(Boullé, , 2006a) ont mis en évidence les apports de la méthode, tant d\u0027un point de vue informatif que prédictif.\nExtension à l\u0027analyse bivariée supervisée\nNous présentons dans cette section la nouvelle méthode d\u0027analyse bivariée, qui étend l\u0027approche MODL à l\u0027analyse supervisée des paires de variables exogènes. Après avoir introduit le problème au moyen d\u0027un exemple illustratif, nous présentons un critère d\u0027évaluation et un algorithme d\u0027optimisation de ce critère.\nIntérêt du partitionnement joint de deux variables\nLa figure 3 présente le diagramme de dispersion des variables V1 et V7 du jeu de données Wine (Blake et Merz, 1996), catégorisé par valeur endogène. Chaque variable isolément est faiblement discriminante. La variable V1 ne peut séparer les classes 1 et 3 au delà de la valeur 13. De même, la variable V7 confond les classes 1 et 2 au-delà de la valeur 2. Les deux variables conjointement autorisent une meilleure discrimination des classes.\nL\u0027approche utilisée pour qualifier l\u0027information prédictive contenue dans la paire de variables repose sur un partitionnement des individus en une grille de données. Chaque variable exogène est partitionnée en intervalles (ou groupes de valeurs selon son type). Le produit cartésien des deux partitions univariées répartit les individus sur une grille de données, dont les cellules sont définies par des paires d\u0027intervalles. Le lien avec la variable endogène se fait au moyen de la distribution des valeurs endogènes dans chaque cellule. Par exemple dans la figure 3, la variable V1 est discrétisée en 2 intervalles (borne 12.78) et la variable V7 en 3 intervalles (bornes 1.235 et 2.18). Les individus se répartissent dans les 6 cellules de la grille bidimensionnelle ainsi définie. Dans chaque cellule, nous obtenons une distribution des valeurs endogènes. Par exemple, le tableau de la figure 3  \nCritère d\u0027évaluation\nNous utilisons ici la même approche que dans le cas univarié pour rechercher le meilleur compromis entre information et fiabilité, en introduisant une famille de modèles de partitionnement bivariés, puis en choisissant le meilleur modèle au moyen d\u0027une approche Bayesienne. Nous nous intéressons d\u0027abord au cas des variables exogènes numériques, avant de généraliser à tout type de paires de variables, numériques, catégorielles ou mixtes. Définition 1. Un modèle de partitionnement bivarié supervisé est défini par une partition en intervalles pour chaque variable exogène, et par la distribution des valeurs endogènes pour chaque cellule de la grille de données déduite du croisement des deux partitions univariées.\nNotations.\n-  De façon similaire au cas de la discrétisation univariée, les deux premiers termes d\u0027a priori correspondent au choix de la partition (nombre d\u0027intervalles et bornes) de la première variable exogène. De même, les deux termes suivants correspondent au choix de la partition de la deuxième variable exogène. Le dernier terme d\u0027a priori, en fin de première ligne, repré-sente le choix de la distribution des valeurs endogènes dans chaque cellule. Le dernier terme de la formule 3, sur la deuxième ligne, représente la vraisemblance, c\u0027est à dire la probabilité d\u0027observer les valeurs de la variable endogène dans les cellules de la grille connaissant le modèle de partitionnement bivarié supervisé.\nLe critère d\u0027évaluation bivariée se généralise au cas des variables exogènes catégorielles, en remplaçant les termes de partition en intervalles (de type log(N) + log( \nAlgorithme d\u0027optimisation\nNous proposons un algorithme d\u0027optimisation, qui partant d\u0027une solution initiale de partitionnement bivarié aléatoire, procède en alternant les optimisations partielles par variable:\n1. initialiser une grille bivariée aléatoire, basée sur O(N ½ ) parties par variable, 2. tant que amélioration du critère, répéter:\n(a) optimiser la partition de X 1 , la partition de X 2 étant fixée, RNTI -X -(b) optimiser la partition de X 2 , la partition de X 1 étant fixée.\nDans le cas univarié, on remarque que les critères d\u0027évaluation se décomposent de façon additive en un coût de partition et des coûts par partie. Le coût de partition C (E) ne dépend que des caractéristiques globales de l\u0027ensemble d\u0027apprentissage (nombre total d\u0027individus, nombre total de valeurs des variables) et de la taille de la partition. Les coûts par partie C Prenons maintenant le cas de la discrétisation bivariée supervisée (formule 3) et montrons que le critère obtenu en fixant une des partitions se décompose de façon additive sur l\u0027autre partition. En fixant par exemple la partition de la variable X 2 , nous obtenons\nlog log log log\nLe nombre de parties I 2 étant fixé (au même titre que le nombre de valeurs endogènes J), le coût de partition C (E) ne dépend effectivement que des caractéristiques globales de l\u0027ensemble d\u0027apprentissage et de la taille de la partition I 1 . Les coûts par partie C (P) i ne dépendent que des caractéristiques locales de chaque partie. Nous pouvons alors réutiliser l\u0027algorithme de discrétisation univariée pour optimiser la partition de la variable X 1 , la partition de X 2 étant fixée. La complexité algorithmique est en O(JI 2 Nlog(N)), le facteur I 2 J s\u0027expliquant par le coût d\u0027évaluation des termes pour chaque partie.\nLes expérimentations montrent que l\u0027algorithme procédant par optimisations univariées alternées converge extrêmement rapidement, rarement en plus de deux itérations, ce qui confère à l\u0027algorithme une complexité en O(JN 3/2 log(N)). Il est à noter que le choix initial de O(N ½ ) partie par variable est un choix heuristique, visant à assurer un bon compromis entre finesse de la partition initiale et complexité algorithmique.\nExpérimentation\nCette section présente des résultats d\u0027expérimentation, permettant d\u0027évaluer l\u0027impact de la méthode de partitionnement bivarié supervisé sur les performances en classification.\nProtocole\nLes expérimentations sont menées en utilisant 30 jeux de données de l\u0027UCI (Blake et Merz, 1996)  Afin d\u0027évaluer la méthode d\u0027analyse bivariée intrinsèquement, on introduit un nouveau type de classifieur appelé BestBivariate (B2). Ce classifieur recherche d\u0027abord la meilleure paire de variable, celle qui maximise la probabilité que son modèle de partitionnement en grille explique la variable endogène. Pour classifier un individu en test, on recherche la cellule exogène associée aux valeurs de l\u0027individu pour la paire de variables, et on prédit la valeur endogène majoritaire de la cellule (d\u0027après les effectifs collectés en apprentissage). Si cette cellule était vide en apprentissage, on prédit la valeur endogène majoritaire de l\u0027ensemble d\u0027apprentissage. On évalue également le classifieur BestUnivariate (B1) qui procède de la même façon à partir de l\u0027analyse univariée, et on rappelle pour référence le taux de bonne prédiction du classifieur majoritaire (M).\nAfin d\u0027évaluer l\u0027impact de la méthode sur un classifieur multivarié, on évalue le classifieur Bayesien naïf (Langley et al, 1992), basé sur les prétraitements univariés (NB1) ou bivariés (NB2). On utilise également l\u0027amélioration de ce classifieur (SNB1 et SNB2) décrite dans (Boullé, 2006b), qui incorpore d\u0027une part une méthode régularisée de sélection de variables et d\u0027autre part une agrégation de modèles, aboutissant à une pondération des variables 1 . L\u0027exploitation de l\u0027analyse bivariée est ici rudimentaire, en considérant chaque partitionnement bivarié comme une nouvelle variable calculée enrichissant l\u0027espace de représentation.\nLe taux de bonne prédiction en test est évalué au moyen d\u0027une validation croisée stratifiée à 10 niveaux. Les différences significatives sont évaluées au seuil de 95% au moyen d\u0027un test de Student.\nRésultats\nLes résultats d\u0027évaluation sont résumés de façon synthétique dans le tableau 2, en reportant pour chaque méthode la moyenne arithmétique de son taux de bonne prédiction en test sur les 30 jeux de données de l\u0027UCI. Le nombre de différences significatives vis-à-vis du classifieur SNB2 est également présenté, ainsi que le rang moyen de chaque méthode. On constate que le classifieur basé sur une seule variable (B1) est aussi performant que le meilleur classifieur multivarié (SNB2) dans environ un quart des cas (pas de différence significative dans 7 cas sur 30), et que celui basé sur deux variables uniquement (B2) atteint la meilleure performance obtenue dans un tiers des cas (10 cas sur 30). La figure 4 analyse plus finement les apports prédictifs du meilleur classifieur univarié (B1), du meilleur classifieur bivarié (B2) et du classifieur Bayesien naïf (NB1), avec comme référence le classifieur majoritaire (M). Le classifieur bivarié est systématiquement meilleur que le classifieur univarié, ce qui confirme la capacité de la méthode d\u0027évaluation bivariée à correctement sélectionner une paire de variable performante. Il est toutefois dominé de façon significative par le classifieur Bayesien naïf, qui exploite l\u0027ensemble de toutes les variables. \nFIG. 4 -Différence du taux de bonne prédiction par rapport au classifieur majoritaire (M) pour les classifieurs BestUnivariate (B1), BestBivariate (B2) et Bayesien naïf (NB1).\nLa figure 5 analyse l\u0027apport prédictif de la prise en compte de toutes les paires de variable (NB2) et de la sélection de variables (SNB1 et SNB2), en prenant pour référence le classifieur multivarié le plus simple (NB1). L\u0027utilisation de paires de variables agrandit l\u0027espace de représentation, ce qui permet potentiellement de détecter de nouvelles informations prédicti-ves. En revanche, les informations redondantes déjà présentes en univarié sont ici multipliées, ce qui éloigne la représentation des données de l\u0027hypothèse d\u0027indépendance des variables sur laquelle repose le classifieur Bayesien naïf.\nLa figure 5 montre que ces deux phénomènes sont constatés sur les jeux de données de l\u0027expérimentation quand on prend en compte les paires dans le classifieur NB2, avec de fortes dégradations de performances sur les jeux de données 1, 2, 6, 9, 22, 26, et de fortes améliora-tions sur les jeux de données 16, 18, 20, 27. La méthode de sélection de variables (Boullé, 2006b) utilisée dans SNB1 confirme son apport systématique, mais faible par rapport au classifieur Bayesien naïf NB1. Conjuguée à l\u0027utilisation des paires de variables, le gain de performance devient à la fois important, avec une amélioration moyenne de 2.5% (15% sur Letter), et significatif, avec 14 victoires significatives pour 0 défaites. \nConclusion\nLa méthode d\u0027évaluation bivariée supervisée présentée dans cet article se base sur un modèle de partitionnement (en intervalles ou groupes de valeurs) de chaque variable exogène, ce qui induit une partition bivariée. Cette partition bivariée permet de qualifier l\u0027information apportée conjointement par les deux variables exogènes sur la variable endogène. Cette information est quantifiée au moyen d\u0027une approche Bayesienne. Le critère d\u0027évaluation obtenu est optimisé au moyen d\u0027une heuristique gloutonne en alternant les améliorations partielles par variables.\nDes évaluations intensives sur 30 jeux de données de l\u0027UCI démontrent que le critère permet de sélectionner des paires de variables fortement informatives. L\u0027ajout des paires de variables dans un classifieur Bayesien naïf n\u0027est pas concluant en moyenne, les interactions constructives entre variables étant compensées par la redondance accrue de la représentation des données. En revanche, couplée avec une méthode performante de sélection de variables, la prise en compte des paires de variables apporte une amélioration systématique des performances, significative dans la moitié des cas.\nDe travaux futurs sont envisagés pour limiter le nombre de paires à évaluer et pour adapter les méthodes de classification à une prise en compte efficace des prétraitements bivariés.\n"
  },
  {
    "id": "982",
    "text": "Introduction\nLe problème de classification automatique (clustering) est considéré comme une des problématiques majeures en extraction des connaissances à partir de données. Parmi les techniques de classification, la classification floue (fuzzy) via Fuzzy C-Means (FCM) est très connue. FCM a été introduite par Jim Bezdek en 1981(Bezdek (1981) comme une amélioration des méthodes clustering précédentes, et a été beaucoup développée dans les années 90. Cette approche a été appliquée avec succès dans plusieurs problèmes (diagnostic médical (Whitwell (2005)) , classification de textes (Rodrigues and Sacks (2004))), et est de plus en plus utilisé dans le domaine du data mining.\nDans un travail récent (Le Thi et al. 3 (2006)) nous avons formulé le modèle de FCM pour la classification floue sous la forme d\u0027un programme DC (Difference of Convexe functions) et développé un schéma de DCA (DC Algorithm) pour sa résolution numérique. La programmation DC et DCA ont été introduits par Pham Dinh Tao en 1985 et intensivement développés par Le Thi Hoai An et Pham Dinh Tao depuis 1994(voir (Le Thi Hoai An (1997) - (Le Thi et al. 2 (2006)), ), (Pham Dinh Tao and Le Thi Hoai An (1998)) et leurs références) pour devenir maintenant classiques et de plus en plus populaire. Ils ont été appliqués avec succès à nombreux problèmes d\u0027optimisation non convexe différentiable ou non de grande dimension dans différents domaines des sciences appliquées, en particulier aux problèmes du data mining (voir par exemple (Le Thi et al. 1 (2006)), (Le Thi et al. 2 (2006)), (Liu et al (2003)), (Neumann et al. (2004)), (Weber et al. (2005))). Les résultats numériques présentés dans (Le Thi et al. 3 (2006)) montrent que, comme pour les autres problèmes déjà traités en data mining, DCA est efficace pour FCM. Ils prouvent également la superiorité de DCA par rapport à K-means. Cet algorithme est itératif et consiste en la résolution d\u0027un programme convexe à chaque itération. Le temps de calculs de DCA est donc proportionnel à celui de la méthode utilisée pour résoudre les programmes convexes générés. Dans (Le Thi et al. 3 (2006)) l\u0027algorithme du gradient pro-jeté a été utilisé du fait que la projection en question est explicite, même s\u0027il est connu pour être lent. Sans doute qu\u0027avec d\u0027autres décompositions DC on peut améliorer DCA pour la résolution de FCM. L\u0027objectif de ce travail est de développer un nouveau schéma de DCA dans lequel la résolution des problèmes convexes générés est moins coûteux. Nous proposons une autre décomposition DC qui donne naissance à un DCA très simple dont les calculs sont explicites à chaque itération : le sous problème convexe est en fait la projection d\u0027un point sur une boule. Les expériences numériques comparatives entre FCM et l\u0027algorithme DCA étudié dans (Le Thi et al. 3 (2006)) sur les données réelles montrent la robustesse, la performance de cette nouvelle version de DCA et sa supériorité par rapport à FCM.\nLe papier est organisé de la façon suivante. Dans la deuxième section, nous présentons la formulation du problème FCM. La résolution de ce probème par la programmation DC et DCA est étudiée dans la troisième section. Finalement, les résultats numériques de nos algorithmes DCA et FCM sont rapportés dans la dernière section.\nUne nouvelle formulation du modèle de FCM\nSoit X :\u003d {x 1 , x 2 , ..., x n } l\u0027ensemble de n points à classer. Chaque point x i est un vecteur dans l\u0027espace IR p . Nous avons à classer ces n points dans c (2 ? c ? n) classes différentes. Considérons une matrice de pourcentage U de taille (c × n) dont chaque élément u i,k définit le pourcentage d\u0027appartenance d\u0027un point x k à la classe C i . Il est clair que\nSi la matrice de pourcentage U est déterminée, on en déduit la classification selon la règle suivante : le point x k (pour k \u003d 1, . . ., n) est classé dans la classe C i (pour i \u003d 1, . . ., c) si et seulement si u i,k \u003d max{u j,k : j ? {1, . . ., c}}.\nConsidérons la fonction J m définie par :\noù . désigne, dans tout le papier, la norme Euclidienne de l\u0027espace correspondant, V est une (c × p) -matrice dont chaque ligne v i correspond au centre de la classe C i , et m ? 1 un paramètre entier qui définit le degré de flou du modèle. Chercher une classification revient ainsi chercher la matrice de pourcentage U et les centres v i . Le modèle mathématique de FCM s\u0027écrit ainsi :\noù seule la variable U est a priori bornée. En fait on peut aussi restreindre la variable V à un domaine borné. En effet, la condition nécessaire d\u0027optimalité du premier ordre en\nConsidérons les nouvelles variables\nCe dernier est un problème d\u0027optimisation non convexe dont la résolution sera décrite dans la suite. \nLa programmation DC et DCA pour la résolution de FCM\nPour faciliter la compréhension de notre approche, nous présentons, en premier lieu de cette section, une brève description de la programmation DC et DCA.\nIntroduction à la programmation DC et DCA\nLa programmation DC joue un rôle central en programmation non convexe (différen-tiable ou non) car la quasi totalité des problèmes d\u0027optimisation de la vie courante est de nature DC. Elle connaît des développements spectaculaires au cours de cette dernière décen-nie. DCA est une méthode de descente (de type primal-dual sans recherche linéaire) pour la résolution d\u0027un programme DC de la forme\noù g, h sont les fonctions convexes semi-continues inférieurement et propres sur IR p . Une telle fonction f est appelée fonction DC, et les fonctions convexes g et h les composantes DC de f. Il est à noter que la minimisation d\u0027une fonction DC sur un ensemble convexe fermé C de IR p se ramène à un problème de type (5) car la contrainte x ? C peut être incorporée dans la fonction objectif à l\u0027aide de la fonction indicatrice ? C définie par ? C \u003d 0 si x ? C, +? sinon. Lorsqu\u0027une de ses composantes DC est polyédrale la fonction f est dite DC polyédrale et le programme DC correspondant DC polyédral. La programmation DC polyédrale joue un rôle crucial en programmation non convexe.\nLa congugaison d\u0027une fonction convexe g, notée g * est définie par\nLa dualité DC est définie via la conjugaison des composantes DC et le programme dual de (5) est donné par (ici l\u0027espace dual de IR p est identifié à lui-même) :\nPuisque chaque fonction h ? ? 0 (IR p ) est caractérisée comme le supremum d\u0027une famille finie des fonctions affines, c.à.d.\nIl est clair que (P y ) est un programme convexe et\nPar suite ? \u003d inf{h\nFinallement on obtient, avec la convention naturelle +? ? (+?) \u003d +? :\nOn observe ainsi la symétrie parfaite entre les programmes DC primal et dual : le dual de (6) est exactement (5). Le transport des solutions optimales globales entre l\u0027ensemble des solutions optimales P de (5) et celui de (6) noté D s\u0027exprime de la manière suivante ((Le Thi Hoai An and Pham Dinh Tao (2005)), )) :\nLa relation (8)  \nL\u0027égalité des valeurs optimales des programmes primal et dual (5) et (6) peut être traduite de manière équivalente par \n(Un tel point x * vérifiant (10) est appelé point critique de g ? h). La condition nécessaire d\u0027optimalité locale (9) est également suffisante dans plusieurs cas rencontrés en pratique -par exemple, quand la fonction objectif f :\u003d g ?h est DC polyé-drale avec h polyédrale, ou quand f est localement convexe en x * . Basé sur les conditions d\u0027optimalité locale et la dualité DC, DCA consiste en la construction de deux suites {x k } et {y k }, candidats respectifs aux solutions des problèmes primal et dual que l\u0027on améliore à chaque itération (les deux suites {g( \nLa première interprétation de DCA est simple : à chaque itération on remplace dans le programme DC primal la deuxième composante DC h par sa minorante affine\ndont l\u0027ensemble des solutions optimales n\u0027est autre que ?g * (y k ). De manière analogue, la deuxième composante DC g * du programme DC dual (6) est remplacée par sa minorante affine (g * ) k (y) :\u003d g * (y k ) + y ? y k , x k+1 au voisinage de y k pour donner naissance au programme convexe\ndont ?h(x k+1 ) est l\u0027ensemble des solutions optimales. DCA opère ainsi une double linéari-sation à l\u0027aide des sous-gradients de h et g * . Il est à noter que DCA travaille avec les composantes DC g et h et non pas avec la fonction f elle-même. Chaque décomposition DC de f donne naissance à un DCA. Pour un programme DC donné, la question de décomposition DC optimale reste ouverte, en pratique on cherche des décompositions DC bien adaptées à la structure spécifiques du programme DC étudié pour lesquelles les suites {x k } et {y k } sont faciles à calculer, (si possible) explicites pour que les DCA correspondants soient moins coûteux en temps et par conséquent capables de supporter de très grandes dimensions.\nLa convergence de DCA : ((Le Thi Hoai An and Pham Dinh Tao (1997)), (Le Thi Hoai An and Pham Dinh Tao (2005)), (Pham Dinh Tao and Le Thi Hoai An (1997)), (Pham Dinh Tao and Le Thi Hoai An (1998)), (Le Thi Hoai An and Pham Dinh Tao (2003)))\nSoient C (resp. D) l\u0027ensemble convexe qui contient la suite {x k } (resp. {y\nDCA est une méthodes de descente sans recherche linéaire, qui possède les propriétés suivantes : i) Les suites {g(\nDans ce cas DCA se termine à l\u0027itération k (convergence finie de DCA).\n• h\nDans ce cas DCA se termine à l\u0027itération k (convergence finie de DCA).\n(resp. {{y k+1 ? y k 2 } converge. iii) Si la valeur optimale ? du problème (5) est finie et deux suites {x k } et {y k } sont bornées alors tout valeur d\u0027adhérence x (resp. y) de la suite {x k } (resp. {y k }) est le point critique de g ? h (resp. h * ? g * ). iv) DCA a la convergence linéaire pour les programmes DC généraux. v) DCA a la convergence finie pour les programmes DC polyédraux.\nPour une étude complète de la programmation DC et DCA, se référer aux (Le Thi Hoai An (1997)) -(Le Thi Hoai An and Pham Dinh Tao (2005)), ), (Pham Dinh Tao and Le Thi Hoai An (1998) \nNouvelle formulation DC de FCM\nDans toute la suite nous utilisons la présentation matricielle qui nous semble plus comode, sachant que l\u0027on peut identifier une matrice et un vecteur (par ligne ou par colonne). La fonction objectif de (4) peut s\u0027écrire de la manière suivante :\nPour tout (T, V ) ? S × C on a\nDans le lemme suivant nous donnerons les conditions pour que la fonction H soit convexe.\nLemme : soit B :\u003d ? n k\u003d1 B k , où B k est la boule de centre 0 et de rayon 1 dans IR c . La fonction H(U, V ) est convexe sur B × C pour toute valeur de ? telle que\nH est convexe si toutes les fonctions\nConsidérons la fonction suivante :\nLe Hessien de la fonction f est donné par :\nPour tout (x, y) : 0 ? x ? 1; y ? ?, on a :\nalors | J(x, y) |? 0, pour tout (x, y) ? IR 2 tels que 0 ? x ? 1, | y |? ?. Ainsi avec ? défini par (17), la fonction f est convexe sur [0, 1] × [??, ?]. Par conséquent les fonctions\nIl en est de même pour les fonction h i,k , car\nAinsi, avec les valeurs données ci-dessus de ? et ?, la fonction H(T, V ) est convexe sur B × C. Dans toute la suite nous travaillons avec ces valeurs de ? et ?.\nIl est clair que, pour tout T ? B et un V ? C fixé, la fonction J 2m (T, V ) est concave en variable T (car H(T, V ) est convexe), par suite son minimum sur B est atteint sur la frontière S de B,i.e., min\nLe problème (4) peut être alors reformulé comme\nqui est un programme DC avec la décomposition DC suivante :\nest bien évidemment une fonction convexe grâce à la convexité de B et C.\nRésolution de (18) par DCA\nSelon la description de DCA dans la section 2.1, la résolution de FCM via la formulation (18) par DCA consiste en la détermination de deux suites (\nse ramène à la résolution du problème suivant (voir Section 2.1)\nIl s\u0027en suit que (Proj étant l\u0027application de projection)\nPlus précisément :\n3.3.1 Schéma DCA Initialisation :\nConstruction des classes\nExpériences numériques\nPour comparer la performance de notre algorithme, nous avons réalisé les tests numériques sur deux ensembles des données : le premier ensemble de données contient 4 exemples très connus et beaucoup utilisés dans le domaine de classification pour l\u0027évalua-tion des algorithmes :\n-PAPILLON : un jeu de données connu sous le nom \"jeux de papillon\". Le deuxième ensemble de données est composé de deux jeux de données de biopuces : \"Yeast\" et \"Serum\" téléchargeables sur http ://genomics.stanford.edu/. \"Yeast\" : 2945 points (gènes) dans l\u0027espace de dimension 15 ; \"Serum\" : 517 points dans l\u0027espace de dimension 12 (voir (Dembele et al. (2003)) pour la description de ces données).\nLes tests ont été réalisés sur un ordinateur de 2.8MHz, 512Mb Ram. La valeur de est fixée à 10 ?7 . La valeur de m est égale à 2 pour le premier ensemble de données. Pour les données de biopuces nous considérons différentes valeurs de m dans l\u0027intervalle (1, 2) (il a été prouvé dans (Dembele et al. (2003)) que le choix de m \u003d 2 n\u0027est pas convenable à ces données).\nDans le Tableau 1, nous comparons notre nouvelle méthode DCA (DCA2) avec l\u0027algorithme DCA développé dans (Le Thi et al. 3 (2006) Tableau 3 : Résultats comparatifs de données de biopuces \"Serum\".\nConclusion.\nNous avons introduit une nouvelle formulation DC du modèle de FCM pour la classification floue et développé un schéma de DCA pour sa résolution numérique. Avec cette décomposition DC notre algorithme itératif est extrêmement simple, il consiste en la détermination de la projection d\u0027un point sur une boule Euclidienne, qui est explicite et non coûteux. Les résultats numériques montrent que, comme pour les autres problèmes déjà traités en data mining, DCA est efficace pour FCM. Ils prouvent la superiorité de DCA non seulement par rapport à l\u0027algorithme FCM standard mais aussi par rapport au schéma de DCA proposé dans (Le Thi et al. 3 (2006)). Dans l\u0027étape suivante nous devront exploiter cet algorithme pour la classification des données biologiques, en particulier des donnéées de biopuces.\n"
  },
  {
    "id": "984",
    "text": "Introduction\nDepuis les travaux de Agrawal et al., (1993) les règles d\u0027association ont été un modèle très utilisé pour extraire des tendances implicatives dans des bases de données. Rappelons que lorsqu\u0027on dispose d\u0027un ensemble E d\u0027individus décrits par p variables {a, b, ….}, qui peuvent être des conjonctions de variables atomiques et que l\u0027on supposera ici binaires, une règle d\u0027association a ? b signifie que si a est vérifiée alors généralement b l\u0027est également.\nLorsque l\u0027on extrait un ensemble de telles règles partielles d\u0027association, il est pertinent de s\u0027interroger sur les « relations » que ces règles entretiennent entre elles. Cette question a été abordée dans la littérature selon différents points de vue. Dans une optique de structuration de l\u0027ensemble des règles, différentes méthodes de classification ont été proposées (e.g. Lent et al., 1997 ;Gras et Kuntz, 2005). Des représentations visuelles bien adaptées permettent également de mettre en évidence des dépendances entre les règles (e.g. Lehn, 2000ou Couturier et Gras, 2005. Remarquons que des travaux antérieurs (Suzuki et Kodratoff, 1999 ;Suzuki et Zytkow, 2005)  Or, il existe, comme nous le verrons en donnant des exemples, des situations naturelles où un caractère exceptionnel associe les trois variables. Pour le prendre en compte et en étudier un modèle, nous étendons ici le sens précédent en accentuant ainsi le caractère surprenant (d\u0027exception) d\u0027une règle dérivée de deux règles simples.\nPour illustrer ce type de règle, nous faisons référence tout d\u0027abord au cas de l\u0027incompatibilité de groupes sanguins en ce qui concerne le facteur Rhésus.\nCertaines femmes, non primo-parturientes, dont les globules rouges sont porteurs de deux allèles Rh-et dont l\u0027immunisation anti-Rh+ est active, possèdent alors le phénotype Rh-(caractère a). Quel que soit le père en général, l\u0027enfant qu\u0027elles portent ne présentera pas, à la naissance, de problème sur le plan sanguin (caractère c). Nous sommes en présence de la règle : a ? c.\nUn homme, de génotype Rh+ et Rh+, possède le phénotype Rh+ (caractère b). Quelle que soit la mère en général, l\u0027enfant qu\u0027il engendrera n\u0027aura pas de problème à sa naissance (caractère c). C\u0027est la situation où la règle b ? c est valide.\nEn revanche, un couple où la femme est Rh-et remplit les conditions a et l\u0027homme est Rh+ (caractère b) pourra donner naissance à un enfant qui présentera un risque important du fait de l\u0027incompatibilité Rhésus (caractère non c). Dans des cas exceptionnels, en effet, la mère s\u0027immunisant contre le facteur Rh du foetus, fabrique des anticorps, qui détruisent les globules rouges de l\u0027enfant. Même si la conjugaison des caractères a et b est rare, on rencontre cependant la réalisation de la règle, que nous avons appelée « règle d\u0027exception », (a et b) ? non c. On sait d\u0027ailleurs que des précautions sont prises pour éviter ce problème dès que sont connus les phénotypes des parents à la faveur d\u0027une prévention adaptée (par exemple l\u0027exsanguino-transfusion).\nOn trouve une situation comparable d\u0027apparition de règle d\u0027exception dans l\u0027étude des phénomènes d\u0027interférences lumineuses, par exemple dans l\u0027expérience classique des franges de Young (Bruhat G., 1959). La même source lumineuse franchissant deux fentes identiques n ? ] sera proche de 1 : autrement dit, en général, on observe plus de contreexemples dans des circonstances aléatoires que l\u0027on en a observés dans la contingence. Dans ce cas, le seul hasard conduit donc, en moyenne, à plus de contre-exemples que ce qui est observé.\nLes parties hachurées correspondent aux contreexemples observés ou  (Gras, 1979 ;Lerman, 1981a et Lerman et al. 1981. On centre et on réduit cette variable en la variable Q(a, b ) ; l\u0027observation contingente, sa réalisation, est q(a, b ). Par exemple, dans le cas de Poisson, on obtient :  La deuxième approche est basée sur l\u0027extension, que nous avons proposée, des règles en R-règles (règles de règles) de type R? R\u0027, où R et R\u0027 sont elles-mêmes des règles (Gras et Kuntz, 2005). Intuitivement, ces règles sont comparables à celles qui apparaissent en mathématiques où un théorème R a pour conséquence un autre théorème R\u0027 ou est suivi d\u0027un RNTI -X -corollaire R\u0027. Elles sont construites selon un algorithme récursif utilisant un indice appelé « cohésion ». Celui-ci rend compte de la qualité des liaisons implicatives des variables de la règle R avec les variables de la règle R\u0027.\nRappelons, qu\u0027en logique formelle, la règle de règle, ou R-règle, a ? (b ? non c), \nExemple numérique\nNous avons construit un fichier fictif de 200 sujets (cf. un tableau partiel en Annexe où nous en montrons la construction), sujets sur lesquels nous observons les variables binaires : a, b , a ? b, c et non c. Les valeurs associées des différentes intensités sont données dans TAB 1. Elles sont obtenues par le logiciel CHIC (Couturier et Gras, 2005)   Une modélisation hypergéométrique est écartée car elle n\u0027induit pas de différence entre une implication et sa réciproque (Gras et al., 1996b).\nEtablissons pour chacun de ces deux modèles retenus les intensités d\u0027implication de la conjonction a ? b sur les variables c et non c (encore notée c ). Nous utiliserons la relation simple :   \nModèle binomial\nConclusion\nLorsque deux variables impliquent une 3 ème , que leur conjonction implique plutôt la négation de cette 3 ème , nous considérons que cette règle est d\u0027exception, en un sens voisin mais différent de celui de E. Suzuki et Y.Kodratoff (1999). Nous avons étudié et illustré par un exemple numérique et un exemple de génétique, l\u0027expression de ce caractère exceptionnel. Puis nous avons précisé les relations entre les paramètres des variables dans les deux modélisations selon lesquelles est construite l\u0027Analyse Statistique Implicative : un modèle de Poisson et un modèle binomial, l\u0027un et l\u0027autre convergeant vers le même modèle gaussien.\nRNTI -X -\n"
  },
  {
    "id": "985",
    "text": "Introduction\nLa Catégorisation de Textes (C.T) consiste à assigner une ou plusieurs catégories parmi une liste prédéfinie à un document. En d\u0027autres termes, elle permet de chercher une liaison fonctionnelle entre un ensemble de textes et un ensemble de catégories (Sebastiani (2002)). La grande importance accordée cette dernière décennie au traitement des données multilingues, a donné naissance à un nouveau domaine de recherche. C\u0027est la catégorisation de textes multilingues.\nDans cet article, nous allons proposer une nouvelle approche qui consiste à étendre l\u0027utilisation de WordNet en C.T pour catégoriser des documents provenant de différentes langues. L\u0027approche proposée est basée sur la traduction des documents à catégoriser vers la langue de Shakespeare afin de pouvoir bénéficier de l\u0027utilisation de WordNet par la suite. Cette hybridation entre l\u0027utilisation des techniques de traduction et l\u0027utilisation de WordNet offre les avantages suivants: -Sans l\u0027utilisation des techniques de traduction, il devient nécessaire de construire une ontologie WordNet pour chaque langue. Cette construction est très coûteuse en terme de temps et personnels.\n-L\u0027utilisation d\u0027une ontologie bien construite et riche tel que WordNet permet de corriger certains erreurs de traduction en utilisant des relations tel que l\u0027hypéronymie et la synonymie (Cruse (1986)).\nL\u0027approche que nous proposons dans cet article se compose de deux phases. La première phase est la phase d\u0027apprentissage, elle consiste à:\n-Utiliser WordNet pour mapper les mots en synsets; -Enrichir l\u0027espace de représentation par l\u0027extraction des hypéronymes ; -Utiliser la méthode ? 2 multivariée (Clech et al. (2003)) pour sélectionner les synsets caractérisant chaque catégories par rapport aux autres catégories. Les synsets sélectionnés pour chaque catégorie forment son profil conceptuel. La deuxième phase est celle de la classification, elle consiste à assigner le texte à catégo-riser à une ou plusieurs catégories en se basant sur les profils conceptuels déjà trouvés dans la première phase. Cela nécessite les étapes suivantes:\n-Traduire le texte à catégoriser vers la langue anglaise afin de pouvoir utiliser WordNet pour créer le vecteur conceptuel ; -Pondérer les profils conceptuels des catégories ainsi que le vecteur conceptuel du texte à catégoriser ; -Calculer la distance entre le vecteur conceptuel du texte à catégoriser avec les profils conceptuels des catégories ;\nExpérimentations\nAfin de pouvoir montrer l\u0027utilité de l\u0027utilisation de WordNet en catégorisation multilingue, nous avons testé l\u0027approche proposée sur le corpus monolingue Reuters21578 1 , ainsi que sur le corpus bilingue ILO. Les résultats des expérimentations ont montré que les performances obtenues sur les deux corpus sont très proches. Ce rapprochement dans les résulats nous mènent à confirmer que l\u0027utilisation de WordNet en catégorisation multilingue est une piste prometteuse.\n"
  },
  {
    "id": "986",
    "text": "Introduction\nOn veut montrer brièvement les divers degrés d\u0027exigence (vis-à-vis des résultats) que l\u0027on peut avoir lorsque l\u0027on procède à une analyse en axes principaux. Ces degrés correspondent à des modalités d\u0027usage du bootstrap (Diaconis et Efron, 1983;Efron et Tibshirani, 1993). On examinera successivement le bootstrap partiel (section 2), trois types de bootstrap dit total (section 3), d\u0027autres formes plus spécifiques de bootstrap (section 4). On revient ensuite sur les subtilités du bootstrap total de type 3 (section 5). On illustrera ces propos par une étape de travail extraite d\u0027une analyse en composante principales (ACP).\nBootstrap partiel\nLes axes principaux calculés à partir des données originales, non perturbées, jouent un rôle privilégié (en ACP, par exemple, la matrice des corrélations initiale C est en effet l\u0027espérance mathématique des matrices C k « perturbées » par la réplication k). Pourquoi calculer des sous-espaces de représentation prenant en compte des perturbations, et donc moins exacts que le sous-espace calculé sur les données initiales? La variabilité bootstrap s\u0027observe mieux sur le repère fixe initial, non perturbé. C\u0027est l\u0027option qui sera prise dans la suite de cet article. La technique de bootstrap que l\u0027on appellera bootstrap partiel (sans recalcul des valeurs propres) proposée notamment par Greenacre (1984) dans le cadre de l\u0027analyse des correspondances, répond à plusieurs des préoccupations des utilisateurs dans le cas de l\u0027analyse en composantes principales 1 . Une réplication consiste en un tirage avec remise des n individus (vecteurs-observations), suivi du positionnement des p nouvelles variables ainsi obtenues en \"variables supplémentaires\" sur les q premiers axes de l\u0027analyse de base. Les procédures décrites ci-dessus peuvent être mises en oeuvre avec un programme classique de projection d\u0027éléments supplémentaires. On calcule donc les réplications de ce coefficient, ce qui revient à repondérer les individus avec les \"poids Bootstrap\" 0, 1, 2, ... qui caractérisent un tirage sans remise. On obtient, comme sous-produit, des réplications de la variance sur l\u0027axe, qui sont évidemment distinctes de ce que seraient des réplications des valeurs propres.\nLes s réplications étant projetées sur un repère commun (celui de l\u0027analyse initiale), on caractérisera graphiquement la dispersion des réplications d\u0027une variable donnée soit par l\u0027enveloppe convexe de l\u0027ensemble de ses réplications, soit par un ellipsoïde d\u0027ajustement du nuage des réplications, qui résultera en fait d\u0027une petite analyse en composantes principales de ce dernier nuage. L\u0027enveloppe convexe a l\u0027avantage de l\u0027exhaustivité (toutes les réplications sans exception sont enveloppées), l\u0027ellipsoïde a l\u0027avantage de prendre en compte la densité du nuage des réplications, et d\u0027être moins sensible à d\u0027éventuelles rares réplications aberrantes.\nBootstrap total\nLe bootstrap total consiste à réaliser autant d\u0027analyses en axes principaux qu\u0027il y a de réplications. Mais le système d\u0027axes n\u0027est plus le même d\u0027une analyse à une autre. Il peut y avoir des changements de signes (les axes factoriels sont définis aux signes près), des interversions d\u0027axes, des rotations d\u0027axes 2 . Il faut donc procéder à une série de transformations afin de retrouver des axes homologues au cours des diagonalisations successives des s matrices de corrélation répliquées C k (C k correspond à la k-ème réplication).\nBootstrap total de type 1\nC\u0027est une épreuve sévère, très pessimiste : simple changement (éventuel) de signes des axes homologues pour les réplications. Il s\u0027agit seulement de remédier au fait que les axes sont définis au signe près. Un simple produit scalaire entre axes originaux et axes répliqués de mêmes rangs permet de rectifier le signe de ces derniers. Le bootstrap total de type 1 ignore les possibles interversions d\u0027axes et rotations d\u0027axes. Il permet de valider des structures stables et robustes. Chaque réplication doit produire les axes initiaux avec les mêmes rangs (ordre des valeurs propres).\nBootstrap total de type 2\nC\u0027est une épreuve assez sévère, plutôt pessimiste : il y a changements de signes et éventuellement correction des interversions d\u0027axes. Les axes répliqués sont affectés (séquentiellement, sans remise en cause d\u0027affectations antérieures) du rang des axes originaux avec lesquels ils sont les plus corrélés en valeur absolue. Puis on procède à un éventuel changement de signe des axes, comme en bootstrap de type 1. Le bootstrap total de type 2 est idéal si on veut valider des axes, c\u0027est-à-dire des dimensions cachées, sans attacher une importance particulière aux rangs de celles-ci.\nBootstrap total de type 3\n3 C\u0027est une épreuve plutôt laxiste si on s\u0027intéresse à la stabilité des axes, mais apte à décrire la stabilité des sous-espaces de dimension supérieure à 1: une rotation dite procrustéenne (cf. Gower et Dijksterhuis, 2004) permet de rapprocher de façon optimale les systèmes d\u0027axes répliqués et les systèmes d\u0027axes initiaux. Le bootstrap de type 3 permet de valider globalement un sous-espace engendré par les axes principaux correspondant aux premières valeurs propres. Comme le bootstrap partiel, le bootstrap total de type 3 peut être qualifié de laxiste par les utilisateurs qui s\u0027intéressent à l\u0027individualité des axes, et pas seulement aux sous-espaces engendrés par plusieurs axes consécutifs (cf. section 5).\nAutres types de bootstrap\nOn va mentionner trois techniques plus spécifiques. Comme les techniques précitées, les méthodes évoquées dans cette section sont implémentées dans le logiciel DTM qui peut être librement téléchargé à partir du site www.lebart.org.\nLe bootstrap sur variables\nCette procédure n\u0027a de sens que si il existe un « univers des variables » pour lequel la notion de « tirage de variables » a un sens. Les variables sont par exemple des événements nombreux, des instants, des zones échantillonnées, où, comme dans le cas de l\u0027exemple de la sémiométrie (Lebart et al., 2003), des mots. Pour tester la stabilité des structures vis-à-vis de l\u0027ensemble des variables, nous proposons de répliquer l\u0027ensemble des variables lui-même par la méthode du bootstrap total. Nous supposons ainsi implicitement que l\u0027ensemble des variables actives constitue un échantillon de m variables extrait aléatoirement d\u0027un ensemble de variables potentielles. Nous perturbons cet échantillon de variables selon les mêmes principes que le bootstrap sur individus. Pour chaque réplication, les variables non tirées participent à l\u0027analyse de l\u0027échantillon répliqué avec un poids infinitésimal (variables supplémentaires) ce qui permet d\u0027obtenir des nuages de réplications pour chaque variable (et pour chaque observation) (exemples dans l\u0027ouvrage cité).\nLe bootstrap spécifique (ou hiérarchique)\nLe bootstrap spécifique intervient notamment dans les analyses de données textuelles, dans le cas des questions ouvertes, par exemple, pour lesquelles il existe deux niveaux d\u0027individus statistiques : les mots, qui sont les individus des tables de contingences lexicales, et les répondants, qui sont les individus statistiques classiques des enquêtes. Le tirage avec remise des occurrences de mots peut être remplacé par un tirage avec remise des répondants qui sont en fait des « grappes de mots ». Les données ainsi répliquées peuvent donner lieu aux bootstraps partiels ou totaux. Elles conduisent en général à des zones de confiances plus larges si les réponses sont de tailles très différentes, circonstance fréquente en pratique.\nLe bootstrap partiel pour cartes auto-organisées\nLe bootstrap partiel peut être appliqué à d\u0027autres opérateurs projections que ceux des variables supplémentaires usuelles. Ainsi, l\u0027analyse de contiguïté permet de trouver des sousespaces de projection les plus proches possibles d\u0027une carte auto-organisée de Kohonen (respectant la topologie de la carte au sens de la variance locale), ce qui permet de représenter par des zones de confiance l\u0027incertitude sur la position des points. La projection des points répliqués ne se fait plus sur un plan factoriel, mais un plan sur lequel se projette la carte auto-organisée de façon optimale (ce plan est optimal au sens de la variance locale, qui est une variance calculée seulement à l\u0027intérieur des classes et entre classes contiguës sur la carte). On trouvera détails et exemples d\u0027application dans Lebart (2006).\nPrécisions sur la validation procrustéenne\nRevenons maintenant sur le bootstrap total de type 3 qui implique la forme la plus élaborée de traitement statistique des réplications. On doit dans ce cas fixer un paramètre t (t comme tolérance), qui est le nombre d\u0027axes pour lesquels on s\u0027autorise à effectuer une transformation procrustéenne. Si par exemple le sous-espace des dix premiers axes répliqués coïncide avec celui des dix premiers axes initiaux, on pourra trouver une rotation qui fera coïncider les axes (ce qui nous ramène dans cas au bootstrap partiel).\nNotons X (p,q) le tableau des q coordonnées factorielles initiales pour chacune des p variables, et X k sa k-ième réplication. Si les lignes de X k , d\u0027ordre (p,q), subissent toutes un une même rotation, X k est transformé en X k B où B (q, q) est une matrice orthogonale (rotation ou symétrie par rapport à l\u0027origine). On cherchera à rendre minimale la somme des carrés S des écarts entre X et X k B, qui peut s\u0027écrire : S \u003d trace (X -X k B )\u0027 (X -X k B) L\u0027analyse procrustéenne orthogonale implique alors, pour chaque réplication X k la décomposition aux valeurs singulières de X\u0027X k et donc la diagonalisation de la matrice X\u0027 X k X k \u0027 X.\nSi l\u0027on s\u0027intéresse à la stabilité du sous-espace de dimension q\u0027 \u003c q formé par les q\u0027 premières colonnes de X, on peut ne garder que les q\u0027 premières colonnes de X k , et chercher une matrice B(q\u0027,q\u0027) (test sévère). Mais on peut aussi tolérer que certains des q\u0027 premiers axes aillent s\u0027égarer dans un sous espace plus grand, et donc garder t colonnes, avec q\u0027\u003c t \u003cq.\nLes figures 1 et 2 sont relatives aux données sémiométriques disponibles sur http://ses.enst.fr/lebart/ (rubrique : logiciel, exemple EXA08 de DTM). \nFIG. 1 -Ellipses de confiance de trois mots (données sémiométriques) dans le plan (1, 2), ajustant 30 réplications. Les plus grandes ellipses correspondent à des rotations procustéennes pour t \u003d 3 axes, les ellipses internes pour t \u003d 12 axes.\n70 mots sont notés par 300 individus. La figure 1 montre ainsi la différence d\u0027ampleur des zones de confiance pour trois mots (Désir, Livre, Perfection). Pour t \u003d 12, les ellipses sont plus petites que pour t \u003d 3. Autrement dit, on gagne en précision dans le plan (1, 2) si l\u0027on se contente de la stabilité d\u0027un espace à 12 dimensions (sur 70 au départ). Si l\u0027on exige un espace à 3 dimensions, on doit se contenter d\u0027une précision moindre. \nFIG. 2 -Evolution de la somme des variances des réplications (liée à la taille moyenne des ellipses de confiance) dans le plan (1, 2) en fonction de la dimension t du sous-espace dans lequel sont effectuées les rotations procrustéennes des réplications.\nLa figure 2 fait un bilan global de la variance totale interne des ellipses pour l\u0027ensemble des 70 mots. L\u0027augmentation de la précision avec t est confirmée, mais reste modérée.\n"
  },
  {
    "id": "987",
    "text": "Introduction\nDans cet article, nous proposons une technique de clustering dynamique de données évo-lutives. Cette problématique est née de l\u0027objectif initial de nos travaux visant à permettre, au cours de l\u0027exécution d\u0027un système multi-agents, de détecter des groupes d\u0027agents liés à des phénomènes d\u0027auto-organisation. On se trouve donc face à un problème de clustering dynamique qui présente les deux particularités suivantes : le cardinal de l\u0027ensemble de données à clusteriser n\u0027est pas constant et des données déjà clusterisées peuvent être modifiées du fait de l\u0027évolution des agents correspondants.\nCela peut entraîner des modifications ou des réorganisations de l\u0027ensemble existant de clusters. Ainsi, une méthode de clustering dynamique est nécessaire afin d\u0027adapter continuellement l\u0027ensemble des clusters afin qu\u0027ils reflètent le mieux possible l\u0027état courant des données.\nTravaux connexes\nIl existe de nombreux travaux portant sur les techniques de clustering où l\u0027ensemble des données à clusteriser n\u0027est pas totalement connu dès le départ comme en clustering classique. On trouve en particulier dans cette catégorie les techniques de clustering de flux de données et de flux de données évolutifs. Malheureusement, ces algorithmes ne prennent pas en compte le fait que des données déjà clusterisées puissent elles aussi évoluer.\nLes travaux les plus proches de notre problématique concernent un algorithme de clustering de données mobiles présenté dans [Li et al. (2004)] : un micro-clustering est effectué en enrichissant les données d\u0027un vecteur vitesse. Cependant, dans un deuxième temps, l\u0027algorithme k-means doit être utilisé pour regrouper les micro-clusters, ce qui oblige à donner un nombre de clusters attendu et à ce que ce nombre soit constant.\nNotre approche\nLes algorithmes fourmis de clustering semblent plus adaptés à la prise en compte de l\u0027évo-lution des données. Ainsi, l\u0027algorithme AntClass [Monmarché (2000)] associe successivement en quatre phases un algorithme de fourragement et l\u0027algorithme k-means. Cette approche n\u0027étant pas compatible avec l\u0027aspect dynamique de notre problématique, nous avons décidé de ne garder que l\u0027algorithme fourmi utilisé dans AntClass et d\u0027associer une couche d\u0027agents cluster aux fourmis, chaque tas créé par les fourmis étant encapsulé dans un agent cluster.\nUn agent cluster évolue au fur et à mesure que les fourmis lui donnent ou lui prennent des données et que les données qu\u0027il contient évoluent. Mais il peut également se développer en attaquant d\u0027autres agents cluster avec lesquels il est en intersection. Les clusters sont alors mis à jour par fusion complète ou partielle lors de la fuite de l\u0027agent agressé. Ceci permet de compenser la tendance des fourmis à construire trop de tas et de prendre en compte l\u0027évolution des données.\nPrototypes et expérimentations\nNous avons développé une plateforme de simulation qui, connectée à la plateforme de clustering dynamique, sert de support à nos expérimentations. Les deux plateformes ont été développées en JAVA en utilisant l\u0027environnement de développement de SMA MADKIT. La plateforme de simulation permet de simuler l\u0027évolution de populations d\u0027agents. Une population est considérée comme étant un ensemble d\u0027agents qui vont, a priori, évoluer de manière similaire. Une population doit donc être normalement perçue, au niveau du clustering, comme un cluster qui va évoluer au cours du temps.\nL\u0027analyse de nos résultats repose pour le moment sur trois types de courbes temporelles : nombre de données agrégées, nombre de clusters et pureté moyenne des clusters. De cette analyse, il ressort que les clusters détectés sont en général purs, mais un peu trop nombreux, et que quelques données restent non clusterisées. L\u0027algorithme fonctionne également bien sur des données bruitées.\nConclusion et perspectives\nDes résultats prometteurs nous incitent à poursuivre dans cette voie en améliorant un certain nombre de points, notamment pour mieux prendre en compte la dissociation de clusters. Nous comptons notamment prendre en compte la vitesse des données comme cela est fait dans [Li et al. (2004)]. Enfin, d\u0027un point de vue implantation, nous travaillons actuellement sur une nouvelle version des plateformes permettant un meilleur contrôle sur l\u0027activation des différents agents. À plus long terme, notre objectif est d\u0027appliquer cette méthode à différents problèmes concernant notamment l\u0027émergence dans les systèmes multi-agents. \nSummary\nIn this paper, a multi-agents algorithm for dynamic clustering is presented.\n"
  },
  {
    "id": "989",
    "text": "Introduction\nB-Ontology est un projet de recherche appliquée dont l\u0027objectif est de construire le prototype d\u0027une application capable d\u0027extraire et d\u0027organiser de l\u0027information biographique. Cette information sera exploitée dans le cadre du processus de rédaction d\u0027une agence de presse. L\u0027agence Belga diffuse quotidiennement plus de 250 dépêches en deux langues (français et néerlandais). Cette masse textuelle représente environ 70.000 mots par jour (25 millions de mots en un an) par langue. Dans ce projet, nous nous intéresserons aux informations qui concernent les personnes, les organisations et les événements dans lesquels elles interviennent. Le résultat est stocké dans un ensemble de données structurées facilement consultable. Des systèmes comparables existent déjà (NewsExplorer 1 , KIM 2 ) mais ne couvrent cependant pas toutes les fonctionnalités désirées ici et sont souvent uniquement adaptés aux textes en anglais.\nLa première partie exposera les méthodes d\u0027extraction d\u0027information. La deuxième s\u0027attardera sur le choix de l\u0027organisation des données. Une troisième partie, présentera une réalisation concrète, mais limitée, de la base de connaissances et quelques aspects de data mining.\nExtraction d\u0027information\nDéfinitions des entités et du formalisme d\u0027annotation\nL\u0027extraction d\u0027information passe par l\u0027annotation sémantique du texte. Cette tâche néces-site avant tout une bonne définition des types d\u0027entités recherchées. On définit le concept d\u0027entité de manière assez large, dépassant ainsi la conception habituelle de l\u0027entité nommée présentée par Chinchor (1998). Dans le cas d\u0027une personne ou d\u0027une organisation, elle peut être constituée uniquement du nom (l\u0027entité nommée au sens strict), mais peut également être accompagnée d\u0027un ensemble d\u0027informations complémentaires. Celles-ci, que l\u0027on défi-nit comme des entités associées, sont souvent accolées à l\u0027entité nommée proprement dite. En les regroupant, on obtient une entité complexe. Une nomencalture contenant les principaux types d\u0027entités a été mise au point (voir tableau 1). Tous les codes d\u0027entité (tableau 2) peuvent se retrouver imbriqués dans n\u0027importe quel autre code. Certaines combinaisons ne seront cependant que peu ou jamais rencontrées en raison de la sémantique trop éloignée des codes en question. Le nombre de niveaux d\u0027imbrication n\u0027est pas limité. À un niveau donné, plusieurs éléments de code identique peuvent coexister.\nExtraction par grammaires locales et transducteurs\nL\u0027approche privilégiée consiste en l\u0027utilisation de méthodes linguistiques qui offrent des possibilités d\u0027analyses très fines. Les principaux types de ressource d\u0027extraction sont les transducteurs à états finis (ou graphes) et les dictionnaires électroniques. Les transducteurs constituent un formalisme simple permettant une description très précise des entités à extraire. Pour des raisons de modularité et de facilité dans la construction des graphes, l\u0027extraction s\u0027effectue en plusieurs phases successives (cascade). Le processus mis en oeuvre est décrit de manière plus approfondie dans Kevers (2006). L\u0027objectif est de repérer et de catégoriser des entités de plus en plus complexes, telles que celle illustrée ci-dessous :  Enfin, nous désirons disposer d\u0027une structure sur laquelle il est possible de réaliser des inférences. La masse de données pourra ainsi être enrichie grâce à la mise en commun des informations extraites et des connaissances de raisonnement incluses au système.\nOntologie et base de connaissances\nUn choix : l\u0027ontologie\nL\u0027ontologie, concept bien connu dans le domaine du web sémantique, correspond aux exigences exprimées ci-dessus. Elle est définie par Gruber (1993) comme an explicit and formal spécification of a conceptualization. La construction d\u0027une ontologie revient donc à exprimer de manière formelle la perception que l\u0027on a d\u0027un domaine. Cette formalisation (ici de l\u0027information biographique) sert ensuite de modèle pour le stockage de données réelles.\nDans une ontologie, les concepts s\u0027organisent en classes et disposent de propriétés. Cellesci se rapportent à une autre classe ou à un type de données particulier. Les classes peuvent être organisées de manière hiérarchique et diverses restrictions peuvent être exprimées. Les données réelles qui actualisent les classes sont apellées des instances. L\u0027ensemble des données qui remplissent l\u0027ontologie constitue une base de connaissances. En ce qui concerne les capacités de raisonnement, les raisonneurs OWL (ou DL-reasoner) et les langages de règles constituent les principales possibilités. Il est important de souligner qu\u0027en la matière, il n\u0027existe pas réellement de solution standard. Beaucoup de langages et d\u0027implémentations cohabitent, sans qu\u0027aucun ne s\u0027impose pour l\u0027instant. Ce contexte induit un certain flou quant au choix de la technologie adéquate.\nChoix technologiques pour l\u0027implémentation\nLes raisonneurs OWL disposent de capacités d\u0027inférence qui sont essentiellement utilisées pour vérifier l\u0027intégrité d\u0027une ontologie ou d\u0027une base de connaissances, ainsi que pour déter-miner si une classe est une sous-classe d\u0027une autre classe (ce qui permet d\u0027établir la hiérarchie des classes). Ils se basent sur la logique de description qui est un sous ensemble de la logique des prédicats. Citons entre autres Racer 5 , Pellet 6 , Hoolet 7 ou encore FaCT++ 8 . L\u0027utilisation d\u0027un langage de règles (aussi appelé règles de Horn) permet quant à elle d\u0027effectuer des inférences plus complexes. SWRL (Horrocks et al., 2004) semble être la solution émergeante. Il s\u0027agit d\u0027un sous ensemble de la logique des prédicats, mais disjoint de la logique de description.\nEnfin, l\u0027implémentation de clients ou de modules aditionnels pour protégé peut se faire à l\u0027aide du langage Java. Cela permet de décliner les clients sous diverses formes : en interface graphique «classique», en application web 9 ou encore en service web 10 .\nStructure partielle de l\u0027ontologie biographique\nUn premier prototype partiel sert de base au système complet. La définition de l\u0027ontologie commence par la spécification des classes ( figure 1, cadre A)  figure 1 (cadre B).\nFIG. 1 -Liste des entités, définition de «Organization» et conditions pour «Employment».\nLes conditions associées aux classes sont des conditions nécessaires (si une instance appartient à une classe, elle doit vérifier ces conditions) ou des conditions nécessaires et suffisantes (si une instance est membre d\u0027une classe, elle doit remplir ces conditions ; si elle remplit les conditions elle est membre de la classe). Elles font appel à la restriction existentielle (? ; hasProperty some Class décrit les instances qui ont au moins une relation hasProperty avec une instance de Class) ou à la restriction universelle (? ; hasProperty only Class décrit les instances dont toutes les relations hasProperty s\u0027appliquent uniquement à des instances de Class).\nPour Employment (figure 1, cadre C), il est spécifié que a) toute instance de Employment est aussi une instance de Event ; b) si une instance de Employment a une relation hasContext, elle porte nécessairement sur une instance des classes Organization ou Person ; c) si une instance de Employment a une relation hasFunction, elle porte nécessairement sur une instance de la classe Function ; d) une instance de Employment possède au moins une relation hasFunction ou une relation hasContext ; e) si une instance de Employment a une relation isEmploymentOf, elle porte nécessairement sur une instance de la classe Person ; f) pour qu\u0027une instance soit membre de la classe Employment, il suffit qu\u0027elle soit reliée à une (ou plusieurs) instance(s) de Person (et uniquement à cette classe) par la relation isEmploymentOf ET qu\u0027elle possède soit au moins une relation hasFunction soit au moins une relation hasContext.  ( ?a, ?b) ? isSiblingOf( ?x, ?y). Cette règle crée une proriété isSiblingOf chez x (vers y) et y (vers x) si les conditions sont vérifiées.\n"
  },
  {
    "id": "990",
    "text": "Introduction\nMotivée par de nombreux domaines d\u0027applications (e.g. marketing web, analyses financières, détections d\u0027anomalies dans les réseaux, traitements de données médicales), l\u0027extraction de motifs séquentiels fréquents est un domaine de recherche très actif Mobasher et al. (2002); Ramirez et al. (2000); Lattner et al. (2005). Les travaux menés ces dernières années ont montré que toutes les approches qui visent à extraire l\u0027ensemble des motifs séquentiels deviennent cependant inefficaces dès que le support minimal spécifié par l\u0027utilisateur est trop bas ou lorsque les données sont fortement corrélées. En effet, dans ce cas, et plus encore que pour les itemsets, les recherches sont pénalisées par un espace de recherche trop important. Par exemple, avec i attributs (appelés aussi items), il y a potentiellement O(i k ) séquences fréquentes de taille k Zaki (2001). Pour essayer de gérer au mieux ces problèmes de complexités spatiale et temporelle, deux grandes tendances se distinguent à l\u0027heure actuelle. Dans le premier cas, les propositions comme PrefixSPAN Pei et al. (2004) ou SPADE Zaki (2001) se basent sur de nouvelles structures de données et une génération de candidats efficace. Les approches de la seconde tendance considèrent l\u0027extraction d\u0027une représentation condensée Mannila et Toivonen (1996). Même si l\u0027utilisation d\u0027une représentation compacte a montré son intérêt dans le domaine de l\u0027extraction d\u0027itemsets, la complexité structurelle des motifs séquentiels fait qu\u0027il existe cependant peu de travaux utilisant une représentation condensée dans ce contexte.\nAinsi, seuls Clospan Yan et al. (2003) et Bide Wang et Han (2004) ont abordé ce problème en cherchant à extraire des motifs clos. Le problème que nous cherchons à résoudre dans cet article est le suivant : Est-il possible de trouver une nouvelle représentation condensée pour répondre à la problématique de l\u0027extraction de motifs séquentiels ? Notre objectif est d\u0027établir les premières bases formelles pour calculer les bornes supérieures et inférieures de la valeur du support d\u0027un motif en utilisant le principe de l\u0027inclusion-exclusion Knuth (1973). Ce principe nous permet d\u0027obtenir des règles de dérivations via lesquelles nous pouvons déduire le support d\u0027une séquence sans avoir à compter son support dans la base de données. Nous montrons également que ces règles peuvent être utilisées pour construire une représentation condensée de certains types de motifs. Cet article est organisé de la manière suivante. La section 2 introduit les concepts liés aux motifs séquentiels ainsi que les notions formelles utilisées dans le reste de l\u0027article. Nous discutons l\u0027utilisation de règles de déductions dans la section 3. L\u0027approche NDSP est introduite dans la section 4. La section 5 présente les premières expérimentations menées qui confirment l\u0027inté-rêt de notre approche et en discute les limites. Dans la section 6 nous présentons les travaux connexes autour des représentations condensées et des motifs séquentiels. Enfin, la section 7 conclut et présente les principales perspectives associées à ce travail.\nConcepts préliminaires\nDans cette section, nous définissons le problème d\u0027extraction des motifs séquentiels initialement proposé par Srikant (1995); Srikant et Agrawal (1996) et nous introduisons la notion de S-Apparition (une notion similaire est proposée dans Calders et Goethals (2002)     Par manque de place, les preuves de ce lemme, des prochaines propositions et des théorèmes ne sont pas détaillés dans cet article. Le lecteur peut se référer à Raissi et Poncelet (2006). Le lemme 1 sera utilisé dans la prochaine section afin de générer les règles de déductions.\nPour toutes les autres séquences\nRègles de déductions\nDans cette section, nous étendons la définition d\u0027expression de support introduite dans Calders et Goethals (2002)    Calders et Goethals (2002)\nNous montrons maintenant comment générer l\u0027intervalle minimal pour chaque séquence candidate en utilisant l\u0027ensemble des expressions de support, c\u0027est à dire utiliser toute l\u0027information disponible dans l\u0027ensemble des expressions de support. Pour cela, nous présentons les liens théoriques entre l\u0027implication logique définie ci-dessus et un système d\u0027équations linéaires. \nEn utilisant ce type d\u0027inégalités, nous pouvons définir un théorème permettant de déduire directement des règles sur les bornes de l\u0027intervalle de support des séquences candidates S. Ceci est possible car les algorithmes construits niveaux par niveaux contiennent l\u0027information Support(J) \u003d s J pour chaque sous-séquence de S.\nL\u0027utilisation de la séquence théorique maximale SP n\u0027est pas possible d\u0027un point de vue pratique. Pour cela, nous limitons la portée de notre lemme à la séquence S uniquement en utilisant un principe de projection de séquence sur la base de données D. Contrairement au problème d\u0027extraction d\u0027itemsets, cette projection modifie le support des sous-séquences de S puisque certaines séquences peuvent être incomparables à S tout en ayant des sous-séquences en commun. -I(? S (C trans )) ? I(S) (élimination de tous les items de C trans qui ne sont pas dans S).\nun ensemble de transactions d\u0027un client est incomparable avec la séquence projetée, il est inchangé). La projection de la base de données\nLemme 3 Soit D une base de données, pour chaque sous-séquence\nD\u0027après le lemme 2, il existe une variable x S pour chaque sous-séquence S SP. Le lemme 3 permet de réduire considérablement le système d\u0027équations Sys(S) associé à l\u0027ensemble des expressions de support S. Donc, avec une projection sur la séquence S, nous pouvons restreindre les variables x X à uniquement celles dont X S. \nPour résoudre Sys(S) nous séparons les coefficients en une matrice booléenne :\nSi nous représentons chaque matrice par un symbole, Ax \u003d S avec A une matrice de contraintes de taille n × n (n étant le cardinal de l\u0027ensemble contenant S et toutes ses sousséquences), x un vecteur colonne avec n entrées et S un vecteur colonne avec n entrées, la solution générale en utilisant la méthode d\u0027élimination de Gauss-Jordan (A ?1 .S) est :\nX où ? J X est la valeur absolue de l\u0027élément xj de la matrice inverse A ?1 . Soit s S un entier choisi arbitrairement, d\u0027après le lemme 4 l\u0027ensemble des supports d\u0027expression S pour la séquence S est satisfiable si et seulement si il existe une solution entière au système d\u0027équations Sys(S). Donc d\u0027après (1), Sys(S) est satisfiable si et seulement si :\nX (Notons que (2) est similaire à la formule d\u0027inclusion-exclusion de Knuth (1973)). En isolant (?1) |S?X| ? S X .s S et s s de la somme nous avons :\nComme ?X J ? S, nous obtenons :\nX Les règles déduites de (3) sont les bornes supérieures de la valeur du support de la séquence S et les règles déduites de (4) sont les bornes inférieures. Ces règles seront notées R X (S) comme dans Calders et Goethals (2002) et la borne, notée ? X (S) est définie telle que :\nNous pouvons alors, pour chaque séquence S, déduire des règles de chaque sous-séquence X S. Ces règles peuvent être utilisées pour définir un intervalle sur la valeur du support de S avec U S (respectivement L S ) la valeur minimale des bornes supérieures (respectivement la valeur maximale des bornes inférieures) et donc L s ? Support(S) ? U s .\nExemple 2 Considérons la base de données D suivante :\nRemarquons pour cette séquence que nous pouvons directement inférer le support sans avoir à le compter. ( est égal à 2 (2 ? Support( ? 2).\nCorollaire 1 Soit X S, la différence entre la borne ? \nL\u0027extraction des motifs séquentiels est fortement limitée par son aspect combinatoire. Afin de résoudre ce problème, il est souvent nécessaire et plus efficace d\u0027extraire un sous-ensemble de motifs contenant ou pouvant permettre l\u0027extraction des mêmes informations que l\u0027ensemble des motifs séquentiels. Les règles de déductions peuvent être utilisées afin de construire une nouvelle représentation condensée des motifs séquentiels. Si les règles permettent de déri-ver exactement la valeur du support (i.e. U S \u003d L S ) d\u0027une séquence S en utilisant ces sousséquences (cf. exemple 2 avec la séquence alors il n\u0027est pas nécessaire de garder S. Dans ce cas, S est appelée une séquence dérivable, notée DS. De la même façon, les séquences non-dérivables, notées N DS, sont les séquences qui ne peuvent pas avoir leur support dérivé de manière exacte. Nous allons montrer que l\u0027ensemble des N DS permet la construction de tout l\u0027ensemble des motifs séquentiels.\nThéorème 2 Soit S une séquence et soit ? un item, l\u0027intervalle calculé par les règles de dé-rivations pour la valeur du support de la séquence\n(respectivement 2? S?? X ) plus petit que l\u0027intervalle calculé pour la valeur du support de la séquence S.\nCorollaire 2 Monotonie\nSoit X S une séquence. Si X est une DS, alors S est aussi une DS.\nPreuve. Si X est une DS alors U X ? L X \u003d 0. En utilisant le théorème 2 nous savons que :\nL\u0027algorithme NDSP\nDans cette section nous présentons notre approche afin de construire une représentation condensée des motifs séquentiels à partir des règles de déductions extraite du théorème 1 et du corollaire 2. L\u0027avantage d\u0027une représentation condensée est qu\u0027elle est souvent plus petite que l\u0027ensemble des motifs séquentiels, noté F , ce qui rend cette représentation adéquate dans le cadre d\u0027extractions de motifs à partir de données fortement corrélées ou très denses. Les motifs séquentiels non-dérivables sont donc adéquats pour l\u0027extraction de grands ensembles de motifs séquentiels qui ne pourraient pas être obtenus à l\u0027aide d\u0027algorithmes classiques. \nPreuve. Extension de Calders (2003). La preuve est construite par induction sur la taille de la séquence S (Raissi et Poncelet (2006)).\nDans notre approche, la valeur ? S X utilisée afin de calculer nos règles de déductions n\u0027est pas extraite d\u0027une matrice inversée, afin d\u0027optimiser les calculs, mais calculée par la fonction :\nOù ? est le nombre de sous-séquences de taille |S ? 1| tel que X S. Pour le cas spécial où J \u003d S, Si X \u003d {} alors ? S {} \u003d 1.\nCorollaire 3 Soit S une séquence régulière alors S est non-dérivable.\nPreuve. Notons qu\u0027une séquence régulière possède une unique sous-séquence X avec |S ? X| \u003d 1. A partir de l\u0027équation (5) \nNous avons developpé un algorithme NDSP (Non-Derivable Sequential Patterns) qui permet de de construire la représentation condensée N DSF (D, ?). NDSP est un algorithme par niveau et est complétement indépendant de la structure de données utilisée pour la représen-tation des séquences. L\u0027algorithme 1 est basé sur la stratégie classique du générer-élaguer et est divisé en 3 étapes distinctes : (i) la génération de candidats (ligne 1 et 15 avec la fonction CandidateGeneration()) ; (ii) Le comptage de support (ligne 5) ; (iii) Déterminer les séquences non-dérivables dans F level grâce à la fonction ComputeBounds() (ligne 8), les séquences dérivables sont élaguées ligne 11. Le processus s\u0027arrête quand il n\u0027y a plus de candidats générés.   \nUn algorithme d\u0027extraction de motifs séquentiels clos (représentation condensée), noté\nClosF : CloSpan Yan et al. (2003).\nLes tests ont été faits sur plusieurs jeux de données synthétiques générés avec l\u0027outil DatGen 1 qui est une extension de l\u0027outil IBM QUEST. Les différentes caractéristiques des jeux de données sont représentées dans la figure 2. Les tests se concentrent principalement sur les performances au niveau de la représentation condensée. L\u0027algorithme NDSP a été implementé en JAVA et utilise une structure de données arborescente pour le stockage des séquences et des supports. La figure 3 montre les résultats d\u0027extraction et les performances pour les 2 premiers jeux de données. Pour 0.05 ? ? ? 0.3, NDSP dépasse CloSpan et largement PrefixSpan au niveau de la condensation des motifs séquentiels extraits. De plus, pour ? \u003d 0.1, le nombre des motifs séquentiels non-dérivables décroît plus rapidement que les deux autres approches. L\u0027extraction s\u0027arrête pour l\u0027algorithme NDSP au niveau de profondeur 6 alors que les deux autres algorithmes s\u0027arrêtent à la profondeur 8. Pour CL1000TR10000IT500I40, NDSP teste beaucoup moins de séquences candidates que PrefixSpan ou CloSpan. De plus, le nombre de motifs séquentiels non-dérivables tend à décroître plus rapidement avec le jeu de données CL10000TR1000IT1000I10. En effet ceci est dû principalement à la taille des séquences puisque le jeu de données CL1000TR10000IT500I40 contient un ensemble de motifs séquen-tiels très long avec très peu d\u0027items différents sachant que les séquences longues ont plus de chance d\u0027être dérivables puisqu\u0027elles contiennent plus d\u0027informations. NDSP réalise donc un meilleur taux de compression avec des données denses contenant de longues séquences. Le reste des jeux de données est documenté dans Raissi et Poncelet (2006).\nEtat de l\u0027art\nCes dernières années, de nombreuses recherches se sont intéressées à des représentations condensées pour les itemsets. Les représentations concises les plus importantes sont les itemsets fréquents clos Boulicaut et Bykowski (2000) qui sont basés sur l\u0027opérateur de fermeture sur le treillis. De nombreux algorithmes ont été développés comme CLOSET Pei et al. (2000) et CHARM Zaki et Hsiao (2002)  \nConclusion\nDans cet article, nous avons abordé la problématique des représentations condensées pour les motifs séquentiels. Nos contributions principales sont les suivantes : Premièrement, nous avons jetés les bases formelles pour une nouvelle représentation. Nous introduisons les concepts théoriques des motifs séquentiels non-dérivables et prouvons que les bornes sur la valeur du support d\u0027une séquence peuvent être déduites à partir de règles. Celles-ci sont calculées grâce au principe d\u0027inclusion-exclusion. A notre connaissance, ce travail est le premier travail à introduire une nouvelle représentation condensée autre que la représentation close des motifs sé-quentiels. Deuxièmement nous avons developpé un algorithme NDSP qui dépasse, en terme de taux de compressions, les algorithmes actuels d\u0027extractions de motifs séquentiels et de motifs séquentiels clos. Ce travail offre de nombreuses perspectives. Tout d\u0027abord, le lemme 4 doit être affiné afin de prouver la complétude et l\u0027adéquation de notre méthode, permettant ainsi l\u0027extraction de motifs non-dérivables sans avoir à tester les cohérences des règles et passer par l\u0027actuelle étape de comptage. De plus, cette approche peut-être couplée à des algorithmes très efficaces comme SPADE ou PrefixSPAN. Ce couplage permettrait d\u0027augmenter la vitesse\n"
  },
  {
    "id": "991",
    "text": "Introduction\nL\u0027Extraction de Connaissances dans les Données (ECD) a été proposée afin d\u0027aider les utilisateurs à mieux comprendre et appréhender des quantités de données de plus en plus volumineuses. La recherche de règles d\u0027association constitue une question centrale de l\u0027ECD. La plupart des travaux se sont focalisés sur la tâche d\u0027extraction de règles d\u0027association alors que les aspects visualisation de ces règles et interaction avec l\u0027utilisateur-expert sont très peu représentés. De manière générale, le nombre de règles générées croît de manière exponentielle avec la taille des données. En situation réelle, un expert n\u0027a ni le temps, ni les capacités cognitives de traiter ces flots d\u0027information. Pour l\u0027aider à y faire face, différents travaux proposés dans la littérature tournent autour de deux axes complémentaires : la réduction du nombre de règles d\u0027association extraites et le développement d\u0027outils de visualisation interactive. Dans ce papier, nous focalisons notre intérêt sur les méthodes de visualisation.\nUn état de l\u0027art des différentes techniques de visualisation de règles d\u0027association est dé-crit dans Couturier et Mephu-Nguifo (2007). La limitation commune qui en ressort est que lorsque le nombre de règles est élévé, l\u0027interaction avec l\u0027utilisateur devient difficile. Partant de ce constat, nous proposons ici une nouvelle approche effectuant un regroupement de règles d\u0027association, et particulièrement de règles génériques (Bastide et al. (2000)), en exploitant une représentation en oeil de poisson couplée à une représentation textuelle, 2D ou 3D pour la visualisation de toutes les règles. La représentation en oeil de poisson, dont une première approche est développée dans Couturier et al. (2006), va permettre à l\u0027utilisateur de se focaliser sur un sous-ensemble de règles. Notre approche intègre donc une phase d\u0027extraction de bases génériques et une phase de visualisation en oeil de poisson de ces règles génériques. Ces différentes phases peuvent bien évidement être itérées, et permettent un couplage fort des phases de fouille et de post-traitement. Cette approche est implémenté via le prototype, CB-VAR (Clustered-based Visualizer of Association Rules) permettant de gérer simultanément l\u0027extraction et la visualisation de règles dans le but de pouvoir traiter des quantités plus importantes.\nLe reste du papier est organisé comme suit : dans la seconde section, nous présentons le prototype que nous avons développé pour la visualisation de clusters de règles. La troisième section est consacrée à la présentation des résultats de nos expérimentations. Enfin, dans la dernière section, nous donnons nos conclusions et perspectives.\nLe prototype CBVAR (Clustered-based Visualizer of Association Rules)\nDans ce papier, nous attaquons le problème du nombre de règles à visualiser sur deux fronts différents. Tout d\u0027abord, nous travaillons sur des bases génériques de règles d\u0027association, constituant un ensemble générateur de toutes les règles d\u0027association, tout en étant de taille très compacte. Grâce à ce choix, nous réduisons en amont du processus le nombre de règles à traiter. Ensuite, l\u0027organisation de l\u0027ensemble de règles à visualiser se fait grâce à un ensemble de clusters. Ceci permet de diminuer la charge cognitive et de mettre en place un dispositif de visualisation interactif et coopératif. En aval du processus, nous optimisons l\u0027espace écran pour visualiser notre ensemble de règles. En nous basant sur ces deux points, nous proposons le prototype CBVAR implémenté en JAVA et qui fonctionne actuellement sous l\u0027environnement UNIX 1 . Sa principale caractéristique est qu\u0027il intègre un module d\u0027extraction de règles génériques et un module de visualisation qui sont présentés ci-après.\nExtraction de règles génériques\nAfin d\u0027obtenir nos clusters, nous utilisons différents outils existants qui sont regroupés au sein d\u0027un script shell. L\u0027extraction des règles d\u0027association nécessite en entrée un fichier texte (dat, txt, etc.). Chaque ligne de ce fichier contient la liste des items composant un objet. Les itemsets fermés fréquents et leurs générateurs minimaux associés, ainsi que les règles génériques sont stockés dans un fichier (.txt) grâce à l\u0027algorithme PRINCE (Hamrouni et al. (2005)). Ce dernier génère également un fichier XML (.xml) contenant les informations relatives au support minimum (minsup) et à la confiance minimum (minconf). Ce fichier respecte le standard PMML (Predictive Model Markup Language) dans le but de ne pas limiter la portée de notre prototype sur des applications utilisant des DTD spécifiques. En outre, un fichier (.gen) contenant le méta-contexte associé est généré par PRINCE. Cette étape est très coûteuse en temps de calcul. Pour cette raison, notre prototype peut réutiliser un méta-contexte existant spécifié par l\u0027utilisateur. S\u0027il n\u0027en sélectionne aucun, il sera regénéré. Durant la dernière étape de l\u0027extraction, un fichier (.clu) est généré contenant pour chaque règle, un cluster associé. Le nombre de cluster k est spécifié par l\u0027utilisateur. Grâce à ce fichier et au fichier XML correspondant, k fichiers (.xml), correspondant aux k clusters sont générés avec en plus, un fichier (.cluxml) de méta-cluster listant ces k fichiers XML. Le paquetage contenant ces fichiers constitue le point d\u0027entrée du module de visualisation de CBVAR. Un exemple est présenté dans la figure 1. Chacun des outils utilisés est un paramètre possible pour la génération des clusters. Il est tout à fait possible de les changer en modifiant le script shell associé. Ce script est exécuté par CBVAR en utilisant différents paramètres fixés par l\u0027utilisateur : le support, la confiance, le méta-contexte (si nécessaire) et le nombre de clusters.\nVisualisation de règles d\u0027association : L\u0027approche Fisheye View\nPlusieurs représentations connues en Interface Homme Machine (IHM), ont été proposées pour représenter des informations abondantes (Couturier et al. (2006)). Parmi ces représenta-tions, la déformation en oeil de poisson (Fisheyes view (FEV)) présentée dans Furnas (1986) nous paraît la plus adaptée et une première approche appliquée à la visualisation de règles d\u0027association a d\u0027ailleurs été proposée dans Couturier et al. (2006). Dans ce travail, la repré-sentation exploite une matrice 2D et une FEV pour visualiser des règles d\u0027association tel que chaque règle constitue un point d\u0027intérêt. Ce principe est réutilisé dans notre approche à la différence que notre point d\u0027intérêt sera un cluster (cf. Figure 2).\nNous couplons les approches par matrice 2D et 3D dans notre prototype. La représentation 2D va permettre d\u0027obtenir une vue globale des règles, dans laquelle chaque case va représen-ter une règle de chaque cluster. Cette règle est sélectionnée en fonction des mesures d\u0027intérêt comme le lift (par défaut), le support ou la confiance. Pour obtenir le détail d\u0027un cluster, l\u0027utilisateur va activer la FEV sur l\u0027une des cases. Pour le représenter au niveau du centre d\u0027intérêt de la FEV, une représentation 3D en projection cabinet, gérant le problème d\u0027occlusions (Couturier et Mephu-Nguifo (2007)), est utilisée pour les contextes épars alors qu\u0027une représentation 2D l\u0027est pour les contextes denses. Un contexte dense est caractérisé par la présence d\u0027au moins une règle exacte, puisque au moins un générateur minimal est différent de sa fermeture associée, ce qui n\u0027est pas le cas pour les contextes épars.\nExpérimentations\nAfin de démontrer la valeur de notre approche, nous avons mis en oeuvre des expérimen-tations sur deux jeux de données Le nombre de clusters est défini en fonction du nombre x de règles d\u0027association que peut contenir un cluster. Nos résultats sont présentés dans la table 1. Tous les fichiers XML sont initialement illisibles dans un même espace écran à cause du nombre important de règles. Notre approche permet de visualiser ces jeux de données dans un même espace écran, avec approximativement les même temps d\u0027affichage pour un nombre de clusters égal à ( # regles generiques 100 ) pour des données denses ou éparses. Nos tests exploitant différentes valeurs de x (i.e., x \u003d 25 and x \u003d 50) requièrent en général plus de temps à l\u0027affichage. Cependant, pour les clusters contenant moins de 100 règles, il est possible de réduire le temps relatif à l\u0027affichage d\u0027un cluster puisqu\u0027il est le seul à être affiché en détail tout en contenant moins de règles. Afin d\u0027interagir en temps réel, il est nécessaire de charger toutes les règles en mémoire. Cependant, nous pouvons observer qu\u0027avec des valeurs de minsup (resp. minconf ) égales à 0,2 (resp. 0,2) pour les données MUSHROOM et avec des valeurs de minsup (resp. minconf ) égales à 0,004 (resp. 0,004) pour les données T10I4D100K, il n\u0027est pas toujours possible de le faire avec les visualisations classiques sans clustering. Notre approche permet de visualiser le premier jeu de données en 17 953 ms avec # regles generiques 100\nclusters. Cet ensemble représente 7 057 règles qui sont incluses dans le même espace écran (cf. Tableau 1). Le second jeu de données est affiché avec notre approche pour les différentes valeurs de x utilisées pour nos tests. Dans ce cas, l\u0027ensemble de règles d\u0027association contient jusqu\u0027à 3 623 règles (cf. Tableau 1). Dans ce contexte, nous n\u0027avons pas utilisé de valeur de minsup élevée puisque le nombre de règles n\u0027aurait pas été assez important pour justifier l\u0027intérêt de notre approche. Une approche classique aurait elle aussi permis de les traiter.\nConclusion et perspectives\nDans ce papier, nous avons présenté une méthode de visualisation de grands ensembles de règles d\u0027association. Ainsi, nous avons proposé d\u0027utiliser le clustering sur un ensemble de règles génériques pour réduire la charge cognitive de l\u0027utilisateur. Nous avons implémenté le prototype CBVAR correspondant. Les expérimentations que nous avons menées ont mis en exergue la possibilité de visualiser rapidement dans un même espace écran quelques milliers de règles en quelques secondes et que notre prototype est performant sur des quantités importantes de données. Ainsi, grâce à l\u0027hybridation d\u0027une représentation 2D et d\u0027une FEV, notre approche permet d\u0027obtenir simultanément une vue globale et détaillée de nos règles.\nPour la suite, nous souhaitons principalement focaliser nos efforts sur l\u0027étape de clustering qui est centrale dans notre approche. Ainsi, la piste des k-hiérarchies faibles sera considérée. En outre, comme la famille des regroupements produite par une hiérarchie faible constitue une famille de Moore, alors la structure du treillis pourrait devenir une structure de visualisation privilégiée. Du point de vue IHM, nous souhaitons réaliser une évaluation utilisateur.\n"
  },
  {
    "id": "992",
    "text": "Introduction\nLa plate-forme de visualisation de graphes Tulip 1 (Auber, 2003) développée au LaBRI est dédiée à l\u0027exploration de grands graphes. Elle autorise un utilisateur expert à visualiser un graphe à partir d\u0027algorithmes de dessin parmi les plus récents. Elle facilite le calcul et le rendu visuel de statistiques sur les graphes afin d\u0027en rechercher les propriétés structurelles. Conçue pour la manipulation et le calcul du clustering de grands graphes, l\u0027interface utilisateur permet de fouiller l\u0027information ainsi découpée en inspectant les clusters et leurs éléments, tout particulièrement lorsque le clustering produit une hiérarchie de sous-graphes.\nDe fait, Tulip se prête à la fouille interactive de données munies de relations binaires et enrichies d\u0027attributs, numériques ou textuels. Le calcul d\u0027un graphe à partir d\u0027indices de similarité est un exemple typique où l\u0027exploration interactive vient en appui au travail de fouille de données et d\u0027extraction de connaissances. On peut penser au cas où des documents sont liés deux à deux par un indice de similarité (de leur contenu). En seuillant cet indice, on peut induire sur l\u0027ensemble des documents une structure de graphes qu\u0027il est alors utile d\u0027examiner.\nNous proposons dans cet article de décrire une étude de cas exhibant les qualités de la plateforme Tulip et démontrant l\u0027apport de la visualisation à la fouille de données et à l\u0027extraction de connaissances.\nExploration visuelle et fouille de données\nNous nous pencherons sur le cas où l\u0027on souhaite étudier une collection de documents afin d\u0027avoir une idée des thématiques abordées dans la collection de manière globale, pour savoir si un sujet rassemble une majorité de documents, ou si au contraire il ne concerne qu\u0027une toute petite part d\u0027entre eux, par exemple. Typiquement, on extraira des documents un ensemble de mots-clés qui capturent leur contenu à divers degré. Le plus souvent, la présence de motsclés donnent lieu au calcul d\u0027indices de similarité entre documents : deux documents seront d\u0027autant plus similaires qu\u0027ils ont des mots-clés en commun et que ceux-ci y apparaissent souvent. (Nombre de variantes existent ; voir (Hammouda et Kamel, 2004) par exemple, ou (Dubois et Bothorel, 2006) pour une approche récente intégrant les usages.) A l\u0027inverse, on peut associer les mots-clés deux à deux afin de voir émerger certains mots au rang de concept (Figure 1). Les réseaux de co-citation, mêlant auteurs et publications scientifiques, sont un autre exemple de graphes qui se prêtent à l\u0027examen visuel afin de définir une stratégie d\u0027analyse des données sous-jacentes.\nEn d\u0027autres mots, partant d\u0027une collection de N documents, on se ramène à l\u0027étude d\u0027un graphe dont les sommets représentent les documents. La mesure de similarité est interprétée comme la donnée des arêtes qui sont alors valuées (voire orientées). Lorsque le graphe ainsi obtenu est complet ou lorsque son nombre d\u0027arêtes avoisine N 2 (ou N (N ?1) dans le cas d\u0027un graphe orienté, ou N 2 si on admet les boucles ou auto-référence), on peut filtrer certaines des arêtes pour alléger le graphe, en espérant ne retenir que les relations les plus marquées, c\u0027est-à-dire les plus à même de rendre la structure inhérente à la collection de documents. Cette idée souvent empruntée remonte à (Tenenbaum et al., 2000) et s\u0027avère utile, malgré quelques limitations attribuables à la difficulté du problème de la réduction de la dimension d\u0027un jeu de données (dimensionality reduction) -voir (Balasubramanian et Schwartz, 2002). La Figure 1 illustre ce procédé ; dans cet exemple, les mots-clés sont associés selon leurs indices de cooccurences dans la collection de documents.\nToutefois, l\u0027utilisateur peut rapidement être dépassé par le nombre d\u0027éléments du graphe (sommets et arêtes) à étudier, et par la complexité du réseau qu\u0027ils forment. C\u0027est ici que la visualisation interactive entre en jeu : l\u0027utilisateur pourra interagir sur la carte et acquérir une compréhension du réseau à travers sa navigation, plutôt que d\u0027en rester à une carte statique. L\u0027analyste peut s\u0027appuyer sur une visualisation de l\u0027ensemble des documents et l\u0027explorer interactivement afin de percevoir des motifs structuraux dans l\u0027organisation des liens. Soulignons que la visualisation n\u0027est, la plupart du temps, pas une fin en soi mais permet de définir une stratégie d\u0027analyse du corpus. A l\u0027inverse, la visualisation évoluera en proportion de la compréhension que gagne l\u0027analyste sur cet ensemble de données complexes. Ce scénario a été testé en taille réelle sur un jeu de données diffusé sur le web (Delest et al., 2004) 2 ; il a aussi été repris plus récemment dans le contexte bio-informatique (Iragne et al., 2005) 3 et pour l\u0027exploration de grands réseaux spatiaux en géographie (Amiel et al., 2005) 4 -voir Figure 2. Il s\u0027agit là, d\u0027une certaine manière, d\u0027un scénario très souvent mis en oeuvre avec Tulip pour venir en appui à la fouille et à l\u0027exploration interactive de données complexes 5 .\nVisualisation de graphes\nLa plate-forme Tulip prend le parti d\u0027offrir en priorité à l\u0027utilisateur un grand choix d\u0027algorithmes de dessin de graphes (Graph Drawing 6 ), c\u0027est-à-dire de représentations des graphes dans le plan ou dans l\u0027espace à l\u0027aide de sommets (des points) et d\u0027arêtes (traits rectilignes).\nC\u0027est en quelque sorte une spécialisation de Tulip à un type précis d\u0027abstractions visuelles ou de transformations visuelles, pour emprunter les termes de la taxonomie de Chi (Chi, 2000). Ainsi, on peut facilement faire appel aux algorithmes de dessin d\u0027arborescences [ (Reingold et Tilford, 1981)  (Eades, 1992)  (Grivet et al., 2004)], de graphes orientés sans circuit (DAG), de graphes planaires et de graphes plus généraux à l\u0027aide des algorithmes basés sur les analogies physiques (souvent appelés masses-ressort) [(Fruchterman et Reingold, 1991)  (Fricke et al., 1994)] ou s\u0027appuyant sur l\u0027analyse spectrale [   ].\nCela dit, l\u0027interface, le format de description et les fonctionnalités internes à Tulip permettent à l\u0027utilisateur de représenter les sommets par différentes objets graphiques simples. En particulier, Tulip autorise l\u0027utilisateur à calculer, outre les positions des sommets dans le plan, divers attributs numériques pour les sommets et/ou les arêtes. La visualisation de ces attributs est alors naturellement traduits sous forme d\u0027indices visuels comme la couleur ou la taille des sommets. Cette idée très simple a fait ses preuves pour aider l\u0027analyste dans l\u0027exploration de grands graphes (voir (Herman et al., 2000) par exemple).\nComme on peut s\u0027y attendre, l\u0027utilisateur est à même d\u0027affecter à chaque sommet une forme différente et paramétrable, une couleur qui varie en fonction d\u0027un indice structurel ou d\u0027un attribut contextuel. S\u0027il est possible d\u0027effectuer ce choix au moment de la visualisation, on peut tout aussi bien l\u0027insérer au niveau du fichier de description, ou encore concevoir un plug-in spécifique qui s\u0027intégrera facilement à la plate-forme (voir Section 5). La plate-forme offre déjà nombre d\u0027algorithmes calculant des indices structuraux sur les graphes -indices de centralité (Freeman, 2000), indice de clustering (Watts et Strogatz, 1998), . . . -et les indices plus habituels comme le degré entrant ou sortant, etc. D\u0027autres algorithmes permettent de tester certaines propriétés comme l\u0027acyclicité ou la connexité, et effectueront la sélection de composantes connexes, d\u0027arbres couvrants ou de graphes acycliques couvrants, par exemple.\nCes dernières manipulations s\u0027avèrent des plus pratiques pour sélectionner un sous-ensemble d\u0027un jeu de données et le faire passer au rang de sous-graphe. Cet objet affiché dans une fenêtre qui lui est propre permettra de travailler à plus petite échelle, tout en héritant des propriétés calculées sur le graphe dont il est issu (voir Figure 5).\nCas d\u0027étude\nLa Figure  \nFIG. 2 -Histogramme des indices de similarités (normalisés).\ncartographie (voir (Bertin, 1998), (Denègre, 2005)). Dans la Figure 1, la taille d\u0027un sommet correspond à son degré dans le graphe. Le degré donne en effet une première indication du rôle que joue un sommet dans le réseau, au moins dans son voisinage immédiat. La couleur du sommet est, elle, associée à la connexité de son voisinage (selon l\u0027indice de clustering de Watts (Watts et Strogatz, 1998)). Le choix des couleurs peut lui aussi être varié. L\u0027exploration interactive permet à l\u0027utilisateur, après avoir repéré un phénomène saillant (voisinage dense, sommet de degré plus élevé, etc.) de circonscrire son exploration à une région particulière du graphe. La Figure 3 illustre une manipulation typique, aisément exécutée avec Tulip. L\u0027utilisateur aura sans aucun doute perçu la complexité du voisinage du sommet (concept) administration, et pourra alors sélectionner le sommet. Par déplacement, il peut ensuite isoler ce sommet en l\u0027écartant de son voisinage. L\u0027ensemble des liens suit et on est à même de percevoir comment est tissé cette région. Les arêtes du sommet sont mises en exergue par un effet de coloration pour signaler leur sélection, rendant à la fois l\u0027effet de la manipulation et cette partie de la carte plus lisible et permettant à l\u0027utilisateur de constater les liens avec les concepts processus, surveillance, ou programme, par exemple.  Figure) est organisé de manière similaire et suit une topologie en étoile typique de la méthode utilisée ici. Cette image est elle-même une vue partielle de tout le réseau : nous avons en effet accédé au cluster illustré à la Figure 4 et à son organisation multi-niveaux en zoomant à l\u0027intérieur de l\u0027une des composantes du graphe de départ. Observez le cluster contenant le mot administration qui avait retenu notre attention à la Figure 3, qui se trouve sur la droite. Avec le mot subversion auquel il est lié de manière significative, il forme une paire indissociable dans le graphe quotient. Les autres voisins du mot administration sont passés au second ordre, et reste encapsulé dans le noyau.\nClustering de graphes\nOn peut d\u0027une certaine manière penser aux composantes placées en périphérie du noyau comme aux mots-clés formant avec certains de leurs voisins des composantes indissociables, mais qui se détachent suffisamment du noyau central. Typiquement, tout algorithme de clustering est susceptible de repérer un voisinage formant une clique -un sous-graphe complet, comme celui dans la partie gauche de la Figure 5. Au niveau le plus bas, on trouvera des composantes irréductibles, dans le sens où l\u0027algorithme de clustering ne saura trouver de critère pour en extraire un sous-graphe. C\u0027est évidemment le cas pour les cliques, mais aussi pour les graphes comme celui situé à droite dans la Figure 5, qui sans être une arborescence est principalement constitué de longues chaînes de mots sans voisinage touffu (sauf peut-être pour le voisinage du mot java). Incidemment, on peut chercher à suivre dans ce sous-graphe les chemins allant d\u0027un mot à l\u0027autre pour y trouver une proximité de sens et ainsi interpréter le contenu de la collection de documents. \nConclusion\nPlus qu\u0027une suite logicielle, Tulip est une plate-forme de développement de laquelle peuvent être déclinées des applications spécifiques [EVAT (Auber et al., 2003b) pour la visualisation et la comparaison de grandes arborescence -arborescences de fichiers, classification des espèces, etc. -ProViz (Iragne et al., 2005)   (Auber et al., 2003a) ne reconnaît aucune composante à extraire.\nd\u0027algorithmes de clustering ou de dessin de graphes est tout aussi facile. A l\u0027inverse d\u0027autres outils de visualisation comme IVTK (Fekete, 2004) ou prefuse (Heer et al., 2005), Tulip est développé en C++, permettant ainsi de manipuler des données massives. La déclinaison du coeur de Tulip en une application dédiée nécessite l\u0027utilisation de la librairie QT 8 , pour ce qui concerne l\u0027environnement de fenêtrage et les modalités de l\u0027interface graphique, auquel vient se greffer le noyau Tulip pour la gestion de la structure de graphes (hié-rarchiques) et l\u0027utilisation d\u0027algorithmes spécifiques. Notons au passage que Tulip est inclus dans plusieurs distributions Linux (Suse, Debian, . . .). Tulip est distribué sous licence GPL et est hébergé sur le site SourceForge 9 . Les fonctionnalités de la plate-forme Tulip constituent sans aucun doute une boîte à outils des plus utiles pour découvrir, concevoir et tester les hypothèses des chercheurs oeuvrant dans le domaine de l\u0027extraction et la gestion des connaissances. Idéalement, la visualisation sera couplée aux méthodes et techniques propre à ce domaine afin d\u0027offrir aux utilisateurs une cartographie sémantique interactive pour fouiller les grands espaces d\u0027information, structurés ou non, ou encore comme interface d\u0027accès à l\u0027information.\n"
  },
  {
    "id": "993",
    "text": "Introduction\nLe volume de données stocké double actuellement tous les 9 mois (Lyman et al, 2003) et donc le besoin d\u0027extraction de connaissances dans les grandes bases de données est de plus en plus important (Fayyad et al, 2004). La fouille de données (Fayyad et al, 1996) vise à traiter des ensembles de données pour identifier des connaissances nouvelles, valides, potentiellement utilisables et compréhensibles. Cette utilisabilité est fonction des buts de l\u0027utilisateur donc seul l\u0027utilisateur peut déterminer si les connaissances extraites répondent à ses attentes. Les outils de fouille de données doivent donc être interactifs et anthropocentrés. Notre approche consiste à impliquer plus fortement l\u0027utilisateur dans le processus de fouille par des méthodes graphiques interactives dans un environnement de fouille.\nDe nombreuses méthodes de visualisation ont été développées dans différents domaines et utilisées pour l\u0027analyse exploratoire et la fouille de données (Fayyad et al, 2001), (Keim, 2002). Les méthodes de visualisation peuvent être utilisées pour le pré-traitement de données (par exemple la sélection de données) ou en post-traitement (par exemple pour voir les résultats). Des méthodes récentes (Ankerst, 2001), (Do et Poulet, 2004a et b), (Munzner, 1997) essayent d\u0027impliquer plus significativement l\u0027utilisateur dans le processus de fouille de données par le biais de la visualisation. Parmi les avantages que ce type d\u0027approche présente on peut citer : l\u0027utilisateur peut être un spécialiste du domaine des données et utiliser son expertise tout au long du processus de fouille, la confiance et la compréhensibilité du modèle sont accrues car l\u0027utilisateur a participé à sa construction et enfin on peut utiliser les performances de la perception visuelle humaine en reconnaissance des formes.\nNous présentons une méthode d\u0027exploration interactive des résultats des algorithmes d\u0027arbres de décision comme C4.5 (Quinlan, 1993). Les arbres de décision sont des méthodes efficaces et populaires (Kdnuggets, 2003(Kdnuggets, et 2004 pour la classification. Un arbre de décision est un classifieur représenté sous la forme d\u0027un arbre dans lequel chaque noeud est soit une feuille contenant l\u0027étiquette de la classe, soit un noeud interne contenant un test à effectuer sur l\u0027un des attributs avec un fils pour chaque résultat possible du test. Une règle d\u0027induction (de la forme si alors) est créée pour chaque chemin partant de la racine de l\u0027arbre et parcourant les tests (en faisant des conjonctions) jusqu\u0027à la feuille qui est l\u0027étiquette de la classe. Les arbres de décision sont particulièrement appréciés car ils permettent une compréhension aisée, mais lors d\u0027une tâche de classification relativement complexe l\u0027utilisateur n\u0027est plus capable d\u0027explorer efficacement les résultats obtenus sous forme textuelle.\nQuelques techniques de visualisation d\u0027arbres de décision existent déjà. Le TreeVisualizer de Mineset (Brunk et al, 1997) permet une visualisation en 3D avec une vue partielle des résultats. Seuls les premiers niveaux de l\u0027arbre sont affichés initialement et le reste apparaît au fur et à mesure que l\u0027utilisateur descend dans l\u0027arbre.\nLe Cone Tree (Robertson et al, 1991) utilise aussi une représentation 3D de l\u0027information hiérarchique, le noeud courant est le sommet d\u0027un cône contenant tous ses fils répartis à la base de ce cône 3D.\nLa technique focus+context (Lampig et Rao, 1996) utilise une portion de l\u0027espace plus importante pour la zone où l\u0027utilisateur se positionne. Le principe de cette visualisation est l\u0027utilisation d\u0027un plan hyperbolique et d\u0027une projection sur une sphère.\nLes arbres hyperboliques (hyperbolic trees (Munzner, 1997)) visualisent les arbres sous la forme de graphes dans un espace 3D hyperbolique. Cette technique utilise une métrique hyperbolique et optimise le placement des fils d\u0027un noeud sur une hemisphère autour de la base du cône. L\u0027exploration animée (Yee et al, 2001) de graphes dynamiques avec visualisation radiale est une technique d\u0027animation de la transition d\u0027un noeud à l\u0027autre. Pour permettre une transition facile à suivre, l\u0027animation correspond à une interpolation linéaire entre les noeuds source et destination avec des contraintes sur l\u0027ordre et l\u0027orientation des objets.\nEn ce qui concerne les arbres de décision dans un contexte de fouille de données, les impératifs importants pour l\u0027utilisateur sont : une compréhension facile, la possibilité d\u0027extraire des règles pertinentes et la possibilité d\u0027effectuer un élagage de l\u0027arbre dans une étape de post-traitement. Nous présentons une nouvelle méthode de visualisation radiale des arbres de décision pour l\u0027exploration interactive. Nous utilisons pour cela des méthodes de visualisation telles qu\u0027une représentation à la manière d\u0027un explorateur, le focus+context, le fisheye, la visualisation hiérarchique et les techniques interactives pour représenter des arbres de grandes tailles de manière graphique plus intuitive que les résultats habituels des algorithmes d\u0027arbres de décision. L\u0027utilisateur peut explorer interactivement l\u0027arbre de décision, extraire des règles intéressantes et élaguer l\u0027arbre obtenu. Les résultats numériques des tests sur des ensembles de données réels montrent que les méthodes proposées améliorent la compréhension des résultats des arbres de décision.\nReprésentation type explorateur\nUne représentation type explorateur va projeter la racine de l\u0027arbre dans le coin supérieur gauche de la vue (habituellement de coordonnées (0,0)). Un noeud est représenté par un carré dont la couleur représente la classe (ou classe majoritaire). La taille d\u0027une feuille sera proportionnelle aux nombres d\u0027individus contenus dans cette feuille. Chaque noeud peut être développé ou non par un clic de souris. La figure 1 est un exemple de visualisation de type explorateur avec l\u0027ensemble de données Shuttle de l\u0027UCI (Blake et Merz, 1998) contenant 58000 individus en dimensions 9 et 7 classes.\nL\u0027utilisateur peut ainsi explorer facilement les résultats de l\u0027algorithme d\u0027arbre de décision dans un mode graphique plus intuitif que les résultats habituels sous forme de texte. La représentation du nombre d\u0027individus contenus dans les feuilles aide à déterminer l\u0027importance de la règle de décision. Les feuilles ne contenant que peu d\u0027individus sont généralement de moindre importance. Ces indications sont une aide précieuse lorsque l\u0027utilisateur cherche à élaguer l\u0027arbre de décision obtenu.\nFIG. 1 -Représentation type explorateur de l\u0027arbre de décision des données Shuttle\nL\u0027utilisateur peut extraire des règles d\u0027induction, en cliquant sur une feuille de l\u0027arbre, le chemin de la racine à la feuille est sélectionné et la règle correspondante est alors affichée. L\u0027exemple de la figure 2 montre une règle extraite de l\u0027arbre de décision obtenu sur l\u0027ensemble de données Shuttle. Grâce aux techniques interactives telles que le développement ou non d\u0027un noeud, le focus ou le zoom, l\u0027utilisateur peut facilement naviguer dans l\u0027arbre avec l\u0027affichage des informations associées à chaque noeud telles que le nombre d\u0027individus RNTI -XTree-Viz ou le nombre d\u0027erreurs (individus mal classifiés). Avec ces éléments l\u0027utilisateur a une aide précieuse pour effectuer lui-même l\u0027élagage de l\u0027arbre de décision. Cet élagage permettra d\u0027améliorer le taux de bonne classification sur l\u0027ensemble de test en minimisant le surapprentissage. \nFIG. 2 -Extraction de règle d\u0027induction\nVisualisation radiale, fisheye, focus+context et techniques hiérarchiques\nPour des arbres de grandes tailles nous avons essayé d\u0027améliorer la représentation 2D en utilisant une visualisation radiale, le fisheye et le focus+context dans une visualisation hiérarchique. Avec l\u0027algorithme de visualisation radiale, la racine de l\u0027arbre de décision est projetée sur le centre de la vue, les noeuds fils sont disposés en cercles ou arcs de cercle autour du noeud parent. L\u0027espacement est fonction du nombre de noeuds descendants de chaque noeud, comme le montre l\u0027exemple de la figure 5.\nRNTI -X -\nTree-Viz\nNous utilisons ensuite la technique du fisheye qui aide l\u0027utilisateur à détailler l\u0027environnement immédiat du noeud courant ce qui lui permet de se focaliser sur les parties qu\u0027il juge intéressantes de l\u0027arbre de décision.\nFIG. 5 -Visualisation radiale\nNous proposons aussi une nouvelle technique dérivée du fisheye et permettant un zoom sur une zone d\u0027intérêt au détriment des autres parties. Considérons la racine O projetée en O1 et un noeud P projeté en P1, comme sur l\u0027exemple de la figure 7. La racine va être translatée de O1 vers O2 en préservant l\u0027angle ?\u003d (Ox, OP). Le noeud P doit donc lui aussi être translaté de P1 à P2. L\u0027angle obtenu (O1x, O1P2) est supérieur à l\u0027angle (O1x, O1P1). Voici comment obtenir la position de P2 : P 2 (x) \u003d x 1 + r*cos(?) ( 1 ) P 2 (y) \u003d y 1 + r*sin(?) ( 2 ) P 2 (x) \u003d x 2 + t*cos(?) ( 3 ) P 2 (y) \u003d y 2 + t*sin(?) ( 4 ) En substituant P2(x) [resp. P2(y)] dans (1) [resp.(2)], on obtient les équations (5) et (6). r*cos(?) \u003d x 2 -x 1 + t*cos(?) ( 5 ) r*sin(?) \u003d y 2 -y 1 + t*sin(?) ( 6 ) \u003d\u003e r 2 \u003d (?x + t*cos(?)) 2 + (?y + t*sin(?)) 2 (7) avec ?x \u003d x 2 -x 1 et ?y \u003d y 2 -y 1 t 2 + 2(?x cos(?) + ?y sin(?))t + ?  \nFIG. 7 -Focus sur les noeuds d\u0027intérêt\nLes expérimentations des systèmes de visualisation d\u0027arbres de décision (Barlow et Neuville, 2001) ont montré qu\u0027il n\u0027y a pas une technique meilleure que les autres pour l\u0027exploration de grands arbres de décision avec à la fois la simplicité, la vitesse, le focus, une vue globale et une bonne compréhension de l\u0027utilisateur. Nous souhaitons donc combiner plusieurs techniques pour tirer partie des avantages de chacune. Notre méthode de visualisation utilise une visualisation type explorateur, une visualisation radiale, le fisheye et le focus. Le principe est de diviser l\u0027espace écran en différentes zones dans lesquelles une visualisation différente peut être affichée. Nous avons choisi par défaut de visualiser l\u0027arbre RNTI -XTree-Viz complet avec la visualisation radiale, le fisheye et le focus. L\u0027utilisateur peut sélectionner un sous-ensemble de l\u0027arbre et le visualiser avec une autre méthode comme la représentation type explorateur sur l\u0027exemple de la figure 9.\nFIG. 8 -Focus sur une région d\u0027intérêt\nFIG. 9 -Visualisation hiérarchique et type explorateur de Segment\nLa visualisation hiérarchique permet de préserver une vision globale de l\u0027arbre avec une représentation plus fine grâce au fisheye et au focus. Le sous-arbre sélectionné peut alors être exploré aisément dans la représentation type explorateur. On allie donc la simplicité, la rapidité pour effectuer la tâche, la facilité d\u0027utilisation et la compréhension du modèle. La figure 9 représente la visualisation de l\u0027arbre de décision obtenu sur l\u0027ensemble de données Segment de l\u0027UCI (2310 individus en dimension 19 et 7 classes). L\u0027environnement développé permet la visualisation d\u0027arbre de taille relativement importante en permettant simultanément une vision de la totalité de l\u0027arbre et un zoom sur les zones d\u0027intérêt. Il essaye d\u0027associer à la fois la hiérarchie complète de l\u0027arbre, la simplicité d\u0027utilisation, la rapidité d\u0027exécution de la tâche et la satisfaction et bonne compréhension de l\u0027utilisateur.\nExploration des résultats de la classification de spam\nConclusion et perspectives\nNous avons présenté un nouvel environnement graphique pour l\u0027exploration des résultats des algorithmes d\u0027arbre de décision (comme C4.5). Notre but était de satisfaire les demandes des utilisateurs dans le contexte de la fouille de données : facilité d\u0027interprétation, extraction de règles pertinentes et élagage efficace de l\u0027arbre dans une phase de post-traitement. Nous avons développé une nouvelle méthode de visualisation radiale pour permettre l\u0027exploration interactive des résultats des arbres de décision. Cette méthode a été utilisée en collaboration avec d\u0027autres méthodes telles que la représentation type explorateur, le focus+context, le fisheye, la visualisation hiérarchique et les techniques interactives pour permettre la visualisation d\u0027arbres de tailles importantes de manière plus intuitive que les habituelles sorties texte des algorithmes d\u0027arbres de décision. Grâce à l\u0027ensemble des techniques telles que le développement ou non des noeuds, le focus, le zoom ou la rotation l\u0027utilisateur peut aisément explorer les résultats des arbres de décision. Il peut aussi prendre connaissance des informations associées aux noeuds de l\u0027arbre comme le nombre d\u0027individus ou d\u0027erreurs. Cette connaissance peut lui permettre d\u0027extraire des règles d\u0027induction ou d\u0027élaguer lui-même l\u0027arbre dans un mode graphique interactif et intuitif basé sur la profondeur, la taille, la couleur et l\u0027information liée aux noeuds de l\u0027arbre. L\u0027utilisateur a ainsi une meilleure compréhension de l\u0027arbre obtenu. Les test sur l\u0027ensemble de données Spambase montre que la méthode proposé permet de mieux appréhender les résultats des algorithmes d\u0027arbres de décision. Une extension future est de trouver une abstraction permettant de visualiser des arbres de taille plus importante, une autre extension envisagée est d\u0027étendre cette approche à d\u0027autres algorithmes de fouilles de données pour permettre de d\u0027expliquer les résultats obtenus par ces algorithmes.\n"
  },
  {
    "id": "995",
    "text": "Le temps nécessaire pour écouter un flux audio est un facteur réduisant l\u0027accès efficace à de grandes archives de parole. Une première approche, la structuration automatique des données, permet d\u0027utiliser un moteur de recherche pour cibler plus rapidement l\u0027information. Les listes de résultats générées sont longues dans un souci d\u0027exhaustivité. Alors que pour des documents textuels, un coup d\u0027oeil discrimine un résultat interessant d\u0027un résultat non pertinant, il faut écouter l\u0027audio dans son intégralité pour en capturer le contenu. Nous proposons donc d\u0027utiliser le résumé automatique afin de structurer les résultats des recherches et d\u0027en réduire la redondance.\nStructuration\nRecherche Résumé parlé Audio utilisateur Les données radiophoniques exploitées pour cette approche sont issues de la campagne ESTER (Galliano et al., 2005), évaluatrice de la structuration automatique d\u0027émissions et de bulletins à caractère informatif. Le processus de structuration de notre système est le suivant : segmentation en classes acoustiques , segmentation en locuteurs (Istrate et al., 2005), transcription de la parole , segmentation thématique (Sitbon et Bellot, 2004), et reconnaissance d\u0027entités nommées (Favre et al., 2005). Grâce à cette structuration, un moteur de recherche basé sur le modèle vectoriel permet de présenter à l\u0027utilisateur la liste des segments correspondant à son besoin en information.\nFondé sur l\u0027observation que 70% des phrases d\u0027un résumé écrit manuellement proviennent des textes d\u0027origines, le résumé par extraction est l\u0027approche la plus utilisée actuellement en domaine ouvert pour le texte. En prenant pour hypothèse que cette observation est similaire pour la parole (les titres des journaux radiodiffusés), nous l\u0027appliquons à la fois pour extraire des étiquettes thématiques structurant hiérarchiquement les résultats et pour extraire les segments les plus représentatifs du contenu des résultats.\nL\u0027algorithme Maximal Marginal Relevance (MMR), proposé par (Goldstein et al., 2000) pour sélectionner les segments maximisant la couverture en information tout en minimisant sa redondance, peut être appliqué pour sélectionner des mots-clés comme étiquettes thématiques dont on obtient une hiérarchie en faisant varier la granularité. Le critère de sélection par gain en -273 -RNTI-E-6\nAccès aux connaissances orales par le résumé automatique couverture de MMR est modifié en transposant le paradigme de représentation des documents par des vecteurs de mots, afin de représenter des mots par des vecteurs de documents.\nIci, t est le vecteur modélisant un mot-clé, c res le vecteur centroïde des résultats, c sel le vecteur centroïde de la sélection courante et sim() la similarité mesurée par le cosinus de l\u0027angle entre les vecteurs. Dans le domaine de l\u0027information radiodiffusée, les mots-clés utilisés sont des entités nommées car les noms de lieux, de personnes et d\u0027organisation permettent de caractériser des événements. Ces étiquettes thématiques sont proposées à l\u0027utilisateur qui, en les sélectionnant, implique la restriction des résultats par conjonction avec les termes de la requête. Parallèlement, le résumé des segments audio est généré selon MMR classique pour permettre à l\u0027utilisateur d\u0027écouter l\u0027équivalent d\u0027un court bulletin d\u0027informations.\nBien que le système permette une forte réduction du temps d\u0027écoute, le résumé audio est soumis aux mêmes problèmes majeurs que le résumé textuel, à savoir les références non réso-lues et la réduction de redondance à l\u0027interieur même des segments. S\u0027ajoutent les erreurs de la structuration automatique et les désagréments liés à la parole comme les difficultés d\u0027élocution ou les recouvrements de locuteurs dont l\u0027impact est présent à l\u0027écoute. Nous projetons pour la suite de ces travaux, d\u0027adresser ces problèmatiques et d\u0027évaluer le système d\u0027accès aux flux de données parlées.\n"
  },
  {
    "id": "997",
    "text": "Introduction\nNotre idée-clé est de s\u0027attaquer au problème de la réduction des files d\u0027attente à partir de l\u0027analyse des journées d\u0027hospitalisation non-pertinentes. Les études effectuées jusqu\u0027ici ont été trop spécialisées (Vardi A., 1996). L\u0027objectif de cette communication est de proposer un outil efficace, de haute qualité, accessible à un non-spécialiste d\u0027aide à la décision pour réduire les files d\u0027attente des patients, basé sur la visualisation des composantes des journées d\u0027hospitalisation nonpertinentes dans les services cliniques aigus.\n477 patients ont été inclus à partir de 3 spécialités différentes soit 4834 journées en soins aigus évaluées dans 4 services cliniques répartis dans 3 hôpitaux. Après avoir mis en évidence des associations (à facteur constant) de variables liées à la non-pertinence (Huet B., 2005), nous avons fait des analyses en correspondances multiples (sous SAS V8.2 / PC).\nLes données visualisées\nL\u0027analyse des « processus de gestion médicale » (PGMs) de tous les patients (477) de tous les services a montré que 84% de la variance des données peuvent être modélisés en deux axes (figure1). Le 1er axe (70%) dépend essentiellement du nombre de journées nonpertinentes (30%), des causes de non-pertinence (25%), du taux de non-pertinence (jnp/jtot) (24%), durée de séjour (21%) tandis que le second axe (14%) dépend essentiellement de la durée de séjour (35%), du taux de non-pertinence (jnp/jtot) (28%), du nombre de journées non-pertinentes (24%), des causes de non-pertinence (13%). Ces 2 axes classent les PGMs selon une hyperbole classique, par leur « poids composé de non-pertinence »: du plus « léger » au plus « lourd » : blessures cutanéo-muqueuses, ablation de matériel opératoire, désintoxication alcoolique, fracture simple, chirurgie de courte durée, gastro-entérologie, médecine interne (non gériatrique), fracture complexe, chirurgie longue durée, médecine interne (gériatrique), démence et médecine interne, démence et troubles neuro-psychiques. RNTI-E-6\nAide en gestion hospitalière\nFIG. 1 -Projection d\u0027analyse des Processus de Gestion médicale classés selon leur « poids composé de non-pertinence ».\nDiscussion et Conclusion\nCette visualisation permet une lecture immédiate d\u0027une information hautement significative, l\u0027utilisation opérationnelle de ces données est parfaitement valable elle permet des non-spécialistes d\u0027avoir accès à ces données hautement spécialisées. \nRéférences\nSummary :\nWe present the visualizing of components associated with inappropriate hospital days with their causes and their queues. It is a highly significant information whose interpretation can be made by a non-specialist (hospital manager,…).\n"
  },
  {
    "id": "998",
    "text": "Introduction\nNous nous intéressons à la recherche d\u0027outliers (individus atypiques) dans les ensembles de données ayant un grand nombre de dimensions. Pour pouvoir traiter de tels ensembles de données (par exemple les ensembles de données de fouille de texte ou de bio-informatique), la plupart des algorithmes de fouille de données actuels nécessitent un prétraitement permettant de réduire le nombre de dimensions (avec plus ou moins de perte d\u0027information). L\u0027approche la plus intuitive pour appréhender le problème des grandes dimensions est d\u0027énumérer tous les sous-ensembles de dimensions possibles et de rechercher le sousensemble qui satisfait la problématique traitée. Cependant, le fait d\u0027énumérer (rechercher) toutes les combinaisons possibles est un problème NP-difficile (Narenda et Fukunaga, 1977). Parmi les solutions proposées pour ce problème, on retrouve la réduction de dimensions (combinaison de dimensions, généralement linéaire) et la sélection de dimensions (on n\u0027utilise qu\u0027un sous-ensemble des dimensions originales). L\u0027avantage de cette dernière solution est que nous ne perdons pas l\u0027information que pourrait apporter la dimension, car elle est considérée individuellement non en combinaison (linéaire) avec d\u0027autres dimensions. Les techniques de sélection de dimensions consistent donc à réduire l\u0027ensemble des dimensions considérées. L\u0027objectif est de réduire la complexité, augmenter la précision de la prédiction et/ou réduire le temps de traitement des données, en sélectionnant le sousensemble de dimensions de taille minimale (Dash et al., 1997). L\u0027étude du problème de sélection de dimensions se justifie facilement par le fait qu\u0027une recherche exacte a un coût exponentiel en temps de calcul et en espace mémoire. En effet, la sélection d\u0027un sousensemble de dimensions demanderait l\u0027exploration de tout l\u0027espace de recherche. Pour |D| dimensions, la recherche exhaustive consiste à explorer 2  \n?\n. Lorsque l\u0027on s\u0027attaque à des problèmes réels, il faut se résoudre à un 0 s \u003d compromis entre la qualité des solutions obtenues et le temps de calcul utilisé. Au milieu des années 1970 sont apparues des méthodes qui supervisent l\u0027évolution de solutions fournies par des heuristiques. Ces méthodes assurent un compromis entre diversification (quand il est possible de déterminer que la recherche se concentre sur de mauvaises zones de l\u0027espace de recherche) et intensification (on recherche les meilleures solutions dans la région de l\u0027espace de recherche en cours d\u0027analyse). Ces algorithmes ont été appelés métaheuristiques et ont pour objectif de trouver des solutions dont la qualité est au-delà de ce qu\u0027il aurait été possible de réaliser avec une simple heuristique (Jourdan, 2003). Dans cet article, nous proposons un algorithme génétique pour le choix du sous-ensemble de dimensions à retenir.\nPar ailleurs nous souhaitons donner un rôle plus important à l\u0027utilisateur dans le processus de recherche et de sélection de l\u0027algorithme génétique, pour cela nous avons choisi d\u0027utiliser un algorithme génétique interactif. Nous présentons donc une nouvelle méthode interactive, proposant elle-même des solutions potentielles à l\u0027utilisateur. Les solutions de l\u0027algorithme génétique se présentent sous forme de sous-ensembles de dimensions. Puisque le nombre de dimensions utilisé est faible, on peut ensuite visualiser les éléments de l\u0027ensemble de données sur ces sous-espaces de dimensions (à l\u0027aide de matrices de scatter-plot (Carr et al., 1987) ou de coordonnées parallèles (Inselberg, 1985)) pour permettre à l\u0027expert d\u0027interpréter les résultats obtenus. Nous présentons donc des visualisations d\u0027un ensemble de données projeté sur quelques sous-ensembles de dimensions à l\u0027utilisateur, ce dernier pourra lui même juger de la pertinence de la visualisation présentée selon ses objectifs (repérer les dimensions les plus pertinentes pour la détection d\u0027outlier). Pour cela, nous avons choisi d\u0027utiliser un algorithme génétique interactif (AGI). D\u0027une manière générale, cet algorithme fonctionne de la façon suivante : une première évaluation automatique se fait à l\u0027aide d\u0027un critère d\u0027évaluation des sous-espaces de dimensions pour la détection d\u0027outlier basé sur les distances, les données sont ensuite visualisées et présentées graphiquement à l\u0027utilisateur pour une seconde évaluation visuelle. Ce dernier choisit celles qui lui semblent les plus pertinentes. Les caractéristiques visuelles des sous-espaces de données sélectionnés sont prises en compte par l\u0027algorithme pour la génération suivante de sous-espaces, qui sont à nouveau présentés à l\u0027utilisateur et ainsi de suite jusqu\u0027à ce que la recombinaison des caractéristiques permette de générer une projection visuelle de données complètement satisfaisante pour l\u0027utilisateur. Un des avantages de cette méthode est de faire collaborer deux méthodes, automatique et visuelle. La première automatique, à l\u0027aide des critères d\u0027évaluation permet d\u0027éliminer les solutions redondantes ou bruitées et la seconde visuelle et interactive permet à l\u0027utilisateur de participer au processus de recherche et d\u0027aborder un aspect d\u0027évaluation des solutions présentées sous forme visuelle qui représente justement un nouveau domaine de recherche. Un autre avantage est que la méthode s\u0027adresse plus particulièrement au spécialiste des données qui peut utiliser les connaissances du domaine pour l\u0027interprétation visuelle des résultats tout au long du processus de fouille et ainsi apporter un aspect d\u0027aide à la décision. La méthode lui permet d\u0027étiqueter les éléments atypiques, par exemple est-ce que ce sont des erreurs ou simplement des individus différents de la masse. Nous détaillons dans une première partie notre algorithme puis commentons les résultats obtenus sur quelques ensembles de données, nous essayerons ensuite d\u0027interpréter visuellement les résultats obtenus, puis nous terminons par la conclusion et les travaux futurs.\n2 Algorithme : Viz-IGA Face au problème de la prise en compte des préférences de l\u0027utilisateur, des auteurs ont montré comment ce dernier pouvait sélectionner lui même directement les solutions qui le satisfont le plus, sans passer par une fonction automatique parfois impossible à définir (Takagi, 2001), (Hayashida et Takagi, 2000). Une nouvelle catégorie d\u0027algorithmes génétiques est née, connue sous le nom d\u0027Algorithmes Génétiques Interactifs. Ces algorithmes génétiques interactifs (AGIs) permettent ainsi des applications nouvelles comme l\u0027obtention de belles images de synthèse ou de sons polyphoniques. Dans ces applications l\u0027utilisateur note selon ses critères les individus qui représentent des images (ou des sons) et l\u0027AGI fait évoluer les individus selon les préférences de l\u0027utilisateur. Les algorithmes génétiques interactifs (AGIs) sont une extension des AGs dans lesquels a lieu une interaction entre la méthode de recherche et l\u0027utilisateur, ce dernier guidant la méthode vers les solutions ayant les caractéristiques qu\u0027il préfère. L\u0027algorithme génétique standard doit être modifié comme suit : les étapes d\u0027évaluation et de sélection automatique des individus sont remplacées par une présentation des individus à l\u0027utilisateur qui sélectionne en un certain nombre. Cela implique notamment de limiter la population à un petit nombre d\u0027individus. Les principales conditions d\u0027application des AGIs citées précédemment sont particulièrement intéressantes dans notre cas d\u0027évaluation visuelle des sous-espaces de dimensions sélectionnés. En effet, l\u0027interprétation visuelle des résultats obtenus est importante pour la détection d\u0027outlier et valider des sous-espaces qui présentent mieux les solutions attendues est aussi une étape importante dans le processus de recherche de sous-ensembles de dimensions. L\u0027utilisateur peut ainsi intervenir dans le processus de recherche des sousespaces de dimensions pertinents.\nInitialisation\nNous considérons que l\u0027utilisateur veut avoir des représentations graphiques de données dans un sous-espace de dimensions sur lesquels il peut voir des outliers facilement détectables. Le but recherché est d\u0027aider l\u0027utilisateur à comprendre et interpréter ses données à travers les résultats de l\u0027algorithme. Pour cela, on va lui proposer des représentations graphiques en k-D des données avec une des méthodes de visualisation de données (coordonnées parallèles (Inselberg, 1985), matrices de scatter-plot (Carr et al., 1987) ou star plot (Card et al., 1999)).\nLe choix de k, de l\u0027ensemble de données à traiter et de la méthode de visualisation revient à l\u0027utilisateur. Nous proposons ces méthodes de visualisation car elles permettent à l\u0027utilisateur d\u0027interagir avec l\u0027ensemble des données sous la forme d\u0027une série de projections, l\u0027utilisateur peut voir la pertinence d\u0027une dimension et le comportement des éléments de l\u0027ensemble de données.\nReprésentation de l\u0027individu et opérateurs génétiques\nUn individu est donc un sous-ensemble de dimensions représenté par une combinaison d\u0027axes (Axe 1 , Axe 2 , …, Axe k ), ce sous-ensemble étant sélectionné par un algorithme génétique (Boudjeloud et Poulet, 2005). Le codage choisi consiste à fixer un nombre s (taille du sous-ensemble de dimensions à sélectionner), ainsi un individu de l\u0027AG (ou de l\u0027AGI) représente une combinaison possible de s dimensions. Ce type de codage permet une meilleure interactivité avec les dimensions. La taille s est un paramètre d\u0027entrée de l\u0027algorithme génétique. Pour notre problème nous nous basons sur des petites tailles pour faciliter l\u0027interprétation visuelle des résultats (typiquement inférieur à 10). L\u0027opérateur de croisement échange des sous-groupes de dimensions des individus parents, en respectant la contrainte de non présence de clones (les individus qui ont le même sous-ensemble de dimensions, présentées dans un ordre différent ou pas, sont interdits dans notre algorithme de même que des dimensions identiques dans un même individu). L\u0027opérateur de mutation échange aléatoirement un gène en respectant les mêmes conditions. Nous avons opté pour un point de coupe \"optimisé\" (Boudjeloud et Poulet, 2005), où l\u0027on détermine dans ce cas le meilleur point avant d\u0027opérer la coupe, ce qui implique une évaluation de chaque individu issu de chaque coupe possible.\nEvaluation visuelle semi-interactive\nPour que l\u0027utilisateur puisse évaluer visuellement les individus de la population et avoir des représentations visuelles de sous-espaces de données sur lesquelles il peut voir des outliers facilement détectables, nous présentons les individus à l\u0027écran (figure 1-a) en faisant une première évaluation automatique à l\u0027aide d\u0027un critère d\u0027évaluation à base de distance (Boudjeloud et Poulet, 2005). Neuf individus choisis aléatoirement dans la population de l\u0027AG sont affichés simultanément pour ne pas surcharger l\u0027interface et faciliter les comparaisons. Pour évaluer la qualité d\u0027un individu, l\u0027expert dispose de la représentation en coordonnées parallèles qu\u0027il peut éventuellement changer en matrices de scatter-plot ou starplot, selon ses préférences ( figure 1-b). Il ne faut pas oublier que nous traitons des ensembles de données de grandes dimensions (de l\u0027ordre de dix à cent milles dimensions, cf. figure 4), notre objectif principal est d\u0027obtenir des visualisations de données pas trop surchargées, pour cela l\u0027AG traite et présente des visualisations de données avec des sous-ensembles de petite taille (de 4 à 10 pour que les visualisations restent claires). L\u0027utilisateur peut aussi agrandir ou faire un zoom sur la visualisation d\u0027un individu en particulier, changer l\u0027ordre des dimensions et changer la méthode de visualisation, (figure 1-c). Les individus peuvent être affichés individuellement avec les matrices de scatter-plots (figure 1-d), les coordonnées parallèles (figure 1-e) ou avec la méthode Star Plot (figure 1-f). Trois possibilités sont offertes à l\u0027utilisateur pour sélectionner une solution potentielle en cliquant directement sur la visualisation de l\u0027individu, en le sélectionnant sur la partie droite de l\u0027interface (figures 1-c, d, e, f)) ou en faisant un clic droit sur une visualisation en particulier, le choix de la sélectionner est alors offert à l\u0027utilisateur ( figure 1-b). Une fois les sélections effectuées les solutions apparaissent de couleurs différentes, comme sur l\u0027exemple de la figure (1-a)  \nDescription des étapes\nNotre objectif principal est d\u0027obtenir des visualisations de données significatives et pas trop surchargées, il est préférable de fixer s à de petites tailles (\u003c10 pour que les visualisations restent claires). Nous avons choisi de représenter les données à l\u0027aide des coordonnées parallèles (Inselberg, 1985) et des matrices de scatter-plot (Carr et al., 1987), cependant, ceci n\u0027est pas figé, on pourra remplacer ou introduire d\u0027autres méthodes de visualisation de données, nous avons notamment la méthode Star Plot (Card et al., 1999) dans les exemples présentés. Nous utilisons un algorithme génétique pour la recherche de sous-ensembles de dimensions pertinentes. L\u0027interaction avec l\u0027utilisateur intervient sur certaines générations afin de ne pas faire converger l\u0027AG trop rapidement. Nous faisons intervenir l\u0027utilisateur dans le processus de recherche dans deux étapes : l\u0027évaluation et la sélection.\nPopulation initiale : les individus de l\u0027algorithme génétique représentent des sous-espaces de dimensions constitués à partir des dimensions décrivant l\u0027ensemble des données. Une fois la population de départ prête, nous l\u0027évaluons une première fois à l\u0027aide d\u0027un critère d\u0027évaluation à base de distance décrit dans (Boudjeloud et Poulet, 2005).\nEvaluation automatique : vu le nombre important de combinaisons de dimensions possibles, il est nécessaire de faire une sélection de dimensions en utilisant ce critère de validité des sous-espaces avant de les présenter à l\u0027utilisateur. Une fois cette présélection automatique faite, l\u0027utilisateur peut intervenir interactivement selon que la visualisation générée le satisfait ou pas.\nEvaluation interactive : une fois la population évaluée et triée selon les différents objectifs, nous présentons à l\u0027utilisateur 9 visualisations. Ces visualisations représentent la projection des données dans des sous-ensembles de dimensions choisis aléatoirement dans la population de l\u0027AG (un individu de l\u0027AG représente un sous-ensemble de dimensions, 9 individus sont choisi aléatoirement et sont présentés visuellement). Notre choix s\u0027est fixé à 9 représentations pour ne pas surcharger l\u0027interface. Les solutions sont représentées par des projections en coordonnées parallèles ou d\u0027autres méthodes de visualisation selon le choix de l\u0027utilisateur ( figure 1-b). Nous opérons un croisement et une mutation, puis, toutes les 100 générations, nous proposons à l\u0027utilisateur d\u0027autres visualisations, il peut en sélectionner certaines s\u0027il le souhaite en cliquant dessus, selon qu\u0027elles sont assez significatives pour lui. Pendant ces 100 générations l\u0027AG travaille tout seul sans intervention de l\u0027utilisateur. L\u0027algorithme prend en compte le choix de l\u0027utilisateur pour les prochaines générations dans le processus de recherche de deux manières. Nous avons choisi 100 générations pour que l\u0027AG puisse éliminer les solutions redondantes, les moins intéressantes automatiquement et éviter d\u0027avoir toujours les mêmes solutions qui seront affichées (présentées à l\u0027utilisateur).\nSélection interactive : les solutions sélectionnées par l\u0027utilisateur seront stockées dans une mémoire E\u0027 que nous faisons intervenir dans deux étapes de l\u0027algorithme, la première étant la reproduction, la seconde pour remédier à la stagnation de la recherche.\nReproduction : nous faisons intervenir les solutions de E\u0027 (sélectionnées par l\u0027utilisateur) dans la reproduction de la façon suivante : chaque nouvel enfant généré aura une partie des gènes d\u0027un parent issu de E\u0027 et une partie des gènes d\u0027un parent issu d\u0027une sélection par tournoi où l\u0027on sélectionne aléatoirement et uniformément 2 individus en ne gardant que le meilleur.\nStagnation : dès que la solution stagne (ne s\u0027améliore pas pendant un certain nombre de générations) nous générons de nouvelles solutions à partir des solutions de E\u0027 (sélectionnées par l\u0027utilisateur) en les faisant intervenir dans le processus de mutation. Lorsqu\u0027un gène doit être muté, il sera changé par un gène d\u0027un individu de E\u0027 (l\u0027emplacement du gène et l\u0027individu de E\u0027 sont aléatoires).\nL\u0027espace de recherche étant grand, il est important d\u0027avoir une grande capacité d\u0027exploration. Ces mécanismes permettent de maintenir une diversité dans la population en introduisant à certains moments de nouveaux individus. Ils permettent aussi d\u0027éviter une convergence prématurée ou une stagnation des solutions. Nous utilisons ces mécanismes lorsque le meilleur individu est le même durant un certain nombre de mutations ? mut et de croisements ? croi (ces deux paramètres sont exprimés en pourcentage de la taille de la population). Alors, tous les individus de la population qui ont une valeur d\u0027évaluation en dessous de la moyenne de la population (sous la médiane) sont remplacés par de nouveaux individus générés en respectant les conditions de non-présence de clones et de dimensions identiques dans un même individu. La différence entre notre AGI et les autres AGIs existants est que nous faisons coopérer les deux méthodes visuelle et automatique et que nous faisons intervenir l\u0027utilisateur dans deux processus de l\u0027AG : l\u0027évaluation et la sélection.\nRésultats et interprétation\nLe système a été implémenté sous Windows 2000 dans un environnement très intuitif pour l\u0027utilisateur. Les différentes possibilités de Viz-IGA ont été testées sur des ensembles de données du Kent Ridge Biomedical Dataset Repository (Jinyan et Huiquing, 2002). Les différentes figures (3a, 3b, 3c, 3d) sont créées à partir d\u0027un exemple de détection d\u0027outlier sur l\u0027ensemble de données Breast cancer, Lung cancer, Ovarian et MLL. Notre méthode permet de retrouver des outliers sur des sous-espaces de dimensions identiques à ceux trouvés sur l\u0027ensemble total des données détectés par LOCI (Papadimitriou et al., 2003) un algorithme récent, qui détecte les éléments outliers de l\u0027ensemble des données que nous avons testé (Boudjeloud et Poulet, 2005). De plus, Viz-IGA permet de mettre en évidence les dimensions prépondérantes et souligner la pertinence et l\u0027intérêt de certaines d\u0027entre elles pour la détection d\u0027outlier. Il permet d\u0027isoler et de voir correctement l\u0027élément outlier. Le nombre de dimensions peut être réduit jusqu\u0027à un facteur 1000 en moyenne sans perte significative d\u0027information, puisque nous arrivons à retrouver le même outlier dans les sousespaces de dimensions que sur l\u0027ensemble total des données. Il faut entre 2 et 10 minutes pour arriver à la visualisation la plus pertinente pour les problèmes mentionnés précédemment, soit environ de 4 à 20 interventions de l\u0027utilisateur. Au-delà de 15 minutes, l\u0027expérience montre que l\u0027utilisateur se lasse et commence à se fatiguer. Une autre difficulté qui peut lasser l\u0027utilisateur est la stagnation des solutions. Il peut en effet avoir plusieurs fois les mêmes solutions proposées. Il peut par exemple penser qu\u0027il a obtenu la solution finale alors que c\u0027est juste un optimum local. C\u0027est le cas par exemple lors des tests effectués sur l\u0027ensemble de données Colon où à chaque intervention de l\u0027utilisateur on voit bien sur la figure 2 l\u0027amélioration de la courbe et la convergence de l\u0027algorithme en comparaison avec l\u0027AG (Boudjeloud et Poulet, 2005) aux mêmes générations (Viz-IGA converge en moins de générations que l\u0027AG). Cependant, on voit sur la courbe de VIz-IGA des moments de stagnation, par exemple entre les générations 800 et 1000, les générations 1200 et 1400 (il y a deux niveaux de stagnation) et les dernières générations à partir de la génération 1600. Dans ces cas là, il peut arriver que les mêmes solutions soient présentées à l\u0027utilisateur plusieurs fois à la suite. \nModélisation de l\u0027expertise\nLa visualisation des résultats obtenus sur quelques ensembles de données (figure 3) montre bien que les points détectés sont éloignés et présentent un comportement atypique par rapport au reste des données, néanmoins nous ne pouvons fournir plus d\u0027explication sur le type des points détectés par notre algorithme (par exemple erreur ou \"outlier réel\"). En effet, dans le cas de valeurs extrêmes on ne sait pas dire si cette valeur est une valeur possible ou non. Seul l\u0027expert des données peut répondre à cette question. Dans le cas où le point détecté est une erreur on l\u0027élimine de l\u0027ensemble des données et dans le cas contraire on le garde dans les données car il peut représenter à lui seul des informations importantes. Un des moyens de combler cette lacune est de créer un modèle des données permettant de qualifier les éléments détectés comme outliers ou erreurs. Ainsi, étant donné un nouvel élément introduit dans l\u0027ensemble des données, nous pourrons utiliser le modèle pour prédire son état : outlier, erreur ou donnée normale.\nFIG. 3 -Visualisation des résultats sur les différents ensembles de données.\nNous proposons donc de construire un modèle de l\u0027expertise de l\u0027expert. Celui-ci doit tout d\u0027abord étiqueter les éléments qui ont été détectés comme étant outliers (on peut supposer qu\u0027il n\u0027y a que 2 types d\u0027éléments : les erreurs et les \"vrais outliers\"). A partir de cet ensemble de données étiquetées, on utilise un algorithme de classification supervisée (par exemple un algorithme d\u0027induction d\u0027arbre de décision) pour construire un modèle de l\u0027expertise du spécialiste des données. Les nouveaux éléments outliers seront alors analysés avec le modèle construit et la présence de l\u0027expert n\u0027est plus indispensable pour qualifier ces outliers.\nConstruction du modèle\nConcernant la partie expertise de la détection d\u0027outlier, nous n\u0027avons pas pu avoir accès à un ensemble de données avec un spécialiste pouvant étiqueter les éléments détectés. Nous avons donc décidé de le faire à partir de l\u0027ensemble de données Colon Tumor (2000 dimensions, 62 éléments) de (Jinyan et al., 2002). Le nouvel ensemble de données est créé en rajoutant des éléments que nous avons étiqueté nous même d\u0027erreur ou d\u0027outlier. Par exemple des éléments qui présentent des valeurs extrêmes sont des erreurs et ceux qui présentent un comportement différent par rapport au reste des données sont des outliers. Nous obtenons donc l\u0027ensemble Colon plus quelques nouveaux éléments. L\u0027ensemble de données Colon a 62 éléments, 5 ont été détectés comme outlier par notre algorithme, nous les étiquetons comme tel, nous créons leurs clones (5), ces clones ont les mêmes valeurs que les originaux sur l\u0027ensemble des dimensions mais sont présentés dans un ordre différant en permutant les valeurs de certaines dimensions. Nous rajoutons au nouvel ensemble de données 10 éléments avec plusieurs valeurs extrêmes qui vont être considérés comme erreurs. Nous obtenons un ensemble de données que nous avons appelé \"Colon-Bis\" de 2000 dimensions, 77 éléments et trois classes :\nClasse 1 : données correctes (57 éléments). Classe 2 : outliers (10 éléments). Classe 3 : erreurs (10 éléments). L\u0027ensemble de données crée n\u0027a qu\u0027un petit nombre d\u0027éléments erreurs et d\u0027éléments outliers, s\u0027ils étaient nombreux, ils ne seraient plus considérés comme tels. Nous avons pris pour l\u0027ensemble d\u0027apprentissage 67 éléments choisis aléatoirement et les 10 éléments restants pour l\u0027ensemble de test. Reste à choisir un algorithme d\u0027apprentissage qui pourra prédire la classe des nouveaux individus. De nombreux algorithmes d\u0027apprentissage automatique peuvent être utilisés, nous avons choisi comme algorithmes les k-PPV (Cover et Hart, 1967), C4.5 (Quinlan, 1993), CART (Breiman et al., 1984) et LibSVM (Fan et al., 2005). Nous avons effectué des tests dont nous présentons les résultats dans le tableau 1. Ces résultats sont très satisfaisants, nous arrivons à prédire les nouveaux éléments avec un taux de précision de 100% avec le modèle établi par LibSVM. Une fois le modèle établi, le besoin d\u0027étiqueter par le spécialiste des données n\u0027est plus nécessaire. \nConclusion\nNous avons présenté un algorithme génétique semi-interactif pour la sélection de dimensions appliqué à la détection d\u0027outlier. Nous avons introduit une nouvelle représentation de l\u0027individu de l\u0027algorithme génétique. Notre choix s\u0027est fixé sur des petites tailles de sous-ensembles de dimensions pour faciliter l\u0027interprétation visuelle des résultats et souligner la pertinence des dimensions pour chacune des applications, ajoutant ainsi un aspect d\u0027aide à la décision. Cependant, l\u0027utilisateur est libre de fixer la taille des sousensembles de dimensions. Il peut aussi intervenir sur le choix de la méthode visuelle utilisée et sur l\u0027ordre des dimensions dans la visualisation proposée. Notre algorithme nous permet la détection d\u0027outliers dans des ensembles de données ayant un grand nombre de dimensions en n\u0027utilisant qu\u0027un sous-ensemble de dimensions de l\u0027ensemble initial. Puisque le nombre de dimensions utilisé est faible, on peut ensuite visualiser ces éléments (à l\u0027aide de matrices de scatter-plot ou de coordonnées parallèles) pour permettre à l\u0027utilisateur de choisir les solutions qui lui paraissent pertinentes. Elles sont alors utilisées pour générer et visualiser d\u0027autres solutions et aussi pour expliquer et valider les résultats obtenus. Il ne faut pas oublier que l\u0027on travaille sur des données de grandes dimensions. Cette étape n\u0027est possible que parce que nous n\u0027utilisons qu\u0027un sous-ensemble restreint de dimensions de l\u0027ensemble de données initial. Cette interprétation des résultats serait absolument impossible en considérant l\u0027ensemble des dimensions comme le montre la figure 4.\nFIG. 4 -Visualisation de quelques centaines d\u0027attributs de l\u0027ensemble de données Colon Tumor.\nNous pensons étendre nos applications à des données symboliques et aussi améliorer notre méthode pour optimiser l\u0027ordre des dimensions dans les visualisations en utilisant des critères de validité et étendre la méthode au clustering. Viz-IGA a permis de montrer que l\u0027on peut gagner beaucoup en augmentant l\u0027interaction entre l\u0027expert du domaine et son outil de fouille de données. Pour finir, l\u0027étude des AGI est certainement prometteuse dans d\u0027autres domaines. Ces algorithmes proposent une interaction très simple et efficace pour un utilisateur non informaticien, ce qui peut leur assurer un certain succès dans les applications nécessitant une interaction homme/machine. Nous avons fait coopérer les méthodes automatiques et les méthodes de visualisation de données sur deux aspects, l\u0027interaction avec l\u0027utilisateur dans le processus de recherche en le faisant participer dans la sélection et l\u0027évaluation des solutions proposées par l\u0027AG et dans l\u0027interprétation et la qualification des éléments détectés comme outlier à travers le modèle d\u0027expertise du spécialiste des données. Nous avons proposé une partie expertise, à l\u0027aide des visualisations présentées l\u0027expert des données peut qualifier les outliers détectés (par exemple en deux classes : erreur ou élément significativement différent de la masse). Il ne faut pas oublier que l\u0027on travaille sur des fichiers de grandes tailles. Cette étape n\u0027est possible que parce que nous n\u0027utilisons qu\u0027un sous ensemble restreint de dimensions de l\u0027ensemble de données initial. Cette qualification des outliers serait absolument impossible en considérant l\u0027ensemble des dimensions comme l\u0027illustre très bien l\u0027exemple de la figure 4 où l\u0027on ne peut détecter aucune information à propos des éléments de l\u0027ensemble de données ou des dimensions. Une fois la qualification effectuée, nous utilisons un algorithme d\u0027apprentissage pour créer un modèle de l\u0027expertise du spécialiste des données. Les nouveaux outliers peuvent alors être qualifiés par le modèle construit sans la présence de l\u0027expert des données. Cette étape souligne l\u0027importance de la visualisation de données pour l\u0027interprétation des résultats et son apport pour l\u0027aide à la décision. Les tests effectués pour l\u0027expertise ont été effectués sur un ensemble de données artificiel crée par nos soins car nous n\u0027avons pas pu avoir accès à un ensemble de données et un spécialiste pouvant qualifier les éléments détectés d\u0027erreur ou outlier réel. Nous avons obtenu des résultats satisfaisants par ce premier travail qui nous a permis de faire participer l\u0027utilisateur dans le processus de recherche de sous-ensembles de dimensions pertinents pour détecter, interpréter visuellement et qualifier des éléments outliers.\n"
  },
  {
    "id": "999",
    "text": "Introduction\nLes ontologies ont été créées dans le but de conceptualiser et partager des connaissances de manière structurée (Gruber, 1993). Leur usage en gestion des connaissances s\u0027amplifie avec l\u0027essor du Web sémantique. En effet, les ontologies ont la vertu de se traduire sous des formes très variées depuis de simples taxonomies comme les systèmes catégories (Yahoo, OpenDirectory), en passant par des systèmes de métadonnées interopérables (Dublin Core Metadata initiative) et allant jusqu\u0027aux ontologies lourdes décrivant de véritables théories logiques. Notamment, on trouve des ontologies différentes portant sur le même domaine. Il s\u0027avère donc nécessaire de disposer de techniques pour relier ces ontologies. Dans cette optique, l\u0027alignement vise à trouver des relations entre deux ontologies (entre les classes, les relations, les propriétés...).\nDans la littérature, de nombreux travaux traitent de méthodes d\u0027alignement. Ces approches reposent sur des techniques très différentes (Kalfoglou et Schorlemmer, 2003) comme l\u0027apprentissage bayésien des probabilités jointes entre concepts (Doan et al., 2004), la classification conceptuelle (Stumme et Maedche, 2001), la fusion de schéma de bases de données (Madhavan et al., 2001), les modèles logiques en graphe conceptuels (Fürst et Trichet, 2005), la recherche de morphismes entre graphes représentant les ontologies (Melnik et al., 2002). La distinction entre ces travaux peut être faite au niveau des méthodes d\u0027alignement utilisées pour la comparaison. La classification proposée par (Euzenat et Valtchev, 2003), distingue quatre familles de méthodes : (1) les méthodes terminologiques basées sur des mesures de similarité entre chaînes de caractères ou faisant intervenir une ressource terminologique externe ; (2) les méthodes structurelles comparant, d\u0027une part, deux concepts à partir de mesures de similarité entre les constituants (attributs, propriétés) des concepts ou à partir de leur position respective dans leur hiérarchie (Noy et Musen, 2000) ; (3) les méthodes extensionnelles comparant les concepts à partir de leur ensemble d\u0027instances respectif (Doan et al., 2004) ; (4) les mé-thodes sémantiques basées sur un modèle sémantique théorique utilisé pour la comparaison des concepts (Giunchiglia et al., 2004), (Fürst et Trichet, 2005).\nLa plupart de ces travaux utilisent des relations symétriques de similarité. Pourtant, d\u0027autres types de relations asymétriques peuvent être utilisées dans le but d\u0027enrichir l\u0027alignement produit. Par exemple, la recherche d\u0027implications (généralisations) permet de trouver les concepts équivalents (exemple : si auto ? voiture et voiture ? auto alors auto ? voiture), mais elle permet aussi de découvrir si un concept est plus général (ou plus plus spécifique) qu\u0027un autre. Parmi les méthodes prenant en compte la relation d\u0027implication, nous pouvons citer S-MATCH (Giunchiglia et al., 2004). Cette dernière évalue entre autre des relations d\u0027équiva-lence et d\u0027implication en s\u0027appuyant sur un thésaurus (Wordnet). L\u0027objectif de notre papier est de proposer une méthode extensionnelle d\u0027alignement basée sur la découverte des relations asymétriques d\u0027implication entre deux ontologies. Nous nous restreignons à des ontologies constituées d\u0027une hiérarchie de concepts dont les concepts sont associés à des documents textuels partageant un vocabulaire commun.\nNotre approche est divisée en deux parties consécutives qui utilisent toutes deux le modèle probabiliste d\u0027écart à l\u0027indépendance appelé intensité d\u0027implication (Gras, 1979;Gras et al., 1996) : -L\u0027extraction des termes pertinents pour chacune des deux hiérarchies. Ce processus permet d\u0027extraire, puis de sélectionner, à partir des documents associés aux hiérarchies, un ensemble de termes pertinents pour chaque concept. -L\u0027extraction d\u0027implications entre les concepts sous forme de règles d\u0027association (Agrawal et al., 1993). Nous prenons en compte la relation de spécialisation de chaque hiérar-chie afin d\u0027extraire les règles les plus générales et ainsi réduire la redondance.\nNous présenterons dans une première section, la méthodologie suivie ainsi que la formalisation des données. Ensuite, nous préciserons les détails de notre approche en commençant par la phase d\u0027analyse et de sélection des termes pertinents, suivie de la découverte de règles entre les deux hiérarchies. Finalement, une évaluation expérimentale nous permettra d\u0027analyser et d\u0027évaluer les performances de notre système.\n-152 -RNTI-E-6 \nMéthodologie et formalisation\nNous avons en entrée du processus, deux hiérarchies conceptuelles (figure 1). Chaque hié-rarchie est composée d\u0027un ensemble de concepts C structurés par une relation d\u0027ordre partiel is ? a (notée par la suite ?). Des parties d\u0027un ensemble de documents D sont associées aux concepts de la hiérarchie. Nous représentons une hiérarchie par un n-uplet O \u003d (C, ?, D, ? 0 ), où ? 0 ? C × D est une relation qui associe les concepts aux documents. ? 0 (c) désigne l\u0027ensemble des documents associés au concept c. Chaque document d est composé d\u0027un ensemble de termes {t|t apparait dans d}. Nous appellerons T , l\u0027union des ensembles de termes contenus dans les documents et ? ? T × D, la relation associant les termes aux documents. Nous noterons ?(t), l\u0027ensemble des documents qui contiennent le terme t. A partir de la relation d\u0027ordre partiel portant sur les concepts, nous définissons la relation :\nUne première étape (figure 1) d\u0027extraction, de sélection et d\u0027association des termes aux concepts, nous permet de représenter une hiérarchie O par le quadruplé O \u003d (C, ?, T , ? 0 ), où T ? T représente l\u0027ensemble des termes sélectionnés, et ? 0 ? C × T est une relation qui associe à chaque concept, ses termes significatifs. Nous notons ? 0 (c), l\u0027ensemble des termes significatifs du concept c. A partir de la relation d\u0027ordre partiel ?, nous définissons la relation suivante :\nLa deuxième étape concerne la découverte d\u0027implications significatives entre les concepts issus des hiérarchies O\n. Pour cela, nous nous appuyons sur le modèle des règles d\u0027association (Agrawal et al., 1993). L\u0027extraction des règles est réalisée sur l\u0027ensemble des termes communs aux deux hiérarchies. Ainsi, nous définissons  O\u0027\nFIG. 2 -prétraitements linguistiques et extraction des termes pertinents\n. La règle A ? B signifie que les termes significatifs du concept A ont tendance à être significatifs du concept B.\nExtraction et sélection des termes significatifs\nNotre objectif consiste à extraire un ensemble de termes significatifs pour chacun des concepts. L\u0027idée principale de ce processus est la suivante : Un terme t sera significatif d\u0027un concept c si il existe relativement peu de documents contenant le terme t qui ne sont pas associés au concept c. Nous choisissons d\u0027associer le terme t au concept c si la règle d\u0027association t ? c est significative selon l\u0027intensité d\u0027implication.\nAfin d\u0027extraire les règles t ? c, nous traduisons les relations ? et ? par la table a, figure 2. Ainsi, chaque document est représenté avec d\u0027une part les concepts auxquels il est associé, et d\u0027autre part les termes qu\u0027il contient. L\u0027ensemble des termes T 0 est constitué de verbes et de termes binaires (termes composés de deux mots significatifs). L\u0027extraction de ces termes binaires se justifie par le fait qu\u0027ils sont plus porteurs d\u0027information et donc moins ambigus que de simples mots. L\u0027acquisition des termes binaires est réalisée par le logiciel ACABIT (Daille, 2003) à partir des textes préalablement étiquetés (grammaticalement) et lemmatisés par la suite logicielle MontyLingua (Liu, 2004).\nLa deuxième étape (étape 2, figure 2) consiste à évaluer toutes les règles d\u0027association binaires t ? c, avec t ? T 0 et c ? C, afin de constituer pour chaque concept c, un ensemble de termes significatifs ? 0 (c) défini par :\noù n t?c \u003d card(I(t) ? ?(c)) représente le nombre observé de documents contenant le terme t qui ne sont pas associés au concept c et N t?c représente le nombre attendu (sous hypothèse d\u0027indépendance des descriptions t et c) de documents contenant le terme t qui ne sont pas associés au c. Comme les phénomènes étudiés sont rares, nous modélisons la variable aléatoire N t?c par une loi de Poisson de paramètre ? \u003d n t .n c /n où n t est le nombre de documents contenant le terme t, n c , le nombre de documents non associés au concept c et n le nombre total de documents.\nNotons que les fréquences des termes dans les documents ne sont pas prises en compte, nous nous intéressons seulement à la présence ou absence des termes dans les documents.\nDécouverte d\u0027implications significatives entre concepts\nCritères de sélection d\u0027une implication significative\nA partir de deux hiérarchies O 1 et O 2 , cette deuxième partie a pour objectif de découvrir des implications sous forme de règles binaires entre les concepts des deux structures. Pour cela, nous nous inspirons de la méthode de découverte de règles d\u0027association généralisées proposée par (Srikant et Agrawal, 1995). \nFIG. 3 -Implication entre concepts\nLa significativité d\u0027une règle est exprimée par la valeur d\u0027intensité d\u0027implication et par le caractère spécifique de sa conclusion combiné à la généralité de sa prémisse. Ainsi pour A ? C 1 et B ? C 2 , une règle A ? B sera dite \"significative\" si :\nLe deuxième critère traduit la capacité d\u0027une règle à générer d\u0027autres règles. Il permet ainsi de réduire la redondance dans l\u0027ensemble des règles extraites. En effet, à partir de la règle A ? B, nous pouvons déduire toutes les règles de la forme :\nAinsi, nous dirons que la règle A ? B est génératrice de l\u0027ensemble des règles de type X ? Y . Par exemple, sur la figure 3, la règle A2 ? B4 permet de générer les règles A2 ? B1, A4 ? B4, A5 ? B4, A4 ? B1 et A5 ? B1.\nContrairement à (Srikant et Agrawal, 1995) qui s\u0027appuie sur les mesures de support et de confiance, nous préférons utiliser la mesure d\u0027intensité d\u0027implication ?. La confiance et le support ne sont pas adaptés pour découvrir des règles génératrices car ils favorisent les règles ayant des conclusions générales. En effet, la confiance d\u0027une règle A ? B est plus grande ou égale à la confiance d\u0027une règle de la forme A ? Z avec Z ? 2 B. Par contre, ? prend en compte la taille de la conclusion et tend vers 0 quand la conclusion devient trop générale : par exemple, sur la figure 3, ?A ? C 1 , ?(A ? B1) \u003d 0 car ? 1?2 (B1) \u003d T 1?2 .\nAlgorithmes\nLa prise en compte de la relation d\u0027ordre partiel entre les concepts des hiérarchies permet d\u0027optimiser la phase de sélection des implications significatives. En effet, une recherche descendante de règles (du haut de la hiérarchie vers le bas) nous permet d\u0027éviter l\u0027évaluation Le premier algorithme (figure 4) prend en entrée un concept A de la hiérarchie O 1 , et un ensemble de concepts B courant ? C 2 de O 2 . Pour chacun des concepts de B courant , la recherche et la sélection des conclusions valides sont effectuées par le second algorithme (figure 5). Ensuite, la procédure est relancée récursivement sur les fils de A et une copie de l\u0027ensemble B courant mis à jour par le second algorithme. L\u0027ensemble de concepts B courant recense les sous-parties de la hiérarchie O 2 qui ne contiennent aucun concept figurant dans les conclusions des règles sélectionnées lors des récursions précédentes. Cette liste permet d\u0027éviter d\u0027évaluer des règles ne satisfaisant pas le critère 2 explicité dans la section 4.1.\nLe deuxième algorithme (figure 5) prend en entrée un concept A de la hiérarchie O 1 , et un concept B de O 2 . Il recherche parmi l\u0027ensemble {B x |B x ? 2 B}, un sous-ensemble de conclusions valides pour A. Une conclusion B s sera sélectionnée, si elle respecte les deux critères de la section 4.1.\nLa recherche se faisant de manière descendante dans la hiérarchie des descendants de B, nous choisissons d\u0027arrêter la recherche dans une des branches si ?(A ? B x ), est en dessous de la valeur seuil ? r et si aucune spécialisation de la conclusion ne permettra de repasser au dessus du seuil ? r . Pour cela, nous nous appuyons sur une propriété de l\u0027intensité d\u0027implication qui définit A ? B x comme étant la meilleure spécialisation de la conclusion de A ? B x .\nLors de la recherche de règles, nous ignorons les racines des hiérarchies. En effet, les concepts racines des hiérarchies sont associés à l\u0027ensemble des termes étudiés. Ainsi, la valeur d\u0027intensité d\u0027implication, ne peut être évaluée ou est nulle si un concept racine est en prémisse ou en conclusion. \nRésultats et analyse\nAfin d\u0027analyser le comportement de notre méthode de manière quantitative et qualitative, nous avons étudié dans un premier temps, l\u0027influence du choix des valeurs seuils d\u0027intensité d\u0027implication ? t et ? r sur le nombre de règles sélectionnées. Pour les deux jeux de test présen-tés ci-dessus, nous avons testé notre algorithme en faisant varier (de 0,8 à 1) les valeurs seuils pour la sélection des groupes de termes pertinents et pour l\u0027appariement de concepts.\nSur les deux graphiques de la figure 6, on peut remarquer que le choix du seuil ? t influence plus la quantité d\u0027implications extraites. Prenons l\u0027exemple de l\u0027alignement des catalogues Cornell et Washington : pour une augmentation du seuil ? t (resp. ? r ) de 0,1 unité, le nombre d\u0027implications extraite baisse en moyenne de 2,15 (resp. 1,2).\nDans un deuxième temps nous avons réalisé un test qualitatif à partir du jeu de test \"Course catalog\" et des alignements manuels fournis sur le site de A. Doan. Nous avons confronté les résultats produits par notre approche aux alignements manuels. Comme ces derniers sont de nature symétriques, nous avons procédé aux recherches d\u0027implications dans les deux sens (Cornell vers Washington, puis Washington vers Cornell). Ensuite, à partir des implications extraites, nous déduisons des relations d\u0027équivalences. Sur la figure 7, nous notons, comme pour le nombre d\u0027implications extraites (figure 6), une plus grande influence du seuil ? t par rapport au seuil ? r . Nous pouvons observer une évolution de la précision sur des bons scores (de 0,71 à 1). Le choix des deux seuils influencent quasiment de la même manière l\u0027évolution de la précision. Cependant, le rappel présente une faible moyenne de 0,29 (meilleur score égale à 0,54).\nCes scores mitigés au niveau du rappel sont justifiés, premièrement, par l\u0027existence de concepts pour lesquels il n\u0027y a aucun terme associé. Ce problème est dû à un manque de spéci-ficité du vocabulaire utilisé dans les descriptions des concepts et à une sélection trop stricte des règles terme ? concept. Ces implications entre concepts sont intéressantes même si elles ne figurent pas dans les alignements manuels. Nous pouvons aussi noter que notre méthode n\u0027est pas sensible aux noms des concepts (une approche ne faisant intervenir qu\u0027une similarité entre chaînes de caractères ne pourrait pas trouver la règle \"Cognitive Studies Program -\u003e Psychology PSYCH\"). Ainsi, notre méthode prend en compte la sémantique des concepts.\n-160 -RNTI-E-6\nConclusion\nNous avons proposé dans ce papier une approche d\u0027alignement d\u0027ontologies basée sur la découverte de relations d\u0027implication significatives entre concepts provenant de deux ontologies distinctes. Notre méthode, est décomposée en deux phases : (1) L\u0027extraction à partir des documents puis l\u0027association d\u0027un ensemble de termes pertinents pour chaque concept. (2) La découverte de règles d\u0027implication significatives entre concepts en nous appuyant sur les termes sous-jacents. Les intérêts de notre méthode sont d\u0027une part la prise en compte de la sé-mantique en utilisant des termes binaires contenus dans les documents associés aux concepts, et d\u0027autre part la recherche d\u0027implications permettant d\u0027enrichir l\u0027alignement produit. Notre démarche a été testée sur deux jeux de données réels portant respectivement sur des profils d\u0027entreprises et sur des catalogues de cours d\u0027universités. La confrontation des résultats obtenus et des alignements manuels fournis avec le jeu de test montrent tout d\u0027abord que notre approche distingue parmi les alignements manuels des relations d\u0027équivalence et des relations d\u0027implication. De plus notre méthode permet d\u0027extraire des relations pertinentes qui sont ignorées par l\u0027ensemble d\u0027alignements manuels.\nActuellement, nous n\u0027avons considéré que des hiérarchies conceptuelles associées à un corpus de documents. Nous projetons d\u0027étendre notre approche afin d\u0027exploiter la définition et la structure interne des concepts, puis de permettre la recherche d\u0027implications entre les relations.\n"
  },
  {
    "id": "1000",
    "text": "Introduction\nLes indicateurs techniques sont des fonctions des données de marché, historiques et actuelles, qui produisent un signal d\u0027achat ou de vente. Ce sont les « briques » qui permettent de construire des stratégies de « trading » en réaction aux indicateurs et en fonction de la composition du portefeuille de l\u0027investisseur.\nCertaines techniques de fouille de données permettent d\u0027attribuer une signature aux configurations de marché précédant le déclenchement d\u0027un indicateur technique. La comparaison des performances de l\u0027indicateur seul et du même indicateur précédé d\u0027une signature permet de choisir les signatures qui améliorent les performances de l\u0027indicateur. La stratégie de « trading » peut donc exploiter un filtrage pour se restreindre aux transactions qui se déclenchent après une signature et qui ont produit des gains dans le passé. Les signatures sont testées sur une période d\u0027apprentissage afin d\u0027exclure celles pour lesquelles l\u0027indicateur technique a sous performé. Les meilleures signatures sont gardées pour la période suivante, dite de validation ou de « trading » pendant laquelle l\u0027indicateur technique est pris en compte uniquement s\u0027il est précédé par une « bonne » signature. L\u0027analyse des signatures montre une remarquable stabilité temporelle, et l\u0027indicateur filtré par les « bonnes » signatures de la pé-riode d\u0027apprentissage sur performe régulièrement l\u0027indicateur seul pendant la période de validation.\nNous utilisons une technique de fouille de données classique (i.e., la recherche de motifs fréquents) pour identifier les signatures qui caractérisent les quelques jours précédant le déclenchement de l\u0027indicateur technique. Nous obtenons ainsi des règles d\u0027analyse technique spécifiques au sous-jacent, basées sur plusieurs jours et adaptées à la période considérée.\nUne première contribution de ce travail consiste à caractériser les configurations de marché afin que l\u0027extraction de motifs fréquents puisse s\u0027appliquer sur ce type de données. A notre connaissance, l\u0027idée d\u0027améliorer un indicateur technique par la méthode des signatures est également originale. Cet article ne propose pas un nième indicateur technique mais plutôt une méthodologie générale pour améliorer des stratégies quantitatives existantes.\nL\u0027amélioration des indicateurs techniques est un objectif bien différent de la prédiction des cours boursiers et a reçu moins d\u0027attention. F. Allen et R. Karjalainen ont travaillé sur la combinaison et le calibrage des seuils de déclenchement des indicateurs par la programmation génétique (Allen et Karjalainen 1999). Les travaux plus récents de Becker rapportent que les règles de « trading » générées par la programmation génétique peuvent battre la stratégie « acheter et garder » en tenant compte des frais de transaction (Becker 2003). Ces travaux concernent le paramétrage d\u0027un ensemble de règles de « trading » déjà connues et ne cherchent pas à caractériser une configuration de marché en addition d\u0027un indicateur technique. Notre approche est donc complémentaire à ce type de travaux.\nLa Section 2 présente la méthodologie appliquée et le système de « trading » réalisé est rapidement présenté en Section 3. La Section 4 présente les résultats expérimentaux obtenus sur des données réelles. La Section 5 est une brève conclusion.\nMéthodologie\nPour mettre en place la stratégie de « trading » basée sur des signatures il faut d\u0027abord dé-finir le contenu d\u0027une configuration de marché (scénario) et le transformer dans le format attendu par l\u0027algorithme d\u0027extraction des motifs utilisé (discrétisation et agrégation)..\nLa description des scénarii\nUn scénario regroupe l\u0027ensemble des données de marché historiques sur « N » jours qui constitue le domaine de l\u0027indicateur. Les indicateurs techniques sont donc des fonctions des scénarii f qui associent à un scénario une valeur dans l\u0027ensemble {Acheter, Vendre, Rien}. Par exemple, la fonction « f » pourrait être de type Acheter, si la moyenne mobile sur 2 semaines est supérieure à la moyenne mobile sur 30 jours, sinon Rien. Il s\u0027agit évidemment d\u0027un exemple hypothétique.\nComme dans la plupart des travaux dans ce domaine, chaque journée composant un scé-nario sera caractérisée par la valeur de l\u0027indicateur technique, le cours d\u0027ouverture (O) et de clôture (C) ainsi que les maxima (H) et minima (L) journaliers. Le nombre de jours « N » composant un scénario est déterminé empiriquement, et s\u0027étend généralement sur le domaine de l\u0027indicateur technique.\nDe nombreuses techniques de fouille de données travaillent sur des données discrètes et supposent un pré-traitement. Ainsi, nous voulons transformer les données de marché composant les scénarii en une représentation non numérique. L\u0027association de chaque valeur à sa qquantile1 apparaît comme un choix de discrétisation simple facilement mis en oeuvre. Les deux scénarii quantilisés associés aux journées J1 et J2 seront graphiquement représentés comme suit : La méthodologie exposée dans cet article associera une signature aux scenarii fréquents en passant par l\u0027extraction de motifs fréquents. L\u0027extraction de motifs fréquents est l\u0027une des tâches de fouille de données qui a été la plus étudiée depuis une dizaine d\u0027années et nous supposons que le lecteur est familiarisée avec cette technique. Une annexe rappelle cependant le principe algorithmique qui a été utilisé.\nUn motif f-fréquent (noté simplement fréquent dans la suite) est un sous-ensemble commun à une collection d\u0027ensembles (scénarii dans le cas présent, collection de transactions dans le cadre classique des données de ventes) qui apparaît en au moins f ensembles. L\u0027extraction des motifs fréquents intervient sur des ensembles de valeurs et il faut donc représen-ter chaque scénario sous une forme transactionnelle. On préfère souvent une représentation suffisamment riche pour pouvoir reconstituer le scénario de départ à partir d\u0027un ensemble non ordonné, i.e., une bijection. La bijection a l\u0027avantage de pouvoir retrouver la configuration de marché à partir de sa signature, et permet à un expert humain de comprendre a posteriori les règles générées par la technique de fouille de données.\nEn introduisant un système de coordonnées (x, y) comme sur la Figure 2, il est possible de définir une bijection pour chaque valeur composant un scénario.\nUn exemple de transformation est donné par la formule : Code_ens \u003d 5*x*q+y*q+value(x,y) où \" q \" est le nombre de quantiles et 5 le nombre de données par jour (indicateur, ouverture, clôture, maxima, minima).\nFIG. 2 -Transformation de la configuration de marché en une représentation ensembliste.\nPour les deux dernières colonnes des scénarii de la Figure 1, on obtient les ensembles suivants : (q\u003d3). Le calcul de la seconde valeur de la colonne J2-1 a été explicité. J1\u003d{2 ; 6 ; 9 ; 11 ; 13 ; 16 ; 5*3*1+3*1+2\u003d20 ; 24 ; 25 ; 29} J2\u003d{3 ; 6 ; 9 ; 10 ; 15 ; 17 ; 20 ; 24 ; 25 ; 29} Après l\u0027étape de discrétisation et de codification, chaque jour est représenté par un ensemble de 5 x N symboles obtenus par la formule précédente. C\u0027est bien un ensemble et non pas une séquence, l\u0027ordre des symboles n\u0027a plus d\u0027importance.\nLes signatures\nOn souhaite caractériser les scénarii fréquents qui peuvent être à la base d\u0027une stratégie quantitative. Dans le contexte d\u0027amélioration des indicateurs techniques il n\u0027est pas intéres-sant de signer les événements rares : il est souhaitable que la fréquence du motif soit comparable à la fréquence de l\u0027indicateur et les motifs fréquents apparaissent donc opportuns.\nLa signature d\u0027un scénario est un sous-ensemble (sous-motif) fréquent (dans les ensembles des scénarii) des motifs qui le décrivent. Les scénarii peu fréquents n\u0027ont pas forcément de signature et un scénario peut avoir plusieurs signatures.\nNous donnons un exemple de signature (i.e., un sous-motif commun) des deux scénarii de la Figure 1. Le sous-ensemble commun de J1 et de J2 est {6 ; 9 ; 20 ; 24 ; 25 ; 29} ce qui, grâce à la bijection, correspond au motif visuel donné dans la Figure 3.\nNotons que les sous-ensembles des signatures sont également des signatures. Dans le contexte de l\u0027amélioration des indicateurs, la période étudiée comprend quelques centaines d\u0027ensembles et les seuils de fréquences utilisés sont de l\u0027ordre de quelques dizaines. RNTI-E-6 I. Albert-Lorincz et al. \nAlgorithme d\u0027extraction\nL\u0027extraction des sous-ensembles d\u0027une collection de scénarii est une tâche difficile car les approches naïves se heurtent à une explosion combinatorique. De très nombreux algorithmes ont été proposés pour réaliser des extractions complètes de tous les ensembles fréquents dans des contextes réalistes. Pour notre système, nous avons utilisé notre propre implémentation des « FP-Tree » (Han et al. 2004) (voir Annexe) étendue à la gestion des motifs fermés (Pasquier et al. 1999). La structure « FP-tree » a fait ses preuves lors des extractions de très grande échelle. Elle est adaptée aux tâches courantes en finance quantitative qui ne concernent généralement que quelques milliers d\u0027ensembles. L\u0027extraction se fait quasiment en temps réel, ce qui rend la méthode proposée éligible pour le « trading » haute fréquence.\nLe système de « trading »\nLe comportement du sous-jacent est assujetti à des phénomènes de mode et à des régimes de « trading » qui varient dans le temps. Aussi, est-il nécessaire de suivre la validité des signatures dans le temps, d\u0027enrichir la base des signatures par les nouveaux motifs qui apparaissent dans les cours et d\u0027écarter (invalider) les motifs qui commencent à sous performer. Nous décrivons maintenant ce mécanisme d\u0027apprentissage progressif. Les dates T0 à Tn déterminent n-1 périodes. La première est utilisée pour l\u0027initialisation du système (apprentissage des « bonnes » signatures) qui seront utilisées pour le « trading » pendant la période suivante. A partir de l\u0027apprentissage initial, chaque période sert à traiter avec les bonnes signatures identifiées jusqu\u0027au début de la période et alimente les signatures de la période suivante.\nLa première période fournit les motifs m1. Cet ensemble est testé contre la performance de l\u0027indicateur seul pendant cette même période pour n\u0027en garder que les motifs µ1 qui amé-liorent les performances de l\u0027indicateur. Le contenu de µ1 générera des transactions de la deuxième période, et sera ajouté aux motifs m2 de cette période pour calculer l\u0027ensemble µ2, contenant les signatures de bonne qualité identifiées jusqu\u0027à T2 (la fin de la 2° période).\nLe système de « trading » bâti autour de ce re-balancement est illustré sur la Figure 4. Suivons d\u0027abord les flèches qui émergent de la période p-1 jusqu\u0027au rectangle « filtrage ». Cette partie correspond à l\u0027apprentissage du système. L\u0027étape « extraction » fournit les motifs fréquents de la période p-1. La mesure de performance évalue les gains de chaque motif fréquent identifié. Pour ce faire, la stratégie de « trading » à optimiser est effectuée sur la période p-1 et l\u0027on calcule la performance de l\u0027indicateur seul. Ensuite, pour chaque motif fréquent extrait, la stratégie est à nouveau appliquée. Cette fois, l\u0027indicateur n\u0027est retourné vers la stratégie que si son scénario d\u0027occurrence contient le motif en cours d\u0027étude. Si la performance de la stratégie précédée par un motif est supérieure à la performance de l\u0027indicateur seul, le motif est ajouté à l\u0027ensemble µp-1, i.e., la collection des bonnes signatures, et utilisé par le « trading » de la période suivante. Les indicateurs de la période p sont pris en compte uniquement si leur scénario d\u0027occurrence contient une « bonne » signature de la pé-riode précédente. Dans ce cas, le filtrage génère un signal qui est envoyé à la stratégie. Après le trading, la période p devient une période d\u0027apprentissage pour la période p+1.\nFIG. 4 -Système de trading.\nUn système de « trading » sur trois périodes (apprentissage, sélection, trading) est difficile de mettre en ouvre sans une diminution drastique de la fréquence des transactions. En fait, les signatures sont attachées à un régime de marché et leur apport disparaît si elles ne sont pas appliquées immédiatement en « trading » (impossibilité d\u0027avoir une période de sé-lection). Le raccourcissement de la période d\u0027apprentissage est délicat, car la diminution de points pour l\u0027étape de fouille de données conduit à un sur-apprentissage, i.e., un phénomène de sur-spécialisation qui produit des bonnes performances pendant la période d\u0027apprentissage mais qui se comporte de manière médiocre ultérieurement.\nPour démontrer le caractère générique de la méthode, elle a été appliquée à des données financières de type « action » et « fixed income », qui ont des comportements (volatilité, retour à la moyenne, etc.) très différents. Les indicateurs utilisés sont simples et bien connus dans le monde financier : Williams %R (Achelis 2000), croisement de moyennes mobiles, et un indicateur confidentiel, appelé boite noire (BN). La période étudiée s\u0027étend sur cinq ans. La performance a été mesurée comme le gain réalisé en « N » jours.\nLe Williams%R permet de déterminer quand un actif est sur vendu ou sur acheté sur une période (d\u0027habitude 2 semaines). On voit bien que l\u0027indicateur signé ne surpasse pas l\u0027indicateur seul sur toutes les pério-des. Elle a néanmoins un meilleur comportement en moyenne. La somme des retours journaliers (dS/S) pour le Williams %R signé termine à 12.59 en retour cumulé alors que l\u0027indicateur seul ne produisait que 7.29 sur 10 ans, soit un gain de 5.3% avec la méthode des signatures.\nL\u0027analyse des actions composant l\u0027indice CAC40 depuis le 1 er janvier 2000 a permis de valider l\u0027apport de la technique des signatures sur un grand nombre de périodes et d\u0027actions. La méthode proposée a apporté 6% de gains par rapport au seul W%R sur 5 ans. Compte tenu du nombre de périodes comparées, le test t de Student a permis de conclure que les performances de la technique signée sont supérieures à l\u0027indicateur seul de manière significative. Pour une stratégie quantitative dans la finance, ce gain est énorme ! La moyenne des performances de l\u0027indicateur W%R signé est 96.93, comparé au W%R qui fait 90.63. La P(T\u003c\u003dt) unilatéral est 10,6%. Les tests peuvent être obtenus auprès des auteurs.\nNous avons également testé l\u0027apport des signatures sur un indicateur propre à l\u0027entreprise CIC et qui est appelé boîte noire (BN). L\u0027amélioration est encore plus significative (seuil de 1%). La moyenne de la BN signé est 104.22, alors que la BN seul n\u0027a fait que 98.13%. Cette fois la P(T\u003c\u003dt) unilatéral valait 0.011, ce qui est un très beau résultat. Données de type taux d\u0027intérêt. Compte tenu de l\u0027efficacité accrue des marchés de capitaux, le W%R ne dégage pas de performance sur la période étudiée. Cependant, les signatures permettent de baisser les pertes. Comme les graphiques suivants le montreront, par pério-des le W%R signé produit des gains alors que le W%R s\u0027inscrit dans une tendance baissière. La Figure 5 illustre le « wealth process » de la stratégie, i.e., la valeur du portefeuille produit par la stratégie tout au cours de sa vie. Le W%R signé se distingue pendant les périodes ou le W%R perd, voir il réalise des gains par période (de 92.93 à 96.13, soit 3.2% de mieux).\nFIG. 5 -Wealth process du taux swap 5 ans.\nLe Tableau 2 illustre les arguments développés pour la Figure 5 \nDiscussion\nLa stratégie de « trading » en réponse de l\u0027indicateur a été paramétrée pour mesurer les performances à N jours après la transaction. (5 jours pour les statistiques précédentes). Les signatures caractérisent une configuration de marché ponctuelle et ont un pouvoir de prédic-tion d\u0027environ une semaine. Il est normal que leur validité soit limitée dans le temps : l\u0027ensemble qui décrit la configuration de marché n\u0027inclut aucune information sur le comportement historique du cours. La \"mémoire longue\" et l\u0027auto corrélation ne sont pas utilisées. Nous pensons qu\u0027en enrichissant la description des scenarii par des éléments du passé (minimum et maximum sur un passé plus long, moyenne mobile), le pouvoir de prédiction peut être étendu de quelques jours, mais l\u0027extraction des motifs fréquents devient plus difficile à cause d\u0027un alphabet plus large. La durée de vie d\u0027une signature est assez courte, elle dépasse rarement trois périodes (y compris celle d\u0027apprentissage).\nDans le cas du W%R, les signatures sur performent la stratégie non signée en perdant un peu moins d\u0027argent. On doit alors se demander si l\u0027amélioration des performances n\u0027est pas essentiellement liée au filtrage ? En effet, appliquer moins souvent une stratégie perdante améliore forcément les performances. Cependant, l\u0027étude visuelle intuitive du comportement des signatures sur la Figure 5 nous laisse à penser que non ! En effet, dans le cas d\u0027un filtrage aléatoire, l\u0027écart entre les deux courbes devrait se constituer progressivement, alors que l\u0027indicateur filtré se distingue aux moments ou l\u0027indicateur seul commence à perdre. De plus, le filtrage produit des résultats significativement meilleurs pour l\u0027indicateur BN, et présente des gains alors que la stratégie de base est légèrement déficitaire. Stratégies croisement des moyennes mobiles. Ce type d\u0027indicateur est basé sur la mémoire longue non corrélée avec les signatures qui caractérisent la configuration présente. Aussi, ces indicateurs ne sont améliorés par la méthode des signatures. Frais de transactions. L\u0027effet du filtrage par les signatures est de limiter le nombre des transactions en gardant seulement celles qui ont le plus de chances d\u0027aboutir. En conséquence, il réduit également les frais de transaction. L\u0027étude présente sous-estime donc l\u0027apport des signatures d\u0027une part en négligeant les économies sur les frais de transactions et d\u0027autre part en ignorant la rémunération du compte de dépôt.\nConclusion\nLes tests effectués promettent d\u0027aboutir à un système de « trading » automatique. Le « back testing » des indicateurs avec re-balancement s\u0027est montré capable d\u0027identifier les signatures caractéristiques à chaque période. Elles sont suffisamment stables dans le temps pour pouvoir améliorer les performances de l\u0027indicateur technique durant la période suivante. Les techniques décrites apportent un moyen pour améliorer les stratégies quantitatives existantes et ne demandent pas la mise en oeuvre de nouvelles stratégies. Dans les exemples traités, la technique des signatures a produit 3% et 6% de gains en plus par rapport à l\u0027indicateur seul pour trois indicateurs différents. D\u0027autres mesures sont en cours pour tester l\u0027impact de la méthode sur davantage d\u0027indicateurs techniques. L\u0027application à des données « haute fré-quence » est également à l\u0027étude.\nRemerciements. Les auteurs souhaitent remercier Swann Chmil (BNP Paribas Asset\nManagement) pour sa contribution à cette recherche et les relecteurs anonymes pour leurs suggestions.\nPour simplifier la présentation, chaque scénario sera représenté par un extrait de l\u0027ensemble qui le décrit. Les lettres tiennent pour des valeurs discrétisés précédemment. On cherchera à identifier tous les sous-ensembles 2-fréquents (présents dans au moins deux ensembles) des extraits du Tableau 1 avec l\u0027algorithme FP-growth (Han et al. 2004).\nA) La première étape consiste à parcourir tous les ensembles pour trouver la fréquence de chaque symbole. Les ensembles sont ensuite réorganisés selon un ordre donné par la fré-quence des symboles : (f:3) \u003e (d:3) \u003e (c:3) \u003e (a:2) \u003e (b:2), en laissant de coté les symboles qui ont une fréquence d\u0027apparition inférieure au seuil requis (3° colonne du Tableau A1). Le symbole \"e\" disparaît. On note que l\u0027ordre et le FP-tree résultant ne sont pas uniques. B) Les ensembles réordonnés sont ensuite insérés dans une structure arborescente. Chaque ensemble est représenté par un chemin dans cet arbre, de manière à ce que les sousensembles communs soient proches de la racine. Chaque noeud de l\u0027arbre contient un symbole et un compteur des ensembles qui le contiennent sur cette branche de l\u0027arbre. Concrète-ment, le premier scénario va créer la branche \u003cf:1, d:1, a:1\u003e. Le deuxième scénario partage un préfixe commun avec le premier. Il fait augmenter les compteurs sur la portion commune et créer un fils du noeud \"d\" pour \"c\". L\u0027insertion des ensembles restant produit l\u0027arbre à gauche de la Figure A1. Pour des raisons exposées plus loin, cet arbre est enrichi de pointeurs transversaux (Figure A1 à droite) qui réunissent toutes les occurrences d\u0027un même symbole. Le vecteur des pointeurs respecte l\u0027ordre établi au point (A). Toute l\u0027information présente dans les scénarii du Tableau A1 est contenue dans la structure FP-tree. Les chemins de l\u0027arbre regroupent les symboles qui paraissent souvent ensemble, mais il faut utiliser les pointeurs transversaux pour trouver tous les motifs fréquents. En effet, la collection des branches (entre la racine et le symbole) le long des pointeurs d\u0027un symbole \"X\" quelconque contient tous les ensembles où X est fréquent. Un traitement récursif permet de les extraire. C) Si l\u0027arbre contient plusieurs branches, l\u0027extraction commence par chercher les motifs contenant le symbole le moins fréquent, \"b\" dans le cas présent. La sommation des compteurs le long de la liste chaînée fournira la fréquence de \"b\". En suivant cette liste, tous les chemins entre la racine de l\u0027arbre et le noeud contenant le symbole courant \"b\" {fdc:1, dca:1} sont regroupés. La fréquence de chaque branche est égale à la fréquence du symbole en cours sur la branche. Après l\u0027extraction des branches, la procédure recommence à l\u0027étape A avec les ensembles correspondant aux branches pour trouver tous les motifs fréquents contenant le symbole \"b\". Comme ni \"f \", ni \"a\" n\u0027apparaissent suffisamment souvent avec \"b\", l\u0027arbre contiendra un seul chemin (dc:2) qui donnera les motifs fréquents bcd:2, bc:2 et bd:2. Quand l\u0027arbre contient une seule branche, e.g., \u003cd:2, c:2\u003e, les motifs fréquents sont donnés par tous les sous-ensembles des symboles le long de la branche, e.g., d:2, c:2, et dc:2.\nAprès le traitement de \"b\", l\u0027extraction continue avec le symbole immédiatement plus fréquent que \"b\", \"a\" dans le cas présent. Cette étape livrera les motifs contenant \"a\" mais ne \nSummary\nFrequent itemsets were used to improve the performances of the technical trading indicators. Our method assigns a signature to the frequent market configurations, and overrides the usual trading rules by the information deduced from the signatures. An iterative back testing selects the best signatures, and combines them with the technical indicator to improve its performances. Combining frequent itemsets with technical trading indicators is an original contribution. The method statistically improves the performances of the trading strategies, and the authors recommend it for any automatic trading system based on technical indicators. This study was based on daily interest rate and stock market data, and three different indicators: Williams % R, moment crossing, and a confidential black box strategy. Signatures are particularly well suited for short memory processes.\n"
  },
  {
    "id": "1001",
    "text": "Introduction\nDe par le développement rapide des techniques de stockage et de diffusion, les vidéos, notamment digitalisées, sont de plus en plus nombreuses et accessibles. En particulier, les agences de presse, les diffuseurs TV, les agences de publicité travaillent sur des ressources vidéo grandissantes. Pour être à même de travailler sur de tels volumes, des technologies adaptées doivent être mises en oeuvre. La « fouille des usages de la vidéo », qui cherche à analyser les comportements des utilisateurs sur des ensembles de vidéo est l\u0027une des techniques clé émergentes pour optimiser les accès aux vidéos.\nDans cet article, nous proposons d\u0027analyser le comportements des utilisateurs d\u0027un moteur de recherche vidéo pour améliorer la qualité de l\u0027indexation textuelle. Notre objectif est de comprendre pourquoi et comment chacune des séquences vidéo est visionnée. Par exemple, les utilisateurs recherchant des vidéos concernant le mot-clé « montagne » visionnent successivement les vidéos (18,73,29) qui sont retournées dans cet ordre par le moteur de recherche. Si l\u0027on note que dans la majeure partie des cas, la vidéo 29 est visionnée totalement alors que les vidéos 18 et 73 ne le sont que partiellement, on en déduit que, selon l\u0027utilisateur, le concept de « montagne » est mieux exprimé par la vidéo 29 que par les vidéos 18 et 73. En conclusion, la vidéo 29 doit être proposée en premier aux utilisateurs lors des futures recherches sur le concept « montagne ». Son poids dans la vidéo 29 s\u0027en trouve augmenté et celui des vidéos 18 et 73 réduit.\nDans ce papier nous présentons une approche qui combine usage intra-vidéo et usage intervidéo pour générer des profils de visite sur un moteur de recherche vidéo dans le contexte de\nFIG. 1 -Description globale du processus de fouille de données vidéo.\nUn comportement intra-vidéo est modélisé par un modèle de Markov du premier ordre non caché. Ce modèle est construit en utilisant les différentes actions proposées aux utilisateurs lors des visionnages (lecture, pause, avance rapide, retour rapide, saut, stop). Nous proposons une technique de regroupement de ces modèles (K-Models). Cette technique est une adaptation de la technique classique des K-Means [MacQueen (1966)] adaptée à l\u0027utilisation de modèles en lieu et place des moyennes. Nous caractérisons ainsi plusieurs comportements type (lecture totale, lecture partielle, survol...). Grâce à ces comportements type, nous sommes à même de savoir quelle fut l\u0027utilité ou l\u0027importance d\u0027une séquence vidéo lors d\u0027une visite (si elle est la plus importante ou seulement le résultat d\u0027une recherche infructueuse...)\nUn comportement inter-vidéo est modélisé par une session. Cette session est une séquence ordonnée des visionnages des séquences vidéo. Chaque visionnage étant représenté par l\u0027identifiant de la vidéo et le modèle de comportement qui lui correspond pour cette visite. Pour regrouper ces sessions, nous nous sommes basés sur une technique de regroupement hiérar-chique ascendante que nous avons adaptée à ces données particulières.\nS. Mongy\nCet article est organisé comme suit. La section 2 présente l\u0027état de l\u0027art dans le domaine de la fouille des usages de la vidéo et en spécifie les particularités. La section 3 décrit le contexte applicatif de notre approche qui est le montage de films. Elle présente ensuite notre modéli-sation à deux niveaux des comportements des utilisateurs exploitant un moteur de recherche vidéo. Enfin, la section 4 présente les premiers résultats obtenus sur des données test. La section 5 regroupe les conclusions et introduit les futures lignes directrices de notre travail.\nFouille des usages de la vidéo\nQuelques approches existent dans ce domaine précis. Les travaux les plus proches peuvent être classés en deux catégories.\nLa première concerne l\u0027analyse des comportements des utilisateurs sans considérer le contenu de la vidéo. Ces travaux permettent de produire des statistiques sur le comportement des utilisateurs et l\u0027analyse des fréquences des accès vidéo. Par exemple dans [Reuther et Meyer (2002)] on analyse l\u0027usage d\u0027un système d\u0027enseignement multimédia par les étudiants. Cette analyse se base sur les types de personnalités des étudiants. En effet, les besoins et les attentes en apprentissage dépendent des caractéristiques du type de personnalité de l\u0027étudiant. Pour conduire cette analyse, les auteurs ont développé un programme permettant d\u0027extraire les actions effectuées par les étudiants sur le système multimédia et construit des profils utilisateurs permettant de tracer ce que fait chaque étudiant chaque fois qu\u0027il utilise le système. Ces profils utilisateurs comportent les statistiques suivantes : le nombre de sessions de visionnage des vidéos, le nombre total de secondes passées à visionner les vidéos, le nombre de sessions ayant duré plus de 20 minutes, la durée moyenne d\u0027une session, le nombre moyen de commandes par minute lors d\u0027une session, (lecture, pause, saut...). En se basant sur les statistiques collectées sur les divers types de personnalité des étudiants, les auteurs analysent comment le système d\u0027enseignement multimédia peut être amélioré.\nDans [Acharya et al. (2000)] une analyse des données tracées obtenues à partir des accès utilisateurs à des vidéos sur le Web est présentée. Les auteurs examinent des propriétés telles que : les variations journalières des requêtes des utilisateurs utilisant des accès vidéo. Ils proposent de tirer avantage de ces propriétés pour la conception des systèmes multimédia tels que : les systèmes de proxy, de cache et les serveurs de vidéo. A titre d\u0027exemple leur analyse a montré que généralement les utilisateurs visionnent la première portion d\u0027une vidéo pour savoir s\u0027ils sont intéressés ou pas. S\u0027ils sont intéressés, ils poursuivent le visionnage, sinon ils l\u0027interrompent. Cette analyse suggère que mettre en cache les premières minutes d\u0027une vidéo permettrait d\u0027améliorer les performances d\u0027accès au serveur vidéo.\nLa seconde catégorie concerne l\u0027analyse du comportement de l\u0027utilisateur sur une vidéo unique.\nUn projet, réalisé par Microsoft [Yu et al. (2003)], développe un modèle centré « utilisateur », combinant l\u0027analyse de contenu de la vidéo et la fouille de données log vidéo afin de générer des résumés vidéo. Le modèle utilise l\u0027expérience des visionnages antérieurs pour guider les futurs visionnages. La contribution essentielle du travail réside dans la pondération des plans « ShotRank » de la vidéo, et l\u0027utilisation de cette pondération dans l\u0027extraction des résumés de la vidéo. Le « ShotRank » mesure la probabilité de visionner un plan donné durant l\u0027exploration de la base de vidéo. La pondération des plans est calculée par un algorithme d\u0027analyse des liens, et utilise les interactions de l\u0027utilisateur (« Vote ») pour évaluer l\u0027utilité et l\u0027importance de chaque plan. Des expériences avec des vérités de terrain ont confirmé que la pondération du plan « ShotRank » estime l\u0027utilité et l\u0027importance de chaque plan de vidéo, et améliore l\u0027exploration des futurs visionnages.\nUn autre projet similaire, réalisé par IBM [Syeda-Mahmood et Ponceleon (2001)], utilise également les données de log pour déterminer les type de comportement des utilisateurs visionnant des données vidéo. En se basant sur les actions réalisées et sur l\u0027implémentation de modèles de Markov cachés, les auteurs proposent de définir des types de comportement tels que : -curieux ; -recherchant quelque chose en particulier ; -a trouvé un passage intéressant...\nLe point non traité dans les travaux précédents est la non-corrélation des comportements généraux des utilisateurs avec leur comportement sur chacune des séquences vidéo. Ils ne prennent pas en compte les actions réalisées durant les visionnages lors de l\u0027analyse de la navigation entre les vidéos (la session). Les concepts de navigation et de recherche dans une grande base de données vidéo ne sont pas définis. De plus, il n\u0027existe à notre connaissance aucun jeu de données ou banc d\u0027essai relatifs aux données log vidéo, ce qui ne simplifie pas la tâche de validation des résultats.\nDeux points importants discriminent notre approche des travaux actuels. Tout d\u0027abord, il n\u0027existe aucun outil analysant l\u0027utilisation complète d\u0027une base de données vidéo. Les seuls travaux que nous avons référencés dans le domaine de l\u0027analyse vidéo ne considèrent qu\u0027une vidéo à la fois.\nEnsuite, nous avons développé une technique de regroupement qui correspond à la nature de nos données. En effet, nombre de techniques du web mining utilisent des algorithmes basés uniquement sur les distances et l\u0027approche par voisinage produisant des résultats difficiles à analyser. Il n\u0027est pas rare de retrouver deux éléments totalement différents dans une même classe pour peu qu\u0027ils soient connectés par une chaîne de voisins très proches les uns des autres. Nous proposons ici d\u0027introduire un modèle de classe qui capitalisera les informations données par tous les éléments d\u0027une même classe, éléments qui devront correspondre à ce modèle.\n3 Notre approche 3.1 Contexte L\u0027un des besoins des professionnels de l\u0027audiovisuel est de pouvoir retrouver facilement des séquences vidéo particulières dans d\u0027importantes bases de films afin de les réutiliser pour le montage de nouveaux films. Notre approche met en oeuvre un moteur de recherche adapté à ce besoin. Les recherches s\u0027appuient sur une indexation des séquences vidéo. Mais beaucoup d\u0027informations peuvent également être extraites des usages et utilisées pour optimiser la pertinence des séquences retournées par le moteur de recherche.\nPour réaliser cette analyse, nous devons tout d\u0027abord définir ce qu\u0027est une utilisation du moteur de recherche vidéo. Un tel comportement peut être divisé en trois parties. -1. Ecriture d\u0027une requête : l\u0027utilisateur définit les attributs sur lesquels porte la recherche et entre une valeur pour chacun d\u0027entre-eux. Ces attributs sont utilisés pour trouver les vidéos pertinentes dans la base de vidéos indexée -2. Exploitation des résultats : les séquences vidéo retrouvées sont présentées dans l\u0027ordre de leur proximité aux attributs. -3. Visionnage des séquences sélectionnées : l\u0027utilisateur visionne les séquences qui lui semblent les plus pertinentes. Ce -394 -RNTI-E-6 visionnage est réalisé dans un lecteur offrant les fonctionnalités usuelles d\u0027un lecteur vidéo (lecture, pause, avance rapide, retour rapide, stop, saut).\nLes groupes de séquences visionnées forment des sessions. Ces dernières correspondent à une visite d\u0027un utilisateur. Elles sont composées de plusieurs groupes recherche -exploitation et visionnage des résultats.\nCollecte des données\nToutes ces données sont tracées et écrites dans des fichiers de log. Pour les créer, nous avons défini un langage adapté basé sur XML. La grammaire du langage de trace d\u0027une session est le suivant. Une première partie contient la requête exécutée et la liste des séquences vidéo retournées. Une seconde partie trace le visionnage des séquences.\nTout comme les logs web, notre outil trace les actions de tous les utilisateurs. Pour les regrouper sous forme de sessions, nous avons développé un convertisseur XSLT (eXtensible Stylesheet Language Transformation) [w3c]. Ce convertisseur extrait et regroupe les sessions depuis les fichiers de log dans un format XML. La suite de l\u0027article présente sous quelle forme sont modélisées les sessions.\nModélisation du comportement : un modèle à deux niveaux\nA partir des données collectées précédemment, nous générons deux modèles pour repré-senter le comportement des utilisateurs. Le premier montre comment un utilisateur a visionné une séquence vidéo (lecture, pause, avance rapide, saut, stop). A ce niveau, nous définissions le « visionnage d\u0027une séquence vidéo » comme une unité de comportement. Le second trace les transitions entre chaque visionnage. Cette partie regroupe les requêtes, les résultats et les séquences visionnées successivement. A ce niveau plus général, nous proposons la « session » comme unité de comportement.\nUne session est une liste de séquences vidéo visionnées. L\u0027intérêt des logs vidéo est de permettre de définir l\u0027importance de chaque séquence pour chaque session. Plus que de les caractériser par un simple poids, comparable à l\u0027exploitation du temps passé sur chaque document [Wang et Zaiane (2002)], nous allons ici construire plusieurs comportements type (lecture totale, aperçu, ouverture-fermeture, lecture partielle...).\nModélisation et regroupement des comportements intra-vidéo\nUn comportement intra-vidéo est modélisé par un modèle de Markov du premier ordre (figure 2). Ce modèle représente les probabilités d\u0027exécuter une action à chaque seconde du visionnage d\u0027une vidéo. Les sommets correspondent aux différentes actions accessibles aux utilisateurs durant le visionnage. Ces actions sont Lecture, Pause, Stop, Avance Rapide, Retour Rapide, Saut, Stop. Par exemple, la transition de l\u0027état Lecture vers l\u0027état Pause de la figure (figure 2) signifie que quand un utilisateur regarde une séquence, il y a une probabilité de 8% qu\u0027il effectue une pause la seconde suivante.\nCe modèle est totalement déterminé par les paramètres suivants : ? i la probabilité de démarrer dans l\u0027état i.\nA ij la probabilité de passer d\u0027un état V i à un état V j la seconde suivante. Cette discrétisation du temps (avec la seconde comme unité) permettant de prendre en compte le paramètre temps dans un modèle de Markov a été introduite par [Branch et al. (1999)]. Elle permet de considérer le temps sans ajouter de paramètre supplémentaire.\nSa complexité limitée (nombre d\u0027états peu important, premier ordre, modèle non caché) permet de proposer une méthode de regroupement efficace de ces comportements. Nous allons ici introduire la technique des K-Models. Cette technique correspond à une adaptation de la méthode bien connue des K-Means pour utiliser des modèles en lieu et place des moyennes. Nous essayons de découvrir K classes dans un ensemble de visionnages (actions exécutées sur la séquence vidéo par un utilisateur) par partitionnement de l\u0027espace. Chaque classe est représentée par l\u0027un des modèles de Markov décrits précédemment. La différence réside dans l\u0027utilisation des probabilités en lieu et place des distances pour associer les visionnages aux classes. Nous calculons la probabilité qu\u0027un visionnage ait été généré par les modèles. Nous l\u0027assignons ensuite à la classe ayant la plus grande probabilité de l\u0027avoir généré.\nUn tel algorithme se découpe en trois phases : initialisation, allocation, maximisation. Allocation Pour chaque visionnage e \u003d (e 1 ..e l ) e i appartenant à N , de longueur l, pour chaque classe définie par un modèle k, nous calculons la probabilité que k ait généré e. e est associé à la classe avec la plus grande probabilité.\nMaximisation Chaque modèle k représentant une classe c de cardinalité m est mis à jour en fonction des données appartenant à c. Cette mise à jour correspond à compter chaque S. Mongy transition dans chaque élément e i et d\u0027attribuer ces comptes aux probabilités de transition du modèle. Pour chaque cluster c, les probabilités sont mises à jour de cette manière.\nA partir de ces modèles, nous construisons un vecteur v e de comportement pour chaque visionnage. Ce vecteur correspond aux probabilités que le visionnage ait été généré par chacun des modèles (1).\nModélisation et regroupement des comportements inter-vidéo\nA partir des données initiales et des vecteurs de comportement générés par le regroupement intra-vidéo, nous construisons une représentation séquentielle des sessions. Une session est une séquence ordonnée dans le temps des vidéos visionnées. Chaque visionnage est caractérisé par l\u0027identifiant de la vidéo et par le vecteur de comportement qui lui est associé. nous avons développé un algorithme de regroupement qui répond aux besoins suivants : -chaque élément appartenant à une classe a au moins une partie commune avec les autres éléments de la classe ; -le niveau d\u0027homogénéité des classes repose sur la définition de paramètres entrés par l\u0027utilisateur. Ces paramètres sont présentés ci-après.\nCes besoins ont conduit à la représentation suivante des classes : une classe c est représen-tée par un ensemble de S sessions s c de longueur minimale l. Une session s est attribuée à une classe si elle correspond au moins à p des S sessions. Une session s correspond à s c si s c est une sous-séquence extraite de s (2).\nAinsi, nous assurons l\u0027homogénéité des classes et le fait qu\u0027il y ait un facteur commun entre tout élément d\u0027une même classe. De fait nous évitons de créer des classes composées d\u0027éléments totalement différents, connectés par une chaîne de proches voisins généralement produites par les techniques de regroupement basées sur les distances [Guha et al. (1998)]. La longueur minimale d\u0027une séquence représentative et le nombre de séquences nécessaires pour modéliser une classe sont donnés par l\u0027analyste, lui permettant d\u0027obtenir des classes d\u0027une généricité en rapport avec ses besoins. L\u0027algorithme de regroupement lui-même est un algorithme hiérarchique ascendant. Il dé-bute en considérant des petits groupes de sessions comme classes et fusionne itérativement les classes les plus proches. Le regroupement prend fin lorsque le niveau d\u0027homogénéité requis est atteint. Son originalité réside dans la représentation des classes. Les techniques de regroupement de séquences usuelles ne traitent qu\u0027une unique séquence pour représenter une classe. Nous avons développé un outil capable de comparer et de fusionner des classes représentées non plus par une unique séquence mais par un ensemble de séquences.\nComparaison des classes\nAnalyse du Comportement des utilisateurs exploitant une base de vidéos Cette fonction de distance (3) est basée sur la comparaison des sessions, comparaison elle-même basée sur l\u0027extraction de la plus longue sous-séquence commune. Etant donnée I \u003d [(i 1 , i 2 )] la liste de longueur l des indices des éléments sélectionnés des deux sessions comparées s 1 et s 2 , la distance entre elles est définie par (4) où v xy est le vecteur de comportement du y me élément de la session x. (5) est la fonction de distance entre deux vecteurs de comportement.\nFusion des classes La fonction de fusion est également basée sur l\u0027extraction de la plus longue sous-séquence commune. Elle extrait les plus longues sous-séquences en comparant les séquences des modèles 2 à 2. En fusionnant les séquences 2 à 2, nous assurons que la proportion p est conservée jusqu\u0027à la fin de l\u0027exécution. Soient\ni et j sont choisis pour maximiser la longueur des séquences fusionnées. Pour fusionner deux séquences, l\u0027algorithme extrait la plus longue sous-séquence commune sans tenir compte du vecteur de comportement. Une fois cette sous-séquence créée, il fusionne les comportements correspondants des deux séquences en calculant la moyenne sur chaque élément du vecteur. Soient\n) deux comportements à fusionner, le résultat de la fusion est donné par (7).\n4 Resultats expérimentaux Cette partie met en avant les deux avantages de notre approche comparée aux techniques usuelles. Tout d\u0027abord, nous allons voir comment l\u0027analyse du comportement intra-vidéo permet de différencier des groupes de sessions composées des même séquences vidéo mais visionnées de manière différente. Ensuite, nous allons montrer l\u0027avantage de décrire les classes par un groupe de sessions comparé à une simple extraction de sous-séquence.\nCréation des jeux de données\nEn raison de la nouveauté des données exploitées ici, nous avons réalisé nos expérimenta-tions sur des jeux de données de test.\nLa création des jeux de données de test se présente en deux phases. Premièrement, nous avons créé les modèles de comportement intra-vidéo. Nous avons défini quatre comportements type (lecture totale, survol rapide, lecture d\u0027un passage précis, fermeture rapide). De nouvelles expérimentations sur des données réelles nous permettront de valider totalement ces modèles. A partir de ces modèles, nous avons créé aléatoirement un vecteur de comportement pour chaque visionnage des sessions vidéo.\nEnsuite, pour générer les sessions vidéo, nous avons défini pour chaque classe une source de génération avec un ensemble de séquences d\u0027identifiants vidéo. Pour chaque classe, nous avons généré les sessions en fusionnant aléatoirement ces séquences. Nous avons enfin ajouté dans ces sessions entre 5% et 20% de bruit en introduisant des identifiants de vidéo n\u0027ayant pas de lien avec le contenu de la classe dans les séquences. Nous avons finalement généré différents jeux de données composés de 2000 sessions. Chacune est composée de 5 à 20 visionnages correspondant à des comportements de 10 à 20 actions de base (lecture, pause...). Les fichiers de tests sont donc composés d\u0027environ 100.000 à 800.000 actions de base. La figure 3 présente quelques sessions générées pour le deuxième scénario ci-après. Une session est une séquence de vidéos visionnées à la suite d\u0027une navigation par un utilisateur dans un intervalle de temps donné.\nFIG. 3 -Exemple des sessions analysées.\nLa suite présente deux exécutions correspondant à des scénari prédéfinis qui mettent en avant les avantages de notre technique. Considérons dans la suite une base de données de vidéo en rapport avec la nature contenant des vidéos de montagnes et de volcans.\nExploitation du comportement intra-vidéo\nCe scenario est basée sur l\u0027affirmation suivante. La base vidéo n\u0027est pas correctement indexée et de nombreuses vidéos traitant des volcans sont indexées comme des vidéos de montagne.\nNous avons généré deux classes. La première correspond à une recherche sur les volcans et retourne uniquement une seule vidéo complètement visionnée. Elle correspond à la séquence (1, 2, 3, 4, 5, 6, 10) dans laquelle seulement la vidéo 10 est visionnée (représentée en gras dans la table 1). La seconde est le résultat d\u0027une recherche sur les montagnes et toutes les vidéos sont visionnées complètement (table 1). Elle correspond à la séquence (1, 2, 3, 4, 5, 6) dans laquelle toutes les vidéos sont visionnées totalement. Avec une approche classique, ne prenant pas en compte le comportement intra-vidéo, les deux classes ne sont pas découvertes et le regroupement retourne une seule classe {(1, 2, 3, 4, 5)}.\nAvec notre approche à deux niveaux, le regroupement est à même de découvrir que les vidéos ont été exploitées différemment et les deux classes sont effectivement découvertes. Pour TAB. 2 -Regroupement du second jeu de données En positionnant le nombre de séquences représentatives à 2 ou 3, les sessions correspondant à l\u0027une des trois premières classes sont fusionnées pour former une classes « sessions traitant des stations de ski ». Avec une valeur de 4, cette classe est divisée et chaque classe source est découverte. Pour ces valeurs, la classe correspondant au trekking est correctement analysée et n\u0027est pas fusionnée avec les autres données. Mais si nous positionnons le nombre de séquences à 1, nous plaçant ainsi dans le cadre d\u0027une simple extraction de sous-séquence, toutes ces données sont fusionnées dans une unique classe et la différence entre les stations de ski et le trekking n\u0027est pas détectée lors du regroupement. On voit finalement l\u0027intérêt d\u0027une approche multi-séquences. Elle permet de retrouver des éléments correspondant à plusieurs recherches réalisées les unes à la suite des autres dans un ordre variable et de les reconstituer dans une même classe. Sur notre exemple, on peut ainsi reformer une classe générique correspondant au travail sur les stations de ski (minSize \u003d 2 ou 3).\nExploitation des modèles générés\nUne fois ces modèles générés, l\u0027objectif est de s\u0027appuyer sur les connaissances extraites pour optimiser la qualité de l\u0027indexation. Plusieurs applications sont envisagées.\nPondération des termes de l\u0027index Pour des recherches courantes, nous pourrons mettre à jour l\u0027ordre des vidéos retournées en fonction de l\u0027usage fait de ces vidéos par les utilisateurs précédents. Par exemple, dans la recherche concernant la montagne et retournant les vidéos (18,73,29,41), si l\u0027on note que la vidéo 29 est systématiquement la plus regardée, elle devra apparaître en tête de liste lors des recherches contenant les vidéos 18, 73 et 41.\nDétection des erreurs d\u0027indexation\nSi une vidéo n\u0027est jamais visionnée lors d\u0027une requête précise (comme dans notre premier exemple), c\u0027est que son indexation a été mal réalisée. Dans ce cas, il faut découvrir avec quel donnée sémantique ne correspond pas au contenu et la supprimer. Dans la recherche précédente, si la vidéo 41 n\u0027est jamais visionnée alors qu\u0027elle est pourtant retournée, cela signifie qu\u0027elle ne correspond pas au concept de montagne pour les utilisateurs et cette notion doit disparaitre de son indexation.\nCorrection des erreurs d\u0027indexation Lorsqu\u0027une erreur d\u0027indexation est découverte, il est également possible de tenter de la corriger plutôt que de la supprimer. Pour ce faire, il faut trouver, en fouillant les modèles, avec quelles autres vidéos la vidéo mal indexée est exploitée pour en déduire les bonnes données d\u0027indexation.\nConclusion et travaux futurs\nNous proposons ici un modèle de représentation des utilisateurs exploitant un moteur de recherche vidéo à deux niveaux. Le premier niveau conduit à modéliser les comportements relatifs au visionnage d\u0027une séquence vidéo (intra-vidéo), le second à modéliser les comportements observés sur un ensemble de séquence vidéo (inter-vidéo). A partir de cette repré-sentation, nous avons développé une technique de regroupement qui permet de retourner les vidéos les plus souvent retournées et les modes de visionnage de ces vidéos. Ces derniers nous permettent de pondérer l\u0027importance et de mettre à jour les termes de l\u0027index.\nNotre futur objectif est d\u0027utiliser les profils découverts pour optimiser le moteur de recherche. En effet, ces résultats nous permettront de repérer les données mal indexées et de proposer aux utilisateurs les séquences vidéo en relation avec l\u0027historique de leur session.\n"
  },
  {
    "id": "1002",
    "text": "FIG. 1 -page web et ontologie présentées à l\u0027expert\nle concept le plus spécifique avec lequel un élément doit être annoté. L\u0027annotation est donc totalement dépendante de l\u0027ontologie fournie.\nDans une première section, nous présentons le processus de marquage de la page par un expert. La seconde section présente l\u0027algorithme d\u0027apprentissage exploitant la structure arborescente d\u0027une page web. La section 3 présente l\u0027annotation de documents dont la structure est similaire. Enfin, nous évaluons notre méthode par rapport aux systèmes d\u0027annotation séman-tique existants.\nGénération d\u0027annotations primaires par marquage\nLa première étape du processus est un marquage permettant de former un corpus d\u0027apprentissage ; il s\u0027agit de fournir au système quelques exemples d\u0027éléments pertinents à partir desquels le système apprend à reconnaître l\u0027ensemble des éléments à annoter. Pour cela, un expert marque des éléments pertinents de la page web, c\u0027est-à-dire correspondant à des concepts de l\u0027ontologie. Il dispose à cet effet d\u0027un outil de visualisation, à la manière d\u0027un navigateur web, qui lui permet de sélectionner un élément dans la page et de choisir dans l\u0027ontologie le concept qui lui correspond. Dans l\u0027exemple figure 1, le marquage est effectué en fonction de l\u0027ontologie SWRC 1 , qui modélise notamment les personnes, organismes et projets d\u0027une équipe de recherche. Pour chaque concept, un nombre suffisant d\u0027éléments pouvant y être associé doivent être marqués ; ce nombre dépend de la régularité de la page d\u0027apprentissage et des pages à annoter ; pour des pages très régulières, 2 ou 3 exemples suffisent pour chaque concept.\nDe manière interne, la page est représentée par son arbre DOM (W3C) dans lequel les noeuds contiennent les éléments de structure HTML et les feuilles les éléments de texte. Un chemin unique est ainsi défini depuis la racine jusqu\u0027à chaque feuille. Lorsque l\u0027expert marque\nFIG. 2 -Arbre DOM et annotations primaires issues du marquage\nun élément la chaîne de caractères sélectionnée, le chemin de la feuille contenant la chaîne et le concept de l\u0027ontologie associé sont enregistrés au format XML ; la figure 2 présente un exemple d\u0027enregistrement du marquage de la page web de la figure 1. L\u0027ensemble de ces éléments marqués sont des annotations primaires qui jouent ainsi le rôle de corpus pour l\u0027algorithme d\u0027apprentissage.\n2 Apprentissage exploitant une structure arborescente Définition d\u0027un chemin dans l\u0027arbre issu du DOM L\u0027algorithme d\u0027apprentissage est dérivé des travaux de Kushmerick et al. (1997) sur l\u0027induction de wrappers. Un wrapper est une procédure utilisant les régularités syntaxiques d\u0027un document pour identifier des éléments. Là où les travaux initiaux s\u0027appuyaient sur des structures à plat, en considérant le document comme une suite de chaînes de caractères, notre système exploite la structure arborescente fournie par la représentation DOM de la page web.\nLe DOM permet de définir le chemin de chaque élément (noeud ou feuille) de l\u0027arbre. Pour chaque élément, nous définissons ce chemin comme un ensemble d\u0027étapes depuis la racine. Chaque étape est un couple (balise :position) défini à partir de l\u0027étape précédente (on considère l\u0027étape 0 comme étant la racine du document). La position est le numéro du fils du noeud défini à l\u0027étape précédente tandis que la balise est la balise HTML que le noeud représente. Par exemple, une page web contient un élément racine \u003chtml\u003e qui a deux fils, \u003chead\u003e et \u003cbody\u003e. Le chemin de l\u0027élément \u003cbody\u003e est donc body : 1. Cette définition de chemin est celle employée pour les annotations primaires présentées figure 2.\nA partir de cette définition du chemin d\u0027un élément de l\u0027arbre, on définit la notion de chemin similarisé. Un chemin similarisé est la factorisation des chemins de plusieurs élé-ments. Le chemin ainsi généré est ainsi un chemin de plusieurs éléments. Pour cela, les étapes sont comparées 2 à 2 et les différences marquées par une astérisque. Prenons l\u0027exemple des deux premières annotations primaires présentées figure 2. Le chemin du premier élément est body : 1,table :0,tbody :0,tr :0,td :0,b  Relations entre la représentation arborescente de la page web et l\u0027ontologie Cette défini-tion de chemin est fondamentale pour l\u0027apprentissage. Notre stratégie d\u0027annotation est fondée sur l\u0027hypothèse d\u0027une corrélation entre la représentation arborescente d\u0027un document et les concepts et rôles définis par l\u0027ontologie. Ces hypothèses sont les suivantes : -chaque instance de concept est exactement une feuille de l\u0027arbre, -les instances de rôles sont contenues dans des sous-arbres De ces hypothèses, on déduit qu\u0027identifier une instance de l\u0027ontologie revient à déterminer le chemin depuis la racine vers une feuille de l\u0027arbre pour une instance de concept et vers un noeud, racine du sous-arbre, pour une instance de rôle. L\u0027apprentissage consiste donc à déterminer un chemin similarisé pour chaque concept et chaque rôle de l\u0027ontologie.\nApprentissage de chemins similarisés Pour chaque concept dont des exemples ont été marqués par l\u0027expert, le chemin similarisé du concept est généré à partir de l\u0027ensemble des chemins enregistrés dans les annotations primaires pour ce concept. Dans l\u0027exemple figure 1, 5 annotations primaires sont définies pour le concept Project. En factorisant les chemins deux à deux, le chemin similarisé obtenu est body : 2, table : * , tbody : 0, tr : * , td : 1, a : 0, f ont : 0. Il ressort ainsi que les éléments correspondant aux concept Project sont situés dans la deuxième colonne des tableaux du document.\nPour les instances de rôles, une première étape consiste à déterminer les racines des sousarbres de chaque rôle tel que :\n-il existe un rôle R A,B dans l\u0027ontologie reliant des concepts A et B, -au moins une instance de A et une instance de B ont été marquées. Alors pour chaque instance marquée de A :\n-le plus petit parent commun (pppc) dans l\u0027arbre de cette instance avec chaque instance de B est déterminé, -le noeud le plus profond dans l\u0027arbre parmi ces pppc est alors un noeud racine pour le rôle R A,B . Le chemin similarisé des noeuds racines générés est alors inféré. La sortie de l\u0027apprentissage est donc un chemin similarisé de chaque concept et de chaque rôle de l\u0027ontologie ayant des instances marquées dans le document.\nAnnotation par génération d\u0027instances de l\u0027ontologie\nAnnotation par application des chemins similarisés Les chemins similarisés sont appliqués sur une page dont la structure DOM est similaire à la page d\u0027apprentissage. Les noeuds -308 -RNTI-E-6 reconnus par le chemin similarisé appris pour chaque rôle R A,B sont les racines des sousarbres en dessous desquels chaque instance de a est liée à une instance de b par une instance de R A,B . Les feuilles reconnues par le chemin similarisé d\u0027un concept sont des candidates pour être instanciées par ce concept. Deux cas sont possibles : si une feuille n\u0027est reconnue que par un seul chemin similarisé, cette feuille est instanciée par le concept correspondant à ce chemin. Pour toutes les feuilles situées dans un sous-arbre, une relation est générée entre les instances de concepts définies par le rôle. Si plusieurs chemins similarisés conduisent à la même feuille, un mécanisme de raisonnement doit être appliqué pour déterminer à quel concept cette feuille appartient. On atteint les limites d\u0027une méthode purement syntaxique.\nDans notre exemple, le chemin similarisé du concept Project décrit ainsi le fait qu\u0027il est associé aux éléments contenus dans la colonne de droite des tableaux tandis que les concepts Lecturer et FacultyMember sont associés au contenu de la colonne de gauche.\nAnnotation par un concept plus général dans l\u0027ontologie Lorsqu\u0027un même élément peut être annoté par deux concepts différents, un raisonnement est effectué au niveau de l\u0027ontologie pour déterminer le concept subsumant les deux concepts candidats. Dans notre exemple, le raisonneur Pellet (Sirin et Parsia (2004) évalue les performances de Pellet pour la classification et les requêtes) est utilisé pour classifier les concepts de l\u0027ontologie et déterminer le concept subsumant Lecturer et FacultyMember dans SW RC. Il s\u0027agit de AcademicStaff. Une instance de ce concept sera donc générée pour les éléments reconnus par les chemins similarisés appris à partir des concepts Lecturer ou FacultyMember.\n"
  },
  {
    "id": "1004",
    "text": "Introduction\nLa découverte de connaissances temporelles est un enjeu majeur pour le diagnostic de systèmes dynamiques (Das et al., 1998), (Dousson et Vu Duong, 1999), (Keogh et Smyth, 1997), (Agrawal et al., 1995), (Faloutsos et al, 1994). Récemment, Bouché P. et  ont proposés une approche stochastique pour découvrir des modèles de chroniques à partir d\u0027une séquence d\u0027événements discrets. Nos travaux visent à compléter cette approche pour identifier les classes d\u0027événements contribuant le plus significativement à la prédiction de l\u0027occurrence d\u0027une classe particulière.\nLes arbres de décisions (Breiman, 1984), (Murthy, 1998), sont largement utilisés pour classer des séquences de données (Kadous, 1999), (Geurts, 2001), (Drucker et Hubner, 2002), (Rodriguez et Alonso, 2004). Récemment, l\u0027algorithme ID3 (Quinlan, 1986) a été adapté pour construire des arbres temporels de décision (Console et al., 2003) à partir d\u0027un ensemble de situations. Cette adaptation montre que l\u0027entropie informationnelle permet d\u0027identifier les variables contribuant le plus significativement à une prise de décision.\nNous proposons donc d\u0027utiliser un critère entropique pour analyser des modèles de chroniques. Après un bref rappel sur les arbres temporels de décision, cet article présente une adaptation de l\u0027algorithme proposée par Console pour la déduction de modèles de chroniques à partir d\u0027un ensemble de séquences d\u0027occurrences d\u0027événements discrets et montre sur un exemple comment l\u0027approche entropique peut être utilisée pour compléter l\u0027approche stochastique.\nArbres temporels de décision\nUn arbre de décision est une structure T\u003d \u003cr, N, E, L\u003e où N\u003dN I ?N L est l\u0027union d\u0027un ensemble N I \u003d{x i } de noeuds internes désignant une variable x i et un ensemble N L \u003d{a i } de noeuds feuilles désignant une décision ai, r?N est le noeud racine de l\u0027arbre, E ? N I × N est un ensemble d\u0027arcs, un arc attribuant une valeur v j à une variable x i et L est une fonction d\u0027étiquetage définie sur N?E, qui retourne le nom de la variable x i associé au noeud de N I , la décision a i associée à une feuille de N L ou la valeur v J associée à un arc de E.\nL\u0027algorithme ID3 utilise l\u0027entropie informationnelle dans un ensemble de cas ? pour construire un arbre de décision de profondeur minimal. Un cas e est une collection de valeurs v j prises par un ensemble de variables x i conduisant à une décision a i particulière. A chaque noeud, ID3 choisit la variable x qui minimise l\u0027entropie ?(x, ?) dans l\u0027ensemble ? des cas : Temporal ID3 est une extension d\u0027ID3 à des données datées selon une horloge à temps discrète (Console et al, 2003). Un arbre temporel de décision est un arbre de décision où un noeud est un couple (x i , t k ), x i désignant une variable et t k la date d\u0027observation de sa valeur, et un arc défini une valeur v j de x i à la date t k (i.e. x i (t k )\u003dv j ). Il s\u0027agit donc d\u0027une structure T\u003d \u003cr, N, E, L, ?\u003e dotée d\u0027une fonction d\u0027étiquetage du temps ?: N I ?? + qui donne la date associée à un noeud interne. L\u0027ensemble d\u0027apprentissage est une collection de situations S\u003d {s e\u003d0,..,m }. Une situation s e est l\u0027ensemble des valeurs v j prises par un ensemble de variables X\u003d {x i } à chaque instant d\u0027observation t k conduisant à une décision a n particulière (Table 1). Une situation s e réfère à une horloge à temps discret où t k ?kT, k?? et T?? + , T est une période d\u0027échantillonnage. \nTAB. 1 -Exemple de table temporelle de décision.\nDans la table 1, les variables x 1 , x 2 , x 3 prennent une valeur qualitative n, h, l, ou v aux instants d\u0027observations t 0 , t 1 , t 2 , t 3 . Dans la première situation s 1 (resp. s 2 ), la décision a 1 (resp. a 2 ) doit être prise au plus tard à la date t 3 (resp. t 2 ). Cette date est dite « limite » car la connaissance de la valeur des variables au-delà de cette date est inutile à la prise de décision.\n[ ]\nUne partition S e est un sous ensemble de S contenant des situations identiques sur un intervalle de temps :\nAinsi, à chaque instant d\u0027observation t, S est partitionné en un ensemble de partitions S ?,t \u003d {S e\u003d0,.., m } (équation 3). TemporalID3 construit un arbre en recherchant un intervalle de temps qui maximise un critère lié au nombre de partitions. Puis, de la même manière qu\u0027ID3, TemporalID3 choisit la variable x i (t) qui minimise l\u0027entropie sur cet intervalle (4) et créé le noeud correspondant. Toutes les valeurs des variables à tous les instants précédent t sont éliminées du tableau, y compris celle de x i (t), puis TemporalID3 recommence son traitement. \nUne séquence ?\u003d{o k } k\u003d0…m-1 est une suite ordonnée de m occurrences o k ?(t k , x, i) d\u0027événements discrets e k ?(x, i) où t k ???? + est la date de l\u0027affectation de la valeur i à la variable x . Un couple (o k , o k+1 ) de deux occurrences successives liées à une même variable x décrit l\u0027évolution temporelle de la fonction discrète x(t), définie sur ? (équation 5). L\u0027ensemble des événements discrets E d \u003d{e k ?(x, i)} est partitionné en un ensemble de classes C j \u003d{e k }. La notation \"o i ::C k \" signifie que l\u0027occurrence o i appartient à la classe C k . Une fonction d fournit la date d\u0027une occurrence en sorte qu\u0027une séquence ? n d\u0027un ensemble ?\u003d{? n } définie son propre sous ensemble ? ?n \u003d {t j } de date inclus dans l\u0027ensemble des dates ? défini par ? (équation 6 où O désigne l\u0027ensemble des occurrences de ?). Un modèle de chroniques est un ensemble de relations binaires temporellement contraintes entre des classes d\u0027événements (équation 7).\nUne séquence classée Ok, notée ? n ok , est un exemple qui respecte toutes les contraintes logiques et temporelles d\u0027un modèle de chroniques. Une séquence classée Ko, notée ? n ko , est un contre exemple. Un contre exemple respecte toutes les contraintes d\u0027un modèle de chroniques sauf la dernière relation binaire qui concerne la classe à prévoir. \nExtraction d\u0027un modèle de chroniques\nLa table temporelle à temps continu est une matrice H définie sur ?×X×? où un élément h[? i , x, t] défini la classe liée à la valeur de la variable x à l\u0027instant t dans la séquence\nLa date limite d\u0027une séquence est la date de la dernière occurrence sans prendre en compte les occurrences ajoutées par complétion. La figure 3 montre le résultat de l\u0027application de TemporalID3 sur ?. Le modèle de chroniques est construit en ne considérant que les branches menant à une décision Ok en parcourant l\u0027arbre en profondeur d\u0027abord depuis les feuilles terminales à la racine de l\u0027arbre (les dates ont été inversées). Le modèle est initialisé par la classe C 1 correspondant à la décision Ok (i.e. « B »). Pour tout arc (n 1 ?n 2 )?? de l\u0027arbre, la classe C 2 associée n 2 est créée dans le modèle de chroniques et une relation R(\n) est la moyenne des durées écoulées entre les occurrences de la même classe C i dans les séquences ? n de ?. Le modèle de chroniques de la figure 3 est construit à partir de la branche ((v_a, 0.092626)?(v_c, 0.68979)?(Ok)). La relation R(A, C, [0,2.14]) du modèle de la figure 1 n\u0027apparaît pas dans le modèle produit par l\u0027approche entropique. Cela induit l\u0027idée que cette relation n\u0027apporte que peu d\u0027information pour prédire une occurrence de la classe B.\nConclusion\nCet article propose une adaptation de l\u0027algorithme TemporalID3 pour la découverte des modèles de chroniques à partir d\u0027un ensemble de séquences d\u0027occurrences d\u0027événements discrets. L\u0027intérêt de cette approche est d\u0027utiliser un critère de minimisation entropique pour identifier les classes des modèles de chroniques les plus signifiantes afin de prédire l\u0027occurrence d\u0027une classe dans une tâche de diagnostic de systèmes dynamiques. Les premiers résultats obtenus invitent à envisager une combinaison des approches entropiques et stochastiques pour découvrir des connaissances temporelles avec un fort pouvoir anticipatif.\nRéférences\nAgrawal, R., K. I. Lin, H. S. Sawhney, et K. Shim (1995). Fast similarity search in the presence of noise, scaling, and translation in time-series databases. In Proc. of the 21st Int\u0027l Conf. on Very Large Databases, pp 490-50.\n"
  },
  {
    "id": "1005",
    "text": "Summary\nIn this Paper, we present a database of Arabic image writing for the use in Arabic OCR systems. The topics addressed by ARABASE concern different styles of documents: machine printed text, off line and on line handwriting. Data corresponds to a variety of context: city names, literal amounts, isolated characters, digits, free texts, words/sub-words, isolated characters. ARABASE contains also information describing the process of data acquisition. Therefore, we use the method oriented object UML for modelling the system. ARABASE provides multiple functionalities to their users (webmaster and clients).\n"
  },
  {
    "id": "1007",
    "text": "Introduction\nCertains hôpitaux sont des entités complexes faites de plusieurs bâtiments plus ou moins dispersés. Des informations importantes sont rattachées à ces bâtiments et des décisions doivent être prises.\nOr habituellement toutes ces informations sont éparpillées dans des schémas et des tableaux de chiffres de telle sorte que les décideurs qui utilisent ces données ont une représen-tation fragmentée de la réalité sous-jacente.\nC\u0027est dans ce contexte que nous avons décidé d\u0027utiliser la méthode Caseview généralisé pour créer un support permettant de convoyer de façon synthétique des informations à la fois topographiques et quantitatives concernant un hôpital.\nMéthode\nLa méthode Archiview est une méthode issue de la méthode Caseview généralisé (Lévy, 2004). Cette dernière consiste à visualiser des données au moyen d\u0027un référentiel bidimensionnel construit en identifiant un pixel avec une entité informationnelle. Les pseudo pixels sont alors ordonnés selon 3 critères : un critère binaire, un critère nominal et un critère ordinal. \nArchiview, un outil de visualisation topographique\nUne fois le référentiel construit on l\u0027utilise pour visualiser des paramètres variés : chaque valeur associée à chaque entité informationnelle est placée dans le pseudo pixel lui correspondant dans le référentiel. Puis la définition d\u0027une échelle de couleurs permet de visualiser le paramètre étudié. Dans la méthode Archiview les entités informationnelles sont les étages des bâtiments d\u0027un hôpital. Le critère nominal est un critère topographique : chaque colonne du référentiel contient les « pixels-étages », regroupés par bâtiment, appartenant à des bâti-ments proches. Le critère ordinal est l\u0027ordre des étages : dans la zone correspondant à chaque bâtiment les « pixels-étages » sont ordonnés de bas en haut par rapport à la base. Il n\u0027y a pas de critère binaire.\nRésultats\nL\u0027exemple choisi est la visualisation du nombre de points d\u0027archivage d\u0027un hôpital (FIG.1). On voit que ces points sont dispersés dans tout l\u0027hôpital, le bâtiment 1 en ayant le plus grand nombre.\n"
  },
  {
    "id": "1009",
    "text": "Introduction\nCes dix dernières années un grand nombre de travaux ont été proposés pour rechercher des motifs fréquents dans de grandes bases de données. En fonction des domaines d\u0027applications les motifs extraits sont soit des itemsets (Srikant, 1995;Zaki, 2001;Pei et al., 2001;Ayres et al., 2002) soit des séquences (Agrawal et al., 1993;Han et al., 2000). Récemment les travaux issus de la communauté des chercheurs en base de données et en fouille de données considèrent le cas des data streams où l\u0027acquisition des données s\u0027effectue de façon régulière, continue ou incrémentalement et cela sur une durée longue voire éventuellement illimitée.\nCompte tenu de la grande quantité d\u0027information mise en jeu dans le cas des data streams, le problème de l\u0027extraction de motifs fréquents est toujours d\u0027actualité ( (Li et al., 2004;Jin et al., 2003;Demaine et al., 2002;Manku et Motwani, 2002;Golab et Ozsu, 2003;Karp et al., 2003)). Dans ce contexte, un motif est dit ?-fréquent s\u0027il est observé au moins une fraction ?, appelée support du motif, sur tout le stream. Le paramètre theta, tel que 0 \u003c ? \u003c 1, est fixé par l\u0027utilisateur.\nDans le cas des data streams, sujets à des mises à jour régulières et fréquentes, les approches traditionnelles ne conviennent pas car les résultats obtenus pour l\u0027ancienne base ne sont que partiellement valables pour la nouvelle et il n\u0027est pas envisageable de relancer l\u0027algorithme sur la base de données mise à jour. En effet, dans tous les travaux développant une approche par mise à jour incrémentale (Masseglia et al., 2003;Cheng et al., 2004), la problématique principale d\u0027optimisation et de performance consiste à construire et à maintenir, au fur et à mesure des différentes mises à jour successives, un ensemble de motifs candidats. Celui-ci est utilisé pour mettre à jour les motifs fréquents et éviter de relancer l\u0027algorithme depuis zéro.\nIl convient également de souligner une autre caractéristique intrinsèque des data streams qui découle du fait que la connaissance du stream n\u0027est que partielle quel que soit l\u0027instant considéré. En conséquence, il est nécessaire de prendre en compte l\u0027incertitude engendrée par la connaissance toujours incomplète du data stream. Précisément, cela se traduit dans le cas de la recherche de motifs fréquents en soulignant que les motifs fréquents obtenus ne sont en fait que des motifs fréquents observés. En fait, à cause de cette incertitude, deux sources d\u0027erreurs doivent être considérées :\n-Les motifs observés comme fréquents ne sont peut être plus du tout fréquents sur une longue période d\u0027observation du stream. -Inversement des motifs classés comme non fréquents peuvent le devenir sur une plus longue période d\u0027observation du stream. Pour éviter ces erreurs, il est nécessaire de développer une approche pour connaître si un motif est fréquent sur une partie déjà examinée du stream. Cette approche doit de plus être prédictive pour savoir avec quelle probabilité un motif est fréquent ou non sur l\u0027ensemble du stream. De nombreuses applications, par exemple dans le domaine de la prévision météorologique ou encore dans l\u0027analyse de tendance en finance nécessitent ce type d\u0027approche. Même en disposant d\u0027une très grande partie du stream, d\u0027un point de vue statistique, il est impossible de s\u0027affranchir de ces deux sources d\u0027erreur (Vapnik, 1998). Notre objectif sera donc d\u0027essayer d\u0027approcher le mieux possible une solution optimale.\nDans cet article nous proposons une approche qui permet, tout en considérant l\u0027incertitude inhérente à la connaissance des streams, de construire et de maintenir des ensembles de motifs candidats bien choisis. Ceci constitue un préalable fondamental et nécessaire à toute approche pertinente dans le cadre de la mise à jour incrémentale des data streams.\nLa suite de la présentation est organisée de la façon suivante. Dans le paragraphe 2, nous introduisons les concepts qui permettent de contrôler l\u0027incertitude découlant des sources d\u0027erreurs. Au paragraphe 3, nous montrons comment obtenir les ensembles de motifs pertinents pour effectuer la mise à jour incrémentale. Le paragraphe 4 présente une expérimentation de notre approche, suivie d\u0027une analyse comparative avec des travaux connexes au paragraphe 5. Nous concluerons notre étude au paragraphe 6.\ndépendemment à partir d\u0027une distribution D sur laquelle nous ne formulons aucune hypothèse, excepté celle de dire qu\u0027il n\u0027y a pas de biais. Nous renvoyons le lecteur intéressé par des approches prenant en compte le biais dans les cas de fouilles de données supervisées aux travaux de (Fan et al., 2004;Wang et al., 2003). La partie du stream observée est représentée par S dans la figure 1. A partir d\u0027une valeur fixée par l\u0027utilisateur du paramètre ?, le support théorique, on voudrait connaître l\u0027ensemble des motifs vrais ?-fréquents de X. Cet ensemble nommé X ? est représenté en grisé sur la figure 1. Hormis l\u0027incertitude et les aspects liés à l\u0027estimation statistique, l\u0027approximation de l\u0027ensemble X ? recouvre un aspect combinatoire qui provient de la très grande taille de X même si cet ensemble peut être fini. Nous nommons S l\u0027ensemble des motifs observés extraits du stream avec |S| \u003d m (|S| \u003c\u003c |X|). Nous réduisons cette différence à l\u0027aide d\u0027un algorithme qui permet d\u0027obtenir un super ensemble S * de S avec |S * | \u003d m * \u003e m. Typiquement, S * contient des motifs supplémentaires obtenus à partir d\u0027une généralisation des éléments de S (Mannila et Toivonen, 1997). Ce n\u0027est pas le propos de cet article de traiter l\u0027aspect combinatoire, l\u0027essentiel est que S * ne sera jamais suffisamment grand pour englober X ? , au regard de la façon dont il est construit (voir figure 1). Ainsi l\u0027importance du problème d\u0027estimation statistique demeure entier.\nSoit l\u0027ensemble X * ? (figures 1 et 2) qui représente l\u0027ensemble des motifs vrais ?-fréquents de X pour l\u0027ensemble S , nous rappelons que cet ensemble ne peut être connu totalement compte tenu non seulement du fait que |S| \u003c\u003c |X|, mais aussi des deux sources d\u0027erreurs précédemment indiquées qui en découlent. Nous recherchons ? pour approcher au mieux l\u0027ensemble X * ? , en utilisant S * ? (figure 2) qui représente l\u0027ensemble des motifs observés ?\nPour apprécier l\u0027approximation effectuée, consécutivement aux deux sources d\u0027erreurs , les sous-ensembles X RNTI-E-6\nBordures statistiques pour la fouille incrémentale dans les data streams A partir des ensembles définis ci-dessus, on peut indiquer les formules de la précision et du rappel qui permettent d\u0027estimer l\u0027approximation effectuée.\nLa précision permet de quantifier la proportion des motifs ?-fréquents estimés qui sont en fait non vrais ?-fréquents, en dehors de S * ? . Si on cherche à Maximiser P, cela revient à minimiser la première source d\u0027erreurs. Symétriquement, le rappel permet de quantifier la proportion de motifs, vrais ?-fréquents manquant dans S * ? . Si on cherche cette fois à maximiser R, cela revient à minimiser la seconde source d\u0027erreurs. Une approche naïve pour approcher au mieux l\u0027ensemble X * ? consisterait à choisir ? \u003d ? ; autrement dit, la question que l\u0027on pourrait se poser est de savoir si ? est un support statistique pour lui-même. Malheureusement, la principale et seule propriété de l\u0027ensemble S * ? dans ce cas, est qu\u0027il tend à correspondre avec une probabilité de 1 à l\u0027ensemble X * ? lorsque le cardinal de l\u0027ensemble S * tend vers ? selon le lemme de Borel-Cantelli (Devroye et al., 1996). Cela reviendrait à connaître tout le stream, ce qui n\u0027est pas possible en pratique. Le théorème de Glivenko-Cantelli permet de montrer que l\u0027on peut borner l\u0027erreur commise sur les motifs vrais ?-fréquents en fonction de divers paramètres parmi lesquels le cardinal de l\u0027ensemble S * , mais on ne peut pas faire mieux. Bien souvent, en traitement de l\u0027information ou encore en medecine, plutôt que de simplement borner l\u0027erreur, il est beaucoup plus important d\u0027estimer et de contrôler des parties de l\u0027erreur. On peut par conséquent développer une approche qui consiste à maximiser soit la précision P soit le rappel R. Le choix des valeurs aux limites de ? pourraient convenir en ce sens où l\u0027on maximise la précision ou le rappel mais ces valeurs sont inintéressantes pour les applications en fouille de données. Par exemple, si on choisit ? \u003d 0, on obtient S * 0 \u003d S * et ainsi R \u003d 1. Mais, nous avons également dans ce cas P \u003d |X * ? |/|S * |, qui correspond à une valeur trop faible pour bien des applications, et donc tous les éléments de S * sont considérés comme des motifs vrais ?-fréquents du stream. On pourrait aussi choisir ? \u003d 1 pour être sûr de maximiser P cette fois, mais il en découle que R \u003d 0 et il se pourrait qu\u0027aucun élément de S * ne soit un motif vrai ?-fréquent. Ces exemples avec les valeurs aux limites de ? nous permettent de bien cadrer les principes de notre approche. L\u0027idée générale est de choisir subtilement ? différent mais suffisamment proche de ?, respectivement plus grand ou plus petit que ?, de sorte qu\u0027il soit possible de contrôler en maximisant soit la précision soit le rappel, pour garantir avec une très forte probabilité que P \u003d 1 ou respectivement que R \u003d 1 tout en limitant la dégradation du paramètre non contrôlé. On obtient ainsi un ensemble S * ? pas trop petit contenant des informations significatives. Il y a une barrière statistique autour de ? qui empêche que ? ne soit ni trop proche ni trop éloigné de ? pour conserver la contrainte que P \u003d 1 ou alors que R \u003d 1, avec une forte probabilité. Notre objectif pour maximiser la précision ou le rappel avec une forte probabilité est par conséquent de rechercher les valeurs de ? les plus proches possibles de cette barrière. Le théorème ci-dessous permet d\u0027établir les valeurs des supports statistiques que nous proposons en calculant la valeur de ? :\nChoix de ?\nSi on fixe ? \u003d ? + ?, alors P \u003d 1 avec une probabilité au moins de 1 ? ?. Si on fixe ? \u003d ? ? ?, alors R \u003d 1 avec une probabilité au moins de 1 ? ?. ? est le risque statistique lié aux data streams. Les valeurs ? + ?, ? ? ? sont les supports statistiques au sens de la définition 1.\nLes supports obtenus (? \u003d ? ± ?) sont statistiquement presque optimaux. Faute de place, nous renvoyons à l\u0027article (Nock et al., 2005) 1 pour la démonstration complète. Celle-ci repose sur l\u0027utilisation d\u0027inégalités de concentration de variables aléatoires, qui, dans ce cas préçis, permettent d\u0027obtenir des résultats statistiquement presque optimaux. Par optimalité, nous entendons que toute technique d\u0027estimation obtenant de meilleures bornes est condamnée à se tromper (le critère à maximiser n\u0027est plus ègal à un) quelque soit son temps de calcul.\nBordures statistiques pour la mise à jour incrémentale\nDans cette section nous introduisons deux bordures statistiques (supérieure et inférieure) qui vont être pertinentes dans le choix des motifs fréquents à conserver lors de la mise à jour incrémentale. L\u0027objectif avec la bordure statistique inférieure est de maximiser la précision P, tandis qu\u0027avec la bordure statistique supérieure il s\u0027agit de maximiser le rappel R. Nous adoptons la notation probabiliste définie par (McAllester, 1999).\n2\nA partir du théorème 1, nous définissons l\u0027ensemble suivant : pour un risque statistique ? fixé par l\u0027utilisteur, en choisissant ? \u003d ? + ?, on construit l\u0027ensemble S * ?+? tel que ? ? , S * ?+? ? X * ? avec ? ? , P \u003d 1. Ainsi, il n\u0027y a plus la première source d\u0027erreurs avec une forte probabilité. Tous les motifs de S * ?+? sont des motifs, vrais ?-fréquents de X * ? , mais on ne les a pas tous (voir figure 3). S * ?+? est le plus grand ensemble possible qui contient uniquement des motifs vrais ?-fréquents de X * ? après un temps d\u0027observation du stream. Nous définissons cet ensemble comme étant la bordure statistique inférieure de X * ? à partir de l\u0027échantillon des motifs S * de X. De façon symétrique, à partir du théorème 1, nous définissons l\u0027ensemble suivant : pour un risque statistique ? fixé par l\u0027utilisteur, en choisissant ?\nAinsi, il n\u0027y a plus la deuxième source d\u0027erreurs avec une forte probabilité, c\u0027est à dire que S * ??? contient tous les motifs, vrais ?-fréquents de X * ? , mais il en contient d\u0027autres (figure 4). S * ??? est le plus petit ensemble possible qui contient tous les motifs vrais ?-fréquents de X * ? après un temps d\u0027observation du stream. Nous définissons cet ensemble comme étant la bordure statistique supérieure de X * ? à partir de l\u0027échantillon des motifs S * du stream X.\nFIG. 5 -Bordures statistiques pour la mise à jour incrémentale.\nLa figure 5 illustre l\u0027utilisation de bordures dans un processus de mise à jour incrémentale. Nous recherchons les motifs ?-fréquents à partir des bordures statistiques (notées (a) et (b) sur la figure 5). Ces bordures sont construites pour le support ? choisi à partir de la valeur de ?, calculée pour S (DB). Ces bordures, nous permettent d\u0027approcher au mieux l\u0027ensemble de tous les motifs vrais ?-fréquents pour S , qui représente la base DB mise à jour par l\u0027ajout de db, S \u003d S ? db. Dans la section suivante lors de nos expérimentations, nous comparerons les motifs ?-fréquents obtenus grâce aux bordures statistiques par rapport à l\u0027ensemble X * ? ( (c) sur la figure 5). Cet ensemble représente les vrais motifs ?-fréquents après mise à jour incrémentale (DB ? db). . Ces données représentent la navigation d\u0027utilisateurs sur ce site. La taille du fichier de log représente environ 2,54Go (132k transactions). La deuxième base de données, appelée \"BuAG\", est obtenue à partir des 3,48Go (54k transactions) de Web log du serveur web de la bibliothèque de l\u0027Université 4 . Pour analyser la qualité de nos bordures stastiques, nous évaluons différentes situations en faisant varier un ensemble de paramètres (une exception est faite pour ?, qui est fixé à .05). Les variations de ces paramètres sont décrites dans la figure 6. Le premier paramètre définit les différentes valeurs de support sur lesquelles nous allons réaliser l\u0027expérimentation (\"?\" ). Le second paramètre, définit la taille de DB par rapport à la taille du stream simulé (\"taille DB\"). Le dernier paramètre, quant à lui, définit la taille de db par rapport à celle de DB. Dans notre cas db représentera au plus 50% de DB (\"taille db\"), ce paramètre permet de contrôler la taille de l\u0027incrément par rapport à la partie stockée initialement. Pour gérer et organiser ces expériences un générateur est chargé de coordonner tous les tests à effectuer.\nAu lieu d\u0027utiliser un vrai data stream, qui aurait pu limiter la qualité de l\u0027évaluation de nos bordures statistiques, nous avons choisi d\u0027en simuler un à l\u0027aide de la connaissance de son domaine X. Plus précisément, pour simuler le stream nous échantillonnons chaque base de données en fragments DB (S). Par exemple, nous pouvons considérer que les données arrivent successivement depuis la base de données \"Dragons\" et que nous ne pouvons en stocker que 20%. Pour cela nous prenons 20% des transactions contenues dans cette base de données et nous les conservons dans DB. Il ne reste alors plus qu\u0027à prendre un incrément, db par exemple de taille 10%, de cette base. Nous construisons alors les bordures statistiques (bordure inférieure et bordure supérieure) définies dans la section 3 pour DB.\nLes figures 7 et 8 montrent les résultats d\u0027expériences obtenues, avec ? \u003d 0.05, respectivement sur les bases Dragons et BuAG. Pour évaluer la qualité de ces bordures pour la mise à jour, nous représentons leurs comportements pour P et R par rapport à S (S \u003d DB ? db). Ainsi, les courbes P ?+? et P ??? , représentent la précision respectivement pour la bordure statistique inférieure et pour la bordure statistique supérieure. De même les courbes R ?±? représentent le rappel. Les courbes P ? et R ? correspondent au choix trivial ? \u003d ?. Nous constatons que ces courbes ont un comportement assez similaire :\n-la précision P vaut ou approche 1 pour la plupart des bases stockées quand ? \u003d ? + ?,  \nFIG. 7 -représente les courbes P (à gauche) et R (à droite) sur la base Dragons pour trois valeurs de\nvaleurs de ? et deux tailles de S (% par rapport à |X|).\n-le rappel R vaut ou approche 1 pour la plupart des bases stockées quand ? \u003d ? ? ?. Ces observations sont en accords avec les résultats théoriques de la section 2. Nous observons également un autre phénomène : le rappel R associé à ? \u003d ? + ? n\u0027est pas très éloigné de celui associé à ? \u003d ?. Il en est de même pour la précision P associée à ? \u003d ? ? ?. Ce qui montre que la maximisation de la précision P ou du rappel R est obtenue avec un coût réduit au niveau de la dégradation de l\u0027autre paramètre. Nous notons aussi que les courbes de précision P ont de meilleures performances que celles du rappel R notamment sur la figure 8. Ceci n\u0027est pas très surprenant (c.f. section 2), car la fourchette de valeur pour la précision P est beaucoup plus restreinte que pour le rappel R.\nLa variation de la taille de S (DB) possède également une importance et vient conforter un résultat théorique auquel on pouvait s\u0027attendre. En effet si on s\u0027intéresse au rappel R, on constate, que ce soit sur les figures 7 ou 8, que les valeurs observées pour cette quantitée augmentent avec la taille de S. Cela traduit le fait que plus nous possédons de données observées plus la qualité de la prédiction augmente.\nSur ces bases de données un autre phénomène semble apparaître. Premièrement, à cause des faibles valeurs de ?, certains tests n\u0027ont pas pu être réalisés car la valeur de ? ? ? était inférieure à 0. De plus, les différences observées entre les courbes semblent être liées aux tailles des bases de données utilisées. La base de données BuAG est plus petite que celle de Dragons d\u0027un facteur 2.4. Nous pensons que ceci explique la différence entre les courbes : il s\u0027agit de phénomènes liés à des bases de données de petites tailles et qui ne devraient pas être présents dans des bases de données plus conséquentes ou dans de vrais data streams.\nSur ces courbes (figures 7 et 8), le choix de ? \u003d ? donne de meilleurs résultats en ce qui concerne la moyenne de P et R que le choix des deux valeurs de ? , sachant que ni P, ni R ne sont très proches de 1 avec une forte probabilité dans ce cas. Avec notre approche, on peut efficacement optimiser le processus de mise à jour incrémentale. Dans le cas où on aurait un  \nFIG. 8 -représente les courbes P (à gauche) et R (à droite) sur la base BuAG pour trois valeurs de\nvaleurs de ? et deux tailles de S (% par rapport à |X|).\nespace suffisant de stockage, on choisirait ? \u003d ? ? ?, la bordure supérieure, pour laquelle on possède des garanties importantes sur la présence des futurs fréquents en son sein (R \u003d 1). Dans ce cas, le nombre de calculs supplémentaires sera faible lors de la mise à jour. Par contre dans le cas où on devrait se limiter pour des raisons de taille à conserver une bordure disons moins volumineuse, ? \u003d ? + ?, la bordure inférieure, sera alors utilisée car elle contient les informations les plus pertinentes (P \u003d 1). On peut ainsi voir les bordures statistiques comme des outils de mise à jour incrémentale indiquant les limites au delà desquelles : soit il est inutile de stocker plus d\u0027informations dans le cas de ? \u003d ? ? ? (valeur limite inférieure pour ? ) ; soit la perte d\u0027information serait trop importante ? \u003d ? + ? (valeur limite supérieure pour ? ).\nTravaux connexes\nDepuis 1996, de nombreux travaux de recherche se sont focalisés sur la maintenance des ensembles de motifs fréquents obtenus dans les bases de données statiques. Dans ce paragraphe nous regardons leur adéquation par rapport à notre problématique. Partition et FUP (Fast UPdate) (Cheung et al., 1996) sont deux algorithmes où un partitionnement de la base de données est effectué pour rechercher dans un premier temps les itemsets fréquents locaux relatifs à chaque partition puis dans un second temps en déduire les itemsets fréquents globaux par validation croisée. Cela repose sur l\u0027hypothèse que les itemsets fréquents de la base doivent l\u0027être dans au moins l\u0027une des partitions. (Parthasarathy et al., 1999) ont développé un algorithme de mise à jour incrémentale ISM (Incremental Sequence Mining) en maintenant un treillis, de motifs de la base, construit à partir de tous les motifs fréquents et de tous les motifs de la bordure négative. (Zheng et al., 2002) ont développé un algorithme IUS( Incrementally Updating Sequence) en utilisant une valeur support pour limiter la taille de l\u0027espace des candi-dats de la bordure négative. Ces différentes approches se heurtent aux inconvénients inhérents à l\u0027utilisation de la bordure négative : -l\u0027espace des motifs candidats à maintenir est très important ; -il est nécessaire de considérer les relations structurelles qui existent entre les motifs notamment dans le cas des motifs qui auraient une faible valeur de support. Les différents algorithmes utilisant la bordure négative sont très coûteux en temps et sont consommateurs d\u0027espace mémoire. (Masseglia et al., 2003) ont développé un algorithme de mise à jour incrémentale ISE utilisant une approche par génération et test de candidats. Les inconvénients sont que :\n-l\u0027espace des candidats peut être très grand, ce qui rend la phase de test très lente ; -l\u0027algorithme requiert plusieurs passages sur toute la base. Cela est très coûteux en temps particulièrement pour les motifs séquentiels à séquences longues.\nVrais fréquents L\u0027algorithme IncSpan (Incremental Mining of Sequential Patterns in large database) développé par (Cheng et al., 2004) repose sur une approche statistique où l\u0027on construit un ensemble de motifs semi-fréquents (SFS) en diminuant la valeur du support à partir d\u0027un ratio. L\u0027idée est de dire qu\u0027à partir d\u0027un ensemble de motifs \"presque fréquents\" (SFS), plusieurs des motifs fréquents de la base mise à jour proviendraient de SFS ou alors seraient déjà observés fré-quents dans la base connue précédant la mise à jour. Autrement dit, SFS constituerait une zone frontière entre les motifs fréquents et non fréquents. Sur la figure 9, nous représentons un exemple d\u0027ensemble SFS. Cette approche présente des insuffisances majeures d\u0027un point de vue statistique tant pour l\u0027estimation de l\u0027incertitude intrinsèque liée aux data streams que pour la construction de l\u0027ensemble des motifs candidats permettant la mise à jour incrémen-tale. En effet, pour diminuer la valeur du support, le choix du ratio est simplement heuristique sans justification théorique. Ainsi, cette approche n\u0027offre aucune certitude ni indication sur l\u0027erreur commise lors de la construction de l\u0027ensemble des motifs semi-fréquents quant aux motifs vrais ?-fréquents du stream (zone identifiée avec les symboles -sur la figure 9). De plus, nous n\u0027avons aucune garantie sur la minimalité de cet ensemble (zone identifiée avec les symboles + sur la figure 9), ce qui est fortement pénalisant pour sa réutilisation dans l\u0027objectif d\u0027optimisation de la méthode de mise à jour incrémentale.\nConclusion\nDans cet article, nous abordons la problématique de la mise à jour incrémentale pour les motifs fréquents dans le cas des grandes bases de données. Dans le contexte des data streams, il est plus pertinent, de construire et de maintenir des ensembles de motifs vrais ?-fréquents et non simplement observés comme tels. Plusieurs travaux, (Kearns et Mansour, 1998;Nock et Nielsen, 2004;Vapnik, 1998), ont montré l\u0027intérêt de l\u0027approche statistique notamment dans la détermination de règles de prévision pour l\u0027optimisation d\u0027algorithmes. Notre contribution majeure porte précisément sur ce point par l\u0027introduction de supports statistiques en complé-ment des supports classiques permettant d\u0027obtenir des bordures statistiques qui constituent les ensembles statistiquement presque optimaux (à une constante près) à considérer dans le cadre de la mise à jour incrémentale. Les expérimentations présentées montrent la robustesse de l\u0027approche, dans le cas des motifs séquentiels, au regard de la taille des bases stockées et des différentes valeurs supports testées. Ces résultats encourageants sont des points positifs quant à l\u0027applicabilité et le passage à l\u0027échelle de la méthode. Plusieurs extensions de ces travaux sont possibles dans bien des domaines de recherche en fouille de données. On citera notamment les travaux qui portent sur les structures de données où l\u0027on cherche à maintenir des ensembles d\u0027items qui sont observés fréquents avec un rappel maximum, (Jin et al., 2003). Nos préoc-cupations actuelles concernent la question suivante : est-il possible de construire un ensemble intermédiaire qui conserverait au mieux les propriétés de chacune de ces bordures ? Celui-ci représenterait un compromis entre une bordure statistique trop imposante à stocker et une autre pour laquelle le nombre d\u0027accès à la base de données est trop important.\n"
  },
  {
    "id": "1010",
    "text": "Introduction\nLa visualisation des corrélations et similarités principales dans un échantillon de données est l\u0027objectif des méthodes factorielles (Lebart et al., 1984). Ces méthodes cherchent souvent des directions informatives orthogonales dans un nuage de données. Ces directions concentrent l\u0027essentiel de la variance projetée car l\u0027inertie est porteuse de sens. Une décomposition pertinente de l\u0027inertie sur des plans de projection révèle quels individus sont similaires et quelles variables sont dépendantes. Bien que ces méthodes soient très pertinentes, les grands échan-tillons de données demandent de nouvelles méthodes efficaces pour leur analyse. Dans ce contexte, les cartes de Kohonen (1997) sont connues dans le domaine de l\u0027analyse exploratoire des données pour généraliser les méthodes factorielles telles que la méthode d\u0027Analyse en Composantes Principales ou ACP (Lebart et al., 1984) pour les données continues. Plus généralement, les cartes auto-organisatrices ou SOM (Kohonen, 1997) sont des méthodes de classification avec une contrainte de voisinage sur les classes conférant un sens topologique à la partition finale. Le GTM ou Generative Topographic Mapping (Bishop et al., 1998) est une carte auto-organisatrice probabiliste avec des contraintes sur les moyennes d\u0027un mélange gaussien pour données continues, mais ce modèle est inopérant pour des données catégorielles ou binaires. Des modèles récents (Girolami, 2001;Kabán et Girolami, 2001;Tipping, 1999) ont été proposés pour étendre le GTM aux modèles de mélanges classiques pour données discrètes. Hofmann et Puzicha (1998) ont par contre proposé l\u0027approche du modèle symétrique à aspects\nLe modèle BATM\nLe modèle proposé repose sur l\u0027hypothèse d\u0027indépendance des I × J cellules x ij ? {0, 1} d\u0027un tableau binaire en modélisant chaque probabilité unidimensionnelle d\u0027observer x ij comme un mélange de K lois de Bernoulli : P r(x ij \u003d 1) \u003d E(x ij ) \u003d k ? ki a jk avec ? ki les proportions des K composants telles que k ? ki \u003d 1. Ce modèle génératif correspond à sélectionner pour chaque ligne x i \u003d (x i1 , x i2 , · · · , x iJ ) fixée, une distribution discrète ? i de composantes ? ki , puis pour chaque j-ième composante x ij , sélectionner un état k avec la probabilité ? ki afin de lui attribuer une valeur binaire selon la loi de Bernoulli de paramètre a jk . Un modèle comparable pour la classification sans contrainte a été récemment proposé. La log-vraisemblance\nAfin d\u0027induire une auto-organisation topologique des probabilités, nous considérons les K coordonnées {s k } k\u003dK k\u003d1 d\u0027une grille bidimensionnelle régulière qui modélise un plan discrétisé sur lequel l\u0027ensemble des données est disposé par le BATM. La grille est projetée non linéaire-ment dans un espace de plus grande dimension L, par une transformation non linéaire constituée de L bases fonctionnelles ? , et telle que\nT . Les a jk forment alors les noeuds d\u0027une surface non linéaire discrète : les probabilités de Bernoulli sont paramétrées par des fonctions a jk \u003d ?(w\nCe modèle s\u0027interprète comme une version binaire cartographique du LSA probabiliste ou pLSA de Hofmann et Puzicha (1998). Les paramètres inconnus sont estimés dans la section suivante.\n-446 -RNTI-E-6\nEstimation par GEM\nL\u0027inférence de notre modèle se réalise en maximisant la log-vraisemblance par une mé-thode itérative ; une solution analytique exacte n\u0027existe pas en raison des non linéarités du mélange et des fonctions sigmoïdes. Donc nous étudions l\u0027approche de l\u0027algorithme de montée de gradient par EM (Dempster et al., 1977) généralisé (GEM) de McLachlan et Peel (2000). Cette approche suppose la vraisemblance complétée par la connaissance de la parti-\navec z i les variables latentes ayant pour support {1, 2 · · · , K}. L\u0027algorithme EM ou ExpectationMaximisation repose sur la maximisation de l\u0027espérance conditionnelle sachant les données et les paramètres de l\u0027itération précédente. Ayant P (t) (Z|D) du pas t précédent, nous maximisons au pas t + 1 :\nk|i,j,xij /J. Pour résoudre w (t+1) \u003d argmax w Q(?|? (t) ), nous effectuons des dérivations élémentaires du critère qui aboutissent au gradient Q (t) (t) j et au bloc de la hessienne H j . Le pas de NewtonRaphson suivant augmente alors localement la log-vraisemblance :\nA la convergence du GEM, nous obtenons un estimateur au maximum de vraisemblance notéˆ?notéˆ notéˆ?. Pour éviter un surapprentissage et une instabilité numérique, nous ajoutons à la fonction Q un paramètre de régularisation bayésien (MacKay, 1992) : ?? j w T j w j . Cette correction ajoute ??w j au gradient Q j et ?? à la diagonale de la hessienne H j . La valeur de l\u0027hyperparamètre ? est choisie manuellement comme la plupart du temps dans la littérature, ici nous avons pris ? \u003d 0.01.\nFormulation IRLS\nNous écrivons l\u0027algorithme de Newton sous une forme matricielle qui est proche d\u0027un pas d\u0027Iteratively Reweighted Least Squares ou IRLS (McCullagh et Nelder, 1983) pour la régression logistique. Pour j de 1 à J :\n-447 -RNTI-E-6\nCarte auto-organisatrice probabiliste sur données binaires Nous avons R (t) j la matrice de taille K ×I qui compte pour cellules les probabilités a posteriori P (t) (t) (t) k|i,j,xij , la matrice diagonale G j a pour éléments non nuls i P k|i,j,xij , A j est le vecteur de i-ième composante a ij , a (t) (t) j est un vecteur colonne avec a jk pour k-ième composante, F (t) j est la matrice diagonale avec a\nPour accélérer numériquement l\u0027algorithme, l\u0027approche de Bohning (1993) remplace la matrice exacte, relativement lourde à calculer en pratique, par une matrice alternative fixe. Par exemple, la matrice B \u003d ?\nj ? B est non négative, symétrique, ce qui permet de maximiser la vraisemblance. Comme la convergence est lente, nous proposons un algorithme de type variationnel alternatif.\nEstimation variationnelle\nEn suivant la borne 1 (Saul et al., 1996) sur log(1 + exp(? T k w j )), nous obtenons le nouveau critère à optimiser :\nvariationnel à estimer en maximisant˜Qmaximisant˜ maximisant˜Q. En dérivant ce nouveau critère, nous obtenons le pas de maximisation :\noù A j est le vecteur colonne ayant x ij ? 0.5 pour i-ème composante. Finalement, trois algorithmes, et un quatrième décrit ci-après, sont présentés pour estimer les paramètres du modèle. Ayant éliminé la solution du gradient simple mais inefficace, on constate que l\u0027algorithme IRLS donne la meilleure vraisemblance dans notre cas comme le montre les expériences dans la section suivante.\nSimulations\nDans cette section, nous abordons tout d\u0027abord deux éléments complémentaires à la mé-thode proposée, l\u0027initialisation des paramètres du modèle et l\u0027auto-organisation des probabilités sur les lignes afin d\u0027obtenir la meilleure carte projective finale possible. Nous décrivons alors les résultats numériques de nos simulations sur des données binaires réelles.\nInitialisation du modèle\nDes tirages aléatoires répétés des paramètres initiaux sont une solution aux minima locaux que rencontrent les algorithmes basés sur le gradient. Pour obtenir la meilleure convergence possible on procède à une \"bonne initialisation\". Puisque les cartes de Kohonen sont des gé-néralisations de l\u0027ACP, le premier plan de cette méthode fournit une intéressante première position (Elemento, 1999)   (Jolliffe, 2002), AFC (Benzécri, 1992), LSA (Deerwester et al., 1990) ou même celles obtenues suite à une projection non linéaire telle qu\u0027un MDS (Sammon, 1969). Alors une grille régulière est dessinée sur cette première projection et chaque cellule de la grille correspond à un facteur du modèle BATM : x i est affectée à la z ki ? h(k, z i ) pour une fonction de lissage telle que celle de voisinage des cartes de Kohonen, i.e. h(k, z \nLissage des paramètres sur les lignes\nIl peut être intéressant d\u0027ajouter une contrainte topologique sur les paramètres partitionnant les lignes afin d\u0027accélérer la convergence de l\u0027algorithme et améliorer la carte finale. Comme une solution par un soft-max (Bishop, 1995) nous apparaît relativement lourde, nous proposons une solution alternative en ajoutant un simple lissage par un terme de pénalisation issu de l\u0027approche du TNEM (Priam, 2003). Brièvement, il s\u0027agit de classer les vecteurs de données avec un lissage spatial sur les composantes ? ki du modèle BATM, à la manière d\u0027un champ de Markov caché (Zhang, 1992;Celeux et al., 2003;Ambroise et Govaert, 1998). On pose :\noù ? i est le vecteur de composantes ? ki , et V est -soit la matrice des fonctions de voisinage de la carte auto-organisatrice, i.e. V k \u003d h(k, -soit la matrice binaire d\u0027adjacence du treillis correspondant à notre carte probabiliste, i.e. V k \u003d 1 ssi le k-ième noeud est voisin du Le pas complémentaire associé s\u0027écrit :\net se résout en itérant l\u0027égalité et en réinjectant dans le membre de droite les anciennes valeurs courantes des ? (t+1) ki jusqu\u0027à ce que la stabilisation de leurs valeurs soit atteinte. Nous retrouvons évidemment le pas d\u0027estimation non contrainte en annulant ?. Nous avons éliminé le terme additif du TNEM qui porte sur les entropies des ? i , donc nous obtenons un nouvel algorithme appelé TNEM2, plus général que le TNEM original puisqu\u0027il s\u0027applique à des probabilités non forcément a posteriori. Une alternative au TNEM est une estimation des ? ki paramétrés comme le GTM. La fonction à optimiser s\u0027écrit alors Q I (?|?\noù les w i sont les inconnues à déterminer. Leur estimation s\u0027effectue comme précédemment, par une montée de gradient en réalisant une boucle sur l\u0027indice i des lignes, de 1 à I, et en calculant les gradients Q ki . Nous proposons finalement le quatrième algorithme d\u0027estimation noté IRLS+TNEM2 qui associe une maximisation sur les paramètres des colonnes par l\u0027IRLS à un lissage des probabilités des lignes par le TNEM2. Nous expliquons dans la suite comment ce lissage se comporte en pratique.\nPost-processing de la carte finale\nLa carte finale montre une grille de centres de classes bien organisées ; à chacune on affecte les données dont elle est le plus proche. Pour les cartes auto-organisatrices classiques on utilise la distance euclidienne entre le vecteur centre et le vecteur donnée. Ici le modèle permet une alternative probabiliste puisque nous avons la probabilité de génération d\u0027une donnée par un composant k du mélange. Donc chacun des vecteurs x i est affecté à un centre par un maximum a posteriori (MAP), i.e. ˆ z i \u003d argmax k ˆ ? ki . De la même manière, la j-ième variable est affectée au centre de labeî z j \u003d argmax k ˆ a jk . Le MAP aboutit aux positions bidimensionnelles p i \u003d s ˆ zi et p j \u003d s ˆ zj pour les lignes et colonnes de la matrice projetée. Une seconde manière de projeter chaque donnée est par sa position moyenne (Bishop et al., 1998)  \nExpériences\nNous expérimentons notre modèle sur plusieurs échantillons de données pour valider notre approche, par exemple, sur l\u0027échantillon Zoo 2 , qui compte 101 animaux avec sept classes et 21 caractéristiques binaires. Notre méthode BATM converge vers une carte bien organisée où l\u0027on reconnaît les sept classes que l\u0027algorithme a projetées. La segmentation de la grille sur la figure 1 s\u0027effectue à l\u0027aide d\u0027une procédure automatique consistant (Vesanto et Alhoniemi, 2000) en une classification ascendante hiérarchique avec agrégation par le diamètre (completelinkage) associée à une distance du ? 2 sur la matrice desˆ?desˆ desˆ? ki , qui donne les meilleurs résultats en pratique. En remarque, la classe contenant les reptiles est peu homogène d\u0027après nos expé-riences, car ces animaux se regroupent mal. L\u0027évolution de la log-vraisemblance par les quatre algorithmes est présentée à la figure 2 pour le tableau du Zoo, démontrant la supériorité de l\u0027algorithme IRLS comparativement à des algorithmes plus récents alternatifs. Cependant, un surapprentissage peut amener à une solution non suffisamment lissée, et nous préférons l\u0027approche IRLS+TNEM2 à cause de son efficacité malgré sa vraisemblance moins élevée. Cette valeur plus faible s\u0027explique par le terme de pénalisation qui aide à une plus rapide et meilleure auto-organisation des lignes comme vérifiée ici puisque cet algorithme s\u0027arrête bien plus tôt que les trois autres, pour un critère d\u0027arrêt identique (log-vraisemblance relative inférieure au seuil 10e-5). Notre initialisation originale par une régression adéquate se base sur le premier plan principal d\u0027une Analyse des Correspondances (AFC). Celle-ci s\u0027illustre sur la figure 3, démontrant l\u0027intérêt d\u0027une carte auto-organisatrice : alors que la méthode factorielle linéaire n\u0027est pas capable de montrer les sept classes sur ce premier plan, notre carte par BATM extrait ces sept classes et trouve leur lien statistique grâce à la propriété de voisinage. Un ensemble de données textuelles est également projeté. Cette base est un échantillon du fichier Classic3 (Dhillon et al., 2003) qui compte trois classes (MEDLINE, CISI, CRANFIELD). Nous avons tiré aléatoirement (tirage équiprobable sans remise) 450 documents de ce fichier en prenant 150 documents dans chaque classe. Nous avons sélectionné les termes dont la fréquence totale est supérieure à 30 sur le corpus entier et pour l\u0027ensemble du vocabulaire de 4303 termes. Nous aboutissons, en éliminant les textes vides, à une matrice de taille aléatoire : 450 par 170 environ. Nous montrons les positions moyennes des labels des documents correspondants et projetés sur la figure 4 pour l\u0027une de ces matrices. Nous sommes en mesure de visualiser assez distinctement les trois classes séparées par notre projection non linéaire.\nConclusion et discussion\nNous avons présenté une nouvelle méthode de carte auto-organisatrice -récapitulée sur la figure 5-pour données binaires, comme on peut en trouver dans le domaine du traitement de l\u0027image et du texte. De nouveaux résultats pour l\u0027initialisation d\u0027une méthode de projection probabiliste de données qualitatives a également été introduit.\nUne perspective de nos travaux est la construction de biplots non linéaires par carte topologique. Nous travaillons actuellement à la projection de matrices de taille plus importante ainsi que sur des ensembles d\u0027images binaires qui donnent des résultats encourageants. Ensuite des variantes au modèle BATM se posent en remplaçant E(x ij ) par une hypothèse alternative, i.e. un mélange de lois différent tel que par exemple E(\n. Le modèle BATM s\u0027étend également à d\u0027autres types de données comme il est proposé dans le paragraphe suivant. L\u0027estimation peut encore être améliorée en déterminant notamment le meilleur hyperparamètre ?. En conclusion, le récent modèle du Block Clustering (Govaert et Nadif, 2003, 2005   Initialisation de ? (0) \u003d ({a\nPost-processing final de la carte de paramètresˆ?paramètresˆ paramètresˆ? ? Tableau segmenté par CAH, projections moyennes, biplot.\nFIG. 5 -Schéma récapitulatif de la méthode BATM.\nde mélange classique à un modèle de mélange croisé. Celui-ci est un modèle génératif flexible qui s\u0027avère une alternative efficace et prometteuse au modèle à aspects. Il serait intéressant de l\u0027étendre en lui ajoutant une propriété d\u0027auto-organisation.\nAnnexe : paramétrisation probabiliste alternative au soft-max\nLorsque la matrice de données est un tableau de contingence, la loi de Bernoulli n\u0027est plus valable, et une hypothèse de loi multinomiale est généralement supposée. Un paramétrage soft-max est alors introduit pour le cas de probabilités contraintes en régression et classification notamment. On écrit dans notre cas p j|k \u003d e w T T j ?k / j e w j ?k avec j p j|k \u003d 1. Donc cette paramétrisation implique l\u0027inversion d\u0027une matrice hessienne pleine pour procéder à l\u0027optimisation. Nous proposons un moyen alternatif plus efficace. L\u0027idée principale est d\u0027aboutir à de nouveaux paramètres -sans la contrainte de somme à l\u0027unité classique pour une multinomialeen écrivant p j|k comme une loi jointe de variables de loi de Bernoulli de paramètres inconnus. Il s\u0027agit d\u0027écrire la loi jointe de la j-ième colonne associée en mettant à l\u0027unité la composante qui nous intéresse, et à zéro les autres, puis en supposant des lois de Bernoulli sur l\u0027ensemble des composantes prises indépendantes pour le vecteur binaire résultant. L\u0027expression p j|k \u003d p jk j \u003dj (1 ? p j k ) p jk avec p jk ? [0, 1] donne une solution valide au maximum de vraisemblance d\u0027une loi multinomiale, pour des probabilités assez petites (éventuellement par l\u0027ajout de composantes artificielles supplémentaires pour diminuer les valeurs), i.e. p  Nous retrouvons l\u0027expression classique de l\u0027estimation des paramètres de la loi multinomiale, \n"
  },
  {
    "id": "1011",
    "text": "Introduction\nLa classification, la segmentation et l\u0027étiquetage de données séquentielles sont des problématiques au coeur de nombreux domaines comme la bioinformatique, la reconnaissance de l\u0027écriture, l\u0027extraction d\u0027information. Une des problématiques principales dans ce type de domaine consiste en effet à transformer une séquence observée (un signal écrit par exemple) en une séquence d\u0027étiquettes (on utilise également le terme de labels). Cette tâche peut être réalisée à différents niveaux. On cherche à segmenter le signal écrit d\u0027une phrase en une séquence de mots, de même que le signal écrit de chaque mot doit être segmenté en une séquence de caractères, etc.\nLes modèles Markoviens cachés (MMC) constituent l\u0027approche la plus utilisée pour résoudre ce type de tâches bien qu\u0027ils reposent sur des hypothèses d\u0027indépendance fortes sur les données et qu\u0027ils soient appris de façon non discriminante. Ce dernier point vient du fait que ce sont des modèles génératifs et qu\u0027ils définissent une loi de probabilité conjointe sur la séquence d\u0027observations X et la séquence d\u0027étiquettes associée Y. Diverses ) , ( Y X P 1 Ce travail est en partie financé par le programme IST de la communauté européenne, à travers le réseau d\u0027Excellence PASCAL IST-2002-506778. méthodes ont été proposées pour introduire de l\u0027information discriminante dans des systèmes de type Markovien ou plus généralement basés sur des modèles génératifs (Jaakkola et al., 1999, Bahlmann et al., 2002, Moreno et al., 2003. Ces travaux reposent en grande partie sur des méthodes à noyau et des machines à vecteur support.\nDes modèles sont apparus récemment qui visent à palier à l\u0027ensemble des défauts des MMC. Ce sont des modèles conditionnels qui s\u0027attachent à modéliser la loi de probabilité conditionnelle . On peut citer les modèles de Markov à entropie maximale (McCallum et al., 2000) et les champs de Markov conditionnels (Lafferty et al., 2001). Ces modèles ont été essentiellement utilisés jusqu\u0027à présent dans le traitement de documents textuels, pour la reconnaissance d\u0027entités nommées, l\u0027extraction d\u0027 ) / ( X Y p information ou l\u0027étiquetage morphosyntaxique. Les caractéristiques employées et les algorithmes d\u0027apprentissage sont par conséquent adaptés à ces contextes applicatifs. Cette contribution explore l\u0027emploi de modèles conditionnels pour la reconnaissance de données de type « signal », telles que la parole ou l\u0027écriture en ligne. Plusieurs adaptations sont nécessaires. Tout d\u0027abord, les observations sont de nature différente, ce sont généralement des séquences de vecteurs réels dans . Ensuite, les classes sont souvent multimodales, il y a par exemple plusieurs façons d\u0027écrire un « a ». Enfin, l\u0027étiquetage des données dans la phase d\u0027apprentissage est le plus souvent partiel --on connaît la classe d\u0027une séquence d\u0027observations (e.g. un « a ») mais on ne connaît pas la séquence d\u0027états correspondante --alors que les algorithmes proposés dans la littérature requièrent une base de données totalement étiquetée.\np R\nDans la suite, nous commençons par introduire les modèles conditionnels. Puis nous présentons des modèles conditionnels adaptés à des classes multimodales et dérivons les algorithmes pour apprendre ces modèles avec des données partiellement étiquetées. Enfin, nous fournissons des résultats expérimentaux pour deux tâches de classification de séquences, la reconnaissance de caractères manuscrits en ligne et la reconnaissance de comportements de l\u0027utilisateur en se basant sur les mouvements de son oeil 2 , en comparant modèles conditionnels et modèles Markoviens standards.  Markov cachés (MMC) puis les modèles de Markov à entropie maximale (MMEM) (McCallum et al., 2000). Nous présentons ensuite les champs de Markov conditionnels (CMC) (Lafferty et al., 2001) ainsi qu\u0027une extension, les CMC semi-Markoviens (Sarawagi et Cohen, 2005).  \nModèles conditionnels pour données séquentielles\nModèles de Markov cachés (MMC)\nEn comparant à l\u0027équation (2) on voit que l\u0027on a remplacé les probabilité de transition par des probabilités conditionnelles de la forme et les probabilités d\u0027émission n\u0027apparaissent pas dans cette formule. L\u0027apprentissage est ainsi focalisé sur ce qui différencie une transition d\u0027une autre sans avoir à modéliser complètement le processus de génération des données, i.e. on ne modélise pas la marginale P(X). Les MMEM sont des modèles discriminants. Un autre avantage vient du fait que cette modélisation ne réclame pas d\u0027hypothèses particulières sur X et que l\u0027on peut paramétrer les lois de probabilité de\ntransition par des caractéristiques exploitant des dépendances complexes entre les observations de la séquence X. (Lafferty et al., 2001) ont toutefois mis en évidence un comportement indésirable des MMEM qu\u0027ils ont appelé « label bias » que nous ne décrivons que succinctement. Ce problème vient du fait que les lois de probabilités de transitions à partir d\u0027un état sont normalisées, c\u0027est à dire que 1 )\n. Cela induit que si la \u0027 s structure du modèle est telle qu\u0027un état n\u0027a qu\u0027un état successeur, l\u0027observation X n\u0027a aucune influence sur le décodage.\nChamps de Markov conditionnels (CMC)\nLes champs de Markov conditionnels sont une instance particulière des champs aléatoires. Ces derniers permettent d\u0027estimer des lois de probabilité jointes sur un graphe de noeuds représentant des variables aléatoires. Les champs aléatoires conditionnels font de même mais en conditionnant les valeurs des noeuds représentant les variables à estimer (les labels) par les valeurs de noeuds représentant les variables d\u0027entrée. La figure 1 illustre les différences fondamentales entre les modèles MMC, MMEM et CMC en les représentant sous forme de modèles graphiques. La figure 1 (c) est une représentation graphique d\u0027un champ aléatoire conditionnel (avec une structure de chaîne). Cette représentation est à comparer avec celle d\u0027un MMC ( Fig. 1 (a)) et celle d\u0027un MMEM ( Fig. 1 (b)). Notons que ces deux derniers modèles sont représentés par des graphes orientés qui permettent d\u0027exprimer les lois de probabilité, jointe ou conditionnelle, suivant les formules (2) ou (3). Notons également que les MMEM et les CMC, étant des modèles conditionnels, ne requièrent pas d\u0027hypothèses particulières sur X, ce qui explique les noeuds non décomposés X dans les figures 2 (b) et (c).\nFIG. 1 -Représentation de MMC (a), MMEM (b) et CMC (c) sous forme de modèles graphiques, les noeuds grisés correspondent aux variables observées.\nNous considérons ici des champs aléatoires --définis par un graphe de noeuds et de liens--tels que, conditionnellement à X, la séquence d\u0027étiquettes Y obéit à la propriété Markovienne exprimée par le graphe des noeuds.\nsignifie que t et t\u0027 sont des voisins dans le graphe. En exploitant la théorie des champs aléatoires (Lafferty 2001), on peut montrer que la probabilité conditionnelle d\u0027une séquence d\u0027étiquettes Y connaissant l\u0027observation X peut se mettre sous la forme :  Fig. 1 (c)), les caractéristiques sont définies soit sur un état, soit sur deux états successifs. Soit l\u0027ensemble des fonctions caractéristiques. Alors : Des caractéristiques typiquement utilisées dans des problèmes d\u0027étiquetage morpho syntaxiques sont par exemple « le mot commence par une majuscule ». Dans le cas de données comme la parole ou l\u0027écriture en ligne on ne peut pas définir des caractéristiques de haut niveau de ce type. Dans ces domaines, un signal d\u0027entrée est transformé après prétraitement en une séquence de vecteurs de p caractéristiques réelles. On utilisera alors ces p caractéristiques dans les CMC.\nL\u0027inférence est réalisée à l\u0027aide d\u0027un algorithme de programmation dynamique. Si la topologie du graphe est une chaîne on utilise l\u0027algorithme de Viterbi, si c\u0027est un arbre on peut utiliser l\u0027algorithme Belief Propagation (Weiss, 2001) et pour un graphe quelconque on utilise Loopy Belief Propagation (Pearl, 1988, Murphy et al., 1999. En apprentissage, on dispose d\u0027une base d\u0027apprentissage de K exemples complètement étiquetés,\nk est la séquence des étiquettes (i.e. noeuds) correspondant à X k . Durant l\u0027apprentissage, on cherche les paramètres W qui maximisent la log-vraisemblance conditionnelle sur l\u0027ensemble d\u0027apprentissage. Ce critère est convexe et peut être maximisé par une méthode de gradient. Il s\u0027écrit :\nIl faut noter que le facteur de normalisation implique une somme sur un nombre exponentiel de séquences d\u0027états possibles, mais cette somme peut être calculée efficacement via un algorithme de programmation dynamique.\nChamps de Markov conditionnels semi-Markoviens (SCMC)\nLes CMC présentés précédemment permettent de déterminer la séquence de labels de probabilité maximale pour une séquence d\u0027observations donnée . Cela correspond bien à des problématiques comme l\u0027étiquetage morpho syntaxique, où l\u0027on cherche une étiquette pour chaque mot. Cependant, on peut souhaiter travailler sur des données séquentielles telles que les observations prises isolément ont moins de sens que des segments de plusieurs observations successives. Afin d\u0027intégrer des informations segmentales, (Sarawagi et Cohen, 2004) ont proposé une extension des CMC appelée CMC semi-Markoviens ou semi-CMC (SCMC). Etant donnée une observation , on cherche la meilleure segmentation de X en étiquetant les \nCMC pour la classification de séquences\nSCMC pour la classification de données monomodales\nLes CMC sont, comme nous l\u0027avons déjà mentionné, traditionnellement utilisés pour l\u0027étiquetage de données séquentielles. Pour concevoir un système de classification de séquences basé sur un CMC on peut utiliser une architecture basée sur une structure de chaîne pour chaque classe. La structure de chaîne est en effet naturelle pour modéliser des séquences d\u0027une même classe dans lesquelles on veut distinguer les différentes parties comme le début, le milieu, la fin. Dans le cas de la reconnaissance de l\u0027écriture manuscrite en ligne par exemple, on utilise un modèle de type chaîne pour chaque lettre. Dans la chaîne correspondant au caractère « a », le premier noeud correspond au début du tracé du « a », le second noeud à une partie intermédiaire, etc. On utilise alors une architecture du type mélange de structures en chaîne, telle que celle illustrée figure 2. Il y a une « branche » par classe. Les branches peuvent avoir des nombres d\u0027états différents, il s\u0027agit d\u0027un choix a priori sur la structure du modèle. Bien entendu, la représentation « dynamique » de ce modèle est similaire à la Figure 1 (c), la différence vient du fait que toutes les transitions entre noeuds ne sont pas autorisées. Lorsque l\u0027on apprend un modèle de ce type avec des données des N classes, on apprend des paramètres W qui permettent de discriminer au mieux entre les séquences d\u0027observations des différentes classes.\nFIG. 2 -Un champ aléatoire pour la classification de séquences dans le cas de données monomodales. Chaque « branche » correspond à une classe.\nL\u0027étiquetage disponible dans des bases d\u0027apprentissage pour la classification de signaux est souvent minime et réduit à la classe. Cela signifie que l\u0027on sait que la segmentation correspondant à une séquence d\u0027observations correspond à une branche particulière (e.g. celle du « a ») mais on ne connaît pas la segmentation plus précisément. Or les algorithmes d\u0027apprentissage des CMC requièrent cette information. Nous montrons ici comment apprendre un CMC sans disposer de cette information.\nOn dispose donc d\u0027une base d\u0027apprentissage de K exemples,  (Quattoni et al., 2004). Pour limiter les problèmes numériques et simplifier l\u0027implémentation on peut choisir d\u0027approximer la quantité précédente par :\nEn apprentissage, on estime les paramètres maximisant la log-vraisemblance conditionnelle, la segmentation (non disponible) des séquences d\u0027apprentissage est donc apprise au fur et à mesure pendant l\u0027apprentissage (Do, 2005). Il faut noter qu\u0027à cause de ces variables cachées, le critère a plusieurs maximums locaux, l\u0027optimisation n\u0027assure donc pas de trouver l\u0027optimum global.\nSCMC pour données multimodales\nBien souvent les classes sont multimodales et la modélisation précédente peut s\u0027avérer insuffisante. Nous proposons dans ce cas des modèles que l\u0027on peut voir comme des mélanges de champs aléatoires conditionnels. Plutôt que d\u0027avoir une structure en chaîne pour chaque classe, on en considère plusieurs, chacune pouvant se spécialiser, pendant l\u0027apprentissage, sur une modalité de la classe.\nLa difficulté de l\u0027apprentissage consiste ici encore dans le manque d\u0027étiquetage des données en apprentissage. Ici, on dispose de la même information que précédemment pour une séquence d\u0027apprentissage, sa classe. On sait par exemple qu\u0027une séquence d\u0027observations est un « a » mais on ne connaît pas l\u0027allographe (i.e. la branche) correspondant ni la segmentation dans cette branche. On introduit alors un deuxième type de variable cachée, l\u0027indicateur de la branche. On obtient donc :\nOù B désigne une branche, désigne l\u0027ensemble des branches correspondant à la classe , et S , et ont la même signification que précédemment.\nPour simplifier les calculs et l\u0027implémentation on peut choisir d\u0027approximer la quantité précédente en approximant la somme au numérateur par un maximum, comme dans l\u0027équation (8). En apprentissage, on estime les paramètres maximisant la log-vraisemblance conditionnelle, la segmentation (non disponible) des séquences d\u0027apprentissage est donc apprise au fur et à mesure pendant l\u0027apprentissage. Il faut noter qu\u0027à cause des variables cachées, l\u0027optimisation n\u0027assure pas de trouver l\u0027optimum global.\nFIG. 3 -Modèle SCMC avec plusieurs modèles par classe pour prendre en compte les allographes.\nMises en oeuvre expérimentales\nLes implémentations des CMC ont été réalisées en exploitant une bibliothèque implémentant un algorithme de gradient avec la méthode de quasi-Newton avec mémoire limitée LBFGS (Nocedal, 1989).\nDéterminer l\u0027intérêt d\u0027un utilisateur par des mouvements de l\u0027oeil\nL\u0027étude réalisée dans cette partie a été réalisée dans le cadre d\u0027un challenge organisé par des membres du réseau d\u0027excellence PASCAL. Ce challenge consiste à explorer la possibilité d\u0027inférer l\u0027intérêt d\u0027un utilisateur dans les différentes lignes de texte qui lui sont présentées à l\u0027écran (typiquement retournées par un moteur de recherches) en fonction des mouvements de son oeil (Salojärvi et al., 2005). Les données ont été collectées de la façon suivante. Pour une requête donnée, on présente à l\u0027utilisateur 10 titres (un titre est une réponse tenant sur une ligne) correspondant à 10 réponses possibles. Parmi ces 10 titres, un seul est correct, 4 sont pertinents, et 5 sont non pertinents. On observe les mouvements de l\u0027oeil de l\u0027utilisateur qui cherche parmi les 10 titres celui qui le satisfait. Une telle expérience (requête + 10 titres de réponses) est appelée dans la suite un assignement.\nLa trajectoire de l\u0027oeil est segmentée en fixations et saccades, puis on détermine à quels mots de la page ces fixations correspondent. Cette séquence ordonnée temporellement de fixations est transformée en une séquence de vecteurs de caractéristiques, un par fixation. On dispose d\u0027une vingtaine de caractéristiques, fournies par les organisateurs du challenge (Salojärvi et al., 2005). On dispose également pour chaque fixation, du titre auquel appartient le mot fixé, de la position du mot dans le titre et de la longueur (en mots) du titre.\nPuisque la segmentation en titres est connue, en apprentissage ou en reconnaissance, nous avons calculé des caractéristiques segmentales par segment de fixations correspondant à un même titre. Les vecteurs de caractéristiques sont additionnés sur tout le segment. Dans la suite, on considère qu\u0027un assignement est représenté par une séquence de vecteurs de 22 caractéristiques , où représente la somme des vecteurs caractéristiques Lorsqu\u0027un utilisateur visite une seconde, ou une troisième fois un même titre, on peut supposer qu\u0027il ne le visite pas de la même façon et que cela est informatif. Nous avons donc également utilisé des modèles dans lesquels on distingue la première visite d\u0027un titre de la classe N (ou P ou C) d\u0027une deuxième visite d\u0027un tel titre. Pour cela, il suffit de multiplier les noeuds (Figure 4 (b)). Le dernier titre visité est souvent très important, car c\u0027est souvent le titre correct. Pour le prendre en compte automatiquement, nous avons rajouté trois noeuds correspondant à la dernière visite. Le décodage consiste à tester parmi toutes les possibilités (étiquetage des 10 titres parmi N, P ou C) celle de probabilité maximale.\nNous avons comparé ces systèmes conditionnels à des systèmes Markovien plus standard. Pour cela, nous avons appris des MMC similaires à 3, 6 ou 9 états dans lesquels les probabilités d\u0027émission sont modélisées par des lois gaussiennes sur les vecteurs de caractéristiques. Il y a 336 assignements dans la base d\u0027apprentissage et 149 assignements dans la base de test. Les performances sont calculées suivant la procédure proposée dans (Salojärvi et al., 2005), c\u0027est à dire que l\u0027on n\u0027évalue le système que sur les réponses qu\u0027il fournit pour les titres qui ont été effectivement visités. Le tableau 1 montre les performances des systèmes génératifs (MMC) et des modèles conditionnels (CMC). Les meilleurs résultats ont été obtenus avec des MMC à 6 états alors que les SCMC ont montré des performances supérieures quel que soit le nombre d\u0027états, et allant jusqu\u0027à 73% lorsque l\u0027on distingue la première visite, la dernière visite et les visites intermédiaires à un même titre. \nReconnaissance de l\u0027écriture manuscrite en ligne\nUn signal d\u0027écriture manuscrite en ligne est un signal temporel constitué des coordonnées successives d\u0027un stylo, il est capturé sur une tablette digitale ou via un stylo électronique. La base UNIPEN (Guyon et al., 1994) est une base internationale de référence dans le domaine de la reconnaissance d\u0027écriture. Nous avons travaillé sur une partie de cette base regroupant des signaux de 200 scripteurs et correspondant aux 26 caractères. Notre base contient environ 60000 exemples, nous en utilisons 33% pour l\u0027apprentissage et 66% pour le test.\nLe signal d\u0027écriture étant très variable (il existe de nombreux allographes pour tracer un même caractère) l\u0027usage de modèles de mélanges ou de systèmes basés sur des prototypes de tracés typiques est extrêmement répandu. Les systèmes les plus performants sont souvent à base de MMC. L\u0027apprentissage de ce type de modèles n\u0027est pas aisé car le nombre d\u0027allographes ainsi que la topologie des modèles Markoviens les modélisant doivent être fixés à la main. Divers travaux ont été menés pour apprendre complètement les modèles de caractères à partir des données (Lee et al., 2001, Artières et Gallinari 2002, ils sont basés sur la construction d\u0027un MMC à partir d\u0027un seul tracé et utilisent une représentation des tracés sous forme de séquence de codes directionnels. Le système de référence est un système de ce type, dont les détails peuvent être trouvés dans (Marukatat, 2004). Un des intérêts du système de référence Markovien réside dans sa capacité à apprendre la topologie (nombre de branches, nombre d\u0027états) des modèles à partir des données, ce que nous ne savons pas faire aujourd\u0027hui dans le cas de CMC.\nNous avons donc mis en oeuvre des CMC en fixant leur topologie d\u0027après celle apprise par le système Markovien. L\u0027idée consiste à construire un modèle CMC multi branches en reprenant les topologies des modèles MMC appris. De plus, on utilisera dans ce CMC les mêmes caractéristiques que celles utilisées dans les MMC. Le tableau 2 résume les performances du système Markovien et de systèmes SCMC pour la classification des 26 caractères minuscules. Les performances s\u0027améliorent avec la taille des modèles. Comme on le voit, les SCMC surpassent le système Markovien dans toutes les expériences. On voit ici encore que les systèmes à base de CMC surpassent dans tous les cas le système Markovien de référence, ce qui est très intéressant. Car ce dernier système a fait l\u0027objet de nombreux travaux depuis quelques années dans notre équipe et est au niveau de l\u0027état de l\u0027art dans le domaine. Il reste néanmoins du travail à réaliser pour mettre en oeuvre des CMC. Les algorithmes d\u0027apprentissage sont encore relativement coûteux. Et surtout, la structure des CMC est déterminée d\u0027après la structure apprise par le système Markovien.\nConclusion\nNous avons exploré dans cette contribution l\u0027emploi de champs aléatoires Markoviens conditionnels pour la classification et l\u0027étiquetage de signaux. Tout d\u0027abord, nous nous sommes focalisés sur des variantes des CMC exploitant la notion de segments et des caractéristiques segmentales. Nous avons développé des algorithmes pour réaliser l\u0027apprentissage de nos modèles en présence d\u0027une information de segmentation minimale, de type classe. Nous avons fourni des résultats expérimentaux montrant que ces modèles surpassent des modèles Markoviens plus traditionnels dans deux tâches très différentes de classification de données séquentielles.\n"
  },
  {
    "id": "1012",
    "text": "Introduction\nCet article présente une étude expérimentale consistant à évaluer le taux d\u0027élagage le plus adapté pour l\u0027extraction de la terminologie. Nous allons décrire ci-dessous notre méthode globale d\u0027extraction de la terminologie et rigoureusement définir l\u0027élagage.\nLa première phase de notre travail d\u0027extraction de la terminologie à partir de corpus spé-cialisés consiste à normaliser les textes en utilisant des règles de nettoyage décrites par Roche (2004). Les corpus que nous utilisons sont décrits dans la section 3 de cet article. L\u0027étape suivante consiste à apposer des étiquettes grammaticales à chacun des mots du corpus en utilisant l\u0027étiqueteur ETIQ développé par Amrani et al. (2004). ETIQ est un système interactif s\u0027appuyant sur l\u0027étiqueteur de Brill (1994) qui améliore la qualité de l\u0027étiquetage de corpus spé-cialisés. Nous pouvons alors extraire l\u0027ensemble des collocations Nom-Nom, Adjectif-Nom, Nom-Adjectif 1 , Nom-Préposition-Nom d\u0027un corpus spécialisé. L\u0027étape suivante consiste à sé-lectionner les collocations les plus pertinentes selon des mesures statistiques décrites par Roche et al. (2004c); Roche (2004). Les collocations sont des groupes de mots définis par Halliday (1976); Smadja (1993). Nous appelons termes, les collocations pertinentes.\nLes termes binaires (ou ternaires pour les termes prépositionnels) extraits à chaque itération sont réintroduits dans le corpus avec des traits d\u0027union afin qu\u0027ils soient reconnus comme des mots à part entière. Nous pouvons ainsi effectuer une nouvelle recherche terminologique à partir du corpus avec prise en compte de la terminologie du domaine acquise aux étapes précédentes. Notre méthode itérative, proche des travaux de Evans et Zhai (1996), est décrite 1 Corpus en français uniquement 2 État de l\u0027art des méthodes d\u0027extraction de la terminologie De multiples approches de recherche terminologique ont été développées afin d\u0027extraire les termes pertinents à partir d\u0027un corpus. Nous ne traiterons pas ici les approches d\u0027aide à la structuration et au regroupement conceptuel des termes qui sont détaillés dans les travaux de Aussenac-Gilles et Bourigault (2003).\nLes méthodes d\u0027extraction de la terminologie sont fondées sur des méthodes statistiques ou syntaxiques. Le système TERMINO de David et Plante (1990) est un outil précurseur qui s\u0027appuie sur une analyse syntaxique afin d\u0027extraire les termes nominaux. Cet outil effectue une analyse morphologique à base de règles, suivie de l\u0027analyse des collocations nominales à l\u0027aide d\u0027une grammaire. Les travaux de Smadja (1993) (XTRACT) s\u0027appuient sur une méthode statistique. XTRACT extrait, dans un premier temps, les collocations binaires situées dans une fenêtre de dix mots. Les collocations binaires sélectionnées sont celles qui dépassent d\u0027une manière statistiquement significative la fréquence due au hasard. L\u0027étape suivante consiste à extraire les collocations plus générales (collocations de plus de deux mots) contenant les collocations binaires trouvées à la précédente étape. ACABIT de Daille (1994) effectue une analyse linguistique afin de transformer les collocations nominales en termes binaires. Ces derniers sont ensuite triés selon des mesures statistiques. Contrairement à ACABIT qui est fondé sur une méthode statistique, LEXTER de Bourigault (1993) et SYNTEX de Bourigault et Fabre (2000 s\u0027appuient essentiellement sur une analyse syntaxique afin d\u0027extraire la terminologie du domaine. La méthode consiste à extraire les syntagmes nominaux maximaux. Ces syntagmes sont alors décomposés en termes de \"têtes\" et d\u0027\"expansions\" à l\u0027aide de règles grammaticales. Les termes sont alors proposés sous forme de réseau organisé en fonction de critères syntaxiques.\nPour discuter le choix du taux d\u0027élagage selon le nombre d\u0027occurrences, nous allons classer les collocations en utilisant la mesure Occ RV décrite dans les travaux de Roche (2004). Cette mesure qui a le meilleur comportement comme précisé par Roche et al. (2004a,c) classe les collocations selon leur nombre d\u0027occurrences et les collocations ayant le même nombre d\u0027occurrences sont classées en utilisant le Rapport de Vraisemblance de Dunning (1993). Cette mesure est parfaitement bien adaptée à cette étude car le classement effectué selon le nombre d\u0027occurrences permet de discuter le choix du taux d\u0027élagage. Le principe de l\u0027élagage des collocations consiste à exploiter seulement les collocations présentes un nombre de fois minimum dans le corpus. L\u0027élagage permet d\u0027exclure les collocations trop rares qui peuvent se révéler comme non significatives pour le domaine. Sur chacun des corpus expérimentés, le tableau 1 présente différents élagages appliqués.\nDescription des corpus\nLa première observation à relever dans le tableau 1 tient dans le nombre d\u0027occurrences des collocations qui diffère selon les langues. Par exemple, les collocations de type Nom-Nom sont beaucoup moins fréquentes sur les corpus en français par rapport aux corpus en anglais.\nSuivant les domaines de spécialité écrits dans une même langue, les résultats peuvent éga-lement différer de manière importante. Par exemple, sur le corpus de CVs, le nombre de collocations de type Nom-Nom est beaucoup plus important que celui du corpus des Ressources Humaines également écrit en français. Le corpus des Ressources Humaines a pourtant une taille plus grande que le corpus de CVs. Ceci est dû au fait que les CVs sont écrits de manière condensée en employant un vocabulaire très spécifique : \"emploi solidarité\", \"action communication\", \"fichier client\", \"service achat\", etc.\nLe tableau 1 montre qu\u0027en éliminant simplement les collocations présentes une seule fois dans le corpus, plus de la moitié des collocations sont supprimées dans tous les cas, excepté la relation Adjectif-Nom du corpus des Ressources Humaines. Ce corpus a un comportement -3 ème catégorie : La collocation est pertinente mais très générale et pas nécessairement adaptée au domaine (exemple du corpus de CVs : \"situation actuelle\") -4 ème catégorie : La collocation est non pertinente (exemple du corpus de CVs : \"jour quotidienne\") -5 ème catégorie : L\u0027expert ne peut pas juger si la collocation est pertinente (exemple du corpus de CVs : \"master franchisé\").\nPar exemple, les collocations qui sont des instances de concepts peuvent être utilisées pour découvrir des règles d\u0027association entre concepts présents dans les textes. Les concepts peuvent également être utilisés pour construire des patrons d\u0027extraction utiles pour la recherche d\u0027informations.\nPar exemple, pour découvrir des règles d\u0027association, les concepts utilisés doivent être précis afin de déterminer des associations éventuelles. Ce travail a des similarités avec les approches de Srikant et Agrawal (1997) qui consistent à utiliser une taxonomie pour généraliser des règles d\u0027association extraites. Dans nos travaux précédents ; Kodratoff et al. (2003)) et dans la thèse de Azé (2003), les règles d\u0027associations découvertes sont de la forme concept 1 ...concept n?1 ? concept n où n est le nombre de concepts impliqués dans les règles d\u0027association extraites. Le détail de l\u0027algorithme est présenté dans les travaux de Azé (2003).\nÀ titre d\u0027exemple, nous donnons deux règles d\u0027association extraites à partir du corpus des Ressources Humaines :\nCette règle signifie que le stress s\u0027exerce par l\u0027intermédiaire de l\u0027environnement. Cette règle d\u0027association, bien que correcte, n\u0027a pas été jugée comme particulièrement intéressante (n\u0027apportant pas d\u0027informations nouvelles) lors de la phase de validation.\nUn autre exemple de règle qui a été extraite est donné ci-dessous.\n\"Implication dans l\u0027entreprise\" ? \"Environnement\".\nUne telle règle exprimant des informations liées à l\u0027implication dans l\u0027entreprise a été jugée comme intéressante mais qui demande une expertise plus approfondie.\nL\u0027extraction des règles d\u0027association s\u0027effectue avec des concepts très précis qui intéressent l\u0027expert. Ainsi, en reprenant les différentes catégories évoquées au début de cette section, les collocations pertinentes afin de découvrir des règles d\u0027association entre concepts sont les collocations de la catégorie 1. Les collocations issues des catégories 2, 3 et 4 sont jugées comme non pertinentes. Enfin, les collocations de la catégorie 5 ne sont pas prises en considération car l\u0027expert n\u0027a pas été apte à les valider. Cette dernière catégorie correspond en fait à des collocations qui sont ambiguës ou qui n\u0027ont pas pu être évaluées par méconnaissance partielle du domaine.\nÉvaluation de la terminologie et taux d\u0027élagage\nNous allons évaluer les résultats sur le seul corpus de CVs. En effet, pour ce seul corpus, toutes les collocations Nom-Adjectif quel que soit le taux d\u0027élagage établi ont été évaluées par un expert.\nExpertise de la terminologie\nLe tableau 3 donne le nombre de collocations de type Nom-Adjectif associées à chaque catégorie d\u0027expertise. Nous rappelons que chacune des catégories a été décrite dans la section 5 de cet article.\nLe tableau 3 présente les résultats des expertises effectuées selon différents taux d\u0027élagage. Plus les collocations fréquentes sont privilégiées en appliquant un taux d\u0027élagage important et plus la proportion de collocations de la catégorie 1 correspondant aux collocations pertinentes est élevée. À titre d\u0027exemple, si toutes les collocations sont conservées (élagage à un), la proportion de collocations pertinentes est de 56.3% contre plus de 80% en faisant un élagage de quatre, cinq ou six. \nLes mesures de précision, de rappel et de F score\nLa précision est un critère d\u0027évaluation parfaitement adapté à un cadre de travail non supervisé. La précision permet de calculer la proportion de collocations pertinentes extraites parmi les collocations extraites. En utilisant les notations du tableau 4, la précision est donnée par la formule V P V P +F P . Une précision de 100% signifie que toutes les collocations extraites par le système sont pertinentes.\n-210 -RNTI-E-6\nUne autre mesure typique du domaine de l\u0027apprentissage est le rappel qui calcule la proportion de collocations pertinentes extraites parmi les collocations pertinentes. Le rappel est donné par la formule En règle générale, il est important de déterminer un compromis entre le rappel et la préci-sion. Pour cela, nous pouvons utiliser une mesure prenant en compte ces deux critères d\u0027éva-luation en calculant le F score (Van-Risbergen (1979)) :\nLe paramètre ? de la formule (1) permet de régler les influences respectives de la précision et du rappel. Il est très souvent fixé à 1 pour accorder le même poids à ces deux mesures d\u0027évaluation. Le tableau 5 montre que la précision la plus élevée est établie après un élagage important (un élagage supérieur à quatre fournit une précision supérieure à 85%). Cependant, avec une telle précision assez élevée le rappel est souvent très faible, c\u0027est-à-dire que relativement peu de collocations pertinentes sont extraites. Avec ? \u003d 1, nous pouvons relever dans le tableau 5 que le F score est le plus élevé sans appliquer d\u0027élagage. Ceci est dû au fait que le rappel est peu important dès qu\u0027un élagage même faible est effectué. En effet, comme précisé dans le tableau 1, un seul élagage de deux permet d\u0027empêcher l\u0027extraction de 75% des collocations Nom-Adjectif du corpus de CVs.\nLe tableau 6 montre que le fait de faire varier ? afin de donner un poids plus important à la précision donne un F score logiquement plus élevé dans le cas d\u0027élagages importants. Ceci montre les limites d\u0027un tel critère d\u0027évaluation car les résultats du F score peuvent singulière-ment différer selon la valeur de ?. Ainsi, la section suivante présente un autre critère d\u0027évalua-tion global fondé sur les courbes ROC. \nLes courbes ROC\nDans cette section et dans les travaux de Ferri et al. (2002), les courbes ROC (Receiver Operating Characteristics) sont présentées. La notion de courbe ROC est initialement issue du traitement du signal. Les courbes ROC sont couramment utilisées dans le domaine de la méde-cine pour évaluer la validité des tests diagnostiques. Les courbes ROC présentent en abscisse le taux de faux positifs (dans notre cas, taux de collocations non pertinentes) et en ordonnée le taux de vrais positifs (taux de collocations pertinentes). Les courbes ROC sont adaptées aux approches supervisées. Par ailleurs, l\u0027aire sous la courbe ROC (AUC -Area Under the Curve), peut être vue comme la mesure globale de l\u0027efficacité d\u0027une mesure d\u0027intérêt. Précisons que le critère relatif à l\u0027aire sous la courbe est équivalent au test statistique de Wilcoxon-MannWhitney (voir les travaux de Yan et al. (2003)).\nDans le cas correspondant au classement des collocations en utilisant des mesures statistiques, une courbe ROC idéale correspond au fait d\u0027obtenir toutes les collocations pertinentes en début de liste et toutes les collocations non pertinentes en fin de liste. Cette situation correspond à une AUC de 1. La diagonale correspond aux performances d\u0027un système aléatoire, progrès du taux de vrais positifs s\u0027accompagnant d\u0027une dégradation équivalente du taux de faux positifs. Une telle situation correspond à AUC \u003d 0.5. Enfin, si les collocations triées par intérêt décroissant sont telles que toutes les collocations pertinentes sont précédées par les non pertinentes, alors AUC \u003d 0. Une mesure d\u0027intérêt efficace pour ordonner les collocations consiste donc à obtenir une aire sous la courbe ROC la plus importante possible ce qui est strictement équivalent à minimiser la somme des rangs des exemples positifs.\nL\u0027avantage des courbes ROC provient de l\u0027absence de sensibilité dans certains cas où un déséquilibre entre le nombre d\u0027exemples positifs et d\u0027exemples négatifs est rencontré. Illustrons ce fait avec l\u0027exemple suivant. Supposons que nous ayons 100 exemples (collocations). Dans le premier cas, nous avons un déséquilibre entre les exemples positifs et négatifs avec 1 seul exemple positif et 99 exemples négatifs. Dans le second cas, nous avons 50 exemples positifs et 50 exemples négatifs. Supposons que pour ces deux cas, les exemples positifs soient tous placés en tête de listes établies par des mesures statistiques, c\u0027est-à-dire que les collocations pertinentes sont toutes situées en début de liste.\nDans les deux cas, les courbes ROC sont strictement identiques avec une AUC égale à 1 (voir figure 1, (a) et (b)). Ainsi, le fait d\u0027avoir les collocations pertinentes en début de listes est mis en valeur par les courbes ROC et les AUC. L\u0027intérêt principal des courbes ROC est le fait de ne pas tenir compte d\u0027un éventuel déséquilibre entre le nombre de collocations pertinentes et non pertinentes. Dans le cas du calcul du F score (avec ? \u003d 1) qui prend en compte les mesures  Le tableau 7 montre que l\u0027élagage le plus adapté en terme d\u0027AUC correspond à un élagage à trois pour les collocations Nom-Adjectif du corpus de CVs. La figure 2 montre la courbe ROC relative à un élagage à trois. Ce critère objectif fondé sur l\u0027AUC correspond au choix souvent empirique d\u0027élagage à trois appliqué dans les travaux de Jacquemin (1997); Thanopoulos et al. (2002). \nConclusion et perspectives\nL\u0027étude expérimentale menée dans cet article permet de discuter le choix du taux d\u0027élagage pour la terminologie. Différents critères d\u0027évaluation existent tels que la précision, le rappel et bien entendu le F score qui permet de prendre en compte ces deux critères. Le défaut de ce dernier critère réside dans le choix pas toujours évident du paramètre le plus adapté pour privilégier la précision ou le rappel dans le calcul du F score . Ainsi, dans cet article, nous proposons d\u0027utiliser les courbes ROC et l\u0027aire sous celles-ci afin d\u0027évaluer au mieux le choix de l\u0027élagage à effectuer. Ce critère permettant d\u0027estimer rigoureusement l\u0027élagage le plus adapté n\u0027est pas sensible au déséquilibre entre les classes. Ceci est important car selon l\u0027élagage effectué la proportion de collocations pertinentes et non pertinentes peut être très différente.\nNos expérimentations ont alors montré sur un corpus de CVs, qu\u0027un élagage de trois semble bien adapté. Dans nos prochains travaux, nous proposons de comparer ce résultat à partir des autres corpus étudiés. Pour cela, il sera nécessaire d\u0027effectuer une expertise complète des collocations d\u0027autres domaines, ce qui demandera un travail conséquent aux experts.\n"
  },
  {
    "id": "1013",
    "text": "Introduction\nLa classification automatique, comme la plupart des méthodes d\u0027analyse de données peut être considérée comme une méthode de réduction et de simplification des données. Dans le cas où les données mettent en jeu deux ensembles I et J, ce qui est le cas le plus fréquent, la classification automatique en ne faisant porter la structure recherchée que sur un seul des deux ensembles, agit de façon dissymétrique et privilégie un des deux ensembles, contrairement par exemple à l\u0027analyse factorielle des correspondances qui obtient simultanément des résultats sur les deux ensembles ; il est alors intéressant de rechercher simultanément une partition des deux ensembles. Ce type d\u0027approche a suscité récemment beaucoup d\u0027intérêt dans divers domaines tels que celui des biopuces où l\u0027objectif est de caractériser des groupes de gènes par des groupes de conditions expérimentales ou encore celui de l\u0027analyse textuelle où l\u0027objectif est de caractériser des classes de documents par des classes de mots. Notons que dans ce domaine, les données se présentent généralement sous forme d\u0027un tableau de contingence où chaque cellule correspond au nombre d\u0027occurrences d\u0027un mot dans un document.\nPar ailleurs, les modèles de mélange de lois de probabilité (McLachlan et Peel, 2000) qui supposent que l\u0027échantillon est formé de sous-populations caractérisées chacune par une distribution de probabilité, sont des modèles très intéressants en classification permettant d\u0027une part de donner un sens probabiliste à divers critères classiques et d\u0027autre part de proposer de nouveaux algorithmes généralisant par exemple l\u0027algorithme classique des k-means. Dans le cadre de la classification croisée, on a pu ainsi montrer que l\u0027algorithme Crobin (Govaert, 1983) adapté aux données binaires peut être vu comme une version classifiante de l\u0027algorithme block EM (Govaert et Nadif, 2005) dans un cas particulièrement simple de mélange de lois de Bernoulli.\nDans ce papier, nous proposons d\u0027étendre ce travail à la classification croisée d\u0027un tableau de contingence. Dans la section 2, nous définirons le modèle de mélange croisé adapté à ces données. La section 3 sera consacrée à la présentation de l\u0027algorithme Cemcroki2 dont l\u0027objectif est la maximisation de la vraisemblance classifiante associée au modèle précédent. Nous montrerons dans la section 4 les liens de cet algorithme avec les critères du Chi2 et de l\u0027information mutuelle. Dans la section 5, des résultats sur des données simulées et des données réelles confirmeront l\u0027efficacité de cet algorithme et l\u0027intérêt de notre approche qui peut être considérée comme une approche complémentaire de l\u0027analyse des correspondances qui s\u0027appuie sur la même représentation des données.\nNotations Dans tout ce texte, on notera x \u003d (x ij ) le tableau de contingence construit sur les deux ensembles I et J ayant respectivement r et s éléments, n \u003d i,j x ij la somme des éléments du tableau et\n. Une partition en g classes de l\u0027ensemble I sera notée z \u003d (z 11 , . . . , z ik , . . . , z ng ) où z ik \u003d 1 si i est dans la classe k et z ik \u003d 0 sinon. Nous adoptons les mêmes notations pour la partition w en m classes de l\u0027ensemble J. Par ailleurs, pour simplifier la présentation, les sommes et les produits portant sur I, J, z ou w seront indicés respectivement par les lettres i, j et k et sans indiquer les bornes de variation qui seront donc implicites. Ainsi, la somme i,j,k, portera sur toutes les lignes i allant de 1 à r, les colonnes j allant de 1 à s, les classes en ligne k allant de 1 à g et les classes en colonne de 1 à m.\nModèle de mélange croisé\nPour aborder le problème de la classification croisée sous l\u0027aspect modèle de mélange, nous avons proposé (Govaert et Nadif, 2003) un modèle dont la densité s\u0027écrit sous la forme -458 -RNTI-E-6\nPour adapter ce modèle aux tables de contingence, on suppose que chaque valeur observée x ij dans un bloc kk de la table est la réalisation d\u0027une variable aléatoire suivant une loi de Poisson de paramètre ? i ? j ? kk où les deux premiers termes expriment les effets en ligne et en colonne et le dernier correspond à l\u0027effet du bloc kk.\nLa recherche d\u0027une partition s\u0027appuyant sur ce modèle consiste à maximiser la vraisemblance classifiante associée à notre modèle. Pour assurer l\u0027identifiabilité du modèle, nous avons ajouté les conditions\nLe problème de classification alors posé est de trouver les partitions z et w et le paramètre du modèle maximisant le critère\nAlgorithme de classification croisée\nPour maximiser L c (z, w, ?), nous proposons de maximiser alternativement cette fonction en fixant w et ? puis z et ?. /w)+g(x, w, ?) où le premier correspond à une log-vraisemblance conditionnelle associée à un mélange de distributions multinomiales appliquées sur les échantillons u 1 , . . . , u r et le second terme ne dépend pas de z. On peut alors utiliser l\u0027algorithme CEM classique (Celeux et Govaert, 1992) pour obtenir la partition z. En faisant un travail analogue pour la recherche de la partition w, on obtient finalement l\u0027algorithme Cemcroki2 suivant :\nLes expressions des estimations des paramètres du modèle associés à chaque bloc kk sont données par \nLiens avec le Chi2 et l\u0027information mutuelle\nAprès l\u0027étape de maximisation, le critère s\u0027écrit\n. est l\u0027information mutuelle associée au couple de partitions z et w. De plus, en utilisant l\u0027approximation 2x log x ? x 2 ? 1, on obtient aussi\nOn peut ainsi observer que, lorsque les proportions sont fixées, la maximisation de L c (z, w, ?) est équivalente à la maximisation de l\u0027information mutuelle I(z, w) et approximativement équivalente à la maximisation du critère ? 2 (z, w) : la maximisation du critère du ? 2 (z, w) utilisé par exemple dans l\u0027algorithme Croki2 (Govaert, 1983)  \nDonnées réelles\nPour illustrer l\u0027algorithme Cemcroki2 sur des donnés réelles, nous avons choisi les données SMART (ftp.cs.cornell.edu/pub/smart). Ces données sont définies à partir de 1033 résumés issus de la base Medline, de 1460 résumés issus de la base CISI et de 1400 résumés issus de la base CRANFIELD. En sélectionnant alors 2000 mots intéressants, Dhillon (2001) définit ainsi les données Classic3. Nous avons alors comparé les résultats obtenus par Dhillon (2001) et Dhillon et al. (2003)  \nConclusion\nEn utilisant un modèle de mélange croisé de distributions de Poisson, nous avons proposé l\u0027algorithme Cemcroki2 et montré qu\u0027il pouvait être vu comme une extension de Croki2. Ceci permet d\u0027interpréter cet algorithme Croki2 et d\u0027en déduire par exemple que l\u0027utilisation du ? 2 ou de l\u0027information mutuelle supposent implicitement l\u0027égalité des proportions des classes. Cette approche permet alors de prendre en considération de nouvelles situations comme celles où les proportions des classes sont très différentes. Les premières expériences sur des données simulées et réelles montrent que ce nouvel algorithme apparaît clairement meilleur que Croki2 dans cette situation.\n"
  },
  {
    "id": "1014",
    "text": "Introduction\nXML est devenu un standard pour la représentation et l\u0027échange de données. Le nombre de documents XML échangés augmente de plus en plus, et la quantité d\u0027information accessible aujourd\u0027hui est telle que les outils, même sophistiqués, utilisés pour rechercher l\u0027information dans les documents ne suffisent plus. D\u0027autres outils permettant de synthétiser ou classer de larges collections de documents sont devenus indispensables.\nDans ce contexte, de nombreux travaux proposent des méthodes de classification, supervisées ou non, pour organiser ou analyser de larges collections de documents XML. (Denoyer et al. (2003)) combinent plusieurs fonctions d\u0027affectation (classifiers) pour classer des documents XML multimédia, (Despeyroux et al. (2005)) identifient, pour une collection homogène donnée, les types d\u0027éléments XML les plus pertinents pour un objectif de classification. La similarité entre documents peut être définie en étendant le modèle vectoriel pour tenir compte de la structure (Doucet et Ahonen-Myka (2002), Yi et Sundaresan (2000)), ou seulement à partir de la structure d\u0027arbre des documents, selon l\u0027objectif visé ou l\u0027hétérogénéité de la collection. Ainsi, la similarité structurelle peut être basée sur la distance entre arbres (Francesca et al. (2003), Nierman et Jagadish (2002), Dalamagas et al. (2004)), ou sur la détection de § ¤ \u003c?xml version\u003d\"1.0\" ?\u003e \u003cbody\u003e \u003csec sno\u003d\"01\"\u003e \u003cst\u003eTitle of\u003cit\u003eSection 1\u003c/it\u003e: \u003c/st\u003e \u003cp\u003eThis is a paragraph\u003c/p\u003e \u003cau\u003eName of the first author\u003c/au\u003e \u003c/sec\u003e \u003csec sno\u003d\"02\"\u003e \u003cst\u003eTitle 2\u003c/st\u003e \u003cp1\u003eAnother type of paragraph\u003c/p1\u003e \u003cau\u003e Name of the second author\u003c/au\u003e \u003cbib\u003eReferences\u003c/bib\u003e \u003c/sec\u003e \u003csec sno\u003d\"03\"\u003e \u003cst\u003eTitle 3\u003c/st\u003e \u003cp\u003eOne paragraph\u003c/p\u003e \u003cbib\u003eReferences\u003c/bib\u003e \u003c/sec\u003e \u003c/body\u003e ¦ ¥\nFIG. 1 -Le document Test.xml et sa représentation sous forme d\u0027arbre\nsous-arbres fréquents (Termier et al. (2002)), Costa et al. (2004)). Comme ces algorithmes ont une complexité assez élevée, les premiers travaillent sur des résumés d\u0027arbres, tandis que les seconds sont basés sur les relations directes ou indirectes entre noeuds de l\u0027arbre. Un inconvé-nient majeur est que la fréquence d\u0027occurrence de certaines structures, comme les éléments de listes si fréquents dans les documents XML, est perdue pour certaines analyses. Nous proposons ici une nouvelle représentation de documents XML en vue de la classification, qui permet de prendre en compte soit la structure seule, soit la structure et le contenu de ces documents. L\u0027idée est de représenter un document par une linéarisation de son arbre XML en un ensemble de chemins générés selon certains paramètres. Les expressions de chemins vues comme de simples mots, et leur fréquence associée, permettent de se ramener au modèle vectoriel et d\u0027appliquer une méthode simple et standard de classification. L\u0027avantage est que l\u0027on peut travailler sur de larges collections, tout en conservant certaines propriétés structurelles et textuelles des documents.\nReprésentation des documents XML\nLes documents XML sont souvent représentés par des arbres étiquetés dont les étiquettes correspondent aux balises, et éventuellement au nom des attributs. Un exemple de document XML et de l\u0027arbre correspondant est donné en figure 1. Intuitivement, un chemin est le plus court trajet entre la racine de l\u0027arbre et un noeud et un sous-chemin est un segment de chemin. Nous proposons de représenter les documents XML par différents ensembles de sous-chemins. Cette représentation permet d\u0027aplatir les documents tout en conservant une partie de l\u0027information sur leur structure et donc de diminuer la complexité des traitements.\nNotre motivation de prendre en compte les sous-chemins, et pas seulement les chemins, est que la ressemblance entre documents peut apparaître à différents niveaux. Pour des collections de documents très hétérogènes, les différences et ressemblances apparaitront dès les premiers nivaux (proche de la racine). Pour les collections plus homogènes, il peut être intéressant de ne conserver que les sous-chemins proches des feuilles, comme contexte au contenu textuel.\nLa notion de chemin que nous définissons maintenant couvre à la fois celles de chemin et de sous-chemin.\n-Le chemin d\u0027un noeud est la suite des noeuds visités à partir de la racine pour atteindre ce noeud, en parcourant l\u0027arbre de fils en fils. Un chemin terminal est un chemin qui se termine par une feuille de l\u0027arbre. -L\u0027ensemble des sous-chemins d\u0027un noeud est l\u0027ensemble des sous-séquences du chemin de ce noeud. Un sous-chemin est dit terminal s\u0027il se termine par une feuille, initial s\u0027il commence par le noeud racine. -Un chemin ou un sous-chemin textuel est un chemin ou un sous-chemin prolongé par un mot du contenu textuel associé à un noeud. Lorsque le chemin est terminal, on considère le contenu textuel de la feuille. Si le chemin n\u0027est pas terminal, on considère l\u0027ensemble du contenu des feuilles qui sont des descendants du noeud considéré. Les chemins sont individuellement considérés comme des mots. Ce choix pose le problème d\u0027une éventuelle dépendance entre eux. Par exemple \"body.sec.st.it\" et \"sec.st.it\" ne sont évi-dement pas totalement indépendants. Ceci est important lorsqu\u0027on utilise des algorithmes de classification comme le K-means qui supposent que les mots sont indépendants, même si cela n\u0027est pas complétement vérifié en pratique. Pour remédier à ce problème, nous avons séparé ces chemins en utilisant une variable par longueur (pour plus de détail, voir section 3).\nSi l\u0027option \"text\" est spécifiée, on récupère le contenu textuel du dernier noeud du chemin ainsi que celui de tous ses descendants, et chaque mot est attaché au chemin courant, créant autant de chemins textuels que de mots.\nLes tables 1 et 2 présentent quelques ensembles de chemins, avec leur fréquence, pour l\u0027exemple de la figure 1.\nAlgorithme de classification automatique\nSClust est un algorithme, dit de nuées dynamiques (Celeux et al. (1989)), permettant de partitionner un ensemble de données en un nombre k (prédéfini) de classes homogènes (Verde et al. (2001)). Cette méthode est assez proche de celle de K-means, où la distance entre deux classes est basée sur la fréquence des mots du vocabulaire choisi (modalités). La principale différence réside dans la façon de recalculer les centres de gravité : une fois par itération pour -435 -RNTI-E-6 les nuées dynamiques, à chaque affectation d\u0027objet dans la classe pour le K-means. Si l\u0027on utilise la distance euclidienne, la distance entre deux objets x et y est calculées par la formule :\n, où m est le nombre de modalités. De plus, dans SClust, les objets peuvent être décrits par plusieurs variables, et chaque variable contient un ensemble de modalités caractérisant l\u0027objet.\nPar exemple, l\u0027objet voiture possède les modalités poids, longueur, largeur, hauteur et couleur. Il existe clairement une dépendance entre le poids et les dimensions de la voiture, ce qui nous amène à créer deux variables, la première, dimensions, contenant les modalités longueur, largeur et hauteur, la seconde, caractéristiques, contenant poids et couleur.\nDans notre cas (partition des documents), les individus sont les documents XML et les modalités sont représentées par l\u0027ensemble des chemins regroupés par longueur où chaque longueur représente une variable. La distance entre deux documents x et y est alors remplacée par la formule : d(x,y) \u003d p k\u003d1 mk j\u003d1 (x k j ? y k j ) 2 , où p est le nombre de variables et m k est le nombre de modalités pour la variable p.\nLa complexité d\u0027un algorithme de classification automatique est difficile à calculer, car elle dépend de la phase d\u0027initialisation qui est faite de manière aléatoire. Généralement, on peut dire qu\u0027elle est linéairement dépendante de la taille des données, du nombre d\u0027exécutions (dû à l\u0027initialisation aléatoire) et du nombre d\u0027itérations (fixés par l\u0027utilisateur).\nÉvaluation et mesures\nPour évaluer de notre algorithme, quatre métriques sont employées. Elles permettent de comparer le résultat de la classification avec une classification existante, lorsque celle-ci existe :\n-Entropie : mesure la façon dont les classes prédéfinies sont distribuées ou réparties dans chaque classe calculée. Une valeur faible de l\u0027entropie correspond à un meilleur paritionnement (Zhao et Karypis (2001)). -Pureté : mesure la proportion de documents de la classe calculée appartenant à la classe à priori la plus fréquente, c\u0027est à dire le pourcentage de documents de la classe calculée appartenant à la classe majoritaire (Zhao et Karypis (2001)). La pureté d\u0027une classe calculée est égale à 1 si tous les documents sont issus de la même classe à priori. RNTI-E-6\nA.-M. Vercoustre et al.\nFIG. 2 -Chaîne de Traitement\n-F-measure : proposée par Larsen et Aone (1999), combine les mesures de précision et de rappel pour essayer d\u0027assigner chaque classe calculée à une classe prédéfinie de telle sorte que deux classes calculées ne peuvent pas être assignés à la même classe prédéfine. La F-measure mesure un équilibre entre la précision et le rappel. C\u0027est la moyenne harmonique des deux valeurs précédentes. -Rand corrigé : (ou Corrected Rand) proposé par Hubert et Arabie (1985) pour comparer deux partitions. On peut utiliser cette métrique pour comparer le résultat de la classification automatique avec une classification existante, ou bien de comparer deux partitions obtenues à partir de l\u0027algorithme de classification automatique.\nTraitement des collections\nLe traitement pour partitionner une collection de documents XML se compose de trois principales étapes qui sont montrées dans la Figure 2.\nLe prétraitement consiste principalement à réduire le nombre de chemins générés, en essayant de diminuer successivement le nombre de balises, de mots et de chemins différents. Le filtrage sur les balises utilise une connaissance sémantique de ces balises. On peut, optionnellement :\n-Remplacer les balises synonymes par un seul représentant. Par exemple, remplacer ss1, ss2, ss3 par sec pour les sections ; numeric-list, bulletlist par list pour les listes ; h1, h2, h3, h4, h1a, h2a par h pour les headers. -Ignorer la descendance de certaines balises. Par exemple math et f ormula pour les formules mathématiques dont les descendants ont une granularité trop fine pour la classification, et table pour les tables. -Supprimer les balises de mise en forme du texte (bold, italic etc.). Pour le texte, nous utilisons les méthodes classiques de réduction du vocabulaire : -Utiliser une Stopword List qui est un ensemble de mots dits vides qui n\u0027ont aucun rôle à jouer dans la classification automatique et donc ne doivent pas être inclus dans les chemins. -Supprimer les mots de moins de 4 caractères. -Stemming : pour le processus de normalisation des termes, nous avons utilisé l\u0027algorithme de Porter (Porter\u0027s Stemmer) (Porter (1997)). Cet algorithme supprime les suffixes des mots en anglais. La suppression des suffixes permet de réduire la taille et la complexité des données pour améliorer les performances. Le principale avantage de cet algorithme est sa rapidité de traitement de grand volume de données. Enfin nous réduisons le nombre final de chemins en utilisant les fréquences relatives TfIdf de ces chemins : les chemins trop fréquents ou trop rares sont éliminés. Dans Denoyer (2004), la collection a été utilisée pour évaluer des méthodes de classification en essayant de classer automatiquement les documents dans un des 18 journaux de la collection. Devant la mauvaise qualité des résultats, l\u0027auteur a étiqueté la collection en 6 thèmes (Ordinateur, Graphisme, Hardware, Intelligence Artificielle, Internet et Architecture Parallèle) et 2 classes structurelles (les transactions et les autres articles). Nous avons repris ces classifications pour notre évaluation.\nExpériences et résultats\nDifférentes expériences sont lancées sur ce corpus, en utilisant soit la structure seule, soit la structure et le contenu. Avec la structure seulement, nous cherchons à séparer les 2 classes structurelles (transactions vs articles). En utilisant la structure et le contenu, nous cherchons à retrouver soit les 6 thèmes prédéfinis, soit les 18 journaux initiaux.\nUtilisation de la structure seule : transactions vs articles\nDans cette expérience, nous avons représenté les documents par leurs chemins de longueur comprise entre 3 et 5 (groupés dans trois variables), commençant à la racine. La première variable essaie donc de partitionner à partir des structures de haut niveau, tandis que la deux autres variables pourront éventuellement capter des similarités dans des parties plus détaillées du document.\nLa figure 3 montre, pour différents nombres de classes, les valeurs des différentes mesure de similarité avec les 2 classes existantes.\nNous remarquons que les métriques sont cohérentes entre elles et atteignent leurs valeurs optimales lorsque le nombre de classes calculées est égal à 4. Dans ce cas, la répartition des deux classes (transaction et article) à l\u0027intérieur de chaque classe calculée est montrée dans la figure 4.\nNous constatons que les articles se répartissent sur les classes 1, 2 et 3. Les transactions sont bien regroupées dans la classe 4. Dans la classe 1, nous constatons la présence de quelques transactions dont la structure est probablement assez proche de celle des articles.\nNous avons constaté que l\u0027utilisation des attributs dans les chemins n\u0027améliore pas les performances du système.\nFIG. 3 -Mesures d\u0027évaluation (classification en articles et transactions) en fonction du nombre de classes (structure seule).\nFIG. 4 -Répartition des classes à priori Articles et Transactions sur quatre classes calculées.\nFIG. 5 -Mesures d\u0027évaluation (classification thématique) en fonction du nb de classes (structure + contenu).\nFIG. 6 -Mesures d\u0027évaluation (classification en journaux) en fonction du nombre de classes (structure + contenu).\nUtilisation de la structure et du contenu\nDans ces expériences, nous n\u0027avons utilisé que 5% de documents de la collection d\u0027INEX (tirés au hasard), pour limiter la taille des données et donc les temps de calculs. Nous présen-tons ici les résultats en utilisant des chemins textuels de longueur 3, commençant à la racine, faute de place pour présenter plus de résultats. Nous considérons les deux classifications : les 6 thèmes et les 18 journaux.\nLa figure 5 montre la valeur des différentes mesures pour la comparaison avec les 6 thèmes, en faisant varier le nombre de classes calculées.\nOn peut noter que les quatre métriques sont cohérentes entre elles et ceci lorsque le nombre de classes calculées varie entre 8 et 15. Le meilleur regroupement est celui en 9 classes, qui permet d\u0027avoir des valeurs optimales pour l\u0027ensemble des métriques.\nUne deuxième façon de prendre en compte le contenu des documents XML consiste à attacher le texte aux chemins terminaux. Nous avons constaté que les résultats différent peu, selon que l\u0027on utilise des chemins textuels commençants à partir de la racine ou des chemins textuels terminaux. Cela vient sans doute de la faible profondeur des arbres.\nFIG. 7 -Mesure d\u0027évaluation (classification en Thèmes du RA) en fonction du nombre de classes (structure seule).\nFIG. 8 -Mesure d\u0027évaluation (classification en Thèmes du RA) en fonction du nombre de classes (structure + contenu).\nLa figure 6 montre la valeur des différentes mesures pour la comparaison avec les 18 journeaux, en faisant varier le nombre de classes.\nLes résultats obtenus sont moins bons que ceux de la classification thématique. Ceci est due à l\u0027hétérogénéité de la structure et du contenu des documents dans chaque revue, et confirme les résultats obtenus dans Denoyer (2004).\nRapports d\u0027activités INRIA (RA)\nL\u0027INRIA publie un rapport d\u0027activité annuel dont l\u0027annexe scientifique est accessible sur le web. C\u0027est un ensemble de rapports en anglais produits par chaque équipe de recherche. Pour plus de détails sur la structure de ces rapports, se reporter à (Despeyroux et al. (2005)).\nLes équipes de recherches sont regroupées en 5 thèmes : Systèmes Biologiques, Systèmes Cognitifs, Systèmes de Communication, Systèmes Numériques et Systèmes Symboliques. On peut supposer que les rapports d\u0027activité des équipes refléteront plus ou moins ces thèmes dans la présentation de leur activité de recherche et donc que l\u0027on puisse retrouver ces thèmes en classant les documents à partir de leur contenu. Par contre, l\u0027hypothèse est qu\u0027en utilisant seulement la structure des rapports, on a aucune chance d\u0027identifier ces thèmes. En effet la structure des rapports est basée sur un modèle commun (DTD), avec peu de parties optionnelles.\nUtilisation de la structure seule\nLa figure 7 montre que l\u0027hypothèse précédente est vérifiée, puisqu\u0027en utilisant la structure seule (chemins non textuels de longueur comprise entre 3 et 5), la valeur des différentes mesures est uniformément mauvaise, quelque soit le nombre de classes (en particulier le Rand Corrigé est proche de zero). Ceci est tout à fait logique car, comme indiqué en section 4, le Rand Corrigé permet de comparer deux partitions. Or, les deux partitions sont construites de manière totalement différente (l\u0027une est basée sur la structure et l\u0027autre sur le contenu).\nPar contre, nous pouvons essayer d\u0027analyser quantitativement le résultat d\u0027une partition en n classes.\nLe tableau 3 présente les chemins les plus fréquents pour 7 classes (par ordre d\u0027importance). Nous nous sommes limités à ne présenter que les chemins de longueur 2 pour plus de clarté. \nTAB. 3 -Les chemins importants dans les classes\nOn peut interpréter ces résultats en disant, par exemple, que les équipes ayant plus de logiciels et de contrats de collaboration se trouvent dans la classe 3, et que les équipes de la classe 4 se caractérisent par la diversité des activités internationales, le nombre important des publications et des contrats de collaboration. Il faut comprendre qu\u0027un chemin comme raweb.logiciels synthétise (dans le tableau de résultats) en fait beaucoup de chemins de longueur 3 (raweb.logiciels,subsection) correspondant à la descriptions des différents logiciels. On voit ici l\u0027importance pour certaines analyses de conserver tous les éléments de liste et pas seulement leur existence ou non existence.\nUtilisation de la structure et du contenu\nEn utilisant des chemins textuels, nous constatons (figure 8) une amélioration des diffé-rentes mesures de comparaisons avec les Themes. Globalement, Le meilleur regroupement est celui en 11 classes, où l\u0027on constate une décroissance de l\u0027entropie, ce qui signifie que les documents ne sont pas trop éparpillés sur les classes, et une augmentation du Rand corrigé, signifiant que ce regroupement se rapproche un peu de la classification en thèmes.\nÉtat de l\u0027art\nUn aspect important et caractérisant des travaux sur la classification de documents XML est la manière de représenter les documents. Doucet et Ahonen-Myka (2002) représentent un document XML en utilisant le modèle vectoriel dans lequel les modalités sont soit les balises (balises XML), soit les mots du texte (contenu), soit une combinaison des deux (balises et texte). La technique TfIdf est utilisée pour normaliser les éléments du vecteur et l\u0027algorithme de classification utilisé est le K-means. Ceci correspond à notre approche, dans le cas où l\u0027on fixe respectivement la longueur des chemins non textuels à 1, textuels à 1, ou textuels et non textuels à 1. Yi et Sundaresan (2000), proposent un modèle vectoriel structuré afin de tenir compte de la structure des documents XML. Les éléments du vecteur sont des termes (mots) ou un autre vecteur structuré (récursivité). Ceci revient a représenté chaque élément par son chemin à partir de la racine (noeud et mots du texte). Ceci est donc équivalent à notre représentation à partir de chemins commencant à la racine, avec les chemins textuels allant jusqu\u0027aux feuilles. La méthode de classification probabiliste basée sur le modèle de génération de documents de Bernoulli est ensuite appliquée sur ce vecteur.\nUne autre amélioration (Jianwu et Xiaoou (2002)) consiste à modéliser des éventuels liens entre les documents dans le SVM (Structured Link Vector Model), où les éléments du vecteur sont les termes, la structure et le voisinage des documents. La méthode K-means est utilisée comme algorithme de classification. Notre approche ne tient pas compte des liens entre les documents.\nDans Yoon et al. (2001), deux représentations de documents sont proposées. La première consiste à utiliser une matrice binaire (bitmap) à 2 dimensions (2D) : les documents et les ePath (le chemin d\u0027un élément feuille à partir de la racine). Cette représentation a été étendue à 3D : les documents, les ePath, et le contenu. Les ePaths correspondent au cas où nous utilisons les chemins de longueur quelconque allant de la racines au feuilles. Toutefois la methode de classification automatique est très différente. Liu et al. (2004) traite des documents XML homogènes (même DTD). Cette approche travaille sur des arbres XML ordonnés pour extraire des informations selon trois méthodes : chemins, paires des noeuds (père-fils, frères, etc.) ou bien une combinaison des deux (hybride). Ces informations sont, ensuite, transformées en un vecteur de la forme (information, nombre d\u0027occurrence). Pour réduire la taille du vecteur, la technique d\u0027analyse en composante principale (ACP) est utilisée. La première représentation des documents est celle qui est la plus proche de celle que nous utilisons (chemins de longueur inférieure à une certaine valeur, commençant à un niveau donné dans l\u0027arbre). L\u0027idée d\u0027utiliser des chemins commençant à un certain niveau mériterait d\u0027être explorée, car elle peut faciliter l\u0027interprétation des résultats.\nD\u0027autres travaux porte sur des arbres étiquetés, en utilisant plusieurs techniques, comme la détection de sous-arbres fréquents (Termier et al. (2002)), le tree-matching (Costa et al. (2004)) et les résumés d\u0027arbres (Dalamagas et al. (2004)). Ces travaux utilisent une représentation des documents à partir des relations binaires entre noeuds qui s\u0027éloignent de notre représentation. Les méthodes de classification utilisées sont également très différentes et plus appropriées au traitement de collections hétérogènes.\nConclusion\nNous avons proposé dans ce travail un nouveau modèle de représentation des documents XML qui prend en considération la structure et le contenu de ces documents. Ce modèle est basé sur la notion de linéarisation des chemins qui nous a permis d\u0027aplatir les documents. Avec cette représentation, nous avons pu ensuite appliquer un algorithme de classification simple et performant, en l\u0027occurrence SClust, et montré nos résultats obtenus sur les deux collections, INEX et les rapports d\u0027activité.\nBien que les deux collections utilisées ne soient pas très adaptées aux problèmes de classification automatique thématique à partir de la structure, notre modèle nous a permis tout de même de séparer les documents de structure différente (le cas \u0027articles vs transactions\u0027 dans INEX), et d\u0027exhiber des propriétés inconnus des documents en analysant les parties des DTD les plus utilisées. Ce modèle nous offre aussi la possibilité de reconstruire les DTDs, et ceci en analysant l\u0027ensemble des chemins représentant chaque classe.\nComme travaux futurs, nous envisageons les points suivants : -Prendre en compte l\u0027ordre des noeuds dans l\u0027arbre XML (modèle d\u0027arbres ordonné).\n-Appliquer notre approche sur d\u0027autres collections de documents XML, comme MovieDB et WIPO, et surtout sur des collections dédiées au XML Mining comme les collections du track INEX \u0027XML Mining\u0027, collections synthétiques éventuellement plus hétérogènes .\n"
  },
  {
    "id": "1015",
    "text": "Introduction\nLes comptes-rendus de mammographies écrits en texte libre sont difficiles à interpréter et à analyser par un programme machine. La difficulté est liée à la nature informelle de ces comptes-rendus. Trouver un processus qui permet de structurer les comptes-rendus et donner une représentation formelle de leur contenu est une tâche difficile vue la complexité du langage naturel et des connaissances médicales (Zweigenbaum, 1994).\nL\u0027objectif principal de cet article est de montrer une utilisation possible dans le domaine médical des ontologies formelles en OWL, le langage standard d\u0027ontologie du Web (OWL, 2004). Ce travail vise à fournir un outil d\u0027aide à l\u0027interprétation des comptes-rendus médi-caux mammographiques et à leur classification. Il a consisté d\u0027abord à concevoir et réaliser une ontologie regroupant tous les concepts du domaine : concepts radiologiques, concepts pathologiques, et différentes classes ACR. Les classes ont été définies à partir de la classification dite ACR (ACR, 2000) et ont été représentées dans le langage OWL DL en utilisant l\u0027éditeur Protégé et son plugin OWL (Holger, 2004). Notre système a pour tâche, d\u0027extraire les faits correspondant au contenu des comptes-rendus de mammographies, puis, d\u0027inférer la classe pathologique correspondante selon la classification ACR en utilisant le raisonnement par subsumption, et d\u0027en déduire la conduite à tenir. L\u0027interprétation des comptes-rendus utilisée dans le cadre de ce travail est basée sur l\u0027exploitation de la logique de description comme langage d\u0027ontologie, comme (Golbreich, 2003) qui reposait sur C-Classic pour l\u0027indexation et la recherche d\u0027images, et non les graphes conceptuels comme dans le système Menelas (Zweigenbaum, 1994) ou le traitement du langage naturel comme dans le système MedLee (Nilesh et al., 1995). L\u0027avantage de notre méthode est double. Elle permet d\u0027une part l\u0027utilisation du standard du Web Sémantique OWL pour la description des connaissances, donc le partage et l\u0027interopérabilité, et d\u0027autre part des services puissants de raisonnement basés sur la logique de description.\nL\u0027idée principale est de suivre dans chaque phrase du compte-rendu radiologique donné en texte libre la trace des concepts, instances, et propriétés de l\u0027ontologie. Si l\u0027extracteur localise dans une phrase bien identifiée des concepts, instances ou propriétés, il va essayer de déterminer les relations entre eux, à partir des modèles de classes fournis par l\u0027ontologie. Ensuite, un raisonneur pour la logique de description OWL DL identifie pour l\u0027individu associé au compte-rendu la (les) classes possibles et l\u0027attitude à tenir qui en résulte.\nLe reste de l\u0027article est organisé comme suit : dans la deuxième section, nous décrivons la Classification ACR introduite par les systèmes BIRADS et nous motivons notre besoin de construire une ontologie radiologique dans le format OWL ( §2). Ensuite nous décrivons l\u0027ontologie ACR en OWL ( §3). La dernière section présente un aperçu de l\u0027architecture globale du système et des fonctionnalités attendues ( §4).\nLes systèmes BIRADS et la classification ACR\nLa mammographie permet d\u0027établir un diagnostic et de donner des indications pronostiques concernant des lésions observées sur des images mammographiques. Son rôle principal est la détection précoce du cancer du sein chez des patientes asymptomatiques (ACR, 2000).\nLes règles qui permettent d\u0027établir une conclusion diagnostique et/ou pronostique à partir de caractéristiques morphologiques observées dans les images mammographies sont publiées dans le cadre de systèmes de classification. Les systèmes BIRADS de l\u0027American College of Radiology (ACR, 2000) standardisent une classification des images mammographiques en six catégories en fonction du degré de suspicion de leur caractère pathologique. Ils ont pour objectif de standardiser le compte-rendu grâce à une structure claire et un lexique bien défini, afin d\u0027éviter les erreurs d\u0027interprétation. De ce fait, construire une ontologie partageable entre pathologistes et radiologues dispersés géographiquement est crucial. L\u0027ontologie doit regrouper des concepts utilisés par les radiologues dans leur description physique des signes radiologiques issus de deux types d\u0027examens complémentaires : l\u0027échographie et la mammographie ainsi que leur morphologie (forme, taille, localisation, nombre, contour, densité, etc.), des concepts décrivant les anomalies pathologiques mammaires (Kystes, carcinomes, etc.) et les différentes classes données par la classification ACR décrivant la catégorie et l\u0027attitude à tenir.\nDans une première étape, une construction manuelle ou supervisée, élaborée suite à une analyse approfondie de la classification normalisée ACR, une consultation d\u0027une base de comptes-rendus mammographiques et des documents biomédicaux, combinée à des interviews d\u0027experts, a permis d\u0027identifier les principales classes du domaine impliquées dans la classification ACR et de définir une taxonomie des concepts du domaine présentée dans la figure 1. La deuxième étape a été de définir la représentation logique en OWL DL des diffé-rentes classes ACR et autres concepts. (a) Les classes primitives : 6 Classes \u0027primitives\u0027 ont été définies pour les concepts racines, au premier niveau de la figure 1: Anomalie, Attitude, Caractéristique Physique, Lesion, Signe, Anatomie. Notre premier travail a été d\u0027identifier ces concepts et de trouver une taxonomie (Noy, 2001) assez puissante des signes de diagnostic en se basant sur les caracté-ristiques physiques e.g., morphologie, puis de donner les formules logiques combinant les signes radiologiques et les lésions correspondantes pour exprimer les conditions nécessaires et suffisantes ou nécessaires (Golbreich et al 2004) pour chaque Classe ACR.\n(b) Les proprietés : puis nous avons défini les propriétés. Un concept peut être relié à d\u0027autres concepts de l\u0027ontologie par des propriétés OWL comme la forme, le bord, le signe, représentées par des objectProperty en OWL e.g., hasForm, hasBord, hasSign, etc. \nFIG. 2 -Conditions Nécessaires Et/Ou suffisantes De Anomalie1\nLes autres anomalies sont représentées de la même manière, et la classe ACR2 est définie comme l\u0027union des sous-classes d\u0027anomalies de 1 à 8 :\nL\u0027axiome suivant exprime que les anomalies de la classe ACR2 impliquent ni surveillance ni examen complémentaire : ACR2 ? ¬(Survellance ? ExamenComplementaire) La vérification de la consistance et la classification de l\u0027ontologie sont réalisées automatiquement à l\u0027aide de Racer (Haarslev et al., 2001) dans l\u0027environnement Protégé OWL.\nL\u0027application\nL\u0027idée principale de notre application réside dans l\u0027extraction d\u0027instances et de leurs propriétés à partir des comptes-rendus en utilisant des techniques de traitement du langage naturel comme dans (Ricky et al., 2001). Contrairement à cette approche nous exploitons une représentation formelle extraite des comptes-rendus et le raisonnement ontologique par Racer pour identifier les classes ACR des comptes-rendus. L\u0027architecture globale et ses diffé-rents composants sont décrits figure3 : L\u0027analyse structurelle : l\u0027analyser structurelle a pour rôle de reconnaître la structure du compte-rendu : entête, dates, informations patients, contenu, etc. \nConclusion\nDans le présent travail, nous avons conçu un système permettant la classification des comptes-rendus mammographiques en utilisant une ontologie formelle pour classer les images radiologiques-mammaires à partir des signes radiologiques observés et d\u0027une Classification normalisée dite ACR. L\u0027ontologie est représentée en OWL DL à l\u0027aide de Protégé OWL. Un raisonneur comme Racer sera utilisé pour identifier les classes possibles d\u0027appartenance pour les images présentant des signes d\u0027anomalies. L\u0027application est déve-loppée en Java en utilisant plusieurs API. Les analyseurs lexical, syntaxique et sémantique\n"
  },
  {
    "id": "1016",
    "text": "Introduction\nLa classification, ou clustering (Jain et al., 1999), consiste à associer une classe à chaque élément d\u0027un ensemble, les éléments similaires devant être regroupés dans une classe en n\u0027utilisant que la similarité (ou distance) entre deux éléments ou groupes d\u0027éléments. Le formalisme attributs-valeurs ne permettant pas de représenter les domaines complexes, l\u0027apprentissage en logique du premier ordre, ou Programmation Logique Inductive (PLI), a attiré une attention croissante. Le language utilisé en PLI, DATALOG, est un formalisme relationnel ne permettant pas les fonctions, et dont le test de couverture, la ?-subsomption, est une restriction décidable mais NP-difficile de l\u0027implication logique. Cet article présente une méthode permettant l\u0027utilisation d\u0027algorithmes de clustering sur des données relationnelles, en recherchant préliminaire-ment tous les motifs relationnels existant et en les utilisant pour définir une distance entre des clauses en DATALOG.\nPrésentation de l\u0027algorithme\nL\u0027algorithme proposé consiste en trois étapes : la recherche de tous les motifs relationnels de la base, l\u0027élimination des motifs inintéressants et le clustering des clauses DATALOG, en utilisant les motifs pour calculer la distance entre les exemples. La recherche des motifs relationnels est effectuée par JIMI (Maloberti et Suzuki (2003)) qui est une version relationnelle d\u0027un algorithme de recherche en largeur d\u0027itemset fréquents. Chaque exemple est tranformé en un vecteur booléen dont les valeurs correspondent au test de ?-subsomption 1 des motifs contre cet exemple, ces vecteurs permettant d\u0027utiliser les distances existantes. Différents paramètres peuvent être utilisés : différents poids sur les motifs durant le calcul de la distance, tels que la taille des motifs ou l\u0027inverse de la fréquence, utilisation des n premiers niveaux trouvés par JIMI plutôt que tous les niveaux, utilisation d\u0027une partie des motifs (tous les motifs maximaux, i.e. fermés, ou les motifs minimaux).\nNotre méthode a été testée sur 2 ensembles de données réelles avec un algorithme de clustering hiérarchique ascendant et une distance euclidienne. Le premier test concerne la détection d\u0027accès hostiles sur le site web \"www.slab.dnj.ynu.ac.jp\". Les données, dont des résultats ont déjà été publiés dans Narahashi et Suzuki (2003)   Hirose et Suzuki (2005) 0.719 avec 2 clusters. Ce problème n\u0027étant pas relationnel, les 2 premiers niveaux ont les meilleurs résultats, l\u0027utilisation de plus de niveaux n\u0027a conduit qu\u0027à la création de plus de clusters. Le second ensemble de données, décrit dans King et al. (1995), concerne la détection de capacité à provoquer des mutations et représente 230 molécules, dont 138 positives et 92 négatives. Les résultats ont été médiocres, une précision de 0.51, car seule la description des atomes et de leurs relations a été utilisée, ce qui est insuffisant pour obtenir des motifs discriminants.\nConclusion et perspectives\nNous avons proposé une nouvelle méthode permettant le clustering de données relationnelles et nous avons utilisé ce système sur deux ensembles de données. Les résultats prélimi-naires montrent que ce système peut égaler les autres algorithmes sur des données non relationnelles, l\u0027expérimentation sur des données relationnelles n\u0027ayant pas permis de conclure. Parmi les perspectives, l\u0027utilisation d\u0027un algorithme de clustering pouvant gérer de grandes dimensions, tel que le subspace clustering, serait intéressante car le grand nombre de motifs rend les distances très instables mathématiquement.\nSummary\nThis paper presents an algorithm for clustering of relational data in DATALOG formalism which searches all relational patterns in the base, then transforms each example in a boolean vector corresponding to the results of its covering tests against the patterns.\n"
  },
  {
    "id": "1017",
    "text": "Introduction\nL\u0027analyse exploratoire d\u0027un tableau de données, que ce soit un tableau classique croisant unités statistiques et caractères quantitatifs, ou un tableau de contingence croisant les modalités de deux caractères qualitatifs, est généralement réalisée par les quatre étapes de la procé-dure suivante :\n1. Analyse factorielle exploratoire : selon le type de tableau, il s\u0027agit d\u0027une Analyse en Composantes Principales (ACP) ou une Analyse des Correspondances (AFC) ; 2. classification des lignes, à savoir des individus ou des modalités en ligne ; 3. interprétation des classes obtenues à l\u0027aide du comportement des caractères originaux dans chaque classe ; 4. Étude des liaisons entre classes et axes factoriels. L\u0027originalité de l\u0027approche proposée dans cet article est d\u0027unifier, dans une même mé-thode, l\u0027analyse factorielle du tableau et les classifications des lignes et des colonnes. En effet, les plans factoriels obtenus sont directement associés aux noeuds des hiérarchies construites. Ce qui permet d\u0027obtenir une interprétation conjointe des noeuds et des axes factoriels facilitant la synthèse des résultats. Les approches classiques résumées par les quatre étapes décrites ci-dessus ne permettent pas, dans bien des cas, d\u0027obtenir rapidement cette vue synthétique de l\u0027ensemble des résultats En effet, les relations existant entre noeuds et axes factoriels sont souvent difficiles à expliquer dans l\u0027approche classique, surtout pour un utilisateur peu exercé.\nLe problème de la classification des variables n\u0027a pas été beaucoup été traité en littéra-ture, le seul exemple couramment utilisé par les non-spécialistes étant la procédure VAR-CLUS intégrée dans SAS (1999). Pourtant, Lerman (1981) a proposé un véritable système de méthodes de classification des caractères, intégré dans une méthodologie générale de classification.\nLes méthodes qu\u0027on propose ici, développées par Denimal (1997Denimal ( , 2001), se basent, comme d\u0027ailleurs celles proposées par Qannari et al. (1999), sur des ACP ou, dans le cas original d\u0027un tableau de contingence, sur des AFC. Nous nous distinguons, cependant, par le fait que l\u0027on propose des méthodes hiérarchiques ascendantes, ce qui permet de retarder le choix du nombre de classes de la partition à retenir après la construction de la hiérarchie et l\u0027interprétation des résultats. Ainsi, le nombre de classes à retenir est déterminé sur la base des résultats et non pas à priori. En outre, notre approche est en accord avec celle de Lerman pour laquelle chaque classe est également identifiée avec un facteur.\nLes méthodes\nDans les deux cas on suppose les tableaux déjà normalisés, selon deux principes diffé-rents suivant le type de tableau. Au départ de l\u0027algorithme, chaque colonne est vue comme un groupe singleton dont elle est la variable représentative. Ensuite la procédure itérative pour construire la hiérarchie est la suivante : à chaque pas 1. On effectue une analyse factorielle exploratoire sur tout couple de colonnes ; 2. On choisit comme noeud de la hiérarchie à construire le couple de classes dont la deuxième valeur-propre de l\u0027analyse factorielle correspondante est minimum ; 3. Ce noeud établi, on choisit comme variable représentative du groupe ainsi formé le premier vecteur-propre de l\u0027analyse factorielle correspondante. On vient de remarquer que la différence entre les deux analyses consiste en la différente analyse factorielle utilisée : pour le tableau quantitatif on utilise une ACP non normée, à savoir basée sur la matrice de variance-covariance des deux caractères ; pour le tableau de contingence on utilise une AFC, mais dans ce cas on n\u0027obtiendrait qu\u0027un seul facteur. Donc il est nécessaire d\u0027introduire une astuce, consistant à associer à toute colonne j une colonne j* complément de j par rapport à la colonne marginale, à savoir telle que, pour tout i, n ij * \u003d n i . -n ij . Cependant, pour tout couple de colonnes j, k, on effectue l\u0027AFC du tableau de contingence croisant toutes les lignes avec les quatre colonnes {j, k, j*, k*}. Comme j + k \u003d j* + k*, l\u0027AFC ne fournit que deux facteurs non nuls. Une fois effectuée l\u0027AFC, on ajoute les nouvelles colonnes j n1 et j n1 * telles que\noù F 1 (i) est la coordonnée de i sur le premier facteur, f i est la fréquence la ligne i, et k (.,j n1 ) et k (.,j* n1 ) sont quantités définies par des égalités supplémentaires dépendant du signe des coordonnées de j, k, j*, et k* sur le premier facteur (Denimal., 2000).\nOn a les propriétés suivantes : -Le critère d\u0027agrégation est de type Ward, les deuxièmes valeurs-propres, utilisés comme indices de la hiérarchie forment une séquence non décroissante ; -l\u0027inertie totale du tableau est la somme des deuxièmes valeurs-propres des noeuds et de la première du noeud le plus haut ; -à tout noeud un plan factoriel est associé, où sont représentées les colonnes des groupes fusionnés ainsi que l\u0027ensemble des lignes ; -dans ces plans, le premier vecteur-propre peut s\u0027interpréter comme un compromis entre les 2 classes regroupées et le deuxième vecteur propre comme traduisant leurs différences ; -chaque groupe s\u0027interprète comme un dipôle, mettant en opposition des caractères ou des modalités : ceci permet une interprétation des groupes plus imagée.\nLes exemples d\u0027application\nDonnées quantitatives\nDans cet exemple on a appliqué la classification hiérarchique factorielle sur un tableau de données saisies dans une communauté végétale typique des pâturages de Campos dans le Brésil du Sud (Pillar et al., 1992).\nIl s\u0027agit de 60 relevés carrés de 0.5 x 0.5 m. alignés le long de quatre gradients, allant du haut en bas, dont on a la composition des 60 espèces présentes et 21 variables décrivant la structure du sol. On s\u0027occupe ici des variables quantitatives décrivant la structure du sol. Dans le Tableau 1 on voit le dendrogramme de la hiérarchie formée par les variables et dans la Figure 1 le plan factoriel associé à l\u0027avant-dernier noeud de la hiérarchie. \nFréquences\nDans cet exemple, on étude le contenu iconographique des images de sceaux Mésopota-miens de la période Uruk-Jamdat Masr. Il s\u0027agit de petits cylindres de pierre dont les images gravées viennent imprimées, lorsqu\u0027on les fait rouler, sur la craie ou sur autre support.\nPour le codage des images, on les a décrites à l\u0027aide d\u0027un texte formalisé qui a été ensuite traité par des analyses textuelles (telle que l\u0027Analyse des Correspondances Textuelles, Lebart et Salem, 1994;Camiz et Rova, 2001). Ici le tableau de contingence, croisant 834 images avec 169 formes textuelles, a été soumis à la classification hiérarchique factorielle basée sur l\u0027AFC. Dans le Tableau 2 on voit la succession des deuxièmes valeurs-propres de la hiérar-chie, ainsi que l\u0027inertie synthétisée par les variables représentatives des groupes formés. .37306 *********************************** 336 334 335 0.42892 4.07197 90.44504 ******************************************* 337 336 303 0.45188 4.28989 94.73494 ********************************************* Total var. representative 0.55460 5.26505 100.00000\nTAB. 2 -La progression des indices de la hiérarchie des formes lexicales des sceaux.\nDans la Figure 2 on voit le plan factoriel associé à l\u0027avant-dernier noeud, avec les formes plus intéressantes des deux groupes, ainsi que les images des sceaux les plus significatifs de ce type.\nFIG. 2 -Les formes les plus significatives associées aux variables représentatives des groupes 334 et 335 se fusionnant dans le noeud 336 et les images des sceaux correspondants les plus typiques.\nConclusion\nL\u0027objectif des méthodes proposées dans cette nouvelle approche est d\u0027obtenir une information synthétique de manière plus aisée et plus rapide pour l\u0027utilisateur. En particulier on a remarqué que le plan factoriel associé au dernier noeud montre très souvent une structure très proche de celle issue d\u0027une ACP où d\u0027une AFC. En plus, si l\u0027objectif de l\u0027analyse porte sur l\u0027identification des caractères les plus intéressants pour des études ultérieures, leur sélection, par exemple à l\u0027aide de leur corrélation avec les variables représentatives des groupes, peut s\u0027avérer fort intéressante. De même, si on cherche des règles d\u0027extraction de données, on peut associer aux analyses une segmentation des unités pas à pas (Camiz et Denimal, 2003).\nIl reste à comparer ces méthodes avec d\u0027autres méthodes de classification, basées sur d\u0027autres critères, en particulier avec ces de Lerman (1981) où l\u0027idée des facteurs découle aussi des classifications.\nDes généralisations semblent également possibles, soit relativement à d\u0027autres types de données (plusieurs caractères qualitatifs, tableaux multiples), soit dans d\u0027autres domaines, tels que l\u0027analyse discriminante, les arbres de décision, etc.\n"
  },
  {
    "id": "1018",
    "text": "Introduction et objectifs\nPour rendre compte avec exactitude des évolutions temporelles, cruciales dans beaucoup de domaines d\u0027application (ex. : veille d\u0027information), il est nécessaire à notre avis : 1) de partir d\u0027une base stable, c\u0027est-à-dire d\u0027une classification :\n-indépendante de l\u0027ordre de présentation des données (exigence n°1), -indépendante des conditions initiales, que ce soit d\u0027un choix de « graines de classes » arbitraires ou dépendantes des données (exigence n°2), -impliquant un minimum de paramètres, un seul si possible, pour réduire l\u0027espace des choix et tendre vers un maximum de vérifiabilité et de reproductibilité (exigence n°3). 2) d\u0027ajouter aux contraintes d\u0027une bonne classification celle de l\u0027incrémentalité (exigence N°4), afin de saisir les évolutions au fil de l\u0027eau : rectifications de frontières entre classes, apparition de nouvelles classes, voire de « signaux faibles »... Pour nous, il y a incrémentalité véritable si le résultat de la classification est indépendant de l\u0027ordre des données présentées antérieurement (exigence N°5), tout en découlant des données antérieures, par un historique pouvant faire l\u0027objet d\u0027interprétations.\nNotre démarche a été de concevoir une méthode où la contrainte d\u0027incrémentalité participer d\u0027un tout cohérent, en vue d\u0027aboutir à tout instant à une classification qui ait du sens, et dont la différence de représentation par rapport à l\u0027instant précédent ne provient que des effets du temps, et non du mélange de ceux-ci avec la variabilité propre de l\u0027algorithme, à la différence des principales méthodes de classification non supervisée.\nEtat de l\u0027art\nLes méthodes hiérarchiques, divisives ou agglomératives, souvent conviviales et efficaces, satisfont à nos exigences 1 à 3 d\u0027unicité des résultats. Mais au regard de la qualité des partitions obtenues à un niveau donné de l\u0027arbre, un consensus existe pour leur préférer les méthodes à centres mobiles : un modèle hiérarchique de partitions emboîtées impose des déformations à une réalité ayant toutes chances de se rapprocher d\u0027une organisation en treillis de partitions (par ex. treillis de Galois, pour des descripteurs binaires).\nLes méthodes procédant par agrégation autour de centres mobiles, comme les K-means et leurs nombreuses variantes, font partie d\u0027une famille basée sur l\u0027optimisation d\u0027un indicateur numérique global de qualité de la partition, dans laquelle prennent place les méthodes utilisant la procédure EM (Expectation Maximization) -cf. Buntine (2002). Ce problème d\u0027optimisation étant NP-difficile, on ne sait que les faire converger vers un optimum local qui dépend de leur initialisation (par ex. positions initiales des centres choisies arbitrairement, ou en fonction des données), voire de l\u0027ordre des données. Ce qui les disqualifie vis-à-vis de notre exigence N°2 en théorie comme en pratique. Un bon nombre de variantes incré-mentales de ces méthodes ont été proposées, dont on trouvera une revue partielle dans Gaber et al. (2005). Beaucoup sont issues de l\u0027action DARPA « Topic Detection and Tracking » Mais toutes se concentrent sur l\u0027efficacité informatique pour traiter des flux de dizaines ou centaines de milliers de dépêches d\u0027agences, ou autres documents. C\u0027est aussi dans ce cadre d\u0027efficacité que Simovici et al. (2005) proposent un algorithme glouton pour optimiser un critère de qualité de partition original propre aux descriptions par variables nominales.\nA notre connaissance, seules les méthodes basées sur la densité satisfont à notre exigence d\u0027unicité des résultats, hors de portée des méthodes ci-dessus. Elles s\u0027appuient sur la notion de densité d\u0027un nuage de points, locale par définition : étant donné 1) un nuage de points multidimensionnel, 2) une définition de la densité en chaque point de cet espace, 3) la valeur du paramètre de localité de cette fonction densité (son « rayon »), le paysage de densité qui en découle est unique et parfaitement défini. L\u0027énumération de l\u0027ensemble de ses pics -ou éventuels plateaux -représente un optimum absolu ; ces pics balisent des noyaux homogènes de points ; dans les zones intermédiaires, on peut définir de diverses façons des zones d\u0027influence des noyaux. Trémolières (1994) a proposé un algorithme général, dit de percolation, indépendant de la définition de la densité et du type de données, pour délimiter rigoureusement les noyaux, les points-frontière ambivalents et les points atypiques. Il procède par baisse progressive du niveau de densité depuis le point le plus dense, et diffusion autour des noyaux qui apparaissent successivement. D\u0027autres travaux retrouvent le même principe de repérage des noyaux denses, le plus souvent avec une définition spécifique de la densité, et d\u0027extensions de diverses sortes à partir des noyaux : Moody (2001), Guénoche (2004), Batagelj (2002)... A noter que ces méthodes peuvent se traduire en termes de partitionnement de graphe : définir une densité implique d\u0027avoir fixé des relations de voisinage, donc un graphe. DBSCAN d \u0027Ester et al. (1996) utilise une définition de la densité au moyen de deux paramè-tres, dont l\u0027un fixe le seuil à partir duquel les noyaux sont constitués et étendus. Ertöz et al. (2003) utilisent la notion de K plus proches voisins, qui permet de définir un « rayon adaptatif » ; plusieurs possibilités existent alors pour définir une densité au sein de ce voisinage.\nIci aussi des méthodes incrémentales ont été proposées : ainsi une version incrémentale de DBSCAN d \u0027Ester et al. (1998), ou la méthode de Gao et al. (2005), pour des descripteurs quantitatifs (âge, revenus, …). Mais c\u0027est dans le domaine des protocoles auto-organisateurs de réseaux de communication radio dits « ad hoc » que le thème de l\u0027incrémentalité pour le partitionnement dynamique de graphes évolutifs a été principalement abordé -ex. : Mitton, Fleury (2003) -avec des objectifs d\u0027application assez différents des nôtres : les voisinages se modifient à chaque pas de temps, sans nécessairement comporter d\u0027entrées ou sorties du réseau, l\u0027optimalité est moins recherchée qu\u0027une stabilité relative de la composition des classes (strictes) et de l\u0027identité des chefs de classe (clusterheads). Le principe-clé est celui d\u0027un algorithme intégralement distribué, pour lequel la connaissance à l\u0027instant t par chaque unité à classer de ses voisins et des voisins de ses voisins (2-voisinage) est suffisante.\nAlgorithme incrémental GERMEN : modifications locales des voisinages, des densités et des classes\nNous avons publié et expérimenté sur des données documentaires [cf. Lelu, François (2003)] un algorithme de percolation modifié, aboutissant à extraire 1) les points isolés, 2) les noyaux stricts exclusifs d\u0027une classe, 3) les points ambivalents appartenant à plusieurs classes, tous ces points se projetant à des hauteurs diverses sur chaque axe de classe. Nous y renvoyons en ce qui concerne la transformation préalable des données.\nA la différence de cet algorithme, celui que nous présentons ici est incrémental. Dans cette optique, nous nous donnons l\u0027état d\u0027un graphe de similarité des K plus proches voisins à l\u0027instant t, valué et orienté, dont les noeuds ont été caractérisés par leur densité, ainsi que par leur « couleur », c\u0027est-à-dire par leur rattachement à un éventuel noeud « chef de classe », dont le numéro constitue l\u0027étiquette. L\u0027arrivée d\u0027un nouveau noeud va perturber localement cet état : un certain nombre de noeuds dans le voisinage direct ou indirect du nouvel arrivant vont voir leur densité changer -ce changement du paysage de densité induisant à son tour un réajustement des zones d\u0027influence des chefs de classe, voire des changements de chefs de classe, ou l\u0027apparition de nouvelles classes (cf. plus bas le pseudocode détaillé).\nPlusieurs règles d\u0027héritage du numéro du (ou des) chef(s) de classe sont possibles. Mais dans tous les cas nous avons affaire à la mise à jour des « couleurs » d\u0027un paysage de densité sous l\u0027effet 1) de l\u0027arrivée d\u0027un nouveau point (changement structurel), 2) d\u0027un changement localisé des densités (changement quantitatif). Si la mise à jour de la couleur d\u0027un point ne dépend que de la couleur de ses voisins « surplombants », de densité supérieure, alors à paysage de densité donné et à graphe de voisinage donné on obtiendra un coloriage unique : l\u0027attribution des classes sera elle aussi indépendante de l\u0027ordre des données On construit progressivement et en la mettant constamment à jour une structure de données comportant pour chaque noeud la liste de ses \u0026-et 2-voisins, sa densité, et son (ou ses) numéros de chef(s) de classe. A chaque arrivée d\u0027un vecteur (noeud) nouveau, on calcule les changements de densité induits dans son 2-voisinage (il ne peut pas y en avoir ailleurs du fait de notre définition de la densité comme somme des liens présents dans l\u0027ensemble de chaque 1-voisinage), puis les changements de chef(s) de classe induits. Pour ce faire, on met à jour et on parcourt itérativement la liste des noeuds susceptibles de changer de classe, compte tenu de la règle choisie pour l\u0027extension des classes. Au pire cette liste peut comporter tous les documents antérieurs, mais elle ne peut que se vider (en pratique elle se stabilise autour de Valeurs ex-aequo de similarité et densité : leur prise en compte est indispensable pour assurer l\u0027indépendance par rapport à l\u0027ordre des données. Les K plus proches voisins d\u0027un noeud peuvent donc être en nombre supérieur à K… En cas de voisins de même densité et dominant leurs voisinages, le numéro du noeud le plus ancien est attribué.\nIllustration du contenu des classes par leurs descripteurs saillants.\nNous définissons comme suit la contribution relative C i (k) du descripteur i à la classe k, (dont on nomme liens(k) l\u0027ensemble des liens internes) : si x it est le nombre d\u0027occurrences du descripteur i dans le document N°t, de somme x .t pour l\u0027ensemble de ces descripteurs, si c i (t,t\u0027) est la contribution pondérée du descripteur i au lien entre les documents t et t\u0027, de densités respectives d(t) et d(t\u0027), alors :\nComplexité informatique\nDans son implantation actuelle, l\u0027introduction incrémentale du n ème document coûte de l\u0027ordre de O(n) en temps de calcul, donc l\u0027algorithme complet est en O(n²). Des optimisations sont possibles dans le calcul des similarités et des K plus proches voisins (vecteurs-\nA. Lelu données très creux) ; mais c\u0027est surtout en l\u0027exécutant en mode distribué, ce à quoi il est adapté par construction, qu\u0027il pourrait être rendu peu dépendant de la taille des données.\nExemple d\u0027application, évaluation.\nCi-dessous un exemple de thème obtenu sur une base textuelle de test (193 résumés de la collection Gallimard-Jeunesse, vocabulaire de 888 mots retenus, K\u003d3 plus proches voisins), parmi 28 classes obtenues : à t\u003d193 ; environ 45% des documents appartiennent à des noyaux, 5% sont des isolés, et 50% des ambivalents. Pour évaluer la qualité des résultats, les corpus de test des campagnes TDT citées plus haut sont inadéquats pour deux raisons : 1) le nombre de documents va de 16 000 à plusieurs centaines de milliers, ce qui est pour l\u0027instant un ou deux ordres de grandeur au dessus des possibilités de la version actuelle, 2) ce qui est évalué n\u0027est pas « pur », c\u0027est une chaîne indexation + algorithmes utilisant cette indexation. C\u0027est pourquoi nous avons entrepris -cf. Lelu et al. (2005)  \nFIG. 1 -Exemple de thème extrait par GERMEN\nConclusion et perspectives\nNous avons présenté un algorithme de classification non supervisée répondant aux exigences d\u0027un suivi dynamique rigoureux d\u0027un flux de documents : il est à la fois optimal, au sens de la description exhaustive d\u0027un paysage de densité adaptative, et incrémental. Son efficacité actuelle est suffisante pour traiter un flux de plusieurs milliers de documents. Son passage à l\u0027échelle est possible en mode distribué, par construction.. Mais il reste surtout à explorer en grandeur réelle et évaluer les diverses applications et usages nouveaux possibles, avec les problèmes qui vont avec : efficacité informatique, ergonomie de représentation, et\n"
  },
  {
    "id": "1019",
    "text": "Introduction\nDans le cadre d\u0027un processus complet de fouille de textes (Kodratoff et al., 2003, Amrani et al., 2004a, nous nous sommes intéressés à l\u0027étiquetage morphosyntaxique des corpus de spécialité. L\u0027étiquetage morphosyntaxique consiste à affecter à chaque mot dans la phrase son étiquette morphosyntaxique, en prenant en considération le contexte et la morphologie de ce mot. L\u0027étiquette morphosyntaxique est composée de la catégorie syntaxique du mot (nom commun, nom propre, adjectif, etc.) et souvent comporte des informations morphologiques (genre, nombre, personne, etc.). Les outils informatiques nécessaires à l\u0027opération d\u0027étiquetage sont appelés « étiqueteurs ».\nUn problème se pose lorsque les étiquettes des mots sont ambiguës. Par exemple, le mot functions peut être un nom au pluriel (\u0027biological functions are…\u0027) ou bien un verbe au singulier (\u0027this gene functions as…\u0027). Le problème à résoudre est celui de trouver l\u0027étiquette correcte selon le contexte. La correction de ces ambiguïtés est une étape importante pour obtenir un corpus de spécialité « parfaitement » étiqueté. Pour lever ces ambiguïtés et donc diminuer le nombre de fautes d\u0027étiquetage, nous proposons une approche interactive et itéra-tive appelée Induction Progressive. Cette approche est une combinaison d\u0027apprentissage automatique, de règles rédigées par l\u0027expert et de corrections manuelles. L\u0027induction pro-gressive nous a permis d\u0027obtenir un corpus de biologie moléculaire « correctement » étique-té. Nous avons alors utilisé le corpus obtenu pour entraîner quatre étiqueteurs morphosyntaxiques supervisés, puis nous avons effectué une étude comparative.\nÉtiquetage morphosyntaxique 2.1 Les approches d\u0027étiquetage morphosyntaxique\nIl y a deux approches principales pour l\u0027étiquetage morphosyntaxique : l\u0027approche inductive et l\u0027approche linguistique.\nL\u0027approche inductive nécessite la disponibilité d\u0027un grand corpus annoté. L\u0027annotation de corpus est tout apport d\u0027information aux textes bruts. L\u0027information requise ici est l\u0027étiquette morphosyntaxique correcte de chaque mot. Parmi les étiqueteurs inductifs, nous pouvons citer : l\u0027étiqueteur à base de transformation (Brill, 1995), l\u0027étiqueteur à base de « Séparateurs à vaste marge (SVM)» (Giménez et Màrquez 2003) et les étiqueteurs probabilistes (Ratnaparkhi, 1996, Toutanova et al., 2003. Des étiqueteurs plus élaborés ont été développés comme les étiqueteurs basés sur la combinaison de plusieurs étiqueteurs individuels, permettant ainsi de pallier les déficiences de chacun des systèmes pris séparément (Brill et Wu 1998, Halteren et al., 2001. Les résultats publiés de ses étiqueteurs appliqués au corpus classique \u0027WSJ\u0027 sont de l\u0027ordre de 96-97%.\nL\u0027approche linguistique, quant à elle, consiste à coder manuellement les connaissances linguistiques sous forme de règles. Les règles acquises sont ensuite utilisées pour l\u0027étiquetage de nouveaux textes. L\u0027un des travaux les plus important de cette approche est le développe-ment d\u0027une grammaire de contraintes (Karlsson et al., 1995) et son application à l\u0027étiquetage morphosyntaxique (Voutilainen,95). Cet étiqueteur peut être considéré comme le meilleur étiqueteur existant. En effet, il atteint une précision supérieure à 99% d\u0027étiquettes correctes.\nAfin de bénéficier des avantages des deux approches, plusieurs chercheurs ont combiné les étiqueteurs inductifs et les règles linguistiques (Tapanainen et Voutilainen 1994, Samuelson et Voutilainen 1997. Il existe d\u0027autres systèmes d\u0027étiquetage qui utilisent de petits corpus annotés pour accélérer l\u0027annotation d\u0027un corpus plus grand ; ces systèmes combinent l\u0027utilisation d\u0027un étiqueteur appris sur un petit corpus et l\u0027intervention d\u0027un humain via une interface interactive. Nous pouvons citer par exemple, les systèmes ANNOTATE (Plaehn et al., 2000) et KCAT (Won-Ho et al., 2000) qui sont basés sur des étiqueteurs statistiques.\nProblématique de l\u0027étiquetage morphosyntaxique\nQuelque soit le système sur lequel ils sont basés, les étiqueteurs actuels atteignent des performances très satisfaisantes mais il est difficile de dépasser la précision de 96-97%. Plusieurs chercheurs justifient cette difficulté par les incohérences dans le corpus d\u0027apprentissage (Ratnaparkhi, 1996, Toutanova et al., 2003. Les corpus sont annotés manuellement, ils peuvent donc contenir des erreurs. Par conséquent, l\u0027amélioration de la qualité des corpus et la correction des erreurs ont une importance capitale.\nDe plus, les bons résultats des étiqueteurs supervisés s\u0027expliquent par le fait que les travaux en question se situent dans le domaine de l\u0027apprentissage supervisé où le corpus de test est de nature similaire au corpus d\u0027apprentissage. Un véritable problème apparaît lorsque nous voulons traiter des corpus de spécialité pour lesquels nous n\u0027avons pas de corpus anno-tés. L\u0027acquisition d\u0027un tel corpus est coûteuse et elle constitue le goulet d\u0027étranglement pour construire un étiqueteur pour une nouvelle application ou un nouveau domaine.\nLa plupart des étiqueteurs utilisent des informations de nature essentiellement locale (une séquence de deux ou trois mots consécutifs). Par conséquent, ces étiqueteurs butent sur les ambiguïtés qui demandent la prise en considération d\u0027un contexte large. Par exemple : l\u0027ambiguïté relatif/conjonction pour que. Bien que l\u0027approche linguistique engendre des modèles de très bonne qualité et traite efficacement les ambiguïtés difficiles, elle est coûteuse et laborieuse. Par exemple, le développement de l\u0027étiqueteur ENGCG (Voutilainen, 95) a nécessité plusieurs années. Cependant, comme pour les étiqueteurs supervisés, les performances des étiqueteurs linguistiques se détériorent lorsqu\u0027ils sont appliqués à de nouveaux corpus.\nInduction progressive\nLa correction des ambiguïtés difficiles est une étape importante pour obtenir un corpus de spécialité parfaitement étiqueté. Pour corriger ces ambiguïtés et diminuer le nombre de fautes d\u0027étiquetage, nous utilisons une approche itérative appelée Induction Progressive. L\u0027induction progressive est une combinaison d\u0027apprentissage automatique, de règles rédigées par l\u0027expert et de corrections manuelles qui se combinent itérativement afin d\u0027obtenir une amélioration de l\u0027étiquetage tout en restreignant les actions de l\u0027expert à la résolution de problèmes de plus en plus délicats. Le principe de l\u0027induction progressive est le suivant : en utilisant le langage CorTag (détaillé dans la section suivante), l\u0027expert écrit une règle (ou plusieurs règles) pour corriger une ambiguïté spécifique. Les règles de l\u0027expert sont ensuite appliquées au corpus Corp 0 et engendrent un corpus CorpRegExp 0 . Un algorithme d\u0027apprentissage de règles est ensuite utilisé pour apprendre la modification engendrée par les règles de l\u0027expert. Les règles apprises sont aussi appliquées au corpus Corp 0 et engendrent un corpus CorpRegInd 0 . En utilisant une nouvelle version du logiciel interactif ETIQ (Amrani et al., 2004b, Amrani et al., 2005a, les différences entre les deux corpus (CorpRegExp 0 et CorpRegInd 0 ) sont alors visualisées pour faciliter leur analyse. Les points de désaccords sont souvent des cas particulièrement difficiles à étiqueter. Leur visualisation permet de :\n-détecter les erreurs produites par les règles de l\u0027expert.\n-mettre à jour les règles de l\u0027expert si elles se trompent. Pour ce faire, les règles induites ayant trouvé l\u0027étiquette correcte sont une indication précieuse.\n-confirmer ou corriger les étiquettes obtenues par les règles de l\u0027expert, ainsi nous obtenons une base d\u0027étiquettes sûres. Cette base servira pour améliorer progressivement la qualité des règles induites. Le langage offre la possibilité d\u0027écrire des règles complexes en permettant à l\u0027expert de manipuler des éléments dont la position peut être inconnue lors de l\u0027écriture de la règle mais qui seront instanciés lors de l\u0027application de celle-ci. Par exemple, le langage permet d\u0027écrire la règle relationnelle suivante : Si le premier verbe avant le mot that appartient à une liste donnée alors le mot that est étiqueté IN (conjonction de subordination) sauf s\u0027il y a un non commun (singulier ou pluriel) entre le mot that et ce verbe.\nLe langage d\u0027étiquetage : CorTag\nLe langage dispose aussi d\u0027une bibliothèque de fonctions intégrées qui permettent à l\u0027expert d\u0027exprimer des contraintes sur les mots, les étiquettes, les positions et la phrase.\nInduction Progressive\nL\u0027étape 0 consiste à obtenir un corpus (Corp 0 ) de spécialité étiqueté par un étiqueteur morphosyntaxique généraliste. Pour ce faire, nous utilisons l\u0027étiqueteur de Brill (Brill, 1995). Puis, avec l\u0027aide de ETIQ (Amrani et al., 2004b, Amrani et al., 2005b, l\u0027expert adapte les règles morphologiques et contextuelles au domaine étudié. Nous avons constaté que les erreurs dues aux mots inconnus de la spécialité sont facilement et efficacement résolues par les règles morphologiques d\u0027ETIQ. Cependant, les confusions contextuelles qui nécessitent des règles relationnelles sont difficilement résolues. Pour les résoudre, nous utilisons l\u0027induction progressive.\nL\u0027expert identifie alors des erreurs qui lui paraissent importantes, et rédige des règles de correction pour chaque ambiguïté. Ces règles peuvent être rédigées au sein de ETIQ, ou bien, s\u0027il s\u0027agit de règles fortement contextuelles, en utilisant un langage de programmation dédié à la rédaction de ces règles (le langage CorTag). Chaque règle s\u0027applique à un contexte pré-cis et sert à corriger une erreur spécifique. L\u0027expert produit ainsi un certain nombre de règles, qu\u0027il applique au corpus Corp 0 et il obtient ainsi le corpus CorpRegExp 0 . Un problème quasi insurmontable se présente lorsque l\u0027expert travaille sur de gros corpus : le nombre d\u0027application des règles peut atteindre plusieurs milliers, et l\u0027expert ne peut alors vérifier la validité de ses règles que sur un sous-corpus du corpus initial.\nUne fois que l\u0027expert estime qu\u0027il a franchi une étape et qu\u0027il pense avoir résolu un problème à peu près correctement, il fait alors appel à un algorithme d\u0027apprentissage automatique de règles. Cet algorithme sert à apprendre les modifications produites par les règles de l\u0027expert pour résoudre une erreur spécifique. Bien entendu, l\u0027apprentissage correspondant peut se faire sans problème sur le corpus complet. Le logiciel ETIQ permet d\u0027apprendre ces modifications en comparant deux versions du même corpus :\n-Le corpus Corp 0 avant l\u0027application des règles de l\u0027expert.  Exemple. Dans la phrase \"…that rad59 Delta exhibits synergistic effects...\", le mot exhibits est étiqueté incorrectement comme NNS par les règles induites et il est étiqueté correctement comme VBZ par les règles de l\u0027expert (le 1 er exemple du tableau 1). Cependant, dans la phrase \"…gene responsible for adrenal hypoplasia congenita and blocks steroid biosynthesis by…\", le mot blocks est étiqueté incorrectement comme NNS par les règles de l\u0027expert et étiqueté correctement comme VBZ par les règles induites (le 5 ème exemple du tableau 1).\nAinsi nous disposons de trois versions successives du même corpus : CorpRegExp 0 , CorpRegInd 0 et CorpSûr 0 .\n-CorpRegExp 0 est le corpus de départ sur lequel nous appliquons les règles de l\u0027expert.\n-CorpRegInd 0 est le corpus de départ sur lequel nous appliquerons les règles induites.\n-CorpSûr 0 est le corpus dans lequel nous gardons les étiquettes corrigées manuellement (si l\u0027induction \u0027gagne\u0027 comme pour les exemples 5, 6, 7 et 8 du tableau 1) ou confirmées par l\u0027expert (si c\u0027est lui qui a \u0027gagné\u0027 comme pour les exemples 1, 2, 3 et 4 du tableau 1) durant le processus. Encore une fois, si le corpus est suffisamment petit, l\u0027expert peut corriger toutes les erreurs et il n\u0027est pas vraiment nécessaire d\u0027itérer ce processus. Comme nous partons du principe que le corpus est volumineux, l\u0027expert ne peut pas examiner les milliers de cas où on peut voir une différence entre ses règles et les règles induites. Par contre, il peut noter certains des cas où il a \u0027perdu\u0027 par rapport à l\u0027induction et afficher dans ETIQ la règle induite qui a \u0027gagné\u0027 sur lui (voir exemple ci-dessous). Il analyse ces règles et reçoit ainsi une indication sur la façon d\u0027améliorer ses propres règles pour ne plus faire les erreurs qu\u0027il a corrigées à la main. Il applique alors ces nouvelles règles à CorpSûr 0 qui devient le corpus de dé-part de l\u0027itération suivante, pour engendrer CorpRegExp 1 en lui appliquant les nouvelles règles déduites au cours de l\u0027itération « 1 ».\nExemple. Nous avons expliqué dans le tableau 1 que l\u0027expert peut modifier \u0027à la main\u0027 ses erreurs. Il peut aussi apprendre de nouvelles règles qui lui sont suggérées par le programme d\u0027induction de règles. Illustrons ce procédé sur le 7 ème exemple du tableau 1, relatif à la confusion NNS-VBZ. En examinant Corp 0 (le corpus étiqueté par un étiqueteur généraliste), l\u0027expert avait écrit, en utilisant le langage CorTag (voir Section 3.1), quelques règles pour améliorer l\u0027étiquetage. En appliquant ces règles au corpus Corp 0 , nous obtenons le corpus CorpRegExp 0 . En utilisant ETIQ, nous collectons les différences relatives à la confusion NNS-VBZ entre Corp 0 et CorpRegExp 0 . Puis, en nous basant sur ces différences, nous apprenons des règles en utilisant l\u0027algorithme d\u0027apprentissage de règles RIPPER (Cohen 1995). Pour cette confusion, l\u0027algorithme a engendré 50 règles. L\u0027attention de l\u0027expert est attirée par son erreur sur l\u0027exemple 7. Il peut alors cliquer sur le mot incorrectement étiqueté (displays), et faire ainsi apparaître la règle de RIPPER qui a trouvé, elle, la bonne étiquette. Dans ce cas, la règle qui s\u0027affiche est : « SI le mot courant est étiqueté VBZ ou NNS ET SI il est suivi par un déterminant (DT) ALORS attribuer l\u0027étiquette VBZ au mot courant » (c\u0027est-à-dire qu\u0027un verbe est souvent suivi d\u0027un déterminant). Sur le corpus de départ Corp 0 (c\u0027est-à-dire avant l\u0027application des règles induites), cette règle s\u0027applique correctement 630 fois et engendre 20 erreurs. Après avoir affiché les 630 phrases où un VBZ est suivi d\u0027un DT, nous avons constaté que ces 630 étiquettes sont correctes. Cette règle doit donc contenir une certaine vérité linguistique. Nous avons ensuite affiché les 20 phrases où un mot étiqueté NNS est suivi par un déterminant (DT). Nous avons observé quatre cas : -cas 1 : Le déterminant (DT) est both ou each. Par conséquent, la règle était un peu trop générale et devrait tenir compte de ces deux exceptions (both et each). En fait, mis face à ses erreurs, l\u0027expert a plutôt tendance à les corriger et à obtenir des règles plus efficaces. Le danger serait alors que le système induise des règles faisant les mêmes erreurs qu\u0027à l\u0027itération précédente, et que l\u0027expert revoie ces mêmes erreurs à chaque itéra-tion. Nous n\u0027avons jamais constaté ce comportement de la part de notre système inductif qui apprend des règles très différentes quand les ensembles d\u0027apprentissage sont différents. On notera au passage l\u0027importance des corrections manuelles de l\u0027expert, même si elles sont relativement peu nombreuses, afin de partir d\u0027étiquetage vraiment différent.\nEn conséquence, et en pratique, l\u0027expert constate à chaque itération que le nombre de fois où il est mis en défaut par le système inductif diminue, au point qu\u0027après quelques itérations il peut examiner toutes les différences et, éventuellement corriger toutes ses erreurs manuellement, sans recourir à des règles. Le corpus final, CorpSûr p est alors parfaitement étiqueté du point de vue de l\u0027erreur d\u0027étiquetage considérée au départ. Il est évidemment nécessaire de répéter le processus entier pour chacune des confusions que l\u0027on désire corriger.\nLes types de confusions traités par l\u0027induction progressive\nLes fautes les plus importantes sont celles qui faussent la compréhension de la phrase en détruisant la structure syntaxique. Les confusions les plus importantes que nous ayons résolu avec l\u0027induction progressive sont les suivantes :\n-Les erreurs d\u0027étiquetage du mot « that » qui peut être étiqueté comme un déterminant « DT », une conjonction de subordination « IN », un pronom relatif « WDT », un pronom « PRP » (e.g. I do not like that.) ou un prédéterminant « PDT ».\n-Les confusions entre les noms et les verbes, notamment la confusion entre un nom commun au pluriel « NNS » et un verbe au présent, à la troisième personne du singulier « VBZ ». Par exemple : le mot « functions ».\n-La confusion qui concerne un mot qui se termine par « ed » qui peut être un verbe au participe passé « VBN », un verbe au passé « VBD» et adjectif « JJ ».\n-La confusion qui concerne un mot se terminant par « ing » qui peut être principalement un nom au singulier « NN », un verbe au gérondif « VBG » ou un adjectif « JJ ». Ainsi, le processus doit être répété 3 ou 4 fois entièrement, ce qui n\u0027est pas exagéré au vu des avantages qu\u0027il y a à posséder un corpus spécialisé « correctement » étiqueté.\nNous avons appliqué notre approche à corpus de biologie moléculaire composé de 600 résumés d\u0027intérêt parmi un corpus initial (de 6119 résumés) obtenu par requête sur Medline (http://www.ncbi.nlm.nih.gov) avec les mots-clés DNA-binding, proteins, yeast. Ce qui nous a permis d\u0027obtenir un corpus « correctement » étiqueté.\nDe plus, à la fin du processus nous obtenons un ensemble de règles relationnelles intelligibles. Ces règles pourraient être utilisées pour résoudre les ambiguïtés qui ne sont pas bien traités par les étiqueteurs supervisés.\nComparaison des étiqueteurs supervisés\nDans la mesure où il existe déjà d\u0027excellents étiqueteurs avec comme défaut, que nous l\u0027avons déjà évoqué, de ne pas être capable de s\u0027adapter à de nouveaux corpus. La tâche fondamentale d\u0027un chercheur abordant un nouveau domaine consiste à créer un corpus correctement étiqueté. C\u0027est cette tâche que nous venons d\u0027illustrer. Maintenant, il est tout de même intéressant de comparer les performances des étiqueteurs existants. En utilisant le corpus « correctement » étiqueté obtenu, nous avons comparé les étiqueteurs suivants :\n1-L\u0027étiqueteur à base de transformation (Brill 1994). 2-L\u0027étiqueteur à base de SVM (Giménez et Màrquez 2003). Pour entraîner cet étiqueteur nous avons utilisé la boite à outils SVMTool (Giménez et Màrquez 2004). 3-L\u0027étiqueteur probabiliste de Stanford (Toutanova et al. 2003). Cet étiqueteur atteint une précision de 97,22% d\u0027étiquettes correctes sur le corpus WSJ. 4-L\u0027étiqueteur à base d\u0027Entropie Maximale (Ratnaparkhi 1996 \nLes critères d\u0027évaluation\nPour comparer les étiqueteurs nous avons utilisé les mesures de précisions et de rappel pour chaque étiquette. La précision est définie par la formule suivante : Précision \u003d nombre d\u0027exemples positifs couvets / nombre d\u0027exemples couvets. Le rappel est défini par la formule suivante : Rappel \u003d nombre d\u0027exemples positifs couverts / nombre d\u0027exemples positifs. Il est important de déterminer un compromis entre le rappel et la précision. Pour ce faire, nous utilisons la mesure du F score (avec ?\u003d1) : \nRésultat de la comparaison\nPour comparer les quatre étiqueteurs, nous les avons entraînés sur le même sous-corpus. Le sous-corpus d\u0027apprentissage a été constitué à partir de deux tiers de notre corpus (87965 mots) de biologie moléculaire. Pour l\u0027évaluation, nous avons utilisé le tiers restant du corpus de biologie moléculaire. Le corpus entier comporte 131 346 mots. Le tableau 3 présente les F score obtenus pour chaque étiquette par les différents étiqueteurs.\n-254 -RNTI-E-6 A partir de cette expérience, nous remarquons que les performances réalisées par les éti-queteurs entraînés sur un corpus de la même spécialité sont significativement meilleures que celles obtenues avec des étiqueteurs généralistes. De plus, nous pouvons constater qu\u0027un petit corpus d\u0027apprentissage de la même spécialité est plus bénéfique qu\u0027un grand corpus généraliste de plus grande taille. En effet, la taille du corpus d\u0027apprentissage de biologie moléculaire (87965 mots) est inférieure au dixième du corpus du WSJ (1 million de mots).\nComme pour le corpus du WSJ, nous constatons que pour le corpus de biologie molécu-laire les deux meilleurs étiqueteurs sont l\u0027étiqueteur de Stanford et l\u0027étiqueteur à base de SVM, avec un léger avantage pour l\u0027étiqueteur de Stanford.\nBien que les étiqueteurs que nous avons utilisés sont parmi les meilleurs étiqueteurs existants et leur performance sont globalement très satisfaisantes, nous constatons que les F score de certaines étiquettes sont encore assez faibles. Par exemple : les meilleurs F score obtenus pour les étiquettes VBN, VBG, VBD et VB sont respectivement de 92.75, 91.87, 92.35 et 92.63. En effet, même les étiqueteurs les plus performants sont incapables de résoudre ces ambiguïtés difficiles. Par contre, les règles relationnelles écrites par l\u0027expert sont très efficaces pour corriger ces ambiguïtés.\nVoici des exemples de règles relationnelles (CorTag) relatives à la correction des erreurs des VBD et des VBN:\n-si (?1,been,) (*1,,RB) (0,@BEENedVBP,) alors (0,,VBD) « si le mot been est suivi par une suite d\u0027adverbes et cette dernière est suivie par un mot (position 0) appartenant au groupe BEENedVBP alors ce dernier mot est étiqueté VBD » -si (?1,are,) (*1,,RB) (0,@AREedVBP,) alors (0,,VBN) « s\u0027il existe un \u0027are\u0027 suivi par un certain nombre (éventuellement 0 mots) d\u0027adverbes (RB) et un mot appartenant à la classe AREedVBP suit les adverbes alors ce dernier mot est étiqueté comme VBN » D\u0027après ces exemples, nous constatons que la correction des ambiguïtés difficiles néces-site souvent des règles relationnelles. Les prémisses de ces règles sont fondées sur des contraintes pourtant sur les mots contextuels. Pour que ces règles soient efficaces, il est donc nécessaire que les mots du contexte soient bien étiquetés.\nD\u0027autre part, les précisions des étiqueteurs entraînés et évalués sur le corpus de biologie moléculaire sont meilleures que celles des étiqueteurs entraînés et évalués sur le corpus géné-ral du WSJ. Les étiqueteurs se comportent donc mieux lorsqu\u0027il s\u0027agit d\u0027un corpus très spéci-fique. En effet, les corpus spécifiques sont plus homogènes et contiennent plus de régularités que les corpus généralistes (le WSJ corpus ou le Brown corpus).\nLe tableau 4 présente les précisions obtenues par les différents étiqueteurs supervisés sur les corpus du WSJ et de biologie moléculaire : \n"
  },
  {
    "id": "1020",
    "text": "1\n*Institut National des Télécommunications, 9 rue Charles Fourier, 91011 Evry Cedex **SNCF, Dir. de l\u0027Innovation et de la Recherche, 45 rue de Londres, 75379 Paris Cedex 08 {anne.remillieux, christian.blatter}@sncf.fr\nLa SNCF souhaite mettre à la disposition de ses personnels un outil qui leur permette de partager et de développer leurs connaissances et expériences en matière de conduite du changement, c\u0027est-à-dire de prise en compte des facteurs humains pour la réussite d\u0027un projet.\n. Ces connaissances sont, pour la plupart des acteurs, empiriques, particulièrement ancrées dans leur action et donc tacites. Comment recueillir puis formaliser ce type de connaissances en vue de leur partage ?\nNous utilisons deux types de techniques de recueil des connaissances tacites : celle de l\u0027observation de situations de travail, que nous avons mise en oeuvre en assistant aux échan-ges entre acteurs d\u0027une équipe projet dans l\u0027entreprise ; et celle de l\u0027entretien d\u0027explicitation, diffusée en France par Vermersch (1994), dont nous expérimenterons prochainement l\u0027apport pour une problématique de gestion des connaissances. Deux catégories de résultats sont issues de la première observation :\nB a s e d e c o n n a is s a n c e s C o n n a is s a n c e s d i a c h r o n i q u e s C o n n a i s s a n c e s s y n c h r o n i q u e s R e s s o u r c e s S u r l\u0027 a c t io n S u r l e m o n d e S u r l \u0027 a c t i o n S u r l e m o n d e E s t u ti lis é p a r A p o u r c o n te x t e S o u lè v e o u r é s o u t P r o b l è m e ( s ) C h a n g e m e n t ( s ) A c te u r ( s ) P a r a m è t r e s\nFIG.1 -Orientations générales pour la spécification structurelle\n-La première identifie les différents types de connaissances utilisées par les acteurs observés et fournit ainsi de premières orientations concernant la structure du futur outil. Parmi les catégories apparues, nous noterons celle des connaissances sur l\u0027action (fournir aux destinataires une illustration du futur changement le plus tôt possible), opposée à celle des connaissances sur le monde, qui portent sur l\u0027environnement au sein duquel les sujets agissent (Les destinataires du changement redoutent surtout la première mise en main du changement). Par ailleurs, les connaissances « diachroniques » descriptibles sous la forme d\u0027une succession d\u0027étapes dans le temps (la description des étapes à suivre pour concevoir un document de communication «concret») se sont distinguées de celles, « synchroniques », qui La conduite du changement à la SNCF s\u0027énoncent indépendamment d\u0027un facteur temporel (la communication auprès des agents doit être concrète).\n-La seconde catégorie de résultats décrit le « processus de co-construction des choix » de conduite du changement, c\u0027est-à-dire les 7 phases génériques que mènent collectivement les acteurs observés pour aboutir à la mise en place de solutions de conduite du changement. La description de ces phases, mais aussi des acteurs, des connaissances requises et des ressources (non décrites ici) auxquels elles sont associées, permet d\u0027orienter la spécification fonctionnelle de l\u0027outil.\nFIG.2 -Le processus de co-construction des choix de conduite du changement à la SNCF\nPour conclure, précisons que les résultats présentés, destinés à fournir un cadre générique pour notre travail de formalisation, ne rendent pas compte de la seule spécificité de la conduite du changement à la SNCF. En ce sens, ils pourraient vraisemblablement convenir à d\u0027autres types d\u0027activité (comme la conduite de projet par exemple). La particularité des connaissances qui nous intéressent apparaîtra au moment d\u0027insérer des contenus dans le cadre défini.\nRéférences Nonaka, I. (1994), A dynamic theory of organizational knowledge creation, Organization Science Vol. 5, n°1, 14-37. \nSummary\nOur research deals with elicitation, formalization and sharing of tacit knowledge about change management at the SNCF. The observation of the work of a project team in the company enabled us to make first assumptions about this knowledge.\n"
  },
  {
    "id": "1022",
    "text": "Introduction\nLa conception, la réalisation et la maintenance d\u0027un site web volumineux sont des tâches difficiles, en particulier quand le site est écrit par plusieurs rédacteurs. Pour améliorer le site, il est alors important d\u0027analyser les comportements de ses utilisateurs, afin de découvrir notamment les incohérences entre sa structure a priori et les schémas d\u0027utilisation dominants. Les utilisateurs contournent en effet souvent les limitations du site en navigant (parfois laborieusement) entre les parties qui les intéressent, alors que celles-ci ne sont pas directement liées aux yeux des concepteurs. A l\u0027opposée, certains liens sont très peu utilisés et ne font qu\u0027encombrer la structure hyper textuelle du site.\nUne méthode d\u0027analyse dirigée par l\u0027usage consiste à réaliser une classification du contenu du site à partir des navigations enregistrées dans les logs du serveur. Les classes ainsi obtenues sont constituées de pages qui ont tendance à être visitées ensembles. Elles traduisent donc les préférences des utilisateurs. La principale difficulté de cette approche réside dans la nature des observations (les navigations). Comme celles-ci sont de taille variable, on peut en déduire de nombreuses mesures de dissimilarité entre les pages visitées, selon qu\u0027on tient compte de la durée de la visite, du nombre de fois que la page est vue, etc. Dans le contexte de la classification, il est alors difficile de choisir a priori quelle mesure de dissimilarité est la plus adaptée à l\u0027analyse du site.\nDans cet article, nous étudions un site web peu volumineux (91 pages), très bien structuré, et au contenu sémantique bien défini. Grâce à cet exemple de référence, nous comparons différentes dissimilarités afin de mesurer leur aptitude à révéler ce contenu sémantique.\nDonnées d\u0027usage\nPréparation des données\nLes données d\u0027usage d\u0027un site Web proviennent essentiellement des fichiers log des serveurs concernés. Chaque ligne du fichier log décrit une requête reçue par le serveur associé : elle indique ainsi le document demandé, la provenance de la requête, la date de la demande, etc. Diverses techniques de pré-traitement permettent d\u0027extraire des logs des navigations, comme par exemple celles de Tanasa et Trousse (2004), utilisées dans le présent article. Une navigation est une suite de requêtes provenant d\u0027un même utilisateur et séparées au plus de 30 minutes. Elle constitue donc la trajectoire d\u0027un utilisateur sur le site. Nous nous contenterons dans cet article de considérer chaque navigation comme la liste ordonnée des pages demandées par un utilisateur, sans chercher à tenir compte du temps passé sur chaque page.\nAnalyse du contenu du site\nL\u0027un des buts de l\u0027analyse de l\u0027usage est d\u0027améliorer le site web étudié. Pour ce faire, il est important de déterminer si les a priori sémantiques des concepteurs sont validés par les usages. On cherche en particulier à savoir si les catégorisations choisies a priori sont perçues comme telles par les utilisateurs. Pour ce faire, on construit à partir des données d\u0027usage une mesure de (dis)similarités entre les pages du site. Les pages qui sont souvent ensembles dans les navigations seront ainsi considérées comme proches. En classant le contenu du site selon une dissimilarité induite par l\u0027usage, on détermine des groupes de pages liées qu\u0027on peut confronter aux hypothèses des concepteurs du site.\nToute la difficulté réside dans la nature des données. On peut en effet décrire chaque page par l\u0027intermédiaire des navigations, sous forme de données complexes, en considérant les pages comme des individus et les navigations comme des variables. Une page p i est ainsi décrite par les variables n 1 , . . . , n N (pour N navigations). La valeur de la variable n k est l\u0027ensemble des positions de la page p i dans la navigation n k . Si un site contient quatre pages, A, B, C et D, et est visité par la navigation n 1 \u003d (A, B, A, C, D), la variable n 1 vaut {1, 3} pour la page A, {2} pour la page B, {3} pour la page C, et {4} pour la page D.\nCes données sont difficiles à analyser car les variables n\u0027ont pas des valeurs numériques mais plutôt ensemblistes. De plus, le nombre de colonnes du tableau peut être très élevé. En outre, si le nombre de lignes est élevé (site volumineux), le tableau est alors en général très creux (i.e., contient beaucoup d\u0027ensembles vides).\nDissimilarités\nDe nombreuses solutions sont envisageables pour traiter ce type de tableau de données. La plus simple consiste à binariser les valeurs des variables, en remplaçant un ensemble vide par la valeur 0 et un ensemble non vide par un 1. De nombreux indices de (dis)similarités ont été définis pour de telles données binaires (cf e.g. Gower et Legendre (1986)). Nous retenons la dissimilarité basée sur l\u0027indice de Jaccard, définie comme suit\n-410 -RNTI-E-6 F. Rossi et al.\noù n ik vaut 1 si et seulement si la page p i est visitée par la navigation k et où |U | désigne le cardinal de l\u0027ensemble U . Pour cette dissimilarité, deux pages sont proches dès que la plupart des navigations qui passent par l\u0027une passent par l\u0027autre.\nLe défaut principal de la dissimilarité de Jaccard est qu\u0027elle ne tient pas compte du nombre de passages par une page. Pour palier ce problème, on remplace le tableau binaire n ik par un tableau d\u0027entiers positifs m ik : la valeur de m ik est le nombre de passages de la navigation k par la page i. On utilise ensuite une des nombreuses dissimilarités adaptées à ce type de données, comme par exemple la dissimilarité \"cosinus\" définie par\noù N désigne le nombre total de navigations. Une autre dissimilarité intéressante est donnée par le modèle tf×idf, dans lequel on pondère une navigation en tenant compte à la fois du nombre de passages dans une page donnée, mais aussi de la longueur de la navigation : une navigation longue passe par beaucoup de pages et la similarité sémantique entre les pages n\u0027est donc pas garantie, contrairement au cas des navigations courtes. La dissimilarité est donnée par\nk\u003d1 N l\u003d1 m 2 il log P Pl où P désigne le nombre de pages et P k le nombre pages distinctes par lesquelles la navigation k est passée (cf par exemple Chen (1998)).\nSite de référence\nPrésentation\nPour comparer les similarités retenues, nous utilisons le site du CIn, le laboratoire de deux d\u0027entre nous. Ce site est réalisé par l\u0027intermédiaire d\u0027un ensemble de servlets programmées en Java. Les URL des pages sont assez complexes (car elles correspondent à l\u0027appel des servlets) et il est extrêmement peu probable qu\u0027un utilisateur accède directement à une page en saisissant l\u0027URL (une URL complète pour le site comprend une centaine de caractères). Nous supposons donc que les utilisateurs accèdent au site en entrant par la page principale, puis en suivant les liens proposés dans un menu contextuel situé à gauche de la page.\nLe site est petit (91 pages) et très bien organisé, sous la forme d\u0027un arbre de hauteur 5. L\u0027information est essentiellement située dans les feuilles de l\u0027arbre (75 pages) alors que les noeuds internes jouent le rôle de pivots. Le site est très dense en liens, car le menu contextuel comporte toujours la page principale, les 10 pages de premier niveau, ainsi que les parents et les soeurs de la page courante. On dénombre ainsi parfois plus de 20 liens dans le menu lui-même, ce qui ne facilite pas toujours la navigation.\nNous -411 -\nRNTI-E-6\nSémantique de référence\nNous avons étudié le contenu du site et construit manuellement une partition des pages en 13 classes, allant des pages recensant les publications du CIn à celles destinées à l\u0027inscription des étudiants en mastère. Pour comparer les dissimilarités, nous utilisons un algorithme de classification et nous comparons les classes obtenues aux classes a priori. \nClassifications\nAlgorithmes et critères\nPour comparer les dissimilarités présentées dans la section 2.3, nous produisons des classes homogènes de pages, puis nous comparons ces classes à celles issues de l\u0027analyse experte du site de référence. Pour la classification, nous utilisons un algorithme de type nuées dynamiques applicable à un tableau de dissimilarités (cf Celeux et al. (1989)) et une classification hiérar-chique basée sur le lien moyen. D\u0027autres algorithmes sont bien entendu envisageables.\nPour analyser les résultats, nous utilisons deux critères. Pour une analyse classe par classe, nous étudions la F mesure de van Rijsbergen (1979) associée à chaque classe a priori : il s\u0027agit de retrouver au mieux une classe experte dans l\u0027ensemble de classes produites par un algorithme. Pour une analyse globale, nous utilisons l\u0027indice de Rand corrigé (cf Hubert et Arabie (1985)) qui permet de comparer deux partitions. Pour les deux indices, une valeur de 0 correspond à une absence totale de correspondance entre la structure a priori et la structure obtenue, alors qu\u0027une valeur de 1 indique une correspondance parfaite.\nNuées dynamiques\nL\u0027algorithme des nuées dynamiques demande de choisir a priori un nombre de classes. Pour limiter les effets de ce choix, nous étudions les partitions produites pour un nombre de classes allant de 2 à 20. Nous obtenons les résultats suivants : Pour l\u0027analyse globale (indice de Rand corrigé), nous indiquons la taille de la partition maximisant le critère. On constate que tf×idf et Jaccard donnent des résultats assez proches (léger avantage pour la première) alors que cosinus obtient des résultats sensiblement moins bons. Pour l\u0027analyse fine, nous cherchons pour chaque classe a priori une classe correspondante (au sens de la F mesure) dans l\u0027ensemble des classes produites en faisant varier la taille de la partition, toujours de 2 à 20. Nous indiquons le nombre de classes parfaitement retrouvées et la plus mauvaise F mesure pour les classes non retrouvées. La mesure tf×idf apparaît comme la plus performante. Les classes retrouvées parfaitement par les autres dissimilarités le sont aussi par tf×idf (qui retrouve les classes 3, 4, 5, 7, 8, 9 et 12). On constate cependant que les classes parfaitement retrouvées le sont dans des partitions différentes, ce qui explique les indices de Rand relativement mauvais, comparativement aux résultats classe par classe.\nClassification hiérarchique\nNous reprenons l\u0027analyse conduite pour les nuées dynamiques dans le cas de la classification hiérarchique. Nous faisons varier ici le nombre de classes a posteriori, en étudiant tous les niveaux de coupure possible dans le dendrogramme. Nous obtenons les résultats suivants : Au niveau global, on constate une nette domination de Jaccard et une amélioration des résultats pour celle-ci. Le critère du lien moyen utilisé ici, ainsi que la structure hiérarchique, semble permettre une meilleure exploitation de la dissimilarité de Jaccard, alors que les résultats sont nettement dégradés pour les autres mesures. Les résultats classes par classes sont plus difficiles à analyser et semblent ne pas dépendre de la mesure. Cependant, les \"bonnes\" performances de tf×idf et de cosinus correspondent à une bonne approximation des classes pour des niveaux de coupure très différents dans le dendrogramme : il n\u0027est donc pas possible d\u0027obtenir avec ces mesures, une bonne récupération de l\u0027ensemble des classes, alors que Jaccard se comporte globalement mieux.\nDiscussion et conclusion\nLa dissimilarité de Jaccard apparaît globalement comme la plus performante pour retrouver la sémantique a priori du site de référence, à condition d\u0027être utilisée avec une classification hiérarchique. Tf×idf donne des résultats satisfaisants alors que cosinus semble incapable de retrouver les classes. Il est cependant important de confronter ces conclusions au contenu du site.\nIl s\u0027avère en effet que le site du CIn est un peu particulier à cause de sa très grande densité de liens, mais aussi en raison de la présence de pages de navigation, proches de \"tables de matières\". La classe 6 par exemple, contient 8 pages de description du laboratoire, alors que la classe 5 contient 6 pages présentant ses objectifs. Une des pages de la classe 5 est une description générale des objectifs, qui se contente pour l\u0027essentiel d\u0027orienter le lecteur vers les autres pages. En ce sens, elle pourrait faire partie de la classe 6. Cette page joue donc le rôle de pont entre la classe 6 et la classe 5. De plus la structure du site fait que le passage par cette page est obligatoire pour atteindre les autres pages de la classe 6. Ce problème est en fait assez généralisé dans le site, en raison de sa structure d\u0027arbre. Il a des conséquences sur les dissimilarités car il force une certaine proximité entre des pages qui n\u0027ont pas été placées dans une même classe a priori. La dissimilarité de Jaccard est particulièrement sensible à ce problème : elle a tendance, en particulier dans la classification hiérarchique, à regrouper par exemple les classes 5 et 6, excepté la page de présentation générale du CIn (qui est un pont entre la pagre principale du site et celles de la classe 6) et une page indique comme se rendre au CIn.\n"
  },
  {
    "id": "1023",
    "text": "Introduction et etat de l\u0027art\nLes mammographies sont le moyen le plus répandu pour la détection du cancer du sein. Des études ont démontré qu\u0027une lecture double des mammographies augmente la sensitivité du diagnostic jusqu\u0027à 15% (Bird et al., 1992) et de plus, (Destounis et al., 2004), que les outils d\u0027aide au diagnostic automatique du cancer du sein (ADACS) peuvent améliorer même les ré-sultats d\u0027une double lecture des mammographies. Pourtant, les techniques existantes d\u0027ADACS ont une série d\u0027inconvénients.\nLes méthodes existantes d\u0027ADACS peuvent être classées dans deux classes : celles qui essayent d\u0027identifier des signes de cancer (Pluim et al., 2003) et celles qui essayent une classification des mammographies (Zaiane et al., 2002). Dans le premier cas, les inconvénients principals sont le coût, dû au traitement d\u0027images et à la classification des signes trouvés et le fait que les signes des phases de début de cancer sont plus subtiles que ceux recherchés par ces méthodes. Dans le deuxième cas, l\u0027inconvénient principal est le taux de réussite plus faible (qui baisse parfois jusqu\u0027à 56,25%) et sa forte variation selon le jeu de données considéré pour la validation.\nContribution\nPar rapport aux autres techniques existantes dans le domaine d\u0027ADACS, notre approche est basée sur les techniques des médecins et se propose d\u0027utiliser moins le traitement d\u0027images et plus des techniques d\u0027apprentissage automatique afin d\u0027obtenir une classification des clichés dans deux classes : symétriques et non symétriques.\nDans une première étape, nous comparons des zones des clichés pour obtenir une mesure quantitative de la similitude. Pour obtenir les zones, nous avons proposé trois méthodes issues de la pratique des médecins, plus précisément la méthode maillage (qui propose une segmentation tenant compte de la structure de symétrie du sein), la méthode fenêtre (qui représente un balayage vertical ou horizontal de l\u0027image avec une zone de taille fixe) et la méthode rideau (qui propose un traitement progressif du cliché, soit en direction verticale soit horizontale).\nComparaison des mammographies\nLe graphique des différences entre les zones des deux clichés, met en évidence, par des piques, les éventuelles asymétries. La hauteur des piques est une mesure de la taille des différences, tandis que la largeur des piques est une mesure de la localisation. Tenant compte du fait que les dissemblances naturelles sont normalement répandues sur une zone plus large, en temps que les asymétries dues au cancer sont plus localisées, nous avons choisi de prendre en compte la hauteur et la largeur des plus grands piques pour l\u0027étape suivante de classification des clichés dans les deux classes : symétriques et non symétriques. Pour la classification nous avons utilisé les arbres de décision (Breiman et al., 1984), plus précisément des arbres C4.5 (Quinlan, 1993).\nNous avons testé les trois méthodes sur un jeu de données de 202 couples de clichés, en utilisant 73% de données pour l\u0027apprentissage et le reste de 27% pour la validation. La méthode rideau a eu un taux de réussite de 62% et la méthode maillage un taux de réussite de 68%. Les meilleurs résultats ont été obtenus par la méthode fenêtre, avec un taux de réussite de 70%.\nConclusions et perspectives Les résultats que nous avons obtenu, (un taux de réussite de 70%), sont des résultats préliminaires. Tenant compte du fait que l\u0027approche proposée est nouvelle dans le domaine et aussi du fait que nous avons eu une base de données spécialement annotées seulement sur les aspects de cancer, nous considérons les résultats encourageants.\nNous envisageons de continuer les tests sur une base de données mieux annotée, spécia-lement sur les aspects d\u0027asymétrie. Nous envisageons aussi d\u0027effectuer plusieurs tests afin de trouver les meilleurs paramètres des trois méthodes, la meilleure représentation des clichés et les méthodes de classification les plus adéquates.\nRéférences\n"
  },
  {
    "id": "1024",
    "text": "Introduction\nDans la dernière décennie, la conception de mesures d\u0027intérêt adaptées à l\u0027évaluation de la qualité des règles d\u0027association est devenue un défi important dans le contexte d\u0027ECD. Bien que le modèle des règles d\u0027association (Agrawal et al., 1993) permette une extraction non supervisée de tendances implicatives dans les données, il produit malheureusement de grandes quantités de règles, ce qui les rend inexploitables sans la mise en oeuvre d\u0027une étape lourde de post-traitement. Le post-traitement doit aider l\u0027utilisateur (un décideur ou un analyste) à choisir les meilleures règles en fonction de ses préférences. Une manière de faciliter la tâche de choix de l\u0027utilisateur consiste à lui offrir des indicateurs numériques sur la qualité des règles d\u0027association : des mesures d\u0027intérêt adaptées à ses buts et aux données étudiées.\nDans les travaux précurseurs sur les règles d\u0027association (Agrawal et al., 1993;Agrawal et Srikant, 1994) , deux premières mesures statistiques sont introduites : le support et la confiance.\nCelles-ci sont bien adaptées aux contraintes algorithmiques (cf apriori), mais ne sont pas suffisantes pour capturer l\u0027intérêt des règles pour l\u0027utilisateur. Afin de contourner cette limite, de nombreuses mesures d\u0027intérêt complémentaires ont été proposées dans la littérature. (Freitas, 1999) distingue deux types de mesures d\u0027intérêts : les mesures subjectives, et les mesures objectives. Les mesures subjectives dépendent des buts, connaissances, croyances de l\u0027utilisateur et sont combinées à des algorithmes supervisés spécifiques afin de comparer les règles extraites avec ce que l\u0027utilisateur connaît ou souhaite (Padmanabhan et Tuzhilin, 1998;Liu et al., 1999). Ainsi, les mesures subjectives proposent de capturer la nouveauté (novelty) ou l\u0027inattendu (unexpectedness) d\u0027une règle par rapport aux connaissances/croyances de l\u0027utilisateur. Les mesures objectives, quant à elles, sont des indices statistiques qui évaluent la contingence d\u0027une règle dans les données. De nombreux travaux de synthèse en récapitulent les définitions et propriétés (Bayardo Jr. et Agrawal, 1999;Hilderman et Hamilton, 2001;Tan et al., 2004  Gavrilov et al. (1999) ont étudiés la similitude des mesures afin de les classer. Gras et al. (2004) proposent un ensemble de dix critères : croissant avec un ou plusieurs des indicateurs prédéterminés, la dé-croissance doit respecter certaines attentes sémantiques, constraintes à respecter, décroissance avec la trivialité des observations, suffisamment souple et analytiquement générale, résistance discriminative à la croissance du volume de données ou une fonction discriminante, couplée avec sa contraposée b ? a, la comptabilité des propriétés analystiques, la formule et les algorithmes, l\u0027indépendance entre les deux variables a et b.\nCertaines de  Il existe aussi deux outils d\u0027expérimentation sont HERBS (Vaillant et al., 2003) et AR-QAT (Huynh et al., 2005a). ARQAT (Association Rule Quality Analysis Tool) est un outil graphique développé à l\u0027école polytechnique de l\u0027université de Nantes, écrit en Java, avec une interface web. Les caractéristiques principales de cet outil sont : (1) l\u0027analyse des ensembles de règles, (2) l\u0027analyse de corrélation et de cluster, (3) l\u0027analyse des meilleure règles, (4) l\u0027analyse de la sensibilité, (5) l\u0027analyse comparative. L\u0027outil supporte différents formats pour l\u0027exportation/importation des règles d\u0027association et les mesures calculées : PMML, CSV et ARFF (employés par WEKA). Tous les résultats illustrés dans cet article issus de cet outil.\nFIG. 1 -Les cardinalités d\u0027une règle.\nLa cardinalité n ab correspond au nombre effectif de contre-exemples. Soient U et V deux ensembles aléatoires de transactions respectant les contraintes |U | \u003d |A| et |V | \u003d |B|. Nous définissons le nombre de contre-exemples prédits comme la variable aléatoire N ab \u003d |U ? V |. La mesure II est alors définie par la probabilité :\nLa variable aléatoire N ab peut être modélisée selon plusieurs lois (hyper-géométrique, binomiale, ou poisson). En choisissant la loi hyper-géométrique, nous obtenons proba(\n. La formule globale de II peut être efficacement calculée avec une formule récursive.\nUne extension de l\u0027II, appelée l\u0027intensité d\u0027implication entropique (EII) a été proposée par (Blanchard et al., 2003)   \nCC(m\nGraphe corrélé versus graphe non-corrélé\nMalheureusement, le graphe de corrélation issu de la matrice de corrélation est complet, et n\u0027est donc pas directement exploitable par l\u0027utilisateur. Nous devons définir deux transformations afin d\u0027extraire des sous-graphes plus limités et plus lisibles. D\u0027abord, en employant la définition 2, nous pouvons extraire le sous-graphe corrélé partiel (CG+) : la partie du graphe où nous ne retenons que des arêtes liées à une forte corrélation ( ? -corrélation). En second lieu, la définition 3 nous permet de construire le sous-graphe non-corrélé partiel (CG0) où nous ne retenons que les arêtes liées aux valeurs de corrélation proches de 0 (?-noncorrélation).\nCes deux sous-graphes partiels peuvent ensuite être utilisés comme support de visualisation afin d\u0027observer les liaisons corrélatives entre mesures.\nOn peut également y observer des clusters d\u0027indices correspondant aux parties connexes des graphes.\nExtension à graphe de stabilité\nAfin de comparer les corrélations de mesures entre plusieurs jeux de données, nous introduisons une extension des graphes de corrélation aux graphes de stabilité.\nDéfinition 4. Le graphe ? -stable CG+ (resp. ?-stable CG0) d\u0027un ensemble de k jeux de règles R \u003d {R(D 1 ), ..., R(D k )} est défini comme le graphe intersection moyenne des k sousgraphes corrélés (resp. non-corrélés) partiels CG+ (resp.CG0) calculés sur R. Chaque arête retenue est alors valuée par la corrélation moyenne des k arrêtes. Ainsi le graphe ? -stable CG+ (resp. ?-stable CG0) permettra de visualiser les fortes corrélations (resp. non-corrélations) stables, comme étant communes aux k jeux de données étudiés. Leurs complémentaires donnerons les corrélation instables, différentes.\nDéfinition 5. On appellera clusters ? -stable (resp. ?-stable) les parties connexes du graphe ? -stable CG+ (resp. ?-stable CG0).  (Newman et al., 1998), et le second D 2 est un ensemble de données synthétiques T5.I2.D10k (T5 : taille moyenne des transactions est 5, I2 : taille moyenne au minimum des grands itemsets potentiellement est 2, D10k : nombre des items est 100). Les ensembles de règles d\u0027association R 1 (resp. R 2 ) ont été calculés sur D 1 (resp. D 2 ) en employant l\u0027algorithme Apriori (Agrawal et Srikant, 1994).\nDe plus, pour une évaluation plus fine du comportement des mesures sur les \"meilleures règles\", nous avons extrait R 1 (resp. R 2 ) à partir de R 1 (resp. R 2 ) comme l\u0027union des 1000 premières meilleures règles (? 1% des 100000 règles disponibles) selon chaque mesure (voir Tab. 1).\nCes deux jeux de données ont été volontairement choisis pour leur caractère caricatural. Les attributs de la base de données D 1 sont fortement corrélés et délivrent de très nombreuses règles sans contre-exemples (confiance à 1). A contrario, la base de données synthétique D 2 est constituée d\u0027attributs faiblement corrélés et délivre peu de règles sans contre-exemple. Ainsi, nous attendons de ce choix qu\u0027il nous amène à ne découvrir que très peu de stabilités corréla-tives entre les deux jeux de données. \nRésultats et discussion\nDans notre expérience nous avons utilisé les trente-six mesures d\u0027intérêt (trente-quatre mesures sont définies dans Huynh et al. (2005b) en ajoutant deux measures\n. Les mesures EII(? \u003d 1) et EII(? \u003d 2) sont deux versions entropiques de la mesure II.\nNotre expérience vise à trouver des corrélations stables, a priori inattendues, entre les quatre ensembles de règles. A cette fin, nous analysons les résultats produits dans : (1) les quatre graphes CG0 et le graphe CG0 montrant les indices non corrélés stables, (2) les quatre graphes CG+ et le graphe CG+ montrant les indices corrélés stables. Ensuite, nous pouvons observer que les graphes CG+ obtenus sur l\u0027ensemble total des règles (CG + (R 1 ) et CG + (R 2 )) et le sous-ensemble des meilleures règles (CG + (R 1 ) et CG + (R 2 )) sont très semblables. Ceci nous indique que sur les deux jeux de données les corrélations et les clusters formés demeurent stables lorsque l\u0027on sélectionne les meilleures règles.\nD\u0027autre part, comme on l\u0027attendait, on observe un écart important entre les deux jeux de données, ce qui indique une sensibilité des mesures à la nature des données.\nEn revanche, nous pouvons observer un nombre important de corrélations entre mesures sur le jeu de données R 2 -même s\u0027il est deux fois plus faible que sur R 1 -alors que nous en attendions peu du fait de la faible corrélation des données dans D 2 .\nLa figure Fig. 3 permet de visualiser les mesures non-corrélées, dont le point de vue sur les données diffère.\nOn y observe un très faible nombre de non-corrélations, ce qui indique que très peu de mesures sont en désaccord fort sur l\u0027évaluation des règles. Toutefois, le nombre de mesures non-corrélées augmente lorsque l\u0027on passe de la totalité des règles aux meilleures. En revanche, contrairement à ce que l\u0027on pouvait attendre, il y a moins de non-corrélations sur le jeu de données synthétique R 2 .\nEnfin, aucun comportement stable n\u0027apparaît entre les mesures sur les quatre graphes CG0, et donc le graphe CG0 est vide.\nFIG. 2 -Les 4 graphes CG+ (les clusters sont grisés).\nLe graphe CG+ : étude de la stabilité des corrélations\nLe résultat le plus surprenant apparait dans le graphe CG+. En effet, contrairement à notre attente, nous découvrons cinq clusters de mesures ? -stables, c\u0027est-à-dire dont les corrélations demeurent inchangées entre les jeux de données. Ceci dénoterait d\u0027une invariance avec la nature des données ! En analysant plus précisement ces cinq clusters ? -stable, nous notons quelques éléments intéressants.\n(C1), le plus grand cluster, (Confidence, Causal Confidence, Causal Confirmed-Confidence, Descriptive Confirmed-Confidence, Laplace) rassemble des mesures dérivées de la mesure de confiance (Confidence). De plus, ce lien est fort, puisque le graphe est complet et les valeurs de corrélation supérieures à 0.97. Ceci indique un très fort accord entre ces cinq mesures.\n(C2), ce cluster moins fortement corrélé que le premier, est constitué des mesures Phi-\nCoefficient, Lerman, Kappa, Cosine et Jaccard. Ce cluster rassemble des mesures partageant les quatre propriétés de : symmetric under variable permuatation, antisymmetric under row/column permutation, et null invariance (Tan et al., 2004). Les deux mesures Jaccard et Cosine ne partagent que la cinquième propriété (null invariance) proposée par Tan et al. (2004).\n(C3), rassemble trois mesures concernées par la première propriété (symmetry/asymmetry under variable permutation) proposée par Tan et al. (2004). L\u0027existence de ce cluster est né-cessaire pour distinguer la règle a ? b de b ? a.\n(C4), est un cluster constitué par deux versions de l\u0027intensité de l\u0027implication EII et EII 2, ce qui n\u0027est pas surprenant.\n(C5), la stabilité de la corrélation Yule\u0027Q et Yule\u0027Y, est elle aussi sans surprise puisque les deux mesures présentent une dépendance fonctionnelle. Ce cluster est lié à la deuxième propriété (row/column scaling invariance) proposée par Tan et al. (2004).\nLes résultats du graphe ? -stable, donnent une piste intéressante pour construire une base réduite de mesures dont le point de vue est le plus différent sur les données. Il suffit pour cela de proposer à l\u0027utilisateur de choisir cinq mesures, une parmi chacun des clusters. Nous pourrions aussi calculer automatiquement le meilleur représentant de chaque cluster en fonction des valeurs de corrélation. Contre toute attente, une étude de la stabilité des corrélations entre mesures d\u0027intérêt, a fait apparaître cinq groupes de mesures stables entre deux jeux de données choisis pour leur nature opposée.\nBien sûr ces résultats préliminaires restent à confirmer sur un ensemble de données plus important. Nous envisageons de prolonger notre travail dans deux directions. En premier lieu, nous souhaitons un indice de similarité entre mesures meilleur que l\u0027indice de corrélation linéaire dont les limites sont soulignées dans la littérature. En second lieu, nous souhaitons amé-\n"
  },
  {
    "id": "1025",
    "text": "Contexte\nNos travaux s\u0027insèrent dans un projet du réseau ARTCADHi visant à offrir aux chercheurs en Sciences Humaines des assistants à la construction du sens dans des bibliothèques numériques spécialisées. Dans ce cadre, limiter la description des documents à une indexation unique, fixe et effectuée par un tiers, revient à nier leur expertise. Porphyry propose l\u0027instrumentation du travail des chercheurs par l\u0027enrichissement itératif du corpus par des structures hypermédias. Ces structures sont construites par les spécialistes en fonction de leurs problématiques et de leurs spécialisations. Elles sont exprimées sous forme de réseaux de description, une variante des réseaux sémantiques dans laquelle seule existe la relation de composition (Benel A., 2003).\nDans son état actuel, Porphyry offre un moyen de visualiser des points de vue lorsqu\u0027ils sont appliqués aux même cas expérimentaux. Cependant, ce n\u0027est que la première étape dans le processus de confrontation mené par le chercheur, et les réseaux de description ne sont qu\u0027un formalisme parmi d\u0027autres. Nous proposons donc de spécifier un atelier multiformalisme d\u0027aide à la construction de sens par confrontation de points de vue.\nDu fait que Porphyry est adressé à des chercheurs en Sciences Humaines, le désaccord entre deux experts est matière à réflexion et à enrichissement. La confrontation des points de vue va donc au-delà de l\u0027intégration de travaux réalisés de manière transversale dans le but d\u0027en faire un tout unique et cohérent. L\u0027accent est mis sur le partage des idées, la confrontation devant permettre d\u0027outiller l\u0027étude des différents points de vue pour que de nouvelles idées puissent voir le jour plus facilement.\nProposition\nNous envisageons cette démarche dans un cadre très général, bien que la plate-forme Porphyry en soit un élément principal. Nous travaillons sur des points de vue exprimés par leur saisie dans un système informatique, mais nous ne limitons pas ce système à Porphyry seulement. Dans ce cadre, nous définissons un point de vue comme une théorie sur un sujet d\u0027étude exprimée par un modèle dans un langage. Nous regroupons sous le terme « langage » aussi bien la langue ou le formalisme que le modèle du document, qui clarifie les règles diverses auxquelles l\u0027écriture se plie. Nous considérerons cependant de manière plus approfondie le cas des langages formels.\n-725 -RNTI-E-6 \nSummary\nPorphyry today allows experts to express their points of view in a formal context. The next stage, which is discussed here, is to make possible to match these points of view.\n"
  },
  {
    "id": "1026",
    "text": "Summary\nBasel 2 regulations brought new interest in supervised classification methodologies for predicting default probability for loans. An important feature of consumer credit is that predictors are generally categorical. Logistic regression and linear discriminant analysis are the most frequently used techniques but are often unduly opposed. Vapnik\u0027s statistical learning theory explains why a prior dimension reduction (eg by means of multiple correspondence analysis) improves the robustness of the score function. Ridge regression, linear SVM, PLS regression are also valuable competitors. Predictive capability is measured by AUC or Gini\u0027s index which are related to the well known non-parametric Wilcoxon-Mann-Whitney test. Among methodological problems, reject inference is an important one, since most samples are subject to a selection bias. There are many methods, none being satisfactory. Distinguish between good and bad customers is not enough, especially for long-term loans. The question is then not only \"if\", but \"when\" the customers default. Survival analysis provides new types of scores.\n"
  },
  {
    "id": "1027",
    "text": "Introduction\nLes valeurs-tests\nPour faire un test de l\u0027hypothèse nulle H 0 , le statisticien calcule une « probabilité critique » (ou p-value). C\u0027est la probabilité, calculée sous H 0 , d\u0027un événement au moins aussi extrême que l\u0027événement observé. De façon intuitive, on comprend que cette probabilité est d\u0027autant plus faible qu\u0027on est loin de l\u0027hypothèse nulle. Si l\u0027événement observé est très improbable sous l\u0027hypothèse nulle, on jugera que les observations sont vraisemblablement régies par un mécanisme non nul. Il est donc tentant d\u0027utiliser cette valeur numérique pour évaluer l\u0027écart entre ce qu\u0027on a observé et la situation « sans intérêt » correspondant à ce qu\u0027on aurait observé sous H 0 . Dans ce contexte, plus l\u0027évaluation de l\u0027écart est forte (plus la probabilité critique est faible), plus ce qu\u0027on a observé est intéressant (Gras et al., 2002 ;Lerman et Azé, 2003 ;Lallich et Teytaud, 2004). Dans la pratique, on se rend compte que la p-value est difficile à manipuler ; elle peut atteindre des valeurs très faibles, très peu lisibles ; pire, dans certains cas, elle est inutilisable car on se heurte aux limites de l\u0027approximation\nUn critère numérique de classement et de sélection\nLa notion de critère invoquée ici est particulière. On utilise en effet les outils mis en oeuvre dans les tests d\u0027hypothèses mais il ne s\u0027agit en aucun cas de faire des tests au sens de la théorie classique des tests statistiques. L\u0027objectif n\u0027est pas, par exemple, de tester l\u0027indépendance entre A et C puisqu\u0027on tiendra pour vrai que A et C sont liés. La mécanique des tests est en quelque sorte dévoyée pour servir comme outil d\u0027évaluation et non comme outil de prise de décision dans l\u0027incertain.\nOn se place donc délibérément hors du cadre de la théorie de la décision statistique, et par conséquent hors d\u0027atteinte des critiques qui s\u0027y rapporteraient. Cette remarque doit écarter à l\u0027avance les objections que ne manquerait pas de faire à juste titre le statisticien qui lirait ces lignes sans ce préalable.\nLes comparaisons multiples\nComme on le voit dans tout contexte de Data mining et en particulier dans le logiciel SPAD, les valeurs-tests sont calculées pour évaluer, sur le même jeu de données, des dizaines ou des centaines d\u0027écarts entre moyennes ou entre pourcentages, ou pour évaluer autant de corrélations entre variables mesurées sur les mêmes individus.\nDans le cadre de l\u0027application classique des tests statistiques (on accepte de rejeter à tort l\u0027hypothèse nulle avec une probabilité fixée ?), une telle situation nécessiterait de corriger le seuil confiance de chaque test pour assurer un risque global fixé à ? (correction de Bonferroni ou toute autre correction proposée dans la littérature). S\u0027il s\u0027agit d\u0027évaluer dans quelle mesure les observations supportent l\u0027idée qu\u0027on est éloi-gné d\u0027une hypothèse nulle, il n\u0027y a pas de seuil de confiance en jeu : on évalue simplement des écarts relatifs entre les valeurs-tests. Il y a effectivement des comparaisons multiples mais la question de la correction pour comparaisons multiples ne se pose pas dans ce contexte.\nLes inconvénients de la valeur-test\nEvaluer la force ou l\u0027intérêt des liaisons et des écarts est un problème qu\u0027on rencontre dans le contexte du Data mining où le nombre des observations est toujours « très grand », bien au-delà des tailles d\u0027échantillons rencontrés dans une situation de test d\u0027hypothèse. Les probabilités critiques seront par conséquent généralement très faibles, et d\u0027ailleurs souvent difficiles à calculer pour cette raison. En d\u0027autres termes, les valeurs tests seront très grandes et toujours, à phénomène égal, fonctions croissantes du nombre d\u0027observations. Illustrons cette remarque sur un exemple où l\u0027on compare deux pourcentages en utilisant par exemple la loi hypergéométrique (on place les effectifs dans un tableau 2x2 dont les marges sont supposées fixées).\nConsidérons un groupe particulier d\u0027individus représentant 56% des observations totales. Considérons un caractère qui est présent dans 23% de l\u0027ensemble de ces observations, mais dans 28% du groupe particulier (le groupe noté A). Le même écart entre 23% et 28% correspond à une valeur test qui dépend évidemment du nombre des observations sur lesquelles il est calculé. Choisissons quelques cas correspondant à ces mêmes pourcentages pour des tailles croissantes, de n\u003d100 à n\u003d4000. Les données sont présentées dans les tableaux 2x2 de la Figure 1. Ce phénomène est bien connu des statisticiens : si la taille de l\u0027échantillon croît suffisamment, le moindre écart à toute hypothèse nulle finit par devenir « significatif ».\nDans ces conditions, non seulement les valeurs-tests ne mesurent pas la « significativité » d\u0027un écart, mais de plus elles ne peuvent pas être comparées si elles sont calculées sur des nombres différents d\u0027observations.\nLa Valeur-test normalisée VT100 pour les règles d\u0027association\nOn ne rappellera pas ici les nombreux travaux déjà publiés qui s\u0027attaquent au problème de détection des règles d\u0027association « intéressantes » parmi la liste toujours longue des règles fournies par les algorithmes classiques, comme l\u0027algorithme A PRIORI (Agrawal et Srikant, 1994) ou ses divers avatars. On pourra utilement consulter certains articles de synthèse (Lenca et al., 2003 ;Tan et al., 2002).\nLes remarques faites ci-dessus nous conduisent à proposer une sorte de valeur-test normalisée, jouant un rôle analogue à celui de la valeur-test habituelle, mais rendue indépen-dante du nombre d\u0027observations sur lesquelles elle est calculée. A ce titre, son rôle essentiel sera de ranger les règles d\u0027association selon leur ordre d\u0027intérêt et accessoirement de suggé-rer un seuil en deçà duquel les règles n\u0027auront vraisemblablement plus d\u0027intérêt (quelque soit la taille des données).\nLe principe en est simple : on décide de se ramener artificiellement au cas du nombre d\u0027observations n\u003d100. Cette valeur -a priori arbitraire -étant à rapprocher de la taille raisonnable des échantillons utilisés quand la théorie des tests s\u0027est historiquement développée. Pour ce faire, on imagine le mécanisme suivant.\nConsidérons une règle d\u0027association particulière construite sur les n observations disponibles (n étant grand comparé à 100). Il lui correspond un tableau 2x2 où la somme des fré-quences vaut n. Comme on l\u0027a introduit plus haut, on mesure l\u0027intérêt de la règle en s\u0027appuyant sur l\u0027écart à l\u0027indépendance mesuré à partir de la loi hypergéométrique (on verra plus loin que le raisonnement sera analogue pour tout autre critère).\nDans le contexte hypergéométrique, les marges du tableau 2x2 sont fixées. On imagine donc les données comme étant des 0 et des 1 répartis dans deux colonnes à n composantes, de façon aléatoire, le nombre de 1 étant fixé dans chaque colonne. Un échantillon de taille 100 extrait de ces données correspond à un tirage au hasard de 100 lignes dans ce tableau à 2 colonnes de longueur n.\nOn répète un grand nombre de fois le tirage d\u0027échantillons de taille 100. Pour chaque tirage, on construit le tableau 2x2 correspondant et on calcule la probabilité critique hypergéométrique. Finalement on calcule la probabilité critique moyenne (la p-value moyenne) que l\u0027on obtient à partir de ces échantillons. Appelons VT 100 la valeur-test associée à cette probabilité critique moyenne. Cette moyenne VT 100 sera le critère utilisé pour caractériser l\u0027intérêt de la règle.\nCe mécanisme de calcul du critère VT 100 tend à placer l\u0027utilisateur dans le contexte classique d\u0027un échantillon aléatoire de taille n\u003d100 porteur de l\u0027association à évaluer. On serait précisément dans ce contexte classique si la population réelle était l\u0027ensemble des observations et si on ne tirait qu\u0027un seul échantillon. En tirant de nombreux échantillons, on stabilise la valeur de la p value tout en conservant la taille 100 du support de calcul. Dans ce contexte, on ne s\u0027interdira pas de penser ainsi : « si je trouve une VT 100 très supérieure à 1,645 (car 1,645 \nécart type est le seuil des valeurs tests d\u0027un test unilatéral à 5%), je suis vraisemblablement en présence d\u0027une règle très intéressante ».\nAinsi le critère VT 100 permettra de ranger par ordre d\u0027intérêt les règles calculées sur une même base de données. Il présente aussi l\u0027avantage de permettre la comparaison de règles calculées sur des bases de données différentes, par exemple sur des bases de données de tailles différentes extraites à des dates successives.\nCalcul pratique des VT100\nLe mécanisme de tirage répété des échantillons de taille 100 est utile pour présenter le critère. Pour réaliser les calculs, on utilisera une procédure d\u0027approximation rapide et suffisamment efficace : rapide, car elle ne nécessite pas la répétition des tirages, et donc des accès à la base ; efficace, car elle fournit des valeurs proches de celles du tirage répété et permet de classer les règles dans le même ordre. On suivra facilement cette procédure de calcul sur un exemple présenté en Figure 3. Considérons une règle A \u003d\u003eC définie dans le tableau 2x2 cidessous sur 2000 observations. On calcule le tableau correspondant des effectifs (décimaux) ramenés à un total égal à 100. \nFIG. 3 --Les effectifs sont décimaux\nIl s\u0027agit d\u0027approcher par interpolation ce que donnerait une loi hypergéométrique appliquée à ce tableau d\u0027effectifs décimaux. Chacun des trois effectifs décimaux écrit dans le tableau est compris entre deux entiers proches (par exemple 11,30 est compris entre 11 et 12). Ceci conduit à imaginer d\u0027approcher le résultat cherché comme barycentre des résultats hypergéométriques calculés sur les 8 tableaux obtenus en combinant les effectifs entiers les plus proches des 3 valeurs décimales, les coefficients barycentriques étant les écarts aux entiers.\nL\u0027approximation peut paraître naïve mais elle est certainement suffisante pour l\u0027objectif à atteindre qui est de comparer l\u0027intérêt des règles. Rien n\u0027empêche d\u0027ailleurs de lui substituer à volonté un calcul plus raffiné pourvu que le temps de calcul reste raisonnable (noter que, la taille 100 étant fixée, la loi hypergéométrique à calculer n\u0027a plus que 2 paramètres entiers libres, ce qui suggère l\u0027alternative d\u0027une tabulation à double entrée, stockée en mémoire pour éviter tout calcul hypergéométrique dans les applications).\nUn même principe pour différentes mesures d\u0027intérêt\nLorsqu\u0027on se base sur la loi hypergéométrique pour évaluer l\u0027intérêt d\u0027une règle A\u003d\u003eC (c\u0027est-à-dire pour évaluer l\u0027écart à une situation d\u0027indépendance entre A et C), le critère utilisé est symétrique en A et C et donnera le même résultat pour la règle C\u003d\u003eA. L\u0027intérêt de la règle est mesurée en fait par l\u0027écart entre son support et ce que serait ce support en cas d\u0027indépendance de A et de C (si les effectifs respectifs de A et C sont considérés comme des quantités fixées). A ce titre, le critère VT 100 basé sur la loi hypergéométrique pourrait être considéré comme un versant statistique du critère numérique usuel appelé Lift dans le cadre des règles d\u0027association.\nIl y a bien sûr de nombreux autres points de vue possibles pour apprécier l\u0027intérêt d\u0027une règle. A titre d\u0027exemples, nous allons évoquer deux autres utilisations du critère VT 100 . Dans tous les cas, il est commode de se représenter les observations concernant une règle A\u003d\u003eC comme 2 colonnes de longueur n, nommées A et C, contenant des 0 et des 1. Le cas d\u0027indépendance déjà évoqué associé à la loi hypergéométrique consiste à répartir de façon aléatoire un nombre n(A) fixé de 1 dans la colonne A et un nombre n(C) fixé de 1 dans la colonne C.\nCritère VT 100 associé à la confiance d\u0027une règle\nUn autre contexte d\u0027indépendance entre A et C est défini par le mécanisme suivant : dans la colonne A , les 1 sont introduits avec la probabilité constante p(A) et, de façon indépen-dante, les 1 sont introduits dans la colonne C avec la probabilité constante p(C). Dans ces conditions (schéma dit binomial), les marges du tableau 2x2 ne sont pas fixées.\nUne règle A\u003d\u003eC est d\u0027autant plus intéressante que sa confiance, fréquence du conséquent sachant que l\u0027antécédent est réalisé, calculée par n(AetC)/n(A), s\u0027écarte davantage de la fréquence globale du conséquent n(C)/n. Avec le schéma binomial, et si on estime la probabilité p(C) du conséquent par sa fréquence empirique n(C)/n, la loi du support n(AetC) est la loi binomiale de paramètres p(C) et n(A).\nLa probabilité critique d\u0027observer, sous l\u0027hypothèse d\u0027indépendance (loi binomiale), une confiance au moins aussi extrême que celle qu\u0027on a observée servira à mesurer l\u0027intérêt de la règle en terme de confiance. Transformée en nombre d\u0027écarts types d\u0027une loi normale, cette probabilité critique devient la valeur-test. Ce critère évaluant l\u0027intérêt d\u0027une règle possède la particularité de ne pas être symétrique (il est différent pour la règle C\u003d\u003eA) mais, comme le précédent, il a l\u0027inconvénient d\u0027être sensible aux effectifs.\nSelon le principe présenté plus haut, on le normalise en ramenant le paramètre n(A) à la valeur 100. Dans ces conditions, on se trouve en présence d\u0027une loi binomiale dont les paramètres sont connus (taille 100 et probabilité n(C)/n) mais il faudrait calculer la probabilité de dépasser une valeur non entière n(AetC)/n(A). L\u0027approximation de cette valeur peut se faire par interpolation comme précédemment : on calcule les probabilités critiques binomiales pour les deux entiers qui encadrent la valeur décimale et on approche la valeur cherchée par leur barycentre en prenant comme coefficients les écarts aux entiers. La transformation de cette probabilité en nombre d\u0027écarts types de la loi normale sera le critère VT 100 associé à la confiance de la règle.\nOn signale une propriété satisfaisante de ce critère (partagée bien sûr par d\u0027autres critè-res). Considérons deux règles ayant le même antécédent A et les conséquents C 1 et C 2 . Supposons que ces deux règles aient la même confiance, donc n(Aet C 1 ) est égal à n(Aet C 2 ). Si n(C 1 ) est inférieur à n(C 2 ), la règle A\u003d\u003e C 1 sera préférée car elle creuse l\u0027écart entre la confiance (qui est la même) et la fréquence du conséquent. Il est clair que le critère VT 100 calculé à partir de la loi binomiale favorisera bien la règle A\u003d\u003e C 1 .\nCritère VT 100 associé aux contre-exemples\nUn contre-exemple de la règle A\u003d\u003eC consiste en la réalisation de A alors que C n\u0027est pas réalisé. L\u0027intérêt pour une règle est d\u0027autant plus grand que le nombre de contre-exemples noté n(Aet~C) est faible. Dans le schéma binomial précédent (répartition des 0 et des 1 dans les colonnes A et C avec des probabilités constantes), l\u0027hypothèse d\u0027indépendance implique une loi simple pour le nombre X de contre-exemples. On peut en effet estimer la probabilité d\u0027un contre-exemple par le produit des probabilités des deux événements indépendants p \u003d {n(A)/n} {n(~C)/n}. Ainsi X suivra une loi binomiale de paramètres connus n et p.\nL\u0027intérêt d\u0027une règle mesuré sous l\u0027angle des contre-exemples reposera sur la probabilité, calculée sous l\u0027hypothèse d\u0027indépendance, d\u0027un nombre de contre-exemples au moins aussi extrêmes (c\u0027est-à-dire, plus petit) que le nombre observé. Cette probabilité sera évaluée dans le cadre normé d\u0027un échantillon de taille 100 puis transformée en nombre d\u0027écarts types d\u0027une loi normale pour définir la VT 100 associée aux contre-exemples. Ramené à 100 observations, le nombre de contre-exemples devient une valeur décimale ; on approchera donc la probabilité critique cherchée par le barycentre des deux probabilités binomiales calculées pour les entiers qui l\u0027encadrent.\nParmi les propriétés de ce critère, on notera qu\u0027il est non symétrique en A et C et qu\u0027il attribue la même valeur à la règle A\u003d\u003eC et à sa contraposée qui lui est logiquement équiva-lente ~C\u003d\u003e~A.\nRemarque\nPuisque les effectifs utilisés sont supposés ici très élevés, les distinctions entre certaines situations d\u0027indépendance peuvent s\u0027estomper et les distributions de probabilités (hypergéo-métriques, binomiales, khi-2 …) tendre vers les mêmes lois limites. Ceci peut faire apparaî-tre assez artificielles dans certains cas les distinctions faites ici entre les schémas d\u0027indépendance.\nUn exemple d\u0027application\nDonnées et résultats\nLes données utilisées sont extraites du fichier « Adult » disponible sur le site « UCI Machine Learning Repository » (Newman et al., 1998). Les données manquantes ont été remplacées par la moyenne pour les attributs continus, par une nouvelle modalité « manquante » pour les attributs discrets. Le tableau analysé ici possède 14 743 lignes (individus) et 12 colonnes (variables qualitatives dont certaines sont les variables quantitatives d\u0027origine recodées en classes). La dernière variable est une variable Oui/Non indiquant si l\u0027individu a un gain moyen supérieur ou non à US$ 50000. On s\u0027intéresse aux règles dont le conséquent est l\u0027attribut Non de cette variable et dont les antécédents peuvent contenir jusqu\u0027à 3 items. Pour la sélection des règles par un algorithme classique « A PRIORI », on s\u0027est fixé comme seuils un support supérieur à 20%, une confiance supérieure à 60% et un lift supérieur à 1,10. L\u0027implémentation correspond au composant « A PRIORI MR » dans le logiciel TANAGRA (Rakotomalala, 2005), le code source est disponible en ligne.\nPour la clarté de l\u0027illustration numérique, on présente une application \"supervisée\" où les règles doivent conduire à un conséquent unique choisi arbitrairement. Le critère naturellement s\u0027applique avec les mêmes propriétés au cas général de sélection parmi toutes les règles d\u0027association.\nLe tableau 1 liste les 12 premières règles rangées par valeurs décroissantes du critère VT100. Considérons par exemple la première règle : Le critère VT100 est consigné dans la colonne 8 du tableau. Il indique que, si on ramenait la taille de l\u0027échantillon observée n \u003d 14 743 à la taille arbitraire n \u003d 100, la probabilité d\u0027un événement au moins aussi extrême, transposé dans ce contexte, serait égale à la probabilité d\u0027être au delà de 3,2 écarts types de la loi normale (soit une p-value inférieure à 0,0007 calculée sur un échantillon de taille 100). Ce serait donc bien un événement exceptionnel qui nous ferait douter de l\u0027indépendance entre l\u0027antécédent et le conséquent, marquant par-là l\u0027intérêt que présente cette règle. Dans ce tableau 1, les 7 premières règles correspondent à des événements dont la probabilité calculée sur un échantillon de taille 100 serait inférieure à 0,001. La dernière des 12 règles du tableau aurait elle-même une probabilité légèrement inférieure à 0,05. \nN°\nTAB. 1 --Les 12 premières règles calculées sur les 14 743 individus Le support du conséquent est n[C] \u003d 11 221\nQuand il faut limiter la liste des règles fournies par les algorithmes de recherche de rè-gles, on peut choisir de les ranger en fonction du critère VT100 et fixer un seuil d\u0027arrêt. Par analogie avec les seuils conventionnels adoptés par les statisticiens, on pourra s\u0027arrêter à la valeur 1,645 du critère VT100 (correspondant au seuil 0,05 pour une p-value) ou à la valeur 2,326 (seuil 0,01 pour une p-value) ou encore 3,09 (seuil 0,001) dans les cas où il y aurait profusion de règles intéressantes.\nDans la colonne 9 du tableau 1, on fait figurer la valeur du critère VT100 estimée par simulation sur des échantillons réels de taille 100 extraits du tableau des observations. La procédure de simulation adoptée ici est la suivante : nous réalisons un tirage aléatoire avec remise parmi les individus couverts par la règle puis nous calculons la valeur test VT correspondante ; cette procédure est répétée N fois (N \u003d 50) et la valeur test calculée par simulation est la moyenne arithmétique des valeurs test individuelles.\nOn remarque que la VT100 calculée par interpolation à partir de la loi hypergéométrique a tendance à surestimer la valeur obtenue par simulation sur des échantillons de taille 100.\nValidation avec un échantillon test\nPour compléter cet exemple, on a procédé à une approche de validation sur échantillon test. Les 14 743 individus ont été répartis au hasard par moitié dans un échantillon d\u0027apprentissage (7 371 cas) et un échantillon test (7 372 cas). Les règles ont été recalculées sur l\u0027échantillon d\u0027apprentissage. Dans le tableau 2-A, on liste les règles obtenues en les numérotant en colonne 1 avec le numéro qu\u0027on leur a attribué dans le tableau 1. On constate que 9 des 12 règles se retrouvent avec l\u0027échantillon d\u0027apprentissage. L\u0027élimination au hasard de la moitié des cas a entraîné la disparition des règles n°4 et n°7 qui étaient présentes dans le tableau 1 et n\u0027a pas fait apparaître des règles nouvelles en haut du classement par les VT100. On constate dans le bas du tableau deux inversions de classement par les VT100 dont les valeurs numériques restent cependant proches.  \nTAB. 2-B --Echantillon test (7 342 individus), le support du conséquent est n[C] \u003d 5 575\nIl est intéressant finalement de comparer, pour ces 10 règles, le critère VT100 évalué sur l\u0027échantillon total, sur l\u0027échantillon d\u0027apprentissage et sur l\u0027échantillon test. Les résultats qui apparaissent sur le tableau 3 montrent bien que le critère est surévalué sur l\u0027échantillon d\u0027apprentissage (phénomène classique) et qu\u0027il établit un compromis (sorte de moyenne) avec la valeur calculée sur la totalité des cas disponibles.  TAB. 3 --Comparaison du critère VT100 appliqué à l\u0027ensemble des observations, à l\u0027échantillon d\u0027apprentissage et à l\u0027échantillon test\nConclusion\nLe critère VT100 est proposé pour ranger et sélectionner un nombre raisonnable de règles d\u0027association dans les cas d\u0027applications réelles où les données à analyser sont volumineuses.\nCe critère présente des propriétés intéressantes :\n• Facile à comprendre puisqu\u0027on s\u0027appuie sur le mécanisme usuel des tests en raisonnant sur la population des échantillons de taille 100 extraits des observations. • Facile à calculer quelle que soit la taille des données (car on effectue quelques interpolations dans la table hypergéométrique limitée à n\u003d100).\n• Souple puisque ne dépendant que d\u0027un seuil qui peut s\u0027exprimer en terme de pvalue (?\u003d0,05 ou ?\u003d0,01 etc.) ou en terme de nombre d\u0027écarts types d\u0027une loi normale (valeur-test 1,645 ou 2,326 etc.).\n• Indépendant de la taille des données tout en faisant sur les informations (support, confiance, lift, etc.) les compromis que savent faire les critères statistiques classiques (qui, appliqués directement, sont très sensibles à la taille de l\u0027échantillon). Si l\u0027on veut rendre plus robuste la sélection des règles (quand le nombre de cas disponibles est élevé) on propose la stratégie suivante : on divisera par moitié les données en apprentissage et test. On calculera les règles sur l\u0027échantillon d\u0027apprentissage et on évaluera le critère VT100 sur l\u0027échantillon test. On rangera les règles et on les sélectionnera par seuil en fonction du critère VT100 évalué sur l\u0027échantillon test.\nEnfin, notons que le critère VT100 peut être mis en application dans de nombreux autres problèmes de sélection d\u0027items caractéristiques rencontrés dans le Data mining : par exemple le critère de choix des variables de coupure dans un arbre de segmentation ou le critère d\u0027élagage associé ; le classement des items caractérisant une classe dans une typologie, ou caractérisant un facteur dans une analyse factorielle, etc.\n"
  },
  {
    "id": "1028",
    "text": "Introduction\nEn France, l\u0027apprentissage de la chirurgie orthopédique se déroule selon différentes modalités d\u0027enseignement comme le compagnonnage (apprentissage en situation réelle), les travaux pratiques en laboratoire d\u0027anatomie et quelquefois sur des simulateurs. Un travail antérieur que nous avons mené sur l\u0027enseignement du métier de chirurgien nous a permis de montrer l\u0027écart qui existe entre les contenus de la formation théorique et les besoins de la pratique (Vadcard, 2003). La formation théorique n\u0027est pas orientée vers la résolution de problèmes en situation, et la situation réelle, n\u0027étant pas construite à des fins didactiques, ne permet pas à l\u0027apprenant de prendre le temps qu\u0027il lui faut pour comprendre la résolution du problème qui se déroule (Bisseret, 1995). Car les connaissances du chirurgien ne se limitent pas à une partie déclarative et une partie gestuelle. Nous avons pointé l\u0027existence et la valeur opératoire de connaissances pragmatiques, souvent implicites, qui permettent l\u0027activité en situation. Ces connaissances, dont nous avons montré l\u0027absence de prise en charge dans le système d\u0027enseignement, nous semblent être un élément important à prendre en compte pour réduire l\u0027écart entre la formation théorique, qui transmet des connaissances de nature prédicative et la formation pratique, qui transmet des connaissances gestuelles opératoires.\nNotre objectif est ainsi de concevoir un environnement informatique qui constitue une étape intermédiaire entre les enseignements formels et le compagnonnage, et permet une pragmatisation des concepts théoriques et prescriptifs de l\u0027action avant leur mise en situation. L\u0027environnement comprend un simulateur de vissage du bassin, un ensemble de pages web annotées grâce à une ontologie du domaine, et un ensemble de cas cliniques qui sera intégré plus tard. Cet environnement est centré sur un modèle de connaissances, lequel intègre des connaissances tacites, ou pragmatiques, explicitement représentées. Cette représentation permet en particulier d\u0027associer tous les composants et de produire des rétroactions adaptées selon les éléments de connaissance diagnostiqués. Nous menons actuellement ce travail dans le domaine de la chirurgie osseuse, pour la résolution des problèmes de vissage percutané des fractures du bassin.\nDe la didactique professionnelle à la conception d\u0027EIAH\nNos travaux sont de nature pluridisciplinaire : informatique, didactique, psychologie et médecine travaillent à la réalisation de ce système d\u0027apprentissage. Dans cet article, nous montrons comment l\u0027aspect didactique de nos recherches nous permet de modéliser les connaissances en interaction avec les problématiques informatiques de représentation de ce modèle.\nC\u0027est dans le cadre de la construction d\u0027EIAH orientés vers une approche socioconstructiviste de l\u0027apprentissage qu\u0027une problématique didactique peut être intéressante. De ce point de vue, nous considérons la construction des connaissances comme étant le résultat d\u0027une interaction entre le sujet apprenant et son environnement, le milieu pour l\u0027apprentissage (Brousseau, 1998). Ainsi, pour nous le milieu doit être organisé de façon à favoriser l\u0027apprentissage : produire des rétroactions pertinentes en fonction des actions de l\u0027apprenant sur le problème posé. En ce sens, le dispositif informatique d\u0027aide à l\u0027apprentissage devra également pouvoir réagir vers l\u0027apprenant en fonction de ses actions à l\u0027interface. Nous considérons que pour que les rétroactions de l\u0027EIAH soient pertinentes au regard de l\u0027apprentissage il faut que celui-ci réagisse en fonction d\u0027une validation de la résolution proposée par l\u0027apprenant en fonction d\u0027un modèle des connaissances du domaine et non pas uniquement en fonction d\u0027une solution experte déterminée a priori (Luengo, 1999).\nEn tant qu\u0027élément de ce milieu, l\u0027EIAH devra posséder certaines caractéristiques précisées par l\u0027analyse de la connaissance qui est enjeu de l\u0027apprentissage. Nos recherches touchent également au domaine de la formation professionnelle ; nous nous inscrivons ainsi dans l\u0027approche de la didactique professionnelle (Pastre, 2002).\nL\u0027extraction de la connaissance\nLa construction d\u0027un ensemble organisé de règles et de problèmes passe par une analyse des processus d\u0027enseignement et d\u0027apprentissage, par une analyse des connaissances, et par leur représentation. La méthodologie adoptée est structurée autour des points suivants.\nNous analysons et décrivons à partir de notre corpus d\u0027observations la situation prescrite et la situation réelle. Ces deux facettes de l\u0027activité sont analysées parallèlement, et s\u0027enrichissent mutuellement (Pastré, 2002).\n? La situation prescrite est analysée et décrite à partir à partir de cours et d\u0027articles décrivant cette technique;\n-664 -RNTI-E-6 ? La situation réelle professionnelle est analysée et décrite à partir d\u0027observations de l\u0027action du point de vue des interactions entre l\u0027apprenant et l\u0027expert : films et entretiens de verbalisation. Nous nous attachons dans nos analyses à faire apparaître les critères de validation qui sont sous-jacents aux actions et aux prises de décisions. C\u0027est à ce niveau que se joue la conceptualisation de l\u0027action. En particulier, nous identifions des critères de validation de l\u0027action en situation qui n\u0027apparaissent pas dans la situation prescrite. Ce sont des connaissances forgées par l\u0027expert au cours de sa confrontation à la diversité des possibles de la situation (dans notre cas le vissage sacro-iliaque). Elles permettent à l\u0027expert de faire face à la diversité des situations tout en conservant l\u0027invariance globale de la résolution de l\u0027activité (Vergnaud, 1996). Ce type de connaissances nous intéresse tout particulièrement puisque nous les intégrons dans le modèle de connaissances de notre environnement afin qu\u0027il permette une réelle valeur ajoutée par rapport au déroulement actuel de la formation (théorie puis pratique sur le terrain) et comblant au moins partiellement, l\u0027écart qui existe entre ces deux modalités de formation.\nA partir de ces analyses (Vadcard, 2005), nous décrivons une famille de problèmes comme un ensemble de variables didactiques (voir tableau 1). Un champ de problèmes peut être engendré à partir d\u0027une situation par la modification des valeurs de certaines variables qui, à leur tour, font changer les caractéristiques des stratégies de solution (coût, validité, complexité…etc.) Seules les modifications qui affectent la hiérarchie des stratégies sont à considérer (variables pertinentes). En d\u0027autres mots nous nous intéressons aux variables avec lesquelles en agissant sur elles, on pourra provoquer des adaptations et des régulations : des apprentissages (Brousseau, 1998). \nTAB. 2 -Extrait de l\u0027ensemble de problèmes classés à partir des variables didactiques.\nEnsuite pour chaque famille de problèmes nous décrivons un ensemble d\u0027opérateurs qui s\u0027appliquent. Dans le modèle utilisé (Balacheff, 1995), nous appelons un opérateur (R) ce qui permet la transformation des problèmes ; ces opérateurs sont attestés par des productions et des comportements ou d\u0027actions à l\u0027interface. Enfin pour chaque opérateur nous identifions l\u0027ensemble des contrôles (?) qui y sont associés. Une structure de contrôle contient les outils de décision sur la légitimité de l\u0027emploi d\u0027un opérateur ou sur l\u0027état (résolu ou non) d\u0027un problème. Le modèle de connaissance présenté permet de « poser des hypothèses sur l\u0027état de connaissance qui est sous-jacent à une action de l\u0027utilisateur sur l\u0027interface de simulation. \nTAB. 2 -Extrait de l\u0027ensemble d\u0027opérateurs identifiés et des contrôles qui interviennent en fonction de la famille des problèmes PA.\nNous pouvons observer que dans le tableau qui précède nous différencions les contrôles selon leur nature. Les premiers contrôles (1 et 2) sont traités en tant que contrôles liés à la connaissance déclarative, car ils sont explicités et partagés, le deuxième type de contrôle (13 et 14) correspond à la connaissance qui est en partie tacite et qui se forge dans l\u0027action. Cette classification n\u0027est pas statique, elle peut évoluer dans le temps, mais elle nous permet en particulier de calculer la prise de décision didactique ou la forme de la rétroaction comme nous le verrons dans la suite.\nLa modélisation informatique\nLes conceptions du domaine sont formalisées à l\u0027aide de l\u0027ensemble des problèmes, contrôles et opérateurs décrit antérieurement. Nous avons défini les relations de dépendance et de causalité entre les plusieurs ensembles afin de les représenter sous forme d\u0027un réseau bayésien. Nous identifions dans la figure (1) les relations de dépendances suivantes : un problème P est résolu si les opérateurs associés R sont appliqués d\u0027une manière valide. Un opérateur R est appliqué d\u0027une manière valide si les contrôles ? associés et utilisés lors de la résolution de problème P sont valides. Un contrôle ? est valide si les traces des actions de l\u0027apprenant VS (variables des situations) lors de la résolution sont cohérentes par rapport au contexte du problème P. Le réseau permette de diagnostiquer la connaissance mobilisée lors de la résolution de problème avec un degré d\u0027incertitude. Actuellement le diagnostic permet de déduire l\u0027état de contrôles utilisés dans la résolution de problème (valide ou non) en fonction, d\u0027une part, du contexte du problème et, d\u0027autres part, des actions de l\u0027utilisateur dans l\u0027interface de l\u0027environnement (le simulateur de vissage sacro-iliaque). Le résultat de ce diagnostic est sous forme de probabilité sur les contrôles utilisés (Fig. 2).\nSuite au diagnostic nous cherchons à calculer la décision didactique. Pour nous, une décision didactique se pose en termes d\u0027états de connaissance diagnostiqués et visés. Cette prise de décision possède certaines caractéristiques, entre autres :\n? Elle est dépendante du problème dans lequel se situe l\u0027action qui est diagnostiquée, et des contrôles identifiés. Selon le type de contrôle la décision n\u0027est pas la même. Un contrôle diagnostiqué qui est lié à des connaissances déclaratives renverra sur une partie d\u0027un cours en ligne alors qu\u0027un contrôle lié à des connaissances pragmatiques renverra sur un problème à résoudre. ? Elle est incertaine. Ce degré d\u0027incertitude dépendra des informations disponibles lors de la prise de décision. Pour l\u0027automatiser nous avons donc fait le choix de formaliser la prise de décision didactique sous la forme des diagrammes d\u0027influence dans les réseaux bayésiens (Fig 2). Cette décision est ainsi calculée à partir des contrôles identifiés, grâce au diagnostic, et de l\u0027instanciation du réseau par rapport aux actions de l\u0027utilisateur et du problème traité. \nConclusion\nDu point de vue de l\u0027architecture nous séparons le diagnostic de la prise de décision pour pouvoir les étudier et les valider séparément (Mufti-Alchawafa, et al. 2004). La condition pour qu\u0027ils fonctionnent est sous-jacente au modèle : le diagnostic doit pouvoir identifier les contrôles qui sont intervenus dans une résolution, la prise de décision doit se faire en fonction des contrôles identifiés.\nNous travaillons actuellement sur la validation de notre représentation, sous forme de réseau bayésien. Du point de vue informatique, nous utilisons la notion de complétude, et d\u0027adéquation de la représentation vis-à-vis du modèle. Ensuite, si la représentation est validée nous travaillerons sur l\u0027optimisation des algorithmes pour la prise de décision dans le versant informatique et sur l\u0027analyse des formes de rétroaction pour le versant didactique.\nLe composant qui représente la prise de décisions sera ainsi un outil de test pour la didactique et la psychologie cognitive qui permettra l\u0027analyse des différentes formes de rétroactions épistémiques dans environnement informatique d\u0027apprentissage humain. Le but final étant de produire des rétroactions épistémiques adaptées et centrées sur l\u0027activité du sujet, à l\u0027interface, en situation d\u0027apprentissage.\n"
  },
  {
    "id": "1029",
    "text": "Introduction\nPour la recherche, le partage et l\u0027échange de ressources (données, programmes, services), le modèle pair-à-pair constitue une alternative au modèle client/serveur. Les pairs peuvent à la fois offrir (rôle serveur) et demander (rôle client) des ressources. Il existe de nombreuses architectures des systèmes pair-à-pair, se basant sur des techniques différentes de localisation des données, qui se traduisent par des méthodes différentes de routage des requêtes. Pour améliorer la localisation d\u0027une ressource recherchée par un pair, on ajoute de l\u0027information aux tables de routage des requêtes : il peut s\u0027agir du contenu des pairs, de l\u0027historique de leurs requêtes, ou des concepts qu\u0027ils traitent... La difficulté rencontrée lors de l\u0027intégration de la sémantique du contenu des pairs, est de déterminer un espace de représentation commun à tous les pairs du réseau. Quelques systèmes tels que SON (Semantic Overlay Network) (Crespo et al., 2002) utilisent des concepts définis à priori pour résoudre ce problème. Mais cette solution ne s\u0027applique qu\u0027à un domaine précis.\nPour pallier cet inconvénient, PlanetP (Cuenca-Acuna et al., 2002) utilise une signature séman-tique pour représenter le contenu de chaque pair. Cette signature est définie par une structure de données appelée filtre de Bloom (Bloom, 1970).\nNotre travail s\u0027inscrit dans le cadre du projet RARE mené au sein du GET (Groupement des Ecoles des Télécommunications). Dans ce projet, plusieurs approches sont étudiées comme l\u0027utilisation des filtres de Bloom, la propagation efficace des index via des algorithmes de Gossiping ou encore l\u0027apprentissage sur les requêtes passées par des mémoires associatives. Dans ce papier, nous nous intéressons à l\u0027utilisation des filtres de Bloom dans PlanetP et proposons une amélioration permettant de réduire la taille des filtres de Bloom, et par conséquent de faciliter leur diffusion à travers le réseau pair-à-pair, tout en maintenant les performances de la recherche d\u0027information. L\u0027approche est validée par des expérimentations sur les collections de données suivantes : CACM, CISI, MED et CRAN de SMART (Buckley, 1985).\nLes sections 2 et 3 de cet article décrivent l\u0027approche PlanetP et le fonctionnement du filtre de Bloom. La section 4 définit notre approche et la section 5 présente les différentes expérimentations menées.\nFiltres de Bloom\nDéfinition\nUn filtre de Bloom (Bloom, 1970)  Pour introduire un terme dans un filtre de Bloom, on calcule les valeurs des fonctions de hachage et on active (on met à 1) les bits du vecteur correspondants.\nPour tester l\u0027appartenance d\u0027un terme t à un ensemble Y de termes introduits dans le filtre de Bloom, on lui applique les fonctions de hachage. Si au moins un des bits est à 0 alors le terme t n\u0027appartient pas à Y. Par contre, si tous les bits sont à 1 alors t appartient probablement à Y avec un taux de faux positif moyen donné par la relation suivante :\nOù m est la taille du vecteur du filtre de Bloom, n le nombre de termes indexés et k le nombre de fonctions de hachage. Pour minimiser ce taux, on choisit un nombre de fonctions de hachage k respectant la relation suivante : \nRecherche dans PlanetP\nLa recherche d\u0027information est basée sur le modèle vectoriel. PlanetP propose une approximation à la mesure TFxIDF, qui nécessiterait une connaissance de tous les mots du réseau. Cette mesure adaptée aux P2P est l\u0027Inverse Peer Frequency IPF, pouvant être calculée à l\u0027aide des informations locales à chaque pair. La mesure IPF pour un terme t est donnée par :\nOù N est le nombre de pairs connus dans le réseau par le pair effectuant la recherche et N t le nombre de pairs parmi ceux-ci ayant les documents contenant t. Un noeud qui reçoit une requête cherche dans son index local. S\u0027il ne peut pas honorer la requête, il calcule les rangs des pairs de son index global. Pour donner un rang aux pairs, on utilise l\u0027expression suivante :\nOù Q est la requête, BF i le filtre de Bloom du pair i et t un terme de la requête. La requête est alors propagée aux pairs de plus grand rang.\nFiltre de Bloom dynamique\nIl existe des pairs qui contiennent plus de documents que d\u0027autres. Généralement, la ré-partition des documents dans les pairs suit une loi de Zipf (Goh et al., 2005 -sid i ? 81 : quatre fonctions de hachage et un filtre de Bloom de 10 000 bits ; Cette méthode nous permet de réduire d\u0027environ 50% la taille des filtres de Bloom, ce qui facilitera leur diffusion à travers le réseau. Le fait de réduire la taille des filtres de Bloom n\u0027affecte pas le taux de faux positif car le nombre de fonctions de hachage a été choisi selon l\u0027équation ( 2).\nExpérimentations et résultats\nPour nos expériences, nous avons utilisé les quatre collections de documents utilisées par PlanetP pour son évaluation (requêtes et jugements de pertinence associés). La table 1 présente le contenu de ces collections : elles sont composées de fragments de textes et résumés, et sont relativement petites en taille. Nous les avons préalablement traitées grâce à l\u0027outil de recherche d\u0027information SMART (Buckley, 1985), afin d\u0027extraire les mots lemmatisés, leurs fréquences et d\u0027éliminer les mots vides.\nPour tester les performances de notre approche, nous avons utilisé les métriques standard, rappel(R) et précision (P), définies comme suit pour une requête Q : R(Q)\u003d nombre de documents pertinents retournés/nombre de documents pertinents dans la collection P(Q)\u003d nombre de documents pertinents retournés/nombre de documents retournés \nOù f D,t est le nombre d\u0027apparitions du terme t dans D et |D| le nombre de termes dans D. Pour mesurer le rappel et la précision dans les collections, nous distribuons aléatoirement les documents sur 20 pairs virtuels selon la loi de Zipf sans redondance (un document n\u0027est présent que sur un seul pair). Nous construisons la signature de chaque pair en utilisant chacune des méthodes suivantes :\n-Filtre de Bloom fixe construit avec deux fonctions de hachage (comme dans le système PlanetP) avec une taille de 10 000 bits ; -Filtre de Bloom variable. Nous utilisons les configurations de l\u0027exemple de la section 4. La taille des filtres de Bloom a été choisie afin d\u0027assurer un taux de faux positif inférieur à 5%. Le rang des pairs est calculé par l\u0027équation (4) pour chacune des requêtes des collections puis ceux-ci sont triés par ordre décroissant de la mesure du rang. Ensuite p pairs jugés pertinents sont sélectionnés, p variant de 1 à 20 pairs. Les documents contenus dans ces p pairs sont triés par la mesure de similarité selon l\u0027équation (5). Nous extrayons les 30 documents de similarités les plus élevées et mesurons le taux de rappel et de précision. Par manque de place, nous ne présentons dans la figure 1 que les courbes obtenues pour la collection MED, des résultats similaires étant obtenus sur les autres collections. La figure 1 montre le taux de rappel et de précision en fonction du nombre de pairs contactés. Nous observons à travers ces courbes que l\u0027utilisation d\u0027un filtre de Bloom dynamique n\u0027altère pas les taux de rappel et de précision par rapport à un filtre de Bloom fixe.\nConclusion\nLe système PlanetP permet de faire de la recherche textuelle de documents dans un environnement distribué, en proposant un même espace de représentation partagé par tous les pairs, sous la forme de filtres de Bloom de taille fixe. Nous avons proposé de rendre cette taille dynamique, en fonction du nombre de documents stockés par chaque pair. Les expérimentations montrent que la dynamicité ne détériore pas les performances obtenues dans le cas classique (filtre de Bloom fixe). La bande passante utilisée pour la propagation des filtres de Bloom peut ainsi être réduite, ou bien le taux de leur diffusion à travers le réseau peut être augmenté, afin d\u0027enrichir les index globaux. L\u0027évaluation chiffrée de ces gains reste à réaliser, par simulation des échanges entre pairs, en utilisant le simulateur également développé dans le projet RARE.\n"
  },
  {
    "id": "1030",
    "text": "Introduction\nDans un certain nombre de domaines (détection de fraudes, de défaillances, analyse de comportements), la recherche de connaissances temporelles est non seulement utile mais né-cessaire. Certaines techniques d\u0027apprentissage permettent de gérer et de raisonner sur de telles connaissances, (Allen, 1990) a notamment défini des opérations sur des règles associées à des intervalles de temps. Des techniques d\u0027extraction de connaissances cherchent quant à elles à extraire des épisodes récurrents à partir d\u0027une longue séquence (Mannila et al., 1997), (Raissi et al., 2005) ou de bases de séquences (Agrawal et Srikant, 1995), (Masseglia et al., 1998). La recherche de telles informations devient d\u0027autant plus intéressante qu\u0027elle permet de prendre en compte un certain nombre de contraintes entre les évènements comme par exemple la durée minimale ou maximale séparant deux évènements.\nC\u0027est dans ce cadre qu\u0027a été introduite la recherche de motifs séquentiels généralisés dans (Srikant et Agrawal, 1996). Cette technique de fouille de données permet d\u0027obtenir des sé-quences fréquentes respectant des contraintes spécifiées par l\u0027utilisateur, à partir d\u0027une base de données de séquences (par exemple les achats successifs de différents clients d\u0027un supermarché). Différents algorithmes ont été proposés afin de gérer ces contraintes soit directement dans le processus d\u0027extraction, (GSP, Srikant et Agrawal (1996)) soit à l\u0027aide d\u0027un pré-traitement sur les séquences proposé dans GTC (Graph for Time Constraint), (Masseglia et al. (1999)).\nToutefois, si ces méthodes sont efficaces et robustes, elles ont pour principal inconvénient d\u0027être spécifiées par l\u0027utilisateur et nécessitent donc une bonne connaissance a priori des données et des durées à spécifier sous peine d\u0027obtenir des connaissances peu pertinentes. Des travaux ont été proposés afin de déterminer de manière automatique la fenêtre optimale d\u0027observation pour la recherche d\u0027épisodes dans une séquence (Meger et Rigotti, 2004), mais ils sont difficilement adaptables à l\u0027extraction de motifs séquentiels et dans ce domaine, aucun travail à notre connaissance ne propose une détermination automatique des contraintes de temps optimales. Par ailleurs, pour certaines applications il pourrait également être intéressant d\u0027assouplir les contraintes spécifiées par les experts du domaine afin de compléter leurs connaissances. Enfin, le nombre de motifs séquentiels extraits, selon les contraintes de temps utilisés, peut rapidement devenir trop important pour que leur analyse soit efficace. Une mesure permettant l\u0027exploitation des motifs séquentiels généralisés serait donc d\u0027une grande utilité.\nC\u0027est pourquoi nous proposons d\u0027étendre les contraintes de temps proposées pour l\u0027extraction de motifs séquentiels généralisés en utilisant certains principes de la théorie des sousensembles flous. Notre méthode permet, en effet, à partir de contraintes de temps initiales et d\u0027un degré de respect de ces valeurs, d\u0027extraire des motifs séquentiels respectant des contraintes étendues et de fournir pour chacun d\u0027eux sa précision temporelle. Nous offrons ainsi à l\u0027utilisateur une flexibilité dans la spécification de ses contraintes ainsi qu\u0027un outil d\u0027analyse des nombreuses séquences fréquentes extraites.\nAprès avoir présenté les concepts fondamentaux associés aux motifs séquentiels et aux motifs séquentiels généralisés dans la section 2, nous présentons dans la section 3 notre définition des contraintes de temps étendues. La section 4 présente notre proposition d\u0027algorithme mettant en oeuvre la gestion des contraintes de temps étendues lors du prétraitement des données. La section 5 présente ensuite quelques expérimentations montrant la faisabilité et l\u0027efficacité de notre approche. Enfin, la section 6 conclut sur les perspectives qu\u0027ouvrent nos travaux.\nDes motifs séquentiels aux motifs séquentiels généralisés\nMotifs séquentiels\nLes motifs séquentiels ont initialement été proposés par Agrawal et Srikant (1995) et reposent sur la notion de séquence fréquente maximale.\nPrenons par exemple une base de données DB d\u0027achats pour un ensemble C de clients c. Une transaction t est un triplet \u003cid_client, id_date, itemset\u003e qui caractérise le client qui a réalisé l\u0027achat, la date d\u0027achat et les items achetés. Soit I \u003d {i 1 , i 2 , · · · , i m } l\u0027ensemble des items de la base. Un itemset est un ensemble non vide et non ordonné d\u0027items, noté (i 1 , i 2 , · · · , i k ). Une séquence se définit alors comme une liste ordonnée non vide d\u0027itemsets s i qui sera notée \u003c s 1 s 2 · · · s p \u003e. Une n-séquence est une séquence de taille n, c\u0027est-à-dire composée de n items. -604 -RNTI-E-6 C. Fiot et al.\nLes transactions de la base sont regroupées par client et ordonnées chronologiquement, dé-finissant ainsi des séquences de données. Un client c supporte une séquence S si elle est incluse dans la séquence de données du client c. Le support d\u0027une séquence est alors défini comme le pourcentage de clients de la base DB qui supporte S. Une séquence est dite fréquente si son support est au moins égal à une valeur minimale minSupp spécifiée par l\u0027utilisateur.\nLa recherche de motifs séquentiels dans une base de séquences telle que DB consiste alors à trouver toutes les séquences maximales (non incluses dans d\u0027autres) dont le support est supérieur à minSupp. Chacune de ces séquences fréquentes maximales est un motif séquentiel.\nMotifs séquentiels généralisés\nTelle qu\u0027elle a été introduite ci-dessus, la notion de séquence présente une certaine rigidité pour de nombreuses applications. En effet, si l\u0027intervalle de temps entre deux transactions successives est très court, on pourrait envisager de les considérer comme simultanées. A l\u0027inverse, deux évènements trop éloignés peuvent ne pas avoir de lien entre eux. C\u0027est pourquoi la notion de séquence généralisée a été proposée par Srikant et Agrawal (1996) afin de pallier ces restrictions en introduisant la prise en compte de contraintes temporelles.\nCes contraintes sont au nombre de trois : mingap est une durée minimale que l\u0027on doit respecter entre deux itemsets successifs d\u0027une même séquence ; maxgap est l\u0027écart maximal dans lequel doivent se trouver deux itemsets successifs ; windowSize est la fenêtre dans laquelle les items de deux transactions différentes peuvent être regroupés dans un même itemset. On modifie alors la notion d\u0027inclusion décrite précédemment pour tenir compte de ces contraintes.\nUne séquence de données  \nVers des contraintes de temps étendues\nDans cette section, nous allons examiner la mise en oeuvre des contraintes de temps éten-dues par analogie avec la théorie des sous-ensembles flous pour ensuite proposer leur définition et celle de la précision temporelle des séquences soumises à de telles contraintes.\nMise en oeuvre\nLa théorie des sous-ensembles flous, introduite par Zadeh (1965) autorise l\u0027appartenance partielle à une classe et donc la gradualité de passage d\u0027une situation à une autre. Cette théorie constitue une généralisation de la théorie ensembliste classique, des situations intermédiaires entre le tout et le rien étant admises. Dans ce cadre, un objet peut donc appartenir partiellement à un ensemble et en même temps à son complément.\nOn considère par exemple l\u0027univers des tailles possibles d\u0027un individu. Un sous-ensemble flou A (Petit ou Grand par exemple) est défini par une fonction d\u0027appartenance µ A qui décrit le degré avec lequel chaque élément de l\u0027univers considéré appartient à A, ce degré étant compris entre 0 et 1. Ainsi, un individu de 1m63 pourra à la fois être grand et petit avec, par exemple, un degré de 0.7 pour le sous-ensemble flou Grand et 0.3 pour le sous-ensemble flou Petit.\nLes opérateurs en logique floue sont une généralisation des opérateurs classiques. On considère notamment la négation, l\u0027intersection et l\u0027union. L\u0027opérateur ou t-norme (norme triangulaire) est l\u0027opérateur binaire d\u0027intersection :\nNous noterons (resp. ?) l\u0027opérateur (resp. ?) généralisé au cas n-aire.\nNotre proposition d\u0027extension des contraintes de temps pour les motifs séquentiels est fondée sur une analogie avec la théorie des sous-ensembles flous. Ainsi, on ne souhaite plus simplement qu\u0027une séquence respecte des contraintes spécifiées mais permettre à l\u0027utilisateur de relaxer ces contraintes jusqu\u0027à un certain seuil. Ce seuil correspond à un degré minimal de satisfaction des contraintes temporelles. Il sera spécifié par l\u0027utilisateur pour chacune des contraintes, de même que leur valeur initiale. Soit ws, g et G ces valeurs initiales pour les contraintes windowSize, minGap et maxGap. On considère ? ws , ? mg et ? M G les niveaux de précision associés à chacune d\u0027elles, ces niveaux étant compris entre 0 et 1, 0 indique alors que l\u0027on souhaite parcourir l\u0027ensemble des valeurs possibles et 1 que le paramètre correspondant est fixe. L\u0027utilisation de tels degrés implique de considérer plusieurs valeurs pour les paramètres windowSize, mingap et maxgap. Il est alors possible de considérer plusieurs \"chemins\" ou séquences dans les achats des clients pour reconstruire un motif. Cependant chacune de ces séquences respecte \"plus ou moins\" la contrainte initiale ce qui est évalué par les niveaux de précision ? ws , ? mg et ? M G dont le calcul est détaillé au paragraphe 3.3.\nExtension des contraintes de temps\nLa définition des contraintes de temps étendues est basée sur les valeurs limites utiles que les différents paramètres peuvent prendre. Ces valeurs utiles correspondent aux valeurs limites au-delà desquelles on ne pourra générer de séquences candidates respectant les contraintes.\nPrenons l\u0027exemple de windowSize. Dans le cas classique, windowSize prend une valeur fixe ws et la condition (2.2) signifie que date(\nReprenons l\u0027ensemble des clients C. Pour chaque client c, ses transactions ont des dates comprises entre \nFIG. 2 -Variation de la précision selon la valeur de windowSize\nDe même, on a pour maxgap :   \nPrécision temporelle d\u0027une séquence\nPour chacune des séquences fréquentes trouvées à la fin du processus d\u0027extraction, on calcule la précision avec laquelle chacune d\u0027elles respecte les contraintes de temps. On définit la précision d\u0027une séquence s pour un client c comme le degré de respect simultané des trois contraintes de temps (1), (2) et (3), calculé à l\u0027aide d\u0027une t-norme (). Pour chaque client, on cherche, parmi toutes les séquences d\u0027achats ? c , l\u0027occurrence de s qui respecte au mieux les contraintes de temps, en utilisant une t-conorme (?).\nLa précision temporelle d\u0027une séquence s \u003d\u003c s 1 · · · s n \u003e pour le client c est donnée par :\nPour l\u0027ensemble de la base, le degré de respect des contraintes de temps est donné par la moyenne des degrés de chacun des clients, c\u0027est-à-dire :\nc?C\nGETC -Graph for Extended Time Constraints\nL\u0027algorithme GTC proposé dans (Masseglia et al., 1999) permet de transformer une sé-quence d\u0027un client en un graphe de séquences respectant les contraintes de temps. Les graphes de séquences des différents clients sont ensuite utilisés pour déterminer les séquences fré-quentes par un algorithme d\u0027extraction de motifs séquentiels tel que PSP Masseglia et al. (1998). L\u0027efficacité de cette approche ayant été démontrée dans (Masseglia et al., 1999), nous avons choisi de nous en inspirer pour développer la nôtre. Nous proposons donc un algorithme permettant de construire un graphe de séquences pour les contraintes de temps étendues et également de calculer la précision des motifs séquentiels généralisés extraits.\nGETC -les algorithmes\nA partir d\u0027une séquence d\u0027entrée d, l\u0027algorithme GETC construit son graphe de séquences G d . Cet algorithme regroupe un certain nombre de sous-fonctions (addEdge, propagate, pruneM arked et convertEdges), non détaillées ici, qui sont présentées de manière plus approfondies dans (Fiot et al., 2005a).\nGETC commence par créer les sommets correspondant aux itemsets de la séquence puis ajoute à l\u0027ensemble des sommets l\u0027ensemble des combinaisons d\u0027itemsets permises selon les différentes valeurs de windowSize. Pour cela, l\u0027algorithme addW indowSize (non présenté ici) parcourt chaque sommet x et détermine pour chacun d\u0027entre eux quels autres sommets y peuvent être \"fusionnés\" avec x (si y.date() -x.date() ? ws). Chaque sommet correspond \nALG. 1: GETC alors à un itemset x.itemset() qui possède une date de début x.begin() et une date de fin x.end(). Ces sommets sont regroupés en niveau par date de fin des itemsets. Cela permet de tester le respect des contraintes pour un niveau et non pour chaque sommet. On accède à l\u0027ensemble des prédécesseurs d\u0027un sommet x par x.prev() et à ces successeurs par x.succ().\nEnsuite pour chacun des sommets du graphe de séquences, GETC ajoute les arcs respectant les contraintes minGap et maxGap. Ainsi, pour chaque sommet, on cherche le premier niveau accessible pour la contrainte minGap (ie. l.begin() -x.end() \u003e g ? ) et pour chaque sommet z de ce niveau, on construit les arcs (x,z), pour chaque sommet z tel que z.end() -x.begin() ? maxGap. La fonction addEdge permet d\u0027éviter les inclusions de chemins, grâce à la construction d\u0027arcs temporaires dans des cas d\u0027inclusions possibles.\nL\u0027algorithme addEdge construit les arcs entre des sommets qui respectent les contraintes de temps sur minGap et maxGap. Il crée un arc définitif si les sommets ne sont pas déjà liés par une séquence ou une inclusion de leurs successeurs ou prédécesseurs. Dans ce cas, l\u0027arc construit est temporaire et ne devient définitif que si la séquence qu\u0027il forme est maximale. C\u0027est également lors de l\u0027exécution de cet algorithme que les sommets inclus sont marqués, pour pouvoir ensuite être supprimés s\u0027ils sont inutiles.\nDans le cas où pour un sommet x, on ne peut atteindre le niveau l à cause du non respect de la contrainte mingap, on utilise la fonction propagate pour \"propager ce saut\". Pour chacun des sommets y de ce niveau inaccessible, on ajoute si nécessaire et si on respecte les contraintes minGap et maxGap, un arc entre chacun des successeurs de x et ce sommet y. Comme dans addEdge, on construit des arcs temporaires ou définitifs selon que la séquence construite peut éventuellement être incluse ou au contraire n\u0027a aucune chance de l\u0027être.\nEnfin, l\u0027algorithme pruneM arked élimine les sommets de sous-séquences incluses. Puis, convertEdges supprime les arcs de sous-séquences incluses : pour tout arc temporaire de x vers y, si y est inclus dans un successeur z de x et si les successeurs de y sont également tous des successeurs de z, alors il existe une sous-séquence incluse, l\u0027arc est supprimé. Sinon, l\u0027arc est indispensable pour obtenir toutes les séquences maximales.\nGETC étant utilisé comme prétraitement pour la prise en compte de contraintes temporelles en vue de l\u0027extraction de motifs séquentiels, il doit générer absolument toutes les séquences issues d\u0027une séquence de données. Par ailleurs, afin d\u0027améliorer le temps d\u0027extraction, il est nécessaire que GETC n\u0027extraie que les séquences maximales, nous avons donc montré dans (Fiot et al., 2005a) que l\u0027algorithme GETC construit exactement toutes les séquences de la plus grande taille possible pour les séquences respectant windowSize, minGap et maxGap.\nConstruction du graphe de séquences\nNous utilisons GETC comme prétraitement pour la prise en compte des contraintes de temps étendues et PSP (Masseglia et al., 1998) pour l\u0027extraction des motifs séquentiels. Cette approche du type générer-élaguer utilise une structure d\u0027arbre prefixé pour organiser les sé-quences candidates et permettre de trouver plus efficacement l\u0027ensemble des candidats inclus dans une séquence de données. En utilisant ainsi le graphe de séquences obtenu par GETC, la vérification des contraintes de temps est rendue inutile pendant le parcours des candidats, seule l\u0027inclusion devant être vérifiée. Cette méthode est similaire à celle proposée dans (Masseglia et al., 1999) qui permet d\u0027optimiser l\u0027extraction de motifs séquentiels généralisés grâce à un parcours pour les contraintes de temps indépendant et sans retour arrière de l\u0027arbre des séquences candidates et ainsi la vérification du moins de combinaisons possibles.   (1) (2 3) (4) (4) (5) (6)\n18\n(1 2 3)\n( 5 6) (1) (2 3) (3 4) * (4) (4) (5) (6) (7) * (8) * \nCalcul de la précision temporelle d\u0027une séquence\nUne fois le graphe de séquences construit, on connaît toutes les séquences autorisées par les contraintes de temps et celles qui sont interdites. Cependant, certaines séquences respectent les contraintes fortes de l\u0027utilisateur alors que d\u0027autres ont été construites en appliquant les contraintes étendues, elles ne sont donc pas équivalentes. On calcule alors la précision de chacun des chemins (séquences maximales) et on l\u0027affecte aux sous-séquences qui le composent.\nAfin de déterminer l\u0027appartenance du chemin vis-à-vis des contraintes de temps, on value chaque arc (x,y) par (µ mg (y.begin()-x.end()),µ M G (y.end()-x.begin())). Chaque sommet est valué par windowSize. La précision d\u0027une sous-séquence, et donc le degré de respect des contraintes de temps est alors donnée par la formule (5). Grâce à l\u0027algorithme valueGraph (non présenté ici), le graphe est alors parcouru et à chaque sommet s, on attribue la valuation µ ws (s.end()-s.begin()) et à chaque arc entre s et t, la valuation (µ mg (t.begin()-s.end()),µ M G (t.end()-s.begin())).\nExemple 6. A partir de la base de données TAB. 2 et des contraintes de temps spécifiées dans l\u0027exemple 5, on construit les trois graphes de séquences correspondants. En prenant minSupp\u003d70%, on obtient six séquences maximales fréquentes : \u003c(2 3 4)\u003e, \u003c(2 3)(4)(5 6)\u003e, \u003c(2)(4 5)\u003e, \u003c(3 4)(5)\u003e, \u003c(3 4)(6)\u003e et \u003c (3)(4 5  \nExpérimentations\nCes expérimentations ont été réalisées sur un PC équipé d\u0027un processeur 2,8GHz et de 2Go de mémoire DDR, sous systèmes Linux, noyau 2.6. Le but de ces mesures est de comparer le comportement de GETC par rapport à GTC. Pour certaines comparaisons, nous utilisons également une implémentation de PSP, intégrant ou non la gestion des contraintes de temps. Les résultats présentés ici ont été obtenus à partir du traitement de plusieurs jeux de données synthétiques comportant environ 1000 séquences de 20 transactions en moyenne. Chacune de ces transactions comportant en moyenne 15 items choisis parmi 1000.\nLa première étape a consisté à comparer les temps d\u0027exécution en l\u0027absence de contrainte de temps, c\u0027est-à-dire en prenant windowSize \u003d 0, minGap \u003d 0 et maxGap \u003d ? pour GTC ainsi que pour GETC, avec une précision de 1 pour les trois paramètres. Ainsi, nous avons pu comparer le temps d\u0027exécution de notre algorithme avec ceux de PSP et GTC. La figure FIG. 6(a) montre que le comportement de GETC est similaire à celui de GTC et que les temps d\u0027exécution sont quasiment identiques, pour des motifs extraits qui sont les mêmes.\nNous avons ensuite répété ces mesures en introduisant le traitement de contraintes de temps, toujours avec une précision de 1 afin de comparer le comportement de GETC et GTC pour la gestion de contrainte de temps non-étendues. La FIG. 6(b) montre l\u0027évolution du temps d\u0027extraction en fonction de la valeur de windowSize. GETC a un comportement linéaire proche de celui de GTC. La différence provient de la phase de traitement de la précision, dont le temps augmente légèrement avec windowSize, puisque si ce paramètre augmente, on augmente le nombre de sommets dans le graphe de séquences.\nLa deuxième partie de nos expérimentations a porté sur l\u0027analyse des motifs séquentiels extraits par GETC en fonction de la précision des différentes contraintes. La figure FIG. 6(c) présente les temps d\u0027extraction comparés de GTC et GETC en fonction du support minimum selon des valeurs choisies des différents paramètres. Ces valeurs ont été calculées afin que les contraintes de temps utilisées pour GTC et GETC avec une précision de 1 correspondent aux valeurs limites de GETC avec une précision différentes de 1. Les paramètres retenus sont :\n-GETC avec windowSize\u003d0, minGap\u003d1 et maxGap\u003d5 avec une précision de 0.75, qui nous donne ws ? \u003d 4, mg ? \u003d 0 et M G ? \u003d 10, -GETC avec windowSize\u003d4, minGap\u003d0 et maxGap\u003d10 avec une précision de 1, -GTC avec windowSize\u003d4, minGap\u003d0 et maxGap\u003d10, On peut constater que l\u0027utilisation de GETC avec des contraintes de temps étendues n\u0027est pas plus coûteuse que celle de GTC avec les valeurs limites des contraintes, tout en permettant d\u0027obtenir les mêmes motifs séquentiels, accompagnés de leur précision temporel. Il est inté-ressant, dans le cas où on ignore la valeur optimale d\u0027une ou plusieurs contraintes de temps, d\u0027utiliser GETC avec une précision différente de 1 pour certains paramètres, afin de balayer un ensemble de possibilités plus large. L\u0027analyse des motifs obtenus et de leur précision pourra renseigner sur une valeur plus adéquate des contraintes de temps. Enfin, la FIG. 6(d) montre l\u0027évolution du temps d\u0027extraction en fonction de la précision pour un support minimum de 0.37. On constate que le temps d\u0027extraction atteint une valeur limite qui correspond à la valeur maximale utile des trois paramètres de contraintes de temps. \nConclusion et perspectives\nLes motifs séquentiels généralisés présentés par Srikant et Agrawal (1996) permettent une définition plus large de l\u0027inclusion en introduisant l\u0027utilisation de contraintes de temps. Toutefois, cette définition reste encore trop rigide, notamment dans le cas où l\u0027utilisateur n\u0027a qu\u0027une vague idée des contraintes temporelles qui lient ses données. Dans cet article nous proposons donc une extension des contraintes de temps pour les motifs séquentiels généralisés, qui permet plus de souplesse dans la spécification des paramètres de contraintes temporelles. Notre approche se base sur la construction de graphes de séquences pour intégrer les contraintes de temps dans le processus d\u0027extraction de motifs séquentiels. La faisabilité et la robustesse de cette méthode ont été montrées pour GTC dans Masseglia et al. (1999). Le principe de GETC étant le même, nous avons pu montrer son efficacité pour résoudre le probléme de la recherche de séquences généralisées avec des contraintes de temps, étendues ou non, et la similitude de son comportement avec celui de GTC. Nous avons également pu mettre en évidence la flexibilité offerte par la mise en place des contraintes étendues, il nous reste encore à en valider la robustesse. Enfin, nous envisageons d\u0027étendre les motifs séquentiels flous présentés dans Fiot et al. (2005b) à des motifs séquentiels généralisés, avec contraintes de temps étendues ou non.\n"
  },
  {
    "id": "1031",
    "text": "Introduction\nPour améliorer l\u0027efficacité des algorithmes de classification, il existe plusieurs algorithmes de préparation des données, dont la désuffixation. Cependant, le langage médical, et les comptes rendus hospitaliers sont rédigés dans un langage très technique, avec peu de formes flexionnelles. Nous nous sommes demandés si l\u0027implémentation d\u0027un algorithme de désuf-fixation dans ce contexte pouvait améliorer significativement les résultats obtenus. Nous avons mis en évidence qu\u0027il était possible d\u0027obtenir de meilleurs résultats que les algorithmes actuels d\u0027une part en développant un algorithme spécifique basé sur un large corpus de documents, d\u0027autre part en enrichissant ces derniers en fonction des racines lexicales des termes médicaux.\nPlusieurs algorithmes de désuffixation ont été proposés, les plus célèbres d\u0027entre eux étant Porter (1980), Lovins (1968) et Paice (1996. Malheureusement, il s\u0027agit d\u0027algorithmes de désuffixation pour la langue anglaise, dont les dérivés morphologiques se prêtent facilement à ce type d\u0027adaptation.\nPrésentation de l\u0027algorithme EDA et résultats\nAfin d\u0027améliorer les performances des algorithmes de classification de comptes rendus hospitaliers (projet Rhea), nous proposons une technique de désuffixation qui donne des résultats intéressants dans le contexte médical. Nous nous sommes constitué une base de 29 393 comptes rendus, tous utilisés dans cette étude. Par ailleurs, la terminologie médicale possède une structure sémantique forte. Jujols (1991).\nL\u0027algorithme EDA fonctionne en deux phases. La première phase consiste à préparer le mot en appliquant quelques modifications (transformation en minuscules, séparation des caractères ligaturés, suppression des signes diacritiques, etc.). La seconde phase consiste à enrichir le corpus de textes en fonction des structures sémantiques des termes (par exemple : foie\u003dhépat, langue\u003dglosso, rate\u003dspléno, coeur\u003dcardio,…).\nPour expérimenter nos résultats, nous avons choisi d\u0027utiliser Naïve Bayes comme algorithme de classification, et la F-mesure pour l\u0027évaluation. Ce qui donne les résultats suivants :\nDésuffixation\nRésultat (F-mesure) Aucune désuffixation 69.23% Désuffixation avec Carry 72.27% Désuffixation avec EDA 74.72%\nTAB. 1 -Gains sur la F-mesure selon la méthode utilisée.\nConclusion et perspectives\nSur 25 275 termes différents présents dans 30 000 comptes rendus, 10 602 ont été regroupés, soit 42%. L\u0027utilisation de cet algorithme de désuffixation nous a permis de mesurer une amélioration de 5.49 %. Les deux tiers du gain résultent de la désuffixation, le dernier tiers de l\u0027enrichissement des documents par la recherche de racines lexicales des termes mé-dicaux.\n"
  },
  {
    "id": "1032",
    "text": "Introduction\nNotre travail s\u0027inscrit dans le contexte du projet européen SEMIDE (Système euro mé-diterranéen d\u0027information sur les savoir-faire dans le domaine de l\u0027eau). Le SEMIDE vise à développer une ontologie spécifique aux connaissances dans le domaine de l\u0027eau. Ce travail s\u0027est basé dans un premier temps sur un thésaurus du domaine de l\u0027eau, or les ressources d\u0027informations ne cessent de s\u0027accroître de sources hétérogènes dans les formats, mais aussi dans le vocabulaire employé (agences de l\u0027eau, ministères,...) engendrant une ontologie insuffisante et peu structurée. Cette ontologie doit pouvoir s\u0027enrichir au fur et à mesure que de nouveaux documents apparaissent, mais également rester cohérente.\nNous nous intéressons à deux grandes parties : lŠannotation des ressources et l\u0027enrichissement de l\u0027ontologie globale définie par la communauté du SEMIDE. Ces deux grandes parties ne sont pas indépendantes étant donné que l\u0027enrichissement de l\u0027ontologie est fonction des nouvelles ressources et des concepts obtenus lors de l\u0027annotation. La suite de cet article traitera la deuxième partie.\nNotre hypothèse est qu\u0027il serait intéressant de rajouter des relations ontologiques (est-un, partie-de, etc.) à l\u0027ontologie du SEMIDE. Celle-ci prendrait donc la forme d\u0027un pseudo-réseau sémantique ou les noeuds seraient des acceptions. Cependant, nous ne concevons la mise en place d\u0027un tel réseau sémantique que via une automatisation poussée. La validation de certaines occurrences de relations entre acceptions pouvant être éventuellement l\u0027objet d\u0027un travail manuel d\u0027un expert. Cette automatisation peut être envisagée à partir de deux types de sources : des corpus monolingues d\u0027un même domaine technique, et des collections de bi (ou tri)-textes (textes traductions l\u0027un de l\u0027autres). Ce faisant, les occurrences de relations doivent d\u0027abord être identifiées dans les parties monolingues avant d\u0027être migrées dans la partie interlingue.\nNous attaquons le problème de l\u0027enrichissement ontologique selon deux biais. La premier, via l\u0027exploitation de paires de textes traduits, est la mise en correspondance directe de terme identifiés contre traduction mutuelle. Une acception (un sens de mot) peut être artificiellement créée, mais le problème des doublons potentiels et de l\u0027identification et élimination n\u0027est pas directement résolu. La seconde approche, à partir de corpus monolingue, consiste pour des termes cibles, à extraire le plus grand nombre des relations qu\u0027ils peuvent entretenir avec d\u0027autres mots. Les termes cibles sont identifiés comme tels via des méthodes classique de -709 -RNTI-E-6 \nExtraction de nouvelles relations -patrons d\u0027extraction\nNotre travail a consisté dans un premier temps à analyser des documents du Semide afin d\u0027extraire des mots clés qui définiront nos règles d\u0027extraction, cette analyse a donné une liste d\u0027hypothèses d\u0027extraction de relations entre les termes que nous définissons dans ce qui suit. Hypothèse 1 : Si l\u0027expression A est un B où A appartient à l\u0027ontologie du Semide alors B est une spécialisation de A dans l\u0027ontologie. Si par ailleurs, B appartient à l\u0027ontologie globale alors B est une généralisation de A. Hypothèse 2 : Si l\u0027expression C qui a la forme suivante : A de B où A appartient à l\u0027ontologie du Semide alors C est une spécialisation de A dans l\u0027ontologie. Si, par ailleurs, C appartient à l\u0027ontologie globale alors A est une généralisation de C. Hypothèse 3 : Si l\u0027expression C qui a la forme suivante : A B où A appartient à l\u0027ontologie du Semide alors C est une spécialisation de A dans l\u0027ontologie. Si par ailleurs, C appartient à l\u0027ontologie globale alors A est une généralisation de C. Hypothèse 4 : Si on a l\u0027expression C avec la forme suivante A non B où A appartient à l\u0027ontologie du Semide alors C est une spécialisation de A dans l\u0027ontologie. Et si C appartient à l\u0027ontologie globale alors A est une généralisation de C. Les quelques patrons d\u0027extraction présentés ci-dessus ne sont qu\u0027indicatifs de la méthode employée. D\u0027autres patrons sont utilisés, en particulier pour extraire des relations d\u0027autres natures. Par exemple, la relation de méronymie (partie de) est extraite des corpus afin de structurer l\u0027ontologie, et de déterminer le plus finement possible les cas de doublons. Les doublons sont des termes identifiés comme des concepts synonymes et doivent être représentés comme tels dans l\u0027ontologie.\nSummary\nThe description of resources inside a community (or domain) must follow a controlled vocabulary. This is precisely a set of terms defined by a working group in order to tag contents and describe documents. Our problem at hand is slightly different from classical issues in controlled vocabulary as we focus ourselves on relations that may exist between concepts. Still, our resource description is based on ontology. The ontology is the backbone of a controlled and organized vocabulary and corresponds to the formalization of explicit relations created between terms of the vocabulary. Our work sticks to two main directions which are the resources annotations and the global ontology enhancement as defined by the SEMIDE community. The EMWIS (SEMIDE) is an organization viewed as a tool for exchanging information and knowledge on water between countries of the Euro-Mediterranean Partnership.\n"
  },
  {
    "id": "1033",
    "text": "Introduction\nIl est communément admis que le temps de préparation des données peut occuper jusqu\u0027à 80% du temps lors d\u0027un projet industriel de fouille de données (Pyle, 1999). L\u0027hétérogénéité des sources, la présence de valeurs manquantes, les erreurs de saisie ou de calcul, les pannes de capteurs, une mauvaise fusion de données sont autant de causes qui peuvent introduire erreurs et incohérences dans une table de données. ESIEA Datalab est une plateforme évolu-tive programmée en Java qui met à disposition de nombreux outils pour aider à la détection d\u0027incohérences, la correction d\u0027erreurs, la transformation ou la contrainte de variables, etc.\nLe concept du logiciel\nLe nettoyage et la préparation de données peuvent être vus sous la forme d\u0027un processus représenté par la figure 1.\nFIG. 1 -Le nettoyage et la préparation de données vus comme un processus.\nLe logiciel n\u0027impose pas ce processus à l\u0027utilisateur, mais fournit tous les outils nécessai-res à sa réalisation. En parallèle, le nettoyage et la préparation des données sont tracés dans\nESIEA Datalab, un logiciel de nettoyage et préparation de données la console afin de pouvoir retrouver toutes les transformations et modifications effectuées sur les données et des agents fonctionnent en tâche de fond pour faire des suggestions et orienter l\u0027utilisateur.\nLes outils\nOutre un vaste ensemble d\u0027outils classiques, dans lesquels les algorithmes utilisés ont été adaptés à un contexte où toute valeur peut être manquante ou bien en erreur, ESIEA Datalab possède quelques outils originaux puissants qui permettent de traiter facilement des cas difficiles de nettoyage ou d\u0027offrir des moyens de visualisation intéressants.\nType structuré. Grâce à la notion de type structuré, le logiciel est capable de détecter des erreurs dans des données symboliques possédant une structure. Une fois la structure d\u0027une colonne spécifiée ou inférée, on peut contraindre les éléments de la structure à l\u0027aide de formules et mettre ainsi en erreur les valeurs ne respectant pas l\u0027une des contraintes.\nOutils de visualisation.\nParmi les outils de visualisation disponibles, ESIEA Datalab dispose de graphiques interactifs (matrice de nuages de points, coordonnées parallèles, etc.) qui permettent la sélection de valeurs et la réalisation d\u0027actions sur celles-ci. On trouve aussi des outils originaux comme la carte « vue d\u0027avion ». C\u0027est un graphique qui représente dans une forme condensée toute une table, que l\u0027on va utiliser avec des filtres qui vont colorer une sélection de valeurs. On a ainsi une vision totale de la table qui peut par exemple nous aider à estimer la densité des valeurs manquantes ou bien détecter des motifs.\nConclusion\nESIEA Datalab est un logiciel évolutif dont la simplicité d\u0027utilisation des outils et les fonctionnalités adaptées permettent d\u0027obtenir un gain de temps important sur le nettoyage et la préparation des données. Plusieurs améliorations sont en projet, notamment l\u0027ajout d\u0027une passerelle vers la librairie Java WEKA (Witten et Eibe, 2005 \nSummary\nESIEA Datalab is an evolvable Java software program which goal is to clean and prepare data before an analysis. The software looks like a toolbox ready to use, including some interactive visualisation tools, suggestion agents and advanced functionalities implementing Data Mining algorithms.\n"
  },
  {
    "id": "1035",
    "text": "Introduction\nLe problème abordé dans le cadre de cet article est celui de l\u0027accès à une base de connaissances annotée sémantiquement par une ontologie du domaine.\nLes connaissances peuvent être de natures diverses : documents scientifiques et techniques, fiches de retour d\u0027expérience, descriptions de compétences, documents multimédias, etc.. L\u0027utilisation d\u0027une ontologie 2 du domaine permet d\u0027indexer et de classer les éléments de la base de connaissances. L\u0027indexation repose sur l\u0027analyse des contenus textuels (et péri textes ou méta données dans le cas des documents multimédias) au regard du vocabulaire associé à l\u0027ontologie. La classification considère les concepts de l\u0027ontologie comme autant de répertoires virtuels auxquels sont associés les éléments de la base de connaissances.\nPar accès nous entendons la navigation -accès et recherche -guidée par la modélisation du domaine. Nous nous intéresserons plus particulièrement dans le cadre de ce travail à la navigation sous la forme de visualisations interactives. Nous parlerons alors de « cartes sémantiques interactives » dans la mesure où l\u0027on souhaite aider l\u0027utilisateur dans ses accès à la base de connaissances en exploitant les concepts du domaine et leurs relations.\nCette approche relève du « web sémantique » dans la mesure où à chaque information est associée un URI (Uniform Resource Identifier) et que la gestion des informations repose sur une conceptualisation distincte représentée à l\u0027aide de formalismes d\u0027échange du consortium W3C (RDF, RDF Schema, OWL).\nNotre objectif a été dans un premier temps d\u0027étudier et d\u0027évaluer les différents paradigmes de visualisations interactifs pour l\u0027accès et la recherche d\u0027informations annotées sémantiquement. Pour cela, nous avons au préalable défini avec l\u0027aide des utilisateurs un certain nombre de critères d\u0027évaluation comme la capacité à pouvoir se focaliser sur une partie de l\u0027ontologie ou bien encore, la facilité de parcours des liens hiérarchiques tout en gardant une vision globale de la structure de la base de connaissances. Nous avons été ainsi amenés à réaliser plusieurs cartes interactives. L\u0027analyse des retours d\u0027expérience nous a permis d\u0027identifier et de rajouter de nouvelles fonctionnalités, telle que la capacité à parcourir un ensemble d\u0027informations dans un espace non uniforme tout en gardant un point fixe, et nous a permis de spécifier un nouveau type de navigateur dédié à la gestion de bases de connaissances annotée sémantiquement : le « Eye Tree », navigateur de type « polar fisheye view ».\nDomaine d\u0027Application\nContexte\nLe « Groupement pour la Recherche sur les Échangeurs Thermiques » (GRETh) a mis en place un site internet pour la diffusion des connaissances et des informations scientifiques et techniques au service de leurs adhérents, principalement des industriels. Ces informations, articles, thèses, rapports techniques et scientifiques, sont regroupées au sein d\u0027une base de données. Tous ces documents se rapportent aux métiers du GRETh, basés sur la mécanique des fluides et la thermique des échangeurs.\nBesoins\nL\u0027objectif du GRETh est de pouvoir accéder à sa base documentaire, non pas en fonction de mots-clés présents dans les documents, mais selon la modélisation du domaine définie par les experts en termes de concepts métier. L\u0027accès à la base de connaissances doit également gérer le multilinguisme.\nLa modélisation du domaine, c\u0027est-à-dire la représentation des concepts, de leurs relations et de leurs propriétés, a abouti à la construction d\u0027une ontologie spécifique au GRETh. Les concepts étant ici communs et partagés par les différentes communautés, il a été possible d\u0027indexer l\u0027ensemble des documents, quelle que soit leur langue, sur la même ontologie. L\u0027indexation des documents est effectuée de façon automatique par une analyse linguistique multilingue de leur contenu 3 . L\u0027ontologie du GRETh a été réalisée en privilégiant la relation hiérarchique de « généralisation -spécialisation » entre concepts, en considérant qu\u0027elle est simple et non multiple. Les concepts se structurent ainsi sous la forme d\u0027une arborescence.\nLes besoins des utilisateurs peuvent alors se résumer de la façon suivante : comment d\u0027une part appréhender l\u0027ensemble des informations à travers les concepts métier relatifs à la mécanique des fluides et à la thermique des échangeurs ; et comment d\u0027autre part accéder à ces informations en parcourant l\u0027ontologie selon la relation hiérarchique de « généralisation -spécialisation ».\nNous sommes donc ramenés à un problème de construction de « cartographies sémantiques interactives » d\u0027une arborescence de concepts.\nMéthode et critères d\u0027évaluation\nNotre approche repose principalement sur la prise en compte des retours d\u0027expérience (Plaisant 2004)  De l\u0027expression des besoins nous avons pu identifier, pour notre problématique, trois critères d\u0027évaluation des différents paradigmes de visualisation à base d\u0027arborescence de concepts.\n1. Visualisation de l\u0027organisation des concepts : Dans la mesure où la conceptualisation du domaine joue un rôle central dans l\u0027accès aux connaissances, il est important de pouvoir visualiser la structure globale de l\u0027ontologie, et ce quelle que soit sa taille. Étant donné que nous privilégions la relation hiérarchique de « généralisation -spécialisation », il est important que la disposition des concepts dans la carte respecte le mieux possible cette sémantique et ce dans un espace qui peut être réduit. La métaphore graphique à utiliser doit donc exprimer au mieux cette sémantique. 2. Association d\u0027informations aux concepts : À chaque concept sont associées une liste de documents et une liste de termes. Il est donc nécessaire de pouvoir accéder et visualiser ces informations. La représentation des noeuds, en termes de variables graphiques comme la taille, la forme ou la couleur (Bertin et Barbut 1967), doit être porteuse de sens. Un utilisateur doit pouvoir accéder rapidement et intuitivement aux informations associées à un concept. 3. Interaction \u0026 navigation : L\u0027utilisateur doit pouvoir naviguer au sein de son espace informationnel sans se perdre. À tout moment il doit pouvoir se localiser et identifier où il doit aller. Nous avons ensuite mis en oeuvre les principaux paradigmes connus (par exemple les arbres hyperboliques, les « Treemaps », …) pour les soumettre aux utilisateurs afin d\u0027identifier les caractéristiques essentielles à prendre en compte.\nTechniques de visualisation\nPour le domaine technique considéré, celui des échangeurs thermiques, il nous a été demandé de réaliser différents navigateurs graphiques d\u0027accès aux documents techniques en s\u0027appuyant sur la modélisation du domaine (c\u0027est-à-dire de pouvoir parcourir l\u0027ensemble de la base en suivant les liens hiérarchiques de « généralisation -spécialisation » entre concepts). Un concept peut ainsi être interprété comme un « répertoire » contenant les documents qui s\u0027y réfèrent.\nÉtant donné la volonté de privilégier la relation de « généralisation -spécialisation », nous avons retenu les techniques graphiques de type « noeud -lien » appliquées aux données hiérarchiques. En effet, ces techniques ont l\u0027avantage de représenter explicitement la structure de l\u0027arbre et par conséquent elles expriment mieux la sémantique recherchée.\nCette contrainte nous a donc amené à écarter des techniques de type « space-filling » (Baker et Eick 1995)  Cette visualisation exploite : -une structure d\u0027arbre dépliable pour représenter une hiérarchie de répertoires ; -des icônes de dossier pour représenter les répertoires ; -différentes icônes pour représenter les fichiers.\nGénéralement, pour la gestion de fichiers, la vue est découpée verticalement en deux avec à gauche la hiérarchie des répertoires et à droite, une zone pour afficher le contenu du répertoire.\nRetour d\u0027expérience. Cette technique est directement appropriable par l\u0027utilisateur : les répertoires sont étiquetés par les noms des concepts et le déploiement d\u0027un noeud en noeuds plus spécifiques correspond bien à une interprétation naturelle de la relation de spécialisation. De plus, elle permet d\u0027associer aux noeuds un nombre important d\u0027informations qui peuvent être visualisées dans une zone dédiée (par exemple liste de documents).\nEnfin, les interactions et la navigation au sein de l\u0027arbre sont faciles et efficaces et l\u0027utilisateur maîtrise son parcours qui reste visible à tout moment. Ceci est principalement dû au fait que les utilisateurs sont habitués à ce type de représentation.\nCependant, dans le cadre d\u0027applications concrètes où les ontologies peuvent être de taille importante, il devient difficile d\u0027avoir une vue globale de la structure de l\u0027arbre, a fortiori s\u0027il est complètement déplié. L\u0027utilisateur a alors du mal à naviguer au sein de la base de connaissances.\nPrototype 2 : Les arbres de cônes\nPrincipe. Afin de palier à la critique émise sur les simples « treeviews », nous avons réalisé un deuxième navigateur à base d\u0027 «arbres de cônes ». Les arbres de cônes (Robertson et al. 1991), tout comme les simples « treeviews » sont des arbres de type « noeud-lien ». Le principe consiste à dessiner l\u0027ensemble de la hiérarchie en 3 dimensions (et non une vue partielle). Chaque noeud constitue le sommet d\u0027un cône dont les fils se répartissent sur un cercle qui en constitue la base (cf . FIG. 1 -Arbres de cônes).\nRetour d\u0027expérience. Si une telle visualisation donne un aperçu global de la structure de l\u0027arborescence en termes de répartition des concepts, et semble séduisante par son interactivité, l\u0027utilisateur est confronté à un phénomène d\u0027occlusion et l\u0027accès aux noeuds cachés par la structure nécessite de nombreuses manipulations de l\u0027arbre.\nLe parcours de la relation de généralisation -spécialisation est complexe. L\u0027utilisateur n\u0027est pas habitué à évoluer dans un espace informationnel à trois dimensions et se perd rapidement à l\u0027intérieur d\u0027un tel espace. L\u0027effort cognitif est important et la prise en main de l\u0027outil nécessite un long apprentissage.\nFIG. 1 -Arbres de cônes.\nPrototype 6 : Arbres hyperboliques\nPrincipe. L\u0027idée ici n\u0027est plus de vouloir visualiser de manière uniforme tous les noeuds, mais d\u0027en visualiser certains de façon claire tout en permettant l\u0027accès aux autres noeuds. Les arbres hyperboliques (Lamping et al. 1995) utilise une technique graphique de vue non uniforme de type « fisheye » (Furnas 1981;Sarkar et Brown 1992;Leung et Apperley 1994) qui permet de placer dans la vue un nombre important de noeuds.\nCette vue utilise une géométrie non euclidienne : la géométrie hyperbolique. La représentation de la hiérarchie des concepts est alors un arbre radial placé sur un plan hyperbolique. Grâce à la géométrie de ce plan, l\u0027utilisateur à l\u0027impression que la taille des noeuds et la distance entre chaque noeud sont inversement proportionnelles à leur distance au centre du disque. Ainsi, les noeuds sont toujours visibles sinon accessibles et il suffit à l\u0027utilisateur de glisser au centre ceux qu\u0027il souhaite voir plus en détails.\nOn obtient ainsi une vue de type « focus + context » (Card et al. 1999) où le focus est toujours au centre du disque.\nFIG. 2 -Arbre Hyperbolique.\nIl existe des variantes en trois dimensions (Munzner et Burchard 1995;Hughes et al. 2004) mais elles ont l\u0027inconvénient d\u0027apporter des effets d\u0027occlusion éliminant ainsi l\u0027apport de « vision globale » de la vue en deux dimensions.\nRetour d\u0027expérience. Si de prime abord la forte interactivité des arbres hyperboliques séduit, elle souffre de plusieurs défauts qui peuvent en limiter sa réelle utilisation. Dû aux effets de la déformation, les étiquettes associées aux noeuds ne sont pas alignées et parfois se superposent. Mais c\u0027est principalement son utilisation qui pose problème. En effet, lors de la manipulation de la structure, les éléments à la frontière de l\u0027espace de visualisation se retrouvent projetés de façon « imprévisible ». Ces effets ont tendance à perturber l\u0027utilisateur qui cherche en permanence à rétablir la situation engendrant un effort cognitif plus important et une prise en main assez délicate.\nCes effets de projection sont dus à la géométrie utilisée. En effet, les éléments sont représentés dans un plan hyperbolique qui n\u0027est pas commun à nos sens. C\u0027est pourquoi, le résultat des transformations appliquées au plan n\u0027est pas prévisible « naturellement ».\n5 Notre proposition : le paradigme d\u0027 « Eye Tree » Les retours d\u0027expérience de l\u0027utilisation de ces différents types de navigation nous ont permis d\u0027identifier, dans le cadre de notre application, quatre critères principaux pour la réalisation d\u0027une carte sémantique interactive :\nutiliser une technique de type « focus + context » pour permettre à l\u0027utilisateur de se concentrer sur certains éléments tout en facilitant l\u0027accès aux autres éléments ; -utiliser une géométrie euclidienne pour ne pas perturber la perception naturelle des manipulations du plan ; -proposer une vue globale de l\u0027ontologie permettant à l\u0027utilisateur de facilement appréhender l\u0027ensemble des concepts du domaine ; -pouvoir parcourir la base de connaissances tout en gardant un point fixe de référence. Forts de ses résultats nous avons été amenés à définir un nouveau paradigme basé sur une technique de visualisation de type « fisheye » avec un plan qui possède une géométrie euclidienne : la technique « Polar Fisheye View » (Sarkar et Brown 1992) prenant en compte les critères précédents.\nCette technique fait partie des techniques de représentation avec déformation (Leung et Apperley 1994). Pour cela, les noeuds sont répartis radialement dans l\u0027espace euclidien avant de subir une transformation via une fonction d\u0027amplification continue appliquée aux coordonnées polaires des noeuds.\nLa figure suivante illustre les opérations opérées sur le plan avant de le visualiser :\nLe résultat ressemble aux arbres hyperboliques, mais les interactions de l\u0027utilisateur (par exemple translations) sont appliquées sur un plan euclidien. Elles sont donc « naturellement » prévisibles par l\u0027utilisateur. La transformation étant linéaire, le résultat n\u0027est pas perturbant pour les utilisateurs tout comme pour les « Perspective Wall » (Jock et al. 1991).\nVoici le résultat que nous avons obtenu avec un noeud sélectionné et la liste des documents associés : -294 -RNTI-E-6\nLe plan donne l\u0027impression d\u0027être projeté sur une sphère. Lorsque l\u0027utilisateur manipule l\u0027arbre, il a l\u0027impression de déplacer le plan sur la sphère. Le tout donne l\u0027illusion d\u0027un oeil d\u0027où le nom : « Eye Tree ».\nL\u0027Eye Tree permet aussi d\u0027avoir une vue globale de la structure en faisant varier la force de la fonction d\u0027amplification (plus la force est importante, plus les éléments sont ramenés vers le centre) et la distance entre les noeuds à l\u0027aide de deux curseurs.\nFIG. 5 -Vue globale de la structure.\nEnfin, les utilisateurs, face aux problèmes que pose l\u0027utilisation des arbres hyperboliques et en particulier face au fait que toute modification locale entraîne des perturbations globales, ont exprimé la possibilité de pouvoir parcourir un ensemble d\u0027éléments au sein d\u0027un espace avec déformation tout en conservant une référence par rapport à un point fixe, en particulier par rapport à la racine de l\u0027ontologie. Nous avons pour cela introduit les rotations du plan euclidien avec pour centre la racine de la structure hiérarchique. Cette interaction a pour conséquence de faire défiler tous les éléments du même niveau par rapport à un point fixe. Ainsi pour la recherche, il est possible de parcourir avec une seule interaction l\u0027ensemble des sous éléments d\u0027un élément donné (en accord avec le modèle MVC (Krasner et Pope 1988), ces rotations sont associées aux événements de la souris correspondant aux actions de la molette).\nConclusion\nLe choix d\u0027un paradigme de visualisation nécessite de définir au préalable des critères d\u0027évaluation en fonction du type d\u0027application et des attentes des utilisateurs.\nLa problématique de l\u0027exploration de bases documentaires techniques guidée par une ontologie de domaine impose un certain nombre de contraintes. Ces contraintes ont été identifiées suite aux retours d\u0027expérience de l\u0027utilisation de différents modes de visualisation. Ainsi, dans la mesure où l\u0027on se focalise sur les concepts et la relation hiérarchique de « généralisation -spécialisation », les visualisations de type « noeud -lien » sont à privilégier par rapport à des techniques de type « space -filling ».\nDe même, l\u0027utilisation d\u0027une géométrie euclidienne, à l\u0027inverse des géométries hyperboliques, permet de ne pas perturber la perception naturelle des manipulations du plan. Ce point est important dans la mesure où une ontologie est davantage qu\u0027un simple réseau de noeuds : la distribution des concepts doit rester constante dans leur affichage.\nEnfin, la visualisation d\u0027ontologies importantes nécessite une approche de type « focus + context » qui permet de focaliser l\u0027affichage sur certains noeuds tout en permettant l\u0027accès aux autres noeuds. L\u0027ensemble de ces considérations a permis de spécifier et de réaliser un nouveau navigateur dédié à la gestion de documents techniques annotés par une ontologie de domaine : le « Eye Tree ». Ce navigateur de type « polar fisheye view » (focus+ context avec déformations linéaires) permet des interactions dédiées à l\u0027exploration d\u0027ontologies (parcours de sous éléments par rapport à un point fixe).\nPour être en accord avec le mantra de Shneiderman (\"Overview first, zoom and filter, then details on demand\") (Shneiderman 1996), l\u0027accès à ce type de base de connaissances nécessite de combiner une approche globale puis une approche locale. C\u0027est pourquoi, nos prochains travaux porteront sur l\u0027intégration à l\u0027Eye Tree de paradigmes de type « treeview simple » pour permettre à l\u0027utilisateur de se focaliser sur une partie de l\u0027arborescence. De plus, pour valider ces travaux, nous élaborerons une expérimentation pour compléter nos retours d\u0027expérience par des résultats quantifiables.\n"
  },
  {
    "id": "1037",
    "text": "Introduction\nAujourd\u0027hui, la lecture automatique des documents manuscrits se limite à quelques cas applicatifs particuliers : lecture automatique de chèques ou d\u0027adresses postales, reconnaissance des champs d\u0027un formulaire. Cette lecture est possible car le contenu de ces documents est très largement contraint : structure du document stable, position des informations connue, redondance de l\u0027information, lexique limité, etc. Lors de la lecture, le système bénéficie ainsi d\u0027informations a priori importantes permettant de limiter ou de vérifier les hypothèses de reconnaissance, autorisant une lecture fiable des documents.\nPeu de travaux abordent des problèmes de reconnaissance moins contraints car il est alors plus difficile de bénéficier de moyens automatiques de vérification des hypothèses de reconnaissance. C\u0027est le contexte de nos travaux portant sur la lecture automatique des courriers entrants manuscrits. Il s\u0027agit de courriers manuscrits tels que des lettres de réclamation, de changement d\u0027adresse, de modification de contrat, etc., reçus en très grand nombre quotidiennement par des grandes organisations. Contrairement aux applications précédemment citées, aucune information a priori n\u0027est disponible : le contenu, la structure, l\u0027expéditeur ou encore l\u0027objet du document sont totalement inconnus du système de lecture, ce qui rend la lecture intégrale du document extrêmement délicate. Il est cependant possible de considérer des problèmes de lecture partielle du document, visant à en extraire l\u0027information pertinente. C\u0027est ce que nous envisageons dans cet article en proposant une méthode de localisation et de reconnaissance de champs numériques (numéros de téléphones, codes clients, etc.) dans des courriers entrants manuscrits (voir figure 1). La reconnaissance de ces champs permettra par -23 -RNTI-E-6 exemple d\u0027identifier l\u0027expéditeur par le biais du numéro de téléphone, ou de déterminer le type de contrat à l\u0027aide du code client, ce qui autorise un acheminement du courrier vers le service concerné au sein de l\u0027organisation.\nFIG. 1 -Exemple de courriers manuscrits où les champs numériques à extraire sont encadrés.\nLa méthode présentée comporte trois grandes étapes : -Une première étape de localisation rapide sans reconnaissance chiffre ni segmentation permet d\u0027extraire des séquences de composantes susceptibles de constituer des champs numériques. Cette étape basée sur l\u0027exploitation de la syntaxe connue des champs a déjà été présentée dans Koch et al. (2004) puis ameliorée dans Chatelain et al. (2004). Nous la décrivons donc sommairement dans cet article et rappelons ses performances pour justifier les deux étapes de traitement suivantes. -La deuxième étape consiste à soumettre les hypothèses de localisation à un module de reconnaissance de champs fournissant leur valeur numérique. Cette étape repose sur l\u0027utilisation d\u0027un classifieur chiffre et d\u0027un module de segmentation de chiffres liés. -La troisième étape consiste à traiter le problème des fausses alarmes générées par l\u0027étape de localisation. Nous présentons un module de vérification qui accepte ou rejette les hypothèses de champ numérique en exploitant une combinaison d\u0027informations provenant des différentes étapes de traitement. L\u0027article est organisé de la manière suivante : la section 2 décrit sommairement la mé-thode d\u0027extraction des champs dans les documents manuscrits libres. La section 3 présente la méthode de reconnaissance des champs basée sur un classifieur chiffre et une méthode de reconnaissance de chiffres liés. Nous présentons dans la partie 4 les performances de notre -24 -RNTI-E-6 système, ainsi qu\u0027une étape de vérification des hypothèses de reconnaissance des champs afin de rejeter les fausses alarmes.\nLocalisation des champs\n2.1 Une approche \"dirigée par la syntaxe\" L\u0027approche proposée ici pour la localisation des champs est basée sur une modélisation markovienne d\u0027une ligne de texte. Nous avons déjà eu l\u0027occasion de présenter cette approche dans Koch et al. (2004); Chatelain et al. (2004). Rappelons seulement que ce modèle exploite la syntaxe spécifique des champs numériques que l\u0027on souhaite extraire (nombre de chiffres, présence et position de séparateurs...) pour parvenir à localiser les séquences numériques, sans toutefois procéder à la segmentation des composantes connexes ni à la reconnaissance des chiffres. Nous interprétons globalement la séquence des composantes connexes de chaque ligne pour associer à chaque composante son étiquette : textuelle ou numérique. Toutefois, puisque l\u0027approche ne procède pas à la segmentation des composantes connexes, une composante numérique peut correspondre à un ou plusieurs chiffres, ou même un séparateur (point, tiret...). De ce fait, on doit introduire dans le modèle de ligne des étiquettes correspondant à ces situations : D (Digit ou chiffre), DD (Double Digits ou chiffres liés), S (Séparateur). En ce qui concerne les composantes textuelles, le modèle ne comprend qu\u0027une seule classe, appelée classe Rejet, pour décrire l\u0027ensemble des situations possibles : caractère isolé, fragment de mot, mot, diacritique, signe de ponctuation. La figure 2 représente une ligne de texte avec les étiquettes associées à chacune des composantes qui la constitue.\nFIG. 2 -Exemple d\u0027étiquetage des composantes d\u0027une ligne comprenant un code client.\nCes quatres classes constituent les états du modèle markovien. La construction des modèles se fait de la manière suivante : nous fixons le nombre d\u0027état Digit, Double Digit et Séparateur pour chaque type de champ, ainsi qu\u0027un état Rejet. La matrice des probabilités de transitions est déterminée par une estimation statistique sur une base annotée. La figure 3 montre les modèles de Markov ainsi construits, où les flèches entre les états représentent les probabilités de transition non nulles.\nL\u0027alignement des séquences de composantes reconnues sur ces modèles garantit de ne conserver que les séquences syntaxiquement correctes. L\u0027extraction des champs numériques dans les lignes de textes consiste alors à rechercher le meilleur alignement dans le treillis des hypothèses de classification. Ceci est réalisé par l\u0027algorithme de Viterbi Forney (1973).\nLe processus d\u0027extraction des champs repose donc sur les étapes suivantes : Segmentation en lignes : les lignes de texte sont extraites grâce à une approche de regroupement des composantes connexes inspirée de Likforman-Sulem et Faure (1995).\nClassification des composantes connexes : il s\u0027agit de classifier les composantes connexes de chaque ligne selon qu\u0027elles appartiennent à un champ numérique (Digit, DoubleDigit, Sé-parateur) ou non (Rejet). La caractérisation des composantes est réalisée à l\u0027aide de deux jeux -25 -RNTI-E-6 de caractéristiques, présentés à deux classifeurs de type MLP entrainés grâce à l\u0027algorithme de rétropropagation du gradient. Nous combinons ensuite les résultats des deux MLP. Analyse syntaxique : cette dernière étape permet d\u0027extraire les champs recherchés grâce à l\u0027analyse syntaxique des lignes de texte. L\u0027analyseur syntaxique corrige les éventuelles erreurs de classification de l\u0027étape précédente en alignant les hypothèses de reconnaissance sur un modèle markovien d\u0027une ligne de texte pouvant contenir un champ numérique.\nCette méthode d\u0027extraction des champs est une alternative intéressante à l\u0027utilisation d\u0027une stratégie de segmentation-reconnaissance sur l\u0027intégralité du document, puisque seuls les champs extraits seront soumis à un reconnaisseur.\nRésultats\nLes expérimentations ont été réalisées sur deux bases distinctes d\u0027images de courriers entrants manuscrits : la première (292 images) a été utilisée comme base d\u0027apprentissage pour la classification des composantes connexes ainsi que pour déterminer les probabilités de transition des modèles de Markov et pour paramétrer le système ; la seconde (293 documents) a servi à tester notre approche.\nLa détection des champs numériques est réalisée en effectuant l\u0027analyse de chaque ligne d\u0027un document. L\u0027analyseur syntaxique se prononce pour la présence (détection) ou l\u0027absence (rejet) d\u0027un champ sur la ligne en cours d\u0027analyse. Un champ est considéré comme convenablement détecté si et seulement si \"aucune composante du champ étiqueté n\u0027est rejetée et si toutes les composantes connexes dans le champ détecté appartiennent au champ étiqueté\".\nLe tableau 1 donne les taux de détection des champs en rang 1, 2 et 5. \nOn constate que suivant le type de champ, 70 à 80 % des champs sont détectés en première proposition. Ces résultats augmentent significativement lorsque l\u0027on considère les 2 ou 5 premières propositions de l\u0027analyseur. Les résultats sont meilleurs pour les champs qui possèdent une syntaxe plus contraignante tels que le numéro de téléphone et le code client (nombre de chiffres plus important, présence de séparateurs) que sur les champs faiblement contraints (codes postaux).\nLa majorité des champs ont ainsi été localisés, sans reconnaissance chiffre. L\u0027étape suivante consiste à soumettre les hypothèses de localisation des champs à un module de reconnaissance afin d\u0027obtenir leur valeur numérique.\nReconnaissance des champs\nContrairement à la majorité des systèmes de reconnaissance de documents manuscrits où la localisation et la reconnaissance des informations sont intimement liées, l\u0027exploitation de la connaissance a priori sur la syntaxe des champs nous a permis de localiser les champs numériques sans les reconnaître. La reconnaissance intervient donc en fin de traitement et permet la vérification des hypothèses de localisation.\nL\u0027étape de reconnaissance des champs numériques s\u0027appuie sur l\u0027exploitation des hypothèses de classification fournies lors de l\u0027étape de détection. En effet, nous bénéficions pour chaque champ extrait de l\u0027hypothèse de classification \"Digit\", \"Séparateur\" ou \"Double digit\" des composantes. Il s\u0027agit donc de déterminer l\u0027hypothèse de classification chiffre pour chacune de ces composantes (voir figure 4). Pour les composantes dont l\u0027hypothèse de classification est \"Digit\", il suffit de soumettre l\u0027imagette à un classifieur chiffre qui déterminera la meilleure hypothèse de classification \"chiffre\". La description du classifieur chiffre est présentée dans la section 3.1. Les composantes \"Séparateur\" sont ignorées lors de cette étape, puisqu\u0027elles n\u0027interviennent pas dans la valeur numérique du champ à reconnaître. La reconnaissance des composantes classifiées comme \"Double digit\" est effectuée de la manière suivante : comme nous savons que la composante contient deux chiffres liés, il nous faut trouver la meilleure segmentation des deux chiffres, et les reconnaître. Cette étape est présentée dans la section 3.2. Dans la section 3.3, nous présentons les résultats obtenus sur les champs numériques isolés.\nFIG. 4 -Détermination des hypothèses de classification chiffre à partir des hypothèses de classification\nClassifieur chiffre\nLa reconnaissance de chiffres isolés a bénéficié de très nombreux travaux ces dernières années, notamment dans le cadre de la reconnaissance de montants numériques de chèques, de champs numériques dans les formulaires, ou encore de reconnaissance de codes postaux dans les adresses postales Plamondon et Srihari (2000). Ces systèmes reposent sur l\u0027extraction de nombreuses caractéristiques Trier et al. (1996) et l\u0027utilisation de classifieurs performants Jain et al. (2000). Néanmoins, aucun extracteur ni classifieur n\u0027a pu montrer de supériorité incontestable par rapport aux autres. Partant de ce constat, il est intéressant d\u0027exploiter la complémentarité entre plusieurs classifieurs par une combinaison de type parallèle ou séquentielle. Nous avons ainsi choisi d\u0027effectuer une combinaison parallèle de deux classifieurs de type perceptron multicouche (ou \"MultiLayer Perceptron : MLP\") auxquels sont soumis deux vecteurs de caractéristiques : -Le vecteur de caractéristiques du chaincode extrait du contour des composantes a montré son efficacité dans de nombreux problèmes de reconnaissance Kimura et al. (1994 \nReconnaissance des chiffres liés\nNous avons présenté le classifieur chiffre permettant de reconnaître les chiffres isolés, nous nous intéressons dans cette partie à la reconnaissance des composantes dont l\u0027hypothèse de classification lors de la première étape est \"chiffres liés\" (DD). Une stratégie pour cette opération pourrait être la reconnaissance globale de la composante à l\u0027aide d\u0027un classifieur \"chiffres liés\" comportant autant de classes que de combinaison possibles, soit 100 (classifieur -28 -RNTI-E-6 100 classes [00..99]). Cette stratégie nécessite toutefois une base d\u0027apprentissage conséquente comportant un nombre suffisamment élevé d\u0027éléments dans chaque classe, afin de couvrir la variabilité inhérente à un problème réel : différents type d\u0027écriture, nature des liaisons entre les chiffres (liaisons hautes, basses, multiples), etc. La conception d\u0027un tel classifieur est donc a priori difficile à envisager. Nous avons donc orienté notre approche vers une segmentation de la composante pour identifier les deux chiffres qui la constituent. Dans la mesure où il est très difficile de déterminer sans reconnaissance le meilleur chemin de coupure pour séparer deux chiffres liés, nous avons mis en oeuvre une stratégie de segmentation-reconnaissance à l\u0027échelle de la composante. Plusieurs hypothèses de segmentation sont générées et soumises au classifieur chiffre qui se prononce sur les deux chiffres. Le choix de la meilleure hypothèse est déterminé à partir des confiances fournies par le classifieur. Cette stratégie repose donc sur la mise en oeuvre d\u0027un module de segmentation permettant la génération de plusieurs chemins de coupures, et sur un module de décision qui se prononce sur le choix du meilleur chemin de segmentation. Nous décrivons maitenant ces deux étapes.\nSegmentation des composantes\nIl existe de très nombreuses méthodes de segmentation explicite, généralement basées sur l\u0027analyse des contours Casey et Lecolinet (1996). Nous avons utilisé une méthode de segmentation inspirée de l\u0027algorithme \"drop fall\" Congedo et al. (1995), qui consiste à segmenter la composante selon le chemin emprunté par une goutte d\u0027eau qui coulerait selon les contours de la composante. Lorsque la goutte est boquée au fond d\u0027une vallée, celle-ci coupe la composante et continue sa chute. Cet algorithme permet de générer quatre chemins de coupures, suivant que la goutte descende ou qu\u0027elle monte, et suivant la direction prioritaire (gauche ou droite) qu\u0027on lui impose lorsqu\u0027elle rencontre un extrema (mont ou vallée). Ces quatre variantes fournissent généralement des chemins différents contenant au moins une bonne segmentation (voir figure 5).\nSélection du meilleur chemin de segmentation\nNous pouvons donc générer quatre chemins de coupures selon les variantes du drop fall présentées précédemment. Il s\u0027agit dans ce module de sélectionner le \"meilleur\" chemin parmi les hypothèses générées, décision pour laquelle il nous faut définir un critère fiable traduisant la qualité de la segmentation. Nous proposons de soumettre chaque paire de composantes segmentées à notre classifieur chiffre, et d\u0027utiliser comme critère le produit des scores de confiance associés aux propositions du classifieur pour les deux chiffres. En effet, si les chiffres liés sont bien segmentés, les confiances associées aux deux premières propositions seront élevées ; dans le cas contraire, les hypothèses de classification chiffre devraient voir leur score chuter. La figure 5 présente la segmentation et la reconnaissance d\u0027une composante \"double digit\" selon les quatre variantes du \"drop fall\" ; ici le drop fall ascendant gauche maximise le produit des confiances, cette hypothèse est donc conservée.\nLe taux de reconnaissance des chiffres liés est de 90% en première proposition sur une base étiquetée d\u0027environ 150 \"Double Digit\".\nLa reconnaissance de chiffres liés est évaluée sur une base étiquetée d\u0027environ 150 \"double digit\" extraits de séquences numériques. Une composante est comptabilisée comme bien re--29 -\nRNTI-E-6 connue si les deux chiffres qui la constituent sont bien classifiés. Le taux de reconnaissance obtenu sur cette base est de 90%.\nRésultats de la reconnaissance des champs isolés\nPour évaluer la reconnaissance des champs numériques, nous avons constitué une base d\u0027environ 500 champs isolés disposant de l\u0027étiquetage \"syntaxique\" (Digit, Séparateur, Double Digit), et annoté au niveau chiffre. La base provient de courriers entrants manuscrits réels, et les trois types de champ recherchés sont représentés (codes postaux, numéros de téléphone et codes clients). Nous ne comptabilisons comme bien reconnus que les champs dont toutes les composantes ont été bien reconnues au niveau chiffre. Le taux de reconnaissance au niveau champ est de 80%.\nPerformances du système\nRappel-précision du système\nL\u0027évaluation d\u0027un système d\u0027extraction d\u0027information se fait classiquement par la mesure du rappel et de la précision du système. Ces deux critères sont définis de la manière suivante : rappel \u003d nombre de champs bien reconnus / nombre de champs à reconnaître Le rappel traduit donc la capacité du système à localiser et reconnaître correctement tous les champs numériques d\u0027un document. précision \u003d nombre de champs bien reconnus / nombre de champs proposés par le système La précision indique la pertinence des résultats, c\u0027est-à-dire la capacité du système à ne proposer que des champs d\u0027intérêt et à limiter les fausses alarmes. En effet, notre système a tendance à proposer à l\u0027issue de la première étape (localisation sans reconnaissance) des séquences de composantes qui ne sont pas des champs. Ces \"fausses alarmes\" ont plusieurs origines : il peut s\u0027agir de séquences textuelles (détection d\u0027un champ dans une zone de texte en présence notamment de caractères bâtons) ; numériques et textuelles (défaut d\u0027alignement) ; ou même strictement numérique (détection d\u0027un champ dans un autre, défaut d\u0027alignement, ou erreur lors de l\u0027étape de reconnaissance chiffre sur un champ bien localisé).\nNous présentons sur la table 3 le compromis rappel-précision de notre système à l\u0027issue de la reconnaissance des hypothèses de localisation, en fonction du rang considéré. On constate que le système est capable de reconnaître de 54 à 63% des codes postaux, codes clients et numéros de téléphone des documents. La précision du système est en revanche relativement faible. Nous proposons donc un module de vérification permettant d\u0027accepter ou de rejeter les séquences de composantes reconnues.\nVérification des hypothèses de reconnaissance\nLe but de cette étape de vérification est d\u0027analyser les hypothèses de champs de manière à rejeter les fausses alarmes et à accepter les séquences numériques qui étaient effectivement à détecter. Ce module est basé sur l\u0027interprétation d\u0027un certain nombre d\u0027informations obtenues tout au long de la chaine de traitement, permettant d\u0027accepter ou de rejeter ces hypothèses. L\u0027étape de localisation fournit des scores d\u0027alignement des séquences de composantes sur les modèles markoviens traduisant la qualité de l\u0027alignement, l\u0027étape de reconnaissance fournit des scores de confiance permettant de déceler les éventuelles composantes non numériques. Ces scores, auxquels nous avons rajouté des informations sur la régularité des boites englobantes des composantes, constituent les caractéristiques d\u0027un vecteur soumis à un classifieur de type MLP, entrainé sur une base de champs numériques et de fausses alarmes. L\u0027unique sortie du classifieur se prononce sur l\u0027acceptation (sortie du MLP \u003e 0,5) ou le rejet (sortie \u003c 0,5) de l\u0027hypothèse de champ. Le MLP a été entrainé sur une base de 17000 séquences de composantes (16800 fausses alarmes et 200 véritables champs).\nNous décrivons maintenant le vecteur de 14 caractéristiques provenant des trois familles : caractéristiques issues de la localisation, de la reconnaissance, et des boites englobantes des composantes.\nCaractéristiques provenant de la localisation\nLors de l\u0027étape de localisation, l\u0027analyseur syntaxique fournit pour chaque ligne un score d\u0027alignement des composantes sur les modèles (voir figure 6). Ce score est une indication pré-cieuse sur la fiabilité de la localisation du champ et doit donc être retenu comme caractéristique dans notre vecteur. Lorsque le champ n\u0027est pas proposé en première solution par l\u0027analyseur syntaxique, nous remarquons que l\u0027écart entre les scores est généralement faible avec les premiers alignements. Nous avons donc retenu comme caractéristiques les écarts entre le score de l\u0027alignement du champ et les scores des autres alignements de la même ligne. L\u0027expérience montre que la bonne proposition n\u0027est jamais au delà de la cinquième proposition de l\u0027analyseur. Nous avons ainsi retenu 6 caractéristiques issues de l\u0027étape de localisation.\n-31 -\nRNTI-E-6 \nCaractéristiques provenant de la reconnaissance\nUne autre famille de caractéristiques pour la discrimination des fausses alarmes provient de l\u0027étape de reconnaissance. Partant de l\u0027hypothèse selon laquelle une séquence de composantes non numériques produit des confiances basses lors de l\u0027étape de reconnaissance (voir figure 7) \nCaractéristiques morphologiques\nL\u0027observation d\u0027un certain nombre de champs numériques et de fausses alarmes a montré que les boites englobantes des chiffres constituant un champ numérique présentent générale-ment des régularités que ne possèdent pas les fausses alarmes (voir figure 8). Nous avons donc ajouté dans le vecteur 5 caractéristiques traduisant la régularité dans la succession des boites englobantes :\nFIG. 8 -\n-L\u0027écart type des ordonnées minimum et maximum des chiffres -32 -RNTI-E-6 C. Chatelain et al.\n-L\u0027écart type des hauteur et largeur des chiffres -L\u0027écart type entre les abscisses des centres de gravité des chiffres Résultats à l\u0027issue de la vérification La figure 9 montre l\u0027évolution du rappel et de la précision du système avant et après l\u0027étape de vérification des hypothèses de champs numériques.\nFIG. 9 -Courbe rappel/précision du système avant et après vérification des hypothèses de reconnaissance.\nNous constatons que le rejet permet d\u0027améliorer considérablement la précision du système, pour tous les rang considérés. Le rappel du sytème est peu affecté par ce rejet pour le rang1, mais diminue de quelques points pour les rangs plus élevés.\nConclusion et perspectives\nDans le cadre du traitement automatique de courriers manuscrits, nous avons présenté une méthode d\u0027extraction des champs numériques dirigée par la syntaxe, et la méthode de reconnaissance associée. L\u0027intérêt de la méthode réside dans le fait qu\u0027elle utilise la syntaxe d\u0027un champ numérique comme infomation a priori pour le localiser. La reconnaissance des champs est largement contrainte par la méthode de localisation utilisée et permet de reconnaître plus de 60% des champs.\nNotons que l\u0027intégration d\u0027un tel système en milieu industriel pourra bénéficier d\u0027un certain nombre de connaissances a priori spécifiques aux types de champs recherchés (connaissances que nous n\u0027avons pas intégrées ici dans le processus de localisation) afin d\u0027en améliorer les performances et en particulier la précision. Par exemple, un numéro de téléphone commence toujours par un \u00270\u0027 ; les codes postaux se trouvent généralement dans la partie supérieure du document ; etc. Un module de mise en concurence des champs pourra également être développé pour éviter les fausses alarmes dues à l\u0027inclusion d\u0027un champ dans un autre.\nAfin de fiabiliser notre système, la mise en oeuvre de stratégies alternatives pour la localisation des champs pourra être effectuée. Il serait intéressant d\u0027appliquer des modèles de lignes intégrant les valeurs numériques des chiffres, afin de pouvoir prendre en compte les contraintes mentionnées précédemment (numéro de téléphone comencant par \"06\", etc.) dès la phase de localisation. Cette méthode impose la localisation de tous les chiffres dans le document : chiffres isolés et chiffres liés. Contrairement à la méthode présentée dans cet article, une -33 -RNTI-E-6\nExtraction automatique de champs numériques dans des documents manuscrits phase de segmentation des composantes est donc nécessaire. La clé du problème réside dans le contournement des stratégies classiques de sur-segmentation systématique des composantes qui entrainent une combinatoire très importante et donc des temps de traitement conséquents. Nous travaillons actuellement sur ce sujet. Les deux stratégies pourront ainsi être mise en concurrence afin de fiabiliser les hypothèses de localisation et de reconnaissance des champs.\n"
  },
  {
    "id": "1038",
    "text": "Introduction\nLa recherche d\u0027objets vidéo est une tâche difficile compte tenu de la richesse des informations multiples dans l\u0027image. Pour trouver de manière automatique ces objets vidéo, il est important de tenir compte de trois étapes principales qui sont la segmentation, l\u0027identification et le suivi d\u0027objets en mouvement par flot optique.\nLe but de la segmentation active est de détecter et d\u0027extraire des informations pertinentes dans une image. Différents modèles de contours actifs ont été proposés dans la littérature, mais on peut distinguer deux principales approches: Des approches basées contours et d\u0027autres basées régions. L\u0027implémentation de n\u0027importe quel modèle de contour actif exige la minimisation d\u0027une fonctionnelle d\u0027énergie. Cette énergie a deux composantes: énergie externe, qui est caractérisée par la régularité de la courbe et l\u0027énergie interne qui a pour fonction d\u0027attirer la courbe vers les gradients les plus forts (les forts contraste de l\u0027image).\nLes contours actifs classiques ont été proposés pour la première fois par Kass et al (Kass et al., 1987) pour la segmentation d\u0027images médicales. L\u0027idée de base consiste à faire évoluer la courbe vers la frontière de l\u0027objet à détecter. Ce modèle a été confronté à plusieurs contraintes, liées à l\u0027initialisation, au paramétrage et à l\u0027impossibilité de changement de topologie du snake. Une autre méthode a été introduite par Osher \u0026 Sethian (Sethian, 1999) connue par la méthode des ensembles de niveaux. Son principe consiste à faire évoluer une courbe initiale jusqu à ce qu elle détecte la forme de l\u0027objet à extraire. Ensuite, les contours actifs géodésiques (Caselles et al., 1997) ont été présentés comme une alternative géométri-que aux snakes, présentant l\u0027avantage d\u0027être indépendant du paramétrage. Les contours actifs basés régions adoptées par Barlaud et Jehan-Besson (Jehan-Besson et al., 2002). Ils utilisent des descripteurs statistiques des régions, de manière générale, on peut dire que cette méthode est efficace, quand l\u0027ensemble des objets à segmenter est homogène.\nLe flot optique se calcule entre deux images: c\u0027est le champs de vecteurs mouvement, rapportés aux pixels, pour passer d\u0027une image à l\u0027autre. Pour faire ce calcul plusieurs approches ont été proposés (Horn et al., 1981). Il existe aussi une autre classe de méthodes pour estimer le mouvement, telles que celles utilisées en compression: le bloc-matching (Koga et al., 1981).\nDans la section suivante nous présentons notre approche de segmentation basée région qui est une implémentation rapide d\u0027un modèle de contour actif qui permet de tenir compte d\u0027informations de couleur et de texture. Ensuite, nous abordons le problème général du flot optique et présentons notre implémentation de l\u0027estimation du mouvement par une approche basée sur la méthode d\u0027Horn \u0026 Schunck. Dans la section 4, nous proposons une méthode de mise en évidence (à partir de la détection et de suivi d\u0027objets visuels) en utilisant une approche mixte qui combine à la fois les contours actifs et le flot optique. La section 5 sera consacrée aux résultats obtenus à partir des différentes approches présentées, pour la segmentation des objets en mouvement sur une série d\u0027images vidéo. Enfin, nous terminons notre article par une conclusion en indiquant quelques améliorations possibles.\nSegmentation par contour actif\nNous avons amélioré la méthode de chan \u0026 vese  en utilisant une fonction générale de la gaussienne qui permet de mieux tenir compte des caractéristiques divers de texture et de couleur dans l\u0027image. Pour les images en couleur, on a choisi de travailler dans un espace de couleur perceptuel (tels que Lab), ainsi est le vecteur couleur du pixel j.\nle vecteur moyenne des composantes couleur de la région ? et ? la matrice de covariance. Ces deux derniers caractérisent le comportement des deux régions de couleur et la probabilité d\u0027appartenance d\u0027un pixel à une région donnée est présentée dans la fonction cidessous:\n-42 -RNTI-E-6 C est la courbe (ou un ensemble de courbes) qui doit évoluer dans le temps en fonction des régions in/out. Un pixel change d\u0027état en fonction de sa position (intérieur/extérieur) et de l\u0027énergie calculée E CA (positive ou négative).\nEstimation de mouvement par flot optique: Modèle de Horn \u0026 Schunck\nLes méthodes de détermination du flot optique font partie des principales contributions qui ont été présentées pour extraire une information dense et précise du mouvement, sans nécessairement se fier à une connaissance à priori. Horn \u0026 Schunck (Horn et al., 1981)    \nDétection et suivi d\u0027objets visuels par une approche mixte «CAFO»\nL\u0027idée principale consiste à utiliser une segmentation active des régions d\u0027intérêt avec un critère qui est fonction du mouvement. L\u0027algorithme de base est divisé en deux parties: l\u0027estimation du mouvement et la segmentation par contour actif. Pour simplifier la tâche et gagner en rapidité et en efficacité, nous allons minimiser une fonctionnelle d\u0027énergie unique pour la segmentation et l\u0027estimation:\nIl s\u0027agit d\u0027une résolution simultanée du problème d\u0027estimation de mouvement et de segmentation active d\u0027une image vidéo, par minimisation de l\u0027énergie F. La première étape consiste en l\u0027initialisation du contour sur l\u0027image courante. Elle peut être effectuée de manière automatique en utilisant le résultat d\u0027une étape de séparation du fond et des objets. Une fois le contour initialisé, un processus de déformation intervient jusqu\u0027à convergence en -43 -RNTI-E-6 utilisant les forces décrites précédemment. Il va nous permettre de déterminer la position d\u0027un objet(t) à l\u0027instant t en se basant sur sa position précédente objet(t-1). L\u0027intérêt de notre méthode consiste à extraire des objets visuels en mouvement de mêmes contraste que le fond de l\u0027image. L\u0027approche CAFO combine les avantages des deux métho-des «Contour Actif» (CA) et «Flot optique» (FO). En effet, la segmentation active (CA) donne des résultats satisfaisants sur des images complexes (objet + décors). Alors que l\u0027estimation du mouvement par flot optique donne d\u0027assez bons résultats quand les régions d\u0027intérêt sont texturées. L\u0027approche CAFO améliore le processus de segmentation et résout les problèmes d\u0027occlusion et d\u0027ouverture connus quand on estime le mouvement des pixels entre deux images.\nD\u0027un autre côté, cette approche est particulièrement adaptée pour le suivi d\u0027objets. La méthode peut être composé de deux étapes qui sont l\u0027initialisation et la déformation successivement sur chaque image de la séquence vidéo. Tout d\u0027abord le contour est initialisé en utilisant le résultat obtenu à l\u0027image précédente. Il est ensuite déformé en utilisant à la fois des énergies issue du modèle de contours actifs et de la force issue du calcule du flot optique. Le contour final de l\u0027image précédente sera utilisé comme contour initial sur l\u0027image courante. Une fois le contour initialisé, un processus de déformation intervient jusqu\u0027à convergence en utilisant les forces décrites précédemment. Cette méthode va nous permettre de déterminer la position d\u0027un objet(t) à l\u0027instant t en se basant sur sa position précédente objet(t-1). En général, les méthodes différentielles échouent avec ces images. L\u0027estimation de mouvement (ou l\u0027appariement de blocs) entre deux images successives, donne des informations supplémentaires mais insuffisantes pour l\u0027extraction du contenu visuel des séquences. La figure 1.d montre le résultat de l\u0027approche mixte qui intègre à la fonctionnelle d\u0027énergie des régions les forces issues du flot optique. Cela a permis de mettre en évidence plus nettement des objets d\u0027intérêt parmi des décors plus complexes, comme le montre notamment l\u0027image du hall. L\u0027avantage de combiner les informations du mouvement avec la segmentation est multiple. D\u0027abord, cela peut contribuer à l\u0027indépendance de l\u0027approche de segmentation par contour actif, de la phase d\u0027initialisation du contour initial (par exemple, en utilisant le FO comme initialisation de l\u0027image suivante). Ensuite, elle permet de compenser les informations manquantes dans une image, ou non détectables (détails) à cause la moyenne. Enfin, elle permet de privilégier l\u0027une des deux approches (grâce aux poids) en fonction des applications. Toutes les expériences ont été faites en utilisant l\u0027approche de segmentation active, et le calcul du flot optique développés en langage C++ dans l\u0027environnement Pandore.\n"
  },
  {
    "id": "1039",
    "text": "Introduction\nLe problème de l\u0027extraction de motifs séquentiels dans un grand ensemble de données statiques a été largement étudié ces dernières années (Agrawal et Srikant (1995), Masseglia et al. (1998), Pei et al. (2001), Wang et Han (2004), Kum et al. (2003)). Les schémas extraits sont utiles dans de nombreuses applications comme le marketing, l\u0027aide à la décision, l\u0027analyse des usages, etc. Depuis peu, des applications émergentes comme (entre autres) l\u0027analyse du trafic réseaux, la détection de fraude ou d\u0027intrusion, la fouille de clickstream 1 ou encore l\u0027analyse des données issues de capteurs ont introduits de nouveaux types de contraintes pour les méthodes de fouille. Ces applications ont donné lieu à une forme de données connues sous le nom de \"data streams\". Dans le contexte des data streams l\u0027utilisation de la mémoire doit être réduite, les données sont générées de manière continue et très rapide, les opérations bloquantes ne sont pas envisageables et, enfin, les nouvelles données doivent être prises en compte aussi vite que possible. Ainsi, de nombreuses méthodes ont été proposées pour extraire des items ou des motifs dans les data streams (Datar et al. (2002), Chang et Lee (2003), Cormode et Muthukrishnan (2005)). Dans ce domaine, l\u0027approximation a rapidement été reconnue comme un facteur clé pour fournir des motifs à la vitesse imposée par l\u0027application Garofalakis et al. (2002). Ensuite, des méthodes récentes (Chen et al. (2002), Giannella et al. (2003), Teng et al. (2003)) ont introduit différents principes pour gérer l\u0027historique des motifs extraits. L\u0027idée principale est que l\u0027on est généralement plus interessé par les changements récents que par les changements plus anciens. Giannella et al. (2003) a ainsi introduit la notion de logarithmic tilted time window pour stocker les fréquences des motifs avec une granularité fine pour les changements récents et une granularité plus large pour les changements plus anciens. Dans Teng et al. (2003), une technique de regression est utilisée pour représenter les fréquences et une technique permettant de régler la finesse de représentation est introduite. Enfin, dans Raissi et al. (2005), les auteurs proposent une nouvelle structure de données destinée à extraire les motifs séquentiels fréquents d\u0027un data stream. Cependant, dans cet article, nous montrons que les phénomènes combinatoires liés à l\u0027extraction des motifs séquentiels rendent toute méthode exhaustive potentiellement bloquante. En effet, si dans le cas des règles d\u0027association le nombre de possibilité est fini, ce n\u0027est pas le cas des motifs séquentiels, pour lesquels un item peut se répéter à l\u0027infini. Dans cet article, nous proposons l\u0027algorithme SMDS (Sequence Mining in Data Streams) qui est basé sur l\u0027alignement de séquences (comme Kum et al. (2003), Hay et al. (2002)  \nAdapter la problématique des motifs séquentiels\nPour les méthodes traditionnelles de Web Usage Mining, le principe gérénal est similaire à celui de Masseglia et al. (2000). Les données brutes sont collectées dans des fichiers logs par les serveurs. Chaque entrée dans le fichier log représente une requête faite par une machine cliente au serveur. L\u0027objectif est alors de déterminer, grâce à une phase d\u0027extraction, les séquences de ce jeu de données, qui peuvent être considérées comme fréquentes selon la définition 2. Les résultats obtenus sont du type \u003c ( 10 ) ( 30 ) ( 20 ) (30 ) \u003e (ici avec un support minimum de 66% et en appliquant les algorithmes de fouille de données sur le fichier représenté par la figure 1). Ce dernier résultat, une fois re-traduit en termes d\u0027URLs, confirme la décou-verte d\u0027un comportement commun à minSup utilisateurs et fournit l\u0027enchaînement des pages qui constituent ce comportement fréquent. Enfin, l\u0027exploitation par l\u0027utilisateur des résultats obtenus est facilitée par un outil de requête et de visualisation.\nSMDS : motivation et principe général\nNotre méthode est basée sur un environnement de découpage du data stream en \"batches\" (inspiré de celui proposé par Giannella et al. (2003)) et par la structure d\u0027arbre préfixé de PSP Masseglia et al. (1998) pour gérer les séquences extraites. Nous proposons d\u0027abord une étude des limites d\u0027une approche intégrant un algorithme exhaustif d\u0027extraction de motifs sé-quentiels. Nous présentons ensuite notre méthode, basée sur le principe de l\u0027alignement de séquences.\nLimites de l\u0027extraction de motifs séquentiels\nFIG. 2 -Limites d\u0027un environnement intégrant PSP\nDans SMDS, le data stream est traité sous forme de batches de taille fixe. Soient B 1 , B 2 , ... B n , les batches et B n , le batch courant. Le principe de SMDS est d\u0027extraire les motifs sequentiels représentatifs de chaque batch b de [B 1 ..B n ] et de stocker les motifs extraits dans une structure d\u0027arbre préfixé. Considérons que les motifs sont extraits par une méthode exhaustive (comme celles conçues pour les données statiques). Une telle méthode présentera au moins un type d\u0027opération bloquante. Considérons par exemple le cas de l\u0027algorithme PSP Masseglia et al. (1998). Nous avons testé cet algorithme sur des bases de données ne contenant que deux séquences (s 1 et s 2 ). Les deux séquences sont égales et contiennent des répétitions d\u0027itemsets de taille 1. Plus précisément, la première base de test contenait onze répétitions des itemsets (1)(2) (i.e. s 1 \u003d\u003c (1)(2)(1)(2)...(1)(2) \u003e, longueur(s 1 )\u003d22 et s 2 \u003d s 1 ). Le nombre de candidats générés à chaque passe est reporté dans la figure 2. La figure 2 reporte aussi le nombre de candidats générés pour les bases contenant des séquences de longueur 24, 26 et 28. On peut observer que, pour la base contenant des séquences de longueur 28, PSP est incapable de fournir les résultats (la mémoire est saturée par le nombre de candidats). Nous avons fait la même observation pour l\u0027algorithme prefixSpan 2 (Pei et al. (2001)) pour lequel ce cas de figure conduirait à un blocage du data stream. Dans le contexte des flots de données issus des usages d\u0027un site Web, il n\u0027est pas rare de trouver de nombreuses répétitions d\u0027un ou plusieurs items (fichiers pdf, php, etc.). \nPrincipe général\nDans les grandes lignes, SMDS fonctionne de la manière suivante : classification de l\u0027ensemble des séquences de chaque batch de transactions suivi d\u0027un alignement pour chaque cluster ainsi obtenu. Cela permet d\u0027obtenir des clusters de comportements qui représentent les usages du site en temps réel. Pour chaque cluster dont la taille est supérieure à minSize (spé-cifié par l\u0027utilisateur) SMDS ne stocke donc que le résumé du cluster. Ce résumé est donné par l\u0027algorithme d\u0027alignement de séquences appliqué sur chaque cluster.\nAlgorithme glouton de classification des séquences\nNotre schéma classificatoire est basique. Il repose sur le fait que les navigations sur un site sont souvent : soit plutôt proches, soit très éloignées. De manière empirique, on peut constater que les utilisateurs qui demandent les pages concernant les offres d\u0027emplois d\u0027ITA ne vont probablement pas consulter (dans la même session) les pages relatives aux prochains séminaires organisés par l\u0027unité de recherche de Sophia Antipolis. Dans le but d\u0027obtenir une classification des navigations aussi rapide que possible, notre approche gloutonne fonctionne de la manière suivante : l\u0027algorithme est initialisé avec une seule classe, qui contient la première navigation. Ensuite, pour chaque navigation n dans le batch, n est comparée avec chaque cluster c. Aussitôt que n est similaire à une séquence de c alors n est insérée dans c. Si n n\u0027est insérée dans aucun cluster, alors un nouveau cluster est crée et n est insérée dans ce nouveau cluster. La similitude entre deux séquences (sim(s 1 , s 2 )) est donnée dans la définition 4. s est insérée dans c si la condition suivante est respectée : ?s c ? c/sim(s, s c ) ? minSim, avec minSim la similitude minimum, spécifiée par l\u0027utilisateur.  : m i1 , ...x it : m it ), où m it est le nombre de séquences qui contiennent l\u0027item x i à la p eme position dans la séquence alignée. Enfin, n p est le nombre d\u0027occurrences de l\u0027itemset I p dans l\u0027alignement. L\u0027exemple 2 décrit le processus d\u0027alignement de quatre séquences. À partir de deux séquences, l\u0027alignement commence par insérer des itemsets vides (au début, au milieu ou à la fin des séquences) jusqu\u0027à ce que les deux séquences contiennent le même nombre d\u0027itemsets. À la fin du processus d\u0027alignement, la séquence alignée (SA 14 dans la figure 3) est un résumé du cluster correspondant. Le motif séquentiel correspondant peut être obtenu en spéci-fiant k : le nombre minimum d\u0027occurrences d\u0027un item pour que celui-ci soit considéré comme fréquent. Par exemple, avec la séquence SA 14 de la figure 3 et k \u003d 2, la séquence alignée filtrée sera : \u003c(a,b)(e)(h,i)(m,n)\u003e (ce qui correspond aux items qui ont un nombre d\u0027occurrences supérieur ou égal à k).\nExemple 2 considérons les séquences suivantes :\nStockage et gestion des séquences\nLes séquences alignées obtenues à la fin de l\u0027étape précédente sont stockée dans un arbre préfixé similaire à celui de Masseglia et al. (1998). Si une nouvelle séquences s est découverte, alors l\u0027arbre est modifié pour stocker cette nouvelle séquence. Sinon, s est déjà dans l\u0027arbre et son support est mis à jour. La figure 4 donne un exemple d\u0027arbre préfixé. Chaque chemin de la racine à un noeud de l\u0027arbre représente une séquence extraite. L\u0027arbre de la figure 4 contient 6 séquences (\u003c(a c)\u003e\n, \u003c(a d)\u003e, \u003c(b)\u003e, \u003c(c d)\u003e, \u003c(c)(e)\u003e, \u003c(d)(a)\u003e).\nTout chemin de la racine à une feuille représente une séquence et le noeud de profondeur l représente le l eme item de la séquence. Le changement d\u0027itemset est représenté par des branches de différents types. Par exemple, le lien pointillé entre les noeuds c et e de la figure 4 illustre le fait que e n\u0027est pas dans le même itemset que c. À chaque noeud est associé k, le filtre utilisé pour obtenir cette séquence alignée dans le cluster correspondant. L\u0027exemple 3 donne une illustration de la gestion des séquences et de leur support. \nExpérimentations\nLa méthode SMDS a été implémentée en Java sur un Pentium (2,1 Ghz) exploité par un système Linux Fedora. Nous avons évalué notre proposition sur des données synthétiques et des données réelles 3 .\nTemps de réponse et robustesse de SMDS\nDans le but de montrer l\u0027efficacité de SMDS, nous reportons dans la figure 5 le temps né-cessaire pour extraire les motifs séquentiels les plus longs sur chaque batch correspondant à des données d\u0027usage du Web (à gauche de la figure 5) et des données synthétiques (à droite de la figure 5). Pour le site Web de l\u0027Inria, les données ont été collectées sur une période de 14 mois et représentent 14 Go. Le nombre total de navigations est de 3,5 millions pour 300000 navigations. Nous avons découpé le fichier log en batches de 4500 transactions (soit environ 1500 séquences en moyenne). Pour ces expérimentations, le filtre k est fixé à 30% (notons que ce filtre a un impact sur les temps d\u0027exécution, dans la mesure où il modifie la taille des séquences à gérer dans l\u0027arbre préfixé). De plus, nous avons \"injecté\" dans les données des séquences parasites. Le premier batch ne subit pas de modification. Dans le second, nous ajoutons dix séquences contenant deux répétitions de deux items (C.f. les séquences s 1 et s 2 décrites en section 3.1). Dans le troisième batch, nous ajoutons dix séquences de trois répétitions, et ainsi de suite jusqu\u0027au trentième batch qui contient 10 séquences de trente répé-titions. L\u0027objectif est de montrer que les méthodes d\u0027extraction traditionelles (PSP, prefixSpan, ...) risquent de bloquer le data stram alors que SMDS continuera sa tâche d\u0027extraction. Nous pouvons observer que le temps de réponse de SMDS varie de 1800 ms à 3100 ms. PSP propose des motifs avec de très bonnes performances pour les premiers batches et se trouve pénalisé par le bruit ajouté par les séquences parasites (voir le batch 19). Le test a également été fait avec prefixSpan et le comportement exponentiel est similaire. Pour PSP comme pour prefixSpan, le support minimum spécifié était le maximum possible tout en assurant que les séquences \"parasites\" (répétitions) seraient trouvées. Nous avons ajouté à la figure 5 le nombre de sé-quences de chaque batch pour expliquer les différences de temps d\u0027exécution d\u0027un batch à un autre. On peut observer, par exemple, que le batch 1 contient 1500 séquences et que SMDS demande 2700 ms pour en extraire les motifs séquentiels. Pour les données synthétiques, nous avons généré des batches de 10000 transactions (qui correspondent à environ 500 séquences en moyenne). La longueur moyenne des séquences était de 10 pour 200000 items. Le filtre k est fixé à environ 30%. Nous indiquons dans la figure 5 (partie droite) les temps de réponse et le nombre de séquences correspondant à chaque batch. Nous pouvons observer que SMDS traite 10000 transactions en moins de 4 secondes (par exemple pour le batch 2).\nMotifs extraits sur les données réelles\nLa liste des comportements découverts par SMDS couvre plus de 100 objectifs de navigation (classes de séquences de navigations) sur le site Web de l\u0027Inria Sophia Antipolis. La plupart des motifs découverts peut être considérée comme rare (support faible) et pertinente (haute confiance, car le filtre utilisé est élevé). Nous reportons ici quelques exemples de ces comportements. A) k \u003d 30%, taille de la classe \u003d 13, préfixe\u003d\"http ://www-sop.inria.fr/omega/\" : Pour les navigations sur le site de l\u0027Inria Sophia Antipolis, nous avons également constaté que SMDS est capable de détecter les séquences parasites qui avaient été injectées dans les batches. Ces séquences sont simplement regroupées par SMDS dans un cluster qui ne contient qu\u0027elles, sans impact sur les temps d\u0027exécution.\nImpact de la taille des batches\nFIG. 6 -Taille des batches et pire cluster, pas à pas.\nPuisque la complexité de SMDS dépend de la taille de batches, nous avons mené une étude concernant l\u0027impact de cette taille sur les temps de réponse. Nous reportons dans la -635 -RNTI-E-6 figure 6 (partie gauche) les temps de réponses quand S (le nombre de séquences du batch) varie de 100 à 5000 séquences. La courbe temps représente les temps d\u0027exécution, cluster représente les nombre de clusters extraits, cluster 1% représente le nombre de clusters tels que : |c| \u003e 0.1 × S. En effet, nous pensons qu\u0027il ne faut considérer que les clusters dont la taille est supérieure à une certaine proportion du nombre total de séquences (on ne garde que les clusters les plus grands). Avec 1% et un batch de 1000 séquences, par exemple, un cluster c tel que |c| \u003c 10 ne sera pas considéré. time 1% représente le temps d\u0027exécution quand on ne garde que les clusters de taille supérieure à 1% de S. On peut observer que le nombre de clusters augmente de façon linéaire. Le temps de réponse est de toute évidence lié à la taille des batches, mais il est raisonnable de dire que l\u0027utilisateur final peut choisir la taille de ses batches en fonction du degré de rapidité souhaité.\nAnalyse de la qualité des clusters\nFIG. 7 -Distance globale étape par étape et batch par batch\nAfin de mesurer la qualité des classes produites par SMDS, notre principal outil sera la distance entre deux séqunces. Soit s 1 et s 2 , deux séquences, la distance dist(s 1 , s 2 ) entre s 1 et s 2 est basée sur sim(s 1 , s 2 ), la mesure de similitude donnée par la définition 4 et telle que\n2 ) proche de 0 signifie que les séquences sont proches (similaire si cette valeur est nulle) alors que dist(s 1 , s 2 ) proche de 1 signifie que les séquences sont éloignées (ne partagent aucun item si cette valeur est 1). Nous avons utilisé deux mesures pour la qualité des classes. La première est le diamètre d\u0027une classe C. Il s\u0027agit de la plus grande distance entre deux séquences de C. Un diamètre de 0% montre que la classe est constitué uniquement de séquences égales alors qu\u0027un diamètre de 100% montre que la classe contient au moins deux séquences qui ne partagent aucun item. Lors de nos expérimentations, le diamètre moyen a varié entre 2% et 3%. La seconde mesure est la \"double moyenne\". Elle est basée sur le centre de la classe. Soit C une classe, le centre de C est une séquence c telle que : ?s ? C, x?C dist(s, x) ? y?C dist (c, y). Nous sommes donc en mesure de donner, pour C, la distance moyenne (DM ) entre c et toutes\n|C|\n. Nous reportons dans la figure 6 (partie droite) quelque distances moyennes parmi les pires obtenues durant nos expérimentations. Pour chaque séquences ajoutée dans une classe, nous donnons la valeur DM de cette classe. Par exemple, après l\u0027ajout de la dernière séquence dans le cluster 1, la valeur de DM pour ce cluster est 22%. On peut observer que DM varie de 0 (quand |C| \u003d 1 l\u0027unique séquence est le centre) à 50%. DM décroit alors rapidement jusqu\u0027à des valeurs comprises entre 20% et 35%, ce qui est un bon résultat compte tenu du fait que la figure 6 (partie droite) ne comprend que les résultats des clusters les moins homogènes. Les autres clusters sont homogènes et offrent à l\u0027algorithme d\u0027alignement un cadre adéquat. Nous reportons dans la figure 7 (partie gauche) la double moyenne DBM après avoir traité chaque séquence d\u0027un batch. DBM est calculée dist (x,ci) de la manière suivante : soit C l\u0027ensemble des classes, DBM \u003d i?C x?C i |C| avec c i le centre de C i (la i eme classe). On peut observer dans la figure 7 (partie gauche) que pour le second batch, DBM augmente rapidement jusqu\u0027à 2% (séquence 220), puis augmente lentement jusqu\u0027à 3,7%. La valeur finale de DBM à la fin du batch est donnée par la figure 7 (partie droite). On peut y observer que DBM est toujours comprise entre 2% et 9%. A la fin du processus, la valeur moyenne de DBM est de 4,3% (une qualité moyenne des classes de 95,7%).\nConclusion\nDans ce papier, nous avons proposé la méthode SMDS, conçue pour extraire rapidement les séquences d\u0027un data stream et en proposer un résumé significatif. Notre algorithme repose sur une technique de classification combinée avec un alignement des séquences. Le processus d\u0027alignement repose sur un algorithme de classification glouton qui considère que dans le contexte du Web les navigations ont certaines caractéristiques qu\u0027il faut prendre en compte. De plus nous avons proposé une solution adaptée pour gérer efficacement les séquences et leur historique dans un arbre préfixé. Grâce à cette façon de traiter le data stream, SMDS est capable de détecter des comportement partagé par un nombre relativement faible d\u0027utilisateurs (e.g. 13 utilisateurs ou encore 0, 5%) ce qui est proche du difficile problème de l\u0027extraction de motifs séquentiels avec un support très faible. De plus, nos expérimentations ont montré que SMDS traite le data stream assez rapidement pour être intégré dans un contexte temps réel. Nous avons également montré que SMDS propose des classes de très bonne qualité, ce qui permet d\u0027extraire les motifs les plus pertinents et de façon exhaustive.\nSummary\nIn recent years, emerging applications introduced new constraints for data mining methods. These constraints are typical of a new kind of data: the \"data streams\". In a data stream processing, memory usage is restricted, new elements are generated continuously and have to be considered as fast as possible, no blocking operator can be performed and the data can be examined only once. We argue that the main issue is the combinatory phenomenon related to sequential pattern mining. In this paper, we propose an algorithm based on sequences alignment for mining approximate sequential patterns in data streams. To meet the constraint of one scan, a greedy clustering algorithm associated to an alignment method are proposed. We will show that our proposal is able to extract relevant sequences with very low thresholds.\n"
  },
  {
    "id": "1040",
    "text": "Introduction\nLe développement d\u0027Internet comme source d\u0027informations a conduit à l\u0027élaboration de programmes nommés wrappers pour collecter de l\u0027information sur les sites Web. Ces programmes sont difficiles à concevoir et à maintenir. Deux approches sont alors envisageables : la première consiste à assister l\u0027utilisateur, c\u0027est le cas du système Lixto (Baumgartner et al., 2001) dans lequel on spécifie le wrapper dans un langage logique avec l\u0027aide d\u0027un environnement visuel ; la seconde consiste à générer automatiquement le wrapper en limitant l\u0027intervention de l\u0027utilisateur à l\u0027annotation des informations à extraire sur quelques documents. Cette approche est fondée sur le fait que la plupart des documents sur Internet sont générés par programme et présentent des régularités exploitables par les méthodes d\u0027apprentissage automatique.\nLes premiers systèmes d\u0027induction de wrappers n\u0027utilisaient que l\u0027aspect textuel des documents (Hsu et Dung, 1998;Kushmerick, 1997). Avec l\u0027apparition de XML, ces approches textuelles sont devenues insuffisantes. Les systèmes actuels utilisent la structure arborescente des documents du Web (Carme et al., 2005;Cohen et al., 2003;Kosala et al., 2002;Muslea et al., 2003;Thomas, 2003). Nous nous inscrivons dans cette démarche en proposant un système d\u0027induction qui utilise à la fois les vues textuelle et arborescente. Beaucoup de systèmes d\u0027induction de wrappers sont conçus pour des tâches unaires. Un wrapper unaire extrait un ensemble de valeurs, par exemple l\u0027ensemble des noms de produits disponibles sur un site marchand. Un wrapper n-aire extrait les instances d\u0027une relation n-aire, par exemple les couples (nom du produit, prix). Il existe deux approches pour induire un wrapper n-aire : soit combiner n wrappers unaires, soit apprendre directement le wrapper n-aire. La première approche nécessite l\u0027obtention d\u0027un modèle pour la combinaison, ou une intervention de la part de l\u0027utilisateur (Jensen et Cohen, 2001;Muslea et al., 2003), ou encore l\u0027utilisation d\u0027heuristiques. La seconde approche est illustrée par les systèmes WIEN (Kushmerick, 1997) et SOFT MEALY (Hsu et Dung, 1998) utilisant des délimiteurs textuels pour repérer les composantes des tuples et le système LIPX (Thomas, 2003) basé sur la logique du premier ordre.\n-415 -RNTI-E-6\nExtraction de relations dans les documents Web Nous proposons un système d\u0027induction de wrappers n-aire pour les documents du Web utilisant les vues textuelle et arborescente. Pour cela, nous étudions à la section 2 les différentes représentations arborescentes de tables dans les documents Web. Nous proposons ensuite un système basé sur les deux principes suivants : l\u0027extraction est incrémentale, c\u0027est-à-dire que le système extrait l\u0027ensemble des premières composantes, puis l\u0027ensemble des couples pour les deux premières composantes, jusqu\u0027à l\u0027ensemble des n-uples ; pour extraire l\u0027ensemble des tuples de longueur i, des informations sur les tuples de longueur i ? 1 sont utilisées. Les algorithmes d\u0027extraction et d\u0027induction sont présentés dans la section 3. Le système est évalué sur des jeux de données réels dans la section 4. Les résultats montrent que notre système peut appréhender l\u0027extraction de relations dans les documents Web pour des organisations fréquentes que les systèmes existants ne sont pas capables de traiter.\nReprésentations arborescentes d\u0027une relation n-aire\nNous considérons que les données sont stockées dans des documents au format XML ou XHTML qui peuvent être considérés selon plusieurs vues. La vue DOM considère le document comme un arbre (arbre d\u0027analyse) ; la vue séquentielle comme un flux de caractères ; la vue feuillage comme la séquence des feuilles textes de la vue DOM. Ces différentes vues sur les données sont illustrées par la \nFIG. 1 -Vue DOM (gauche), vue textuelle (en haut à droite), vue feuillage (en bas à droite).\nUne étude des documents du Web nous a amené à distinguer les cas suivants : Cas 1. Les données sont dans une table dont la première ligne contient les noms des composantes et les lignes suivantes contiennent les données. Dans la vue DOM, il existe un plus petit sous arbre contenant chaque tuple et seulement lui (Figure 1).\nCas 2. Une autre représentation est celle d\u0027une liste où les tuples sont stockés séquentiel-lement. Pour les vues textuelle et arborescente, il peut être difficile de retrouver les tuples sans information auxiliaire surtout dans le cas de valeurs manquantes.\nCas 3. Les données sont dans une table tournée dont la première colonne contient les noms des composantes et les colonnes suivantes contiennent les données. Les tuples sont entrelacés dans les vues textuelle et feuillage. Dans la vue DOM, les composantes d\u0027un même tuple ont le même numéro de fils dans des arbres différents de racine tr. Ceci est illustré par la relation ternaire (Season, Club name, Score) de la figure 2 où les tuples à extraire sont (2002,PSG,17) et (2002.  Table croisée 3 Apprendre à extraire une relation n-aire\nLes processus d\u0027extraction et d\u0027induction sont incrémentaux : on extrait les premières composantes, puis les tuples de longueur 2, jusqu\u0027aux tuples de longueur n pour une relation n-aire cible. Pour la première composante appelée graine, la tâche d\u0027extraction considérée consiste à extraire certaines feuilles textes d\u0027un document arborescent (nous ne considérons pas les cas où les données à extraire sont situées dans une feuille texte ou sur plusieurs feuilles textes). Nous utilisons un codage attribut-valeur des feuilles : le codage de base d\u0027un noeud p est fourni par les attributs suivants : label, position dans la séquence des fils du père de p, profondeur et hauteur, nombre de fils, taille du sous arbre enraciné en p, label du frère gauche et du frère droit de p, valeurs éventuelles des attributs XHTML class et id. La représentation d\u0027une feuille l est donnée par l\u0027application du codage de base à l, à ses 5 ancêtres dans la vue DOM, puis par le contenu brut des feuilles textes précédente et suivante dans la vue feuillage. Ainsi une feuille est représentée par 57 attributs. Une fois codées en vecteurs d\u0027attributs, les feuilles sont fournies à un classifieur qui classe chacune d\u0027entre elles. Une feuille est extraite si elle est classée comme à extraire.\nConsidérons maintenant une relation cible n-aire et sa restriction aux i premières composantes. Le codage rep i de la relation d\u0027arité i va utiliser des informations sur les composantes 1 à i ? 1, il est définie par : la représentation d\u0027un tuple d\u0027arité i se fait par le codage de base de la i-ième composante notée l et de la description des dépendances entre l et la graine -417 -RNTI-E-6\nExtraction de relations dans les documents Web\nAlgorithm 1 Extraction\nInput: un document d ; n classifieurs ci à valeurs dans {?1, +1} travaillant sur les représentations repi. Notation: L est l\u0027ensemble des feuilles de d, Si est l\u0027ensemble des tuples candidats à l\u0027étape i, Ti est l\u0027ensemble des tuples extraits à l\u0027étape i sélectionnés dans Si. 1: S1 \u003d {(l) | l ? L} ; T1 \u003d {t1 ? S1 | c1(rep 1 (t1)) \u003d +1} 2: for i \u003d 2 to n do 3:\n?ti ? Si, si ci(rep i (ti)) \u003d +1, ajouter ti à Ti 5:\nsi tous les ti basés sur le même ti?1 sont classés comme négatif par ci, ajouter (ti?1, null) à Ti 6: end for Output: Tn l\u0027ensemble des tuples n-aire extraits ainsi qu\u0027entre l et la composante i ? 1. Le codage de la dépendance entre deux feuilles p et m est constituée du codage de leur plus petit ancêtre commun a, des longueurs des plus courts chemins dans la vue DOM entre p et a, m et a et entre p et m, du nombre de feuilles textes se trouvant entre p et m dans la vue feuillage, et pour 1 ? k ? 5 de la différence entre la position (relative à leur père) du k-ième ancêtre de p et du k-ième ancêtre de m. Un tuple est représenté par 220 attributs quel que soit i différent de 1.\nL\u0027ordre des composantes étant déterminé par la relation cible fournie par l\u0027utilisateur du système, l\u0027extraction est réalisée par l\u0027algorithme 1. On peut noter qu\u0027il est possible d\u0027extraire plusieurs tuples de longueur i pour un même tuple de longueur i ? 1 (ligne 4) et que les valeurs manquantes sont gérées en ligne 5.\nDu point de vue de l\u0027apprentissage, la tâche d\u0027induction d\u0027un programme d\u0027extraction naire est définie par l\u0027algorithme 2 et correspond à n problèmes de classification supervisée, chaque problème consistant à classer des tuples d\u0027arité i comme étant à extraire ou pas. L\u0027algorithme de classification supervisée utilisé est C5.0 (Quinlan, 2004) \nExpériences\nToutes les expériences sont réalisées avec deux documents annotés en apprentissage. La qualité de notre système est évaluée à travers la f-mesure (F ) des wrappers produits. Les critères d\u0027évaluation sont stricts : un tuple extrait est correct si toutes les composantes sont égales exactement aux composantes du tuple à extraire.\nDes expériences ont été réalisées sur les jeux de données classiques du domaine, en l\u0027occurrence RISE 1 . Les organisations correspondent aux deux premiers cas de la section 2. Notre système réussit parfaitement sur tous les jeux de données à l\u0027exception de S1 et IAF. Sur S1,\nAlgorithm 2 Apprentissage\nInput: un échantillon S de documents où les tuples à extraire sont annotés.\n1: S + \u003d {t \u003d (l1, . . . , ln)} l\u0027ensemble des n-uplets de feuilles à extraire dans S. 2: C \u003d ? 3: for i \u003d 1 to n do 4:\n+ } # projection des n-uplets à extraire sur les i premières composantes 5: \nsoit ci le classifieur appris par C5.0 avec l\u0027échantillon T\najouter ci à la séquence C de classifieurs 9: end for Output: C   FIG. 4   (town, day, weather, high, low). La composante town est factorisée, elle apparaît dans l\u0027entête de page. Les autres composantes sont présentées dans une table tournée à 5 colonnes comme le montre la figure 4. Nous obtenons F \u003d 100, ce qui montre la capacité de notre système à gérer ce type d\u0027organisation. Ensuite le site Bureau of Labor Statistics 4 et la relation (value, year, quantile). Il s\u0027agit ici d\u0027un exemple d\u0027organisation en table croisée. Nous obtenons F \u003d 98.52 à cause de la présence de valeurs manquantes dans certaines tables.\nConclusion\nNous avons présenté un système capable d\u0027induire automatiquement des programmes d\u0027extraction n-aire à partir de documents du Web. Ce système présente les avantages suivants :\n"
  },
  {
    "id": "1041",
    "text": "Introduction\nA cette date, de nombreuses méthodes d\u0027étiquettage d\u0027entités biologiques pour les corpus de spécialité ont été proposées ; quelles soient à base de règles (Fukuda et al. (1998)) ou encore réposant sur des techniques d\u0027apprentissage (Collier et al. (2000)). Néanmoins, la simple détection de la présence d\u0027une entité nommée dans un texte ne suffit pas pour l\u0027identifier et l\u0027associer à une instance d\u0027entité biologique particulière. Le couplage des méthodes d\u0027extraction des entités nommées avec l\u0027utilisation de dictionnaires semble être une solution particulière-ment adaptée à ce type de problématique (Koike et al. (2003)). De plus, la majorité de ces techniques d\u0027extraction d\u0027entités nommées ont été développées dans le but de ne détecter que quelques types particuliers et spécifiques d\u0027objets biologiques, notamment les gènes et les protéines, et ne peuvent être facilement adaptés à d\u0027autres contextes. Il existe trois principales difficultés à prendre en compte lors d\u0027une recherche à base de dictionnaire :\n-la présence de termes synonymes et la résolution des différentes abréviations et acronymes, -la variabilité des mots tant au niveau de l\u0027orthographe que de la morphologie et de la syntaxe mais aussi d\u0027un point de vue lexico-sémantique, de la présence d\u0027insertions/déletions et permutations, -la présence de noms ambigus que se soit entre des entités de même nature, entre des entités de natures différentes ou des collisions avec le dictionnaire anglais standard. Ces différents points restent particulièrement difficiles à traiter dans les textes de biologie et de médecine. Les problèmes d\u0027ambiguïté dans les corpus biomédicaux sont résumés dans (Tuason et al. (2004)).  (Lindberg et al. (1993)).\nDescription des dictionnaires\nNous stockons dans nos dictionnaires, non pas les noms bruts issus des alias et des acronymes de chaque base de données, mais les formes variantes (orthographiques, morphologiques, lexico-sémantiques, etc) de chaque alias et symbole que nous normalisons. Nous utilisons aussi les informations issues des nomenclatures spécifiques de chaque base de données afin de générer de nouveaux alias d\u0027une entité. Ces formes inédites peuvent être retrouvées dans les publications scientifiques alors qu\u0027elles sont absentes des bases de données. Par exemple, un effort important a été fourni afin de produire l\u0027ensemble des combinaisons de noms complets/formes acronymiques potentielles d\u0027une même entité (\"chemokin like receptor 1\", \"CMKLR1\", \"CMKL receptor 1\", \"CMK light receptor 1\", \"chemokin like R 1\" et \"chemokine LR 1\") et les multiples insertions/déletions et permutations de groupes de mots descripteurs (\"class III alcohol dehydrogenase\", \"alcohol dehydrogenase class III\" et \"adenosine monophosphate deaminase 1 isoform M\", \"adenosine monophosphate deaminase 1\" ). La construction de tels dictionnaires ne sera pas décrite dans cette article. Actuellement, les dictionnaires contiennent un total de 205 736 variants dont 49 656 entités distinctes. Les différentes molécules, cellules, organes et sites de liaison sur l\u0027ADN répertoriés proviennent de l\u0027humain ou à défaut de mammifères.\nRecherche des entités nommées dans les textes\nChaque document est tout d\u0027abord découpé en phrases grâce à des heuristiques puis à chaque mot de chaque phrase est associé son part of speech grâce à GENIA POS Tagger (Tsuruoka et al. (2005)). Nous n\u0027utilisons pas de shallow parser mais uniquement les informations fournies par les part of speech. Ceci à le mérite d\u0027alléger la procédure à condition que l\u0027éti-quettage soit correct (Amrani et al. (2005)). Tous les syntagmes plus ou moins complexes sont extraits de chaque phrase séquentiellement, découpés en plus petites unités, grammaticalement correctes en biologie, et normalisés jusqu\u0027à correspondance exacte avec une entité du dictionnaire. En effet, les entités nommées présentes dans les publications biomédicales peuvent être relativement complexes et s\u0027étendre sur plus d\u0027un groupe nominal.\nExtraction des entités nommées Les groupes de mots pouvant potentiellement représenter un ou plusieurs objets biologiques sont dégagés des textes de la façon suivante :\n1. Les groupes nominaux simples correspondant aux blocs de noms propres ou communs avec les éventuels symboles, cardinaux et adjectifs associés sont extraits. Par exemple \"Interleukine 2\".\n2. Sont rattachés aux groupes nominaux simples les verbes au gérondif ou au participe passé en suffixe, ou en préfixe si le mot précédent le verbe n\u0027est pas un modal, un pronom ou un adverbe. Deux groupes nominaux simples sont concaténés si un de ces verbes permet d\u0027en faire la jonction. Par exemple \"Interferon regulating factor 8\".\n3. Deux de ces groupes nominaux étendus peuvent être ensuite réunis si certaines prépo-sitions ou conjonctions telles que \"of\", \"in\", \"at\", \"on\", \"by\", \"for\", \"to\" ou \"with\" les séparent. Par exemple \"regulator of G-protein signalling 4\" ou \"cell adhesion molecule regulated by oncogenes\".\n4. De la même manière, deux des groupes nominaux étendus provenant de l\u0027étape précé-dente sont reliés entre eux s\u0027ils sont séparés par une conjonction de coordination \"and, \"but\" ou \"or\". Par exemple \"Signal transducer and activator of transcription 3 interacting protein 1\".\nA cette étape, nous pouvons donc avoir des syntagmes de complexité très différentes à analyser. A priori, chaque syntagme représente une seule et même entité. Nous cherchons donc son occurrence au sein de nos dictionnaires après normalisation (cf paragraphe suivant). Néanmoins si la mise en correspondance exacte n\u0027a pu être réalisée, nous considérons que le syntagme contient alors plus d\u0027une entité, chaque entité pouvant être représenté par une portion indépen-dante du texte. Nous devons donc redécouper le bloc de texte contenu dans le syntagme en sous-unités de complexité légèrement moindre. Chaque sous-unité est alors testée individuellement contre nos dictionnaires et si la mise en correspondance s\u0027avère infructueuse, celle ci est décomposée en constituants plus simples, et ainsi de suite, jusqu\u0027à détection positive de la présence d\u0027une entité ou obtenir une portion de texte non résolue et atomique. Le découpage des syntagmes est réalisé grâce à la règle contextuelle décrite ci-dessous que nous appliquons en fonction des séparateurs suivants, séquentiellement :\n1. les conjonctions de coordination, 2. les prépositions (sauf s\u0027ils sont précédés d\u0027un verbe), 3. les gérondifs et participes passés (et l\u0027éventuelle préposition associée).\nLa précédence du séparateur numéro (2) sur le séparateur numéro (3) a été décidé empiriquement en analysant la composition de nos dictionnaires.\nLes différentes combinaisons de blocs de texte de part et d\u0027autre d\u0027un séparateur sont gé-nérées. Par exemple, l\u0027expression \"suppressor of G2 allele of SKP1 pseudogene\" donne les combinaisons \"suppressor of G2 allele\" et \"G2 allele of SKP1 pseudogene\". Des nouveaux syntagmes générés, ceux possédant le plus grand nombre de séparateurs ont précédence sur ceux en contenant moins et sont traîtés en priorité par la suite. De même, en cas de présence de prépositions, la position des entités nommées au sein des expressions est située préféren-tiellement à droite des séparateurs. Nous traitons donc en priorité les nouveaux syntagmes en fin de texte. Sur l\u0027exemple précédent, l\u0027ordre de priorité est désormais : \"G2 allele of SKP1 pseudogene\" puis \"suppressor of G2 allele\". Le syntagme original, non découpé à l\u0027étape en cours, est utilisé par la règle suivante. En revanche, chaque nouveau bloc est de nouveau traité par la règle en cours.\nPar exemple, le syntagme \"modulator of G-protein signalling 4 down-regulated by oncogenes\" contient une entité nommée : \"G-protein signalling 4\" que l\u0027on souhaite découvrir. Les séparateurs détectés dans le texte sont \"of\" et \"down-regulated by\". Nous testons contre nos dictionnaires les blocs de texte suivants dans cet ordre : tout d\u0027abord \"modulator of G-protein signalling 4 down-regulated by oncogenes\" puis -d\u0027une part \"G-protein signalling 4 down-regulated by oncogenes\" puis -\"G-protein signalling 4\" et \"oncogenes\" indépendamment -et d\u0027autre part \"modulator\". Une limite principale à la stratégie actuellement mise en place consiste en l\u0027impossibilité de retrouver des concepts basés sur des groupes verbaux. Ceci n\u0027influe pas sur la capacité du système à détecter des noms d\u0027objets biologiques mais réduit son abilité à reconnaître des concepts de plus haut niveau.\nIdentification des entités nommées\nChaque bloc de texte que l\u0027on souhaite mettre en correspondance avec les entités présentes dans nos dictionnaires sont tout d\u0027abord normalisées (génération des variants morphologiques, suppression des déterminants, lemmatisation et passage sous la forme de compound nouns). Les portions de texte que nous obtenons à l\u0027étape d\u0027extraction sont à base de groupes nominaux plus ou moins complexes, or il est très fréquent de trouver associés aux entités nommées des noms satellites qui peuvent soit décrire une action dont l\u0027objet est l\u0027entité biologique (par exemple \"assimilation\", \"transcription\", \"screening\", etc) ou qualifiant l\u0027entité (par exemple \"gene\", \"protein\", \"experiment\", etc). En anglais, de tels termes sont majoritairement ajoutés en suffixe au nom de l\u0027entité. Aussi nous supprimons au fur et à mesure les noms à droite du texte avant de les soumettre à la recherche dans les dictionnaires. Cette méthode simple permet de répondre convenablement à ce type de problème. Il reste néanmoins des cas de figure non négligeables de construction de groupes nominaux où les noms d\u0027action ou descriptifs sont retrouvés devant le nom de l\u0027objet biologique (par exemple, \"interleukine protein IL2\") et qui sont non résolus automatiquement pour le moment. La présence en préfixe d\u0027adjectifs (par exemple \"ubiquitous\"), cardinaux ou symboles est éga-lement prise en compte. La seule différence avec la technique précédente réside dans le sens de la réduction des termes : ici ce sont les adjectifs, cardinaux ou symboles les plus à gauche du texte qui sont enlevés.\nDésambiguation Pour l\u0027instant l\u0027étape de désambiguation des noms est assez rudimentaire :\n-Afin d\u0027améliorer la qualité des groupes nominaux à tester, sont détectées les énuméra-tions simples du type \"interleukine 1, 2 and 3 receptors\" impliquant des numériques ou des symboles/identifieurs afin d\u0027être explicitées sous la forme \"interleukine 1 receptor and interleukine 2 receptor and interleukine 3 receptor\".\n-Beaucoup d\u0027auteurs d\u0027articles en biologie définissent entre parenthèses des abréviations qu\u0027ils utilisent tout au long du document en lieu et place de l\u0027objet biologique tel qu\u0027il est décrit dans nos dictionnaires. Il est très important de pouvoir les détecter et les associer correctement aux entités qu\u0027elles représentent. Les termes entre parenthèses qui précèdent une entité nommée identifiée mais dont la nature est inconnue seront automatiquement associés à cette entité reconnue lors d\u0027une prochaine occurrence dans le texte.\nTypage Dans nos dictionnaires, nous pouvons avoir une même entité associée à différents descripteurs biologiques. Seul le contexte dans lequel l\u0027entité a été identifiée peut permettre de clarifier sa nature. Lorsque plusieurs entités de nature différente (par exemple facteur de transcription ou site de liaison à un facteur de transcription) correspondent à un même bloc de mots, les noms éliminés à l\u0027étape d\u0027Identification des entités nommées puis d\u0027Extraction des entités nommées permettent de mesurer la vraisemblance respective de chaque descripteur \"contextuel\" associé à l\u0027objet biologique grâce à un lexique de mots contrôlés. Ce lexique contient un ensemble de termes qui sont associés spécifiquement à un ou plusieurs types d\u0027entités dans les textes (par exemple, \"neuropeptide\" qualifie exclusivement une protéine, \"transcription\" un gène et \"assay\" un protocole expérimental). Le type prédominant parmi les mots satellites résolus par le lexique doit ainsi permettre d\u0027aider à clarifier le type contextuel de l\u0027entité. Le recours à des techniques plus perfectionnées est alors nécessaire lorsque l\u0027entité n\u0027est associée à aucun terme du lexique (pour le moment, l\u0027entité reste non-résolvable) ou lorsque des contradictions apparaissent (ici l\u0027entité est a priori considérée comme étant un faux positif).\nRésultats et conclusion\nL\u0027outil a été développé en Java et la base de données implémentée avec PostgreSQL. Nous avons mesuré les performances du système sur le corpus de référence GENIA (JinDong et al. (2003) Les approches par dictionnaire ont pour principale limitation de ne pouvoir détecter des entités encore inconnues mais restent efficaces pour permettre de caractériser les relations entre ces différents objets biologiques ou certaines de leurs propriétés en combinaison avec un système d\u0027extraction d\u0027information en aval. Le principal avantage des techniques mises en oeuvre dans cet article est leur relative généricité permettant de traiter des objets biologiques de natures très différentes sans avoir à utiliser différentes méthodes complexes en parallèle. Plusieurs difficultés restent néanmoins en suspens : la principale, et la plus difficile à résoudre, est l\u0027inévitable cas des termes au sens variable selon le contexte (et notamment comment distinguer la véri-table nature d\u0027une entité et savoir si l\u0027on a affaire à une véritable entité biologique ou non. Ce qui se pose particulièrement pour les sites de liaison aux facteurs de transcription d\u0027après nos résultats préliminaires). Une autre difficulté réside dans l\u0027exhaustivité très relative des dictionnaires présentés ici et la nécessité d\u0027avoir des sources contrôlées et vérifiées, ce qui rend les mises à jour encore assez ardues. Beaucoup d\u0027entités présentes dans les dictionnaires sont inappropriées ou issues d\u0027erreurs de saisie lorsque les bases de données respectent mal ou peu les nomenclatures en vigueur.\n"
  },
  {
    "id": "1042",
    "text": "TIMC-IMAG Institut de l\u0027IngØnierie et de l\u0027Information de SantØ\nFacultØ de MØdecine F-38706 LA TRONCHE cedex Delphine.Bernhard@imag.fr http://www-timc.imag.fr/Delphine.Bernhard Les mØthodes d\u0027extraction automatique de termes utilisent couramment des patrons dØ-crivant la structure des termes (Ibekwe-Sanjuan et Sanjuan, 2004;Enguehard, 1992;Vergne, 2005). Dans les domaines scientiiques ou techniques comme la mØdecine (Namer, 2005), de nombreux termes appartiennent au vocabulaire savant et sont construits partir de formants classiques grecs ou latins situØs en dØbut (extra-, anti-) ou en n de mot (-graphe, -logie). La mØthode que nous proposons utilise la structure morphologique des termes en vue de leur extraction et de leur regroupement MOEme si cette expression rØguli?re est limitØe aux formants se terminant par a, i ou o, elle n\u0027est pas uniquement valable pour le franais. On trouvera, par exemple, \"chimio-hormonothØrapie\" en frannais, \"chemo-radiotherapy\" en anglais ou \"Chemo-radiotherapie\" en allemand.\nUne fois les formants identiiØs, les termes sont repØrØs l\u0027aide d\u0027un patron qui dØcrit leur structure morphologique : F+M oø F est un formant et M un mot du corpus de longueur supØrieure 3. Le caract?re + indique la succession possible de plusieurs formants en dØbut de terme. Lorsque ce patron s\u0027applique un des mots du corpus, deux termes sont reconnus : le terme de structure F+M et le terme de structure M. Ainsi, partir du mot \"radiothØrapie\" qui contient le formant \"radio\", on extrait les termes \"radiothØrapie\" et \"thØrapie\".\nAAn de faciliter l\u0027analyse des termes extraits, des familles de termes sont formØes en regroupant les termes contenant le mOEme mot M. Le mot M est appelØ reprØsentant de la famille. De plus, deux familles sont rØunies si leurs reprØsentants ont une chaane initiale commune de longueur supØrieure ou Øgale 4 et si l\u0027on retrouve le mOEme formant dans un terme de chaque famille. Le reprØsentant nal de chaque famille est le terme le plus frØquent.\nLes rØsultats de l\u0027extraction terminologique sont prØsentØs sous forme de liste pondØrØe au format HTML (voir gure 1). Ce type de liste se caractØrise par l\u0027utilisation d\u0027un code de couleur et d\u0027une taille de police dØpendant de la frØquence d\u0027occurrence d\u0027un terme (VØronis, 2005). Seuls les termes reprØsentants de chaque famille sont afchØs et le poids d\u0027une famille dans la reprØsentation nale est determinØ par la frØquence cumulØe de tous les termes de la famille.\nFIG. 1 Visualisation des termes sous forme de liste pondérée (à gauche) et détail d\u0027une famille de termes (à droite)\nLe syst?me a ØtØ expØrimentØ sur 4 corpus de textes couvrant deux domaines scientiiques distincts, celui de la volcanologie et du cancer du sein, dans deux langues diffØrentes, le franais et l\u0027anglais. Les premiers rØsultats obtenus montrent que l\u0027utilisation de la structure morphologique permet de mettre jour des termes peu frØquents qu\u0027une approche purement frØ-quentielle ne pourrait identiier. Ces deux approches sont donc complØmentaires. L\u0027algorithme de regroupement permet quant lui de rassembler les variantes orthographiques, exionnelles et dØrivationnelles des termes dans une mOEme famille.\n"
  },
  {
    "id": "1043",
    "text": "Classification croisée de données biologiques\nAfin d\u0027étudier les séquences d\u0027acides aminés représentant les protéines, nous avons utilisé des techniques de text mining afin d\u0027extraire des descripteurs. Ces descripteurs nous permettrons de construire un tableau de données Protéines × Descripteurs. L\u0027une des techniques les plus utilisées est l\u0027extraction des x-grammes (Miller et al. (1999), Mhamdi et al. (2004)), x étant la taille d\u0027un descripteur.\nPlusieurs méthodes de classification croisée ont été proposées (Govaert (1977), Ritschard et Nicoloyannis (2000)). Récemment, des méthodes de classification croisée ont été appliquées aux données biologiques (Cheng et Church (2000)). Cependant, plusieurs de ces méthodes restent très coûteuses en temps de calcul.\nFaBR-CL : méthode de classification croisée\nAfin d\u0027effectuer une classification croisée, nous nous sommes basé sur une méthode de classification peu coûteuse en temps de calcul (Erray (2005)). La méthode proposée, FaBR-CL, utilise FaUR dans une approche \"Combinaison itérative de regroupement des lignes et des colonnes\" afin d\u0027obtenir un regroupement complet des protéines et des 3-grammes. Ainsi, nous effectuons le regroupement des protéines, dans un premier temps, et le regroupement des 3-grammes dans un deuxième temps. La complexité de cette méthodes est en O(l log l + p log p), l étant le nombre de protéines et p le nombre de descripteurs.\nNous avons travaillé sur les trois familles de protéines PAD, TLR et AD afin de valider notre approche. Les trois études portant à chaque fois sur des protéines appartenant à deux familles, montrent que la méthode FaBR-CL donne un classement très proche de la réalité (PAD, TLR et AD). Aussi, nous obtenons des groupes de 3-grammes fortement pertinents par rapport à chaque classe de protéines. L\u0027étude de toutes les protéines des trois familles, confirme ces résultats.\nConclusion\nLa méthode proposée permet, avec un coût calculatoire très faible, de classer les protéines. Aussi, cette méthode permet de mettre en évidence des groupes de 3-grammes, de faibles effectifs, et qui permettent d\u0027identifier une classe de protéines par leurs présences ou par leurs absences.\n"
  },
  {
    "id": "1045",
    "text": "Introduction\nDans le cadre de ce travail, nous nous intéressons au problème d\u0027extraction de règles associatives, initialement introduit par Agrawal et al. Agrawal et al. (1993). Plusieurs travaux basés sur l\u0027analyse formelle des concepts (AFC) Ganter et Wille (1999), proposent des approches de sélection de règles associatives qui véhiculent le maximum de connaissances utiles. Ces approches reposent généralement sur l\u0027extraction d\u0027un sous-ensemble générique de toutes les règles associatives, appelé base générique, tout en satisfaisant certaines caractéristiques jugeant de sa qualité, mais qui dans la plupart des cas ne sont pas satisfaites dans leurs totalités Kryszkiewicz (2002).\nDans cet article, nous introduisons une nouvelle approche de génération d\u0027une base minimale et générique (MGB) de règles associatives. L\u0027originalité de cette approche est qu\u0027elle est autonome : elle commence directement à partir du contexte d\u0027extraction pour dériver une base générique minimale de règles associatives FAST-MGB.\nFondements mathématiques\nDans cette section, nous rappelons brièvement les notions mathématiques relatives à l\u0027analyse formelle des concepts Ganter et Wille (1999).\nNotions de bases\nContexte de fouille. Un contexte de fouille est un triplet\nL\u0027opérateur de fermeture de la connexion de Galois Ganter et Wille (1999) est la composi-\n. L\u0027ensemble des concepts réduits fréquents forme un semi-treillis appelé treillis de l\u0027Iceberg de Galois. Bastide et al. (2000).\nLes algorithmes d\u0027extraction de bases génériques\nGénéralement, le nombre de règles associatives dérivées par un processus de fouille de données peut devenir très important surtout quant les mesures de fréquences deviennent assez faibles ou encore dans le cas de bases de données denses telles que les données textuelles.\nUne solution possible à ce problème serait de se restreindre à l\u0027extraction des règles strictement liées aux besoins de l\u0027utilisateur, en d\u0027autres termes, se limiter à une base générique des règles associatives, répondant à certains critères et à partir de laquelle les règles redondantes pourront être dérivées. Différentes approches de dérivation de bases génériques ont été proposées dans la littérature présentant chacune certaines limites. Nous distinguons quatre bases à savoir : la Base représentative (RB P han ) Luong (2001), la Base des règles associatives non redondantes (MNR) Bastide et al. (2000), la Base générique pour les règles représentatives (RR) Kryszkiewicz (2002), et la Base Générique Informative (IGB) Gasmi et al. (2004). Une étude comparative des quatre bases est donnée dans Latiri et al. (2005).\nDans le cadre de notre travail, nous définissons une base générique comme suit :\nDéfinition 1 Une base générique est un ensemble de taille réduite de règles associatives ne contenant aucune règle redondante. Il existe trois critères pour évaluer une base générique quantitativement (en nombre de règles) et qualitativement (par la sémantique), à savoir :\n1. Informativité : c\u0027est la possibilité de déterminer avec exactitude le support et la confiance des règles redondantes dérivées à partir de la base générique, ce qui nécessite de garder la trace des concepts réduits fermés fréquents ou de leurs générateurs. Nous passons dans ce qui suit à la présentation de la nouvelle base générique FAST-MGB.\nUne nouvelle base Générique minimale : FAST-MGB\nNotre contribution consiste à introduire une nouvelle base générique minimale des règles associatives non redondantes, notée par FAST-MGB et ne contenant que des règles implicatives (i.e. prémisse différente de ?). Cette approche assure la compacité et une informativité partielle avec un système axiomatique complet et valide. L\u0027originalité de cette approche c\u0027est qu\u0027elle est autonome : elle commence directement à partir du contexte d\u0027extraction pour dé-river l\u0027ensemble des concepts réduits fréquents, le treillis de l\u0027iceberg de Galois et la base minimale générique de règles associatives FAST-MGB.\nGénération de la base FAST-MGB\nDans le cadre de notre travail, nous considérons les règles associatives qui minimisent le nombre de termes dans la prémisse et qui maximisent le nombre de termes dans la conclusion.\nDéfinition formelle\nLa base générique FAST-MGB est définie comme suit : Définition 2 Soit AR k l\u0027ensemble des règles associatives pouvant être extraites à partir d\u0027un contexte d\u0027extraction k. Une règle R : X ? Y ? AR k est redondante par rapport à R 1 : X 1 ? Y 1 si et seulement si les deux conditions suivantes sont vérifiées :\nDans la suite, nous définissons la base FAST-MGB comme suit :\n1. L c : le treillis de l\u0027iceberg de Galois, contenant tous les itemsets fermés fréquents pouvant être extraits à partir d\u0027un contexte d\u0027extraction k, et associés à leurs géné rateurs minimaux ainsi que leurs supports respectifs.\n2. S : l\u0027ensemble des successeurs immédiats d\u0027un itemset fermé fréquent c i .\n3. G ci : l\u0027ensemble des générateurs minimaux d\u0027un itemset fermé fréquent c i . \nTAB. 1 -Notations utilisées par l\u0027algorithme GEN-FAST-MGB\nFormellement la base générique FAST-MGB est définie comme suit :\nDans ce qui suit, nous présentons l\u0027algorithme de construction de la base FAST-MGB directement à partir du contexte d\u0027extraction (voir l\u0027algorithme 1 et le tableau 1). Étape1 : Générer l\u0027ensemble des concepts réduits fréquents du treillis de l\u0027iceberg de Galois enrichi par les générateurs minimaux L\u0027algorithme GEN-CRF (voir algorithme 2) est itératif. Dans chaque itération k, il construit un ensemble de concepts formels réduits candidats (CRC k ) qui sera élagué ensuite, en respectant la contrainte de minsupp.\nÉtape 2 : Générer le treillis de l\u0027iceberg de Galois enrichi par les générateurs minimaux La dérivation du treillis de l\u0027iceberg de Galois, illustrée par l\u0027algorithme 3 passe principalement par deux étapes, à savoir, i) La génération de la liste de tous les successeurs d\u0027un concept donné et ii) À partir de la liste ainsi obtenue, ne retenir que les successeurs directement placés audessus du concept en question. \n"
  },
  {
    "id": "1047",
    "text": "Introduction\nLa quantité de sources d\u0027information disponible sur Internet fait des systèmes d\u0027échanges pair-à-pair (P2P) un genre nouveau d\u0027architecture qui offre à une large communauté des applications pour partager des fichiers, partager des calculs, dialoguer ou communiquer en temps réel, etc (Miller (2001), Ngan et al. (2003)). Les applications P2P fournissent également une bonne infrastructure pour les opérations sur de grandes masses de données ou avec de très nombreux calculs, comme la fouille de données. Dans ce cadre, nous considérons une nouvelle approche pour améliorer la localisation de ressources dans un environnement P2P non structuré selon deux aspects principaux pour extraire des comportements fréquents :\n1. L\u0027ordre des séquences entre les actions réalisées sur les noeuds (requête ou télécharge-ment) est pris en compte pour améliorer les résultats. 2. Les résultats des calculs distribués sont maintenus via un \"Pair centralisé\" pour réduire le nombre de communications entre les pairs connectés. Connaître l\u0027ordre des séquences des actions réalisées sur les pairs offre une connaissance importante. Par exemple, en examinant les actions réalisées, nous pouvons savoir que pour 77% des noeuds pour lesquels il y a une requête concernant \"Mandriva Linux\", le fichier \"Mandriva Linux 2005 CD1 i585-Limited-Edition-Mini.iso\" est choisi et téléchargé. Cette requête est suivie par la demande des images iso (i.e \"Mandriva Linux 2005 Limited Edition\"), et dans la grande quantité de résultats retournés, l\u0027image \"Mandriva Linux 2005 CD2 i585-LimitedEdition-Mini.iso\" est choisie et téléchargée. L\u0027un des problèmes principaux des systèmes P2P non structurés comme Gnutella est que les requêtes sont envoyées à un trop grand nombre de noeuds (broadcast) entraînant ainsi une consommation excessive de la bande passante (Ng et al. (2003)). Proposer à l\u0027avance à l\u0027utilisateur les fichiers souvent associés à une requête ou à un téléchargement permet d\u0027éviter une consommation excessive de la bande passante dans la mesure où l\u0027on connaît à l\u0027avance les ressources à extraire. Il suffit alors d\u0027enrichir le résultat de la première requête avec des informations complémentaires sur les fichiers majoritairement téléchargés par les autres utilisateurs.\nRechercher des règles d\u0027association ou des motifs séquentiels dans un système aussi distribué que les systèmes P2P n\u0027est pas une tâche facile. En effet, par nature, ces systèmes sont très dynamiques, i.e. les noeuds agissent indépendamment les uns des autres, et les connaissances acquises ne sont alors plus forcément représentatives. Par exemple, quand un noeud disparaît, les séquences de ce noeud disparaissent également de la base distribuée et la connaissance extraite doit être reconsidérée. Les approches d\u0027extraction de motifs séquentiels traditionnelles (Srikant (1995), Pei et al. (2001)) qui considèrent que la base est disponible dans son intégra-lité ne sont donc plus utilisable dans un contexte aussi dynamique. Notre proposition se situe dans ce cadre et prend en considération l\u0027aspect dynamique des systèmes pairs à pairs non structurés.\nDans la suite de cet article, la section 2 présente la problématique de la recherche de motifs séquentiels dans une base de données distribuée. Dans la section 3, nous proposons une nouvelle approche basée sur une heuristique. Les expérimentations menées sont décrites dans la section 4. Puis la section 5 conclut cet article.\nProblématique\nDans cette section, nous étendons la problématique intitiale de la recherche de motifs sé-quentiels Srikant (1995) à un environnement P2P non structuré. Soit I \u003d {x 1 , . . . , x n } un ensemble de littéraux distincts appelés items. Nous considérons par la suite que pour chaque item nous connaissons l\u0027action réalisée, i.e. requête ou téléchargement. Un item est vide jusqu\u0027à ce qu\u0027un noeud envoie sa séquence. L\u0027architecture P2P non structuré que nous proposons utilise un pair spécial (appelé par la suite pair \"Distributed SP \") qui est connecté à tous les nouveaux pairs qui arrivent sur le réseau (l\u0027instruction send(v,@Distributed SP ) dans l\u0027algorithme 2, permet à Distributed SP d\u0027être au courant de l\u0027arrivée du noeud \"v\"). Notre méthode utilise alors une distribution des séquences candidates comme illustrée par la figure 1. Le pair \"Distributed SP \" réalise les instructions de l\u0027heuristique Distributed SP , de \"GetValuation\" à \"Broadcast\". Tout d\u0027abord, l\u0027ensemble des items fréquents est extrait des pairs connectés. Puis l\u0027ensemble de tous les candidats de taille 2 est généré. Ces candidats sont évalués par les pairs connectés (u t ..v t ) pour connaître ceux qui ont un nombre d\u0027occurrences suffisant sur toute la base, i.e. sur\nLes résultats sont récupérés par le pair Distributed SP (i.e. fonction \"GetValuation\"). L\u0027heuristique, basée sur des opérateurs génétiques est alors appliquée et le nouvel ensemble de candidats est envoyé aux pairs connectés pour évaluation. Ce processus est répété tant qu\u0027il existe des noeuds connectés.\nDistributed SP débute lorsqu\u0027un noeud u t se connecte ((recvu t )). L\u0027ensemble des motifs fréquent est alors initialisé avec la séquence de u t . Tant que des noeuds sont disponibles, nous considérons les motifs envoyés à Distributed SP par la fonction getV aluation afin de dé-terminer si une séquence est fréquente. SCORE correspond à une note moyenne donnée par tous les noeuds pour les candidats. Si SCORE est plus grand ou égal à la valeur de support, le candidat devient fréquent et est stocké dans F Dt . Nous conservons également les sous sé-quences candidates non fréquentes, appelées séquences approximatives et stockées dans˜Fdans˜ dans˜F Dt , -471 -RNTI-E-6\nAnalyse des usages dans les systèmes pair-à-pair dont la taille par rapport à la taille totale des séquences candidates vérifie une contrainte de distance spécifiée par l\u0027utilisateur mindist. Ces séquences seront utilisées pour les phases de génération de candidats. Grâce aux séquences fréquentes approximatives et aux opérateurs de voisinage, de nouveaux candidats sont générés et envoyés par broadcast aux noeuds connectés. Par manque de place, nous ne décrivons pas les opérateurs de voisinage utilisés. Le lecteur intéressé peut se reporter à Masseglia et al. (2003) où nous utilisons, dans un autre contexte, des opérateurs génétiques similaires.\nDeux opérations principales sont réalisées dans l\u0027algorithme node. Premièrement, lorsqu\u0027un nouveau pair v t essaye de se connecter à un noeud u t (recv(v,connect)), un message lui indiquant l\u0027adresse de Distributed SP lui est retourné. Deuxièmement, lorsqu\u0027un message de Distributed SP est reçu, un score représentant la distance entre un candidat et les opérations locales effectuées sur le noeud est calculé. Si un candidat est inclus, son score est positionné à 100 + size(candidate). Comme notre approche est heuristique, nous récompensons fortement les candidats complètement inclus dans une séquence. En outre comme nous recherchons les comportements les plus longs, nous récompensons les longues séquences. Ceci est réalisé par l\u0027algorithme Longest-Common-Subsequence (LCS) Cormen et al. (2001).\nExperimentations\nPour évaluer notre approche, différentes expériences ont été menées sur des jeux de données réelles : le fichier de données \"Pumsb\" Repository et un fichier d\u0027access log (AccessLog). Les premières expériences ont été réalisées pour analyser la convergence des résultats ainsi que les coûts de communication. Dans le premier cas, un algorithme traditionnel de recherche de motifs a d\u0027abord été appliqué sur la base globale. Chaque population de candidat proposée par notre approche est alors comparée au résultat réel de manière à déterminer sa qualité. Pour cela, nous mesurons pour chaque population de candidat, la plus longue sous séquence -472 -RNTI-E-6 F. Masseglia et al. commune (LCS) entre les séquences candidates et le résultat réel. Puis la base est partitionnée en différents noeuds et notre approche est appliquée. Les résultats de l\u0027expérience sont décrits par la figure 2. Nous remarquons que pour Pumsb, à la première génération, la qualité de la population candidate est supérieure à 50%. A la seconde, nous avons 70%. Pour les deux jeux de données, à partir de la génération 6, la qualité du résultat est proche de 95% et nous devons attendre la génération 7 pour avoir 100%. Ces résultats montrent que notre approche peut rapidement obtenir des longues séquences fréquentes en ne réalisant que 7 opérations de broadcast.\nFIG. 2 -Qualité des résultats pour les populations proposées\nPour évaluer le comportement de notre approche lorsque des noeuds apparaissent ou disparaissent, nous avons considéré les deux jeux de données. L\u0027idée principale est la suivante : nous voulons analyser le comportement de notre approche lorsque x% séquence de la base d\u0027origine Pumsb sont remplacés par x% séquences de la base destination AccessLog. Nos expériences consistent donc à estimer la qualité des résultats par rapport à un algorithme traditionnel une fois que toute la base a été modifiée. Pour simuler un comportement réaliste du système, nous avons procédé à des remplacements par intervalle de 1-3% à chaque génération. Les résultats ont montré que lorsque Pumsb était remplacé à un rythme de 1% par génération, la qualité des -473 -RNTI-E-6\nAnalyse des usages dans les systèmes pair-à-pair résultats à la fin du processus de remplacement est de 100%.\nConclusion\nDans cet article, nous proposons une nouvelle approche pour améliorer la localisation de ressources dans des systèmes P2P non structurés. Cette approche est inspirée des algorithmes génétiques pour retrouver efficacement les séquences fréquentes dans les noeuds du réseau. Les expériences réalisées ont montré que cette approche est efficace d\u0027une part pour retrouver les comportements fréquents (100% des fréquents sont déterminés en 7 générations quelque soit leur longueur) mais également pour prendre en compte les évolutions dans le réseau (modification forte du comportement des noeuds).\n"
  },
  {
    "id": "1049",
    "text": "Introduction\nPoussés par la demande des étudiants branchés, un grand nombre d\u0027universités et d\u0027éta-blissements scolaires se sont lancés dans le design, le développement et l\u0027utilisation des technologies de l\u0027information et de la communication pour créer, partager et diffuser leur matériel pédagogique.\nLe but de notre de recherche est de favoriser l\u0027accès aux ressources pédagogiques afin de promouvoir la formation continue et la diffusion des derniers résultats de recherche. Plus pré-cisément, nous voulons développer un système de classification et d\u0027organisation qui permettra de donner accès aux ressources pédagogiques créées par les professeurs suivant les besoins des utilisateurs. Cet accès pourra se faire :\n-suivant la structure d\u0027enseignement (plan de cours) ; ce sera le chemin d\u0027accès privilégié des étudiants inscrits dans une université ; -suivant des ontologies de domaines ou par mots-clés ; ce sera le chemin d\u0027accès privilé-gié du grand public qui recherche des documents sur un sujet ou un thème donné -suivant les compétences que permettent de développer la lecture des documents ; ce sera le chemin privilégié des personnes qui veulent parfaire leur formation ou acquérir de nouvelles compétences.\nModèle de compétences et ressources pédagogiques\nL\u0027utilisation de la notion de compétence par les gestionnaires et les spécialistes des ressources humaines, a permis aux organisations de comprendre l\u0027importance de leurs ressources humaines et de reconnaître que les gens, les connaissances, les capacités et les habiletés réunis dans le milieu du travail constituent un levier fondamental pour leur réussite.\nSuite à ce constat, de nombreuses recherches ont porté sur la compréhension et la définition de la notion de compétence. Les conclusions d\u0027une étude transcanadienne montre que les élé-ments communs qui ressortent le plus souvent dans la définition du concept de compétences à travers les organismes canadiens sont : les connaissances, les habiletés, les capacités, les aptitudes, les qualités personnelles, le comportement et l\u0027impact sur le rendement du travail. \nConclusion et Travail futur\nCe travail est la première partie du développement d\u0027un serveur de ressources pédago-giques basés sur les compétences. Ce serveur s\u0027intégrera dans l\u0027architecture de Zone Cours (zonecours.hec.ca), outil de gestion de ressources pédagogiques de HEC Montréal, et viendra compléter nos outils de diffusions de connaissances vers le grand public. Bloom B. (1956) \nRéférences\nSummary\nThe aim of our of research is to give access to teaching resources according to users needs and according to competences they want to acquire. We present here a model of competences and resources on which our future system will be based.\n"
  },
  {
    "id": "1050",
    "text": "Introduction\nLa recherche d\u0027information dans les bases de données image est toujours un défi. Pour l\u0027être humain, l\u0027accès à la sémantique d\u0027une image est naturel et non explicite. Par conséquent, la sémantique provient de l\u0027image sans processus cognitif explicite. Dans la vision par ordinateur, il existe plusieurs niveaux d\u0027interprétation. Le plus bas est celui des pixels et le plus haut est celui des scènes ; entre eux beaucoup de niveaux d\u0027abstraction existent. Le défi est alors de remplir la gouffre entre le bas niveau et le haut niveau.\nIl existe au moins deux issues intermédiaires auxquelles nous nous intéressons. La première est la représentation de l\u0027image sous forme de vecteurs qui est appelée indexation. Elle consiste à extraire quelques caractéristiques (composantes d\u0027un vecteur) à partir de la représen-tation de bas niveau(Pixel). Par exemple, l\u0027histogramme des couleurs, les différents moments, les paramètres de forme, etc. La seconde issue est l\u0027ensemble des étiquettes associées à une image. Ces étiquettes sont fournit par l\u0027humain au moyen de mots, d\u0027adjectifs, ou au moyen de tout autre attribut symbolique. Les étiquettes sont compréhensibles et mieux manipulées. La sémantique peut être considérée comme le résultat du traitement des attributs symboliques qui sont liés à l\u0027image.\nDonner à l\u0027ordinateur la capacité d\u0027imiter l\u0027être humain dans l\u0027analyse de scènes nécessite d\u0027expliciter le processus par lequel il peut se déplacer de la représentation bas niveau à la haut niveau (niveau sémantique). Le bas niveau utilise les caractéristiques, qui peuvent être extraites, à partir des données multimédias comme la couleur, la texture, la forme, etc. Le haut niveau quant à lui, consiste généralement en une liste de mots-clés qui est associée à la donnée multimédia qui sert à décrire son contenu sémantique. L\u0027utilisation d\u0027annotations textuelles présente deux inconvénients principaux : le premier est le fait que cette tâche est lente et très coûteuse ; la seconde est lié à la subjectivité de l\u0027annotation des données multimédias. En effet, par exemple, deux personnes différentes peuvent annoter la même image de deux manières différentes.\nA cause de ces inconvénients, l\u0027interrogation est généralement faite en utilisant les caractéristiques de bas niveau. Dans toutes les approches d\u0027interrogation de données multimédias (l\u0027approche bas niveau ou l\u0027approche haut niveau), chaque donnée est localisée par ses coordonnées dans un espace multidimensionnel R p . Un vecteur de caractéristiques (caractéristiques de bas niveau ou annotations textuelles) est associé à chaque donnée. Rui et Huang (1999) estiment que l\u0027interrogation par le contenu ne peut être effectuée de manière efficace uniquement en combinant les deux niveaux (bas niveau et haut niveau). Cependant, ceci peut soulever le problème de la subjectivité des annotations ce qui est un problème important qui peut détériorer considérablement les performances d\u0027un système de recherche d\u0027informations par le contenu.\nAfin de capturer des aspects sémantiques à partir des caractéristiques de bas niveau, l\u0027utilisation d\u0027un index multimédia est nécessaire. Un index permet de regrouper des individus ayant des caractéristiques assez proches.\nPlusieurs systèmes de recherche d\u0027informations par le contenu se basent sur le principe des k plus proche voisins (Fix et Hudges, 1951) en utilisant une mesure de similarité (Veltkamp et Tanase, 2000). L\u0027idée est de trier les individus de la base de données, en fonction de leur distance, par rapport à l\u0027individu requête, et ensuite répondre à la requête en retournant un nombre k fixe d\u0027individus les plus proches. Par exemple, le système QBIC, dans son implé-mentation pour le musée de l\u0027Hermitage 1 (Faloutsos et al., 1994) renvoie les 12 plus proches images voisines de l\u0027image requête. Les inconvénients d\u0027une telle approche sont discutés dans Scuturici et al. (2004).\nLe modèle de structuration des bases de données multimédias est (ou peut être vu) comme un graphe basé sur des relations de similitude entre les individus, par exemple K-NN (Mitchell, 1997) ou le graphe des voisins relatifs (Scuturici et al., 2004). L\u0027objectif est d\u0027explorer une base de données d\u0027images par les similarités entre les images. Explorer les similarités peut être considéré comme la recherche des voisins des images requêtes.\nLe modèle de structuration est très important car les performances d\u0027un système de recherche d\u0027informations par le contenu dépend fortement sur la structure de représentation (structure d\u0027indexation) qui gère les données.\nPlusieurs systèmes de recherche d\u0027informations multimédias ont été proposés. les systèmes de recherche d\u0027images sont plus répandus que ceux pour la vidéo. Nous pouvons citer par exemple QBIC (Flickner et al., 1995;Niblack et al., 1993), CANDID (Kelly et al., 1995), CHABOT (Ogle et Stonebraker, 1995), VIRAGE (Ogle et Stonebraker, 1995), PhotoBook (Pentland et al., 1994), BlobWorld (Carson et al., 1999), VisualSeek (Chang et al., 1996;Smith et Chang, 1997) et RETIN (Fournier et al., 2001) pour les images, et CVEPS (Chang et al., 1996), JAKOB (La-Cascia et Ardizzone, 1996), VISION (Li et al., 1996), et SWIN (Zhang et al., 1995) pour la vidéo.\nA partir de maintenant, nous considérerons le contexte de l\u0027interrogation des grandes bases de données images par le contenu pour illustrer les propositions de cet article.\nGraphes de voisinage\nLes graphes de voisinage sont utilisés dans divers systèmes. Leur popularité est due au fait que le voisinage est déterminé par des fonctions cohérentes qui reflètent, d\u0027un certain point de vue, le mécanisme de l\u0027intuition humaine. Cependant, plusieurs problèmes relatifs au graphes de voisinage sont toujours d\u0027actualité et exigent des travaux détaillés afin de les résoudre. Ces problèmes sont principalement liés à leur coût de construction élevé et à leurs difficultés de mise à jour. Pour cette raison, les optimisations sont nécessaires pour leur construction et leur mise à jour.\nAfin d\u0027éviter quelques problèmes liés à l\u0027utilisation des K-NN (problème de symétrie, subjectivité liée à la détermination du parcmètre k), l\u0027utilisation d\u0027un autre modèle de structuration basé sur les graphes de voisinage a été proposé dans ( (Scuturici et al., 2004)). Cette proposition a beaucoup d\u0027avantages, c\u0027est pourquoi nous adoptons la même approche (l\u0027utilisation des graphes de voisinage) pour l\u0027interrogation d\u0027images par le contenu. Nous allons dans ce qui suit présenter les graphes de voisinage.\nLes graphes de voisinage ou graphes de proximité sont des structures géométriques qui utilisent le concept de voisinage pour déterminer les sommets les plus proches d\u0027un sommet donné. Pour cela, ils se basent sur les mesures de \"distances\" (Toussaint (1991)). Nous allons utiliser les notations suivantes dans cet article :\nSoit ? un ensemble de points dans un espace multidimensionnel R d . Un graphe G(?,?) est composé de l\u0027ensemble de points ? et de l\u0027ensemble d\u0027arêtes ?. A chaque graphe nous pouvons associer une relation binaire R sur ?, dans laquelle un couple de points (?, ?) ? ? 2 sont en relation binaire si et seulement si (?, ?) ? ?. En d\u0027autres termes, (?, ?) sont en relation binaire si et seulement s\u0027ils sont directement reliés dans le graphe G. A partir de là, le voisinage V (?) d\u0027un point ? dans le graphe G, peut être considéré comme un sous-graphe qui contient le point ? ainsi que tous les points qui sont directement relié à ce point.\nPlusieurs possibilités ont été proposées pour la construction des graphes de voisinage. Nous pouvons citer la triangulation de Delaunay (Preparata et Shamos, 1985), le graphe des voisins relatifs (Toussaint, 1980), le graphe de Gabriel (Gabriel et Sokal, 1969) et l\u0027arbre de recouvrement minimum (Preparata et Shamos, 1985). Dans cet article, nous considérons seulement l\u0027un d\u0027entre eux, le graphe des voisins relatifs (RN G). La motivation principale pour ce choix est sa simplicité et sa large utilisation. Nous décrivons ci-après deux exemples de graphes de voisinage : le graphe des voisins relatifs (RN G) et le graphe de Gabriel (GG).\nGraphe de voisins relatifs\nDans un graphe de voisins relatifs G rng (?, ?), deux points (?, ?) ? ? 2 sont des voisins s\u0027ils vérifient la propriété de voisinage définie ci-après.\nSoit H (?, ?) une hyper-sphère de rayon ? (?, ?) et de centre ?, et soit H (?, ?) une hypersphère de rayon ? (?, ?) et de centre ?. ? (?, ?) et ? (?, ?) sont des mesures de similirité entre les deux points ? et ?. ? (?, ?) \u003d ? (?, ?). Alors, ? et ? sont des voisins si et seulement si la Toussaint (1980)). Formellement :\nLa figure 1 illustre le graphe des voisins relatifs.\nGraphe de Gabriel\nCe graphe est proposé par Gabriel et Sokal (1969) dans un contexte de mesure de variations géographiques. Soit H (?, ?) l\u0027hyper-sphère de diamètre ? (?, ?) (cf. figure 2). Alors, ? est le voisin de ? si et seulement si l\u0027hyper-sphère H (?, ?) est vide.Formellement\nAlgorithmes de construction des graphes de voisinage\nNous pouvons considérer deux situations quand nous traitons le problème d\u0027optimisation des graphes de voisinage. La première situation est quand nous avons à disposition un un graphe déjà construit. Dans cette situation, si nous utilisons une méthode d\u0027approximation, nous risquons d\u0027obtenir un autre graphe avec un voisinage de moindre qualité que l\u0027existant, nous pouvons obtenir plus ou moins de voisins pour quelques individus. Dans ce cas, nous devons trouver une solution pour mettre à jour efficacement le graphe sans le reconstruire entièrement. La deuxième situation est celle où le graphe n\u0027est pas encore construit. Dans cette situation nous pouvons appliquer une méthode d\u0027approximation pour avoir un graphe qui est aussi similaire que possible à celui que nous pouvons obtenir en utilisant l\u0027algorithme standard. Nous sommes intéressés dans cet article par le premier cas.\nPlusieurs algorithmes pour la construction des graphes de voisinage ont été proposés. Les algorithmes que nous citons ci-après concernent la construction du graphe des voisins relatifs.\nL\u0027une des approches commune aux différents algorithmes est l\u0027utilisation des techniques dédé n raffinement ? z. Dans ce type d\u0027approches, le graphe est construit par étapes. Chaque graphe est construit à partir du graphe précèdent, contenant toutes les connexions, en éliminant un certain nombre dŠarrêtes qui ne vérifient pas la propriété de voisinage du graphe à construire. L\u0027élagage (élimination des arrêtes) se fait généralement en tenant compte de la fonction de construction du graphe ou à travers des propriétés géométriques.\nLe principe de construction des graphes de voisinage consiste à chercher pour chaque point si les autres points de lŠespace sont dans son voisinage. Le coût de cette opération est de complexité O(n 3 )(n étant le nombre de points dans lŠespace). Toussaint (Toussaint, 1991) a proposé un algorithme de complexité O(n 2 ). Il déduit le RN G à partir dŠune triangulation de Delaunay (Preparata et Shamos, 1985). En utilisant les voisins géographiques (Octant neighbors) Katajainen (1988)  En ce qui nous concerne, l\u0027approche que nous proposons est une amélioration de celle déjà proposée dans Scuturici et al. (2004). En effet, avec l\u0027ancienne méthode, le graphe n\u0027est pas vraiment mis à jour. Les voisins d\u0027un individu requête sont considérés comme étant les voisins de son plus proche voisin. Cette approche n\u0027est pas correcte car dans un espace multidimensionnel et avec les contraintes géométriques à respecter, les voisins d\u0027un individu ne peuvent être ceux de son voisin le plus proche. Ainsi, en utilisant cette méthode le graphe sera inévita-blement détérioré. Nous proposons dans ce qui suit une méthode de mise à jour locale efficace qui est stable et insensible aux effets de la dimension des données).\nRecherche d\u0027informations par le contenu : approche par graphes de voisinage\nL\u0027interrogation des bases de données images est généralement faite par la soumission d\u0027une requête au système, cette requête est généralement sous forme d\u0027image, le système pré-traite (segmente, égalise, etc.)la requête et produit un vecteur de descripteurs qui représente un point dans un espace multidimensionnel. Ce point est inséré dans la structure de représentation (structure d\u0027indexation) et ses voisins sont alors retournés comme une réponse à la requête.\nDans notre cas, une approche naïve en utilisant les graphes de voisinage serait la reconstruction du graphe de voisinage qui contient les données déjà existantes dans la base de données tout en ajoutant l\u0027individu requête. Cette approche n\u0027est, malheureusement, pas appropriée car elle est très coûteuse particulièrement quand le nombre d\u0027individus dans la base de données est important. Une autre approche est de mettre à jour localement le graphe de voisinage, c\u0027est-à-dire, trouver une manière de telle sorte que seuls les individus potentiellement voisins soient affectés par la possible modification ou interrogation.\nLa tâche de mise à jour locale des graphes de voisinage passe par la localisation du point inséré aussi bien que les points qui peuvent être affectés par la mise à jour. Pour cela, nous procédons en deux étapes principales : nous recherchons d\u0027abord une surface optimale de l\u0027espace de représentation pouvant contenir un nombre maximum de points potentiellement voisins au point requête. La deuxième étape est réalisée dans le but de filtrer les individus trouvés préa-lablement afin de récupérer les vrais voisins en considérant une propriété de voisinage. Cette dernière étape cause la mise à jour effective des relations de voisinage entre les points concernés.\nL\u0027étape principale dans cette méthode est la détermination de la surface de recherche. Ceci peut être considéré comme un problème de détermination d\u0027une hyper-sphère ayant pour centre -16 -RNTI-E-6 le point requête ? maximisant les chances de contenir les voisins du point requête tout en réduisant au minimum le nombre de points qu\u0027elle contient.\nNous tirons profit de la structure générale des graphes de voisinage afin d\u0027établir le rayon de l\u0027hyper-sphère. Nous nous concentrons particulièrement sur le concept du voisin le plus proche et le concept du voisin le plus éloigné. Ainsi, deux observations en relation avec ces deux concepts nous semblent intéressantes :\n-Les voisins du voisin le plus proche du point requête sont des candidats potentiels au voisinage du point requête.\nA partir de là et par généralisation, nous pouvons déduire que : -Tous les voisins directs d\u0027un point sont également des candidats au voisinage d\u0027un point requête pour lequel il est voisin. Concernant la première étape, le rayon de l\u0027hyper-sphère ayant les propriétés citées cidessus est celui comprenant tous les voisins du plus proche voisin de la requête. Ainsi, en considérant que l\u0027hyper-sphère est centrée dans ?, son rayon est égale à la somme des distances entre le point requête ? et de son plus proche voisin et celle entre le voisin le plus proche et son voisin le plus éloigné.\nLe contenu de l\u0027hyper-sphère est traité pour vérifier s\u0027il existe quelques voisins (ou tous les voisins). La deuxième étape constitue une étape de renforcement et vise à éliminer le risque de perdre des voisins ou d\u0027en inclure des faux. Cette étape procède de telle sorte à tirer profit de la deuxième observation. Ainsi, nous prenons tous les vrais voisins du point requête, récupérés précédemment (ceux retournés dans la première étape), ainsi que que leurs voisins et mettons à jour les relations de voisinage entre ces points.\nConsidérons alors ? le point requête et ? son plus proche voisin avec une distance ? 1 . Considérons aussi ? le voisin le plus loin de ? avec une distance ? 2 . Le rayon SR de l\u0027hypersphère peut être exprimé avec la formule suivante : SR \u003d ? 1 + ? 2 + est un paramètre de relaxation, il peut être fixé selon l\u0027état des données (leur dispersion par exemple) ou par la connaissance du domaine. Nous avons fixé expérimentalement ce paramètre à 1.\nLa complexité de cette méthode est très basse et se rejoint parfaitement notre objectif de départ (localisation des voisins d\u0027un point dans un temps trés court). Elle est exprimée par :\navec -n :le nombre d\u0027individus dans la base de données.\n-n :le nombre d\u0027individus dans l\u0027hyper-sphère (\u003c\u003c n). Cette complexité inclue les deux étapes décrites précédemment, à savoir, la recherche du rayon de l\u0027hyper-sphère et la mise à jour du voisinage dans l\u0027hyper-sphère. Le deuxième terme correspond au temps nécessaire pour la mise à jour effective des relations de voisinage entre les vrais voisins. Le temps necessaire pour cette opérations est trés faible. Cette complexité constitue la complexité maximale et peut être optimisée de plusieurs manières. La plus simple est d\u0027employer un algorithme plus robuste pour la recherche du plus proche voisin à la place du parcours séquentiel. L\u0027exemple ci-après illustre et récapitule le principe de la méthode. En ce qui nous concerne, nous sommes intéressés dans la présente section par deux différents types d\u0027évaluation : évaluation de la validité des résultats obtenus par l\u0027utilisation de la méthode proposée et l\u0027évaluation des temps de réponse. Nous utilisons pour cela une base de données d\u0027images (Nene et al., 1996). Cette base de données contient 7200 images repré-sentant 100 objets différents pris avec sous diverses vues. Pour réaliser nos expérimentations, nous pré-traitons la base de données en appliquant quelques algorithmes de traitement d\u0027image pour extraire certaines caractéristiques. Nous employons particulièrement la couleur, quelques caractéristiques de texture et de forme. Chaque image est représentée sous forme d\u0027un vecteur de caractéristiques. Un graphe de voisinage est alors construit en utilisant les vecteurs. L\u0027interrogation est également faite de la même manière en employant un vecteur de caractéristiques. Ainsi, chaque image est représentée comme un point dans un espace multidimensionnel.\nNotre intérêt en effectuant ces expériences est de montrer que notre méthode donne les mêmes résultats que ceux obtenus en construisant un graphe de voisinage avec un algorithme standard (qualité des résultats).\nPour vérifier la validité des résultats obtenus, nous avons besoin d\u0027un graphe de référence. Pour cela, nous prenons la base de données entière (vecteurs de descripteurs) et nous construisons un graphe de voisins relatifs. Après avoir établi le graphe de référence, nous pouvons commencer les tests de validité. Nous prenons arbitrairement un individu de la base de données et nous construisons un nouveau graphe de voisins relatifs avec les individus restant dans la base de données (les n ? 1 individus). Après cela, nous insérons l\u0027individu préalablement retiré dans le graphe en employant la méthode proposée. Les voisins du point inséré sont alors comparés aux voisins de ce même individu dans le graphe de référence. Cette opération est répétée plusieurs fois en prenant différents individus à chaque itération.\nLe Tableau 1 illustre un voisinage obtenu en insérant des individus aléatoirement pris de la collection d\u0027image. Chaque individu dans l\u0027ensemble de données est identifié par un identifiant unique. La première colonne de chaque tableau constitue l\u0027individu requête. Nous pouvons clairement voir que les résultats obtenus après l\u0027insertion de l\u0027individu restant (Graphe 2), sont exactement les mêmes que ceux obtenus en employant l\u0027algorithme standard (Graphe 1). Ainsi, nous ne perdons pas de voisins. Une évaluation globale (utilisant les mesures de rappel et de précision) est donnée dans (Hacid et Zighed (2005)  IMG74  IMG74  IMG87  IMG87  IMG85  IMG118  IMG85  IMG118  IMG131  IMG131  IMG3665  IMG3665  IMG484  IMG484  IMG623  IMG623  IMG7170  IMG7170  IMG4804  IMG4763  IMG4804  IMG4763  IMG4803  IMG4803  IMG4805  IMG4805  IMG6532  IMG6532  IMG4345  IMG4345  IMG6781  IMG6781  IMG6791  IMG6791  IMG6807  IMG6795  IMG6807  IMG6795  IMG6825  IMG6825  IMG6830  IMG6830  IMG74  IMG74  IMG87  IMG87  IMG7196  IMG7171  IMG7196  IMG7171  IMG7195  IMG7195  IMG7197  IMG7197 TAB. 1 -Comparaison du voisinage obtenu en utilisant un graphe de référence (Graphe 1) et en utilisant des insertions locales (Graphe 2)\nLa validité des résultats obtenus étant acquise, à présent nous sommes intéressés par les temps de réponse en utilisant la méthode proposée, c\u0027est-à-dire, les temps que la méthode prend pour insérer un individu requête dans une structure existante. Nous employons le même protocole que celui décrit précédemment, mais au lieu de récupérer les individus voisins, nous prenons en considération uniquement les temps de réponse de chaque requête. Nous utilisons pour ces expériences une machine avec un processeur INTEL Pentium 4 (2.80 Gz) et 512 Mo de mémoire. Les temps de réponse de 10 insertions, sont montrés dans le graphique dans la figure 4.\nLes temps de réponse (exprimés en millisecondes) sont intéressants en considérant le volume de données utilisé, ils sont variable d\u0027un individu à un autre, ceci est due particulièrement \nConclusion et travaux futurs\nLa recherche d\u0027informations par le contenu dans les bases de données multimédias est une tache complexe en raison de, principalement, la nature des données multimédias et la subjectivité liée à leur interprétation. L\u0027utilisation d\u0027une structure d\u0027index appropriée est primordiale.\nDans cet article nous nous sommes intéressés aux graphes de voisinage qui constituent notre structure d\u0027indexation, nous avons proposé une méthode pour mettre à jour localement les graphes de voisinage dans un but d\u0027apporter une amélioration à une approche existante. Notre méthode est basée sur la localisation des individus potentiels, qui peuvent être affectés par la mise à jour. Les expérimentations effectuées sur différents ensembles de données ont montré l\u0027efficacité et l\u0027utilité de la méthode proposée.\nEn tant que travaux futurs, nous projetons de fixer le problème du paramètre de relaxation en proposons une fonction de détermination automatique en tenant compte de quelques statistiques comme la dispersion. Par ailleurs, nous envisageons d\u0027étendre cet algorithme afin de mettre en place une version incrémentale de construction des graphes de voisinage. L\u0027intégra-tion de cette approche et son application dans plus de fonctions liées aux données multimédias comme l\u0027annotation automatique et la classification de données multimédias constituent une suite logique de ce travail. Carson, C., M. Thomas, S. Belongie, J. M. Hellerstein, et M. Jitendra (1999). Blobworld : A system for region based image indexing and retrieval. Visual information and informa-\n"
  },
  {
    "id": "1051",
    "text": "La gestion des connaissances en conception\nLa réutilisation des connaissances métier produites lors des projets antérieurs est une stratégie majeure pour améliorer les processus de conception. Actuellement, il est critique de mettre à la disposition des concepteurs les ressources documentaires et bases de données représentant ces connaissances. Les sources des connaissances métier auxquelles nous nous intéressons sont les Systèmes de Gestion des Données Techniques (SGDT). Ces outils sont considérés parfois comme des systèmes de gestion des connaissances quand il s\u0027agit d\u0027optimiser les liens et les relations entre ressources produites par les différents collaborateurs (Cattan, 2001). Pour rendre les ressources disponibles dans les SGDT au service des concepteurs, il faut prendre en compte non seulement les SGDT de l\u0027entreprise mais aussi les SGDT des partenaires (sous-traitants, clients, fournisseurs…). Dans des travaux précédents (projets industriels), nous avons mis en place des solutions de gestion des connaissances autour des SGDT et nous avons rencontré les obstacles suivants : rigidité des structure des données, difficulté de migration et d\u0027interopérabilité, pauvreté des fonctions de recherches. Pour dépasser ces limites, nous avons fait appel à l\u0027approche du Web Socio-Sémantique.\nLe Web Socio-Sémantique en support des SGDT\nDans le cadre de la conception de produits industriels, notre préoccupation principale est de doter les connaissances métier d\u0027une représentation formelle pour rechercher et réutiliser plus pertinemment ces connaissances. Le contenu des ressources SGDT représentant les connaissances à réutiliser doit être ainsi interprétables par les outils informatiques pour qu\u0027ils soient capables de répondre aux requêtes des utilisateurs. D\u0027où notre recours au Web Séman-tique qui vise, selon Tim Berners-Lee, rapporté par (Dieng et al., 2004), à rendre le contenu sémantique des ressources du Web interprétables non seulement par l\u0027homme mais aussi par des programmes, pour une meilleure coopération entre humains et machines.   \nSummary\nIn this paper we discuss an approach based on the Socio-Semantic Web concept enabling knowledge reuse through an extensible infrastructure, in the product development process.\n"
  },
  {
    "id": "1052",
    "text": "Introduction\nXQuery devenant le standard pour interroger XML, de nouveaux besoins apparaissent pour la recherche d\u0027information. Buston et Rys (2003) spécifient des prédicats et fonctionnalités de recherche d\u0027information à intégrer à XQuery, comme la recherche d\u0027élément contenants des mots-clefs, le classement de résultats selon leur pertinence, la recherche basé sur des suffixes ou préfixes de mots. Un premier ensemble des fonctionnalités requises pour XQuery Text est défini par Buxton et Rys (2003). TexQuery, Amer-Yahia (2004), en est le langage précurseur.\nCertaines des fonctionnalités citées précédemment, comme la simple recherche de motsclefs, sont très communes et présentes dans la plupart des SGBD. Dans le cas de données distribuées, il faut d\u0027abord recomposer les partitions avant de pouvoir effectuer une recherche sur le contenu ; d\u0027importantes fonctionnalités souvent nécessaires aux applications ne sont pas faciles à implanter dans un système distribué. Le classement des résultats, les recherches conjonctives de mots-clefs, les recherches sur les racines de mots, leurs préfixes ou suffixes, sont difficilement réalisables car il faut auparavant recomposer les données dispersées. stockant dans un Patricia trie. Cette approche nécessite des extensions pour garder l\u0027ordre des chemins et permettre la correspondance partielle des noms d\u0027éléments.\nNotre approche propose d\u0027unifier les capacités des sources au travers de vues. Le médiateur définit une vue des données distribuées sur plusieurs sources, et permet son interrogation en XQuery Text. Le médiateur ne matérialise pas la vue pour éviter la réplication des données ; il indexe la position du texte dans les éléments structurant la vue. Nous proposons un système d\u0027indexation efficace du contenu textuel de la vue qui repose sur un guide de la vue (applé ViewGuide), véritable résumé structurel de la vue, dérivé de la définition de la vue. Notre système est adapté pour la localisation de mots-clefs dans la vue, et la localisation des données sur les sources. XQuery/IR, Bremer et Gertz (2002), propose des techniques similaires de recherche d\u0027information pour XQuery. Il est aussi basé sur une indexation adaptée à la structure arborescente de XML, et permet de résoudre des requêtes \"tree pattern\" (arbre de filtres applicables à des données XML) dans un système centralisé. L\u0027originalité de notre approche est d\u0027indexer des vues virtuelles et d\u0027avoir une solution complète opérationnelle et efficace, comme le montre les premières mesures.\nLa suite de cet article est organisée comme suit. La section 2 présente le système d\u0027indexation de vues proposé. La section 3 présente le traitement des requêtes sur les vues indexées et la méthode de classement des résultats suivant leur pertinence. Des résultats expérimentaux de notre système sont ensuite rapportés. La conclusion rappelle les contributions et introduit les travaux futurs.\nIndexation textuelle de vues\nLa principale question est de savoir comment intégrer des méthodes de recherche de contenu sur des sources distribuées et hétérogènes. Les systèmes de médiation utilisent souvent les vues pour cibler la recherche sur les sources de données pertinentes. Pour combiner l\u0027intérêt des vues avec la recherche d\u0027information, nous avons décidé d\u0027utiliser des vues semi-matérialisées : le contenu de la vue est indexé par le médiateur, mais n\u0027est pas stocké.\nPrincipes de base\nNous avons choisi d\u0027indexer le contenu de la vue lors de sa création et de maintenir l\u0027index lors des mises à jour ; la position des termes de la vue est mémorisée au niveau du médiateur, ce qui permet de répondre efficacement à des requêtes XQuery Text. L\u0027index détermine indirectement l\u0027adresse des éléments ; cela évite d\u0027importants transferts de données entre les sources et le médiateur : seules les données pertinentes sont échangées. Cela évite également au médiateur de manipuler au travers d\u0027opérations complexes de recherche d\u0027information l\u0027ensemble des données. Ainsi, gérer un index de vue compact et efficace est le but de notre approche pour éviter au médiateur ces opérations difficiles.\nLes identifiants utilisés dans notre index référencent à l\u0027aide de structures gérées par les adaptateurs des objets sur les sources. Ces structures permettent la localisation, l\u0027extraction et la recomposition de données de la vue efficacement à partir des sources. Lorsqu\u0027une source est mise à jour, celle-ci doit le reporter au médiateur afin de mettre à jour les identifiants dans l\u0027index. Ceci est fait par un mécanisme de trigger ou bien par polling périodique de la source. Le mécanisme de report dépend de l\u0027adaptateur de la source.\nPosition des termes dans la vue\nPour retrouver les éléments contenant un terme, le contenu textuel de la vue doit être indexé de façon précise. La position d\u0027un terme dans la vue est identifiée par le document de la vue et le chemin (path) de l\u0027élément le contenant. Nous proposons un système de d\u0027identification d\u0027élément pour coder cette position de façon compacte et unique.  Le ViewGuide est quelque peu similaire au DataGuide (Widom et al.), mais il diffère par les points suivants : (i) c\u0027est uniquement un résumé structurel des documents de la vue ; (ii) il est dérivé de la définition de la vue (c\u0027est-à-dire de la requête définissant la vue) ; (iii) ses liens sont annotés avec la cardinalité (multiple ou non) des éléments. Le ViewGuide permet d\u0027attribuer un identifiant d\u0027élément (IDE) à chaque élément d\u0027un document de la vue. Chaque élément correspond à un identifiant numérique unique, déterminé par un parcours préfixe de l\u0027arbre. La figure 1 présente un exemple de ViewGuide. Un élément critic d\u0027un document de cette vue ne contient qu\u0027un élément book (monovalué) mais peut englober un ou plusieurs éléments review (multivalué). La création du guide impose que la requête définissant la vue (figure 1) décrive complètement la structure des documents dans sa clause Return.\nSystème de numérotation\nUn IDE est composé de la façon suivante : -Un préfix, l\u0027identifiant correspondant au chemin dans le guide de vue.\n-Un suffixe, regroupant les cardinalités de tous les éléments multivalués traversés de la racine à l\u0027élément concerné. Pour identifier le chemin correspondant à critic/review/p, le ViewGuide traverse les éléments I, VII puis VIII. Seul le dernier identifiant (VIII) est utile car le chemin est unique dans le ViewGuide. Le document exemple de la figure 1 possède un élément review contenant deux éléments p, ce qui correspond au chemin critic/review[1]/p [2]. L\u0027identifiant de cet élément sera donc codé par VIII(1,2). Le suffixe (1,2) associé à l\u0027identifiant de chemin VIII permet d\u0027identifier de manière unique le deuxième élément p du premier élément review. Le suffixe n\u0027est utilisé que dans le cas d\u0027éléments multivalués ; un élément monovalué n\u0027aura pas de suffixe, par exemple VI code le chemin critic/book/title.\nPour indiquer la position d\u0027un élément dans la vue, l\u0027identifiant de document (IDG) est associé à l\u0027identifiant de chemin (IDE). Dans la vue de la figure 1, chaque élément critic est un document. L\u0027élément author de la seconde review du quatrième document de la vue est identifié par \u003c4-X(2)\u003e. Un tel couple identifie de manière unique un élément dans la vue.\nPour coder des chemins multiples, nous utilisons des patterns d\u0027identifiant d\u0027éléments permettant de spécifier un ou plusieurs éléments multivalués traversés. Le chemin critic/review/p[1] ne spécifie pas quel élément review choisir. Il se code avec le pattern VIII(*,1), correspondant au premier p de n\u0027importe quel review. Les patterns d\u0027identifiants sont utilisés pour le traitement de requête.\nDéfinition: Pattern d\u0027identifiants (IDEP)\n. IDE ayant pour suffixe des *, signifiant que tout élément est valide.\nIndex des mots\nLe médiateur gère une liste inversée des termes importants, donnant pour chacun sa position dans la vue virtuelle. \nDéfinition: Index des mots (Word index\nPosition dans les sources de données\nUne structure appelée \"Source Map\" maintient un mapping entre un document (IDG) et les sources contenant les données composant ce document. Ces données sont considérées comme des documents locaux référencés par un identifiant. Cet identifiant est associé à une opération d\u0027extraction.\nDéfinition : Identifiant de document local (IDL).\nIdentifiant numérique alloué par un wrapper permettant de récupérer les données correspondantes sur une source.\nLors de la création de la vue, chaque wrapper de source contenant des données pour la vue reçoit une requête pour extraire des données. Les wrappers fournissent ces données au médiateur qui construit les résultats suivant la définition de la vue. Pour chacune de ces données renvoyées, un IDL est créé.\nLa correspondance IDL vers objet local dépend du wrapper. Pour un wrapper fichier, l\u0027IDL peut être l\u0027URI du fichier. Pour des bases XML, ce peut être un identifiant de document, pour des bases relationnelles, une référence à une requête SQL/XML ou XQuery permettant la transformation de tables relationnelles en XML. A partir d\u0027un IDL, le wrapper est capable d\u0027interroger la source pour renvoyer les données correspondantes. Le médiateur recompose alors le document suivant la définition de la vue en récupérant les données.\nDefinition: Source Map. Structure de mapping entre un IDG et un ou plusieurs IDL. Par exemple, si les données book, review et comment de la vue sont réparties sur trois sources, l\u0027IDG du document exemple aura alors quatre IDL (correspondant successivement à un book, une review et deux comment).\nTraitement de requête\nL\u0027algorithme de traitement de requêtes retrouve les entrées de l\u0027index correspondant à une recherche textuelle (liste de mots-clef). Les parties correspondant aux documents sélectionnés sont alors extraites pour recomposer les documents. Nous détaillons dans cette partie comment s\u0027effectue une recherche d\u0027éléments contenant des mots-clefs et comment étendre cette recherche aux fonctionnalités XQuery Text. Nous introduisons aussi notre système de classement de résultats suivant leur pertinence. \nTrouver les éléments pertinents\nFIG. 2 -Algorithme d\u0027intersection\nLa recherche des éléments résultat s\u0027effectue en 3 étapes : 1. Déterminer l\u0027espace de recherche ; 2. Calculer l\u0027ensemble des éléments contenant chacun des mots-clefs ; 3. Extraire les données des sources et recomposer les résultats. L\u0027espace de recherche se définit en utilisant les patterns d\u0027identifiants d\u0027élément. Le NIP est dérivé de l\u0027expression XPath correspondant au prédicat de la requête.\nLe XPath définissant l\u0027espace de recherche dans la requête exemple est critic/review. Il correspond au pattern VII(*). La recherche des mots-clefs s\u0027effectue sur l\u0027élément review et tous ses descendants ; les mots-clefs peuvent se trouver parmi les éléments comments, comment, p, author ou rating. Pour retrouver les identifiants correspondant au descendant d\u0027un élément, une matrice booléenne indique la relation ancêtre/descendant entre deux identifiants. Un élément C ij \u003d 1 si i est un ancêtre de j dans le guide de la vue. Cette matrice fournit une méthode rapide pour retrouver la relation entre deux éléments. L\u0027espace de recherche critic/review est ainsi déterminé par les patterns allant de VII(*) à XII(*).\nLe médiateur interroge l\u0027index des mots pour calculer l\u0027ensemble des entrées contenu dans l\u0027espace de recherche pour chaque mot-clef. L\u0027ensemble des éléments résultat est\nClassement des résultats\nUne méthode de classement associe un poids, basé sur la pertinence, à chaque résultat. Le médiateur doit classer les résultats provenant de plusieurs sources et les regrouper pour les renvoyer dans l\u0027ordre du classement. L\u0027architecture proposée permet de pré-calculer le poids de chaque résultat avant de recomposer les données ; le calcul est réalisé lors de l\u0027interrogation de l\u0027index des mots. La formule de classement doit être précise mais aussi calculable avec les informations contenues dans l\u0027index.\nLe poids d\u0027un résultat est la somme des poids de chaque élément du résultat contenant un ou plusieurs mots-clefs. Notre approche est basée sur la spécificité de chaque résultat. Cette méthode donne plus d\u0027influence aux éléments proche de la racine du résultat. En effet, les mots proches de la racine sont plus importants que ceux dans des éléments plus profonds de l\u0027arbre résultat. Cette méthode reste encore assez simple puisqu\u0027elle ne prend pas en compte la position des mots relativement entre eux.\nLes éléments contenant plusieurs mots recherchés voient leur influence également augmenter. Le pourcentage de mots-clefs présent dans l\u0027élément par rapport à l\u0027ensemble des mots-clefs recherchés est un facteur polynomial ajustant le poids d\u0027un élément.\nFinalement, la formule suivante calcule le poids d\u0027un élément:\nW i est le poids du mot recherché k i , basé sur le tf.idf du mot, N est le nombre total de mots recherchés dans le prédicat. Ni est le nombre de mots recherchés présents dans l\u0027élément et d est la distance entre l\u0027élément et la racine du résultat (nombre de liens). La constante ? permet de faire varier l\u0027influence de la distance à la racine. ? est un facteur polynomial qui permet d\u0027augmenter l\u0027influence des éléments contenant plus de mots recherchés. Le poids total d\u0027un résultat est la somme des poids de chaque élément contenant des mots-clefs.\nCette formule modulable peut s\u0027adapter au besoin de l\u0027utilisateur. Elle peut être étendue ou remplacée. Ainsi le médiateur peut intégrer d\u0027autres formules reposant sur les informations de l\u0027index des mots. La formule est ajustable suivant l\u0027application.\nD\u0027autres systèmes proposent des solutions concrètes pour classer les résultats d\u0027une requête de recherche de mots-clefs. XRANK, Lin et al. (2003), propose une formule calculée suivant le nombre d\u0027arc entrant et sortant (inter et intra document) d\u0027un élément. Ce système utilise comme métrique de proximité des termes la fenêtre minimum contenant les termes, facteur qui reste trop global ne tenant pas compte de la structure du résultat. La distance est aussi prise en compte comme notre approche, ou les éléments les plus éloignés ont moins d\u0027importance. D\u0027autres systèmes comme XXL, Anja et Gerhard (2002), se base sur un opérateur d\u0027imprécision donnant un degré de similarité entre la structure d\u0027un résultat et la demande de la requête. XIRQL, Norbert et Kai (2001), découpe les documents en objets et recherche ces objets suivant leur pertinence de contenu.\nRetour d\u0027expériences\nNous avons testé les performances du système sur trois jeux de données. Les données présentées dans le tableau 3 sont stockées dans des SGBD XML. La taille des données est la taille de la vue composée suivant la structure de la vue critic. Nous avons mesuré le temps de recherche pour trois prédicats : Le second sous-tableau du tableau 3 donne les temps d\u0027exécution de q01 pour 5 motsclefs. Pour chaque jeu de données, l\u0027exécution est réalisée avec une vue indexée, et avec un opérateur de recherche (le médiateur s\u0027occupe de l\u0027opération de recherche dans chaque document). Le temps présenté pour la recherche avec l\u0027index inclut le temps de recherche dans l\u0027index (Word Index et Source Map), l\u0027extraction et la recomposition des résultats par le médiateur. Pour l\u0027opérateur de recherche, le temps inclut la recomposition des résultats suivant la définition de la vue, puis l\u0027application du prédicat de recherche de mots-clefs par un opérateur de sélection. Comme prévu, l\u0027exécution en utilisant l\u0027index est plus efficace car seulement les résultats pertinents sont extraits des sources. Pour chaque jeu de donnée, un ratio de 3 est obtenu pour des requêtes sélectionnant 66% de l\u0027ensemble des documents. Le temps de recherche dans l\u0027index (algorithme d\u0027intersection de listes) est présenté dans le dernier tableau du tableau 3, pour la requête q01. Comparé au temps d\u0027exécution total de la requête (tableau précédent), la recherche dans l\u0027index représente moins de 1%. Ces tests préliminaires montrent la validité de l\u0027approche : moins de données sont transférées, et l\u0027opération de recherche au niveau du médiateur est plus rapide avec un index.\nLe premier graphe de la figure 3 illustre les temps mis par l\u0027algorithme d\u0027intersection pour retrouver les entrées pertinentes pour les requêtes q02 et q03. On remarque que q02 s\u0027exécute plus rapidement que q03 ; ceci est dû aux sélections faites dans l\u0027algorithme d\u0027intersection, l\u0027espace de recherche de q02 étant plus restreint que celui de q03.\nLe second graphe de la figure 3 présente les temps de recherche dans l\u0027index pour q01. Pour chaque jeu de donnée, le temps de recherche augmente linéairement lorsqu\u0027un nouveau mot-clef est ajouté à la recherche. Finalement, pour des requêtes basiques (moins de 10 mots-clefs), le système est efficace et le temps de recherche dans l\u0027index est négligeable.\nConclusion\nDans cet article, nous avons présenté l\u0027intégration de XQuery Text dans un médiateur XML. La principale difficulté est d\u0027intégrer des sources ne répondant pas à ces capacités. Pour cela nous proposons d\u0027utiliser des vues indexées pour permettre d\u0027intégrer ces fonctionnalités à ces sources. Le médiateur indexe les vues en utilisant un résumé structurel de la vue. Ce guide permet de coder la position des éléments (XPath) de la vue pour en indexer le contenu. L\u0027opérateur de recherche de mots-clefs utilise des algorithmes basés sur ce système d\u0027identifiant pour l\u0027exécution de requête XQuery Text. Le système intègre une formule de classement adaptée à la structure arborescente des résultats XML.\nIl reste d\u0027autres aspects important à aborder dans la gestion des capacités des sources. Lorsqu\u0027une source reconnaît une partie de XQuery Text, les vues construites devraient prendre en compte cette capacité et limiter l\u0027indexation aux sources non capables en distribuant le traitement de la requête aux sources capables. Le classement de résultats semble simple pour une vue ou une source, mais le classement global doit être testé plus en détails, notamment en comparaison avec d\u0027autres formules dans de vraies applications. Le dernier aspect à préciser est la gestion des mises à jours sur les sources et dans l\u0027index, qui doit être mis à jour lors de l\u0027insertion ou de la suppression d\u0027objets dans les sources.\n"
  },
  {
    "id": "1054",
    "text": "Introduction\nLe problème auquel s\u0027intéresse cet article est la découverte de nouvelles familles de ré-actions chimiques à partir de bases de données de réactions. Cet article montre en quoi ce problème peut se reformuler en un problème particulier de fouille de graphes. La découverte de nouvelles réactions présente un grand intérêt pour la synthèse en chimie organique, discipline dont le but est la conception de molécules complexes à partir de composants chimiques usuels et de réactions. En effet, plus un expert de la synthèse a de réactions à sa disposition, plus il peut créer de nouveaux produits à partir d\u0027un ensemble donné de molécules et plus il peut optimiser le plan de synthèse d\u0027une molécule cible donnée. Par ailleurs, la découverte de dizaines de millions de réactions a vite rendu leur recensement nécessaire à travers la constitution de très grandes bases de données de réactions. Ces bases de données réactionnelles sont plus particulièrement exploitées par les experts de la rétrosynthèse. Cette méthode consiste à inférer le plan de synthèse d\u0027une molécule cible en recherchant les réactions qui permettent d\u0027aboutir à la cible, puis à réitérer récursivement le processus en prenant pour cibles les réac-tifs des réactions ainsi trouvées et ce jusqu\u0027à l\u0027obtention de réactifs de départ jugés ordinaires. La rétrosynthèse peut donc tirer un excellent parti de tout modèle prédictif capable de propo-ser des réactions qui n\u0027ont jamais été testées mais qui ont de forte chance d\u0027être réalisables expérimentalement.\nPour établir un tel modèle prédictif qui soit suffisamment fiable, certaines méthodes d\u0027apprentissage automatique ont été appliquées aux bases de données réactionnelles, notamment des méthodes de voisinage symbolique (Régin, 1995). Mais leurs résultats restent limités tant leurs calculs de généralisation (appliqués de surcroît à des graphes) s\u0027avèrent longs , et tant leurs procédures d\u0027induction se révèlent sensibles aux inexactitudes engendrées par la pauvreté des graphes moléculaires en tant que modèle de représentation des réactions. A ce titre l\u0027emploi par Berasaluce et al. (2004) d\u0027une méthode de recherche de motifs fréquents (Agrawal et Srikant, 1994) s\u0027est révélé judicieux dans la mesure où de telles méthodes sont à la fois adaptées à de grands volumes de données et robustes aux incohérences partielles des données puisque basées sur des probabilités. La principale faiblesse d\u0027une telle approche est de travailler sur des données booléennes et donc de ne pouvoir réellement prendre en compte la topologie des atomes dans une molécule, pourtant essentielle à la compréhension des réactions.\nLes travaux présentés dans cet article se situent dans le prolongement de ceux de Berasaluce et al. (2004). La différence majeure réside dans la prise en compte de la topologie des molécules par le recours à des techniques de fouille de graphes, c\u0027est-à-dire de généralisation des méthodes de fouille de données booléennes à des graphes. L\u0027apport principal de cet article est de montrer comment la recherche de nouveaux schémas de réactions à partir de bases de réactions peut se reformuler en un problème particulier de fouille de graphes. Pour se faire, les notions de chimie organique utiles à la compréhension du problème sont introduites (section 2). Les réactions décrites dans les bases de données réactionnelles sont ensuite modélisées sous la forme de graphes de réaction, à partir desquels des schémas de réactions particuliers appelés graphes de réaction partiels sont dérivés (section 3). Après un bref état de l\u0027art des algorithmes de fouille de graphes, le problème d\u0027apprentissage des mécanismes réactionnels est reformulé comme un problème particulier de fouille de graphes de réaction partiels (section 4).\nLes graphes moléculaires et les schémas de réactions\nUne molécule est un assemblage géométrique d\u0027atomes solidaires liés par des liaisons de covalence. La forme développée ou graphe moléculaire est une représentation de la topologie des liaisons d\u0027une molécule sous forme d\u0027un graphe étiqueté g(V, E, ?) où les sommets V (g) \u003d V et les arêtes E(g) \u003d E représentent respectivement les atomes et les liaisons de covalence de la molécule et où la fonction d\u0027étiquetage ? : V ? L associe (au minimum) à un sommet l\u0027élément chimique, tel le carbone (C) ou l\u0027hydrogène (H), de l\u0027atome représenté par ce sommet. Un graphe moléculaire est un graphe particulier en ce sens que les sommets  FIG. 1 -Equation de la réaction de déshydratation du propan-2-ol avec appariement partiel des atomes (numéros encerclés) représentant un même élément chimique ont des degrés (i.e. le nombre d\u0027arêtes incidentes à un sommet) tous égaux à la valence de cet élément (4 pour C, 1 pour H). Une réaction chimique est quant à elle un processus physique qui, en chimie organique, brise certaines liaisons de covalence pour en créer de nouvelles, transformant ainsi un ensemble de molécules appelées réactifs en un ensemble de nouvelles molécules appelées produits. Elle se représente par une équation chimique, comme illustrée sur la figure 1, mettant en rapport les formes développées des réactifs et des produits.\nLes schémas de molécules (resp. les schémas de réactions) sont des graphes moléculaires (resp. des équations chimiques) dont certains sommets représentent des variables, remplaçant par une opération dite de contraction, un radical, c\u0027est-à-dire un groupe d\u0027atomes connectés. De telles variables peuvent être typées auxquels cas leurs ensembles de définition se restreignent à des radicaux d\u0027un type particulier. Certains types de schémas qualifiés dans cet article de partiels peuvent de plus, autoriser à ce que certains sommets ne soient pas saturés, c\u0027est-à-dire que leur degré puisse être strictement inférieur à la valence de leur élément chimique. Un graphe moléculaire satisfait un schéma partiel s\u0027il contient un sous-graphe qui par une série de contractions (compatibles avec les types des variables du schéma) est isomorphe au schéma. Ainsi le schéma de réactions représenté sur la figure 2 est satisfait par la réac-\n2 -Un schéma de la déshydratation d\u0027un alcool secondaire tion de la figure 1, le groupe méthyl CH 3 étant une valeur acceptable pour une variable R de type alkyle, représentant toute chaîne linéaire d\u0027atomes de carbone saturée en hydrogène. Ce schéma est partiel puisque les atomes de carbone (de valence 4) numérotés 1 et 2 ne sont pas incidents à 4 liaisons et sont donc non saturés. Un schéma de molécules (resp. de réactions) permet de représenter une molécule (resp. une réaction) générique instanciée par une famille de molécules (resp. une famille de réactions) de la même manière qu\u0027un concept d\u0027un langage de représentation des connaissances est satisfait par ses instances. produits, pourtant indissociables, ne peuvent être mis en corrélation que par une information totalement étrangère (les appariements) à leur mode de description (les graphes moléculaires). L\u0027introduction d\u0027un graphe de réaction, illustré sur la figure 3 (a), permet de rattacher cause et effet de la réaction en un seul objet. Ce graphe de réaction résulte de la superposition des atomes appariés entre les graphes moléculaires des réactifs et des produits. Formellement ce graphe se construit à partir des graphes moléculaires des réactifs auxquels on ajoute les arêtes élémentaires nouvellement crées par la réaction (une liaison multiple étant dissociée en un nombre de liaisons élémentaires égal à sa multiplicité). De plus chaque arête est marquée d\u0027une étiquette précisant s\u0027il s\u0027agit d\u0027une arête inchangée, brisée ou créée. La figure 3 (a) représente le graphe de réaction associé à l\u0027équation chimique de la figure 1. Les arêtes stables, brisées et créées y sont représentées respectivement en trait continu, en pointillés et en en trait épais. \n, peuvent être définis trois sous-graphes présentant un intérêt particulier.\n-Le graphe du coeur C(R) \u003d R · (E ? (R) ? E + (R)) est le sous-graphe de R réduit 1 par l\u0027ensemble de ses arêtes brisées ou créées. Ce graphe représente le coeur de la réaction, c\u0027est-à-dire l\u0027ensemble des atomes dont les liaisons de covalence sont modifiées lors de la réaction. Le graphe de coeur du graphe de réaction de la figure 3 (a) est représenté sur la figure 3 (b). -Le graphe des réactifs R(R) \u003d R · (E ? (R) ? E 0 (R)) est le sous-graphe de R réduit par l\u0027ensemble de ses arêtes brisées ou inchangées. Ce graphe représente l\u0027union des graphes moléculaires des réactifs. Le graphe des réactifs associé au graphe de réaction de la figure 3 est identique à la partie gauche de l\u0027équation chimique de la figure 1. -Le graphe des produits P(R) \u003d R·(E 0 (R)?E + (R)) est le sous-graphe de R réduit par l\u0027ensemble de ses arêtes inchangées ou créées. Ce graphe représente l\u0027union des graphes moléculaires des produits. Le graphe des produits associé au graphe de réaction de la figure 3 est identique à la partie droite de l\u0027équation chimique de la figure 1. On démontre que le graphe de coeur est un ensemble connexe de cycles de longueurs paires disjoints par leurs arêtes. Chaque cycle est une suite alternée de liaisons brisées et de liaisons créées. La preuve repose sur une démonstration similaire à celle, demeurée célèbre, qu\u0027Euler apporta au problème des ponts de Konigsberg (Pour plus de détails, on se référera à un manuel de théorie des graphes comme par exemple Gondran et Minoux (1995)). Vu que dans un graphe de coeur un cycle de longueur 0 n\u0027a pas de sens et qu\u0027un cycle de longueur 2 peut être vu comme une liaison stable, on peut conclure que les graphes de coeur de réaction sont des systèmes de cycles alternés de longueurs paires supérieures ou égales à 4.\nD\u0027un point de vue purement informationnel, le graphe de réaction est rigoureusement équi-valent à une équation chimique appariée puisqu\u0027il est possible de passer indifféremment d\u0027un formalisme de représentation à l\u0027autre. Mais les avantages du graphe de réaction sont multiples : outre certains avantages en terme de complexité algorithmique qui sont ici hors sujet, le graphe de réaction permet de représenter naturellement le lien entre la cause et l\u0027effet d\u0027une réaction et ce en un seul objet, via un graphe connexe. Cette association est indispensable pour généraliser les réactions et approcher l\u0027expression des mécanismes réactionnels sous-jacents qui rattache nécessairement les effets à leurs causes. Enfin le graphe de coeur peut servir à réaliser une partition et donc une indexation efficace des réactions d\u0027une base de données ré-actionnelles.\nA ce titre, on introduit ici la notion de réaction nulle qui désigne une absence de toute ré-action lors de la mise en présence d\u0027un ensemble donné de réactifs dans des conditions expéri-mentales données. Le graphe de coeur d\u0027une réaction nulle est évidemment le graphe vide (sans sommets). Les graphes de réaction des réactions nulles sont les seuls à ne pas être connexes et dans ce cas uniquement se confondent avec les graphes des réactifs (ou indifféremment les graphes des produits). Les réactions nulles servent d\u0027exemples négatifs supplémentaires utiles pour interdire certaines généralisations irréalistes de réactions. Malheureusement les bases de données réactionnelles ne contiennent pas la description de réactions nulles, ce qui est pour le moins normal vu l\u0027intérêt tout aussi nul qu\u0027elles présentent en synthèse organique. On peut cependant construire des réactions nulles en émettant l\u0027hypothèse que la plupart des réactions d\u0027une base de données réactionnelles forment des produits stables. Le graphe des produits de cette réaction peut alors servir de graphe de réaction pour une nouvelle réaction nulle. C\u0027est pourquoi on suppose désormais qu\u0027une base de données réactionnelle est un ensemble de graphes de réaction indexés par leur graphe de coeur éventuellement nul.\nLes graphes de réaction partiels\nL\u0027introduction des graphes de réaction permet de définir un type particulier de schémas de réactions (au sens de celui défini dans la section 2) tenant compte de l\u0027appariement des atomes des réactifs et des produits. Ces schémas appelés graphes de réaction partiels, tiendront lieu de motifs dans nos algorithmes de fouille de graphes.\nUn graphe de réaction partiel est défini comme un schéma de molécule contenant un et un seul graphe de coeur et dont les variables sont toutes mono-atomiques, c\u0027est-à-dire qu\u0027elles ne peuvent représenter qu\u0027un seul atome. Les types des variables mono-atomiques se confondent alors avec un ensemble (L, d\u0027étiquettes généralisées ordonné selon un ordre de subsomption : les étiquettes les plus spécifiques sont les éléments chimiques et l\u0027étiquette la plus gé-nérale, notée remplace tout atome quel qu\u0027il soit. Les étiquettes intermédiaires dans l\u0027ordre induit par permettent de regrouper les éléments dont les propriétés chimiques sont similaires, comme par exemple la famille des halogènes dont font notamment partie le chlore et le brome. La figure 4 (a) présente un graphe de réaction partiel qui est satisfait par la réaction de la figure 1.\nCes schémas de réactions sont introduits ici car ils revêtent d\u0027excellentes propriétés vis à vis des algorithmes de fouille de graphes. Ainsi chaque sommet ne peut être apparié qu\u0027à un seul atome ce qui facilite la gestion des appariements entre les graphes partiels et les graphes Il est donc possible par une suite de spécialisations de générer tout graphe de réaction partiel à partir de son graphe du coeur. Par ailleurs et compte tenu de la connaissance des mécanismes réactionnels dont les chimistes disposent, la grande majorité des réactions sont le résultat d\u0027une succession de réactions élémentaires dont le graphe de coeur se réduit à un seul cycle. Notre étude peut donc se restreindre à l\u0027étude des réactions élémentaires dont les graphes partiels peuvent être générés à partir de l\u0027ensemble des cycles alternés de longueur paire supérieure ou égale à 4 et dont tous les sommets sont étiquettés par Ce dernier ensemble est lui même trivial à générer. La figure 5 représente la suite (incomplète) de spécialisations passant du  Au coeur de chaque réaction élémentaire se trouve un mécanisme réactionnel, c\u0027est-à-dire un processus temporel et déterministe de transformation qui brise certaines liaisons de covalence pour en créer d\u0027autres. Les graphes de réaction partiels peuvent servir de modèles de représentation des mécanismes réactionnels. Ce modèle n\u0027est pas exact et induit des erreurs de prédiction lorsqu\u0027il est confronté à une base d\u0027exemples de réactions. Ces erreurs se manifestent par des exemples qui satisfont la cause de la réaction générique sans en satisfaire l\u0027effet. L\u0027effet d\u0027un mécanisme réactionnel est modélisé par le graphe de coeur du graphe partiel, traduisant la redistribution des liaisons de covalence, alors que la cause du mécanisme est modélisée par un graphe que l\u0027on décide d\u0027appeler réacton. Le réacton d\u0027un schéma désigne l\u0027ensemble des schémas des réactifs, c\u0027est-à-dire le graphe des réactifs déduit du graphe de réaction partiel. Sur la figure 4 est illustré le réacton (b) associé au graphe partiel (a). Plus formellement étant donné un ensemble R des réactions (y compris des réactions nulles) dans des conditions réactionnelles fixées, on note g r (r) (resp. g re (r)) le graphe de ré-action (resp. le graphe des réactifs) d\u0027une réaction r. Etant donné un graphe partiel de réaction s, la réaction r satisfait s si s est un sous-graphe de g r (r), c\u0027est-à-dire si la cause (l\u0027environnement topologique du coeur) et l\u0027effet (la redistribution des liaisons de covalence) décrits par s se retrouve dans r. A l\u0027inverse une réaction r infirmera s si le réacton g re (s) de s est un sous-graphe du graphe des réactifs g re (r) sans que r satisfasse s, c\u0027est-à-dire si la cause se trouve dans r alors que l\u0027effet ne s\u0027y trouve pas. Un exemple de réaction qui satisfait (resp. qui infirme) un schéma s est dit positif (resp. négatif ) pour s. La figure 6 exhibe un exemple positif (a) et un exemple négatif (b) pour le graphe de réaction partiel de la figure 4. Il est important de noter que ces exemples ne sont valables que dans des conditions réactionnelles bien précises (ici une température supérieure à 110 °C). Si la température devient inférieure à ce seuil de 110 °C, l\u0027expérience montre que la réaction de la figure 1 devient une réaction nulle et passe donc du statut d\u0027exemple positif à celui d\u0027exemple négatif. Etant donné un ensemble {r i } 1?i?n d\u0027exemples de graphes de réaction éventuellement nulles, il est alors possible de définir la fréquence positive f + (s) (resp. la fréquence négative f ? (s)) d\u0027un graphe partiel s comme le nombre d\u0027exemples positifs (resp. négatifs) pour s. Plus la fréquence négative d\u0027un schéma est faible, plus le schéma est fiable, plus la fréquence positive d\u0027un schéma est grande, plus le schéma est général.\nLa fouille de graphes\nLa majorité des méthodes de fouille de données et d\u0027apprentissage s\u0027applique à des données où chaque objet est décrit par la liste des attributs booléens qu\u0027il vérifie. La recherche des motifs fréquents et d\u0027extraction des règles d\u0027association (Agrawal et Srikant, 1994) est une de ces méthodes les plus employées. Etant donné une base d\u0027objets décrits par la liste de leurs attributs, choisis dans un ensemble A, l\u0027extraction de motifs fréquents consiste à énumérer dans un premier temps les conjonctions d\u0027attributs, ou motifs, qualifiés de fréquents, c\u0027est-à-dire dont le nombre d\u0027occurrences (définies par les objets contenant simultanément tous les attributs du motif) dans la base de données, encore appelé fréquence ou support, est supérieur à un seuil fixé arbitrairement. L\u0027extraction des règles d\u0027association consiste dans un deuxième temps à déduire les règles d\u0027association non triviales entre motifs fréquents dont la probabilité conditionnelle, ou confiance, est supérieure à un second seuil.\nCertains travaux initiés notamment par Inokuchi et al. (2000)  \nLa fouille de graphes de réaction partiels\nLes modèles retenus pour la représentation des mécanismes réactionnels sont les graphes de réaction partiels. Etant donné une base de données réactionnelles, chaque graphe partiel g r peut être associé à une fréquence positive f + (g r ) et une fréquence négative f ? (g r ). Ces deux fréquences sont des fonctions décroissantes par rapport à l\u0027ordre partiel (G r , ? G ) des graphes de réaction partiels (i.e.\n. Un graphe partiel est d\u0027autant plus pertinent que sa fréquence positive est élevée et que sa fréquence négative est faible, comme l\u0027illustre la figure 7 élaborée dans le prolongement des exemples précédents. Dans cet exemple, la base de réactions est censée contenir N 1 , N 2 et N 3 réactions mettant respectivement en jeu des alcools primaires, secondaires et tertiaires (i.e des molécules dont l\u0027atome de carbone C portant le groupe alcool OH est relié respectivement à un, deux et trois autres atomes de carbone). Les réactions mettant en jeu des alcools primaires sont nulles alors que les alcools secondaires et tertiaires réagissent selon le coeur de la figure 4 (b). En outre la base de réaction est supposée représenter de manière homogène les différents types d\u0027alcool. N 1 , N 2 et N 3 sont donc supposés tous égaux à N , de manière à pouvoir calculer simplement les fréquences des graphes de réaction partiels de la figure 7 (a), regroupées dans le tableau (b). Il apparaît ainsi que le graphe (c) est plus pertinent que le graphe (a) du fait d\u0027une fréquence négative plus grande pour une même fréquence positive et que le graphe (e) du fait d\u0027une fréquence positive plus faible pour une même fréquence négative. \nFIG. 7 -Graphes de réaction partiels ordonnés et leurs fréquences\nDifférentes familles de graphes partiels peuvent être étudiées, chacune accordant une importance différente aux fréquences positives et négatives. En raison de sa simplicité, on s\u0027inté-resse ici à la famille G(s + ) des graphes partiels dont la fréquence négative est minimale tout en ayant une fréquence positive supérieure ou égale à un seuil s + fixé arbitrairement. Ainsi l\u0027exemple de la figure 7 permet d\u0027établir que G(2 · N ) \u003d {c} et G(N ) \u003d {c, e, f } en supposant par ailleurs que tous les graphes non représentés contenant les graphes (c), (e) et (f) ont une fréquence positive qui leur est strictement inférieure (et donc, pour être totalement exact, que les graphes partiels représentés regroupent l\u0027ensemble des graphes de leur fermé).\nPour calculer facilement une approximation G(s + ) de cet ensemble, on choisit dans le cas tangent où un graphe g 1 est inclus dans un autre graphe g 2 et que ces deux graphes ont les mêmes fréquences négatives, de ne conserver dans G(s + ) que le graphe le plus spé-cifique g 2 . Ce choix certes arbitraire, parfois même contraire au choix du graphe le plus pertinent, permet lorsqu\u0027il est combiné au caractère décroissant de la fréquence négative, d\u0027approximer l\u0027ensemble G(s + ) par la frontière positive de s + c\u0027est-à-dire par l\u0027ensemble des graphes partiels fréquents (positivement par rapport à s + ) maximaux, soit l\u0027ensemble\nG(N ) \u003d {e, f } omet l\u0027élément c pourtant le plus pertinent. Les inexactitudes introduites par cette approximation sont toutefois compensées par la possibilité de traiter le problème comme celui d\u0027une recherche de graphes fréquents maximaux et de le résoudre en tant que tel grâce à certains algorithmes adaptés comme Spin de Huan et al. (2004). Deux modifications doivent malgré tout être apportées à un algorithme vertical de fouille de graphes pour qu\u0027il soit adapté à notre formulation du problème : -D\u0027une part la génération des motifs par les algorithmes de fouille doit être modifiée afin de construire les motifs, c\u0027est-à-dire les graphes de réaction partiels, à partir d\u0027un graphe de coeur, c\u0027est-à-dire un cycle alterné de liaisons brisées et créées. Ceci est nécessaire afin de garantir la présence unique et entière d\u0027un graphe de coeur dans chaque motif. Cette modification est facile à intégrer au sein des algorithmes de type verticaux dont les motifs croissent à partir d\u0027un motif initial, en général égal au motif vide mais qui peut être initialisé à une autre valeur. -D\u0027autre part la phase de détermination des fréquences par le balayage de la base de graphes doit être modifiée pour être capable de calculer deux fréquences. En particulier la fréquence négative doit se calculer en testant dans les graphes de la base l\u0027inclusion non du motif mais celle d\u0027un motif secondaire (le réacton) dérivé du motif initial. Il est donc possible d\u0027adapter certains algorithmes existants de fouille de graphes pour qu\u0027ils extraient l\u0027ensemble des graphes de réaction partiels maximaux fréquents (positivement). Cet ensemble de résultats peut ensuite être soumis à un expert en chimie afin qu\u0027il analyse l\u0027exactitude et l\u0027originalité des schémas de réactions découverts.\nDiscussion\nLe choix arbitraire de l\u0027ensemble de graphes partiels maximaux fréquents comme ensemble d\u0027étude mérite une analyse critique. On propose ici une formalisation abstraite du problème gé-néral de l\u0027apprentissage de mécanismes réactionnels afin de mieux situer la pertinence de la méthode proposée au paragraphe 4.3. Le constat initial pour une telle formalisation est qu\u0027un graphe de réaction partiel représente d\u0027autant mieux un mécanisme réactionnel que sa fré-quence positive est grande et que sa fréquence négative est faible. Ces deux fréquences étant toutes deux des fonctions décroissantes par rapport à ? G , la recherche des graphes partiels les plus représentatifs n\u0027a de sens que si on se donne un critère d\u0027optimalité capable de pondérer l\u0027importance accordée à la fréquence positive relativement à la fréquence négative. Ce critère peut se définir formellement à partir d\u0027un ensemble totalement ordonné (E, ?) et d\u0027une fonction c : R 2 ? E qui associe aux couples des fréquences (f + (g), f ? (g)) d\u0027un graphe partiel g un élément de E. La seule contrainte imposée est que c soit une fonction croissante (resp. décroissante) de f + (resp. de f ? ). Entre deux graphes comparables par ? G , le graphe dont le critère c est le plus petit peut ainsi être éliminé. Les graphes partiels résistant à cette élimina-tion, c\u0027est à dire les maxima locaux de c, sont sans nul doute les plus pertinents au sens de c. Cet ensemble peut alors être mis sous la forme d\u0027une liste L triée selon les valeurs décrois-santes de leur image par c, c\u0027est-à-dire par ordre d\u0027intérêt décroissant. Seuls les motifs dont le critère c est supérieur à un seuil arbitraire c 0 sont conservés dans L. La liste L constitue l\u0027ensemble des graphes de réaction partiels les plus pertinents relativement à c et à la base de données réactionnelles considérée.\nLe choix de c est arbitraire et ouvre de nombreuses possibilités pour qualifier différem-ment l\u0027ensemble L des résultats. L\u0027ensemble G(s + ) présenté au paragraphe 4.3 peut ainsi être formalisé par l\u0027ensemble L associé à un ensemble E \u003d R + × R ? muni de l\u0027ordre lexicographique et par le critère c : (\n. Le caractère alambiqué de l\u0027expression de c paramétré de surcroît par un seuil arbitraire s + tend à prouver le manque de pertinence du choix de G(s + ). Cet ensemble a été présenté dans cet article uniquement parce que certains algorithmes existants de fouille de graphes fréquents permettent de l\u0027estimer directement. Nous envisageons plutôt de travailler avec d\u0027autres critères c, comme celui que nous appelons fiabilité, défini par c(f\nf + +f ? compris entre 0 et 1. Dans la mesure où la base de réactions couvre de manière homogène l\u0027ensemble des phénomènes physiques qu\u0027elle est censée décrire, la fiabilité constitue une approximation de la confiance (c\u0027est-à-dire de la probabilité conditionnelle) associée au schéma de la réaction, interprété dans ce cas comme une règle de transformation reliant une hypothèse à une conclusion. En ce sens la fiabilité est un critère plus pertinent qui par ailleurs croit bien avec f + et décroît avec f ? . Nous exigeons de plus que la fréquence f + reste supérieure à un seuil s + de manière à ne retenir que les sché-mas suffisamment généraux (en particulier en écartant comme schémas les graphes de réaction de la base dont la fiabilité est évidemment égale à son maximum 1 mais dont la fréquence positive valant aussi 1 est trop faible) et surtout de manière à limiter l\u0027espace de recherche infini à un sous domaine fini (contrainte qui s\u0027avère indispensable pour éviter toute récursion infinie de la part des algorithmes de fouille verticaux). Un autre critère envisageable est la différence c(f + , f ? ) \u003d f + ? f ? . Le tableau (b) de la figure 7 évalue pour chaque graphe les deux critères de fiabilité et de différence. Les maxima locaux de fiabilité sont (c), (e) et (f). Il est à noter que la fiabilité ne peut prendre en compte la généralité d\u0027un schéma et favoriser ainsi le graphe partiel (c) vis à vis des deux autres. Pour pallier ce défaut, il faudrait rajouter la fréquence positive comme critère secondaire à la manière du double critère du paragraphe 4.3. Le critère de différence permet de tenir compte très simplement de la généralité : il donne bien le graphe (c) comme unique maximum. A terme d\u0027autres critères inspirés de la théorie de l\u0027information permettront d\u0027optimiser le compromis fiabilité -généralité.\nLa méthode d\u0027optimisation de tels critères c, que nous pensons intéressante, est incompatible avec les algorithmes de fouille de graphes actuels. La recherche de maxima locaux de fonctions c non monotones dans (G r , ? G ) vont à l\u0027encontre du principe de ces algorithmes qui parcourent l\u0027ensemble des motifs selon un ordre prédéterminé en évitant toutes comparaisons avec les motifs voisins. Nous envisageons donc un nouveau type d\u0027algorithme adapté à la formalisation du critère c qui ne peut être détaillé ici par manque de place.\nConclusions\nLe problème de la recherche de réactions génériques fiables peut grâce à une modélisation adéquate se reformuler en un problème de fouille de graphes. Ce problème s\u0027apparente à celui de la recherche de motifs qui sont à la fois fréquents dans une base et non fréquents dans une autre, avec cette originalité qu\u0027un motif s\u0027exprime différemment dans chacune des deux bases. Toutefois des particularités induites tant par la connaissance des mécanismes réactionnels que par notre propre modélisation du problème nous poussent à entrevoir une méthode générale de fouille de graphes pour laquelle les algorithmes existants sont inadaptés. Un nouvel algorithme inspiré de cette modélisation est en cours de développement et devrait à terme valider les idées introduites dans cet article. De nombreuses expériences sur des bases de données réactionnelles seront alors possibles et permettront notamment d\u0027étudier l\u0027influence du critère d\u0027optimisation c. La méthode introduite n\u0027étant pas spécifique à la chimie, il sera également possible de l\u0027appliquer à d\u0027autres types de données modélisables sous forme de graphes. Cette perspective motivante ne doit cependant pas faire oublier que les performances de calcul et plus encore la pertinence des résultats aux yeux des chimistes restent deux inconnues majeures du problème que seule l\u0027expérimentation pourra lever.\n"
  },
  {
    "id": "1055",
    "text": "Introduction\nCe papier traite du problème de forage de plusieurs bases de données gigantesques et géo-graphiquement distribuées dans le but de produire un ensemble de règles de classification qui expliquent les groupements de données observés. Le résultat de ce forage sera donc un méta-classificateur aussi bien prédictif que descriptif. En d\u0027autres termes, nous visons à produire un modèle qui permet non seulement de prédire la classe de nouveaux objets, mais qui permet aussi d\u0027expliquer les choix de ses prédictions. Nous croyons que ce genre de modèles, basés sur des règles de classification, devrait aussi être facile à comprendre par des humains, ce qui est également l\u0027un de nos objectifs. Il faut dire toutefois que nous nous plaçons dans le contexte où il est impossible de rapatrier toutes ces bases dans un même site, et ce, soit à cause du temps de téléchargement, soit à cause de l\u0027impossibilité de traiter la base ainsi agrégée.\nDans la littérature, les techniques de forage distribué de données à la fois prédictives et descriptives sont malheureusement peu nombreuses. La plupart d\u0027entre elles tentent de produire -95 -RNTI-E-6 un méta-classificateur sous forme d\u0027un ensemble de règles à couverture disjointe où un objet est couvert par une et une seule règle. Nous montrerons dans cet article que cette contrainte de couverture disjointe n\u0027est pas nécessaire pour produire un méta-classificateur fiable. Ainsi, nous proposons une technique simple où un objet peut être couvert par plusieurs règles. La relaxation de cette contrainte de couverture disjointe nous permet de produire un classificateur final rapide, sans que le taux d\u0027erreur de celui-ci n\u0027en souffre. Cet article procède comme suit. Dans la section 2, une vue d\u0027ensemble des techniques d\u0027agrégation de modèles les plus connues est présentée. Puis, dans la section 3, nous présen-tons notre solution au forage distribué des données (FDD) employant l\u0027agrégation de modèles (FDD-AM) basée sur un coefficient de confiance. Dans la section 4, nous présentons nos ré-sultats d\u0027expérimentations qui démontrent la viabilité de notre méthode. La section 5 compare la complexité asymptotique de notre méthode à celles rencontrées dans la littérature. Nous présentons finalement une conclusion et nos travaux futurs.\nTechniques existantes d\u0027agrégation de modèles\nNous présentons dans ce papier uniquement les techniques qui ont été développées dans un but de forage distribué de données. Conséquemment, nous ignorons volontiers : le système « Ruler » (Fayyad et al., 1993)  (Fayyad et al., 1996) qui a été construit dans le but de regrouper plusieurs arbres de décision construits sur un même ensemble de données dans un système centralisé, le système d\u0027apprentissage distribué (Sikora et Shaw, 1996) développé dans un cadre de gestion des systèmes d\u0027information afin de bâtir un système apprenant distribué (Distributed Learning System, DLS) et l\u0027approche de fragmentation (Wüthrich, 1995) qui utilise des règles probabilistes. En outre, nous ignorons les techniques purement prédictives telles que bagging (Breiman, 1996), boosting (Schapire, 1990), stacking (Tsoumakas et Vlahavas, 2002), arbiter et combiner (Chan, 1996), (Prodromidis et al., 2000).\nL\u0027algorithme MIL\nL\u0027algorithme MIL (Multiple Induction Learning) a été initialement proposé par Williams (1990) afin de résoudre le conflit entre les règles conflictuelles (voir définition ci-dessous) dans des systèmes experts. Hall et al. (1998a,b) ont repris la technique de Williams pour agré-ger des arbres de décision bâtis en parallèle et préalablement transformés en règles. En outre, ils ont étendu la technique pour prendre en considération d\u0027autres types de conflits. Le processus d\u0027agrégation proposé par ces auteurs n\u0027est autre qu\u0027un regroupement des règles muni d\u0027un processus de résolution des éventuels conflits. Il est à noter que cette résolution des conflits ne traite qu\u0027un couple de règles conflictuelles à la fois. Deux règles se voient en situation de conflit quand leurs prémisses sont consistantes tandis qu\u0027elles produisent deux classes diffé-rentes (Williams, 1990) (appelé conflit de type I), ou lorsque les conditions des prémisses se chevauchent partiellement (Hall et al., 1998a) (appelé conflit de type II) ou quand les règles ont le même nombre de prédicats avec des valeurs différentes associées aux conditions et classent les objets vers la même classe (Hall et al., 1998b) (appelé conflit de type III). La résolution de conflits consiste soit à spécialiser une ou les deux règles en conflit (conflits type I et II), soit à ajuster la valeur de la condition, c.-à-d., la borne de test, pour les conflits de type II et III et éventuellement fusionner les deux règles en conflit (conflit de type III). Dans certains cas -96 -RNTI-E-6 (conflits de type I et II), de nouvelles règles sont ajoutées en se basant sur les ensembles d\u0027entraînement de celles-ci pour récupérer la couverture perdue par l\u0027opération de spécialisation.\nLe système DRL (« Distributed Rule Learner »)\nLa technique DRL (« Distributed Rule Learner ») (Provost et Hennessy, 1996) a été conçue et implantée en tirant avantage de la propriété du cloisonnement-invariant (Provost et Hennessy, 1994). DRL commence par partitionner les données d\u0027entraînement E en nd sousensembles disjoints, assigne chacun (E i ) à une machine, et fournit l\u0027infrastructure pour la communication entre les différents apprenants (nommé RL et roulant chacun sur une machine différente). Quand une règle répond au critère d\u0027évaluation pour un sous-ensemble des données (f ? (r, E i , nd) ? c ; f ? est une fonction d\u0027évaluation d\u0027une règle et c une constante), elle devient une candidate pour répondre au critère d\u0027évaluation global ; la propriété de cloisonnementinvariant étendue garantit que chaque règle qui est satisfaisante sur l\u0027ensemble des données sera acceptable au moins sur un sous-ensemble. Lorsqu\u0027une copie locale du RL découvre une règle acceptable, elle envoie la règle aux autres machines pour mette à jour ses statistiques sur le reste des exemples. Si la règle répond au critère d\u0027évaluation global (f (r, E) ? c ; f est la fonction d\u0027évaluation principale et c une constante), elle est signalée comme règle satisfaisante. Dans le cas contraire, ses statistiques locales sont remplacées par les statistiques globales et la règle est rendue disponible pour la spécialiser encore plus. La propriété de cloisonnementinvariant garantit que chaque règle satisfaisante sur l\u0027ensemble des données sera trouvée par l\u0027un des RL.\nFusion d\u0027ensembles de règles générés en parallèle\nLe travail présenté par Hall et al. (1999) est un mélange des deux derniers travaux présentés ci-dessus, en d\u0027autres termes les travaux de (Williams, 1990), (Hall et al., 1998b) et (Provost et Hennessy, 1996). Spécifiquement, à chaque règle créée lui est associée une mesure de sa « qualité » qui est basée sur la précision ainsi que le nombre et le type des exemples qu\u0027elle couvre.\nLa technique proposée dans (Hall et al., 1999) est l\u0027utilisation de ce que Provost et Hennessy (1996) proposent (voir §2.2), à une différence près où la suppression de la règle de l\u0027espace des règles en considération ne se fait que lorsque la règle classe toutes les données des différentes bases et qu\u0027il s\u0027avère que sa mesure f (r, E) est inférieure au seuil. Il est à noter que chaque règle ne « voyage » pas d\u0027un site à un autre toute seule, mais bel et bien accompagnée des valeurs nécessaires pour calculer la mesure associée à chaque règle.\nToutefois, les auteurs de (Hall et al., 1999) démontrent que, dans le cas extrême, la propriété de cloisonnement-invariant risque de ne pas être satisfaite. Ainsi, ils suggèrent que la précision des règles agrégées peut être très différente de la précision des règles bâties sur l\u0027ensemble d\u0027entraînement entier. En outre, les auteurs soulignent qu\u0027en cas de conflits entre règles, ces derniers peuvent être résolus, comme décrit par (Hall et al., 1998b) et (Williams, 1990).\nPar ailleurs, Hall et al. (1999) traite un nouveau type de conflit entre règles. Il s\u0027agit d\u0027une règle ayant un intervalle (c.-à-d., deux conditions) chevauchant un intervalle d\u0027une deuxième règle. Dans ce cas, une règle plus générale est créée en combinant les deux règles conflictuelles et en ajustant les bornes des intervalles.\nDiscussion\nLa technique MIL souffre de plusieurs défauts. Tout d\u0027abord, le processus de résolution de conflit ne fait que spécialiser encore plus les règles en se basant sur les ensembles d\u0027entraî-nement des règles de classification. Les règles générées peuvent exhiber un faible pouvoir de classification si elles sont appliquées à de nouveaux objets, et ce, surtout dans le cas de bases d\u0027entraînement très bruitées. En plus, si les règles sont déjà très spécifiques à un ensemble d\u0027entraînement, cette méthode est incapable de les généraliser puisqu\u0027elle ne fait que regrouper puis spécialiser encore plus les règles en conflit. En outre, l\u0027adaptation de la technique de Williams afin de traiter des bases distribuées implique une augmentation du volume de données échangées entre les différents sites. En effet, d\u0027une part, chaque règle voyage accompagnée de l\u0027index des objets couverts et, d\u0027autre part, en cas de conflit, tous les objets couverts par une des deux règles en conflit sont rapatriés du site d\u0027entraînement vers le site qui résout les conflits.\nLe plus important inconvénient du système DRL est le temps d\u0027exécution. En effet, lorsqu\u0027une règle est jugée acceptable par un site donné, elle doit passer par tous les autres sites afin de mettre à jour ses variables statistiques en fonction de leurs données. En d\u0027autres termes, toute règle acceptable sur un site doit classer toutes les données de tous les autres sites. Ainsi, la règle doit, d\u0027une part, « voyager » à travers tous les sites, et d\u0027autre part, classer les données de chaque site. Si une règle n\u0027est pas jugée satisfaisante sur l\u0027ensemble des données, celle-ci est spécialisée et le processus recommence si la nouvelle règle est jugée localement acceptable. Il est clair que ce processus risque d\u0027être très gourmand en temps d\u0027exécution.\nQuant au système de fusion de règles générées en parallèle, ce système est identique au précédent à une différence près ; toute règle générée dans un site donné traverse tous les autres sites afin de mettre à jour ses variables statistiques. Ainsi, le nombre de règles voyageant entre les différents sites est plus important que le nombre de règles en transite dans le système DRL. Par conséquent, il est clair que cette technique est encore plus lente que la précédente.\nLa technique d\u0027agrégation de modèles proposée\nAfin de construire notre méta-classificateur, nous proposons une architecture basée sur les agents logiciels. À cette fin, deux types d\u0027agents sont mis en oeuvre : les agents mineurs qui minent chaque base de données répartie et un agent collecteur responsable de regrouper les informations produites par les agents mineurs.\nTâches d\u0027un agent mineur\nLa tâche d\u0027un agent mineur est décrite par la figure 1. Il est à noter que le coefficient de confiance c r d\u0027une règle r est calculé en utilisant le théorème limite centrale. En effet, ce théorème stipule que la somme d\u0027un grand nombre (? 30) de variables aléatoires indépendantes et identiquement distribuées suit une distribution qui peut être approximée par une loi Normale. Ainsi, comme nos classificateurs sont bâtis sur un large volume de données, le taux d\u0027erreur E r (T ) d\u0027une règle r calculé sur un ensemble de test T , disjoint de l\u0027ensemble d\u0027entraînement D, peut être approximé par la loi Normale au vrai taux d\u0027erreur E r , qui est le taux d\u0027erreur de r appliqué à toute la population, avec l\u0027écart-type ? Er . À l\u0027aide du taux d\u0027erreur E r (T ) et de l\u0027écart-type ? Er associés à une règle r, nous pouvons -98 -RNTI-E-6\nPour un agent mineur Ami travaillant sur la base de données DBi, faire : calculer l\u0027intervalle de confiance dans lequel nous retrouvons le vrai taux d\u0027erreur de r, E r , dans N % des cas, comme suit :\noù la constante z n est choisie en fonction du degré de confiance N % désiré. Le coefficient de confiance de chaque règle est déduit de l\u0027intervalle de confiance de l\u0027erreur. Nous l\u0027avons défini comme étant : 1 moins le pire taux d\u0027erreur calculé dans N % des cas : (1 ? E r (T ) ? z n ? Er ) ; en d\u0027autres termes, 1 moins le taux d\u0027erreur de la règle et moins la moitié de la largeur de l\u0027intervalle de confiance de l\u0027erreur, ainsi nous visons à prendre en compte le pire cas.\nTâches de l\u0027agent collecteur\nL\u0027agent collecteur, quant à lui, a pour tâche de regrouper les informations produites par tous les agents mineurs. Sa tâche est détaillée par l\u0027algorithme de la figure 2. Nous pouvons voir dans cet algorithme que l\u0027agent collecteur passe globalement par deux phases. La première phase est la phase principale qui consiste à regrouper toutes les règles dans une même base de règles R. Cette base de règles est notre méta-classificateur original. La deuxième phase, optionnelle, représente une phase de raffinement par filtrage des règles. Il s\u0027agit de supprimer de la base des règles celles qui ont un faible coefficient de confiance. En d\u0027autres termes, il faut supprimer les règles qui, d\u0027après la mesure de confiance, calculée statistiquement rappelonsle, n\u0027auront vraisemblablement pas un bon pouvoir prédictif lorsque confrontées à des données nouvelles. L\u0027ensemble de règles résultant de cette étape est le méta-classificateur R t .\nL\u0027ensemble R comme méta-classificateur\nL\u0027ensemble R représente l\u0027agrégation de tous les classificateurs de base. Cet ensemble de règles est utilisé comme modèle aussi bien prédictif que descriptif. D\u0027un point de vue prédictif, la classe prédite d\u0027un nouvel objet est la classe majoritaire prédite par les différentes règles qui le couvrent pondérée par leurs coefficients de confiance. Toutefois, en cas d\u0027égalité des pondérations, nous proposons d\u0027effectuer un vote à majorité simple. Ce qui revient, à peu près, à déterminer la classe votée par la majorité des classificateurs de base. Il est à noter que, contrairement à ce qui est identifié dans la littérature (voir §2), nous appelons règles en conflit seulement les règles qui couvrent un même objet différemment. Si plusieurs règles couvrent 2. Étape optionnelle, filtrage des règles : Éliminer de R les règles ayant un coefficient de confiance inférieur à un seuil t : Rt \u003d {rik ? R | cr ik ? t} (t est à déterminer empiriquement) ;\nFIG. 2 -Algorithme détaillant les tâches d\u0027un agent collecteur.\nun même objet et prédisent la même classe, nous ne les considérons pas comme conflictuelles. Dans de rares cas, même le vote à majorité simple risque d\u0027aboutir à une égalité. Le cas échéant, nous choisissons la classe majoritaire dans l\u0027ensemble des bases d\u0027entraînement. Il est à signaler que tout objet peut être couvert par au plus nd règles -sachant que nd est le nombre de sites. Le nombre de règles n\u0027est pas exactement égal à nd car la phase de détermination du coefficient de confiance risque dans certains cas d\u0027échouer, et ce, à défaut d\u0027une couverture, et par conséquent la règle en question est ignorée. Par ailleurs, en regroupant les ensembles R i , une même règle peut apparaître dans plus d\u0027un classificateur de base. Dans ce cas, une seule occurrence de la règle est retenue en lui attribuant un coefficient de confiance égal à la moyenne des coefficients de confiance de ses différentes occurrences.\nD\u0027un point de vue descriptif, les règles qui couvrent un objet expliquent sa classe même s\u0027il y a eu égalité du vote à majorité simple ou pondéré. Comme le système est développé dans un but de forage de données, en d\u0027autres termes, comme support à la prise de décision, les règles couvrant un objet sont proposées à l\u0027utilisateur qui doit juger, de par son expertise, de leur pertinence. Le fait de présenter à un décideur plus qu\u0027une règle afin d\u0027expliquer la classe d\u0027un objet a ses avantages puisque celui-ci aura une vue plus large et plus complète des « limites » de chaque classe. Nous rappelons en outre, qu\u0027en apprentissage automatique, la limite qui définit la séparation entre différentes classes n\u0027est généralement pas unique et par conséquent, plusieurs règles produisant une même classe peuvent représentées les « hyperplans » séparant les différentes classes, fournissant diverses vues sur ces données.\nExpérimentation\nAfin d\u0027effectuer nos tests, nous avons utilisé dix jeux de données tirés de la banque de données de l\u0027UCI (Blake et Merz, 1998) et dont la taille varie de 351 objets à 45222 objets. Il s\u0027agit des bases : adult, chess end-game (King+Rook versus King+Pawn), Crx, housevotes-84, ionosphere, mushroom, pima-indians-diabetes, tic-tac-toe, Wisconsin Breast Cancer (BCW) (Mangasarian et Wolberg, 1990) and Wisconsin Diagnostic Breast Cancer (WDBC). La subdivision de ces bases afin de simuler des bases distribuées est bien détaillée dans (Aounallah et al., 2005). Pour des fins de comparaison, nous utilisons l\u0027algorithme C4.5 appliqué sur la totalité des données. Le résultat, l\u0027ensemble de règles R ? , représente le cas idéal où toutes les données peuvent être regroupées dans un site central. Les résultats obtenus avec C4.5 sont seulement à titre de référence et nous supposons qu\u0027en pratique il n\u0027est pas possible de regrouper les données dans un même site. L\u0027algorithme C4.5 est aussi utilisé pour construire les classificateurs de base.\nLes taux d\u0027erreur obtenus avec R et R t\nNous commençons par regrouper les ensembles de règles R i afin de créer le méta-classificateur original R. Le tableau 1 représente le taux d\u0027erreur de R ? pour chaque ensemble de test avec son intervalle de confiance à 95% ainsi que ceux de R. L\u0027avant dernière colonne représente une comparaison entre les taux d\u0027erreur des ensembles R et R ? ; nous y trouvons : -« Empire » (resp. « Améliore ») qui signifie que R est statistiquement à 95% du temps pire (resp. mieux) que R ? du point de vue du taux d\u0027erreur de classification. La dernière colonne indique la valeur absolue de cette différence. -« ? » indiquant que R est statistiquement comparable à R ? . Ce tableau montre bien que dans 8 cas sur 10, le taux d\u0027erreur de R est comparable à celui de R ? et même dans les deux autres cas la différence n\u0027est pas très importante. Toutefois, cet excellent résultat pourrait être la conséquence de bases distribuées très riches en informations. Afin de vérifier si tel est le cas, nous avons appauvri les bases de données BD i en y introduisant du bruit, et ce, en inversant l\u0027attribut de classe 1 pour 10%, 20%, 25% et 30% des objets. \nTAB. 1 -Les taux d\u0027erreur du méta-classificateur original R comparés à ceux de C4.5.\nEn utilisant les bases appauvries (50 jeux de données différents : 10 bases de départ et 4 bases appauvries de chacune), nous constatons que les taux d\u0027erreur de R sont toujours aussi comparables à ceux de R ? . Par ailleurs, R arrive à produire de meilleurs taux d\u0027erreur (statistiquement, avec un taux de confiance de 95%) que R ? , et ce, au fur et à mesure que le bruit augmente dans les bases.\nQuant aux taux d\u0027erreur de R t , avec t \u003d 0.01, seuil optimal sur les 50 bases tel qu\u0027évalué empiriquement, ils sont sensiblement les mêmes (ou comparables statistiquement avec un taux de confiance de 95%) que ceux de R pour les 50 jeux de données.\nLe nombre de règles formant le méta-classificateur\nLe tableau 2 représente le nombre de règles formant le classificateur obtenu : R ? , R, R t . Il est clair de ce tableau que nos méta-classificateurs R et R t ont un nombre de règles raisonnable qui est même dans certains cas inférieur au nombre de règles de notre classificateur de référence. Ce résultat est très encourageant puisque nos méta-classificateurs ne sont ni plus difficiles ni plus faciles à interpréter que R ? .\nAdult \nÉvaluation asymptotique\nDans cette section nous comparons la complexité asymptotique de nos méta-classificateurs R et R t à ceux présentés dans la section 2. Pour ce faire, nous notons par n la taille maximale de l\u0027ensemble d\u0027entraînement dans un site, m le nombre d\u0027attributs dans la base de données, k : le nombre maximum de valeurs par attribut, l : le nombre maximum de prédicats (littéraux) dans une règle, p : le nombre maximum de règles produites des n objets d\u0027entraînement, d : le nombre de sites et n ? : la taille maximale de l\u0027ensemble de test dans un site quelconque.\nCoût de la technique proposée\nLa technique proposée, rappelons-le, fonctionne sur deux phases : une phase distante accomplie par des agents mineurs et une phase centralisée achevée par l\u0027agent collecteur.\nCoût des tâches de l\u0027agent mineur\nLes tâches d\u0027un agent mineur sont détaillées dans la figure 1. Globalement, il s\u0027agit de bâtir le classificateur de base (tâche 1) et de calculer le coefficient de confiance de chaque règle (tâche 2).\nLe coût de la tâche 1 est le coût de l\u0027application de l\u0027algorithme C4.5. Il est bien connu que ce coût est de l\u0027ordre de O(m 2 n). Le coût de la tâche 2 se résume au calcul de la couverture de chaque règle. Ce coût est le suivant :\n-Le nombre de tests à faire afin de savoir si une règle couvre un objet est l (le nombre de prédicats dans une règle). -Coût de déterminer la couverture de toutes les règles (au nombre de p) sur un site ayant n ? objets dans l\u0027ensemble de test est : n ? × l × p. Donc, le coût total des tâches d\u0027un agent mineur est donc O(m 2 n + n ? lp).\n-102 -RNTI-E-6\nCoût des tâches de l\u0027agent collecteur\nLes tâches de l\u0027agent collecteur sont détaillées dans la figure 2. Globalement, il s\u0027agit de regrouper toutes les règles issues des différents agents mineurs dans un même ensemble R (tâche 1) et d\u0027en extraire celles qui ont un coefficient de confiance inférieur à un certain seuil afin d\u0027avoir l\u0027ensemble R t (tâche 2).\nLe coût de la tâche 1 peut être considéré comme négligeable. Le coût de la tâche 2 est égal au nombre de règles dans R. En considérant que le nombre de règles issues d\u0027un seul site est p et le nombre de sites est d, le coût de cette tâche est dp.\nAinsi, le coût total de notre méta-classificateur R t est O(m 2 n + n ? lp + dp). Puisque le nombre de sites d est constant, le terme dp peut être remplacé par p et celuici peut être négligé devant n ? lp et par conséquent le coût de R t est au pire de l\u0027ordre de O(m 2 n + n ? lp) qui n\u0027est autre que le coût de l\u0027agent mineur. Ainsi, le temps de regroupement et de filtrage des règles ne présente asymptotiquement aucun surcoût par rapport au temps nécessaire pour produire (en parallèle) les classificateurs de base.\nCoût des techniques existantes\nL\u0027algorithme MIL\nAfin de résoudre les conflits (de type 1 (Williams, 1990)), l\u0027algorithme MIL a besoin de rapatrier sur un même site tous les objets couverts par les règles en conflit. Dans le pire cas, toutes les règles issues d\u0027un site B provoquent des conflits avec les règles du site A. Afin de résoudre ces conflits, il faut récupérer tous les objets couverts par les règles issues du site B. En d\u0027autres termes, il faut récupérer l\u0027ensemble d\u0027entraînement du site B. Ceci risque d\u0027être très lent, voire même irréalisable, vu les hypothèses que l\u0027on s\u0027est fixées au départ, à savoir que nous nous plaçons dans le contexte où il est impossible de transférer toute une base de données d\u0027un site à un autre.\nAinsi, comme la quantité de données qui transite d\u0027un site à un autre n\u0027est pas bornée, cet algorithme est dans le pire cas non comparable aux autres algorithmes, car il viole l\u0027une des hypothèses de l\u0027apprentissage distribué. Il sera par conséquent ignoré durant notre comparaison.\nLe système DRL\nPour simplifier la comparaison, nous supposons que la complexité de l\u0027algorithme utilisé pour construire l\u0027ensemble de règles (RL) est la même que celle de l\u0027algorithme C4.5 que nous utilisons dans notre technique (voir ci-dessus), malgré que, d\u0027après (Hall et al., 1999), C4.5 est plus rapide que RL.\nCette technique se base sur une fonction d\u0027évaluation de chaque règle. Si cette fonction est évaluée sur un ensemble indépendant de l\u0027ensemble d\u0027entraînement, son coût est le même que celui du calcul de notre coefficient de confiance (voir ci-dessus).\nLorsqu\u0027une règle satisfait au critère d\u0027évaluation local, elle est envoyée à tous les autres sites afin de mettre à jour ses statistiques en fonction de leurs données. Ainsi, cette règle, ayant l prédicats, doit classer tous les objets de tous les autres sites, au nombre de dn. Le coût associé à cette opération pour une règle est O(ldn) \u003d O(ln).\nDans le pire cas, toutes les règles peuvent satisfaire au critère d\u0027évaluation local. Ainsi, le coût de mettre à jour les statistiques des règles d\u0027un site donné, au nombre de p, est O(lnp). Dans le pire cas, chaque site devrait classer les règles des d autres sites. Ainsi, ce coût est aussi de l\u0027ordre de O(dlnp) \u003d O(lnp).\nSi une règle ne satisfait pas le critère d\u0027évaluation global, elle est renvoyée à son site de départ pour qu\u0027elle soit spécialisée encore plus. Nous notons par ? le coût de cette opération. Puis, si la nouvelle règle satisfait toujours le critère d\u0027évaluation local, le processus est réitéré. On aura par conséquent, un autre coût de l\u0027ordre de O(ln) (c\u0027est le coût d\u0027une règle classant les données de tous les autres sites).\nEn conclusion, le coût global de cette technique est de l\u0027ordre de O(m 2 n + n + lnp + ? + ln) \u003d O(m 2 n + lnp + ?).\nFusion d\u0027ensembles de règles générées en parallèle\nL\u0027étude de complexité au pire cas de cette technique est sensiblement la même que le système DRL puisqu\u0027il s\u0027agit exactement de la même technique augmentée par un processus de résolution de conflit selon l\u0027algorithme MIL. Par conséquent, globalement, la complexité de cette technique est pire ou égale à la complexité de la technique précédente.\nComparaison\nLa complexité de notre technique lorsque la validation est réalisée en considérant les échan-tillons comme ensemble de test est de l\u0027ordre de O(m 2 n + ln ? p). La complexité du système DRL ainsi que la technique de fusion de règles en parallèle est de l\u0027ordre de O(m 2 n+lnp+?). Ainsi, les trois techniques ont sensiblement la même complexité asymptotique à un terme près qui est dans notre technique fonction de la taille de l\u0027ensemble de test dans un site, et dans le système DRL, fonction de la taille de l\u0027ensemble d\u0027entraînement. Comme la taille de l\u0027ensemble d\u0027entraînement est généralement plus importante que la taille de l\u0027ensemble de test, notre technique est dans ce cas asymptotiquement plus rapide que le système DRL.\nConclusion\nL\u0027objectif de ce papier est de faire une comparaison entre les techniques existantes d\u0027agré-gation de modèles dans un but de forage distribué de données (FDD), d\u0027une part, et une version simplifiée de notre technique de FDD   (Aounallah et al., , 2005 d\u0027autre part. Pour ce faire, nous avons présenté un survol des techniques d\u0027agrégation de modèles existantes les plus comparables à la nôtre ainsi qu\u0027une description de la version simplifiée de notre technique de FDD.\nLes expériences menées ont démontré que notre technique performe d\u0027un point de vue prédiction aussi bien, ou même mieux, qu\u0027un classificateur bâti sur la totalité des données, utilisé comme point de référence. Par ailleurs, nos méta-classificateurs sont toujours de tailles comparables au classificateur centralisé de référence.\nUne étude asymptotique démontre, en outre, que nos techniques sont asymptotiquement comparables ou plus rapides que les techniques existantes de FDD.\n"
  },
  {
    "id": "1057",
    "text": "Introduction\nIl est communément admis que le temps de préparation des données peut occuper jusqu\u0027à 80% du temps lors d\u0027un projet industriel de fouille de données (Pyle, 1999). L\u0027hétérogénéité des sources, la présence de valeurs manquantes, les erreurs de saisie ou de calcul, les pannes de capteurs, une mauvaise fusion de données sont autant de causes qui peuvent introduire erreurs et incohérences dans une table de données. ESIEA Datalab est une plateforme évolu-tive programmée en Java qui met à disposition de nombreux outils pour aider à la détection d\u0027incohérences, la correction d\u0027erreurs, la transformation ou la contrainte de variables, etc.\nLe concept du logiciel\nLe nettoyage et la préparation de données peuvent être vus sous la forme d\u0027un processus représenté par la figure 1.\nFIG. 1 -Le nettoyage et la préparation de données vus comme un processus.\nLe logiciel n\u0027impose pas ce processus à l\u0027utilisateur, mais fournit tous les outils nécessai-res à sa réalisation. En parallèle, le nettoyage et la préparation des données sont tracés dans\nESIEA Datalab, un logiciel de nettoyage et préparation de données la console afin de pouvoir retrouver toutes les transformations et modifications effectuées sur les données et des agents fonctionnent en tâche de fond pour faire des suggestions et orienter l\u0027utilisateur.\nLes outils\nOutre un vaste ensemble d\u0027outils classiques, dans lesquels les algorithmes utilisés ont été adaptés à un contexte où toute valeur peut être manquante ou bien en erreur, ESIEA Datalab possède quelques outils originaux puissants qui permettent de traiter facilement des cas difficiles de nettoyage ou d\u0027offrir des moyens de visualisation intéressants.\nType structuré. Grâce à la notion de type structuré, le logiciel est capable de détecter des erreurs dans des données symboliques possédant une structure. Une fois la structure d\u0027une colonne spécifiée ou inférée, on peut contraindre les éléments de la structure à l\u0027aide de formules et mettre ainsi en erreur les valeurs ne respectant pas l\u0027une des contraintes.\nOutils de visualisation.\nParmi les outils de visualisation disponibles, ESIEA Datalab dispose de graphiques interactifs (matrice de nuages de points, coordonnées parallèles, etc.) qui permettent la sélection de valeurs et la réalisation d\u0027actions sur celles-ci. On trouve aussi des outils originaux comme la carte « vue d\u0027avion ». C\u0027est un graphique qui représente dans une forme condensée toute une table, que l\u0027on va utiliser avec des filtres qui vont colorer une sélection de valeurs. On a ainsi une vision totale de la table qui peut par exemple nous aider à estimer la densité des valeurs manquantes ou bien détecter des motifs.\nConclusion\nESIEA Datalab est un logiciel évolutif dont la simplicité d\u0027utilisation des outils et les fonctionnalités adaptées permettent d\u0027obtenir un gain de temps important sur le nettoyage et la préparation des données. Plusieurs améliorations sont en projet, notamment l\u0027ajout d\u0027une passerelle vers la librairie Java WEKA (Witten et Eibe, 2005 \nSummary\nESIEA Datalab is an evolvable Java software program which goal is to clean and prepare data before an analysis. The software looks like a toolbox ready to use, including some interactive visualisation tools, suggestion agents and advanced functionalities implementing Data Mining algorithms.\n"
  },
  {
    "id": "1058",
    "text": "Contexte et problématique\nLes systèmes d\u0027aide à la décision visent à transformer les données opérationnelles en informations facilement interprétables par les décideurs afin que ces derniers puissent effectuer des analyses complexes et prendre les meilleures décisions en temps utiles pour assurer la compétitivité et la pérennité de l\u0027organisation considérée. Dans un tel contexte, plus que le patrimoine matériel, le patrimoine immatériel est important pour capitaliser un maximum d\u0027informations, de connaissances et d\u0027expertises afin de prendre les décisions adaptées. Nos travaux visent à proposer aux organisations plus qu\u0027un système d\u0027aide à la décision, un véri-table outil de Mémoire d\u0027Expertises Décisionnelles (MED).\nLes données décisionnelles\nIl est reconnu que les Bases de Données Multidimensionnelles (BDM) sont adaptées pour le stockage et la manipulation des données décisionnelles (Inmon, 1996). Les modèles conceptuels (Ravat et al., 2005) des BDM organisent les données en sujets et axes d\u0027analyses au sein d\u0027un schéma en étoile (Kimball, 1996). Tout sujet d\u0027analyse est représenté par un fait composé de plusieurs mesures (indicateurs d\u0027analyse). Les dimensions représentant les axes d\u0027analyse sont formées de paramètres en fonction desquels les mesures sont étudiées. Les paramètres sont organisés en hiérarchies, de la granularité la plus fine (attribut racine servant d\u0027identifiant à la dimension) à la plus générale (cet attribut est symbolisé par All).\ndéveloppées et d\u0027un prédicat de sélection (Agrawal et al., 1997 ;Gyssens et al., 1997 \nVers une mémoire d\u0027expertises décisionnelles\nLes TM ont pour objectif de faciliter les prises de décisions, mais elles s\u0027avèrent parfois complexes et difficiles à interpréter. En effet, les prises de décisions reposent non seulement sur les données brutes mais également sur les réflexions, les commentaires des analystes voire la confrontation de différentes interprétations. À notre connaissance, il n\u0027existe pas d\u0027outil logiciel permettant aux décideurs d\u0027analyser les données décisionnelles en intégrant les tâches qu\u0027ils conduisent de manière manuelle sur des tableaux de bord : annoter, comparer… La mémorisation et la réutilisation de l\u0027expertise des analystes permettent à l\u0027organisation de préserver un patrimoine tout aussi important que les données elles-mêmes.\nNotre objectif est de proposer un cadre informatique permettant d\u0027exploiter, de partager les données multidimensionnelles tout en supportant des fonctionnalités d\u0027annotation pouvant comprendre des fils de discussion (support de communication utilisé dans les forums). Ainsi, en permettant d\u0027enrichir interactivement les composants des BDM et des TM, les décideurs deviendront des lecteurs actifs en insérant notamment des commentaires. Pour cela, nous proposons un outil informatique permettant :\n-de visualiser les données décisionnelles sous la forme d\u0027une TM ; -de manipuler une TM au travers d\u0027opérations multidimensionnelles ; -d\u0027annoter les schémas de BDM afin d\u0027expliciter les composants décisionnels ; -d\u0027annoter les TM tout en permettant le dialogue sous forme de fil de discussion ; -d\u0027exploiter ces annotations au travers de fonctions d\u0027exploration, de sélection, etc. De nombreux travaux ont apporté une réponse aux deux premiers objectifs : des algèbres pour la définition et la manipulation de BDM ont été proposées (Gray et al., 1996 ;Agrawal et al., 1997 ;Cabibbo et Torlone, 1998 ;Marcel 1999 ;Abelló et al., 2003).\nExistant : l\u0027activité d\u0027annotation\nÀ notre connaissance, les systèmes d\u0027annotation couplés aux BDM n\u0027ont pas fait l\u0027objet d\u0027étude. Une première proposition consisterait à transposer aux BDM le principe des commentaires associés aux schémas de BD transactionnelles. Cette proposition est insuffisante car elle reste difficilement exploitable et très limitée (table, vue ou attribut). La deuxième solution consiste à se baser sur les systèmes d\u0027annotation existants pour la gestion électroni-que de documents. Dans ce cadre, les annotations sont qualifiées de commentaires, notes, explications ou autres types de remarques qui peuvent être associés à tout ou partie d\u0027un document sans avoir à le modifier 1 . Ces annotations sont dites informelles, contrairement aux annotations formelles du Web Sémantique reposant sur l\u0027utilisation d\u0027un langage formel (ex. : ontologie) pour cataloguer et indexer les documents. Dans cet article, seules les annotations informelles seront exploitées car nous ne souhaitons pas contraindre les décideurs à employer un vocabulaire normé et restreint.\nLa majorité des systèmes d\u0027annotation informatisés (SAI) opèrent sur des documents textuels. Dans ce cadre, les annotations sont matérialisées sous différentes formes : texte ou marques libres (astérisques, etc.). Ces marques mettent en valeur des passages en y associant éventuellement un commentaire. À ce jour, on dénombre plus de vingt SAI tels que Amaya du W3C (Koivunen et Swick, 2001) ou Office Web Discussions de Microsoft (Brush, 2002). L\u0027intérêt d\u0027un SAI se situe à deux niveaux : à un niveau personnel, il aide l\u0027utilisateur dans sa tâche de lecture active tandis qu\u0027au niveau collectif il permet de partager ces annotations entre les utilisateurs. Au regard de nos problématiques, nous proposons d\u0027adapter les fondements des SAI à l\u0027analyse décisionnelle. L\u0027intérêt des annotations dans un tel contexte est qu\u0027elles seront utilisées pour véhiculer l\u0027expertise des analystes.\nAnnotations décisionnelles\nLes TM servent de support à des experts pour la prise de décisions. Cependant, afin de spécifier une MED, nous souhaitons permettre aux analystes d\u0027enrichir ces TM avec des annotations qui visent à conserver les décisions et les commentaires formulés lors des analyses de la TM. L\u0027expertise que véhiculent ces annotations est utilisée à des fins personnelles ou collectives et peut par conséquent contribuer à améliorer les analyses futures. Pour cela, les annotations apportent les fonctionnalités et avantages suivants :\n-au niveau du schéma, elles permettent d\u0027améliorer la compréhension des composants d\u0027un schéma de BDM et ainsi de tirer des conclusions analytiques cohérentes ; -au niveau des valeurs, elles peuvent contenir des informations expliquant un phé-nomène général, le contexte d\u0027étude ou signaler la singularité d\u0027une valeur spécifi-que. Les fils de discussion suscités par ces annotations permettent notamment aux experts de valider ou de compléter les commentaires de leurs collègues ; -les liens spécifiés vers des annotations ou des documents permettent de construire un « dossier d\u0027analyse ». Ils permettent également d\u0027illustrer la réflexion des analystes pour expliquer et justifier les éventuelles conclusions qu\u0027ils ont tirées ; -les traitements automatisés tels que la classification permettent de faire émerger des liens implicites entre analyses permettant d\u0027aiguiller les analystes dans leur tâche. \nModèle conceptuel d\u0027annotation décisionnelle\nInformations subjectives\nUne annotation décisionnelle comporte les informations subjectives suivantes :\n-le contenu textuel saisi par l\u0027annotateur ; -le type de l\u0027annotation caractérisant son contenu textuel. Le type, défini par le créateur de l\u0027annotation, peut être : commentaire, question (permet d\u0027interroger les autres analystes directement en contexte au travers de la TM), conclusion (l\u0027annotation présente les conclusions de l\u0027analyse et les éventuelles décisions prises) et réfé-rence (références vers des documents ou d\u0027autres TM). Ces types ne sont pas exclusifs : une annotation peut être un commentaire qui comprend des références ; -la portée de l\u0027annotation qui est soit locale soit globale. Une annotation est locale si elle n\u0027est présentée qu\u0027au travers d\u0027une TM précise. Au contraire, une annotation globale est indépendante des TM. Cependant, la cible d\u0027une annotation détermine parfois sa portée. Ainsi, une annotation sur les valeurs des mesures de la TM sera locale car fortement dépendante de l\u0027analyse courante.\nInformations objectives\nPour chaque annotation décisionnelle ad g , le système définit également l\u0027ensemble IO g contenant les informations objectives suivantes : -son identification : identifiant caractérisant sa position dans le fil de discussion ; -son créateur : son identité (nom, prénom), sa fonction et son adresse email ; -son point d\u0027ancrage spécifiant la localisation précise de l\u0027annotation dans la tma. La définition du point d\u0027ancrage ne peut se faire de manière naïve e.g. repérage en ligne et en colonne car la TM peut regrouper, en ligne et en colonne, une hiérarchisation de paramè-tres d\u0027une même dimension. De plus, l\u0027analyste peut réorganiser les axes des abscisses et des ordonnées grâce à l\u0027opérateur de permutation de paramètres par exemple (Ravat et al., 2005).\nPour ces deux raisons, le point d\u0027ancrage est défini par un CheminTM : ? | \u003c(op(m))+\u003e, ? C 1 (resp. C 2 ) exprime le chemin de la première (resp. deuxième) dimension :\n? | NomDimension (.NomHierarchie)* (/NomParamètre [\u003dValeur])* ? R précise éventuellement la condition de restriction : ? | ExpressionBooléenne, avec: ? représente l\u0027absence de valeur, Valeur précise la valeur exploitée pour le paramè-tre considéré et / représente un opérateur de descendance entre paramètres dans la hiérarchie.\nSeuls les éléments annotés figurent dans le chemin, ce qui explique que tous les champs sont facultatifs. Nous présentons ci-dessous quelques exemples de chemins de localisation représentés par la figure 2 (où R représente l\u0027expression de restriction de cette TM). 3. Le service des ventes avertit les différents analystes : les ventes prises en compte sont celles des télévisions 16/9 e . Le chemin de la cellule où l\u0027annotation sera rattachée est : (VENTES, ?, ?, ?, R).\nCréation, stockage et restitution d\u0027une annotation décisionnelle\nPour créer une annotation, le décideur sélectionne directement dans la table la ou les cellules qu\u0027il souhaite annoter. Pour compléter cette annotation il doit fournir toutes les informations qu\u0027il souhaite inclure dans l\u0027annotation. Nous stockons les annotations sur un serveur dédié i.e. indépendamment des TM pour faciliter le partage et la recherche des annotations.\nLa restitution des annotations est non intrusive et se fait en parallèle de la construction de la TM. Elles sont directement intégrées sous forme de pictogrammes au niveau de la TM.\nConclusion et perspectives\nNos travaux visent à proposer des solutions permettant la constitution d\u0027une mémoire d\u0027expertises décisionnelles pérenne. Cette mémoire permet de stocker et de restituer aussi bien les données nécessaires aux prises de décisions que les annotations qui véhiculent les commentaires des analystes. Les données décisionnelles sont représentées au travers de concepts multidimensionnels et restituées sous forme de tables (TM). Ces différents composants servent de support à la formulation, à l\u0027utilisation et à la restitution d\u0027annotations décision-nelles qui peuvent aussi bien servir à un usage personnel qu\u0027à un usage collectif.\nNous proposons d\u0027étendre ces travaux selon deux directions. La première consiste à éten-dre le modèle conceptuel des TMA en intégrant le concept de groupe d\u0027utilisateurs avec les droits d\u0027accès aux annotations associés. La seconde perspective consiste à coupler le principe de discussion asynchrone avec un outil de workflow. Cet outil pourrait valider et organiser le circuit des annotations ainsi que de définir les délais pour une prise de décision fiable.\n"
  },
  {
    "id": "1059",
    "text": "Introduction\nLa qualité des règles d\u0027association est généralement évaluée par des mesures d\u0027intérêt (classiquement le support et la confiance) et de nombreuses autres mesures ont été proposées (Tan et al., 2002). Mais, on peut légitimement se demander quel est l\u0027intérêt de telles règles, notées LHS AE RHS, si 30 % des données de LHS sont obsolètes, 20% des données de RHS sont imprécises, et 15% des données de LHS proviennent d\u0027une source réputée peu fiable. La thèse défendue dans cet article est que les mesures d\u0027intérêt pour la découverte de règles d\u0027associations ne sont pas autosuffisantes pour représenter effectivement la qualité des rè-gles. Des mesures décrivant la qualité des données à partir desquelles sont calculées les rè-gles doivent être intégrées au processus de découverte, de même que le coût d\u0027une décision de choisir (ou non) ces règles « supposées intéressantes » doit être également considéré. Ceci a motivé donc nos travaux que nous formalisons dans les sections suivantes.\n(I i ? I). Pour chaque itemset I i , le vecteur composé des scores sur toutes les dimensions de qualité (normalisées sur [0,1]) est appelé vecteur qualité et noté q(I i ) dans l\u0027espace qualité Q de tous les vecteurs qualité possibles. Nous définissons la qualité d\u0027une règle d\u0027association R avec une fonction de fusion notée \"o j \" dont la sémantique est spécifique à la dimension de qualité j considérée. Cette fonction permet de fusionner chaque composante des vecteurs qualité correspondants aux ensembles de données présents dans les partie gauche et droite de la règle. La qualité de la règle R est donc un vecteur k-dimensionnel tel que:\nOn peut alors définir la qualité moyenne de la règle R notée q(R) par une somme pondérée de chaque dimension des vecteurs qualité des jeux de données composant la règle : avec\nLe Tableau 1 présente plusieurs exemples de définition ainsi que la sémantique que l\u0027on peut donner à la fonction de fusion pour combiner les scores de qualité sur la dimension considérée pour deux ensembles de données x et y composant la règle.\nL a f r a îc h e u r d e la r è g le x AE y e s t e s t im é e d e f a ç o n p e s s im is t e p a r le p ir e s c o r e d e f r a îc h e u r d e s 2 e n s e m b le s d e d o n n é e s c o m p o s a n t la r è g le .\n2\nP r é c i s i o n q 2 ( x ) . q 2 ( y ) L a p r é c is io n d e la r è g le x AE y e s t e s t im é e p a r le \nTAB. 1 -Différentes fonctions de fusion pour combiner les scores d\u0027une dimension qualité\nNous considérons que choisir et utiliser (ou non) une règle d\u0027association est une décision qui désigne la règle comme étant légitimement intéressante (D1), potentiellement intéressante (D2), ou inintéressante (D3) à la fois sur la base de « bonnes » mesures d\u0027intérêt mais, également, en connaissant la qualité effective des données composant les parties gauche et droite de la règle. Cette décision a nécessairement un coût lié à l\u0027incertitude et à la méconnaissance du jeu de données et du processus de recueil qui ne garantit généralement pas la bonne qualité des données. Pour formalisme, nous employons P CE (x) la probabilité que le jeu de données X soit \"de mauvaise qualité\" sur une ou plusieurs dimensions de qualité et P CC (x) la probabilité que X soit \"de qualité correcte\" dans une gamme de valeurs acceptables sur chaque dimension de qualité. P AE (x) représente la probabilité que X soit détecté correct alors qu\u0027il est effectivement incorrect, P AC (x) représente la probabilité qu\u0027il soit détecté incorrect alors qu\u0027il est effectivement correct (voir Figure 1). Pour q ? Q, la qualité moyenne des données LHS ? RHS de la règle R, on note P(q?Q | CC) ou f CC (q) la probabilité conditionnelle que la qualité moyenne q corresponde à celle des jeux de données qui sont classifiés comme étant de qualité correct (CC). De la même façon, on note P(q?Q | CE) ou f CE (q) la probabilité conditionnelle que la qualité moyenne q corresponde à celle des jeux de données qui sont classifiés erronés ou comme étant de mauvaise qualité (CE). On note d la décision de choisir une règle légitimement intéressante (notée D 1 ), potentiellement intéres-sante (notée D 2 ), ou inintéressante (notée D 3 ) et s le statut de la qualité des jeux de données à partir desquelles a été calculée la règle. On note P(d\u003dD i , s\u003dj) et P(d\u003dD i | s\u003dj) respectivement les probabilités conjointe et conditionnelle que la décision D i soit prise lorsque le statut de la qualité des données à l\u0027origine du calcul de la règle R est j (i.e., CC, CE, AE, AC). On note c ij le coût de la décision D i pour classifier la règle sur la base de la qualité des données j composant les parties gauche et droite de la règle. A partir des coûts présentés dans le Tableau 2, l\u0027objectif est de minimiser le coût moyen c qui résulte d\u0027une telle prise de décision tel que :\nCoût\nPAC(x) Actually correct\nA partir du théorème de Bayes, tel que:\non suppose que q est la qualité moyenne des jeux de données composant la règle tirée aléatoirement dans l\u0027espace de tous les vecteurs qualité possibles. La probabilité conditionnelle P(d\u003dD i | s\u003dj) est définie à partir de la fonction de densité de probabilité f j du vecteur qualité de telle sorte que la variable aléatoire q prenne des valeurs dans l\u0027intervalle [q jMin , q jMax ] correspondant aux seuils de qualité selon j telle que : \nMinimiser ce coût moyen conduit à rechercher la décision optimale pour la classification des règles en trois catégories notées D  \nTAB. 3 -Les 10 meilleures règles découvertes avec leur qualité, coût et décision associés\nLe Tableau 3 montre les 10 meilleures règles d\u0027associations découvertes avec leur confiance, leur support (en nombre d\u0027enregistrements), le bénéfice prédit, les scores par dimension de qualité, la qualité moyenne, le coût de sélection pour chaque règle d\u0027association et enfin, la région décisionnelle de chaque règle. Il est intéressant de noter que le bénéfice prédit par règle peut être considérablement affecté par le coût de sélectionner une règle calculée à partir de données de mauvaise qualité : par exemple, la deuxième meilleure règle R2 dont le bénéfice prévu est $61.73 a un coût de $109.5 si elle est sélectionnée alors qu\u0027elle est classifiée comme \"inintéressante\" à cause de cette mauvaise qualité de données. Dans ces expériences, notre but était également de démontrer que des variations de la qualité de données pouvaient avoir un impact considérable sur la validité des résultats issus de la fouille de règles et dans le cas du challenge KDD-Cup-98, invalider totalement les prédictions de béné-fices. Ainsi, la Figure 2(a) montre le comportement du coût de la décision dans le choix des règles quand la qualité moyenne des données (InitQual) se dégrade de -10%, -30%, à -50% ou s\u0027améliore de +10%, +30% à +50% avec 0 ? \u003d 0.200 et en l\u0027absence de problème de classification. Nous observons que la dégradation de la qualité des jeux de données composant les règles augmente le coût de la sélection de ces règles. L\u0027amélioration de qualité de données se manifeste par une stabilisation du coût de la décision pour des règles légitimement intéres-santes. Un autre résultat intéressant est montré dans la Figure 2(b) : parmi les 10 meilleures règles découvertes pour une qualité de données initiale (InitQual), seulement 5 règles (R1, R5, R7, R9 et R10) sont potentiellement intéressantes. Lorsque la qualité augmente de 30%, 3 règles deviennent légitimement intéressantes (R5, R7 et R9). Cette observation offre deux perspectives intéressantes pour l\u0027exploitation des règles d\u0027associations et pour la gestion de la qualité de données : la première pour guider l\u0027élagage du nombre de règles sur la base à la fois des indicateurs de qualité de données et des coûts de décision dans le choix des règles ; la seconde pour établir des stratégies et des priorités dans l\u0027amélioration de la qualité de données (par un nettoyage ciblé par exemple). \nConclusion\nDans cet article, nous proposons une méthode pour définir la qualité des règles d\u0027associations en intégrant des mesures de la qualité de données à partir desquelles celles-ci sont dé-couvertes. Ensuite, nous proposons un modèle décisionnel probabiliste basé sur le coût que peut engendrer le choix de règles d\u0027association certes intéressantes d\u0027après leur support et confiance mais basées sur des données de mauvaise qualité. Le modèle définit les trois seuils pour déterminer si les règles découvertes sont légitimement, potentiellement intéressantes, ou inintéressantes. Pour valider notre approche, nos expériences sur l\u0027ensemble de données de la KDD-Cup-98 ont consisté à : i) générer des indicateurs synthétiques de qualité des données, ii) calculer les dix meilleures règles d\u0027association (en terme de support/confiance) et calculer leur qualité moyenne à partir de la qualité de leurs données, iii) calculer le coût qu\u0027entraîne la décision de choisir des règles illégitimement intéressantes, iv) examiner le coût et la décision sur le choix de ces règles quand la qualité des données varie. Nos expériences confirment notre hypothèse initiale : les mesures d\u0027intérêt des règles d\u0027associations découvertes ne sont pas autosuffisantes et la qualité d\u0027une règle d\u0027association dépend de la qualité des données à partir desquelles elle est calculée. La qualité de données inclut de diverses dimensions devant être considérées pour assurer et valider la qualité des connaissances extraites.\nRéférences\nTan P-N., Kumar V., and Srivastava J., (2002 \nSummary\nIn this paper we propose to exploit the measures describing the quality of data for defining the quality of association rules resulting from the rule mining process. We propose a probabilistic cost-based decisional model for the selection of legitimately, potentially interesting or not interesting rules when the quality of data they are computed from is correct, medium or low. The experiments on the KDD-CUP-98 dataset show that the Top 10 rules selected with good support and confidence measures are not interesting unless the quality of their data is correct or improved.\n"
  },
  {
    "id": "1060",
    "text": "Introduction\nLa compréhension des évolutions du bâti s\u0027appuie sur l\u0027analyse conjointe de connaissances spécifiques et de connaissances génériques ayant, dans le champ du patrimoine architectural, des caractéristiques très handicapantes vis à vis des technologies actuelles de gestion d\u0027information localisées spatialement. Ces connaissances s\u0027appuient en effet sur des informations hétérogènes, réparties, fortement pluridisciplinaires, mais également floues, incertaines, régulièrement remises en question, à ré-interroger comparativement sur un territoire donné ou entre territoires. Dès lors l\u0027apport attendu de l\u0027application des NTIC au domaine du patrimoine en matière de production et surtout d\u0027échanges de connaissances reste pour l\u0027essentiel prospectif, si ce n\u0027est du strict point de vue de la vulgarisation.\nPourtant, de nombreux travaux menés traitent des aspects liés à l\u0027acquisition de données 3D (De Luca et al., 2003), la gestion d\u0027informations localisées spatialement (Sebillo, 2003), ou encore de la représentation de données spatio-temporelles (Renolen, 1997) (Spaccapietra, et al., 2004). En parallèle, l\u0027acquis en matière de visualisation de données (y compris à caractère spatio-temporelles) dans le champ de la visualisation d\u0027informations constitue une -347 -RNTI-E-6 base de références conséquente. Cependant, l\u0027aptitude de la représentation architecturale « digitale » ( i.e. de la maquette numérique 2D/3D) à transmettre des éléments de connaissance sur un territoire et à souligner les vecteurs de différence/ressemblance entre objets architecturaux, reste encore très largement terra incognita. L\u0027étude du patrimoine architectural peut se voir comme un processus d\u0027acquisition de connaissances à l\u0027appui duquel l\u0027auteur de l\u0027étude rassemble des éléments et indices correspondant à trois grandes familles de ressources : -des connaissances a priori (i.e. théorie : typologies, savoir-faires, courants stylistiques et artistiques, contraintes constructives, solutions canoniques, etc.); -des sources documentaires hétérogènes (manuscrits, représentations artistiques, étu-des provenant de diverses disciplines, cartographie, etc.; -des observations contemporaines (campagnes de fouilles, relevés, etc.).\nCes trois grandes familles de ressources donnent lieu à analyses croisées afin d\u0027une part de mieux comprendre le bâti au sens large et d\u0027autre part de proposer des représentations traduisant graphiquement ce que l\u0027auteur de l\u0027étude a compris de l\u0027évolution de l\u0027édifice. Or si, dans chacune de ces trois grandes familles, des travaux de recherche et de développement sont menés pour mieux prendre en compte les spécificités du patrimoine architectural et de son étude, peu de travaux ont à ce jour porté sur « comment mettre en vis-à-vis des éléments appartenant à ces différentes familles». En réponse, nous avons montré (cf. Blaise et Dudek, 2004b) que l\u0027utilisation de la forme architecturale comme moyen d\u0027organiser et de donner accès à ces jeux d\u0027indications, permettait de visualiser ces derniers au travers de maquettes 2D/3D simulant les évolutions d\u0027édifices ou de tissus urbains. Nous avons en conséquence introduit l\u0027idée qu\u0027un modèle théorique, intensionnel, de l\u0027objet architectural peut jouer le rôle de filtre sur notre jeu d\u0027indications. A l\u0027issu de ce travail, nous avons cependant mis en évidence les limites d\u0027une telle approche intensionnelle face aux réalités souvent confuses du patrimoine bâti (transformations, réemplois, incertitudes diverses) (cf. Fig. 2).\nC\u0027est dans cette continuité que s\u0027inscrit notre contribution. Elle souligne un constat de manque : comment prendre en charge les étapes amont de l\u0027étude d\u0027un lieu, quand nous ne savons encore presque rien de lui ? Comment formaliser de façon opérationnelle un processus d\u0027acquisition de connaissances adapté aux contraintes spécifiques du patrimoine bâti (étapes d\u0027études non-ordonnées, données floues et qui le resteront, interdisciplinarité, etc.) ?\nNous introduisons dans cette contribution une démarche de modélisation réflexive objets physiques-connaissances associées qui s\u0027attache à décrire ce que nous comprenons d\u0027un bâti et pas un bâti idéalisé sans rapport avec les complexités du patrimoine réel. Elle se traduit par la mise en relation dans une maquette virtuelle de l\u0027objet physique (ainsi placé au lieu et au temps HWWGHHFHHTXLLMXVWLILHHVDDSUésence en au temps évaluées par exemple en terme de vraisemblance). Cette démarche met en relation dynamique trois éléments : informations qualifiées par des descripteurs propres au domaine (incertitude par exemple), corpus d\u0027objets architecturaux théoriques multi-échelles et maquettes virtuelles dans lesquelles les informations susmentionnées sont visualisées et délivrées par l\u0027intermédiaire de codes graphiques affectés aux instances du corpus théorique d\u0027objets architecturaux.\nCette démarche de modélisation, que nous nommons modélisation informationnelle, a pour objectif un gain de compréhension du lieu architectural et des informations qui lui sont associées. Notre contribution situe brièvement nos travaux antérieurs sur le sujet, puis introduit les filiations de cette démarche, et discute de son application au cas concret de la place centrale de Cracovie.\n-348 -RNTI-E-6\nContexte et hypothèses\nDe Quatremère De Quincy jusqu\u0027à Le Corbusier l\u0027architecture a été considérée sous le double angle : d\u0027une discipline fondée sur l\u0027idée de mesure et d\u0027un art (Fichet, 1979, et Corbusier, 1958. Il apparait donc légitime, au vu du premier qualificatif, de tenter de circonscrire les qualités observables de stabilité (Lemoigne, 1977) de l\u0027objet architectural, autrement dit de l\u0027inscrire a priori dans un univers de connaissance (Francis et al., 1999) dont un ou des modè-les structurent de façon abstraite les composantes. Nous considérons l\u0027objet patrimonial comme la conjonction d\u0027un bâti (observable ou détruit) et de connaissances hétérogènes. A partir de là, la proposition méthodologique que nous avons faite (Blaise et Dudek 2004a) consiste à établir un canevas de règles permettant d\u0027isoler au sein du corpus architectural les concepts architecturaux à modéliser. L\u0027élément ainsi isolé, identifie une famille d\u0027objets, famille qui transcende les particularismes régionaux et facilite le travail de comparaison (cf. Fig. 1  Blaise et Dudek, 2004b), il s\u0027appuie sur le formalisme objet, largement répandu aujourd\u0027hui et qui impose rigueur dans la relecture des références utilisées pour établir la logique d\u0027identification et de dérivation des concepts (cf. Ducournau et al, 1998). Sa double organisation spécialisation / agrégation se prête bien à la définition d\u0027un modèle médiateur entre plusieurs vues sur l\u0027édifice.\nMais le champ de l\u0027architecture patrimoniale implique des contraintes particulières : -échelonnement des phases de construction et transformation ; -masquages successifs d\u0027états cohérents ; -incomplétude d\u0027un objet, objets \"mal formés\", incertitude sur les données, etc. ; -données évolutives / incomplètes.\nEn conséquence, son application se heurte à des limites opératoires réelles et appelait une prise de distance avec l\u0027approche déterministe, que nous avions privilégiée au départ. En réponse, nous nous sommes focalisés sur l\u0027idée de développer une méthode de travail permettant de ne décrire que ce que nous comprenons d\u0027un bâti, et donc ne faisant appel à l\u0027instanciation du modèle déterministe présenté ci-avant qu\u0027in fine, si et seulement si le niveau de connaissance réel sur l\u0027objet le permet. C\u0027est cette méthode que notre contribution présente, d\u0027abord par ses filiations scientifiques (cf. section 3), puis par son application concrète à l\u0027instrumentation d\u0027un processus d\u0027acquisition de connaissances (cf. section 4). \n-Limites d\u0027un modèle intensionnel : objets réemployés -( a.) appui et deux baies obstruées ; désemployés -( a.) arc de décharge ; déformé -( b.) oculus ; ( e.) linteau, incomplets -( b.) baie ; non reproduits -( d.) ; fragmentaires -( f.). La morphologie canonique de ces objets ne recouvre plus la réalité observée, mais ils conservent leur identité d\u0027instance d\u0027un concept général (arc, baie,…).\nDeux filiations, une méthode de travail\nDans le champ de l\u0027architecture patrimoniale, l\u0027étude des évolutions de lieux s\u0027appuie prioritairement sur l\u0027analyse d\u0027une documentation permettant de circonscrire petit à petit un état de connaissance sur ces lieux. Le terme documentation recouvre ici à la fois données brutes (sources bibliographiques, relevé architectural, compte-rendu de fouilles, ...) ou interprétées (restitutions, hypothèses, ...). Mais une telle documentation est presque toujours hété-rogène, incomplète, contradictoire et progressive. Notre état de connaissance s\u0027en trouve lui aussi sujet à évolution. Ces difficultés nous ont conduit à formuler trois constats :\n-la forme d\u0027un lieu bâti 1 reste le plus souvent hypothétique; -la représentation fixe une étape dans l\u0027évolution de lieu , mais aussi un moment dans notre étude de ce lieu 2 ; -une distinction opératoire claire doit être introduite entre l\u0027acte de restituer une hypothèse et celui de visualiser ce que l\u0027on sait vraiment.\nA partir de là, trois hypothèses de travail ont été identifiées, sur la base desquelles de nouvelles expérimentations ont été conduites (Blaise et Dudek., 2005): -la forme architecturale constitue un médiateur naturel entre les informations à manipuler (parce que l\u0027on peut y rapporter les différentes couches d\u0027information sur l\u0027évolution de l\u0027objet) ; -la forme architecturale est un bon moyen de les trier, de les interfacer (parce qu\u0027elle localise dans l\u0027espace et dans le temps des jeux d\u0027informations) ; -la forme à représenter doit être comprise comme un substitut de l\u0027objet, elle n\u0027est pas à l\u0027image de l\u0027objet,\nelle est l\u0027idée que nous nous faisons de l\u0027objet en fonction des connaissances réelles dont nous disposons au temps t de notre étude.\nDès lors nous sommes en face de deux contraintes à première vue presque contradictoires : d\u0027abord manipuler des formes architecturales et donc les représenter, et de l\u0027autre manipuler des informations sur les formes architecturales et donc les visualiser. Comment sur notre terrain d\u0027expérimentation concilier les exigences de la représentation d\u0027objets physiques (par exemple l\u0027exhaustivité géométrique) et les exigences de fidélité aux informations sous jacentes (par exemple « nous ne savons pas comment était bâti cet édifice ») ?\nEn réponse, la démarche de modélisation informationnelle que nous tentons de circonscrire fait un pont entre le domaine de la modélisation spatiale proprement dite (représentation géométrique, multi-représentations, multi-résolutions, etc.) et celui de la visualisation d\u0027informations au sens d\u0027E.R. Tufte (Tufte, 2001) Force est de constater que l\u0027apparition des NTIC n\u0027a pas contribué de façon notable à l\u0027émergence de cette démarche. Aujourd\u0027hui, dans le champ de l\u0027architecture les scènes virtuelles sont vues avant tout comme un dispositif de vulgarisation. Ces scènes sont réguliè-rement présentées à un large public pour « montrer » comment un objet architectural a pu évoluer au cours du temps, quelle que soit l\u0027échelle considérée (de l\u0027ensemble urbain aux décors) (Bonfigli et al., 2000), voire pour exploiter un lieu à distance (Salonia et al., 2004). Mais cette utilisation du graphique est remise en cause dans notre champ en particulier sur deux points (cf. Kantner, 2000): -un manque de lisibilité des représentations dû au fait que les inférences faites pour la reconstruction géométrique des objets sont masquées dans la scène finale ; -un manque d\u0027efficacité affligeant pour les chercheurs eux-mêmes qui investissent temps et moyens dans la production de reconstructions qui restent un effet de bord de leur étude, puisque ne donnant pas accès aux couches d\u0027informations plus profondes comme la bibliographie par objet, l\u0027inscription typologique, terminologique, etc.. Autrement dit, l\u0027effort d\u0027acquisition et d\u0027analyse d\u0027informations fait pour comprendre l\u0027objet architectural apparaît comme totalement absent du résultat final, une reconstruction dite virtuelle. Une telle représentation n\u0027est pas liée aux sources documentaires en justifiant le contenu, elle n\u0027est pas mise à jour dynamiquement quand de nouveaux éléments d\u0027informations sont rassemblés, elle ne mentionne même pas ce qui est en définitive le plus significatif pour l\u0027analyste : l\u0027incertitude des données initiales. Dans le champ de la visualisation d\u0027informations au contraire, le graphique est non seulement utilisé pour interroger des jeux de données, mais également pour les trier, ou dit autrement pour réduire la multitude des données dans les mots de J.Bertin (Bertin, 1998). Le rôle du graphique, dans les mots d\u0027ER Tufte (Tufte, 1997) « … Nous visualisons des informations pour raisonner sur des connaissances, pour documenter, communiquer et préserver ces connaissances … », nous y semble beaucoup plus compatible avec nos objectifs scientifiques réels.\nNous posons en hypothèse l\u0027existence d\u0027une démarche de modélisation qui s\u0027appuie sur la double filiation établie ici. Dans la pratique, la démarche 2 de modélisation informationnelle se matérialise par un jeu de règles de production graphique qui agissent comme gardefous dans l\u0027implémentation du processus d\u0027acquisition de connaissances (cf. section 4). Nous en donnons ci-dessous un extrait : -s\u0027appuyer sur un modèle théorique autorisant comparaisons et réutilisation ; -affecter les objets a une échelle donnée ; -produire les représentations au vol pour refléter un état de connaissance courant ; -adjoindre a la représentation d\u0027objets une évaluation qualitatives des sources documentaires utilisées pour leur étude (incertitude par exemple) traduite graphiquement ; -développer un jeu de signes graphiques permettant de traduire ce que l\u0027on sait et ce que l\u0027on ne sait pas ; -produire des représentations interrogeables objet par objet pour consulter les sources justifiant les inférences faites au cours du processus d\u0027analyse des formes ; -choisir un niveau d\u0027abstraction adapte au contenu informationnel a délivrer et/ou au niveau de complétude dans l\u0027étude de l\u0027objet ; -enfin, règle des règles, si le graphique ne produit pas un gain d\u0027intelligibilité des sources derrière les objets représentes, le considérer inutile.\nUn processus d\u0027acquisition de connaissances\nL\u0027étude de l\u0027édifice à ses différentes échelles fait appel à une masse importante de documents historiques, que nous décrivons et attachons à des instances d\u0027un modèle architectural théorique. Mais les interprétations qui sont faites des documents ne permettent pas nécessai-rement de faire ce travail d\u0027attachement ou de filtrage de la documentation, soit parce que la quantité d\u0027informations n\u0027est pas suffisante, soit parce que le travail est en cours . Puisque nous voulons utiliser la forme architecturale pour filtrer notre documentation, nous pouvons être en face d\u0027une situation où nous devons mettre en relation l\u0027idée encore floue que nous avons d\u0027un lieu architectural HWW OHH PRGèle théorique décrit en section 2, modèle déterministe, univoque. A partir de ce constat nous établissons d\u0027abord qu\u0027un premier indice est souvent un repère terminologique (nom du concept dont relève le lieu ), complété par un repère toponymique -nom(s) historique(s) de ce lieu 1RXVV DYRQVV HQ conséquence développé d\u0027abord un modèle générique de toponyme architectural qui permet de localiser de façon univoque, dans une hiérarchie simulant la notion d\u0027échelle et dans une chronologie, un jeu de données par rapport aux objets qu\u0027il documente. Cette localisation toponymique définit un lieu particulier, éventuellement repéré dans l\u0027espace par des coordonnées géographiques, ou simplement repéré comme sous-partie d\u0027un toponyme plus géné-ral. La localisation toponymique s\u0027accompagne d\u0027une localisation terminologique, qui définit la notion générale correspondante. Ces deux notions sont implémentées sous la forme de classes (au sens de la POO) liées par agrégation et dont les états des instances sont figés dans des fichiers XML exploités soit pour la visualisation 2D/3D soit textuellement. Ces « bases » terminologiques et toponymiques sont modestes (800 termes, hiérarchie de 20 toponymes) : elles n\u0027ont pas vocation à l\u0027exhaustivité mais doivent se construire au fur et à mesure des expé-riences.\nHiérarchisation / localisation\nInstanciation / Représentation\nInformations spécifiques Documentation (analyse)  4 Indices présence d\u0027un « objet potentiel » déclenchant l\u0027étude, données brutes hétérogènes (de l\u0027archive au relevé), hétérogénéité des systèmes d\u0027information préexistants à l\u0027étude (lorsqu\u0027ils existent), éventail disciplinaire, rôle des comparaisons et des analogies, etc.. Il faut noter que ces éléments ne se situent pas nécessairement dans une continuité, ni en terme de succession (la conclusion d\u0027une phase ne déclenche pas le démarrage d\u0027une autre), ni en terme de planification (l\u0027échelonnement des -   (visualisations, simulations, comparaisons et analogies, affiliations stylistiques phases dans le temps peut être considérable), ni enfin en terme de chaînage (chaque phase apporte son jeu de conclusions intermédiaires propres, une phase n\u0027est généralement pas un pré-requis à une autre).\nTerme du vocabulaire\nInstance(s)\nToponyme\nIndices initiaux\n5 La maquette 2D/3D sert cet objectif (dispositif de visualisation) en phase aval de l\u0027étude. Nous avons ici besoin d\u0027un équivalent à la maquette, mais d\u0027un équivalent plus abstrait. Il s\u0027agit donc en fait d\u0027une démarche de généralisation du modèle architectural tel que présenté dans (Blaise et Dudek, 2004).\n-354 -RNTI-E-6 ou typologiques, etc.). Un dispositif de visualisation (diagramme SVG la forme d\u0027une spirale, cf. Fig. 5, 6, 7) donne de chaque objet architectural étudié une vue synthétique montrant les éléments d\u0027informations rassemblés autour de l\u0027objet, module par module.\nLa présentation de cette spirale de niveau d\u0027investigation permettra de cerner concrète-ment le cadre de ce développement. Elle est utilisée à la fois comme dispositif de visualisation et comme moyen de navigation dans les différents systèmes ou applications adaptés aux jeux de données concernés. Nous avons appliqué ce dispositif en priorité au cas de la place centrale de Cracovie (17 édifices, une cinquantaine d\u0027évolutions) et sur le cas du château comtal de Carcassonne. La spirale est le résultat d\u0027une méthode d\u0027affichage de l\u0027objet MIR (au sens de la POO) qui la produit après lecture des valeurs courantes de ses composants (eux-mêmes objets liés par agrégation). En conséquence, elle reflète un état toujours à jour des connaissances que nous avons rassemblé. Chaque objet partie-de MIR est accessible et manipulable indépendamment de ce concept « de synthèse » au travers d\u0027applications Web construites sur un même modèle 6 . L\u0027interface de navigation dans ces applications (cf. Fig. 6) s\u0027ouvre sur un portail général permettant d\u0027accéder par l\u0027objet architectural (haut droite) aux différents modules rassemblés (terminologie, toponymie, documentation, etc.), mais aussi de manipuler indépendamment chacun de ces modules (bas droite, la terminologie). \nSummary\nIn order to understand and represent the evolutions of architectural artefacts, an issue renewed by the development of information technologies, an analyst bases its study on pieces of knowledge and information progressively gathered and filtered. In our domain of interest i.e. the architectural heritage, these elements are often space related (a link between the artefact\u0027s GHVFULSWLRQQDQGGLWVVORFDWLRQQ (ex. heterogeneity, ambiguity, contradicting character, etc.). We take advantage of the spacerelation property in order to gather and unify the various pieces of knowledge related to an artefact: generic knowledge, documentation, present-day observations. This approach, that we have named informative modelling, aims at gaining insight into an architectural artefact\u0027s evolution and into the information gathered about it. Our contribution introduces the scientific background of our approach, and the methodological framework we have derived from it. It also discusses its implementation -the Main Square in Cracow (Poland) -in order to evaluate the possible benefits in terms of knowledge management and visualisation.\n"
  },
  {
    "id": "1061",
    "text": "Introduction\nLe cadre général de l\u0027apprentissage automatique part d\u0027un fichier d\u0027apprentissage comportant n lignes et p colonnes. Les lignes représentent les individus et les colonnes les attributs, quantitatifs ou qualitatifs observés pour chaque individu ligne. Dans ce contexte, on suppose également que l\u0027échantillon d\u0027apprentissage est relativement conséquent par rapport au nombre d\u0027attributs. Généralement la taille de l\u0027échantillon est de l\u0027ordre de 10 fois le nombre de variables pour espérer obtenir une certaine stabilité, c\u0027est-à-dire une erreur en généralisation qui n\u0027est pas trop loin de l\u0027erreur en apprentissage. De plus, l\u0027attribut à prédire est supposé à valeur unique. C\u0027est une variable à valeurs réelles dans le cas de la régression et c\u0027est une variable à modalités discrètes, appelées classes d\u0027appartenance, dans le cas du classement. Ces questions relatives aux rapports entre taille d\u0027échantillon et taille de l\u0027espace des variables sont étudiées de façon très approfondies dans les publications relatives à l\u0027apprentissage statistique (Vapnik, 1995). Dans ce papier nous décrivons une situation d\u0027apprentissage qui s\u0027écarte significativement du cadre classique tel que décrit plus haut. En effet, le contexte expérimental ne nous permet pas de disposer immédiatement d\u0027un ensemble d\u0027apprentissage conséquent, chaque individu peut appartenir à plusieurs classes simultanément, et chaque individu, au lieu d\u0027être décrit par un ensemble attributs-valeurs, l\u0027est par un texte en langage naturel en anglais.\nAvant de décrire l\u0027approche que nous préconisons pour apprendre dans ce contexte, nous allons tout d\u0027abord rappeler la problématique de l\u0027application visée (section 2). En section trois, nous décrivons l\u0027approche méthodologique retenue. Dans la section quatre, nous décrivons les étapes mises en oeuvre pour mettre en forme les données et notamment, la stratégie d\u0027analyse linguistique mise en oeuvre pour extraire les principaux concepts qui vont jouer le rôle de variables. Nous décrivons ensuite, section 5, les modèles topologiques à base de graphes de proximité qui nous permettent de gérer le multi-classes. Dans un but comparatif, nous utilisons une méthode à base d\u0027arbre de décision qui nous sert également à mieux identifier les concepts discriminants. En section 6, nous présentons les résultats issus de l\u0027analyse linguistique et de l\u0027apprentissage. Nous décrivons également le principe de l\u0027apprentissage par boucle de pertinence (relevance feedback). Ce concept est central car il met l\u0027usager dans la boucle visant à améliorer le modèle de prédiction. Nous détaillons les performances obtenues. En section 7, nous concluons et détaillons les perspectives de ce travail, notamment l\u0027utilisation de méthodes de règles d\u0027association.\nCadre expérimental\nProblématique\nCe travail s\u0027inscrit dans un projet en collaboration avec le Bureau International du Travail (BIT). Plusieurs pays ont signé des conventions avec le BIT qui les lient au droit du travail international. Plus concrètement, l\u0027accord porte sur deux conventions élaborées par le BIT, 1 La Convention n°87 et la Convention n°98. Celles-ci contiennent une série d\u0027articles de lois que le signataire s\u0027engage à respecter. Ces derniers sont soumis, une fois par an, à une inspection ayant pour but de vérifier la bonne application de ces conventions. A la fin de chaque inspection, les experts du BIT délivrent un rapport au pays concerné. Le rapport fait état des règles non appliquées, des violations constatées à partir de faits concrets et souligne les efforts à mettre en place pour être en adéquation avec les conventions. Il est en texte libre sans codification rigide des violations. Tous les rapports d\u0027experts sont stockés dans une banque de données accessible aux membres et aux experts qui effectuent régulièrement des analyses, définissent de nouvelles recommandations et étudient les évolutions du droit du travail dans les différents pays, etc. L\u0027objectif de notre travail est définir et de mettre en place des méthodes et des outils de data mining permettant de traiter plus efficacement et plus rapidement ces corpus qui deviennent inexploitables manuellement. Les experts du BIT souhaiteraient avoir des outils permettant le repérage automatique des textes signalant la violation d\u0027une ou plusieurs règles par pays. La finalité étant la catégorisation automatique des textes non étiquetés. Les experts pourront alors synthétiser plus vite les difficultés que rencontrent les différents pays dans l\u0027application de ces conventions, et, le cas échéant, identifier les moyens de les aider.   Soit ? un algorithme d\u0027apprentissage supervisé. Cela peut être un graphe d\u0027induction (Zighed et Rakotomalala, 2000), un SVM (Vapnik, 1995), etc. Le résultat d\u0027un apprentissage est un modèle noté M et un taux d\u0027erreur ? en généralisation estimé sur échantillon test ou par cross validation.\nDescription du corpus\nL\u0027application du modèle M sur un échantillon de textes anonyme ? de taille relativement modeste, disons une vingtaine de cas, permet de prédire pour chaque individu anonyme ?? les règles qui seraient violées, ?(??) \u003d {c , c } par exemple. Le   (Toussaint (1980)) qui font partie des méthodes d\u0027apprentissage à base d\u0027instance permettent cela. Toutes ces techniques supposent par ailleurs que les individus sont plongés dans un espace de représentation sur lequel on peut définir une métrique. Les textes doivent par conséquent être transformés en un ensemble de vecteur. Chaque texte pourra être alors considéré comme un point de R p . Les coordonnées d\u0027un texte ? dans cet espace seront X(?) \u003d (X 1 (?), X 2 (?),…, X p (?)) . Que représente alors ces variables, comment sont elles extraites ? C\u0027est l\u0027objet de la partie analyse linguistique. L\u0027objectif étant de trouver les concepts et les plus adaptés.\nApplication sur les données du BIT\nNous effectuons l\u0027extraction de la terminologie sur l\u0027intégralité du corpus. La finalité de cette extraction est la construction de concepts relatifs aux deux Conventions. Les concepts ainsi extraits constituent l\u0027espace de représentation des documents. Les textes étiquetés par les experts du BIT (violations connues), nous servent ensuite de base d\u0027apprentissage. Nous utilisons deux classifieurs : C4.5 (Quinlan, 1993) et les graphes des voisins relatifs (GVR) (Toussaint, 1980) dans le but de prédire les violations contenues dans les textes non étiquetés. La méthodologie est décrite par (figure 1). RNTI-E-6 V. Pisetta et al.\nFIG. 1 -Méthodologie d\u0027analyse\nEspace de représentation des textes\nExtraction de la terminologie\nNous choisissons de construire notre espace de représentation par extraction de concepts. Un des avantages de cette technique par rapport à des méthodes telles que les N-grammes, ou les matrices de co-occurrences de mots, est la réduction importante de la dimensionnalité, permettant notamment l\u0027usage de classifieurs utilisant des mesures de similarité. Diverses applications basées sur ce principe ont données des résultats intéressants (Kumps et al., 2004). Deux méthodes différentes existent pour la construction de concepts : par apprentissage et par extraction.\nLa première (statistique) recherche les mots les plus discriminants selon un attribut à prédire. Les mots sont ensuite regroupés en concepts sur la base de leur co-occurrences ou à partir de règles d\u0027association (Kumps et al., 2004). La seconde méthode (linguistique) consiste à extraire la terminologie du corpus et à regrouper les termes extraits selon leur proximité sémantique.\nNotre préférence se porte vers les techniques d\u0027analyse linguistique. Ce choix se justifie par le fait que l\u0027analyse linguistique permet de lutter contre la polysémie et de lever certaines ambiguïtés liées au contexte (Flurh, 2000). Elle fonctionne également sur de petites unités textuelles (Pouliquen et al., 2002). De plus, notre base d\u0027apprentissage comportant peu d\u0027exemples, il nous semble difficile d\u0027utiliser les techniques d\u0027apprentissage décrites plus haut. Notre travail est effectué en collaboration avec des experts du domaine juridique, ce qui est une raison supplémentaire pour utiliser les techniques linguistiques. Nous utilisons la chaîne de traitement décrite en (figure 2).\nFIG. 2 -Chaîne de traitement linguistique\nAprès une phase de normalisation (traitement des noms propres, conservation ou non des majuscules, etc.), nous effectuons un étiquetage grammatical du corpus grâce au logiciel BRILL (Brill, 1995). Cette opération a pour but d\u0027attribuer à chaque mot son étiquette grammaticale. Nous passons ensuite à l\u0027extraction de la terminologie à l\u0027aide d\u0027EXIT (Roche et al., 2004). La méthodologie mise en place dans EXIT est avant tout basée sur une approche statistique, contrairement à d\u0027autres approches (Bourigault et Jacquemin, 1999). L\u0027extraction terminologique passe par la recherche de candidats-termes. Ces derniers sont des ensembles de deux ou plus unités adjacentes lexicales (mots), syntaxiques (mots étiquetés grammaticalement, ce qui est notre cas) ou sémantiques (mots étiquetés conceptuellement). Nous groupons ensuite les candidats-termes extraits selon leur proximité sémantique, de manière à repérer les concepts présents dans les textes.\nChoix des candidats termes et création de concepts\nNous reprenons ici une méthodologie utilisée par (Baneyx et al., 2005). Nous distinguons deux étapes dans la sélection des candidats termes : -Dans un premier temps, nous parcourons l\u0027ensemble des résultats donnés par EXIT, et nous étudions tout d\u0027abord les termes dont la fréquence d\u0027apparition est supérieur à un seuil l. Dans un premier temps, nous fixons un seuil élevé. Cette étape préliminaire permet de repérer les grands axes conceptuels ; -Nous regroupons ensuite les candidats termes sémantiquement proches à l\u0027aide d\u0027outils tels que WordNet (Miller et al., 2005). Le recours à l\u0027expert est ici primordial. Certaines plates-formes proposent des outils plus sophistiqués comme Syntex-Upery (Bourigault et Jacquemin, 1999) qui permettent d\u0027analyser la proximité distributionnelle entre les candidats-termes.\nLes phases de construction 1 et 2 étant itératives, nous augmentons très rapidement la représentation en examinant les candidats-termes dont la fréquence d\u0027apparition dans le corpus est inférieure au seuil l. L\u0027augmentation progressive du nombre de candidats-termes possède deux issues : -certains « nouveaux » candidats-termes viennent renforcer des concepts existants ; -d\u0027autres « nouveaux » candidats-termes créent de nouveaux concepts qui peuvent être des concepts fils de ceux existants.\nReprésentation vectorielle des documents\nA ce niveau se pose le problème du choix du modèle de représentation. Nous avons choisi le modèle vectoriel (Salton, 1971) qui nous paraît plus adapté que le modèle booléen. La raison est qu\u0027il semble simpliste d\u0027appliquer une logique binaire à une recherche d\u0027information. De plus, le modèle vectoriel permet de calculer des scores de similarités entre différents documents (Pouliquen et al., 2002).\nLe modèle vectoriel propose de représenter un document sur les dimensions représentées par les mots. Nous l\u0027avons adapté pour représenter un document par un vecteur de concepts. Et, plutôt que de le représenter en fonction de la fréquence du concept dans le document, nous utilisons la pondération TF x IDF (Salton et Buckley, 1988). Ce score permet de donner une importance au concept en fonction de sa fréquence dans le document (TF \u003d Term Frequency) pondérée par la fréquence d\u0027apparition du concept dans tout le corpus (IDF \u003d Inverse Document Frequency). Ainsi un concept très spécifique au document aura un score correspondant à sa fréquence d\u0027apparition, par contre, un concept apparaissant dans tous les documents du corpus aura une pondération maximale. Nous calculons donc, pour chaque concept dans un document, son score TF x IDF. \nModélisation et outils de généralisation\nNous avons à présent défini un espace de représentation pour les documents. L\u0027objectif est maintenant d\u0027utiliser des techniques d\u0027apprentissage dans le but de classer automatiquement les documents. Dans notre étude, nous sommes amenés à classer des textes « multi-étiquettes », autrement dit, susceptibles de comporter plusieurs violations. Deux possibilités sont alors envisageables :\n-une approche globale ; -une approche binaire en une division en m sous-problèmes.\nNous effectuons les deux approches. Deux classifieurs différents sont utilisés. Nous utilisons les arbres de décision dans l\u0027approche binaire et les graphes de voisinage dans l\u0027approche globale.\nPrédiction par le graphe des voisins relatifs\nLa représentation vectorielle de nos documents est d\u0027une dimensionnalité très raisonnable et nous permet par conséquent d\u0027avoir recourt à des classifieurs basés sur la notion de voisinage. Nous avons choisi les graphes de proximité provenant de la géométrie computationnelle (Preparata et Shamos, 1985) plutôt que les k-NN. Les graphes présentent plusieurs avantages par rapport aux k-NN et permettent de mieux définir la proximité entre des individus (Clech, 2004). Ils nécessitent une mesure de dissimilarité (Toussaint, 1980). Nous choisissons la distance euclidienne. Plusieurs modèles de graphes existent. Notre choix se porte sur le graphe des voisins relatifs qui est un bon compromis entre nombre de voisins  ( , ) est vide (Toussaint, 1980). De façon formelle: \nPrédiction par arbre de décision\nNous nous plaçons ici dans l\u0027optique de prédire la présence ou l\u0027absence de chaque violation. Nous construisons par conséquent autant d\u0027arbres qu\u0027il existe de règles. Plus formellement, nous considérons chaque règle comme étant un attribut booléen c i \u003d {0,1} . S\u0027il existe k règles pour la violation v ? , nous construisons k arbres. Chaque arbre est alors un modèle M i prévoyant la présence ou l\u0027absence de chaque règle c i . Nous obtenons ainsi k modèles qui renvoient c i si la règle i est estimée violée, ? sinon. Notons que cette approche est valable dans la mesure où les violations sont a priori indépendantes. Il suffit ensuite d\u0027agréger les modèles pour obtenir un « méta-modèle » donnant la liste des violations détectées pour le texte ? \u0027 . La discrimination est effectuée par l\u0027algorithme C4.5.\nRésultats, méthodes et comparatif\nA l\u0027issue de l\u0027analyse linguistique, nous obtenons 17 concepts pour la Convention n°87 et 11 concepts pour la convention n°98. Nous présentons les résultats observés sur la Convention n°98. Notre base de textes étiquetés est de taille modeste (65 textes). A ce jour, une étape du processus de relevance feedback a été réalisée. Elle concerne 20 textes qui ont été étiquetés par les experts du BIT et qui n\u0027étaient pas présents initialement dans la base d\u0027apprentissage. Nous présentons les résultats de la prédiction sur ces 20 textes issue de C4.5 et GVR. Dans un but comparatif, nous avons utilisé les SVM selon le même principe que pour C4.5. Les SVM sont des méthodes robustes résistant très bien à la forte dimensionnalité des données (Joachim, 1998). La différence essentielle réside dans le fait que les SVM sont utilisées sans pré-traitement des textes (excepté la normalisation). Les résultats sont présentés en 6.1.\nRésultats obtenus\nNous présentons les résultats obtenus en terme de reclassement. Les résultats sont décrits dans (tableau 1).\nOn observe de bon taux de reclassement. Notons qu\u0027il n\u0027existe qu\u0027une seule violation pour laquelle SVM fait mieux que C4.5 ou GVR. La non prise en compte de séquences de mots par SVM rend les prédictions parfois instables, ce qui se traduit par une mauvaise sensibilité ou spécificité. Nous observons des taux de sensibilité-spécificité parfois nuls pour GVR. Ceci est dû au fait que deux des dix violations (n°4 et n°10) sont peu fréquemment rencontrées dans le corpus d\u0027apprentissage. Ainsi, il y a peu de chances que les quelques textes contenant ces violations soient en nombre suffisant pour être pris en compte dans le voisinage de l\u0027individu à étiqueter. Ce problème peut éventuellement se résoudre par la technique de retour pertinent décrite précédemment. \nC4.5 GVR SVM\nConclusion et perspectives\nLa finalité de ce travail est de proposer un modèle de prédiction capable de déterminer les violations de plusieurs pays concernant deux convention de droit du travail. Une approche d\u0027apprentissage automatique a été adoptée. Dans un premier temps (préparation des données), nous avons extrait, grâce aux techniques d\u0027analyses linguistiques, un ensemble de candidats termes qui nous permettent ensuite de construire des concepts relatifs au corpus étudié. Cette opération a pour but de réduire la dimensionnalité de l\u0027espace de représentation des textes du corpus. Nous avons été ainsi en mesure d\u0027utiliser les graphes de voisinage, en plus d\u0027une méthode plus classique (C4.5) pour la catégorisation automatique.\nLes résultats semblent intéressants dans la mesure où les deux méthodes de prédiction que nous utilisons aboutissent à des taux de reclassement tout à fait acceptables en dépit d\u0027une base d\u0027apprentissage comportant peu d\u0027exemples. Nous envisageons à présent d\u0027augmenter la taille de celle-ci dans le but d\u0027améliorer la prédiction et d\u0027aboutir à des résultats plus robustes. La phase de test avec les experts du BIT est en cours. La liste des concepts extraits du corpus a été validée par ces derniers.\nL\u0027une des perspectives de ce travail est d\u0027observer l\u0027impact du relevance feedback sur la qualité de prédiction. En effet, cette dernière devrait augmenter au fur et à mesure du nombre d\u0027interventions des experts. De plus, il serait intéressant de comparer de nouveau la qualité de prédiction de notre approche avec les SVM lorsque la base d\u0027apprentissage sera plus conséquente. L\u0027utilisation d\u0027autres techniques de catégorisation textuelles, comme Winnow (Dagan et al., 1997) et éventuellement d\u0027autres classifieurs peut aussi s\u0027avérer intéressantes.\n"
  },
  {
    "id": "1062",
    "text": "Problématique\nTout géographe s\u0027accorde à dire que tout phénomène à un endroit est lié à l\u0027influence du voisinage (première loi en géographie). Ceci revient à dire que les données spatiales ne sont pas indépendantes et que leurs analyses nécessitent, en plus des caractéristiques des objets à analyser, la prise en compte des caractéristiques des objets du voisinage et des relations spatiales qui les relient.\nApproche proposée\nDans notre état de l\u0027art, nous avons recensé des insuffisances dans les outils d\u0027analyses spatiales ; et afin d\u0027y remédier, nous avons proposé une méthodologie pragmatique fondée sur des bases théoriques en tenant compte : de l\u0027inexistence des entrepôts de données dans la majorité des organismes ; de la nature complexe des données à référence spatiale ; des limites des fonctionnalités analytiques des outils existants entre autre Systèmes d\u0027Informations Géographiques ( SIG) et datamining ; etc. Afin de résoudre cette problématique, nous proposons la combinaison d\u0027un SIG avec un ensemble de techniques de datamining.\nDans un premier temps, nous avons proposé le cadre conceptuel permettant de définir la manière selon laquelle la combinaison devrait s\u0027opérer. Ce cadre a été illustré par la présentation d\u0027un enchaînement de phases devant constituer le processus décisionnel incluant un SIG et un ensemble de techniques d\u0027extraction. Ce processus se présente comme suit : Préparation des données : Consiste à préparer la base de données géographiques .\nConsultation des données :\nConsiste à analyser les données préparées pour vérifier si les critères du problème posé ont été respectés.\nCréation d\u0027un index de jointure : Cette étape est d éfinie comme la spécificité du datamining spatial par rapport au datamining classique. Elle permet de pré-calculer la relation spatiale exacte entre les objets spatiaux de deux collections puis de les stocker dans une table, pour y appliquer les techniques de datamining pour une meilleure exploitation. La méthode que nous avons développée pour la création des index de jointures est une méthode proposée par Zeitouni (2000). Cette méthode est une extension de l\u0027index de jointure qui stocke la valeur de distance entre les objets. Contrairement aux anciennes méthodes proposées pour la jointure basée sur la distance, seuls les objets ayant une distance raisonnable (définie par le concepteur de l\u0027index) sont stockés dans cet index. Ce qui optimise à la fois la construction et l\u0027utilisation de l\u0027index. Dans le cadre de cette étude, une attention particulière a été portée au paramétrage du critère de jointure.\nChoix de l\u0027algorithme de calcul : Notre étude porte sur une seule technique de datamining spatial qui est la classification supervisée par les arbres de décisions. Un arbre de décision a pour but de trouver les attributs explicatifs et les critères précis donnant le meilleur classement. L\u0027arbre est construit par l\u0027application successive de critères de subdivision sur une population d\u0027apprentissage afin d\u0027obtenir des sous populations plus homogènes. Dans le cadre de notre étude, nous proposons une extension de la méthode CART Zeitouni (2000) ainsi qu\u0027une extension de la méthode ID3 Zeitouni (2000). Ces deux dernières reposent sur le calcul d\u0027un gain informationnel pour apprécier la subdivision.\nExécution de l\u0027algorithme de calcul : Une fois que l\u0027index de jointure est créé, nous pouvons lancer l\u0027exécution de l\u0027algorithme de classification par arbre de décision.\n"
  },
  {
    "id": "1063",
    "text": "Introduction\nPour créer un nouveau médicament, la pharmacologie opère en deux temps. Tout d\u0027abord elle synthétise un grand nombre de molécules. Ces molécules sont ensuite appliquées sur un substrat simulant la pathologie que le médicament recherché doit combattre. Le débit de molécules synthétisées puis testées a grandement augmenté ces dernières décénnies avec l\u0027introduction de la synthèse combinatoire et le criblage à haut débit (Hou et al., 2004). Ce processus peut néanmoins être encore amélioré. En effet, une propriété essentielle des médicaments est de pouvoir être solubles pour circuler à travers le système sanguin afin d\u0027atteindre la partie malade de l\u0027organisme, or cette propriété n\u0027est pas vérifiée par toutes les molécules. Idéalement, les molécules non solubles ne devraient être ni testées ni même synthétisées afin d\u0027accélérer le processus.\nLa solubilité d\u0027une molécule est représentée par un attribut numérique nommé indice de solubilité. Les laboratoires pharmacologiques connaissent cette valeur pour un grand nombre de molécules. Ceci motive l\u0027utilisation de méthodes issues de la fouille de données pour induire un modèle qui, à partir de la structure d\u0027une molécule, prédit son indice de solubilité.\nDans le cadre de cette application, une base de données permet de décrire les molécules à partir de trois tables : - La table molécule contient les caractéristiques globales de la molécule, réduites,  Dans cet article, nous commençons par proposer une méthode de construction automatique d\u0027attributs. Nous détaillerons ensuite notre algorithme de modèle d\u0027arbre couplé à du bagging qui, à partir des attributs précédemment créés, permet d\u0027induire un modèle de régression. Enfin, nous étudions les performances de notre approche, en démontrant l\u0027apport de chacune des techniques utilisées et en comparant nos modèles induits avec les meilleurs de la littérature sur ce sujet.\nConstruction automatique d\u0027attributs\nL\u0027enrichissement manuel des données demandant du temps, des compétences et des logiciels couteux, nous proposons donc une technique pour construire automatiquement des attributs à partir de la structure des molécules.\nJusqu\u0027à récemment, la construction des nouveaux attributs à partir de données relationnelles se basait soit sur la sélection (Kramer et al., 2001) qui construit des attributs booléens (par exemple est-ce que la molécule a une liaison carbone-oxygène ?), soit sur l\u0027agrégation (Perlich et Provost, 2003) qui contruit des données nominales ou numériques (par exemple le nombre d\u0027atomes de la molécule). Seule l\u0027union de l\u0027agrégation et de la sélection permet d\u0027exprimer des attributs comme : nombre de liaisons carbone-oxygene. Vens et al. (2003) sont les seuls, à notre connaissance, qui combinent l\u0027aggrégation et la sélection. Ils utilisent des forêts aléatoires pour effectuer une sélection dans l\u0027espace des attributs constructibles.\nNous nous proposons d\u0027utiliser les graphes de sélections afin de définir des attributs constructibles. Les graphes de sélection introduits par (Knobbe et al., 1999) permettent d\u0027exprimer graphiquement des motifs sur des données multi-relationnelles. Un motif est un ensemble de caractéristiques de structure qui peuvent être, ou non, vérifiées par une molécule. Un exemple est donné à la figure 1 représentant une liaison (de type quelconque) entre un atome de carbone et un atome d\u0027oxygène.\nFIG. 1 -Exemple de graphe de sélection à gauche avec son interprétation comme motif chimique à droite.\nAvant de créer les nouveaux attributs, nous cherchons les motifs dans les données. Pour ce faire, nous construisons les graphes de sélection suivant une grammaire simple. Les graphes de sélection sont composés d\u0027un noeud molécule, suivi d\u0027une alternance de noeuds atome et liaison. Au niveau des motifs cela permet d\u0027exprimer des séquences d\u0027atomes. Pour des questions de calculabilité, nous nous limitons à des séquences d\u0027au plus 2 atomes. En effet, nos tests ont montré que des séquences plus longues (jusqu\u0027à 4 atomes) n\u0027améliorent pas le résultat.\nL\u0027algorithme considère l\u0027ensemble des graphes de sélection constructibles pour les données, et pour chacun construit un nouvel attribut numérique. La valeur des attributs construits sera, pour chaque exemple, le nombre d\u0027occurrences du graphe de sélection.\nL\u0027algorithme construit également des attributs plus complexes. Pour ce faire, nous procé-dons comme précédemment, mais au lieu de se placer au niveau de la molécule, nous nous plaçons au niveau des atomes et des liaisons en ajoutant, par exemple, pour chaque atome, un attribut représentant le nombre de ses liaisons avec d\u0027autres atomes. Il faut néanmoins noter que ces attributs ne sont pas des descriptions en soit de la molécule mais uniquement des composants de la molécule. Pour répercuter l\u0027information au niveau de la molécule, nous utilisons les opérateurs classiques d\u0027agrégation suivants : moyenne, somme, minimum, maximum. Ainsi, on peut ajouter à la description de la molécule le nombre moyen de liaisons de chacun de ses atomes par exemple.\nAlgorithme d\u0027induction de modèles de régression\nLe problème de prédiction de l\u0027indice de solubilité est généralement résolu en utilisant soit un réseau de neurones multicouches (Tetko et al., 2001;Huuskonen, 2000) soit une régression linéaire multiple (Hou et al., 2004;Delaney, 2004).\nDans le domaine de la fouille de données, d\u0027autres algorithmes ont été développés et donnent de bons résultats, nous nous intéressons particulièrement aux algorithmes à base d\u0027arbres.\n-671 -RNTI-E-6 Dans l\u0027approche proposée, l\u0027algorithme induit un arbre de modèles où les feuilles et les noeuds internes jouent des rôles complémentaires. Une feuille effectue la tâche de régression proprement dite. A partir d\u0027un ensemble d\u0027exemples, la feuille effectue une régression linéaire multiple avec les attributs autorisés par ses noeuds ascendants. Une feuille racine n\u0027a accès à aucun attribut. Un noeud interne, appelé un raffinement, a pour but de simplifier la tâche de sa descendance. Pour cela, un noeud peut soit partitionner l\u0027espace des exemples (cas typique des arbres de décision) selon un seuil sur un des attributs numériques construits, soit proposer un nouvel attribut qui sera accessible à sa descendance. Le nombre de fils d\u0027un noeud dépend du type de raffinement effectué, un raffinement partitionnant l\u0027espace des exemples selon une condition booléenne aura deux fils, tandis qu\u0027un noeud introduisant un nouvel attribut n\u0027aura qu\u0027un fils. La construction de l\u0027arbre se déroule de la façon suivante. L\u0027algorithme débute en créant une feuille initiale. Comme elle n\u0027a accès initialement à aucune variable numérique, la régression linéaire se réduit à une constante. Ensuite cette feuille est raffinée, l\u0027algorithme teste tous les raffinements possibles, c\u0027est-à-dire tous les ajouts d\u0027attributs et tous les partitionnement possibles. Enfin le raffinement améliorant au mieux l\u0027erreur du modèle est conservé.\nL\u0027arbre ainsi induit est bien souvent sujet au sur-apprentissage. Dans le cas des arbres, une façon de palier le sur-apprentissage est de procéder à un élagage de l\u0027arbre obtenu. Nous utilisons l\u0027élagage par réduction d\u0027erreur définie par Quinlan (1987) avec un jeu de validation représentant un tiers du jeu d\u0027apprentissage.\nEnfin nous utilisons la technique du bagging (Breiman, 1996) en induisant 50 modèles avec chacun une partition jeu d\u0027apprentissage et jeu de validation différent, ce qui donne une indépendance aux modèles induits. La prédiction finale est la moyenne des prédictions des 50 modèles.\nRésultats\nLes jeux de données utilisées dans nos tests correspondent à des ensembles de molécules de différents types dont la solubilité est connue. Ces données nous ont été fournies par le laboratoire d\u0027Infochimie ULP/CNRS UMR 7551. Dans tous les cas les résultats sont obtenus à l\u0027issue d\u0027une validation croisée en 10 partitions.\nLes premiers test ont été réalisés sur un jeu de 511 molécules. Les résultats obtenus sont présentés dans la table 1. On peut remarquer que l\u0027utilisation conjointe du bagging et des arbres de modèles permet de réduire sensiblement l\u0027erreur. Le bagging donne de meilleurs résultats sur les arbres de modèles que sur les régressions linéaires multiples.\nNous avons également comparé nos résultats à ceux obtenus dans le milieu de l\u0027infochimie. Pour ce faire, nous avons utilisé un jeu d\u0027apprentissage de 1635 molécules et le jeu de Yalokowsky (Yalkowsky et Banerjee, 1991)  \nConclusion\nDans cet article nous avons proposé une nouvelle méthode d\u0027induction de modèles de pré-diction de la solubilité des molécules. Cette méthode se base sur l\u0027utilisation de techniques nouvelles dans ce domaine d\u0027application, comme les arbres de modèles et le bagging. Nous proposons également une méthode pour utiliser les données multi-relationnelles brutes, sans l\u0027ajout d\u0027attributs experts, par construction automatique d\u0027attributs numériques en utilisant des opé-rateurs d\u0027agrégation. Nos résultats sont proches de ceux obtenus par les meilleurs approches développées jusqu\u0027à présent, approches utilisant des attributs experts.\nPlusieurs voies d\u0027évolution sont possibles. La première consisterait à augmenter la capacité de construction d\u0027attributs en ne la limitant plus à des motifs simples. Cela peut passer par l\u0027utilisation d\u0027éléments de plus haut niveau dans les motifs, comme l\u0027utilisation des cycles aromatiques (plusieurs atomes de carbone formant un cercle). Une seconde voie consiste en l\u0027extension des arbres de modèles qui, plutôt que de se limiter à des régressions linéaires multiples, utiliseraient, quand c\u0027est indiqué, des réseaux de neurones de topologie simple.\nRéférences Appice, A., M. Ceci, et D. Malerba (2003). Mining model trees : A multi-relational approach.\nIn ILP, Volume 2835 of Lecture Notes in Computer Science, pp. 4-21. Breiman, L. (1996). Bagging predictors. Machine Learning 24(2), 123-140.\n"
  },
  {
    "id": "1064",
    "text": "Introduction\nDepuis plusieurs années, des travaux de recherche importants sont déployés pour permettre aux radars de réaliser des tâches liées à l\u0027intelligence artificielle, telle que la reconnaissance des cibles. Pour l\u0027homme, l\u0027acquisition des images et l\u0027identification de cibles s\u0027effectuent par l\u0027intermédiaire du système visuel. L\u0027oeil humain peut être défini comme un capteur qui va transmettre ses données au cerveau de façon à traiter les informations et prendre une décision. Le système de perception visuel humain « oeil-cerveau » est régi par des mécanismes très complexes, qui ont toutefois des limitations. Par exemple, l\u0027oeil n\u0027est sensible qu\u0027à certaines longueurs d\u0027ondes, son spectre de visibilité est limité et sa sensibilité diminue avec l\u0027obscurité. Pour résoudre ces problèmes, des systèmes d\u0027aide à la décision ont été développés. Ils sont capables de percevoir l\u0027environnement au-delà du système sensoriel et de réaliser des étapes de perception de plus en plus fines. Pour répondre à ce besoin, dans le domaine militaire, différentes technologies ont été mises au point par l\u0027intermédiaire de capteurs spécifiques tel que le radar. En contrepartie les quantités d\u0027informations à gérer sont devenues gigantesques et délicates voire impossible à traiter rapidement pour prendre une décision. Ainsi que la sensibilité des signaux radar aux conditions opérationnelles, sujettes aux perturbations environnementales et aux conditions de mesure, exige une prise en compte. Le problème traité dans ce papier s\u0027insère dans le cadre général de l\u0027identification noncoopérative d\u0027une cible aérienne à partir de la rétrodiffusion d\u0027un signal radar multifréquentiel (Toumi et al., 2005) ( Hoeltzener et al., 2003).\nCadre méthodologique et processus ECD\nLes aspects académiques concernent davantage le problème de l\u0027extraction de primitives les mieux adaptées aux données radar et la prise en compte des imperfections avant de passer à l\u0027étape de fouille de données (Frawley et al., 1991). Se rajoute à cela, l\u0027idée d\u0027un fonctionnement en mode supervisé pour la validation des informations candidates. L\u0027intervention de l\u0027opérateur humain s\u0027est montrée utile notamment, dans le cadre des expérimentations et simulations des données radar (cf. section suivante)\nBase de données en expérimentation radar\nL\u0027utilisation conjointe des données synthétiques et réelles est une pratique courante pour la validation des différentes méthodes de reconnaissance. C\u0027est dans ce contexte que nous avons eu recours à des données expérimentales acquises en laboratoire dans une chambre anéchoïde1 en utilisant des maquettes à l\u0027échelle 1/48 ème modélisant l\u0027interaction radar cible. En outre, le volume de la base de données augmente très rapidement avec le nombre de cibles pour un état donné (configuration) du système d\u0027acquisition, la première base de données renseignée a été réalisée à partir de simulations fines du système d\u0027acquisition produisant l\u0027ensemble de données 1D et 2D. La base des données 2D contient plus de 2430 images pour 15 maquettes et par polarisation (4 polarisations au total).\nDans ce papier, nous traitons que les données images (images ISAR) (Kok, 1998) reconstruites à partir des signature à haute résolution à une dimension (profils distance) par la transformée de Fourier.\nOrientation vers le calcul de descripteurs de forme\nPour la phase de préparation des images ISAR, nous aurons recours aux techniques essentiellement issues de recherches effectuées en reconnaissance de forme. Plusieurs états de l\u0027art on été présentés dans (Rui et al., 1999). Généralement les primitives visuelles sont regroupées en trois classes (Mezhoud et al., 2000) : les descripteurs liés à la couleur (histogramme), les descripteurs de texture (matrice de cooccurrence, indice de direction principale et de rugosité, filtre de Gabor et ondelettes) et les descripteurs de formes (descripteur de Fourier et des moments, points caractéristiques, etc.).\nDans ce contexte de travail, nous nous intéressons à trouver une présentation pertinente des images ISAR via des primitives visuelles significatives et fiables. Nous traiterons la modélisation et l\u0027indexation logique des images ISAR avec la prise en compte de l\u0027imprécision liée au système de mesure (section 4).\nDans notre approche, le premier traitement à effectuer est la segmentation des images ISAR en niveau de gris. Il est difficile de définir, de manière absolue, une bonne « segmentation ». Sa qualité est en partie fonction des résultats obtenus par les traitements situés en aval qui utilisent les primitives extraites.\nPour le cas des images ISAR à traiter, les techniques de détection de contour fondées sur des techniques dérivatives ne fournissent que des ensembles de contours non fermés révélant un certain nombre de disparités localisées au sein même de la signature de la cible. Leur traitement à ce titre entraînerait une complication pour la reconstruction d\u0027une forme plus générale de la cible. Afin d\u0027atteindre l\u0027objectif, le choix s\u0027est porté sur des opérateurs de morphologie mathématiques (Beucher et Meyer, 1993), la technique retenue est celles des lignes de partage des eaux (LPE).\nSegmentation des images ISAR par LPE\nLa segmentation morphologique consiste à appliquer la LPE sur l\u0027image gradient, dérivée de l\u0027image originale. La LPE est une technique très importante parmi les techniques de segmentation. Elle utilise la terminologie de la géographie qui définit la LPE comme la crête qui forme la limite entre deux bassins versants dans une image considérée comme une surface 3D. Le choix de telle méthode est sollicité par le fait que les points brillants sont caractérisés par des pics assez importants des autres régions reflétant le signal émis par un radar.\nL\u0027application directe de la technique LPE sur le gradient des images ISAR, donne des images sursegmentées. Pour dénouer ce problème, nous avons procédé au renforcement les variations de niveaux de gris en passant par l\u0027image simplifiée. Par conséquent, l\u0027image initiale va être transformée en image mosaïque (Beucher et Meyer, 1993). L\u0027image mosaïque peut s\u0027interpréter comme un graphe sur lequel sont évalués des arcs. Le graphe valué de l\u0027image mosaïque est appelée gradient mosaïque. Ce gradient est la fonction h définie sur tous les arcs de l\u0027image et ses niveaux de gris correspondent à la différence de valeur existant entres deux composantes connexes. En éliminant les premiers niveaux, nous arrivons à garder l\u0027information nécessaire révélant la forme générale de la cible (cf. figure 1).\nUne fois l\u0027image segmentée, on a besoins d\u0027extraire les informations du contour pour procéder à la reconnaissance des formes. C\u0027est pourquoi, elle est généralement représentée dans un formalisme par des descripteurs de forme (cf. Section 3.1.1). \nFIG. 1 -LPE du gradient mosaïque.\nFormes et descripteurs\nLe problème fondamental dans la reconnaissance de forme est de déterminer dans quelle mesure deux formes sont similaires, indépendamment de leur position dans l\u0027image. Il en découle que les descripteurs de forme doivent être précis, compacts et invariants à un certain nombre de transformations géométriques (translation, rotation, changement d\u0027échelle et du point origine de la forme). Nous trouvons une étude plus détaillée des descripteurs de forme utilisant des méthodes standards telles que Fourier (Mezhoud et al., 2000) ou Fourier Mellin (Teoch et al., 2004) qui sont largement utilisés dans les systèmes de recherche actuels. Nous proposons ici, de modéliser la forme d\u0027une cible par les descripteurs de Fourier.\nOn considère f(x) comme étant une fonction périodique continue différentiable définie sur [0,2?] qui pourra être une des fonctions de contours de la forme à étudier (en forme complexe). Une telle fonction peut être approximée par une série de Fourier C(k) où les coefficients de Fourier dépendent généralement de la forme de la cible : les coefficients tendent à décrire les caractéristiques globales d\u0027une image pour des k petits et décrivent beaucoup plus finement ces formes pour des k plus grands. Enfin, seule une vingtaine de descripteurs de Fourier (conservation de l\u0027énergie par l\u0027égalité de Parseval) est gardée pour représenter la forme d\u0027une façon compacte et précise pour des mises en correspondance lors du processus de recherche/reconnaissance.\nCependant, un grand défi réside dans le fait que les connaissances dont nous disposons sont imparfaites. La prise en compte de ces imperfections dans l\u0027étape d\u0027acquisition et préparation des données (en référence au processus ECD) doit conduire à tenir compte de la variabilité/hétérogénéité des primitives extraites pour différentes expérimentations.\nPrise en compte de l\u0027imperfection\nVu que les données acquises sont fortement liées au scénario d\u0027acquisition, les données sont souvent dépendantes des paramètres environnementaux (le taux d\u0027humidité, la chaleur, la présence d\u0027autres objets, etc). La manière la plus commune de caractériser l\u0027imperfection attachée à une mesure consiste alors à répéter cette mesure dans les mêmes conditions expérimentales (même expérimentateur, même matériel, même protocole). Le résultat de la mesure varie généralement d\u0027une expérience à une autre. Ce phénomène peut malheureusement engendrer des imperfections au niveau de l\u0027étape de préparation de données, telle que, l\u0027obtention pour différentes expérimentations, de différentes formes extraites des images pour une même cible. Par conséquent, nous avons mis en place une démarche pour la prise en compte de cette imperfection en introduisant la notion de qualité de données.\nMéthodologie\nLa gestion d\u0027imperfection s\u0027appuie donc sur un modèle de référence qui est considéré le plus précis et le plus complet pour un paramétrage donné du système d\u0027acquisition. Ce modèle est caractérisé par des connaissances jugées suffisantes et extraites d\u0027un ensemble de données simulées, mais aussi par un ensemble de règles établies par l\u0027expert du domaine radar. Ces règles sont exprimées sous la forme de règles floues (Dubois et Prade, 1992)  (1) Si écart est important alors la qualité est mauvaise (2) Si la bande est large alors la résolution est fine\nNous cherchons à partir de ces règles à attribuer une qualité aux données en vue de la sélection de tel ou tel traitement ultérieur, voire d\u0027aider à leur paramétrage, et ceci avant la phase de fouille de données.\nPour ce qui est de la règle (1) et de l\u0027estimation de l\u0027écart, nous évaluons le signal réfléchi durant l\u0027acquisition des données, par rapport à un ensemble de propriétés : Spectre, distribution de probabilité du signal, etc. Sachant que cette évaluation se fait au niveau pour chaque réponse impulsionelle de chaque angle de visée du radar, une agrégation est réalisée à partir des différentes réponses qui participent à la reconstruction d\u0027une image (exemple : on sélectionne un domaine angulaire de 20° pour reconstruire une image ISAR). C\u0027est ainsi qu\u0027un écart noté Ecart est calculé entre une donnée de référence D réf et la données acquise D acq pour la propriété P i : \nConclusion\nDans ce papier nous avons présenté les travaux concernant la phase de préparation de données du processus ECD appliqué en expérimentation radar (reconstruction des images ISAR et modélisation de la forme par les descripteurs de Fourier après extraction par LPE). Notre démarche, motivée par la présence des imperfections dans les données et qui sont liées à leur sensibilité aux conditions de mesure et aux perturbations environnementales, tire profit des fonctions d\u0027évaluation floues. Ceci permet de prendre en compte l\u0027imperfection du système de mesure dans la préparation des données.\nCes travaux conduisent aussi à catégoriser les paramètres de la segmentation et de l\u0027extraction de la forme pour chacune des qualités attribuée à une image. Il est en ce cas possible de prévoir une automatisation du système.\n"
  },
  {
    "id": "1065",
    "text": "Introduction\nNous nous intéressons au problème de prétraitement de grands ensembles de données. Notre but est de réduire les informations contenues dans les ensembles de données volumineux aux informations les plus significatives. Il existe des techniques expérimentalement validées pour ce faire. D\u0027un point de vue applicatif, un problème majeur se pose quant au choix d\u0027une de ses méthodes. Une solution qui constitue notre contribution dans ce travail serait d\u0027utiliser une combinaison de techniques ou de stratégies. A cet effet, nous nous appuyons sur la théorie du consensus. L\u0027utilisation de cette combinaison de stratégies ou d\u0027expertises peut être justifiée par l\u0027un des faits suivants : -il n\u0027est pas possible de déterminer a priori quelle méthode de sélection de sous-ensemble d\u0027attributs est meilleure que toutes les autres (en tenant compte des différences entre le temps d\u0027exécution et la complexité), -un sous-ensemble optimal d\u0027attributs n\u0027est pas nécessairement unique, -la décision d\u0027un comité d\u0027experts est généralement meilleure que la décision d\u0027un seul expert. Les résultats obtenus après des expérimentations permettent de conclure que l\u0027approche proposée réduit de façon significative l\u0027ensemble de données à traiter et permet de les traiter interactivement. Cette contribution commence par un état de l\u0027art et la problématique du sujet abordé, puis, l\u0027algorithme de sélection d\u0027attributs est présenté. Enfin, nous procédons à des expérimentations avant la conclusion.\nEtat de l\u0027art et problématique\nNous essayons de résoudre le problème suivant : comment sélectionner des attributs d\u0027un ensemble de données pourvu de plusieurs attributs et rejeter les autres sans nuire à la qualité de l\u0027algorithme de fouille visuelle utilisé ensuite ? Ceci tout en sachant que : -la visualisation de plus de deux dizaines d\u0027attributs rend souvent inutilisable la fouille visuelle de données, -un sous-ensemble optimal d\u0027attributs n\u0027est pas nécessairement unique, -il n\u0027est pas possible de déterminer a priori quelle méthode de sélection de sous-ensemble d\u0027attributs est meilleure que toute les autres, -la décision d\u0027un comité d\u0027experts est généralement meilleure que la décision d\u0027un seul expert. Des techniques performantes (John et al., 1994), (Kira et Rendell, 1992), etc. de sélection de sous-ensembles d\u0027attributs ont été développées mais, il n\u0027existe pas une méthode qui soit meilleure que toutes les autres dans tous les cas.\nNous avons défini un nouvel algorithme de sélection d\u0027attributs qui combine des déci-sions pondérées de plusieurs experts. Plus précisément, étant donné deux ou plusieurs mé-thodes de sélection de sous-ensembles pertinents d\u0027attributs dans un ensemble de données, la question est de savoir comment l\u0027on peut utiliser ces différentes méthodes pour fournir un résultat efficace. Afin de répondre à cette question, nous nous sommes appuyés sur la théorie du consensus qui peut être définie comme un procédé de prise de décision qui utilise entiè-rement les ressources d\u0027un groupe. La théorie du consensus trouve l\u0027une de ses justifications dans le fait qu\u0027une décision prise par un groupe d\u0027experts est meilleure en terme d\u0027erreur quadratique moyenne que la décision d\u0027un seul expert. Une telle démarche possède de nombreux avantages : statistiquement parlant, la consultation de plusieurs expertises lors de la résolution d\u0027un problème est une façon subjective d\u0027accroître la taille de l\u0027échantillon dans une expérience, un ensemble d\u0027experts permet d\u0027obtenir plus d\u0027information qu\u0027un seul expert (Clemen et Winkler, 1999).\nLa section suivante présente l\u0027algorithme de sélection d\u0027attributs proposé.\nautres. Chaque critère possède des attributs de qualité spécifiques. Il est nécessaire de prendre en considération tous les différents attributs de qualité.\nNous avons aussi un sous-ensemble d\u0027attributs\nChaque attribut sélectionné par un sous expert e j a une fréquence d\u0027apparition freq \u003d 1/nb dans la décision finale, où nb est le nombre d\u0027attributs sélectionnés par le sous expert. Nous définissons un critère de préférence d\u0027un attribut (règle de consensus) comme étant le produit des fréquences d\u0027apparition de l\u0027attribut dans les sous-ensembles d\u0027attributs des experts. Nous utilisons la formule ci-dessous pour le calcul de la préférence d\u0027un attribut d. Les poids affectés aux experts doivent à cet effet être proportionnels à leurs décisions. La méthode d\u0027affectation de poids que nous proposons a pour fondements théoriques un principe de la théorie de Gestalt (une vue d\u0027ensemble est meilleure que la somme des parties) et des propriétés pré-attentives de la vision humaine. En ce qui concerne le principe de Gestalt, en visualisant l\u0027ensemble d\u0027éléments intervenant dans une décision, un processus cognitif se met en place. L\u0027application du principe de Gestalt se résume en une représentation graphique multi vues à base de coordonnées parallèles (Inselberg, 1985) des attributs sélectionnés. Les coordonnées parallèles permettent de représenter en 2D des données multidimensionnelles sans perte d\u0027information. Chaque vue représente le point de vue de chaque expert (une des données d\u0027entrée de CTBFS), comme l\u0027indique la figure 1.\nFIG. 1 -Outil d\u0027affectation visuelle de poids aux experts intervenant dans CTBFS.\nSix experts de type filtre ont servi à la sélection des attributs visualisés dans la figure 1. L\u0027expert 1 représente le critère de sélection consistance, l\u0027expert 2 représente l\u0027entropie de Shannon, l\u0027expert 3 quant à lui utilise la distance comme fonction d\u0027évaluation. La fonction d\u0027évaluation pour l\u0027expert 4 est le gain d\u0027information, le coefficient de Gini pour l\u0027expert 5 et le coefficient de Cramer pour l\u0027expert 6.\nIl est à noter que les outils usuels d\u0027affectation de poids sont des « boîtes noires ». L\u0027avantage principal de l\u0027approche ainsi proposée tient au fait que l\u0027utilisateur est impliqué et participe dans le processus de prise de décision. Il existe un ensemble de propriétés visuelles qui sont traitées de manière pré attentive très rapidement, avec précision et sans effort particulier, ce qui permet aux utilisateurs d\u0027affecter des poids convenables aux différents experts.\nDe plus, les techniques de visualisation permettent d\u0027améliorer la résolution de problè-mes. La visualisation permet de découvrir plus aisément des motifs dans les données, de réduire l\u0027espace de recherche d\u0027information par rapport aux méthodes automatiques, de procéder à des opérations perceptuelles d\u0027inférence et d\u0027augmenter la mémoire et les ressources de traitement de l\u0027utilisateur (Card et al., 1999).\nExpérimentations\nPour les besoins d\u0027expérimentation de la technique proposée qui a été développée sous Windows avec Java et le langage R, nous utilisons un pentium IV, 1.7 GHz. Les ensembles de données que nous utilisons ont été référencées par (Blake et Merz, 1998) et (Jinyan et Huiqing, 2002). Pour les besoins de ces expérimentations, les poids affectés aux différents experts ont pour valeur 1.\nLe domaine considéré dans le cadre de cette première expérimentation est constitué d\u0027un ensemble M constitué de 3 experts de type filtre et de 3 experts de type enveloppe E \u003d {consistence, entropie de Shannon, distance, (LDA, QDA, Kppv)  (Ripley, 1996)}, le nombre d\u0027attributs susceptibles d\u0027être traités convenablement est C cmd \u003d 20.\nLes résultats de l\u0027algorithme proposé (CTBFS) sont comparés à ceux de Las Vegas Filter (Liu et Setiono, 1996), un algorithme de type filtre et StepClass du package KlaR (langage de programmation R), un algorithme de type enveloppe. A cet effet, nous évaluons les performances des ensembles de données pourvus des attributs sélectionnés par ces trois métho-des avec l\u0027algorithme des k plus proches voisins kppv (implémentation de WEKA (Witten et Eibe, 2005)). Nous avons fixé le paramètre K de l\u0027algorithme des kppv à 1.\nLes ensembles de données à traiter dans le cadre de cette première expérimentation sont pourvus de nombreux attributs (colonne 2 du tableau 1) qu\u0027il serait impossible de visualiser en une seule fois à l\u0027écran quelque soit la méthode de représentation graphique.\nLes résultats exposés dans le tableau 1 permettent d\u0027observer que l\u0027algorithme CTBFS permet de réduire considérablement le nombre d\u0027attributs des ensembles de données comme le montre les résultats de la colonne 3. La colonne 5 de ce tableau quant à elle fait observer que la précision de l\u0027algorithme de kppv est améliorée pour 4 ensembles de données sur 7. Pour les trois autres ensembles de données, on assiste certes à une perte de précision avec un écart maximal de 16.97% avec un minimum de précision de 68.87% mais l\u0027ensemble de données final peut être visualisé et traité de manière interactive, ce qui n\u0027est pas le cas des ensembles de données initiaux comme nous l\u0027avons souligné.\n-62 -RNTI-E-6 On observe à travers la colonne 3 du tableau 2 que la méthode LVF permet de sélection-ner un nombre très important d\u0027attributs, qu\u0027il serait impossible de visualiser (par exemple pour les ensembles de données Arrhythmia, Isolet, ColonTumor et CentralNervSyst). Par rapport à la méthode proposée, la précision obtenue pour ces ensembles de données est équi-valente voire supérieure par exemple pour l\u0027ensemble de données Isolet, sachant que l\u0027algorithme CTBFS renvoie au maximum 20 attributs. En ce qui concerne l\u0027algorithme Stepclass, l\u0027ensemble de données Promoter possède aussi un nombre important d\u0027attributs.\nEn terme de précision, en dehors de l\u0027ensemble de données Promoter pour lequel CTBFS a une précision inférieure à celle de Stepclass et de LVF, la précision obtenue pour les autres ensembles de données avec l\u0027algorithme proposé est au moins égale suivant les cas à celle de LVF ou à celle de Stepclass mais avec un nombre d\u0027attributs qui convient à la fouille visuelle de données.\nConclusion\nNous avons présenté un algorithme basé sur la théorie du consensus et l\u0027affectation visuelle de poids pour la sélection d\u0027attributs significatifs en FVD. En effet, lorsque le nombre d\u0027attributs et/ou le nombre d\u0027observations d\u0027un ensemble de données est important, il s\u0027avère impossible ou alors pénible de représenter graphiquement l\u0027ensemble de données et d\u0027observer des corrélations dans cet ensemble de données.\nLa technique présentée permet de définir un nombre maximum d\u0027attributs à sélectionner dans l\u0027ensemble de données à traiter, rendant possible la visualisation de ces données. La\n"
  },
  {
    "id": "1066",
    "text": "1\nFrançois Jacquenet, Christine Largeron, Cédric Udréa Laboratoire EURISE -Université Jean Monnet 23 rue du Docteur Michelon -42023 Saint-Etienne Cedex 2 -France {Francois.Jacquenet,Christine.Largeron,Cedric.Udrea}@univ-st-etienne.fr Deux voies sont envisageables pour limiter le nombre de motifs extraits dans un processus de fouille de données. La première s\u0027efforce, lors de la génération des motifs, de ne conserver que les seuls motifs semblant présenter un intérêt immédiat pour l\u0027utilisateur (Boulicaut, 2005), tandis que la seconde voie consiste à stocker tous les motifs extraits par les algorithmes de fouille de données dans des structures de données efficaces et à développer des outils d\u0027interrogation et de manipulation permettant de les traiter (Grossman et al., 1999;Tuzhilin et Liu, 2002;Zaki et al., 2005). C\u0027est en suivant cette démarche que nous nous sommes intéressés à la recherche de règles d\u0027association non redondantes alors que la plupart des travaux antérieurs consacrés à ce problème se sont plutôt attachés à l\u0027extraction de règles non redondantes directement à partir des données (Zaki, 2000;Bastide et al., 2000;Li et Hamilton, 2004;Goethals et al., 2005).\nDans la suite, en nous inspirant d\u0027une définition de (Bastide et al., 2000), nous considérons qu\u0027une règle d\u0027association B ? H est non redondante si et seulement si il n\u0027existe pas de règle de la forme B ? H telle que B ? B et H ? H . Chaque partie de la règle d\u0027association peut être représentée par un vecteur qui possède autant de bits qu\u0027il existe d\u0027items dans la base de transactions (Morzy et Zakrzewicz, 1998). Chaque bit est alors associé à un item particulier et la valeur du bit est de \u00271\u0027 si et seulement si l\u0027item correspondant est présent dans la partie de la règle associée au vecteur de bits.\nEn utilisant ce codage, nous proposons de déterminer la redondance d\u0027une règle R \u003d B ? H vis-à-vis d\u0027une autre règle R \u003d B ? H , en exploitant la propriété suivante :\nk }) le vecteur de bits correspondant à la partie gauche (respectivement droite) de la règle X où IB X i (respectivement IH X i ) est égal à 1 si l\u0027item i est présent dans la partie gauche (respectivement droite) de la règle X, 0 sinon. Nous démontrons alors que la règle R est redondante par rapport à la règle R si et seulement si\ndé-signe le nombre de \u00271\u0027 dans IB X , N h (X) le nombre de \u00271\u0027 dans IH X et (R AND R ) désigne la règle ayant en partie gauche l\u0027intersection des parties gauches des règles R et R et en partie droite l\u0027intersection des parties droites des règles R et R . Nous avons développé un algorithme, basé sur cette propriété, et réalisé plusieurs tests pour comparer les temps nécessaires pour extraire les règles non redondantes d\u0027un ensemble \n"
  },
  {
    "id": "1067",
    "text": "Introduction\nÉtant donné l\u0027explosion du volume de données disponibles sur Internet, il devient indispensable de proposer de nouvelles approches pour faciliter l\u0027interrogation de ces grandes masses d\u0027information afin de retrouver les informations souhaitées. L\u0027une des conditions sine qua non pour permettre d\u0027interroger des données hétérogènes est de disposer d\u0027un (ou de plusieurs) \"schéma général\" que l\u0027utilisateur pourra interroger et à partir duquel les données sources pourront être directement accédées. Malheureusement les utilisateurs ne disposent pas de moyen de connaître les modèles sous-jacents des données qu\u0027ils souhaitent accéder et l\u0027un des challenges dans ce contexte est donc de fournir des outils pour extraire, de manière automatique, ces sché-mas médiateurs. Un schéma médiateur est alors considéré comme une interface permettant à l\u0027utilisateur l\u0027interrogation des sources de données : l\u0027utilisateur pose ses requêtes de manière transparente et n\u0027a pas à tenir compte de l\u0027hétérogénéité et de la répartition des données.\nXML étant maintenant prépondérant sur Internet, la recherche de moyens d\u0027intégration de tels schémas est un domaine de recherche actif. Si les recherches permettant l\u0027accès aux données, quand un schéma d\u0027interrogation est connu, sont maintenant bien avancées (Xylème, 2001), les recherches concernant la définition automatique d\u0027un schéma médiateur restent incomplètes et non satisfaisantes (Tranier et al., 2004). Il est alors intéressant de considérer les travaux réalisés dans le contexte de la fouille de données afin d\u0027obtenir un schéma fréquent ou un ensemble de sous-schémas fréquents. Ces derniers offrent alors des éléments pertinents pour la construction du schéma médiateur. Dans le but de proposer une approche permettant de ré-pondre à cette dernière problématique, nous nous focalisons sur la recherche de sous-structures fréquentes au sein d\u0027une base de données de schémas XML. Une sous-structure fréquente est un sous-arbre se trouvant dans \"la plupart\" des schémas XML considérés. Cette proportion est examinée au sens d\u0027un support qui correspond à un nombre minimal d\u0027arbres de la base dans lesquels le sous-arbre doit se retrouver pour être considéré comme fréquent. Une telle recherche est complexe dans la mesure où il est nécessaire de traduire l\u0027ensemble des sché-mas en une structure aisément manipulable. Cette transformation des données conduit parfois à doubler ou tripler la taille de la base initiale dès lors que l\u0027on souhaite utiliser des propriétés spécifiques permettant d\u0027améliorer le processus de fouille. Il n\u0027existe pas de solution efficace à ce problème alliant une représentation compacte à des propriétés intéressantes. L\u0027objet de cet article est la définition d\u0027une approche de fouille de données de type XML répondant à cet objectif.\nCet article est structuré de la manière suivante : la section 2 introduit les définitions des différentes inclusions dans le contexte des structures hiérarchiques et propose un aperçu des principales approches existantes de fouille de données arborescente. Nous présentons égale-ment en détail la problématique étudiée. La section 3 présente notre proposition : une méthode de recherche de sous-schémas fréquents utilisant les propriétés d\u0027une structure de données arborescentes compacte et originale. Les différentes expérimentations menées sur des bases de schémas XML sont décrites dans la section 4. Enfin, la section 5 conclut et présente les principales perspectives associées à nos travaux.\nDéfinitions, problématique et travaux connexes\nDéfinitions préliminaires\nUn arbre est un graphe orienté, connexe sans cycle. Il est composé d\u0027un ensemble de noeuds reliés par des arcs et il existe un noeud particulier nommé racine. Il s\u0027agit d\u0027un arbre ordonné s\u0027il existe un ordre entre les fils d\u0027un noeud et d\u0027un arbre non ordonné sinon. \nDéfinition 1 Un arbre enraciné, étiqueté et ordonné\nDe plus, dans la suite de cet article, nous utilisons le mot arbre pour un un arbre enraciné, étiqueté et ordonné. \n3. ? préserve les relations :\nUne inclusion est dite induite si les relations de parenté sont préservées. Par ailleurs, si les relations d\u0027ancestralité sont respectées, il s\u0027agit d\u0027une inclusion incrustée. Par exemple, considérons les arbres S, T 1 , T 2 , et T 3 représentés dans la figure 1. Si les relations de parenté sont respectées, il s\u0027agit d\u0027une inclusion induite, donc S est inclus de manière induite dans l\u0027arbre T 1 (S T 1 ). Si les relations d\u0027ancestralité sont conservées, alors on trouve une inclusion incrustée avec S T 1 et S T 3 . S n\u0027est pas inclu dans T 2 car ? ne préserve pas l\u0027ordre entre les frères.\nDans la suite de cet article, nous considérons une inclusion de type induite car nous souhaitons traiter l\u0027ordre existant entre les noeuds dans la hiérarchie de façon directe et l\u0027ordre entre les noeuds de même niveau de façon indirecte. Nous pouvons donc définir le support d\u0027un sous-arbre selon cette inclusion de la manière suivante :\nProblématique\nLa problématique étudiée au sein de cet article est la recherche de sous-structures fré-quentes, i.e. de sous arbres qui apparaissent suffisamment fréquemment dans des documents XML. Nous considérons, par la suite, qu\u0027une étape initiale de pré-traitement est réalisée sur les documents XML de manière à ne retenir que leur structure sous forme d\u0027arbre. Nous considérons également qu\u0027à l\u0027issue de cette phase, l\u0027étiquetage des noeuds est homogène, i.e deux noeuds de même étiquette dans deux arbres différents partagent non seulement la même syntaxe mais également la même sémantique.\nL\u0027objectif consiste alors à rechercher, à partir de la forêt d\u0027arbres obtenue D et en fonction d\u0027un support minimal spécifié par l\u0027utilisateur, les sous arbres qui apparaissent suffisamment fréquemment, i.e. dont leur nombre d\u0027occurrences dans D est supérieur ou égal au support minimal. Pour répondre à cette problématique, nous nous trouvons donc confrontés aux deux problèmes suivants :\n1. Quelle structure de représentation efficace utiliser ? Idéalement, étant donné que nous considérons de grandes quantités d\u0027arbres, nous souhaitons avoir une structure qui non seulement soit efficace en mémoire mais également adaptée aux traitements que nous souhaitons faire.\nComment tester efficacement l\u0027inclusion d\u0027un arbre dans un sous arbre ?\nRechercher l\u0027ensemble des sous arbres fréquents nécessite de parcourir tous les arbres et d\u0027effectuer de très nombreuses comparaisons pour réussir à extraire des sous parties communes. Il est donc indispensable de pouvoir trouver rapidement à partir de quel noeud la comparaison peut être effectuée si nous souhaitons améliorer l\u0027efficacité de la recherche.\nLes travaux existants\nDans cette partie, nous nous intéressons non seulement aux approches de recherche mais nous examinons également les méthodes de représentation des arbres. Les travaux dans le domaine de la fouille de données arborescentes peuvent être distingués selon qu\u0027ils traitent les arbres ordonnés ou non. Nous situant dans le contexte de schémas XML, il s\u0027avère nécessaire de traiter l\u0027ordre des éléments si celui-ci est spécifié. Nous nous focaliserons donc sur des propositions prenant en charge les arbres ordonnés.\nA notre connaissance, il existe très peu de travaux proposant des méthodes d\u0027extraction pour les arbres ordonnés (Zaki, 2002;Asai et al., 2002). Ainsi Zaki (2002) propose l\u0027algorithme TreeMiner pour extraire des sous-arbres fréquents selon une inclusion incrustée. Une représentation originale des arbres facilite la gestion des candidats et offre des performances intéressantes. (Asai et al., 2002) traitent également de la problématique des arbres ordonnés selon la définition de l\u0027inclusion induite. L\u0027approche proposée, FREQT, adopte une structure de représentation du type « first-child/next-sibling » comme illustrée figure 2. Lors du processus de fouille, pour chaque structure fréquente, FREQT conserve la liste des noeuds les plus à droite dans les arbres de la base de données supportant cette structure. Nous illustrons ceci figure 3 où pour le fréquent a, les 6 positions dans la base de données sont stockées, et pour le fréquent c ? a les 3 positions les plus à droite sont stockées. Cette information représente les positions où cette structure est supportée dans la base.\nSi nous examinons plus attentivement la représentation verticale adoptée dans TreeMiner, elle aboutit en fait à stocker trois fois la taille d\u0027un arbre, i.e. 3|T |. De la même manière la structure utilisée dans FREQT offre des performances attractives, mais cette représentation conduit également à tripler la taille de la base afin de stocker les informations nécessaires.\nMême si elles n\u0027abordent pas la même problématique, des approches de représentation efficaces des arbres en 2|T | ont été récemment proposées (Wang et al., 2004;Chi et al., 2004Chi et al., , 2003. Cependant, outre le fait qu\u0027elles ne considèrent pas la notion d\u0027ordre, elles n\u0027utilisent pas des propriétés aussi intéressantes que les travaux précédents afin d\u0027améliorer le processus d\u0027extraction.\nNotre objectif est donc de permettre une recherche de sous-arbres ordonnés mais, contrairement aux approches existantes dans ce contexte, d\u0027utiliser une représentation peu coûteuse en mémoire, i.e. en 2|T |. Cette structure doit en outre posséder des propriétés intéressantes  pour améliorer le processus d\u0027extraction. C\u0027est dans ce contexte que se situe notre proposition RSF décrite à la section 3.\nProposition\nDans cette section, nous proposons de nouveaux algorithmes permettant l\u0027extraction efficace de sous-arbres fréquents ordonnés au sein d\u0027une base de données arborescentes. Dans un premier temps, nous décrivons la structure adoptée et nous en soulignons ses intérêts. Dans un second temps, nous proposons un survol de notre approche d\u0027extraction et nous montrons comment les propriétés de la structure sont utilisées pour améliorer le processus de fouille. Finalement, nous décrivons plus formellement les algorithmes proposés.\nPour illustrer nos propos, nous utiliserons la base d\u0027arbres de la figure 4.\nReprésentation des arbres\nPour représenter les différents arbres manipulés au sein de notre approche, nous adoptons la représentation proposée dans (Del Razo et al., 2005). Un arbre est ainsi décrit à l\u0027aide de deux vecteurs comme proposé dans Weiss (1998) Cette représentation permet de retrouver en temps constant le père d\u0027un noeud. De plus, elle permet la localisation directe de la feuille la plus à droite par rapport à l\u0027index k. En parcourant l\u0027arbre, il est ainsi possible d\u0027obtenir toutes les relations directes père-fils entre noeuds. Le deuxième vecteur, nommé lb, est utilisé pour enregistrer les étiquettes de l\u0027arbre avec lb[i], i \u003d 0, 2, ..., k ? 1 représentant l\u0027étiquette de chaque noeud n i ? T .\nLa structure adoptée permet une représentation des arbres peu coûteuse puisqu\u0027elle se ré-duit à 2|T |. De plus elle possède des propriétés intéressantes, évoquées au paragraphe suivant, pouvant être utilisées lors de la recherche de sous-structures fréquentes.\nAperçu général\nNotre proposition est basée sur une approche classique de type « générer-élaguer », i.e. à chaque étape, nous générons différents candidats et nous testons si ceux-ci sont inclus dans la bases d\u0027arbres. L\u0027inclusion dans notre cas est bien entendu définie comme étant de type « induit ».\nLa méthode de représentation des arbres que nous proposons permet de générer de manière efficace les sous-arbres candidats puis d\u0027élaguer les sous-arbres non fréquents (après calcul du support). Les candidats de taille 2 sont générés en combinant deux à deux tous les fréquents de taille 1. La génération des candidats de taille k ? 3 s\u0027effectue de la même manière que dans les approches classiques de type Apriori (Agrawal et Srikant, 1994), par combinaison des fréquents de taille k ? 1. Nous adoptons la stratégie de génération de candidats selon la branche la plus à droite comme proposée dans (Asai et al., 2002;Zaki, 2002) et illustrée figure 7. Nous pouvons ainsi constater l\u0027intérêt de notre structure de représentation puisque, naturellement, il suffit d\u0027ajouter un nouvel élément dans la représentation de l\u0027arbre en spécifiant le père du nouveau noeud.\nLe calcul du support de chaque candidat consiste à compter le nombre d\u0027arbres de la base qui contiennent ce sous-arbre candidat. Ainsi pour chaque arbre de la base, nous recherchons les points d\u0027ancrage sur lesquels la racine du sous-arbre à tester peut s\u0027instancier. Ces points correspondent en fait aux noeuds dans l\u0027arbre qui correspondent à la racine de l\u0027arbre à tester. Pour chaque point d\u0027ancrage trouvé, on cherche alors à instancier l\u0027ensemble des noeuds de l\u0027arbre candidat au sein de l\u0027arbre courant testé, i.e. les fils du noeud à tester. Notons que dans le cas d\u0027une inclusion induite, nous recherchons une instanciation exacte du candidat au sein des arbres de la base. Si tous les noeuds du candidat ont été trouvés, l\u0027arbre supporte le candidat et le support de la structure candidate est alors incrémenté.\nLes algorithmes\nL\u0027algorithme RFS (Algorithme 1) fonctionne de la manière suivante : un premier parcours sur la base est réalisé pour extraire les items dont le nombre d\u0027occurrences est supérieur au support minimal. Ces items constituent des arbres résumés à une seule racine, l\u0027item considéré. Nous obtenons ainsi l\u0027ensemble F 1 des arbres fréquents de taille 1. Ces derniers sont combinés entre eux pour former des candidats de taille 2 et un parcours sur la base permet d\u0027obtenir l\u0027ensemble F 2 constitué des arbres de taille 2. L\u0027algorithme se poursuit en générant des candidats de taille k+1 et en effectuant un parcours sur la base pour compter le nombre d\u0027occurrences de chaque candidats. Lorsque plus aucun candidat ne peut être généré l\u0027algorithme se termine.\nL\u0027algorithme GenCandidats(F k?1 ) (Algorithme 2) décrit la génération des candidats qui utilise la branche la plus à droite des sous-arbres fréquents de taille k ? 1 afin de proposer des candidats de taille k. Pour chaque arbre fréquent de taille k ? 1, il génère un nouveau candidat en étendant l\u0027arbre par la branche la plus à droite. Cette génération est obtenue par l\u0027intermédiaire de la fonction Bpd. Ainsi, pour chaque noeud, nous lui ajoutons les seules extensions possibles, i.e. celles qui s\u0027avèrent fréquentes dans F 2 .\nEntrée : D \u003d {T 1 ,T 2 ,..,T n } base de données d\u0027arbres ; ? le support minimal. Sortie : F sous-arbres fréquents.\nEntrée : F k?1 des (k ?1)-sous-arbres fréquents. Sortie : C k des (k)-sous-arbres candidats.\nF 1 ? arbres fréquents de taille 1; F 2 ? arbres fréquents de taille 2;\nAlgorithme 2: GenCandidats(F k?1 ).\nEntrée : C candidat , T un arbre, i index de la racine de l\u0027ancrage. Sortie : vrai si T supporte C.\nsi trv alors + + cnt;\nAlgorithme 3: Support(C).\nAlgorithme 4: Ancre(C, T, i).\nLe calcul du support de chaque candidat consiste à compter le nombre d\u0027arbres de la base qui contiennent ce sous-arbre candidat.\nPour chaque arbre de la base, une recherche est effectuée pour voir s\u0027il existe des points d\u0027ancrage sur lesquels la racine du sous-arbre à tester peut s\u0027instancier (appel à l\u0027algorithme Ancre). Si un sous-arbre existe son nombre d\u0027occurrences est alors incrémenté et son support est retourné.\nConsidérons l\u0027algorithme de gestion des points d\u0027ancrage (Algorithme 4). Pour chaque point d\u0027ancrage trouvé, i.e. pour chaque noeud du sous arbre candidat c qui possède le même label dans l\u0027arbre T , on cherche à instancier l\u0027ensemble des noeuds de l\u0027arbre candidat au sein de l\u0027arbre couramment testé T . En d\u0027autres termes, nous souhaitons projeter le sous-arbre candidat c dans l\u0027arbre T . Ceci est réalisé par l\u0027intermédiaire des algorithmes Ancre et Poursuit (cf. algorithmes 4 et 5).\nL\u0027algorithme Poursuit est utilisé pour chercher une instanciation exacte du candidat au sein des arbres de la base. Si tous les noeuds du candidat ont été trouvés, l\u0027algorithme retourne alors la valeur V RAI (l\u0027arbre supporte le candidat). Il retourne la valeur F AU X si tous les noeuds de l\u0027arbre ont été parcourus sans trouver l\u0027ensemble des noeuds du candidat.\nAlgorithme : Poursuit(N niv_act , T, nbnoeuds)) Entrée : N niv_act ensemble de noeuds à trouver ; T l\u0027arbre ; nbnoeuds le nombre de noeuds vérifiés. Sortie : vrai si tous les noeuds de N niv_act ont été trouvés.\nsi (nbnoeuds\u003d |N niv_act |) alors retourner vrai; sinon retourner faux;\nAlgorithme 5: Poursuite de la recherche.\nExpérimentations\nNos expérimentations ont été réalisées avec un PC Pentium ayant 512 Mo RAM sous le système Linux 2.4. Les programmes ont été développés en C++ et compilés avec gcc 3.2.2.\nNous avons utilisé 6 bases de données construites en employant le programme de généra-tion d\u0027arbres XML proposé par (Termier et al., 2002) Nous avons souhaité évaluer notre proposition selon deux aspects : temps de réponse et occupation mémoire. En effet, nous argumentons notre proposition comme étant plus efficace pour un réel passage à l\u0027échelle mais ceci n\u0027est pas toujours synonyme d\u0027efficacité en temps de réponse. En fait, les expérimentations réalisées prouvent que notre proposition répond aux deux critères.\nPour évaluer les performances sur les temps d\u0027exécution, nous nous sommes comparés à l\u0027algorithme FREQT-nodd sans détection des duplicats de (Asai et al., 2002) permettant de rechercher des inclusions induites puis à une optimisation de celui-ci FREQT-dd limitant le parcours dans les arbres lors de la vérification des candidats.\nLa figure 8-(a) représente l\u0027occupation mémoire utilisée pour la représentation de la base de schémas XML. Comme nous nous y attendions RSF occupe moins d\u0027espace mémoire puisqu\u0027il adopte une structure de représentation plus réduite que FREQT-nodd et FREQT-dd. Ces deux derniers adoptent la même structure.\nLes Nous souhaitons à présent mettre en oeuvre une optimisation de parcours de la structure proposée afin d\u0027améliorer les performances en terme de temps d\u0027exécution. Une telle optimisation est tout à fait réalisable et constitue l\u0027une de nos perspectives principales. Nous devrions alors obtenir des performances supérieures à celles obtenues pour FREQT-dd tout en conservant une structure en 2|T |. \nConclusion et perspectives\nDans cet article, nous proposons une approche efficace d\u0027extraction de sous-arbres fré-quents. RSF est la première proposition de recherche de sous-arbres fréquents selon une inclusion induite à l\u0027aide d\u0027une représentation de la base de schémas en 2|T |. Les premières expérimentations réalisées sur des données synthétiques soulignent l\u0027intérêt de notre proposition par rapport aux approches de référence. Les perspectives immédiates concernant RSF suivent deux axes :\n-Tout d\u0027abord, il est possible d\u0027améliorer l\u0027algorithme en optimisant les parcours réalisés lors de la vérification des candidats comme proposé dans l\u0027optimisation de FREQT (Asai et al., 2002). Toutefois, nous souhaitons mettre en place un procédé moins coûteux en terme d\u0027espace mémoire. -Ensuite, nous souhaitons utiliser la même structure de représentation des arbres pour réaliser une recherche de sous-arbres fréquents en se basant sur une inclusion incrustée. Ces travaux ont pour objectif d\u0027être utilisés dans le cadre de la médiation de données, les sous-arbres fréquents extraits servant de support à la construction automatique d\u0027un schéma médiateur. Une telle solution peut également être adoptée dans le cadre de la fouille de données en ligne (data streams) pour le traitement à la volée de données XML. Cette perspective permettra de traiter les gros volumes de données transitant sur Internet de manière efficace et rapide.\n"
  },
  {
    "id": "1068",
    "text": "Introduction\nLes progrès techniques récents ont eu pour conséquence l\u0027augmentation du nombre de flux d\u0027information et la croissance rapide de leurs débits. L\u0027architecture traditionnelle de l\u0027analyse de données -où les données, préalablement stockées, sont analysées puis rafraîchies -étant inadaptée au traitement de ces flux, une nouvelle famille de techniques, dites de stream mining, se propose d\u0027inverser radicalement cette architecture et de mettre en oeuvre des systèmes reposant sur des capacités de stockage minimales qui sont mises à jour à la vitesse du flux. L\u0027objectif de cet article est d\u0027expliquer comment nous avons utilisé des techniques de stream mining afin d\u0027identifier en temps réel, dans un réseau IP, les préfixes dont la contribution au trafic dépasse une certaine proportion de ce trafic pendant un intervalle de temps donné.\nconsidéré. Cependant, les concepts et les algorithmes que nous allons présenter ici peuvent naturellement s\u0027appliquer à tout flux de données de la forme précédente 1 . Classiquement, sur ce type de flux de données, on définit le compte a i (? ) d\u0027un identifiant i ? U à l\u0027instant ? par a i (? ) \u003d ? t\u003d0 c t ? i,it , où ? i,it vaut 1 si i \u003d i t et 0 sinon. Dans l\u0027exemple ci-dessus, le compte a i (? ) d\u0027une adresse IP i représente le nombre total d\u0027octets à l\u0027instant ? qui ont été envoyés vers l\u0027adresse i et qui ont transité par le point P considéré. On peut alors s\u0027intéresser aux objets massifs (heavy hitters) du flux à l\u0027instant ? , c\u0027est-à-dire, aux identifiants i dont le compte a i (? ) est supérieur ou égal à une fraction ?N (? ) du compte total N (? ) \u003d ? t\u003d0 c t du flux. Dans le cas particulier où l\u0027ensemble U des identifiants étudié peut être organisé de façon hiérarchique, il peut également être intéressant d\u0027effectuer une recherche d\u0027objets massifs au sein de cette hiérarchie 2 . Ainsi, dans l\u0027exemple ci-dessus, on peut regrouper les adresses IP par préfixe 3 , puis associer à un préfixe donné la somme des comptes des adresses IP commençant par ce préfixe et finalement rechercher à un instant donné tous les préfixes dont le compte est supérieur ou égal à une fraction donnée du compte total du flux. L\u0027inconvénient de cette approche est que si un préfixe donné est un objet massif, alors tous les préfixes contenus dans ce préfixe (autrement dit tous les préfixes qui sont des ancêtres du préfixe considéré) seront aussi des objets massifs, alors qu\u0027il est parfois souhaitable de ne plus tenir compte de la contribution de cet objet massif lorsque l\u0027on recherche des objets massifs parmi les préfixes plus courts. C\u0027est pourquoi la notion d\u0027objet massif hiérarchique (hierarchical heavy hitters) a été introduite (Cormode et al., 2003). Les objets massifs hiérarchiques d\u0027un flux dont les identifiants appartiennent à une hiérarchie sont définis de façon récursive : les objets massifs hiérarchiques de niveau 0 (le niveau le plus bas de la hiérarchie) sont les objets massifs du flux ; les objets massifs hiérarchiques de niveau l \u003e 0 sont les sommets de la hiérarchie de niveau l dont la somme des comptes des identifiants qui sont leurs descendants et qui n\u0027appartiennent pas à des objets massifs hiérarchiques de niveau inférieur à l, est supérieure ou égale à ?N (? ). Pour une présentation plus détaillée de la notion d\u0027objet massif hiérarchique, on pourra consulter Cormode et al. (2003).\nL\u0027algorithme de recherche d\u0027objet massif hiérarchique de Cormode et al.\nlaquelle on peut obtenir, pour tout instant ? , une estimationâestimationˆestimationâ S (? ) du compte a S (? ) de chaque sommet S de niveau l de la hiérarchie 4 . Pour obtenir la liste (estimée) des objets massifs hié-rarchiques à un instant ? , on explore récursivement la hiérarchie en commençant par le haut de la hiérarchie (cf. Cormode et al. (2003, § 4) pour plus de détails). La liste estimée obtenue coïncide avec la liste exacte lorsque toutes les comptes estimésâestimésˆestimésâ S (? ) utilisés lors de l\u0027exploration de la hiérarchie coïncident avec le compte exact a S (? ). Dans Cormode et al. (2003), cette estimation est effectuée à l\u0027aide de l\u0027algorithme Random Subset Sums (RSS) introduit dans Gilbert et al. (2002). L\u0027algorithme RSS est un algorithme de Monte Carlo qui garantit que l\u0027on a P {|â S (? ) ? a S (? )| ?N (? )} 1 ? ?, où ? et ? sont des réels positifs inférieurs à 1, en utilisant un espace mémoire en O( \nL\u0027algorithme que nous avons mis en oeuvre\nRécemment, Cormode et Muthukrishnan (2005) ont proposé un autre type de sketch, le Count-Min Sketch (CMS). Comme l\u0027algorithme RSS, l\u0027algorithme CMS permet d\u0027obtenir une estimationâestimationˆestimationâ S (? ) du compte a S (? ) d\u0027un sommet S à l\u0027instant ? . Cependant, l\u0027algorithme CMS a pour avantage de garantir que l\u0027on a ˆ a S (? ) a S (? ) et P {â S (? ) a S (? ) + ?N (? )} 1 ? ? en utilisant un espace mémoire en seulement O( 1 ? ln(1/?)) mots pour le stockage d\u0027un sketch et un nombre d\u0027opérations en seulement O(ln(1/?)) pour la mise à jour d\u0027un sketch, ainsi que pour le calcul de l\u0027estimateur du compte d\u0027un sommet. C\u0027est pourquoi, comme suggéré par Cormode et Muthukrishnan (2005), nous avons utilisé l\u0027algorithme CMS à la place de l\u0027algorithme RSS pour estimer le compte de chaque sommet. Nous obtenons ainsi un algorithme de recherche des objets massifs hiérarchiques qui nécessite un espace mémoire en O( h ? ln(1/?)) mots pour le stockage de l\u0027ensemble des sketches, un nombre d\u0027opérations en O(h ln(1/?)) pour la mise à jour de l\u0027ensemble des sketches et un nombre d\u0027opérations en O( hq ? ln(1/?)) (où q est le nombre maximum d\u0027enfants de chaque sommet de la hiérarchie) pour la recherche des objets massifs hiérarchiques. La réduction substantielle du nombre d\u0027opérations nécessaire pour chaque mise à jour 5 est particulièrement appréciable. En effet, si l\u0027on souhaite traiter le flux de données en temps réel, il est indispensable d\u0027effectuer cette mise à jour à la cadence à laquelle on reçoit les éléments du flux ; par conséquent, en ayant nettement moins d\u0027opérations à effectuer par mise à jour, on pourra, à puissance de calcul identique, traiter en temps réel des flux de données arrivant à une cadence bien plus élevée.\nApplication au cas d\u0027un réseau IP\nL\u0027algorithme présenté au § 2.3 a été appliqué à des données réelles provenant d\u0027un routeur Cisco installé sur un réseau IP de France Télécom. Pour des raisons pratiques, l\u0027analyse a été effectuée sur les adresses IP des flots de données enregistrés par la sonde Netflow qui a été activée sur ce routeur (Sommer et Feldmann, 2002). La trace dont nous présentons ici l\u0027analyse corrrespond à une vingtaine de minutes de trafic, 3,6 millions de paquets IP et 2 Go de volume. Notre analyse a été réalisée sur les adresses sources, toutes destinations confondues, pour un seuil ? \u003d 10 ?2 que nous avons jugé représentatif des seuils choisis par les utilisateurs 6 . La hiérarchie utilisée pour l\u0027analyse avait pour hauteur h \u003d 32. Nous avons tracé le taux de faux négatifs 7 et le taux de faux positifs 8 en fonction de la précision ?, pour différentes valeurs de la probabilité d\u0027échec ?. A titre d\u0027exemple, nous avons représenté sur les figures 1 et 2 les courbes obtenues pour une probabilité d\u0027échec ? \u003d 10 ?2 9 . Nous avons également étudié la quantité de mémoire utilisée par l\u0027algorithme décrit au § 2.3 en fonction des paramètres ? et ? 10 . Les résultats que nous avons obtenus montrent que, si l\u0027on choisit judicieusement les paramètres ? et ?, l\u0027algorithme présenté au § 2.3 permet d\u0027obtenir, sur la trace étudiée, un taux de faux positifs et de faux négatifs négligeable tout en nécessitant une quantité de mémoire raisonnable en pratique : par exemple, en prenant ? \u003d 10 ?2 et ? \u003d 10 ?4 , on obtient un taux de faux positifs et de faux négatifs inférieur à 1% en utilisant environ 2 millions de mots.\nConclusion\nComme suggéré par Cormode et Muthukrishnan (2005), nous avons utilisé l\u0027algorithme CMS à la place de l\u0027algorithme RSS afin de rechercher les objets massifs hiérarchiques à l\u0027aide de la méthode présentée par Cormode et al. (2003, § 4). Cependant, alors que Cormode et Muthukrishnan (2005) ne préconisent cette solution que dans le cas de flux de données dont la marque c t peut être négative 11 , nous avons délibérément appliqué cet algorithme au cas de flux de données dont la marque c t est obligatoirement positive ou nulle. En effet, les méthodes proposées par Cormode et al. (2003, § 3) pour rechercher les objets massifs hiérarchiques dans des flux de données dont la marque c t est obligatoirement positive ou nulle présentent pour nous les deux inconvénients suivants : d\u0027une part ces méthodes nécessitent une quantité de mémoire qui (pour une précision donnée) augmente avec le compte total N (? ) 12 et d\u0027autre part ces méthodes ont été conçues uniquement pour rechercher les objets massifs hiérarchiques dans des flux de données dont la marque c t est obligatoirement positive ou nulle ; leur adaptation au cas de flux de données dont la marque c t peut être négative nous paraît très difficile. La solution que nous avons mise en oeuvre ne présente pas ces inconvénients. Dans notre cas, le nombre de mots nécessaire en mémoire reste constant avec le temps et en particulier ne dépend pas du compte total N (? ) : il n\u0027est pas nécessaire d\u0027estimer finement à l\u0027avance le volume de trafic qui sera observé ni la durée totale pendant laquelle l\u0027analyse sera effectuée 13 . De plus, comme la mémoire nécessaire aux méthodes de Cormode et al. (2003, §  6 , qui est rapidement atteint en pratique dans l\u0027application que nous avons étudiée. Par conséquent, la solution que nous avons mise en oeuvre est bien pertinente et nous permet de mieux exploiter la mémoire disponible que les méthodes proposées par Cormode et al. (2003, § 3). D\u0027autre part, l\u0027adaptation de la solution que nous avons mise en oeuvre au cas de flux de données dont la marque c t peut être négative est extrêmement simple : il suffit pour cela de modifier l\u0027estimation du compte de chaque préfixe en remplaçant la minimisation qui intervient dans cette estimation par une recherche de médiane (Cormode et Muthukrishnan, 2005). La solution que nous avons mise en oeuvre possède donc une grande flexibilité qui nous permettra de réutiliser plus facilement les programmes déjà écrits lorsque nous étudierons des flux de données dont la marque c t peut être négative. sous-utilisé la mémoire disponible si le trafic est plus faible que prévu durant la période étudiée. 13 Il faut simplement s\u0027assurer au préalable que la taille des mots utilisée reste suffisante pour toutes les situations envisageables.\n-57 -RNTI-E-6\nRecherche de préfixes IP massifs Les auteurs remercient Guillaume Picard pour ses commentaires très pertinents sur une version préliminaire de cet article.\n"
  },
  {
    "id": "1069",
    "text": "Introduction\nLes patients hospitalisés en unités de réanimation sont soumis à une surveillance étroite de la part du personnel soignant. Un grand nombre de variables physiologiques sont enregistrées en ligne à des fréquences élevées (une mesure par seconde) sur ces patients. Ces enregistrements produisent des flots de données temporelles importants, que le personnel soignant doit analyser à chaque visite au patient. Les services de réanimation sont en demande d\u0027outils d\u0027aide à l\u0027interprétation de ce flot de données, afin de limiter la charge cognitive que leur interprétation représente (Calvelo et al.,99,Lowe et al.,01,Hunter and McIntosh,99).\nAfin d\u0027aider le médecin dans sa tâche d\u0027analyse des données, nous avons développé une méthode d\u0027extraction en ligne d\u0027épisodes temporels permettant de transformer une série temporelle univariée en une succession d\u0027intervalles décrivant l\u0027évolution de la variable. L\u0027information fournie par la méthode est de la forme suivante : « la variable est stable depuis l\u0027instant t1 jusqu\u0027à l\u0027instant t2, à la valeur v1. Elle est croissante de l\u0027instant t2 à l\u0027instant t3 de la valeur v1 à la valeur v2 … ». L\u0027information fournie sur la tendance du signal {stable, croissant, décroissant} correspond au vocabulaire utilisé par les médecins pour décrire l\u0027évolution d\u0027un e physiologique. La méthode d\u0027extraction d\u0027épisodes se règle à partir de 3 paramètres de réglages dont les valeurs dépendent des variables physiologiques traitées, mais sont indépendants du patient ou de l\u0027enregistrement, l\u0027hypothèse sous-jacente étant que le bruit s\u0027ajoutant sur une variable biologique ne dépend pas du patient mais de la variable monitorée. Or, dans la pratique, cette hypothèse n\u0027est pas toujours vérifiée. La variance des variables monitorés peut changer, suivant l\u0027état physiologique du patient ou le contexte des soins. Par exemple, la variance des variables respiratoires (ex: la fréquence respiratoire) sera très différente suivant que le patient est en ventilation spontané ou en mode de ventilation contrôlé par le ventilateur. Les informations sur la tendance à extraire ne sont alors plus les mêmes. Une petite modification de la fréquence respiratoire pourra être porteuse d\u0027information (modification de l\u0027état du patient) si le patient est en mode de ventilation débit contrôlée, alors qu\u0027une modification du même ordre ne sera pas significative pour un patient en ventilation spontanée. Actuellement, pour que la méthode d\u0027extraction d\u0027épisodes temporels fonctionne de façon optimale, il faut préciser le mode de ventilation du patient. Le réglage d\u0027un des paramètres de la méthode sera alors automatiquement divisé par 2. L\u0027objectif de ce papier est de présenter la version adaptative de la méthode d\u0027extraction d\u0027épisodes et son utilisation pour reconnaître de manière automatique des aspirations trachéales. Les résultats obtenus par cette méthode sur des variables enregistrées sur des patients dans des contextes de soins différents seront présentés. Dans un premier temps, nous rappellerons le principe de la méthode d\u0027extraction d\u0027épisodes temporels, puis nous présente-rons les modifications apportées pour rendre la méthode adaptative. Nous présenterons ensuite quelques résultats obtenus en utilisant cette nouvelle méthode pour reconnaître des évènements particuliers à partir des signaux : les aspirations trachéales.\nExtraction d\u0027épisodes temporels en ligne\nLa méthode d\u0027extraction d\u0027épisodes temporels se décompose en quatre phases successives (Charbonnier, 2005) : décomposition en ligne des données en segments de droites, classification des segments en 7 formes temporelles, transformation des formes en épisodes temporels semi-qualitatives, agrégation de l\u0027épisode temporel courant avec les précédents\nDécomposition en segments de droite\nUn algorithme de segmentation consiste à décomposer en ligne les données en une succession de segments de droite (Charbonnier 2005). La technique utilisée pour déterminer l\u0027instant où l\u0027algorithme doit recalculer une nouvelle approximation linéaire est celle de la CUSUM, qui correspond à une intégration numérique des différences entre le signal et l\u0027extrapolation par la dernière droite calculée. La valeur absolue de la cusum est comparée à chaque période d\u0027échantillonnage à 2 seuils appelé th 1 et th 2 . Si la CUSUM est inférieure à th 1 , l\u0027approximation linéaire est acceptable. Si la CUSUM est supérieure à th 1 , la valeur du signal et le temps correspondant seront stockés dans une liste appelée : Liste des valeurs anormales. Si la CUSUM est supérieure à th 2 , l\u0027approximation linéaire n\u0027est plus acceptable et une nouvelle approximation linéaire sera calculée par la méthode des moindres carrés, à partir des données stockées dans la liste des valeurs anormales, si la taille de la liste est supé-rieure à 3. Dès qu\u0027une nouvelle approximation linéaire a été calculée, la cusum est remise à zéro. La décomposition en segments de droite est essentiellement réglée par le seuil th 2 . Si th 2 est petit, les segments seront recalculés fréquemment et des petites oscillations dans le signal seront segmentées, alors que si th 2 est grand, les segments seront plus longs et certains phénomènes transitoires seront filtrés.\nClassification en formes temporelles\nChaque nouveau segment calculé par l\u0027algorithme de segmentation, associé au segment précédent constitue une forme qui peut être classée dans l\u0027une des 7 catégories retenues : stable, croissant, décroissant, échelon positif ou négatif, échelon positif, transitoire croissantdécroissant ou décroissant-croissant.\nLa classification est effectuée en calculant des caractéristiques sur les segments : diffé-rence entre la fin du segment précédent et la fin du segment courant, différence entre la fin du segment précédent et le début du segment courant, différence entre le début du segment courant et la fin du segment courant .\nUn arbre de décision permet le classement en 7 formes, à partir d\u0027un seuil de réglage th c . Ce seuil correspond à la valeur à partir de laquelle une variation sur la variable sera considé-rée significative. Si la valeur d\u0027une des 3 caractéristiques dépasse le seuil fixé, th c , la forme temporelle est du type croissant ou décroissant. Sinon, la forme est stable. Si le seuil th c est grand, les petites variations sont considérées comme non significatives et n\u0027apparaissent pas dans les épisodes temporels extraits. Seules les grandes variations sont exprimées sous forme d\u0027épisodes.\n3. Les formes temporelles sont ensuite découpées en 1 ou 2 épisodes temporels élémen-taires, définis par trois grandeurs {stable, croissant, décroissant} 4. Les épisodes temporels courants sont ensuite agrégés avec les épisodes précédents.\nLa méthode est donc réglé par 3 paramètres th 1 et th 2 qui déterminent la segmentation du signal et th c qui fixe le niveau de variation à partir duquel une forme est considérée comme croissante (ou décroissante) plutôt que stable. Dans la version non adaptative de l\u0027algorithme, ces seuils gardent une valeur fixe qui dépend de la variable à traiter et, pour certaines variables respiratoires, du mode de ventilation.\nAdaptation des seuils de réglages de l\u0027algorithme d\u0027extraction d\u0027épisodes\nLa variance des variables physiologiques peut varier en fonction du contexte de soins dé-livrés au patient. Afin de prendre en compte ce changement de variance sur les variables, nous avons développé une version adaptative de la méthode d\u0027extraction de tendance. Les seuils th 1 , th 2 et th s ne sont plus fixes et réglés en fonction du mode de ventilation, mais s\u0027adaptent en ligne en fonction de la variance estimée du signal.\nEstimation de la variance du signal\nL\u0027estimation de la variance du signal s\u0027effectue à partir des résidus. On appelle résidus la différence, à chaque période d\u0027échantillonnage, entre le signal et l\u0027approximation linéaire calculé par l\u0027algorithme de segmentation. Cette différence (résidu) correspond à la partie du signal qui n\u0027est pas expliquée par l\u0027approximation linéaire. La variance des résidus est alors estimée à chaque période d\u0027échantillonnage sur une fenêtre glissante de 60 secondes et la médiane de la variance obtenues sur les x dernières secondes est calculée. Nous noterons cette valeur Mx. Nous avons choisi une fenêtre glissante de 60 secondes, car c\u0027est une durée suffisamment courte pour que l\u0027hypothèse de stationnarité du signal reste plausible et suffisamment longue pour faire une estimation peu biaisée de la variance. Le calcul final de la variance se fait par valeur médiane sur une fenêtre d\u0027apprentissage de x secondes, ce qui permet de filtrer les augmentations de variance dues à des artéfacts sur le signal. La taille de la fenêtre, x, a été choisi égale à 300 secondes. C\u0027est un compromis entre la durée de l\u0027information passée à prendre en compte et la sensibilité de la mesure Mx à des artéfacts sur le signal, qui sont d\u0027autant mieux filtrer que la période d\u0027apprentissage est longue.\nAlgorithme d\u0027adaptation des seuils\nA chaque nouvelle segmentation, de nouvelles valeurs sont affectées aux seuils th 1 , th 2 et th s , en fonction de la valeur de Mx. La fonction d\u0027adaptation que nous avons choisi est un cycle d\u0027hystérésis. 2 jeux de seuils { th 1 , th 2 , th s } important et { th 1 , th 2 , th s } faible sont applicables à l\u0027algorithme. Ils correspondent aux jeux optimaux de réglage quand la variance du signal est faible et quand elle est plus importante. { th 1 , th 2 , th s } important correspondent aux valeurs de seuils proposés dans (Charbonnier, 2005). { th 1 , th 2 , th s } faible correspondent aux valeurs de { th 1 , th 2 , th s } important divisé par 2. L\u0027utilisation d\u0027un cycle d\u0027hystérésis permet de limiter des alternances répétées entre les 2 jeux de réglages, quand la mesure de Mx est proche du seuil de commutation.\nChoix des seuils de commutation\nAfin de ne pas augmenter trop fortement le nombre de seuils à régler sur l\u0027algorithme, les seuils de commutation (Com1 et Com2) du cycle d\u0027hystérésis ont été normalisés pour chaque variable physiologique, à partir de la fonction de répartition de la variance des différentes variables. La fonction de répartition de la variance des différentes variables physiologiques a été estimée à partir d\u0027une ensemble d\u0027enregistrements obtenus sur une vingtaine de patients différents. La variance du signal a été estimée à chaque période d\u0027échantillonnage en utilisant une fenêtre glissante de 60 secondes sur l\u0027ensemble de ces enregistrements. L\u0027ensemble des mesures de variance calculées sur chaque enregistrement et sur une variable a été mis en commun et un tirage aléatoire a été effectué. Un sous-ensemble constitué du quart des valeurs de la base de données a été réalisé et a servi à estimer la fonction de réparti-tion de la variance pour chaque variable physiologique. La valeur correspondante au 95 ème percentile a été relevée pour chaque variable. Les seuils de commutation du cycle d\u0027hystérésis ont été choisis arbitrairement à 15% et 25% de la valeur du 95 ème percentile pour chaque variable physiologique. Le jeu {th1, th2 et ths} faible est appliqué à l\u0027initialisation de l\u0027algorithme et est maintenu tant que la valeur de Mx ne dépasse pas 25% du 95 ème percentile. Quand la commutation vers le jeu {th1, th2 et ths} important a eu lieu, le jeu sera maintenu jusqu\u0027à ce que la valeur de Mx devienne inférieure à 15% du 95 ème percentile.\nRésultats\nNous avons appliquée la méthode adaptative sur l\u0027ensemble de la base de données dont nous disposons. La version adaptative de l\u0027algorithme a été appliquée aux variables physiologiques suivantes : Pression artérielle systolique (PAS), fréquence cardiaque (FC), pression maximale dans les voies aériennes (Pmax), débit maximal dans les voies aériennes (Dmax), volume expiré (VE), fréquence respiratoire (FR), ventilation minute (VM).\nAfin de valider les résultats obtenus, nous avons utilisé les épisodes temporels extraits pour reconnaître un évènement particulier : les aspirations trachéales. Cette validation a été effectuée hors ligne. L\u0027algorithme a été appliqué sur chaque enregistrement sur l\u0027ensemble des variables physiologiques et les épisodes temporels extraits ont été stockés. Un algorithme de reconnaissance d\u0027évènements a ensuite été appliqué, qui utilise ces épisodes.\nUne forme temporelle multivariable correspondant à une aspiration trachéale a été modé-lisée de la manière suivante : {épisode Décroissant sur Pmax ou VE ou VM dont la valeur finale est inférieure à une valeur seuil} et, simultanément, {transitoire décroissant_croissant ou instabilité sur Pmax, VE, VM, FR, Dmax.}. Un transitoire décroissant_croissant est une forme temporelle monovariable composé de 3 épisodes successifs (stable, décroissant, croissant) ou (décroissant, croissant, stable). Une instabilité est un intervalle de temps où la variance des résidus d\u0027une variable est supérieure ou égale au 95 ème percentile. Un indicateur de la certitude de la présence d\u0027une aspiration trachéale est proposé, en ajoutant une valeur à chaque forme temporelle monovariable. Ainsi, un transitoire décrois-sant_ croissant et une instabilité ont une valeur de 1. Une chute a une valeur de 2 si la valeur finale est inférieure à un certain seuil et une valeur de 3 pour un seuil plus petit. La valeur attribuée à une variable correspond à la valeur maximale relevé sur cette variable. Une aspiration trachéale est reconnue si la valeur totale de l\u0027événement (somme des valeurs de toutes les variables observées simultanément) est d\u0027au moins 3, qu\u0027au moins deux variables différentes interviennent dans le calcul de la valeur totale et qu\u0027on a détecté au moins une chute sur une variable. Ainsi, pour reconnaître une aspiration trachéale, la condition minimale est que soient présents simultanément une chute d\u0027une des variables sous une valeur seuil et une instabilité ou un transitoire sur une autre variable (valeur totale égale à 3).\nUne analyse quantitative des résultats est en cours qui permettra de préciser le nombre d\u0027aspirations trachéales correctement reconnues, non reconnues et faussement reconnues. Elle nécessite l\u0027analyse de tous les signaux par un observateur extérieur, qui n\u0027a pas encore été effectuée. Un exemple provenant d\u0027un enregistrement sur un patient hospitalisé en réani-mation est présenté figure 1. La méthode a détecté 3 aspirations trachéales, au cours des 4 heures d\u0027enregistrement. Les zones d\u0027aspirations sont représentées par les étoiles en haut des variables.\nLes résultats préliminaires obtenus sur les 25 enregistrements de la base de données sont les suivants : 51 aspirations trachéales ont été détectées par l\u0027algorithme, 29 d\u0027entre elles correspondent à des formes temporelles détectées sur au moins 3 variables, et 22 à des formes temporelles détectées sur 2 variables seulement.\nLa figure 2 présente l\u0027histogramme des valeurs obtenues sur ces 51 aspirations. 28 aspirations trachéales ont été détectées avec une valeur d\u0027au moins 5. Ces aspirations semblent concordantes avec l\u0027analyse visuelle des signaux. Les 23 autres sont plus difficiles à classer et pourraient correspondrent à des toux ou des périodes d\u0027instabilité du patient. La méthode est capable de détecter des aspirations trachéales aussi bien sur des patients ventilés en mode débit contrôlé que sur des patients en ventilation spontanée.\nConclusion\nDans ce papier, nous présentons des travaux en cours de développement : la version adaptative d\u0027un algorithme d\u0027extraction d\u0027épisodes temporels. L\u0027extraction des épisodes se fait en ligne et les seuils de réglages de la méthode sont recalculés en cours d\u0027extraction. Des choix ont été effectués pour définir l\u0027algorithme d\u0027adaptation des seuils. Les résultats partiels semblent montrer qu\u0027il est possible de reconnaître des évènements particuliers (les aspirations trachéales) à partir des signaux, sans connaître à priori le mode de ventilation du patient.\n"
  },
  {
    "id": "1070",
    "text": "Introduction\nL\u0027important volume de documents disponibles en langue naturelle et leur évolution rapide font émerger la nécessité de définir des approches permettant de retrouver rapidement des informations pertinentes dans ces documents.\nCe papier présente une approche qui utilise une ontologie de domaine pour identifier automatiquement des concepts du domaine dans un corpus en langue naturelle. Cette identification de concepts peut servir dans différents contextes : annotation des documents, indexation d\u0027une collection de documents, etc. L\u0027approche proposée est complètement automatique et non-supervisée, mise à part l\u0027utilisation d\u0027une ontologie de domaine. Etant donnés une ontologie O et un corpus C, le but est de retrouver dans C des termes w qui sont l\u0027expression linguistique des concepts de l\u0027ontologie O. On peut ainsi étiqueter les termes retrouvés dans le corpus par des concepts de l\u0027ontologie. Cet étiquetage est réalisé en trois étapes : (1) une première étape emploie des techniques de fouille de textes pour identifier des termes du domaine dans le corpus; (2) pour chaque terme w retrouvé, le voisinage sémantique V(w) est identifié ; (3) en supposant que les relations dans le voisinage du terme w soient déjà dans l\u0027ontologie, le positionnement des relations dans l\u0027ontologie et des mesures statistiques sont utilisés pour étiqueter le terme w.\nLe papier présente l\u0027approche adoptée en répondant à un certain nombre de questions : Comment extraire des termes à partir du corpus ? Comment identifier le voisinage sémanti-que des termes extraits ? Ces questions sont traitées dans le paragraphe 2. Etant donnés le terme et son voisinage sémantique, quelles sont les stratégies d\u0027étiquetage ? Une réponse est apportée dans le paragraphe 3. Le paragraphe 4 présente les résultats d\u0027une première expéri-mentation en accidentologie; les perspectives à donner à ce travail sont discutées dans le paragraphe 5.\nExtraction des termes et du voisinage sémantique\nLa fouille de textes est employée pour retrouver des termes du domaine qui représentent l\u0027expression linguistique des concepts (Ville-Ometz et al., 2004). La technique adoptée consiste à rechercher dans le corpus des associations de catégories lexicales susceptibles d\u0027engendrer des regroupements de mots valides. Une telle association de catégories lexicales constitue un patron lexical, par exemple (Verbe, Préposition, Nom).\nUn algorithme de reconnaissance (Ceausu et Desprès, 2005) identifie, dans le corpus annoté par l\u0027analyseur syntaxique TreeTagger (Schmid, 1994)  (square, esplanade). L\u0027algorithme suivant permet d\u0027assigner les termes aux concepts de l\u0027ontologie en utilisant cette heuristique : (1) identification des classes de verbes dans l\u0027ensemble d\u0027instances des patrons verbaux ; (2) identification des arguments -sujet et complément -des constructions verbales du type « verbe, prépo-sition » ; (3) utilisation de la représentations des relations dans l\u0027ontologie pour étiqueter les termes. Un pré-traitement de regroupement des arguments des constructions verbales est utilisé pour réduire la variance linguistique entre les arguments dont le sens est voisin. Des mesures statistiques entre les chaînes de caractères, présentées infra, sont utilisées.\nMesures de similarité lexicale\nUne mesure de similarité associe un nombre réel r à une paire de chaînes de caractères (S1,S2). Une valeur importante de r indique une similarité importante entre (S1,S2). Diffé-rentes approches permettent de calculer les similarités entre chaînes de caractères (Cohen et al., 2003). Les mesures de Jaccard, Jaro, Jaro-Winkler, Monge-Elkan ont été implémentées dans le cadre de ce travail.\nLa mesure de Jaccard estime la similarité entre deux chaînes S et T :\n(1) Cette mesure est le rapport entre le nombre des sous chaînes communes à S et T et le nombre total de sous chaînes de T et de S. Si les sous chaînes sont des caractères, la mesure de Jaccard correspond au nombre de caractères communs aux deux chaînes. \nLes mesures de Jaro et\nLa mesure de Jaro-Winkler (1999) est une extension de la mesure de Jaro qui utilise la taille P du plus long préfixe commun aux deux chaînes. En posant 1 m a x ( , 4 ) P P ? , on écrit :\n(1 ( , )) 1 0\nP J a r o W in k le r s t J a r o s t J a r o s t\nIl existe aussi des approches hybrides qui calculent les similarités entre deux chaînes de manière récursive, en analysant des sous chaînes des chaînes initiales. Ainsi, la mesure de Monge-Elkan estime la similarité entre \nPré-traitement des classes de verbes : regroupement des arguments\nLe rôle de cette étape de prétraitement est d\u0027identifier des similarités entre les arguments des relations du type « verbe, proposition » pour les regrouper. Pour une relation donnée, les arguments présentent différents niveaux de granularité, par exemple : partie -partie gauche -partie droite ; rétroviseur -rétroviseur extérieur -rétroviseur intérieur. Un algorithme de regroupement des arguments, fondé sur l\u0027utilisation de la plus grande spécificité des arguments composés de plusieurs mots sur les arguments mono mot, permet de construire des clusters de termes similaires. Un cluster est composé d\u0027un terme central appelé centroïde c et ses k plus proches voisins.\nL\u0027algorithme construit une liste de centroïdes composée des arguments mono-mot et utilise la fonction Monge-Elkan pour ajouter des termes aux clusters. Cette fonction est utilisée car elle a la capacité d\u0027agglomérer autour d\u0027un mot les termes dérivés de ce mot.\nEtiquetage des termes en utilisant l\u0027ontologie\nDans ce paragraphe on présente l\u0027étiquetage des termes extraits par des concepts de l\u0027ontologie. On dispose d\u0027une ontologie O, contenant un ensemble C de concepts liés par des relations appartenant à un ensemble R et des classes de verbes contenant des constructions grammaticales de type : (sujet), (verbe, préposions), (complément objet). Le prétraitement des classes a regroupé les arguments en clusters homogènes.\nL\u0027hypothèse de travail est que les relations verbales appartiennent à R . Pour chaque relation verbale du type (verbe, préposition), la relation r qui lui correspond dans l\u0027ontologie est retrouvée. Les concepts de l\u0027ontologie liés par r sont identifiés et utilisés pour étiqueter les termes en adoptant une des stratégies d\u0027étiquetage décrites ci-dessous.\nStratégies d\u0027étiquetage\nLes stratégies d\u0027étiquetage définissent la manière dont les termes seront assignés aux concepts de l\u0027ontologie. Les termes extraits sont déjà organisés en clusters, chaque cluster ayant un centroïde et des termes qui lui sont similaires (cf. 2.2.3).\nUne première stratégie traite un cluster comme un ensemble non hiérarchisé de termes. Pour chaque terme du cluster ses similarités avec les concepts de l\u0027ontologie sont évaluées en utilisant les mesures (1) à (3). Le terme est étiqueté par le concept qui maximise la valeur de cette similarité, si cette valeur dépasse un seuil imposé. Si toutes les valeurs des similarités se situent au-dessous du seuil, le terme sera étiqueté comme « inconnu ».\nLes stratégies suivantes prennent en compte la structure hiérarchique de chaque cluster. Ainsi, la stratégie top-down identifie d\u0027abord les concepts de l\u0027ontologie qui vont étiqueter les centroïdes. Si le centroïde d\u0027un cluster est étiqueté comme inconnu, la même étiquette est attribuée à chaque terme du cluster. Si le centroïde d\u0027un cluster est étiqueté par un concept c, les étiquettes pour les termes du cluster seront cherchées parmi les sous-concepts de c. Cette stratégie a l\u0027avantage de réduire l\u0027espace de recherche.\nUne troisième stratégie adopte une approche bottom-up. Pour chaque cluster, on évalue d\u0027abord les similarités entre ses termes et les concepts de l\u0027ontologie. Une des mesures (1) à (3) est utilisée et les termes sont étiquetés selon le principe de la première stratégie. Ensuite, la similarité du centroïde avec un concept de l\u0027ontologie est donnée par : Les termes extraits de ce corpus sont étiquetés et les résultats obtenus seront analysés selon deux points de vue : pour le même coefficient de similarité, comparer les résultats de chaque stratégie d\u0027étiquetage ; pour une même stratégie d\u0027étiquetage, comparer les résultats de chaque coefficient. Les arguments objets du verbe «circuler avec » sont étiquetés. La stratégie bottom-up permet d\u0027éliminer le centroïde « feu », qui est étiqueté comme « inconnu ». Elle pénalise les centroïdes ayant engendrés des clusters de taille réduite, qui sont assignés aux concepts de l\u0027ontologie avec un faible coefficient, ou sont étiquetés «inconnu». Les résultats des trois stratégies sont similaires pour les coefficients Jaro et Jaro-Winkler, Cette similarité est normale car Jaro-Winkler représente juste une variation de Jaro. Dans le cas du coefficient Jaccard, la stratégie bottom-up montre une défaillance, en assignant le terme « véhicule » au concept «véhicule de service». Quel que soit le coefficient choisi, l\u0027étiquetage top down est plus rapide et donne de meilleurs résultats.\nConclusion et perspectives\nNous avons présenté une approche permettant d\u0027assigner des termes extraits d\u0027un corpus en langue naturelle aux concepts d\u0027une ontologie. Des métriques pour calculer la similarité entre chaînes de caractères ont été implémentées et interviennent dans différentes étapes de l\u0027approche. Une première expérimentation dans le domaine de l\u0027accidentologie montre que les coefficients de Jaro et Jaro-Winkler donnent des estimations de similarités plus fines que Jaccard. Parmi les stratégies d\u0027étiquetage, l\u0027étiquetage top-down est plus rapide et engendre de meilleures assignations des termes aux concepts de l\u0027ontologie.\nEn perspective, des ressources terminologiques telles que WordNet peuvent être prises en compte afin d\u0027améliorer l\u0027estimation des similarités entre les termes du corpus et les concepts de l\u0027ontologie. Cela permettra d\u0027enrichir le voisinage sémantique du terme par d\u0027autres types de relations, comme la synonymie. Une autre perspective peut être l\u0027ajout d\u0027un feed-back dans le processus décrit, permettant à l\u0027utilisateur non seulement d\u0027étiqueter les termes du domaine, mais aussi d\u0027intégrer dans l\u0027ontologie certains des termes découverts.\nRéférences Alfonseca, E. and S. Manandhar. ( 2001 \nSummary\nThis paper presents an ontology supported approach to automatically recognize concepts of a specific field in a natural language corpora. This in a non-supervised solution that can be applied to any field for which an ontology was already created. A natural language corpora of the field is used in which specific concepts are recognized. In a first phase of the process, terms are extracted from the corpora using text mining techniques. Then, a domain ontology is used to label these terms. A label is assign to each term according to his semantic neighborhood and statistic measures. This paper gives a brief overview of employed text mining techniques and then it focus on the labeling process. A first experimentation of our approach in the field of accidentology was done and his results are also presented.\n"
  },
  {
    "id": "1071",
    "text": "Calcul du gain\nAfin de mesurer le gain d\u0027information d\u0027une règle, nous nous appuyons sur les variations possibles du support du motif M obtenu en réunissant les propriétés des parties gauches et droites sans que les supports des sous-motifs ne changent. L\u0027intervalle de variations obtenu a un centre, et nous décidons que le gain d\u0027information correspondant aux motifs de support central est nul. Plus le support du motif s\u0027éloigne de ce centre, plus la valeur absolue du gain augmente. Cela donne la formule suivante pour le gain : g\u003d2^(L-1)*(s-c), où s est le support du motif M, L la longueur de ce motif et c le centre de l\u0027intervalle de variation.\nLe gain de la règle fait partie des indices de qualité au même titre que le support, la confiance et la plupart de ceux dont on peut trouver la définition dans Guillet (2004). Toutefois, il ne mesure pas comme les autres indices la qualité intrinsèque d\u0027une règle, mais la valeur additionnelle d\u0027une règle avec prémisse composée par rapport à celles avec prémisses plus simples. Nous avons défini précédemment des RA floues sur des propriétés numériques (Cadot et Napoli, 2004). Le calcul du gain se prolonge sans problème à ces RA floues, les valeurs du support et du centre n\u0027étant plus nécessairement entières.\nApplication\nLe corpus traité est constitué de 3203 notices bibliographiques extraites de la base PASCAL sur le thème de la géotechnique et indexées manuellement. Nous avons calculé Règles d\u0027association avec prémisse composée : Mesure du gain d\u0027information. quatre classifications avec la méthode des K-means axiales (Lelu et François 1992)  C50 Pression Pores AE C20 Inélasticité C50 Champ pétrole AE C20 Inélasticité A première vue l\u0027intitulé \"Champ pétrole\" peut paraître surprenant. L\u0027analyse des données qui sont regroupées dans ces classes (titre des articles, résumés, indexation) permet de comprendre cette règle. En effet la classe \"Champ pétrole\" est essentiellement consacrée aux roches magasins et aux distributions des contraintes dans ces roches. La classe \"Inélasticité\" est dominée par des aspects liés à l\u0027élastoplasticité et à l\u0027analyse des champs de contraintes. Cette règle apporte ainsi un gain d\u0027information par rapport aux règles simples puisqu\u0027elle lie les notions de pression de pores (donc de roches poreuses plus ou moins saturées) et de distribution des contraintes dans des roches magasins (roches poreuses plus ou moins saturées) avec la notion de champ de contraintes dans le domaine élastoplastique.\nConclusion\nLe gain que nous proposons combine les avantages des indices de qualité des RA, et de l\u0027élagage du jeu de RA. Il garde les règles simples, construites sur deux propriétés qui ont été extraites à l\u0027aide d\u0027un indice de qualité choisi pour sa valeur sémantique, et sont donc aisément interprétables. Les autres règles, qui ne sont gardées que si leur gain est significatif, sont également simples d\u0027interprétation car elles renforcent l\u0027information tirée des premières. Au final, l\u0027ensemble des règles obtenu est de taille réduite. Malgré tout, le filtrage par ce gain laisse quelques règles incohérentes. La construction d\u0027un test permettant d\u0027établir la significativité du gain est en cours afin de les éliminer.\nSummary\nIn order to filter set of Association Rules with complex premises, we define a criteria which measures the improvement of information supported by the rule ABAEC compared to the simple rules AAEC or BAEC. Application to clustering results.\n"
  },
  {
    "id": "1072",
    "text": "Introduction\nLa conception de personnages virtuels simulant un comportement humain réaliste, y compris d\u0027un point de vue émotionnel (Aylett et Luck, 2000), connaît un engouement croissant. Dans ce contexte, il est alors nécessaire de doter des agents intelligents virtuels de caractéristiques psychologiques humaines. Pour ce faire, les informaticiens sont amenés à recueillir l\u0027expertise de psychologues.\nNotre travail s\u0027inscrit dans ce processus et consiste à modéliser l\u0027expertise psychologique de spécialistes dans le but d\u0027appliquer leurs connaissances à l\u0027élaboration d\u0027agents intelligents. Nous montrerons donc comment les graphes orientés et RDF peuvent permettre d\u0027accomplir cette tâche.\nLa modélisation des interactions\nDifférents concepts psychologiques sont exploités dans ce projet. Tout d\u0027abord, le comportement définit par l\u0027ensemble des réactions observables chez une personne. Il est propre à chaque individu. La société PerformanSe en a développé un modèle selon 10 dimensions bipolaires : couples de traits de personnalité antagonistes.\nEnsuite, les émotions qui caractérisent un ressenti à plus court terme. Le modèle OCC (Orthony et al., 1988), conçu par des psychologues, offre une modélisation facilement implémentable.\nEnfin, les interactions sociales qui sont une des notions clés dans les comportements collectifs d\u0027individus. Elles représentent la faculté de ressentir, d\u0027exprimer et d\u0027interpréter les émotions.\nC\u0027est l\u0027expertise de la société PerformanSe concernant la perception de l\u0027état émotionnel d\u0027autrui, qui a été transposée en langage naturel semi-structuré, que nous cherchons à modéliser. La recherche d\u0027une solution permettant la représentation formelle et l\u0027exploitation de ces connaissances fait l\u0027objet de notre étude.\nL\u0027expertise psychologique des spécialistes a été exprimée sous la forme d\u0027un ensemble de règles. Chacune d\u0027entre elles se compose d\u0027un ensemble d\u0027actions dont l\u0027exécution est conditionnée par une condition booléenne. Une règle peut s\u0027exprimer sous la forme : condition ? {action1, action2, …, actioni} En initiant le concept de Web Sémantique, Tim Berners Lee (Berners, 1999) a jeté les bases des langages rendant possibles la représentation sémantique des contenus. Parmi ces langages fédérés par le W3C et organisés en couches, nous avons choisi d\u0027utiliser RDF (Resource Description Framework), car il propose un niveau de complexité adapté à notre projet.\nUn document RDF pouvant se représenter sous la forme d\u0027un graphe orienté et étiqueté, nous avons été amenés à adapter les formalismes existants (graphes ET/OU et hypergraphes) afin de représenter une condition booléenne sous cette forme. Les actions associées aux règles d\u0027interactions sociales consistent à faire évoluer un composant émotionnel de l\u0027agent, ce qui se concrétise par l\u0027évolution d\u0027un attribut. Ceci ce représente facilement sous la forme de graphe, de plus RDF propose une classe nommée \"sac\" (bag), permettant de stocker l\u0027ensemble des actions associées à une règle.\nConclusion\nL\u0027avantage majeur du codage en RDF des règles d\u0027interactions sociales est sa simplicité de mise en oeuvre. En effet, grâce à la disponibilité de bibliothèques spécialisées dans ce domaine (comme JENA), il est relativement simple d\u0027exploiter ces connaissances.\nDe plus, la solution proposée offre la possibilité d\u0027exprimer les règles sous la forme d\u0027expressions dont la syntaxe est facilement maîtrisable par un non-informaticien. Ce dernier point est primordial dans notre projet car la manipulation des connaissances psychologiques doit être accessible aux psychologues travaillant sur le projet. L\u0027enrichissement de la base de connaissances nous permettra à terme de valider le modèle. \nRéférences\nSummary\nDesigning an emotional intelligent agent implies to model the expertise of psychologists in term of emotions, cognition and social interactions. This poster presents our work for modeling this knowledge thanks to directed graphs expressed in the RDF language.\n"
  },
  {
    "id": "1073",
    "text": "Introduction\nLe projet RAMCESH est un projet dans lequel sont impliquées diverses organisations ayant trait à la géotechnique. Son objectif est de réaliser un système d\u0027aide à la conception pour les projets géotechniques.\nUne approche de la géotechnique\nOn définit la géotechnique comme l\u0027étude l\u0027interaction d\u0027un sol et d\u0027un construit, qu\u0027il soit ouvrage d\u0027art, bâtiment ou route. Le sol est un système qui défie l\u0027étude en ce qu\u0027il est majoritairement invisible et demande certaines approximations pour être appréhendé : on lui applique le résultats de sondages qu\u0027on estime représentatifs. Le construit lui-même est, en phase de conception de projet, hypothétique, et ses interactions avec le sol sont donc d\u0027autant plus difficiles à évaluer.\nDe surcroît, la géotechnique est un domaine hétérogène à deux titres : elle dépend de conditions régionales, et rassemble des spécialités différentes (chimie, mécanique, géologie, etc…). Cette hétérogénéité ajoute à l\u0027inconnaissabilité une complexité méthodologique et terminologique discernable dans les documents du domaine.\nDans un tel contexte, le spécialiste géotechnicien adopte une attitude pragmatique se reposant sur un ensemble de savoirs et de savoir-faire très souvent tacites, mais essentiellement construits par analogie d\u0027un contexte vis-à-vis d\u0027un autre.\nUn modèle pour la géotechnique\nQui veut modéliser le domaine géotechnique est donc confronté à un problème de grande ampleur ; cependant, la communauté géotechnique aurait l\u0027utilité d\u0027outils informatiques qui puisse l\u0027assister dans la gestion quotidienne de cette complexité.\nIl faut un modèle flexible qui puisse s\u0027adapter aux diversités du domaine et rendre compte des disparités d\u0027usage pour faciliter un accès pertinent aux connaissances du système. De plus, l\u0027expérience du domaine des années 1980 et 1990 avec les systèmes Représentation des connaissances en géotechnique experts a enseigné qu\u0027un système réellement utile aux spécialistes est un système qui serait maîtrisé et implémenté par les spécialistes eux-mêmes, sans passer par un intermédiaire.\nUne solution est donc de choisir un modèle supportant une sémantique riche, mais peu formalisé afin de ne pas dérouter les spécialistes-utilisateurs.\nUne première approche de la modélisation du domaine est donc envisagée à l\u0027aide d\u0027une ontologie informelle structurée qui tienne compte des variations terminologiques au moyens d\u0027ensembles de synonymes et cas d\u0027usage spécifiques fondés sur des emplois métaphoriques (métonymies, synecdoques, etc…) très courants dans la documentation du domaine.\nCette ontologie repose sur deux hiérarchies, subsomption et agrégation, et supporte un formalisme nommé granule de connaissances, dont le rôle est de représenter la connaissance contextuellement, en situation.\nLe granule de connaissances\nLe granule de connaissances rassemble des concepts définis dans l\u0027ontologie en plus de relations spécifiques. Articulé autour de deux clauses (prémisses et conclusions) liées par une relation d\u0027implication (qui peut servir à définir une hiérarchie de causalité au niveau des granules eux-mêmes), il subdivise les concepts ontologiques en fonction d\u0027un rôle thématique et d\u0027un rôle prédicatif. Le thème, obligatoire, est représenté par un concept unique et correspond à la définition sommaire « ce dont il s\u0027agit ». Le prédicat peut être vide ou rassembler autant de concepts que nécessaire et correspond à la définition « ce qui est dit du thème ». Chaque ensemble thème-prédicat définit ce qu\u0027on appelle une phrase, liée à d\u0027autres phrases au sein de la même clause par un ensemble de booléens.\nLes prémisses définissent un contexte spécifique, les conclusions décrivent leur implication. Cet ensemble est extrait des documents du domaine.\nUn granule rassemble également les instances et valeurs associées au contexte décrit. L\u0027approximation d\u0027un contexte s\u0027opère avec la variation des éléments conceptuels du granule selon les hiérarchies de l\u0027ontologie (opération baptisée « glissement sémantique »). L\u0027agrégation de granules de connaissances autorise également la représentation de projets géotechniques complets. La méthode utilisée pour agréger des granules est similaire à celle de l\u0027agrégation des knowledge components.\nLes travaux en cours concernent les premiers développements collaboratifs de granules de connaissance par les spécialistes du domaine et l\u0027utilisation d\u0027une ontologie descriptive d\u0027environ 5000 mots.\n"
  },
  {
    "id": "1075",
    "text": "Introduction\nLa classification supervisée constitue un problème d\u0027apprentissage classique. On dispose dans ce cas, en plus des variables descriptives (ou endogènes), d\u0027une variable cible (ou exogène). En phase d\u0027exploration des données, c\u0027est la dépendance de la variable cible vis-à-vis des variables descriptives qu\u0027on vise à expliciter. En phase de modélisation, le but est de fournir la meilleure prédiction possible pour toute nouvelle instance à classifier. Quelle que soit la situation, la connaissance est à extraire d\u0027un échantillon de N instances étiquetées.\nUne méthode de classification usuelle est la règle de classification suivant le plus proche voisin introduite par Fix et Hodges (1951). Elle consiste à attribuer à une instance l\u0027étiquette de l\u0027instance la plus proche parmi celles constituant l\u0027échantillon. La mise en oeuvre de cette modélisation soulève deux questions fondamentales : -Quelle mesure de similitude employer ? -Quelles instances de l\u0027échantillon conserver ? La première question couvre plusieurs champs d\u0027investigation : gestion de la présence jointe de variables continues et symboliques, normalisation des variables continues, prétrai-tement des variables symboliques, pondération de la contribution des variables, etc. Dans le cas continu, l\u0027usage a consacré l\u0027emploi de la distance euclidienne et des distances L p (p ? 1) de Minkowski. La distance de Mahalanobis (Duda et al., 2001), en effectuant une transformation globale des instances et au prix d\u0027un coût de calcul plus élevé, permet d\u0027intégrer dans le calcul de la similitude les corrélations entre couples de variables descriptives. Pour des mesures successives d\u0027une même quantité, le Dynamic Time Warping est un procédé traitant la corré-lation temporelle (Berndt et Clifford, 1996). Dans le cas symbolique, la distance de Hamming est d\u0027autant plus simplificatrice que le nombre de modalités des variables croît. C\u0027est pourquoi des mesures de similitude basées sur les probabilités d\u0027occurence sont souvent utilisées, par Stanfill et Waltz (1986) entre autres. Un procédé de gestion de la mixité (variables continues et symboliques) est proposé par Wilson et Martinez (1997a). On ne s\u0027intéresse pas dans cet article à la question du choix d\u0027une mesure de similitude et on se focalise sur la sélection d\u0027instance.\nLa classification par le plus proche voisin impose, pour chaque nouvelle instance à classifier, de parcourir l\u0027ensemble de l\u0027échantillon. Ceci entraîne un coût de déploiement rédhibi-toire. Une étape de sélection des instances permet de diminuer le coût de recherche. Classiquement, les méthodes prédictives s\u0027attellent à qualifier le degré d\u0027utilité prédictive d\u0027une instance et conservent les instances jugées utiles. Ces méthodes sont décrites à la section 2.\nLes instances sélectionnées (ou : prototypes) induisent une partition de Voronoi de l\u0027espace, à chaque groupe étant associé un prototype. L\u0027ensemble des instances se répartit dans les groupes de cette partition. On considère alors la distribution des étiquettes dans chacun des groupes. Là où les méthodes prédictives ne prennent en compte que l\u0027étiquette du prototype , on associe à chaque prototype cette distribution de probabilité. Cette dichotomie reflète celle observée dans tout processus de fouille de données entre la phase de préparation des données et celle de modélisation (Chapman et al., 2000). On présente à la section 3 une approche descriptive de l\u0027évaluation de la qualité de telles fonctions, ainsi que le critère qui en découle. La question du sur-apprentissage étant prise en charge par le critère, on propose à la section 4 une heuristique d\u0027optimisation poussée. Enfin, une comparaison expérimentale sur données réelles est menée à la section 5.\nLa sélection d\u0027instances\nLa méthode CNN (pour Condensed Nearest Neighbor) décrite par Hart (1968) est la plus ancienne méthode de sélection d\u0027instances. Toute instance mal classifiée par son plus proche voisin parmi les prototypes déjà sélectionnés est aussitôt conservée. Ce procédé incrémental est itéré tant qu\u0027il existe des instances mal classifiées par l\u0027ensemble de prototypes. La méthode est consistante, dans le sens où tout élément de l\u0027échantillon est bien classifié par son plus proche prototype. La complexité au pire de cet algorithme est un O(N 3 ). Une amélioration est proposée par Gates (1972) : RNN (pour Reduced Nearest Neighbor). Une fois la règle CNN appliquée, toute suppression d\u0027un prototype ne provoquant aucune mauvaise classification d\u0027une instance est validée. L\u0027intérêt de cette méthode réside dans sa capacité à produire un sous-ensemble de prototypes de taille minimale relativement à la condition de consistance, sous réserve qu\u0027un tel sous-ensemble soit inclus dans la solution proposée par CNN.\nLes décisions prises par ce type de techniques sont peu robustes (conservation du bruit, par exemple). Afin d\u0027y remédier, une idée consiste à prendre en compte les K plus proches voisins (typiquement, K \u003d 3). Le procédé est décrémental et une instance est éliminée si elle est mal classifiée par un vote à la majorité sur ses K plus proches voisins. C\u0027est la règle ENN, pour Edited Nearest Neighbor, présentée par Wilson (1972 Dans ce but également, la notion d\u0027association est utilisée par Wilson et Martinez (1997b). Pour K ? N est fixé, si x est l\u0027un des K plus proches voisins de y, on dit que y est associé à x. Autrement dit, x est associé à y s\u0027il participe à la classification de y. Dès lors, x est éliminé si le nombre de ses associés bien classifiés ne diminue pas après sa suppression. La règle ENN est préliminairement appliquée. De plus, les instances sont considérées par ordre décroissant de distance à la plus proche instance de classe différente. La méthode obtenue, DROP3, est de complexité un O(KN 2 ). A la croisée des chemins entre IB3 et DROP3, on trouve un test statistique évaluant l\u0027hypothèse de non contribution d\u0027une instance à la classification de ses associés (Sebban et al., 2002). Ce critère est paramétrique et son calcul nécessite l\u0027approximation de la densité de la statistique associée. Une adaptation de l\u0027algorithme AdaBoost permettant de traiter des classifieurs locaux (les prototypes) aboutit à une heuristique de recherche incrémentale. Dans sa version la plus rapide, l\u0027algorithme est de complexité un O(KN 2 ). Cameron-Jones (1995) a proposé un critère d\u0027évaluation de la qualité prédictive d\u0027un ensemble de prototypes. L\u0027approche adoptée est de type MML (pour Minimum Message Length) et le critère obtenu s\u0027écrit :\navec K le nombre de prototypes, N le nombre d\u0027instances, E le nombre d\u0027instances mal classifées par leur plus proche prototype et J le nombre de classe cibles. La quantité F (U, V ) mesure la longueur du mot de code nécessaire à la spécification de U instances parmi V et est évaluée par la formule :\nu où log * (x) désigne la somme des termes positifs log 2 (x), log 2 (log 2 (x)), etc. Les termes K log 2 (J) et E log 2 (J ?1) correspondent aux longueurs de code nécessaires à la spécification des étiquettes des K prototypes et des E exceptions respectivement.\nUne heuristique est également proposée, qu\u0027on nomme ici Explore. Une première phase itérative consiste à ajouter une instance si la valeur du critère diminue. Une fois toutes les instances considérées, tout prototype dont la suppression conduit à la diminution de la valeur du critère est effectivement éliminé. Enfin, 1000 mutations sont évaluées et acceptées si la valeur du critère décroît. Une mutation est soit un ajout d\u0027une instance à l\u0027ensemble des prototypes, soit une suppression d\u0027un prototype, soit un échange entre une instance et un prototype. Au final, la méthode est de complexité un\nUne approche descriptive\nL\u0027approche prédictive classique consiste à évaluer la qualité prédictive d\u0027un classifieur, en mesurant le risque structurel empirique par exemple. C\u0027est le parti pris par l\u0027ensemble des mé-thodes de sélection. On s\u0027intéresse pour notre part à la qualité de la distribution des étiquettes conditionnellement aux instances. Dès lors, une mesure de cette qualité doit être proposée et on adopte ici une approche descriptive.\nNotations\nSi P ? X, la partition de Voronoi V (P ) \u003d (V (p)) p?P associée à P est définie par :\nPour p ? P , la cellule de Voronoi V (p) contient les points x pour lesquels p est l\u0027élément de P le plus similaire, relativement à ?. L\u0027élément p est appelé prototype de la cellule V (p). La figure 1 donnent des exemples de telles partitions.\nFIG. 1 -Exemple de partitions de Voronoi.\nOn définit ici un modèle (descriptif) comme un couple (v, ?) où v est une partition de Voronoi formée de K cellules et ? une matrice de taille (K, J) dont le coefficient (k, j) donne la probabilité de l\u0027étiquette j dans la cellule k. Autrement, à chaque cellule (i.e. à chaque prototype) est associée une distribution de probabilité sur L.\nSi v est une partition de Voronoi composée de\n-424 -RNTI-E-6\nFormalisation\nOn considère le modèle comme étant aléatoire et on cherche à spécifier la probabilité jointe\n. Cette probabilité est décomposée à l\u0027aide de la formule des probabilités itérées. Du fait qu\u0027on s\u0027intéresse à cette probabilité, et non à la probabilité de mauvaise classification du classifieur associé, comme décrit par Vapnik (1996), on qualifie l\u0027approche de descriptive.\nPlus précisément, si K désigne le nombre de cellules de V , on commence par écrire :\nce qui permet de comparer des ensembles de prototypes de différentes tailles. On itère ensuite la dépendance en utilisant la formule de Bayes :\nOn suppose les comportements des distributions dans chaque cellule conditionnellement indépendants, ce qui donne :\nleurs étiquettes. On applique une nouvelle fois la règle de Bayes et on obtient : \nSpécification\n) a été décomposée à l\u0027aide de la formule de Bayes. On spécifie maintenant chacune des probabilités, en précisant à chaque étape le support de la probabilité concernée et en appliquant un a priori uniforme.\nEn ce qui concerne le nombre de cellules, i.e. la probabilité P (K/D (x) ), les valeurs possibles de K sont comprises entre 1 et N . L\u0027application de l\u0027a priori uniforme donne :\nLa partition de Voronoi est caractérisée uniquement par ses prototypes. Les ensembles de prototypes considérés sont les parties de D (x) . Adopter un a priori uniforme nous conduirait à introduire le coefficient binomial N K , puisque l\u0027on doit choisir K prototypes parmi les N instances. Mais ce coefficient est symétrique relativement à K. Comme on préfère les valeurs faibles de K, on utilise le coefficient N +K?1 K?1 , croissant avec K, proche de N K pour les faibles valeurs de K, nul pour K \u003d 1, caractérisant ainsi plus finement notre préférence :\nDans la k eme cellule, on exploite la dépendance aux données et on restreint le support des distributions possibles aux probabilités rationelles avec N k pour dénominateur. Formellement, le support est\nLa partition et les fréquences des classes cibles dans chaque cellule sont à ce stade connues. Il reste à spécifier les étiquettes de chaque instance dans chaque cellule. Dans chaque cellule, le support est restreint relativement à la dépendance : pour la k eme cellule (1 ? k ? K), le problème revient à placer les éléments de la cellule dans J urnes, sous la contrainte d\u0027effectif N kj dans la j eme urne (1 ? j ? J). Le coefficient multinomial donne le nombre exact de ces possibilités et on obtient :\n, un modèle descriptif M est évalué par la formule suivante :\nLe premier terme correspond à la description du nombre de groupes, le second à la description des prototypes, le troisième à la description des fréquences des étiquettes dans les cellules et le dernier à la description de l\u0027attribution des étiquettes aux instances dans les cellules. Notons que, d\u0027après l\u0027approximation de Stirling (log x! ? x log x?x+O(log x)), le dernier terme de la formule se comporte asymptotiquement comme N fois l\u0027entropie conditionnelle de la distribution des Y n en connaissance de la fonction d\u0027assignement associée à la partition :\nHeuristique d\u0027optimisation\nOn dispose d\u0027un critère évaluant tout sous-ensemble de l\u0027ensemble des instances. L\u0027espace de recherche a pour cardinal 2 N , rendant la recherche exhaustive peu réaliste. On propose une nouvelle heuristique, encapsulant une optimisation gloutonne descendante d\u0027un ensemble de prototypes dans une méta-heuristique de recherche à voisinage variable.\nOptimisation gloutonne d\u0027un ensemble de prototypes\nL\u0027heuristique gloutonne GLOUTON(P ) s\u0027applique à tout ensemble P de p prototypes. Chaque ensemble obtenu par suppression d\u0027un élément de P est évalué. Parmi ces ensembles, celui minimisant le critère est déclaré vainqueur de l\u0027étape. Ce procédé est itéré par application aux vainqueurs successifs jusqu\u0027à l\u0027évaluation finale d\u0027un singleton. Le meilleur ensemble rencontré lors du parcours est renvoyé. Autrement dit, l\u0027algorithme GLOUTON(P ) s\u0027écrit :  (N p log p).\nA chaque étape, toute suppression d\u0027un prototype conduit à réattribuer uniquement les instances appartenant à la cellule de ce prototype. A l\u0027étape K, les N instances ne sont donc à traiter qu\u0027une fois, pour un coût qu\u0027on peut rendre constant.\nEn effet, si l\u0027on dispose pour chaque instance de la liste triée des éléments de P par distance croissante, l\u0027acquisition du plus proche prototype suivant se fait à coût constant. L\u0027algorithme GLOUTON(P ) se voit donc adjoindre une phase d\u0027initialisation qui devient prépondérante avec une complexité temporelle un O(N p log p) (construction de N listes triées de taille p). Le stockage de ces N listes induit une complexité spatiale en O(N p).\nLa valeur du critère est également mise à jour à coût constant. Seuls les deux derniers termes dépendent de la répartition des instances dans les cellules. La suppression d\u0027un prototype conduit tout d\u0027abord à soustraire sa participation à la valeur du critère. Ensuite, la réattribution d\u0027une instance à son plus proche prototype k suivant induit une simple incré-mentation unitaire des compteurs N k et N kj0 , où j 0 est l\u0027indice de la classe à laquelle appartient l\u0027instance. Le terme du critère porté par le prototype k est donc mis à jour en ajoutant log(N k + J) ? log(N kj0 + 1).\nNotons que l\u0027on peut introduire dans GLOUTON une contrainte de préservation de certains prototypes : si P ? P , GLOUTON(P ,P ) n\u0027évalue la suppression d\u0027un prototype que si celui-ci n\u0027appartient pas à P . Dans l\u0027optique d\u0027une inclusion dans une méta-heuristique, cette modification permet de limiter la redondance de la recherche.\nRecherche à voisinage variable\nL\u0027heuristique gloutonne est par nature susceptible de s\u0027empêtrer dans un optimum local. Il est donc naturel d\u0027envisager la remise en question de la solution proposée par GLOUTON. Pour cela, on applique la méta-heuristique de recherche à voisinage variable décrite par Hansen et Mladenovic (2001). Celle-ci consiste à modifier localement une solution et à réappliquer l\u0027heuristique de base, ici l\u0027heuristique gloutonne. Si on n\u0027obtient pas ainsi de meilleure solution, on réitère en explorant un voisinage plus éloigné. Sinon, on réitère en considérant un voisinage de taille minimale de la nouvelle meilleure solution. Le nombre d\u0027étape est usuellement contrôlé par une valeur maximale spécifiée par l\u0027utilisateur.\nUne notion de voisinage d\u0027une solution doit être définie. Pour un ensemble P 0 de p prototypes, un voisin est tout ensemble de prototypes P \u003d P 1 P 2 tel que P 1 est inclus dans P 0 et P 2 est un ensemble d\u0027instances appartenant aux cellules de V (P 0 ) associées aux éléments de P 0 \\ P 1 . Si t ? [0, 1], le voisinage V t (P 0 ) contient tous les voisins P \u003d P 1 P 2 de P 0 tels qu\u0027une proportion t de prototypes dans P 0 est remplacée par une proportion t d\u0027instances dans l\u0027union des cellules correspondantes (cf Figure 2).\nFIG. 2 -Exemple de partitions voisines pour t \u003d 0.35. (a) Répartition des instances dans les cellules de la partition. (b) 2 prototypes (soit 35% des prototypes) sont remis en cause et 3 instances (soit 35% des instances appartenant aux cellules associées) les remplacent. (c) Partition voisine obtenue.\nUn unique paramètre Niveau quantifie le degré d\u0027optimisation souhaité par l\u0027utilisateur. Une incrémentation unitaire de ce paramètre revient à doubler le temps consacré à l\u0027optimisation. L\u0027algorithme RVVGLOUTON(N iveau), de complexité au pire un O(2 N iveau N 2 log N ), s\u0027écrit alors :\nExpérimentation\nOn évalue les méthodes de sélection d\u0027instances selon trois axes : la performance prédictive (i.e. le taux de bonne classification en test), le taux de compression (i.e. le rapport du nombre de prototypes au nombre d\u0027instances) et la robustesse (i.e. le rapport du taux de prédiction en test au taux de prédiction en apprentissage). Ces indicateurs sont estimés par validation croisée stratifiée à 10 niveaux. Notre méthode est comparée à IB3, DROP3, Explore et la règle de classification par le plus proche voisin NN. Le niveau de RVVGLOUTON est fixé à 5, ce qui donne un temps de calcul du même ordre que celui d\u0027Explore. Les jeux de données sont issus de l\u0027UCI (Blake et Merz, 1996). Les jeux de données utilisés (tableau 3) sont ceux pour lesquels la performance pré-dictive de la règle de classification par le plus proche voisin est significativement supérieure à celle du prédicteur majoritaire (qui attribue à toute nouvelle instance la classe majoritaire sur l\u0027échantillon). Afin d\u0027éviter toute interférence sur les résultats relative au choix de la distance, du type de prétraitement, etc, on ne considère que des jeux de données sans valeurs manquantes, que les variables continues. La distance de Minkowski L 1 fait office de mesure de similitude.\nRVVG\nLe taux de bonne prédiction est reporté dans le tableau 1 et le taux de compression moyen dans le tableau 2. Les méthodes classiques, représentées par IB3 et DROP3, réalisent une compression de l\u0027ordre de 20 à 30 pour cent, avec une perte en terme de performance prédictive (69.2% et 71.5% respectivement contre 74.8% pour la classification par le plus proche voisin). Notre méthode sélectionne un minimum d\u0027instances (1.7% contre 2.5% pour Explore en moyenne) sans que la performance prédictive n\u0027en soit affectée.\nLes méthodes se plaçant dans le cadre de l\u0027apprentissage de modèles sont donc plus performantes que les méthodes classiques basées sur des définitions (nécessairement heuristiques) d\u0027utilité individuelle. L\u0027approche descriptive adoptée ici permet de gagner encore en compression par rapport à Explore. Ceci conduit à améliorer la robustesse (tableau 2).\nLa fiabilité du résultat est une propriété importante, que l\u0027approche descriptive permet encore d\u0027améliorer. Ceci est d\u0027autant plus intéressant que les modèles considérés (des fonctions de probabilités conditionnelles) sont plus riches que les modèles usuels (des classifieurs) : à chaque prototype est associé une distribution de probabilité sur les étiquettes. Notre méthode extrait donc plus de connaissance, et de manière plus fiable. Ceci rend profitable son utilisation en phase de préparation des données, là où les méthodes prédictives sont inadaptées. \nConclusion\nLa classification suivant le plus proche voisin repose sur la construction d\u0027une partition de Voronoi. Dans cet article, nous avons proposé un critère d\u0027évaluation des partitions induites par les ensembles de prototypes inclus dans l\u0027ensemble des instances formant l\u0027échantillon. L\u0027approche descriptive adoptée ayant permis de faire porter la gestion du sur-apprentissage par le critère, nous avons également proposé une heuristique d\u0027optimisation poussée de ce critère.\nLes expériences sur jeux de données de l\u0027UCI ont montré que notre méthode est compétitive en terme de performance prédictive, tout en sélectionnant un minimum d\u0027instances. L\u0027étude de la robustesse a également illustré le fait que la méthode ne sur-apprend pas : la décision prise est fiable et pertinente. Cela concourt à l\u0027emploi et au déploiement de modèles de classification par le plus proche voisin. De par la richesse de la connaissance extraite, l\u0027utilisation de la méthode n\u0027est pas limitée à la prédiction.\n"
  },
  {
    "id": "1076",
    "text": "Introduction\nA l\u0027heure actuelle, les données arrivent plus vite que la capacité de traitement des algorithmes de fouille de données ne permet de les traiter. L\u0027amélioration des performances des algorithmes de fouille de données est indispensable pour traiter de grands ensembles de données. Nous nous intéressons au cas de la classification supervisée et plus particulièrement à une classe d\u0027algorithmes : les SVM [Vapnik, 1995]. En règle générale, ils donnent de bons taux de précision mais, l\u0027apprentissage des SVM se ramène à résoudre un programme quadratique et est donc coûteux en temps et mémoire. Pour remédier à ce problème, les méthodes de décomposition [Platt, 1999], [Chang et Lin, 2003] travaillent sur des sousensembles arbitraires de données, on utilise alors des heuristiques [Do et Poulet, 2005] permettant de choisir les sous-ensembles de données. D\u0027autres travaux visent à construire des algorithmes incrémentaux [Fung et Mangasarian, 2002] dont le principe est de ne charger qu\u0027un petit bloc de données en mémoire à la fois, de construire un modèle partiel et de le mettre à jour en chargeant consécutivement des blocs de données. Les SVMs parallèles et distribués ] utilisent un réseau de machines pour améliorer les performances. Nous présentons un nouvel algorithme de SVM linéaire et non-linéaire pour traiter de grands ensembles de données dans un temps restreint sur du matériel standard. A partir de l\u0027algorithme de Newton-GSVM [Mangasarian, 2001]  (i \u003d 1, 2, …, m) dans l\u0027espace réel en dimension n. La matrice diagonale de ±1 D [mxm] représente les classes y i des m individus. I est la matrice identité, e est le vecteur colonne de 1, c est une constante positive, z est la variable de ressort (slack) et w, b sont les coefficients et le scalaire de l\u0027hyperplan. Le paragraphe 2 présente le principe de l\u0027algorithme de Newton-GSVM. Ensuite, nous décrivons une version incrémentale de l\u0027algorithme de Newton-GSVM dans le paragraphe 3 puis la construction de l\u0027algorithme parallèle et distribué de Newton-GSVM. Les résultats numériques sont présentés dans le paragraphe 5 avant de conclure sur nos travaux. \nMéthode de Newton pour GSVM\n(1) où e est un vecteur colonne de 1 Les distances des erreurs sont notées par des variables de ressort (z i ? 0 ; i\u003d1, 2, …, m). Si l\u0027individu x k est du bon côté de son plan support, alors z k est égal à 0. La recherche de l\u0027hyperplan optimal se ramène à simultanément maximiser la marge et minimiser les erreurs. La formulation primale du problème est exprimée par le programme quadratique (2) :\noù une constante c \u003e 0 est utilisée pour contrôler la marge et les erreurs. Le plan optimal (w, b) est obtenu par la résolution du programme quadratique (2) dont la mise en oeuvre est coûteuse en temps et mémoire vive. L\u0027algorithme de generalized SVM (GSVM) [Mangasarian, 1998] modifie l\u0027algorithme de SVM en maximisant la marge par (1/2) ||w, b|| 2 et minimisant les erreurs par (c/2) ||z|| 2 . On obtient alors la formule primale de GSVM : \n, on peut réécrire le problème (5) en : [Mangasarian, 2001] a proposé d\u0027utiliser la méthode itérative de Newton pour résoudre efficacement le problème d\u0027optimisation (6). Le principe de la méthode de Newton est de minimiser successivement les approximations au second ordre de la fonction objectif ? en se basant sur un développement de Taylor au second ordre au voisinage de X v .\nOn minimise la fonction quadratique ?(X), ce qui fournit (8) pour que la dérivée première ?\u0027(X) soit égale à zéro.\nA l\u0027itération p, on construit ? p , approximation quadratique ? au voisinage de X p , que l\u0027on minimise pour obtenir X p+1 , défini par (8). Pour minimiser la fonction ?(X) dans le problème d\u0027optimisation (6), on calcule d\u0027abord la dérivée première\n. L\u0027algorithme itératif de Newton pour GSVM est construit à partir des calculs de la dérivée première (9)  \nL\u0027algorithme incrémental en ligne de Newton-GSVM linéaire peut traiter de très grands ensembles de données sans difficulté. Entre deux étapes incrémentales ne sont conservées en mémoire que la dérivée première ?\u0027(X), vecteur colonne de taille (n+1) et le Hessien ?\u0027\u0027(X), matrice de taille (n+1)x(n+1). Si l\u0027on se place dans le cas où les données sont telles que le nombre d\u0027individus est beaucoup plus important que le nombre de dimensions, la matrice ?\u0027\u0027(X) conserve une taille raisonnable car seulement fonction du nombre de dimensions ce qui explique les bonnes performances de l\u0027algorithme de Newton-GSVM linéaire incrémental dans ce cadre d\u0027utilisation.\nParallélisation et distribution de l\u0027algorithme incrémental de Newton-GSVM\nLa quantité de données stockées ne cesse de croître, à l\u0027heure actuelle elle dépasse parfois les possibilités de traitement. Pour pouvoir faire face à cet afflux, une solution est de paralléliser et distribuer le processus de fouille. Nous avons parallélisé l\u0027algorithme incrémental de Newton-GSVM. Les deux caractéristiques incrémentale et parallèle de cet algorithme permettent à la fois d\u0027optimiser au mieux l\u0027utilisation de la mémoire (grâce à l\u0027aspect incrémental) et le temps d\u0027exécution (grâce à l\u0027aspect exécution en parallèle).\nSoit un grand ensemble de données découpé en k blocs lignes A 1 , D 1 , …, A k , D k et distribué sur plusieurs machines distantes (PC 1 , PC 2 , … PC k ). Sur chaque machine distante, le bloc de données peut être traité en une seule fois ou être encore re-décomposé en blocs lignes. Cet aspect est intéressant pour la mise en oeuvre du calcul sur un ensemble de machines disparates : on adapte les données à la capacité mémoire disponible. Le calcul du modèle (partiel) est effectué sur chaque machine distante et le résultat envoyé au serveur qui effectue la mise à jour de la solution pour chaque itération de l\u0027algorithme. Le résultat final est absolument identique à ce que l\u0027on aurait obtenu en utilisant l\u0027algorithme séquentiel sur l\u0027ensemble de données. Pour chaque itération on calcule parallèlement et indépendamment sur les machines distantes les\n. Ces résultats sont envoyés au serveur qui met à jour la solution et renvoie ce résultat aux machines distantes pour le calcul de la prochaine itération si la condition d\u0027arrêt (?\u0027(X) ? 0) n\u0027est pas vérifiée. Nous avons choisi une solution très simple basée sur le mécanisme XML-RPC pour pouvoir travailler directement sur des entrepôts de données hétérogènes distribués sur le WEB, et donc lancer des portions de calcul sur les sites distants quel que soit le système d\u0027exploitation et l\u0027architecture des machines.\nRésultats\nL\u0027ensemble du programme est écrit en C/C++ sous Linux (PC) avec la librairie Lapack++. Nous nous intéressons ici à évaluer les performances en temps d\u0027exécution et taux de bonne classification en fouille de grands ensembles de données. Les jeux de test ont donc été effectués avec les ensembles de très grandes tailles : nous avons utilisé Ringnorm [Delve, 1996] pour générer des ensembles de données de dix mille à 1 milliard d\u0027individus en dimension 20 avec 2 classes où la classe 1 a une moyenne égale à 0 et une variance égale à 4 et la classe -1 a une moyenne égale à 2/sqrt(20) et une variance égale à 1. Une description des ensembles de données est fournit dans le tableau 1. Les tests ont été réalisés sur des PCs Pentium-4 (3 GHz, 512 Mo RAM).\nNous avons découpé l\u0027ensemble de données en blocs (égaux pour chaque machine). Ensuite nous avons fait varier la taille des blocs (les données sur une machine peuvent être traitées en une fois ou par morceaux). Dans le pratique, si la taille des blocs est trop grande alors il y a saturation de la mémoire, le système d\u0027exploitation passe tout son temps à « swapper ». La taille des blocs a donc une forte influence sur la vitesse d\u0027exécution des algorithmes, de même que les caractéristiques matérielles de la machine utilisée. Dans la classification non linéaire des données Ringnorm, [Do et Poulet, 2005]  \nConclusion et perspectives\nNous avons présenté un nouvel algorithme incrémental, parallèle et distribué de Newton-GSVM pour traiter linéairement et non linéairement des grands ensembles de données dans un temps restreint sur du matériel standard : un milliard d\u0027individus en 20 dimensions sont classifiés linéairement en deux classes en 43 minutes et non linéairement en 25 heures sur dix Pentium-4. L\u0027apprentissage incrémental permet de traiter de très grandes quantités de données sans difficulté de mémoire sur une machine standard. Le traitement parallèle et distribué utilise un groupe de machines standard pour améliorer les performances en temps d\u0027exécution. La complexité de l\u0027algorithme varie linéairement avec le nombre d\u0027individus de l\u0027ensemble de données, le nombre de machines utilisées et le carré du nombre de dimensions. Avec une tâche de classification linéaire, un milliard d\u0027individus (voire plus) sont classifiés sans difficulté sur des PCs. Dans le cas non linéaire, l\u0027algorithme prend une matrice de noyau en entrée au lieu de la matrice de données. La complexité de l\u0027algorithme et la qualité du modèle dépendent de l\u0027ensemble des vecteurs support en entrée. Il faut rechercher de bonnes\n"
  },
  {
    "id": "1077",
    "text": "Contexte\nL\u0027ensemble des mandats d\u0027intervention en lien avec une mauvaise qualité de l\u0027air dans les espaces fermés et leurs différentes solutions constitue notre base documentaire qui symbolise l\u0027image représentative de la connaissance et du savoir faire des experts. Le raisonnement que nous mobilisons autour de cette base pour notre système d\u0027aide à la décision est le raisonnement à partir de cas. L\u0027interface interactive que nous avons développée nous permet de sauvegarder l\u0027ensemble des cas dans un formalisme XML reproduisant la structure logique des plaintes. Les modèles de balises correspondent aux différentes rubriques citées dans les textes des plaintes (antécédents, symptômes, environnement, etc.). Le RàPC s\u0027appuie d\u0027abord sur le module de remémoration des cas de pollution similaires au contexte de la plainte courante. Ce module est fortement lié au contenu et à la structure des cas situés en mémoire. Le RàPC repose aussi sur le module d\u0027adaptation qui ajuste le rapport du cas jugé le plus similaire au contexte du cas courant. Il est constitué également du module de consensus assurant la validation humaine multi-experte des cas, et du module d\u0027apprentissage du nouveau cas pour capitaliser la nouvelle expérience en mémoire.\nPhase de remémoration\nNous nous basons sur la mesure de similarité conceptuelle de (Zarga et Salotti, 2004) pour établir ensuite une distance sémantique entre le problème énoncé dans la plainte cible et les problèmes sources. Inspirées des travaux de (Wu et Palmer, 1994) Zarga et Salotti utilisent une mesure de spécificité qui favorise les liens père-fils par rapport aux autres liens :\noù Depth btm (PPS(C 1 , C 2 )) est le nombre maximum d\u0027arcs séparant le concept bottom ( le concept le plus bas du réseau conceptuel) du plus petit subsumant de C 1 et C 2 ; PPS(C 1 , C 2 ). Dis(C 1 , C 2 ) est la distance en nombre d\u0027arcs entre C 1 de C 2 .\n-727 -\nRNTI-E-6\nSystème d\u0027aide à la décision : surveillance des ambiances intérieures 2.1 Modèle de proximité flou (Mercier \u0026 Beigbeder, 2004) estiment que plus les termes de la requête apparaissent proches au niveau d\u0027un élément de la base plus ce dernier est pertinent par rapport à la requête cible. Le modèle de proximité flou du terme A par rapport au terme B est formalisé par:\n(t) désigne l\u0027ensemble des positions prises par le terme t, et k une constante fixe choisie représentant la taille de la fenêtre glissante des cooccurrences des termes.\nNotre nouveau modèle: le modèle de proximité flou sémantique\nLa mesure de (Mercier \u0026 Beigbeder, 2004) est très intéressante, néanmoins elle ne tient pas compte de la sémantique des termes (dans le cas où des termes sémantiquement proches des termes utilisés dans la requête apparaissent directement proches au niveau d\u0027un élément de la base). En effet, ce modèle est limité par la relation de cooccurrence directe des termes qui ne permet pas de capturer la proximité sémantique entre les mots. L\u0027équation présentée dans le modèle de Mercier et Beigbeder devient :\nSimens(A) est l\u0027ensemble des termes proches de A suivant la mesure d\u0027appariement conceptuelle utilisée. Notre nouveau modèle apporte la connaissance de la sémantique au modèle existant. les résultats des degrés de proximité que nous avons obtenus à l\u0027étape d\u0027expérimentation sur des données réelles et sur des données simulées sont très améliorés à l\u0027aide de l\u0027augmentation sémantique. \nRéférences\nSummary\nWe present a new information retrieval model based on the semantic proximity level of term occurrences by using a conceptual network to identify the closest cases of the used request. Using this framework, we aim to implement a case based reasoning approach to help decision making in situations where indoor air is suspected to be responsible of health effects.\n"
  },
  {
    "id": "1078",
    "text": "Introduction\nAujourd\u0027hui, les techniques d\u0027analyse et d\u0027intégration de données sont devenues des atouts majeurs pour les entreprises et les services gouvernementaux. En effet, ces techniques permettent un gain de temps pour regrouper et croiser l\u0027information distribuée. Dans le domaine du développement durable, ces techniques sont notamment indispensables afin de rassembler et d\u0027analyser les pratiques agricoles et ainsi garantir la traçabilité des pratiques. Plus précisément nos travaux se situent dans le cadre d\u0027un projet Dans cet article, nous nous plaçons dans le cadre d\u0027un système de médiation suivant une approche Local As View (LAV), où les vues sont décrites via des requêtes sur le schéma global. Cette approche est connue pour être flexible car l\u0027ajout et la suppression de sources de 1 Ce projet est réalisé en collaboration avec le Cemagref, http ://www.cemagref.fr/ 2 http ://www.acta-informatique.fr/ -77 -RNTI-E-6\nFouille de données pour la réécriture de requêtes données n\u0027affectent pas le schéma global. Nous nous intéressons plus particulièrement au problème de la réécriture de requêtes en termes de vues en présence de contraintes de valeurs. Les contraintes de valeurs correspondent à la notion de type énuméré en base de données. Elles permettent de spécifier les valeurs autorisées pour un attribut donné. Les contraintes de valeurs sont utiles dans beaucoup d\u0027applications, comme par exemple pour la vérification des contraintes d\u0027intégrité ou pour exprimer une forme d\u0027information incomplète (Borgida et Patel-Schneider, 1994  ). Nous présentons un nouveau prédicat, nommé P 2 , issu de la formalisation du problème du calcul de ces nouveaux cas de réécriture dans le cadre de Mannila et Toivonen (1997). Ce prédicat P 2 , en conjonction avec le prédicat P 1 présenté dans (Jaudoin et al., 2004) et rappelé ici, permet de résoudre le problème de réécriture. -Enfin, nous décrivons succintement une implémentation de notre approche de réécriture qui exploite et adapte un algorithme de fouille de données existant, Apriori. Nous pré-sentons ensuite les résultats de nos expérimentations qui viennent conforter l\u0027intérêt des techniques de fouilles de données dans notre cadre. Les premiers résultats montrent la capacité de notre prototype à passer à l\u0027échelle en supportant le traitement d\u0027un grand nombre de vues (jusqu\u0027à 15000). Notons que dans la littérature, peu d\u0027articles présentent les résultats expérimentaux de leurs approches de réécriture. Ils se concentrent généra-lement sur les résultats théoriques. A notre connaissance, (Pottinger et Halevy, 2001) est une des rares références qui décrit l\u0027évaluation des performances d\u0027un algorithme de réécriture. Aussi la réalisation d\u0027un prototype et son expérimentation constituent à notre avis une contribution dans le domaine de la réécriture.\nLa suite de l\u0027article est organisée comme suit. Dans la section 2, nous donnons les prérequis né-cessaires à la formalisation du problème de la réécriture de requêtes en présence de contraintes de valeur dans le cadre des logiques de description. La section 3 reformule ce problème dans un cadre de découverte de connaissances dans les bases de données. La section 4 présente l\u0027implémentation et les expérimentations réalisées. En section 5, une conclusion et des perspectives sont données. Les démonstrations des lemmes et théorèmes de cet article sont donnés dans Jaudoin et al. (2005).\nRéécriture de requêtes en présence de contraintes\nPrérequis sur les logiques de description\nLes logiques de description sont un formalisme de représentation des connaissances qui permet de représenter des structures complexes et de raisonner avec elles (Baader et al., 2003). Elles permettent de décrire un domaine d\u0027application à l\u0027aide de concepts (prédicats unaires) et de rôles (prédicats binaires). Une logique de description est définie par un ensemble de constructeurs. Dans cet article, nous nous intéressons à la logique ALN (O v ), dont les constructeurs sont listés dans la table 1, colonne 1, où C est un ensemble de noms de concepts, N un ensemble de noms de valeurs, R C un ensemble de noms de rôles dont l\u0027image est un concept de C et R V un ensemble de noms de rôles qui prennent leurs valeurs dans N .\nConstructeurs\nSémantique\nTAB. \nPlus précisément, la sémantique des constructeurs de ALN (O v ) est donnée dans la colonne 2 de la table 1. Une interprétation est un modèle pour un concept C ssi C I \u003d ?. Un concept est inconsistant ssi C I \u003d ? pour toute interprétation I.\nEtant donnée cette sémantique, il est possible de définir la notion\nterminologie. Une terminologie est un ensemble fini d\u0027axiomes terminologiques de la forme :\nDans cet article, nous supposons que les terminologies sont acycliques, i.e. aucun concept ne fait référence à lui-même directement ou indirectement dans sa définition ou dans sa spécification. La sémantique des terminologies est obtenue en étendant la notion d\u0027interprétation aux axiomes terminologiques comme suit. Une interprétation I est un modèle pour une terminologie T ssi I est un modèle pour chaque axiome de T .\nPour traiter le problème de la réécriture dans la logique ALN (O v ), nous nous appuyons sur la forme normale donnée dans Jaudoin et al. (2005). La forme normale d\u0027un concept permet d\u0027exprimer un concept sous une forme canonique. Cette forme normale transforme tout concept C en concept ou en une conjonction d\u0027atomes de la forme ?w.P avec w un mot défini sur l\u0027ensemble des rôles de R C ? R V 5 et P est soit un concept atomique A ou une restriction de cardinalité (? nR ou ? nR) ou un ensemble de valeurs E. Par la suite, on note ?w.P ? C si ?w.P apparaît dans la description du concept C.\nRéécriture dans ALN (O v )\nDans cette section, nous définissons le cadre de médiation et le problème de la réécriture dans le cadre des logiques de description. Puis nous donnons les caractéristiques des réécri-tures dans ce contexte. Le schéma global S est une terminologie formée de définitions de concepts dans ALN (O v ). Une requête Q est un concept dans ALN (O v ). Q est décrite en termes des concepts de S. De plus Q est supposée être dans sa forme normale. V est une terminologie formée de spécifications primitives dans ALN (O v ). Les spécifications primitives de V permettent de décrire les vues. Les vues sont décrites en termes de S et sont supposées être données dans leur forme normale.\nDans ce contexte, on cherche à répondre à une requête Q en ayant uniquement connaissance des vues de V. Une technique pour répondre à Q est de reformuler Q en une expression qui utilise uniquement les vues de V. L\u0027expression obtenue est appelée réécriture.  (Levy et al., 1996). Informellement, cette approche fonctionne de la manière suivante. Etant donné une requête Q ? ?w 1 .P 1 . . . ?w n .P n , la principale idée est de considérer chaque atome ?w i .P i de Q isolément. A chaque atome ?w i .P i de la requête est associé un panier/ensemble qui contient toutes les réécritures conjonctives maximales de cet atome. Ensuite dans une deuxième étape, les réécritures candidates de Q sont calculées en effectuant le produit cartésien entre les paniers. Ceci permet d\u0027obtenir un sur-ensemble de toutes les réécritures conjonctives maximales de Q. Pour obtenir effectivement les réécritures conjonctives maximales, les réécritures inconsistantes et non maximales doivent ensuite être supprimées de ce sur-ensemble.\nLes travaux présentés dans Jaudoin et al. (2005)  \n6 au sens de l\u0027inclusion ensembliste 7 la profondeur d\u0027un atome ?w.P est égal à la longueur du mot w, i.e. le nombre de rôles dans w.\n-81 -\nRNTI-E-6\nFouille de données pour la réécriture de requêtes . Aussi, dans la suite de l\u0027article, nous nous concentrons sur les problèmes du calcul des réécritures engendrées par les contraintes de valeurs car ces problèmes posent de nouvelles difficultés en termes de réécriture de requêtes.\nSoit une requête Q ? ?w.P où P est un ensemble de valeurs E ou une restriction de cardinalité ? n R v . Considérons maintenant le problème de la création du panier B(w, P ). Nous nous intéressons aux problèmes du calcul des réécritures des cas (b.1) et (b.2) du lemme 1. Pour définir plus précisément ces problèmes, nous introduisons l\u0027ensemble V w \u003d {V 1 , ..., V p } qui désigne le sous-ensemble de vues de V telles que ?i ? {1, ..., p},\nLe problème E_conj_rewrite(E,w) correspond au calcul des réécritures de type (b.1). Il est défini comme suit :\nLe problème E_conj_rewrite(E,w) consiste à calculer les plus petites conjonctions de vues de V w subsumées par ?w.E.\nLe problème N_conj_rewrite(n,wR) correspond au calcul des réécritures de type (b.2). Il est defini comme suit :\nLe problème N_conj_rewrite(n,wR) consiste à calculer les plus petites conjonctions de vues de V wR subsumées par ?w. ? n R.\nL\u0027exemple suivant illustre les solutions des problèmes présentés ci-dessus.\nExemple 3 Soit une requête Q telle que Q ? ?numDepartement.E ?aRecu. ? 3 typeP roduit, avec E \u003d {63, 43, 03}. Supposons qu\u0027il existe 4 vues V i , i ? {1, ..., 4} telles que V i ?numDepartement.E i où E 1 \u003d {23, 15, 18, 80, 43, 03}, E 2 \u003d {03, 63}, E 3 \u003d {01, 07, 11, 43, 63}, E 4 \u003d {26, 63}, et 3 vues V i , i ? {5, 6, 7} telles que V i ?aRecu.typeP roduit.E i pour i ? {5, 6, 7} où E 5 \u003d {P 1 , P 10 , P 15 , P 20 , P 27 }, E 6 \u003d {P 1 , P 10 , P 15 , P 20 , P 26 }, E 7 \u003d {P 1 , P 10 , P 15 , P 26 , P 27 }. \nIci on a\nDe la même manière à partir de V aRecu.typeP roduit \u003d {V 5 , V 6 , V 7 }, on obtient la solution de N_conj_rewrite(3,aRecu.typeProduit) : V 5 V 6 V 7 . En effet, l\u0027intersection des ensembles E 5 , E 6 , E 7 donne un ensemble dont la cardinalité est inférieure à 3. Ainsi V 5 V 6 V 7 appartient au panier B(aRecu, (? 3typeP roduit)).\nDans la section qui suit, nous montrons comment les deux problèmes E_conj_rewrite(E,w)\net N_conj_rewrite(n,wR) se rattachent à un cadre de découverte des connaissances dans les bases de données.\nVers la mise en place de techniques de fouille de données\nLe cadre de Mannila et Toivonen (1997)\nPour rattacher les problèmes énoncés précédemment à un cadre de découverte de connaissances, nous nous appuyons sur le cadre théorique introduit dans Mannila et Toivonen (1997). Il formalise un problème basique de découverte de connaissances dans des bases de données, qui peut être énoncé de la manière suivante : Soit r une base de données, L un langage pour exprimer des propriétés ou définir des sous-groupes des données, et P un prédicat de sélection. Le prédicat P permet d\u0027évaluer si une phrase X ? L est \"intéressante\" dans r. L\u0027objectif est de trouver la théorie de r selon L et P , i.e. l\u0027ensemble T h(r, L, P )\u003d{ ? ? L |P (r, ?) est vrai}, qui correspond à l\u0027ensemble des phrases intéressantes de r.\nSoit une relation de spécialisation/généralisation, i.e. un ordre partiel , sur les motifs de L. On dit que X généralise Y et que Y spécialise X quand X Y . Soit S un ensemble de phrases de L tel que si ? ? S et ? ? alors ? ? S. Alors S peut être représenté par sa bordure positive Bd + (S) ou sa bordure négative Bd ? (S).\nLa bordure positive correpond aux éléments les plus spécifiques de la théorie, tandis que la bordure négative correspond aux éléments les plus généraux de la théorie. Elles permettent chancune de retrouver toutes les phrases X ? L \"intéressantes\" dans r, i.e. celles pour lesquelles P (r, X) est vrai. Notons que si le prédicat P de T h(r, L, P ) est anti-monotone par rapport à (i.e. si ?X, Y ? L tels que X Y et P red(r, Y ) est vrai alors P red(X, r) est vrai), alors la théorie peut être représentée par ces bordures. Ce cadre peut être appliqué à de multiples problèmes (Mannila et Toivonen, 1997), comme par exemple le problème de la découverte des motifs fréquents (Agrawal et al., 1993). La section qui suit s\u0027attache à montrer comment le problème de la réécriture peut se ramener à une formulation ensembliste, puis être transposé dans le cadre précédemment introduit.\nFormulation des problèmes de réécriture dans un cadre de décou-verte de connaissances\nFormulation ensembliste de la réécriture\nPour un mot w, un ensemble E, un entier n donnés, nous cherchons maintenant à donner une formulation ensembliste des problèmes E_conj_rewrite(E,w) et N_conj_rewrite (n,w). Pour reformuler plus précisément ces problèmes, nous introduisons les définitions suivantes :\nS 1 (w, E) caractérise les plus petits sous-ensembles de F w dont l\u0027intersection des éléments est contenue dans E, tandis que S 2 (w, n) caractérise les plus petits sous-ensembles de F w dont la cardinalité de l\u0027intersection des éléments est inférieure à n. Le lemme suivant caractérise les solutions de E_conj_rewrite(E,w) et N_conj_rewrite(n,wR) avec les ensembles S 1 (w, E) et S 2 (wR, n).\nLemme 2 Soient n un entier, w et wR des mots et E un ensemble. Soient deux problèmes E_conj_rewrite(E,w) et N_conj_rewrite(n,wR). Soit\nLa formulation ensembliste du problème N_conj_rewrite(n,wR) est illustrée dans l\u0027exemple qui suit.\nExemple 5 Reprenons l\u0027énoncé de l\u0027exemple 3. On a F aRecu.typeP roduit \u003d {E 5 , E 6 , E 7 }. On a S 2 (aRecu.typeP roduit, 3) \u003d {{E 5 , E 6 , E 7 }}. On retrouve alors la solution de N_conj_rewrite(3,aRecu.typeProduit\nDans la section suivante, nous montrons comment cette représentation ensembliste peut être transposée dans le cadre de Mannila et Toivonen (1997).\nCalcul des réécritures dans un cadre de découverte de connaissances\nIdentification de S 1 (w, E) Dans ce contexte, le premier ensemble S 1 (w, E) peut se ramener au cadre de découverte de connaissances précédent de la manière suivante :\n-la relation r est vide.\n-le langage L w est l\u0027ensemble des parties de F w , i.e. P(F w ).\n-le prédicat, noté P 1 , est défini de la façon suivante : Soient X ? L w , X \u003d {E 1 , ..., E k } et E un ensemble de valeurs.\nLa théorie T h(?, L w , P 1 ) est alors l\u0027ensemble des éléments de F w qui vérifient le prédicat P 1 . De plus, le prédicat P 1 étant anti-monotone, les notions de bordure positive et négative s\u0027appliquent ici. Le théorème suivant permet de caractériser S 1 (w, E) en fonction de la bordure négative.\nThéorème 1 Soit le problème E_conj_rewrite(E,w). S\nNous pouvons de la même manière caractériser S 2 (w, n) dans le cadre théorique de Mannila et Toivonen (1997).\nIdentification de S 2 (w, n) Comme précédemment, la relation r est vide, L w consiste en l\u0027ensemble des parties de F w \u003d {E 1 , ..., E p }, et la relation d\u0027ordre est l\u0027inclusion. Nous introduisons un nouveau prédicat P 2 (n, X) défini comme suit :\nLe prédicat P 2 (n, X) est anti-monotone par rapport à l\u0027inclusion, ce qui garantit l\u0027existence des bordures. Par conséquent, le théorème suivant donne une caractérisation de S 2 (w, n) en fonction de la bordure négative. N_conj_rewrite(n,w).\nThéorème 2 Soit le problème\nL\u0027exemple qui suit illustre la formulation de S 2 (w, n) dans le cadre introduit ci-dessus.  (Agrawal et Srikant, 1994) pour trouver les solutions de E_conj_rewrite(w,E). Cet algorithme est l\u0027algorithme classique de découverte des motifs fréquents. Il effectue un parcours par niveau de l\u0027espace de recherche, et utilise une stratégie d\u0027élagage à partir de motifs de la bordure négative pour limiter le nombre de motifs générés. Les avantages de cet algorithme pour résoudre notre problème sont multiples. Cet algorithme, tout en recherchant les motifs fréquents, découvre uniquement les motifs de la bordure néga-tive, ce qui n\u0027est pas le cas d\u0027une grande partie des autres approches. De plus, sa stratégie et son efficacité ne dépendent pas du prédicat étudié. A l\u0027opposé, une grande partie des autres algorithmes fondent leur efficacité sur des techniques propres au prédicat \"être fréquent\". L\u0027efficacité de ce type d\u0027approche pour un autre prédicat est donc difficilement prévisible.\nL\u0027implémentation d\u0027Apriori utilisée est une adaptation de l\u0027implémentation C++ de Borgelt (2003). Cette implémentation est reconnue pour être l\u0027implémentation la plus efficace d\u0027Apriori actuellement (Goethals et Javeed Zaki, 2003;Bayardo et al., 2004). L\u0027implémenta-tion initiale d\u0027Apriori a été modifiée de façon à rendre l\u0027algorithme indépendant du prédicat étudié. Plus concrètement, pour pouvoir appliquer Apriori à un nouveau prédicat, il suffit de définir les opérations propres à ce prédicat, et de le passer en paramètre de l\u0027algorithme. Actuellement, en plus de différents prédicats liés aux motifs fréquents, le prédicat P 1 a été implémenté permettant ainsi de trouver les solutions de E_conj_rewrite(w,E) par Apriori. L\u0027avantage de notre implémentation est donc de faciliter l\u0027utilisation d\u0027Apriori pour résoudre d\u0027autres problèmes que ceux de fouille de données, en évitant d\u0027avoir à réécrire à chaque fois l\u0027algorithme.\nExpérimentations Nous nous concentrons ici sur l\u0027expérimentation de la phase de résolu-tion de E_conj_rewrite(w,E). Cette phase étant l\u0027une des plus coûteuse, son étude va nous permettre d\u0027estimer le nombre de vues pouvant être traitées. Notons que cette borne correspond au nombre de vues que l\u0027algorithme de réécriture a identifié comme étant pertinentes à la réécriture d\u0027un atome de la forme ?mot.valeurs, i.e. les vues de V mot . Ainsi cette borne conditionne uniquement la taille de l\u0027entrée d\u0027Apriori et ne fait pas figure de limite sur le nombre de vues que l\u0027algorithme de réécriture peut traiter.\nLes expérimentations ont été réalisées sur des jeux de données synthétiques. Les jeux d\u0027essais ont été créés à l\u0027aide du générateur aléatoire d\u0027Oracle, de façon à ce que la cardinalité des contraintes de valeurs soit égale à un entier n ou comprise entre 1 et un entier n tel que n ? {10, 20, 30, 40}. On a mesuré les temps d\u0027exécution d\u0027Apriori sur ces jeux d\u0027essais. Ces expérimentations ont été réalisées sur un pentium IV pro 2.6 Ghz avec 3 Go de mémoire.\nComme le montre la figure 1, lorsque la taille des contraintes de valeurs est petite, il est possible de prendre un grand nombre de vues en entrée (e.g. 15000 vues pour des contraintes de taille inférieure à 10). Dès que la taille des contraintes augmente, le nombre de vues que peut traiter l\u0027algorithme, diminue (figure 1). En effet, plus la taille des contraintes est grande, plus les contraintes risquent de s\u0027intersecter et que cette intersection ne soit pas incluse dans E. Par conséquent, le nombre de motifs intéressants est susceptible d\u0027être important et le programmme dans ce cas, nécessite plus d\u0027espace mémoire que disponible. Notons que lorsque les contraintes de valeurs sont de taille fixe (figure gauche de la figure 1), Apriori est mis en difficulté plus rapidement que lorsque les contraintes sont de taille variable. Ceci s\u0027explique par le fait que par exemple pour des contraintes de cardinalité au plus 10, les contraintes sont composées en moyenne de 5 valeurs.\nD\u0027une manière générale, ces jeux ont montré qu\u0027il est possible de prendre en entrée pour la réécriture des atomes de la forme ?mot.valeurs, jusqu\u0027à 15000 vues. Néanmoins, il est difficile de comparer les performances de notre prototype avec d\u0027autres applications de réécri-ture de requêtes dans la mesure où dans ce domaine les résultats théoriques ont toujours primé sur les résultats expérimentaux. A notre connaissance, (Pottinger et Halevy, 2001) est une des \nConclusions et perspectives\nDans cet article, nous avons confirmé l\u0027intérêt des techniques de fouilles de données pour traiter le problème de la réécriture en présence de contraintes de valeurs. En effet dans ALN (O v ), de nouveaux cas de réécritures engendrés par les contraintes peuvent bénéficier d\u0027une formulation dans le cadre de découverte de connaissances de Mannila et Toivonen (1997). De plus, l\u0027implémentation de notre approche basé sur une implémentation générique d\u0027Apriori permet de traiter un grand nombre de vues et d\u0027envisager le passage à l\u0027échelle de notre algorithme de réécriture. Toutefois, l\u0027exécution d\u0027Apriori devient problématique quand une grande partie de l\u0027espace de recherche doit être parcourue, i.e. quand il existe de grands motifs intéressants. Dans de telles configurations, pour le problème de la découverte des fré-quents, des algorithmes ont été proposés afin de trouver plus efficacement les motifs de grande taille (Han et al., 2000;Uno et al., 2003;Flouvat et al., 2004). L\u0027adaptation de certaines de ces approches pourrait donc permettre de traiter un nombre plus important de vues.\n"
  },
  {
    "id": "1079",
    "text": "Refléter le vocabulaire du métier\nTeximus Expertise incorpore un outil interactif qui permet aux experts d\u0027un domaine de travailler en utilisant les concepts clés de leur métier. Le logiciel reflète exactement ces concepts et, plus important encore, leur interrelation.\nDans le domaine de la formation, ce vocabulaire parlera de cours, de module, de matériel, d\u0027exercices, d\u0027évaluations, de suggestions, de demandes de changements, de clientèles cibles, de variantes, de versions, etc.\nDans une application Teximus, chacun de ces concepts est reflété directement dans la base de données.\nEnvironnement interactif de capture\nLes caractéristiques d\u0027édition permettent la saisie sur-le-champ et de façon intuitive, tout en garantissant la cohérence de l\u0027information. Pour chaque type de contenu du domaine d\u0027expertise, l\u0027outil définit des fiches de connaissance qui permettent d\u0027entrer l\u0027information. Dès qu\u0027un concept est défini, il est possible de saisir l\u0027information, instantanément.\nTeximus Expertise utilise également les fiches de connaissance pour créer des liens hypertextes, de façon intuitive et sans entretien. Si un objet change de nom, tous les liens hypertextes qui s\u0027y réfèrent changent aussi automatiquement, peu importe leur emplacement, qu\u0027ils soient à l\u0027intérieur d\u0027un texte ou d\u0027une image.\nPrésentation Web instantanée\nTeximus Expertise inclut un assistant à la présentation pour la création de pages Web qui permettent de visualiser et d\u0027éditer le contenu. L\u0027assistant permet d\u0027agencer et de cacher les\nTeximus Expertise : un logiciel de gestion de connaissances sections, de sélectionner le style des liens hypertextes, et supporte la traduction en plusieurs langues.\nSpécifications techniques\nClient Web\nLes auteurs et les lecteurs peuvent utiliser Internet Explorer ou les fureteurs de la série Mozilla, dont Firefox et Netscape 7. La modélisation de la connaissance et l\u0027édition requiè-rent Internet Explorer v.5.5 ou plus récent. SVG sert à afficher les modèles (logiciel SVG gratuit sur www.adobe.com)\nServeur Web\nUn serveur « libre » (open source) est inclus dans la configuration comme serveur Web par défaut. Il s\u0027agit du serveur Resin 3 de Caucho Technologies. En production, Resin peut être utilisé seul ou de concert avec IIS ou Apache.\nBases de données\nTeximus Expertise est testé avec les bases de données Oracle 9, DB2 version 7. \nMatériel\nL\u0027application est hébergée sur un serveur. Pour les prototypes : Pentium IV 2GHz ou mieux; mémoire : 1GB ou mieux; pour la production, 2GB est recommandé. L\u0027espace disque nécessaire varie selon l\u0027application.\nSummary\nTeximus Expertise software is an advanced tool for dynamic knowledge management. It is based on a semantic repository. This integrated suite makes easier knowledge and information sharing across organizations.\n"
  },
  {
    "id": "1080",
    "text": "Introduction\nLes données traitées par l\u0027analyse statistique implicative (en abrégé : A.S.I.) se présentent sous forme de tableaux numériques croisant une population E de sujets, ou individus ou objets, associé chacun à une ligne, et un ensemble V de variables simples ou conjointes (attributs binaires, variables numériques, rang, intervalle) chacune associée à une colonne. A l\u0027intersection de la ligne x et de la colonne j figure la valeur prise par le sujet x selon la variable j. La finalité première de l\u0027A.S.I. vise à dégager de V ou de l\u0027ensemble de toutes les conjonctions d\u0027éléments de V 1 , des règles d\u0027association non symétrique, contrairement à la similarité, sur une base statistique, du type : « si la variable ou une conjonction de variables a est observée sur E alors la variable b a tendance à être observée », règle notée a ? b. Une mesure de qualité, non symétrique, de telles règles 2 est définie par :\nQ(a,b) est le nombre aléatoire de contre-exemples à l\u0027implication (cf. l\u0027algorithme de la vraisemblance du lien de I.C. Lerman (Lerman, 1981a). Ce critère d\u0027admissibilité est comparable à celui du philosophe des sciences H. Atlan dans « A tort et à raison. Intercritique de la science et du mythe », Seuil, 1986. Il écrit : « … [en accord avec Jung] si la fréquence des coïncidences n\u0027excède pas de façon significative la probabilité qu\u0027on peut leur calculer en les attribuant au seul hasard à l\u0027exclusion de relations causales cachées, nous n\u0027avons certes aucune raison de supposer l\u0027existence de telles relations ». . La distribution de la variable aléatoire Q(a,b) dépend des hypothèses de tirage : par exemple, une loi hypergéométrique ou une loi binomiale, ou une loi de Poisson (Lerman et al., 1981b) -360 -RNTI-E-6 R. Gras et al. \nFIG.1 Graphe implicatif à 7 variables FIG.2 Hiérarchie cohésitive à 7 variables\n-ou bien plus ou moins typiques du comportement moyen de la population ; en d\u0027autres termes, le comportement de ces sujets sera ainsi en harmonie avec le comportement statistique de la population à l\u0027origine de la classe C, -ou bien contribuant le plus à la constitution de C ; en d\u0027autres termes, plus ou moins responsables de l\u0027agrégation conduisant à C.\nUne approche comparable est faite pour étudier la typicalité et la contribution des sujets et des variables supplémentaires à la constitution d\u0027un arc ou d\u0027un chemin du graphe 4 .\nPuissance implicative de classe et de chemin\nCouples génériques\nL\u0027idée directrice suivie consiste à porter notre attention sur les « lignes de force », (ou, selon une autre métaphore : les « lignes de crête ») des associations, plutôt que de les retenir avec le risque afférent d\u0027être submergé par leur nombre et contraint par les bruits qui les accompagnent. Plaçons-nous à un niveau k de la hiérarchie où viennent de se réunir, pour \nMais, dans chaque sous-classe de C, existe également un couple générique. Précisément, si C est constituée de g (g?k) sous-classes (C comprise), il y a g couples 4 Le travail présenté ici diffère de celui de (Gras et al., 1996a) par la distinction de ces deux notions. Pour l\u0027étude de la responsabilité du sujet dans la similarité, voir par ex. (Lerman, 1981a). 5 Nous convoquons l\u0027intensité ? mais toute la suite est valable avec l\u0027intensité dite classique ? . 6 C\u0027est ce couple, généralement unique, qui intervient par le sup. dans le calcul de l\u0027implication de A sur B (Gras et al, 1996b  Ainsi, à x, nous pouvons associer g nombres ? x,1, ? x,2,..., ? x,g correspondant aux g valeurs respectivement prises par x selon les g règles génériques de la classe ou du chemin C.\nDéfinition 3 :\nLe vecteur ( ? x,1, ? x,2,..., ? x,g) est appelé vecteur contingent générique de x ou puissance implicative de x sur C. Le sujet théorique xt qui admettrait ( ? 1, ? 2,...., ? g) comme vecteur contingent générique est appelé sujet typique optimal En effet, on peut interpréter ce vecteur comme étant celui d\u0027un individu « typique » des règles génériques puisque les valeurs prises par ce sujet selon ces règles sont exactement celles de l\u0027ensemble de la population. Ce sujet, image conforme de E, n\u0027existe pas réellement en général. Dans ces conditions, on peut munir l\u0027espace des puissances [0,1] g d\u0027une métrique afin d\u0027obtenir un contraste accentuant les effets de fortes intensités génériques ou, réciproquement, minorant les effets d\u0027une faible intensité générique.\nDéfinition 4 : On appelle distance de typicalité d\u0027un sujet quelconque x à la classe ou\nDans le logiciel CHIC, le calcul des typicalités (et des contributions) se fait cependant en modulant ces valeurs, à l\u0027aide d\u0027une fonction ad hoc, afin de mieux prendre en compte la sémantique des valeurs attribuées par x à a et à b. Par exemple, pour a\u003d0 et b\u003d1, la fonction prend, dans CHIC, la valeur 0.682.\nCe nombre, qui vérifie formellement les 3 axiomes d\u0027une distance, n\u0027est autre également que la distance du type ? 2 entre les deux distributions {1-? i } i et {1-? x,i } i qui expriment les écarts entre les implications génériques contingentes et l\u0027implication stricte. Elle exprime, aussi et en particulier, l\u0027écart observé sur les règles génériques entre le sujet considéré x et le sujet théorique typique optimal, écart nuancé par ces intensités. C\u0027est pour cette raison que nous avons choisi le mot typicalité pour quantifier le comportement de x selon les règles génériques. Nous allons le préciser plus loin. Lorsque ? i \u003d1, une légère correction sur cette valeur permet d\u0027éviter la division par zéro (par exemple, prendre ? i \u003d 0.99999999) ce qui ne change pas fondamentalement la distance.\nRemarque : Une classe C étant donnée, on peut définir une structure d\u0027espace métrique sur E par la donnée de la distance indicée par C entre deux sujets quelconques de E, distance qui mesure la différence de comportement des sujets x et y à l\u0027égard de C :\nOn voit alors que la distance de typicalité donnée plus haut n\u0027est que la spécification de d C aux sujets respectivement x et x t . La distance d C permet de conférer à E une C-structure topologique discrète. Cette topologie est équivalente à celle qui serait définie sur l\u0027ensemble des vecteurs contingents (? x,1 , ? x,2 ,...,? x,g) , sous-ensemble d\u0027un espace vectoriel normé de dimension g et de norme :\n. L\u0027opérateur symétrique associé à la forme quadratique qui conduit à cette distance, a pour matrice la matrice diagonale\n-1 pour i\u003d1,…,g -Il est bien évident que la somme de deux tels vecteurs n\u0027a qu\u0027un sens théorique, c\u0027est-à-dire hors du contexte dans lequel nous travaillons en A.S.I..\nUne application intéressante peut consister à déterminer le ou les sujets appartenant à une boule de diamètre donné et de centre l\u0027un des sujets pré-désignés, comme par exemple, l\u0027individu optimal. En prolongement de cette approche métrique, le problème de complétion des données manquantes pourrait y puiser une solution originale. \nTypicalité\nNous définirons la mesure de typicalité à partir du rapport entre la distance de typicalité relative au sujet considéré et la distance à C la plus grande dans l\u0027ensemble des sujets. Cette distance maximale est celle des sujets y dont les ? y,i sont tous nuls ou très faibles. Ces sujets sont donc les sujets les plus opposés aux règles génériques. La typicalité d\u0027un sujet sera alors d\u0027autant plus grande qu\u0027il s\u0027écartera de ces mêmes sujets, donc qu\u0027il aura un comportement comparable à celui du sujet théorique optimal. La typicalité d\u0027une catégorie de sujets ou d\u0027une variable supplémentaire G 8 s\u0027en déduira :\nDéfinition 5 : La typicalité de x à C est :\nAfin de donner au chercheur le moyen de savoir ou de vérifier rapidement si telle catégorie de sujets qui l\u0027intéresse est statistiquement déterminante dans la constitution d\u0027une classe implicative ou d\u0027un chemin transitif, un algorithme a été élaboré en s\u0027appuyant sur les deux notions que l\u0027on définit ci-dessous : groupe optimal et catégorie déterminante.\nDéfinition 6 : Soit E la population étudiée. Un groupe optimal d\u0027une classe implicative ou d\u0027un chemin C, groupe noté GO(C), est le sous-ensemble de E qui accorde à C une typicalité plus grande que le complémentaire de GO(C) et qui forme avec celui-ci une partition en deux groupes maximisant la variance inter-classe de la série statistique des typicalités individuelles des sujets les constituant. Une telle partition est dite significative. L\u0027existence de ce groupe optimal est démontrée dans (Gras R. et al., 1996a et b). Les propriétés utilisées sont aussi celles qui le sont pour établir l\u0027algorithme sur lequel se basent les modules des programmes informatiques qui construisent, automatiquement dans C.H.I.C., chaque sous-groupe optimal.\nEn effet, considérons une partition {G i } i de E. Cette partition peut être définie par une variable supplémentaire correspondant par exemple à un descripteur de E à deux ou plus modalités binaires, par exemple des catégories socio-professionnelles. Soit X i une partie aléatoire de E ayant le même cardinal que G i et Z i la variable aléatoire Card (X i ? GO(C)).\nSelon un modèle équiprobable, Z i suit une loi binomiale de paramètres : card G i et card (GO(C)) / card E qui est la fréquence du groupe optimal de la classe ou du chemin C. Définition 7 : On appelle variable supplémentaire ou catégorie la plus typique de la classe implicative ou du chemin C, la catégorie qui minimise l\u0027ensemble {p i } i des probabilités p i telles que:\nAinsi, établir que G j est la catégorie la plus typique revient à déceler, parmi les catégories, celle dont le nombre de sujets appartenant en même temps au groupe optimal est le plus étonnamment grand eu égard à son cardinal. Une catégorie G 0 est dite déterminante au risque ou au seuil ? si la probabilité associée p 0 est inférieure à ?. Autrement dit, le risque de se tromper en affirmant cette propriété est donc au plus égale à ?.\nPar suite, la signification d\u0027une classe ou d\u0027un chemin ayant été donnée par l\u0027expert, il lui associera la sous-population la plus porteuse de ce sens, celle correspondant au risque minimum. Cette approche est comparable à celle de (Lerman,1981a) pour l\u0027analyse des similarités, mais au moyen d\u0027une modélisation et de concepts différents. D\u0027ailleurs, nous pouvons remarquer qu\u0027il est possible d\u0027associer au groupe optimal une variable binaire correspondant à la fonction indicatrice de ce sous-ensemble de E. De la même façon, nous pouvons également associer à la catégorie G i ou bien à la variable supplémentaire correspondante, une variable binaire dont l\u0027indice de similarité s\u003d n a? b ? n a n b n n a n b n , au sens de I.C. Lerman, vérifie : p i \u003dPr[S?s], S étant la valeur aléatoire dont s est la réalisation. Ainsi, minimiser l\u0027ensemble des probabilités {p i } i revient à maximiser l\u0027indice de similarité entre les variables binaires, indicatrices de sous-ensembles, associées respectivement l\u0027une au groupe optimal GO(C) et les autres aux différentes catégories {G i } i .\nCette remarque permet d\u0027étendre efficacement la notion de variable supplémentaire la plus typique à des variables numériques, prenant leurs valeurs sur [0,1]. Il suffit alors d\u0027extraire la plus forte des valeurs de similarité entre la variable binaire indicatrice définie par le groupe optimal et les différentes variables numériques placées en supplémentaire, l\u0027indice étant calculé selon le principe retenu en analyse implicative pour les variables numériques. Nous savons que sa restriction au cas binaire coïncide avec sa valeur s dans le cas où les 2 variables sont binaires. \u003d 0,90 sont plus typiques que ceux qui lui attribuent la valeur 0,98. Ceux-ci sont à une distance plus grande que les premiers pour le comportement statistique de la population. La nuance entre cette notion et celle de contribution définie dans 2.3 prend tout son sens dans l\u0027étude des variables modales ou numériques.\nSpécificité\nSi à chaque classe ou chemin C j on peut associer au moins un groupe typique, il est pertinent de mettre en évidence le couple (variable supplémentaire G i , classe ou chemin C j ) remarquable quant à l\u0027optimalité de sa conjugaison. D\u0027où la définition : Définition 8 : La variable supplémentaire G i étant donnée, le couple (G i , C j ) est dit mutuellement spécifique lorsque G i est la variable la plus spécifique de la règle associée à C j et lorsque la probabilité (le risque) p i k de G i par rapport aux autres classes de la hiérarchie ou aux chemins C k du graphe implicatif est supérieure à un seuil ? (à la discrétion de l\u0027utilisateur).\nUne analyse étant donnée, il peut exister 0 ou plusieurs couples mutuellement spécifiques. Ce ou ces couples offrent l\u0027intérêt de faire porter l\u0027attention de l\u0027expert sur les plus fortes associations prenant origine dans une variable supplémentaire. Définition 9 : De la même façon, un individu x étant donné, le couple (x, C j ) est mutuellement spécifique lorsqu\u0027il appartient au groupe optimal relatif à la règle associée à C j et que sa typicalité à C j est maximale par rapport à toutes ses autres typicalités aux classes de la hiérarchie cohésitive ou aux chemins du graphe implicatif.\nContribution\nCette notion se distingue de la précédente, ce que nous ne faisions pas en 1996, par l\u0027examen de la responsabilité des individus, puis des variables supplémentaires -qui peuvent en être des descripteurs-à l\u0027existence d\u0027une règle ou d\u0027une règle généralisée entre variables principales.\nSupposons, en effet, que deux variables a et b (resp. plusieurs variables sur un chemin du graphe ou bien deux classes de la hiérarchie) soient réunies par un arc sur un graphe à un certain seuil ( resp. en un chemin transitif C du graphe ou bien en une classe C dans une hiérarchie à un certain niveau). Connaissant la valeur ? x,i attribuée par l\u0027individu à la règle i : a ? b (resp. règle i du chemin C ou bien de la classe C constituée de g règles génériques) supposée admissible, on donne la Définition 10 : On appelle distance de contribution de x à (a,b) ou à C : Remarque : A l\u0027instar de ce que nous avons fait pour la typicalité, nous pouvons définir sur E une topologie discrète d\u0027espace normé dont la norme est associée à la distance entre deux 2 1 g i sujets quelconques suivante :\nDéfinition 11 : On appelle contribution de x à C le nombre :\nCette définition est la restriction de celle de la typicalité au cas où, cette fois, on compare le sujet x aux « pires » sujets par rapport aux règles génériques : leur comportement s\u0027oppose à l\u0027implication de chaque règle (1 pour la prémisse et 0 pour la conclusion). Cette contribution a pour maximum 1 dans le cas où l\u0027individu x a donné la valeur 1 à toutes les règles i. Ceci permet de concilier sémantique et définition formelle. En effet, plus la différence est importante, plus le sujet observé a un comportement voisin de celui du sujet théorique optimal et plus il s\u0027éloigne de ceux qui réfutent les règles génériques : on peut donc dire qu\u0027en contribuant à l\u0027émergence de la classe, ils en sont responsables.\nLa suite des définitions et des algorithmes de calcul (contribution d\u0027une catégorie ou d\u0027une variable supplémentaire G, groupe optimal d\u0027individus, catégorie ou variable supplémentaire la plus contributive , couple mutuellement spécifique) se transpose immédiatement à partir des principes de la typicalité et la spécificité. Mais dans les situations réelles, nous observons la nuance entre les deux concepts ce qui enrichit l\u0027information exploitable par l\u0027utilisateur. Cependant, le concept de contribution est plus volontiers retenu pour l\u0027interprétation dans une perspective inductive.\nApplication\nDans le cadre d\u0027une enquête de l\u0027Association des Professeurs de Mathématiques de l\u0027Enseignement Public (APMEP) auprès de professeurs de mathématiques de classes terminales (séries scientifiques S et ES, littéraires LI et technologiques TE sont les variables supplémentaires), nous avons recueilli et analysé (Bodin et al., 1999)   Considérons la classe C\u003d [E ? (OP8 ? OP7)] ? OPX. Son sens, analysé plus en détail dans (Bodin et al., 1999), est fortement marqué par l\u0027importance accordée à l\u0027imagination et à la recherche personnelle, par les enseignants d\u0027accord avec ces objectifs et ces opinions, La variable la plus typique pour cette classe est S (série Scientifique) avec un risque de : 0.00393.\nEn effet, 116 des enseignants de S parmi les 155 de cette série qui ont répondu au sondage, figurent dans le groupe optimal (GO) de cardinal 201 relatif à C. Soit X une partie aléatoire de même cardinal (155) que S et Z la variable aléatoire égale au cardinal de l\u0027intersection de X et du groupe optimal GO. Selon un modèle équiprobable de distribution des enseignants, Z suit la loi binomiale de paramètres 155 et 201/311 soit 0.656. La probabilité pour que Z soit plus grande que 116 est le risque annoncé, soit 0.00393. Mais pour S, c\u0027est le couple (S, (OP8, OP7)) qui est mutuellement spécifique au seuil ? \u003d 2.10 -5 .\nOn retrouve une telle mutuelle spécificité pour TE avec le couple (TE, (B,K)) à un seuil 5.10 -7 nous confirmant, sans surprise, que les enseignants des sections techniques (TE) considèrent que les mathématiques doivent être utiles à la vie professionnelle (B) et, en conséquence, aux autres disciplines (K) et y sont les plus attachés.\nFIG. 3 -Hiérarchie cohésitive significative\nLes calculs de contribution à la classe C montrent que, cette fois, 111 enseignants sur les 311 sondés, participent au groupe optimal. Le nombre d\u0027enseignants de S a diminué (il passe de 116 à 67) et, surtout, sa proportion est bien moindre que précédemment dans le GO. Ceci se ressent dans le seuil qui est 0.0251, soit un risque 6 fois plus élevé que pour la typicalité. Ce sont les enseignants sondés de S qui sont les plus typiques, c\u0027est-à-dire « conformes » au comportement général de la population elle-même sondée. Mais ils sont moins contributeurs dans les relations strictes entre les 4 variables constituant C. Cette remarque nous montre les nuances apportées par les deux concepts : typicalité et contribution Certaines liaisons apparues et commentées ci-dessus se retrouvent dans le graphe de la FIG. 4. Les contributions calculées dans CHIC montrent encore que les enseignants de la série S contribuent le plus au chemin : E ? OP8 ? OP7 ? OPX avec un risque d\u0027erreur de 0.00746, la transitivité le long de ce chemin étant assurée au niveau 0.75.\nConclusion\nLes applications de la méthode A.S.I. ont d\u0027ores et déjà donné des résultats très satisfaisants, non seulement dans la discipline où elle a pris naissance, la didactique des mathématiques, mais aussi dans d\u0027autres domaines de l\u0027éducation ou de recherche scientifique différente (biologie, économie,…) comme l\u0027a montré la 3 ème Rencontre Interna nationale ASI 3 de Palerme en octobre dernier. Le plus souvent, les interprétations des experts s\u0027appuient complémentairement sur l\u0027analyse de similarités ou/et sur les méthodes factorielles, tout en obtenant des informations qui sont spécifiques de l\u0027A.S.I. en raison de son caractère non symétrique. Mais ces méthodes visent un objectif commun : l\u0027accès à la signification d\u0027un tout non réduit à la somme des significations de la somme de ses parties. Les analyses bénéficient efficacement du logiciel C.H.I.C., qui permet, avec une certaine convivialité, tous les traitements algorithmiques et graphiques des questions évoquées dans cet article. Son développement suit régulièrement toutes les nouvelles avancées de la théorie de l\u0027implication statistique. Ses fonctions respectives de révélateur et d\u0027analyseur qui semblent opérer avec bonheur dans de multiples domaines nous promettent encore d\u0027intéressantes perspectives théoriques et appliquées.\n"
  },
  {
    "id": "1082",
    "text": "Problématique\nLe raisonnement à base de règles générales pouvant comporter différentes exceptions et le raisonnement non-monotone sont des domaines qui ont été bien étudiés et formalisés en Intelligence Artificielle. Ainsi, le Système P (Kraus et al., 1990) fournit un ensemble de postulats de rationalité permettant de définir les conclusions plausibles pouvant être obtenues à partir d\u0027un ensemble de règles pouvant contenir des exceptions. De plus, différentes méthodes de raisonnement, en accord avec le Système P , ont été proposées. Une question cependant subsistait : comment obtenir de telles règles à partir d\u0027informations fréquentielles, en d\u0027autres termes, comment apprendre de telles règles ?\nDe récents travaux ont montré comment se baser sur des distributions de probabilités particulières, les distributions de probabilités à grandes marches (Snow, 1999), afin d\u0027obtenir des règles et leurs exceptions. Dans une distributions de probabilités à grandes marches, chaque élément à une probabilité supérieure à la somme des probabilités des évènements qui lui sont moins probables. Contrairement aux approches classiques basées sur les règles associatives, les règles ainsi extraites peuvent être utilisées dans le cadre du raisonnement non-monotone, en accord avec le Système P et avec la base initiale (Benferhat et al., 2003).\nCependant, ces distributions de probabilités à grandes marches ne peuvent être obtenues qu\u0027en regroupant les différents individus de la base (simple) d\u0027apprentissage, chaque regroupement pouvant aboutir à des ensembles de règles différents et incompatibles. À ce jour, aucun algorithme de regroupement réellement satisfaisant, tant d\u0027un point de vue de temps de calcul que des règles générées, n\u0027avait été proposé.\nLes contributions de ce travail se déclinent en deux points principaux : -la proposition de différents algorithmes de regroupement d\u0027où des règles peuvent être générées ; -l\u0027implémentation de ces différents algorithmes afin de les valider (tant sur les temps de calcul que sur la qualité des règles extraites).\nL\u0027extraction des règles depuis la base d\u0027observations se décompose en 3 phases bien distinctes : une phase de dénombrement de chaque observation ; une phase de regroupement de ces observations afin d\u0027obtenir des distributions à grandes marches ; une phase d\u0027extraction des règles à partir du regroupement précédent.\nLa phase de regroupement est une phase déterminante pour la génération de règles. En effet, l\u0027unique moyen d\u0027influer sur la qualité et la pertinence des règles apprises se situe donc dans la construction des différentes classes de la base initiale. Or, il est impossible de calculer tous les regroupements possibles et de choisir le meilleur, le nombre de ces regroupements étant exponentiel.\nLe logiciel. Le logiciel en démonstration et développé par les auteurs, Area, comporte différents algorithmes reposant sur différents facteurs, le but de chacun de ces algorithmes étant de tendre vers un regroupement optimal permettant de générer le meilleur ensemble de règles possible. Le logiciel offre également la possibilité de modifier les regroupements générés par les algorithmes, au moyen d\u0027outils de manipulation des regroupements permettant à un utilisateur de déplacer les observations. Ces fonctions permettent d\u0027affiner les règles apprises par le système.\nIl peut être noté que la robustesse d\u0027une règle dépend du regroupement dont elle est issue : plus la règle provient d\u0027un regroupement avec une population élevée (ie. plus elle est générique) moins elle sera sensible à l\u0027ajout de nouveaux éléments dans la base. \nSummary\nArea is a Java software which purpose is to extract default rules from simple databases. The main property of these rules is to be compatible with the System P of Kraus, Lehmann and Magidor. Hence, these rules can be used with non-monotonic reasoning systems.\n"
  },
  {
    "id": "1083",
    "text": "Introduction\nAvec l\u0027expansion d\u0027Internet et du Web, on assiste à une prolifération des ressources hété-rogènes (données structurées, documents textuels, composants logiciels, images), conduisant à des volumes considérables. Dans ce contexte les outils d\u0027accès à l\u0027information (moteurs Web, SGBD, etc.) délivrent, dans des temps de plus en plus longs, des résultats massifs en réponse aux requêtes des utilisateurs, générant ainsi une surcharge informationnelle dans laquelle il est souvent difficile de distinguer l\u0027information pertinente d\u0027une information secondaire, ou même du bruit.\nUne solution à l\u0027amélioration de cette pertinence est la personnalisation ou l\u0027adaptation des réponses fournies aux utilisateurs selon leurs profils c\u0027est-à-dire selon leurs besoins et leurs préférences 1 . Ainsi la formulation du besoin d\u0027information est devenue un des éléments clés pour obtenir des résultats pertinents dans un processus d\u0027accès à l\u0027information. Pour aider à cette formulation, des travaux Bouzeghoub (2004), Zhu (2000) et Burgess (2002) proposent d\u0027introduire la notion de qualité. Il est par exemple possible de poser une requête en spécifiant des préférences extrinsèques en termes de qualité comme une réponse rapide ou une information fraîche. Ainsi on peut définir un profil qualité comme un ensemble de préférences ou besoins en termes de qualité d\u0027information caractérisant un utilisateur ou groupe d\u0027utilisateurs.\nDans cet article nous proposons un modèle flexible de qualité de l\u0027information décrivant les différents facteurs de qualité influant sur la personnalisation. Ce modèle va permettre de structurer les différents facteurs de qualité dans une hiérarchie afin d\u0027assister l\u0027utilisateur dans la construction de son propre profil selon ses besoins et exigences en terme de qualité.\nDans la section suivante, nous présentons un état de l\u0027art sur les approches existantes sur la modélisation de la qualité des données. La section 3 sera consacrée à la présentation de notre modèle de qualité d\u0027information. Enfin nous terminerons cet article par des conclusions et des perspectives.\nPersonnalisation et qualité d\u0027information\nLa personnalisation de l\u0027information s\u0027exprime par un ensemble de critères et de préfé-rences spécifiques à chaque utilisateur ou une communauté d\u0027utilisateurs. Les données décri-vant les préférences des utilisateurs sont souvent sauvegardées sous forme de profils. Parmi les données du profil on trouve une dimension relative à la qualité Bouzeghoub (2004). Afin de définir les facteurs de qualité relatifs à l\u0027information influant sur la personnalisation, il est nécessaire d\u0027analyser les différents travaux menés sur le thème de la modélisation de la qualité des données.\nModélisation de la qualité des données. La qualité des données est un domaine de recherche qui a suscité depuis longtemps un vif intérêt, mais qui émerge tout juste comme champ de recherche à part entière, tel que peuvent l\u0027indiquer Wang (1997), Jarke (1997) et Berti (1999). Dans le cadre de la modélisation de la qualité des données, de nombreuses propositions ont été faites. La première difficulté réside dans l\u0027absence de consensus sur la notion même de qualité. Tout le monde s\u0027accorde en effet sur le fait que la qualité des données peut se décomposer en un certain nombre de dimensions, catégories, critères, facteurs, paramètres ou attributs, mais aucune définition ne fait aujourd\u0027hui l\u0027unanimité (TAB. 1). Dans Naumann (2000) les auteurs identifient trois approches d\u0027analyse des critères de la qualité des données :\n? approche orientée sémantique : elle est basée uniquement sur la signification des critères. Cette approche est la plus intuitive (il s\u0027agit d\u0027une approche où les critères sont examinés de façon générale, c\u0027est-à-dire séparés de tout cadre d\u0027information). ? approche orientée traitement : elle classe les critères de qualité de l\u0027information selon leur déploiement dans les différentes phases du traitement de l\u0027information. ? approche orientée objectif : elle est caractérisée par une définition des objectifs de la qualité à atteindre et un classement des critères selon les objectifs définis.\nLimites des modèles existants. L\u0027inconvénient des approches proposées pour caractériser la qualité des données semble être une certaine rigidité qui parait ne laisser que relativement peu de choix à l\u0027utilisateur, sans pour autant l\u0027aider à construire un ensemble cohérent et minimal de critères de qualité ou bien l\u0027assister dans leur spécification. En effet elles repré-sentent la qualité comme une collection de critères. La plupart des approches proposées sont limitées dans leur applicabilité. Elles sont utiles seulement dans le domaine pour lequel elles ont été conçues ainsi la réutilisation de la définition de la qualité est limitée. La majorité des définitions proposées de la qualité des données ne distinguent pas le point de vue utilisateur et le point de vue système. Par exemple pour la fraîcheur des données on distingue la fraîcheur comme un point de vue utilisateur et la fréquence de mise à jour des données comme un point de vue système. Cette confusion rend difficile l\u0027intégration de la qualité dans le processus d\u0027exécution des requêtes.  (2000) »Approche orientée traitement » 3 Classes d\u0027évaluation des critères » 11 Critères qualité de données Zhu et Gauch (2000) »Approche orientée sémantique » 5 Critères de qualité des pages web Marotta (2002) »Approche orientée traitement » 2 points de vue : système et utilisateur » 6 Catégories » 31 Critères TAB. 1 -Quelques approches de modélisation de la qualité des données.\nProposition d\u0027un modèle de qualité de l\u0027information\nObjectifs du modèle\nL\u0027objectif de notre modèle est de fournir une définition des facteurs de qualité de l\u0027information, afin de permettre à l\u0027utilisateur de construire son propre profil de qualité et d\u0027avoir ainsi une personnalisation au niveau de la définition et de l\u0027évaluation de la qualité. Dans notre modèle la définition des facteurs de qualité influant sur la personnalisation de l\u0027information repose principalement sur l\u0027hypothèse suivante :\nHypothèse : la définition de la qualité de l\u0027information est relative à l\u0027utilisateur.\nLa définition de la qualité est propre à l\u0027utilisateur c\u0027est-à-dire elle est relative à la satisfaction de ses besoins en termes de choix et d\u0027appréciation des facteurs de la qualité.\nApproche multidimensionnelle pour la qualité\nLa définition des facteurs de qualité influant sur la personnalisation de l\u0027information ne réside pas dans la définition des facteurs de qualité elle-même mais dans la structuration et la représentation de la qualité. En se basant sur notre hypothèse, notre hiérarchie de qualité se décompose en un ensemble de dimensions (FIG. 1). Dans la suite nous proposons les diffé-rentes dimensions de la hiérarchie de qualité de l\u0027information.\nDimensions source\nCe type de dimension décrit la source ou la provenance de la qualité comme source d\u0027information ou support d\u0027information. Elle se décompose en une ou plusieurs dimensions utilisateur ou système. On part du constat que s\u0027il est difficile de garantir la qualité intrinsè-que de l\u0027information on peut déterminer a priori les sources de qualité :\n? support de l\u0027information : les facteurs de qualité liés aux documents. ? Source de l\u0027information : les facteurs de qualité liés aux fournisseurs de l\u0027information (Base de données, Site Web, Bibliothèque numérique...). ? usage de l\u0027information: les facteurs de qualité liés à l\u0027usage des informations comme par exemple les formes de popularité (citation).\nDimensions système\nLes dimensions système décrivent l\u0027ensemble des critères de qualité vis-à-vis du système. En se basant sur le modèle de Naumann et Rolker (1999) nous proposons l\u0027ensemble de critères préliminaires de la qualité (FIG. 1).\nDimensions utilisateur\nLes dimensions utilisateurs sont des dimensions d\u0027agrégation personnalisables par l\u0027utilisateur. Elles se décomposent en une ou plusieurs dimensions utilisateurs ou système. En s\u0027inspirant de la catégorisation de la qualité de Marotta (2002) nous proposons les principales dimensions utilisateur suivantes :\n-la qualité opérationnelle : l\u0027ensemble des facteurs de qualité liés à l\u0027accès à la source d\u0027information ou support d\u0027information. -la qualité du contenu : l\u0027ensemble des facteurs de qualité liés à la source d\u0027information ou support d\u0027information elle -même. -la qualité opérationnelle de l\u0027usage : les diverses formes de popularité liés à l\u0027accès à l\u0027information comme téléchargement ou liens. -la qualité du contenu de l\u0027usage : les diverses formes de popularité liés à l\u0027appréciation du contenu de l\u0027information comme citation.\nEn raison du nombre de dimensions système disponibles dans notre modèle on a besoin d\u0027une simple hiérarchie permettant à l\u0027utilisateur de trouver facilement les dimensions système souhaitées d\u0027où la proposition des sous-dimensions utilisateur suivantes :\n- \nConclusion et perspectives\nDans ce travail, nous avons présenté un modèle flexible de qualité de l\u0027information. La multi-dimensionnalité de la hiérarchie de la qualité proposée permet à l\u0027utilisateur d\u0027obtenir différents points de vue selon différentes dimensions et selon différents niveaux de « curiosité » personnalisables vis-à-vis de la qualité d\u0027information. En termes de perspectives à notre travail nous comptons : établir les métriques et les méthodes d\u0027évaluation des différentes dimensions de qualité ; proposer un modèle formel de représentation et construction d\u0027un profil qualité. \nRéférences\nSummary\nThis work is included in the general problems of information retrieval and more particularly in personalization and quality of information. In this paper we propose a multidimensional model of information quality describing various quality factors influencing the information personalization. This model makes it possible to structure the various information quality factors in a hierarchy in order to assist the user in the construction of his own profile according to its requirements in term of quality. Résumé. Cet article présente un système automatique d\u0027annotation sémantique de pages web. Les systèmes d\u0027annotation automatique existants sont essentiellement syntaxiques, même lorsque les travaux visent à produire une annotation sémantique. La prise en compte d\u0027informations sémantiques sur le domaine pour l\u0027annotation d\u0027un élément dans une page web à partir d\u0027une ontologie suppose d\u0027aborder conjointement deux problèmes : (1) l\u0027identification de la structure syntaxique caractérisant cet élément dans la page web et (2) l\u0027identification du concept le plus spécifique (en termes de subsumption) dans l\u0027ontologie dont l\u0027instance sera utilisée pour annoter cet élément. Notre démarche repose sur la mise en oeuvre d\u0027une technique d\u0027apprentissage issue initialement des wrappers que nous avons articulée avec des raisonnements exploitant la structure formelle de l\u0027ontologie.\nAnnotation sémantique de pages web\nLe système que nous présentons permet d\u0027automatiser l\u0027annotation sémantique de pages web. Notre objectif est de classifier des pages concernant des équipes de recherche, afin de pouvoir déterminer par exemple qui travaille où, sur quoi et avec qui. La classification s\u0027appuie sur des mécanismes de raisonnement qui nécessitent une représentation formelle du contenu des pages ; nous exploitons ainsi une ontologie qui représente les concepts du domaine et les relations entre les concepts dans un langage de représentation des connaissances.\nNotre système génère des annotations sémantiques qui sont des métadonnées sur les élé-ments d\u0027un document liées à une ontologie. Pour cela nous devons résoudre deux grandes questions. La première est d\u0027identifier automatiquement, dans une page web, les éléments qui sont pertinents. La seconde est de déterminer quels sont les concepts de l\u0027ontologie les plus spécifiques possible, pour annoter chacun de ces éléments.\nL\u0027automatisation repose sur un apprentissage à partir d\u0027un corpus constitué d\u0027éléments marqués par un expert. Le marquage associe à chaque concept de l\u0027ontologie des éléments de la page en rapport avec ce concept. L\u0027apprentissage génère un wrapper capable d\u0027annoter des éléments du document sous la forme d\u0027instances de concepts et de rôles de l\u0027ontologie fournie. Des mécanismes de raisonnement exploitant l\u0027ontologie sont utilisés pour déterminer\nFIG. 1 -page web et ontologie présentées à l\u0027expert\nle concept le plus spécifique avec lequel un élément doit être annoté. L\u0027annotation est donc totalement dépendante de l\u0027ontologie fournie.\nDans une première section, nous présentons le processus de marquage de la page par un expert. La seconde section présente l\u0027algorithme d\u0027apprentissage exploitant la structure arborescente d\u0027une page web. La section 3 présente l\u0027annotation de documents dont la structure est similaire. Enfin, nous évaluons notre méthode par rapport aux systèmes d\u0027annotation séman-tique existants.\nGénération d\u0027annotations primaires par marquage\nLa première étape du processus est un marquage permettant de former un corpus d\u0027apprentissage ; il s\u0027agit de fournir au système quelques exemples d\u0027éléments pertinents à partir desquels le système apprend à reconnaître l\u0027ensemble des éléments à annoter. Pour cela, un expert marque des éléments pertinents de la page web, c\u0027est-à-dire correspondant à des concepts de l\u0027ontologie. Il dispose à cet effet d\u0027un outil de visualisation, à la manière d\u0027un navigateur web, qui lui permet de sélectionner un élément dans la page et de choisir dans l\u0027ontologie le concept qui lui correspond. Dans l\u0027exemple figure 1, le marquage est effectué en fonction de l\u0027ontologie SWRC 1 , qui modélise notamment les personnes, organismes et projets d\u0027une équipe de recherche. Pour chaque concept, un nombre suffisant d\u0027éléments pouvant y être associé doivent être marqués ; ce nombre dépend de la régularité de la page d\u0027apprentissage et des pages à annoter ; pour des pages très régulières, 2 ou 3 exemples suffisent pour chaque concept.\nDe manière interne, la page est représentée par son arbre DOM (W3C) dans lequel les noeuds contiennent les éléments de structure HTML et les feuilles les éléments de texte. Un chemin unique est ainsi défini depuis la racine jusqu\u0027à chaque feuille. Lorsque l\u0027expert marque\nFIG. 2 -Arbre DOM et annotations primaires issues du marquage\nun élément la chaîne de caractères sélectionnée, le chemin de la feuille contenant la chaîne et le concept de l\u0027ontologie associé sont enregistrés au format XML ; la figure 2 présente un exemple d\u0027enregistrement du marquage de la page web de la figure 1. L\u0027ensemble de ces éléments marqués sont des annotations primaires qui jouent ainsi le rôle de corpus pour l\u0027algorithme d\u0027apprentissage.\n2 Apprentissage exploitant une structure arborescente Définition d\u0027un chemin dans l\u0027arbre issu du DOM L\u0027algorithme d\u0027apprentissage est dérivé des travaux de Kushmerick et al. (1997) sur l\u0027induction de wrappers. Un wrapper est une procédure utilisant les régularités syntaxiques d\u0027un document pour identifier des éléments. Là où les travaux initiaux s\u0027appuyaient sur des structures à plat, en considérant le document comme une suite de chaînes de caractères, notre système exploite la structure arborescente fournie par la représentation DOM de la page web.\nLe DOM permet de définir le chemin de chaque élément (noeud ou feuille) de l\u0027arbre. Pour chaque élément, nous définissons ce chemin comme un ensemble d\u0027étapes depuis la racine. Chaque étape est un couple (balise :position) défini à partir de l\u0027étape précédente (on considère l\u0027étape 0 comme étant la racine du document). La position est le numéro du fils du noeud défini à l\u0027étape précédente tandis que la balise est la balise HTML que le noeud représente. Par exemple, une page web contient un élément racine \u003chtml\u003e qui a deux fils, \u003chead\u003e et \u003cbody\u003e. Le chemin de l\u0027élément \u003cbody\u003e est donc body : 1. Cette définition de chemin est celle employée pour les annotations primaires présentées figure 2.\nA partir de cette définition du chemin d\u0027un élément de l\u0027arbre, on définit la notion de chemin similarisé. Un chemin similarisé est la factorisation des chemins de plusieurs élé-ments. Le chemin ainsi généré est ainsi un chemin de plusieurs éléments. Pour cela, les étapes sont comparées 2 à 2 et les différences marquées par une astérisque. Prenons l\u0027exemple des deux premières annotations primaires présentées figure 2. Le chemin du premier élément est body : 1,table :0,tbody :0,tr :0,td :0,b  -chaque instance de concept est exactement une feuille de l\u0027arbre, -les instances de rôles sont contenues dans des sous-arbres De ces hypothèses, on déduit qu\u0027identifier une instance de l\u0027ontologie revient à déterminer le chemin depuis la racine vers une feuille de l\u0027arbre pour une instance de concept et vers un noeud, racine du sous-arbre, pour une instance de rôle. L\u0027apprentissage consiste donc à déterminer un chemin similarisé pour chaque concept et chaque rôle de l\u0027ontologie.\nApprentissage de chemins similarisés Pour chaque concept dont des exemples ont été marqués par l\u0027expert, le chemin similarisé du concept est généré à partir de l\u0027ensemble des chemins enregistrés dans les annotations primaires pour ce concept. Dans l\u0027exemple figure 1, 5 annotations primaires sont définies pour le concept Project. En factorisant les chemins deux à deux, le chemin similarisé obtenu est body : 2, table : * , tbody : 0, tr : * , td : 1, a : 0, f ont : 0. Il ressort ainsi que les éléments correspondant aux concept Project sont situés dans la deuxième colonne des tableaux du document.\nPour les instances de rôles, une première étape consiste à déterminer les racines des sousarbres de chaque rôle tel que :\n-il existe un rôle R A,B dans l\u0027ontologie reliant des concepts A et B, -au moins une instance de A et une instance de B ont été marquées. Alors pour chaque instance marquée de A :\n-le plus petit parent commun (pppc) dans l\u0027arbre de cette instance avec chaque instance de B est déterminé, -le noeud le plus profond dans l\u0027arbre parmi ces pppc est alors un noeud racine pour le rôle R A,B . Le chemin similarisé des noeuds racines générés est alors inféré. La sortie de l\u0027apprentissage est donc un chemin similarisé de chaque concept et de chaque rôle de l\u0027ontologie ayant des instances marquées dans le document.\nAnnotation par génération d\u0027instances de l\u0027ontologie\nAnnotation par application des chemins similarisés Les chemins similarisés sont appliqués sur une page dont la structure DOM est similaire à la page d\u0027apprentissage. Les noeuds reconnus par le chemin similarisé appris pour chaque rôle R A,B sont les racines des sousarbres en dessous desquels chaque instance de a est liée à une instance de b par une instance de R A,B . Les feuilles reconnues par le chemin similarisé d\u0027un concept sont des candidates pour être instanciées par ce concept. Deux cas sont possibles : si une feuille n\u0027est reconnue que par un seul chemin similarisé, cette feuille est instanciée par le concept correspondant à ce chemin. Pour toutes les feuilles situées dans un sous-arbre, une relation est générée entre les instances de concepts définies par le rôle. Si plusieurs chemins similarisés conduisent à la même feuille, un mécanisme de raisonnement doit être appliqué pour déterminer à quel concept cette feuille appartient. On atteint les limites d\u0027une méthode purement syntaxique.\nDans notre exemple, le chemin similarisé du concept Project décrit ainsi le fait qu\u0027il est associé aux éléments contenus dans la colonne de droite des tableaux tandis que les concepts Lecturer et FacultyMember sont associés au contenu de la colonne de gauche.\nAnnotation par un concept plus général dans l\u0027ontologie Lorsqu\u0027un même élément peut être annoté par deux concepts différents, un raisonnement est effectué au niveau de l\u0027ontologie pour déterminer le concept subsumant les deux concepts candidats. Dans notre exemple, le raisonneur Pellet (Sirin et Parsia (2004) évalue les performances de Pellet pour la classification et les requêtes) est utilisé pour classifier les concepts de l\u0027ontologie et déterminer le concept subsumant Lecturer et FacultyMember dans SW RC. Il s\u0027agit de AcademicStaff. Une instance de ce concept sera donc générée pour les éléments reconnus par les chemins similarisés appris à partir des concepts Lecturer ou FacultyMember.\n"
  },
  {
    "id": "1084",
    "text": "Présentation\nDans le cadre du projet RIAM 1 « Relaxmultimédia » mené conjointement avec deux agences de presse (AFP et Relaxnews) nous présentons une approche destinée à gérer deux aspects d\u0027un modèle métier défini avec UML : son extensibilité et la possibilité de naviguer entre les classes et les instances définies à partir de ce modèle. Nous montrons que la transformation du modèle UML en un schéma RDF sur lequel est utilisable SeRQL, un langage d\u0027interrogation, présente des caractéristiques intéressantes pour gérer de tels aspects.  Pour conclure, nous soulignerons que l\u0027utilisation d\u0027UML permet de définir précisément le modèle de base et les points d\u0027extension grâce à l\u0027utilisation de stéréotypes. Il n\u0027offre cependant pas de possibilités sur l\u0027interrogation du modèle. Un modèle fondé sur RDFS, associé au langage SeRQL, permet cette interrogation à la fois sur les classes et les instances ce qui est utile pour permettre une navigation conjointe dans le modèle et dans les données.\nExtensibilité et navigation dans le modèle métier\n"
  },
  {
    "id": "1086",
    "text": "Introduction\nDans le domaine des sports en équipe, de plus en plus d\u0027entraîneurs font appel à des outils informatiques durant leur activité pédagogique, en particulier de logiciels de simulation afin d\u0027enseigner aux joueurs à améliorer leur tactique. Jusqu\u0027à présent, ces logiciels qui permettaient essentiellement à l\u0027entraîneur de faire se déplacer sur un écran des agents joueurs, nécessitaient de sa part de spécifier quasiment trame par trame la position des agents. Par voie de fait, un entraîneur souhaitant montrer le déploiement d\u0027un schéma tactique particulier doit effectuer un important travail avant que la simulation puisse être lancée.\nDès lors, rendre les agents plus autonomes, améliorer le réalisme de leur comportement et leur capacité de prendre des décision allégerait le travail de l\u0027entraîneur, et lui permettrait de n\u0027avoir qu\u0027à spécifier des schémas tactiques relativement abstrait pour voir comment des agents joueurs déploieraient ce schéma « intelligemment » sur le terrain.\nNotre objectif est donc d\u0027utiliser diverses techniques d\u0027intelligence artificielle pour amé-liorer l\u0027autonomie des agents devant déployer un schéma spécifié par l\u0027entraîneur. Cette tâche peut être considéré comme un sous-ensemble du problème de la simulation sportive (par exemple la RoboCup), du fait que les agents se voient indiqués la route à suivre (le schéma tactique), mais doivent pouvoir en dévier s\u0027ils croisent un adversaire qui leur prend la balle.\nDans un premier temps, un système multi-agents a été construit, dans lequel les agents suivent un comportement décrit dans une base de règle. Nous avons montré sur quelques schémas que les comportements obtenus étaient parfois insuffisants, du fait que les agents ne s\u0027autorisaient pas à dévier suffisamment des indications de l\u0027entraîneur pour faire face à l\u0027adversaire. Ensuite, nous avons implémenté un algorithme d\u0027apprentissage par renforcement qui a permis aux agents de se comporter correctement dans les cas ou ils échouaient avec le système à base de règles. Enfin, nous avons créé une plateforme logicielle intégrant ces différents algorithmes et permettant à l\u0027entraîneur de faire des simulations aisément.\nDans ce papier, après avoir introduit les schémas tactiques, nous présentons essentiellement les résultats des expérimentations de l\u0027algorithme d\u0027apprentissage par renforcement, que nous comparons sur différents schémas aux performances des agents basés sur des rè-gles. Nous montrons en particulier que l\u0027apprentissage converge rapidement malgré la dimension importante du problème.\nLa simulation de schémas tactiques\nEn regardant l\u0027état de l\u0027art du sujet en question nous avons trouvé deux axes principaux : les outils commerciaux et les travaux scientifiques.\nLes représentants du premier axe sont destinés aux professionnels du football et traitent effectivement les schémas tactiques, mais ils proposent une solution simple pour le déploie-ment d\u0027un schéma tactique. En fait, ils ne font qu\u0027une animation d\u0027une séquence de positions prédéfinies en utilisant des techniques d\u0027interpolation d\u0027images où les objets ont une trajectoire rectiligne entre deux positions successives. Le résultat final est donc une animation dont la qualité dépend de l\u0027intervalle entre chaque image. Plus l\u0027intervalle augmente plus la fiabilité diminue. Cette approche laisse tout le travail fastidieux et répétitif à l\u0027utilisateur du programme qui doit prévoir et décrire en détail le déplacement des objets à chaque instant. En particulier si un changement se présente ce travail est à refaire.\nLa RoboCup est le plus grand représentant du deuxième axe. Il s\u0027agit d\u0027un projet de coopération international destiné à encourager le développement de l\u0027intelligence artificielle (IA), de la robotique et d\u0027autres domaines connexes. Du point de vue des systèmes multiagents (SMA) le modèle footballistique crée par la RoboCup est un défi intéressant car il regroupe plusieurs caractéristiques (Noda et al., 1997), dont un environnement qui évolue dynamiquement, une nécessité pour les agents de communiquer et se coordonner pour atteindre leurs objectifs.\nActuellement, il n\u0027existe pas encore d\u0027équipe utilisant le déploiement de schémas tactiques tel quel le font les joueurs de football. Les équipes ont déjà suffisamment de problèmes à résoudre avec les contraintes et les définitions imposées par le modèle en question.\nNous proposons une solution qui s\u0027inscrit dans le premier axe (pour les professionnels du sport) pour la simulation de schémas tactiques basé sur les SMA comme la RoboCup mais sans toutes ses contraintes et déterminations. Nous avons opté pour l\u0027utilisation des méthodes d\u0027apprentissage automatique pour la conception des agents, évitant ainsi la tâche complexe de programmer les comportements des joueurs. Dotés de la faculté d\u0027apprendre, les agents gagneront en autonomie, et seront capables de s\u0027adapter à leur adversaire ou à leur environnement.\nEn entraînant des agents à jouer avec certains schémas tactiques, l\u0027entraîneur pourra dé-velopper chez ces agents des aptitudes particulières liées à ces schémas. Par exemple : il pourra créer un défenseur en mettant un agent joueur face à des attaquants avec le ballon, avec pour objectif apprendre à récupérer le ballon.\nAlgorithme d\u0027apprentissage\nNous avons choisi l\u0027apprentissage par renforcement parce que l\u0027agent apprend par interaction avec l\u0027environnement sans avoir besoin d\u0027exemples. Dans les sections suivantes, nous définirons les récompenses et l\u0027agent devra découvrir par un processus d\u0027essais et d\u0027erreurs, l\u0027action optimale à effectuer pour chacune des situations afin de maximiser ses récompenses (Sutton, 1998). Pour modéliser un problème en utilisant de l\u0027apprentissage par renforcement, on doit se poser les questions suivantes : quelles actions peuvent être effectuées par les agents ? Quelle représentation de l\u0027environnement employer ? Quelles récompenses leur donner ?\nLes espaces d\u0027actions et d\u0027état\nEn ce qui concerne l\u0027espace d\u0027état, nous avons choisi une représentation basée sur des distances, afin que les comportements des agents ne dépendent que des positions relatives des uns par rapport aux autres, et non de leur position absolue sur le terrain (voir le TAB. 1). \nTAB. 1 -Description des caractéristiques des états\nEn utilisant comme base les comportements de navigations primaires décrites par Reynolds (1999), nous avons créé les actions suivantes :\n-Déplacement vers un point : elle combine les comportements seek et unaligned collision avoidance afin d\u0027aller vers un point en évitant les collisions ; -Déplacement en groupe vers un point : Cette action permet au joueur de se dépla-cer tout en restant proche du groupe, grâce au comportement cohesion. -Positionnement : inspiré de l\u0027action se positionner de Veloso et al. (1999)  Stone et McAllester (2001) pour trouver le temps nécessaire à l\u0027interception du ballon. Avec cette information l\u0027agent peut savoir vers où il doit aller pour attraper le ballon. Il y a d\u0027autres actions également importantes, mais qui n\u0027utilisent pas les comportements de navigation, on peut lister : faire une passe et prendre le contrôle du ballon.\nLe renforcement\nPour définir les récompenses d\u0027une manière plus facile nous avons déterminé un ordre de priorités. Tout d\u0027abord il faut réaliser le schéma tactique (arriver aux objectifs avec les conditions satisfaites), ensuite ne pas perdre le ballon et enfin ne pas mettre le ballon hors du terrain. D\u0027après les priorités définies, nous avons attribué les valeurs du TAB. 2 comme récom-pense. \nFonction d\u0027évaluation\nNous avons créé une fonction pour évaluer la situation actuelle d\u0027une équipe par rapport à la réalisation d\u0027un schéma tactique. Cette fonction est donc la somme des évaluations individuelles de tous les joueurs de l\u0027équipe.\nPour évaluer individuellement un joueur j nous prenons en compte le nombre d\u0027objectifs qui ont été déjà réussis (noté or i ), la distance normalisée (entre 0 et 1) au prochain objectif o (notée dn oj ), et nous calculons la valeur e( j) \u003d (or j + dn oj ) no j , où no j est le nombre total d\u0027objectifs du joueur j. Dès lors, pour évaluer une équipe t, nous définissons la fonction s(t) égale à la moyenne des évaluations des joueurs de l\u0027équipe.\n!\nExpérimentation et résultats\nPour interagir avec l\u0027entraîneur, réaliser nos expérimentations et voir les résultats, nous avons réalisé une plateforme comportant différents modules dont : un module d\u0027interaction avec l\u0027entraineur, un module d\u0027apprentissage par renforcement. En plus des agents basés sur l\u0027apprentissage par renforcement, nous avons implémenté un autre type d\u0027agents dont le comportement dérive d\u0027un système à base de connaissances (SBC), ce qui constitue une approche plus classique. Ainsi, en comparant les deux approches, nous pouvons mesurer le gain apporté par un algorithme d\u0027apprentissage. Pendant nos expérimentations, nous avons utilisé un CMAC avec 32 tableaux de 9 cases (pour les paramètres continus et 1, 2 ou 3 cases pour les valeurs discrètes). Les configurations utilisées sont résumées par le TAB. 3.\n-146 -RNTI-E-6 Le graphe qui montre l\u0027évolution de l\u0027apprentissage de nos agents selon la quantité d\u0027épisodes (axe des abscisses) et notre fonction d\u0027évaluation (axe des ordonnées) pour la configuration a est présentés par la Fig. 3. -147 -RNTI-E-6\nPour toutes les configurations, nous avons un résultat final similaire, montré par la Fig. 2, où l\u0027agent qui n\u0027a pas d\u0027objectif apprend qu\u0027il faut chercher le ballon (sinon l\u0027équipe adverse va le prendre) pour ensuite faire une passe à son coéquipier pour pouvoir accomplir le sché-ma tactique donné.\nConclusion et travaux futurs\nLe travail présenté ici montre comment une approche SMA adaptative pour la simulation de schémas tactiques peut être mise en oeuvre, quel type de résultats on peut en attendre et quels sont les apports vis-à-vis des autres solutions existantes.\nNous allons poursuivre la recherche en variant les composants de l\u0027architecture du SMA et les composants de la méthode d\u0027apprentissage (e.g. types d\u0027agents, de mémoire, etc.). Nous nous intéressons notamment à l\u0027emploi de techniques pour améliorer l\u0027évolution et la coordination de l\u0027apprentissage. De plus nous étudions l\u0027utilisation de l\u0027apprentissage par imitation pour apprendre à partir de séquences vidéo numérisées, en accélérant donc l\u0027apprentissage.\nA travers ce travail, nous espérons ouvrir une voie nouvelle dans les approches de la simulation numérique dans le milieu de tactiques sportives et contribuer à la conception de nouveaux outils d\u0027aide aux entraîneurs et autres professionnels du sport.\n"
  },
  {
    "id": "1087",
    "text": "Introduction\nLe transit des flux d\u0027information dans le réseau Internet à l\u0027échelle mondiale est régi par des accords commerciaux entre systèmes autonomes. La négociation de ces accords commerciaux repose implicitement sur une hiérarchie des systèmes autonomes et la position relative de deux systèmes débouche sur un accord de type client/fournisseur (un des systè-mes, le client, est nettement mieux classé que l\u0027autre, le fournisseur, et le client paye le fournisseur pour le transit des flux d\u0027information) ou sur un accord de type \"peering\" (transit gratuit du trafic entre les deux systèmes).\nLes politiques de routage déduites de ces accords commerciaux sont ensuite mises en oeuvre via le protocole de routage BGP (Border Gateway Protocol). Ainsi, l\u0027établissement des routes à l\u0027échelle mondiale obéit à des règles d\u0027efficacité économique déduites d\u0027une hiérar-chisation entre systèmes autonomes (une route ne peut pas, par exemple, \"descendre\" d\u0027un fournisseur à son client pour \"remonter\" vers un autre fournisseur : quel client accepterait de payer pour porter les trafic de ses fournisseurs ?), règles bien différentes des règles d\u0027ingénie-rie qui régissent le routage à l\u0027intérieur des systèmes autonomes (Griffin et al (2002), Gao et Wang (2002)).\nEn dépit de l\u0027importance de cette hiérarchisation des systèmes autonomes, que ce soit pour la compréhension des phénomènes de routage à grande échelle dans l\u0027Internet ou pour les systèmes autonomes eux-mêmes à fins de négociation, il n\u0027existe pas de hiérarchie publiquement disponible (les clauses commerciales des accords entre systèmes autonomes ne sont pas nécessairement publiques) ni même de consensus sur le moyen d\u0027en établir une (les diffé-rents fournisseurs de services ont chacun leur propre façon d\u0027établir ce classement).\nNous proposons ci-dessous une méthode permettant d\u0027établir un tel classement qui satisfasse aux contraintes suivantes :\n1. reposer sur des données publiquement disponibles; 2. permettre de simuler les conséquences de l\u0027ajout ou du retrait d\u0027une relation de connectivité entre systèmes autonomes; 3. permettre de pondérer l\u0027importance accordée aux différents systèmes autonomes; 4. permettre d\u0027étudier les contributions de son voisinage au classement d\u0027un système particulier. La notion d\u0027importance d\u0027un système autonome (AS1) dans le cadre du routage dans l\u0027Internet mondial repose sur sa capacité à permettre à d\u0027autres systèmes d\u0027établir une connexion alors qu\u0027ils n\u0027ont pas de connectivité directe (AS2 et AS3); cette capacité ne repose pas forcément sur une médiation \"directe\" AS2 -AS1 -AS3 entre les systèmes autonomes mais peut aussi reposer sur une médiation \"indirecte\" par le biais des connectivités nouvelles offertes à AS2 et AS3 par AS1 : AS2 -AS1 -AS4 -AS3 où il est entendu que AS2 n\u0027a pas de connectivité avec AS4, ni AS3 avec AS1 (voir la Figure 1). Ainsi, dans l\u0027exemple ci-dessus, l\u0027importance de AS1 dépend de l\u0027importance de AS4 : en se connectant à AS4, AS1 \"hérite\" d\u0027une partie de l\u0027importance de AS4 en ce qu\u0027il peut maintenant proposer à AS2 une connectivité plus étendue (les systèmes comme AS3 que AS1 n\u0027atteint pas directement mais auxquels AS4 a accès); la réciproque est vraie pour AS4 qui hérite d\u0027une partie de l\u0027importance de AS1.\nCette notion d\u0027 \"héritage\" à partir des voisins dans la constitution de la notion d\u0027importance est au coeur de l\u0027analyse de l\u0027importance des positions dans les réseaux sociaux et trouve une de ses formalisations dans la notion de \"centralité spectrale\" détaillée ci-dessous. C\u0027est à partir de cette notion d\u0027importance que nous proposons d\u0027établir un classement des systèmes autonomes de l\u0027Internet à partir de leur seul graphe d\u0027interconnexion.\nCe graphe d\u0027interconnexion est facile à établir à partir des tables de routage BGP (Chen et al (2002)). Remarquons que, par construction, ce graphe ne comprend que les liens qui apparaissent dans au moins un chemin BGP.\nCentralité spectrale dans les réseaux sociaux\nNous reprenons ici la généralisation de la notion de centralité spectrale pour les graphes asymétriques introduite par Bonacich et Lloyd (2001). Le vecteur X des importances des noeuds dans un graphe (donné par sa matrice d\u0027adjacence asymétrique pondérée A) possède deux origines de nature différente, un terme intrinsèque E qui ne dépend que du noeud considéré isolément et un terme provenant de l\u0027effet de réseau (héritage linéaire de l\u0027importance des voisins), ce qui se traduit par une équation du type X\u003d?AX+E où ? doit approcher (par valeurs inférieures) l\u0027inverse de la valeur propre principale de A pour que le résultat obtenu par cette méthode soit cohérent avec celui obtenu par la méthode spectrale usuelle dans le cas de graphes non orientés (Bonacich et Lloyd (2001)).\nTechniquement, la solution de l\u0027équation ci-dessus est obtenue par itération jusqu\u0027à convergence de X i+1 \u003d?AX i +E, l\u0027inversion directe X\u003d(I-?A)\n-1 E faisant apparaître une matrice non creuse en général bien trop volumineuse.\nCette formulation de la centralité permet de répondre directement à deux de nos objectifs : 1. la notion d\u0027importance intrinsèque permet de pondérer l\u0027importance accordée aux diffé-rents systèmes autonomes; dans la littérature, on rencontre surtout le choix E i \u003d1, i ? mais rien n\u0027impose ce choix dans l\u0027absolu; la seule condition imposée à E est que les valeurs soient indépendantes des effets de réseau (par exemple, il ne serait pas cohérent de pondérer chaque noeud par son degré  (Decima et al. (2004) 72.7 % Percentage of nodes with degree ? 50. 0.9 %\nTAB. 1 -Propriétés topologiques du graphe d\u0027interconnexion des systèmes autonomes\nLa notion de degré dans le graphe d\u0027interconnexion est une description de la connectivité physique d\u0027un noeud; cette notion de \"connectivité physique\" ne capture pas la notion d\u0027 \"atteignabilité\" (\"reachability\") qui recouvre la capacité d\u0027un système autonome à atteindre d\u0027autres systèmes autonomes en exploitant les chemins à travers l\u0027Internet. Cette dernière notion dépend des relations logiques entre les systèmes autonomes.  (Gao (2000)). La première correspond à un service de transit payant offert par un système autonome (le fournisseur) à un autre système autonome (le client); le deuxième type correspond à un accord de transit gratuit entre deux systèmes autonomes; le troisième type correspond à un cas particulier de transit mutuel entre deux systèmes autonomes et sera ignoré dans la suite. Ces types de relations logiques structurent la hiérarchie des systèmes autonomes de l\u0027Internet car les politiques de routage entre systèmes autonomes répondent à des critères d\u0027efficacité économique. Typiquement, un client doit se trouver en dessous de ses fournisseurs dans la hiérarchie. De même, plus un système autonome est important, plus il sera à même de négocier à son avantage les accords avec les autres systèmes autonomes.\nIl est possible d\u0027inférer ces relations logiques à partir des chemins déduits des tables de routage BGP. Le tableau 2 résume le résultat de cette inférence à partir de la méthode proposée par Gao (2000).\nLa connaissance des relations logiques permet de construire une hiérarchie des systèmes autonomes mais cette façon de procéder ne répond pas à un des objectifs que nous nous sommes fixés dans cette étude, à savoir pouvoir évaluer l\u0027impact de l\u0027ajout ou du retrait d\u0027une connexion dans le graphe; en effet, l\u0027inférence repose sur les chemins BGP qui sont la résul-tante de décisions politiques distribuées de l\u0027ensemble des acteurs et il est impossible, par exemple, de prédire comment les chemins BGP se ré-arrangeraient si on supprimait une connexion. Nous travaillerons donc seulement à partir du graphe d\u0027interconnexion.\nLa centralité fondée sur le degré\nCette notion de centralité est la plus simple qui inclue un effet de réseau; les experts estiment que ce classement est assez satisfaisant pour les systèmes les plus importants (en particulier, les 5 systèmes autonomes les plus importants de l\u0027Internet (\"Tier 1\") qui sont tous reliés entre eux par des accords de peering et ne sont clients de personne sont correctement placés en tête de classement, voir le tableau 3) mais surestime l\u0027importance des systèmes autonomes ayant de nombreux voisins \"terminaux\" (des noeuds qui n\u0027ont qu\u0027un seul voisin).\nPour mémoire, les cinq systèmes autonomes qui forment une clique de pairs au sommet de l\u0027Internet sont Uunet (701), Sprint (1239), ATT (7018), Level 3 (3356), Qwest (209). Une définition moins restrictive du Tier 1 y ajoute les autres systèmes autonomes ayant au moins un lien de peering avec les précédents; s\u0027ajoutent, entre autres, à la liste Cogent (174), NTT Verio (2914), Global Crossing (3549) ou Savvis (ex Cable Wireless) (3561). Ces systèmes ne font pas partie de la clique de pairs mais, ayant au moins une relation de peering avec cette clique, ils ont un accès gratuit à la totalité de l\u0027Internet.\nBien que très simple et obtenant des résultats assez satisfaisants, cette centralité fondée sur le degré ne permet pas de simuler les conséquences de l\u0027ajout ou du retrait d\u0027une connexion au-delà de la seule variation triviale de connectivité des deux systèmes concernés. C\u0027est une conséquence de la différence entre les notions de \"connectivité\" (mesurée par le degré) et d\u0027 \"atteignabilité\" qui nous intéresse ici.\nLa centralité spectrale à partir du graphe d\u0027interconnexion symétri-que\nLe graphe d\u0027interconnexion entre systèmes autonomes tel qu\u0027on peut le déduire des informations de routage publiquement disponibles est évidemment un graphe symétrique.\nOn peut lui appliquer le calcul classique de centralité spectrale pour les graphes symétri-ques (cette approche a également été proposée par Gkantsidis et al (2003)). Le classement obtenu (voir le tableau 3) est clairement insatisfaisant pour les experts du domaine; en particulier, les cinq systèmes autonomes formant le \"Tier 1\" ne sont pas en tête du classement, l\u0027un d\u0027entre eux ne figurant même pas parmi les 20 premiers. \nUne heuristique pour orienter le graphe d\u0027interconnexion en fonction de la hiérarchie des systèmes autonomes\nLe défaut de l\u0027approche précédente est de laisser symétrique la relation entre deux noeuds au lieu de tenir compte de leur différence de centralité pour orienter le graphe au sens d\u0027une relation client/fournisseur.\nNous proposons ci-dessous une heuristique itérative simple permettant d\u0027introduire progressivement cette asymétrie dans le graphe à partir des différences de classement entre systèmes autonomes. Le résultat de cette heuristique est de permettre de transformer progressivement le graphe d\u0027interconnexion \"physique\" (symétrique) en un graphe d\u0027adjacence \"logique\" (asymétrique) en cohérence avec le classement des systèmes autonomes associé.\nL\u0027heuristique proposée est la suivante : Initialisation : 1. tous les noeuds recoivent une importance intrinsèque; 2. le graphe d\u0027interconnexion est considéré comme un (di-)graphe orienté pondéré, chaque arête non orientée donnant naissance à deux arêtes orientées dans des sens opposés et de poids ½; Calcul de la centralité et du graphe d\u0027adjacence \"logique\" : 1. on calcule le score de centralité associé au graphe pondéré obtenu à l\u0027étape précédente et aux importances intrinsèques; 2. on modifie la pondération des arêtes en renforçant l\u0027asymétrie de la relation entre deux noeuds en fonction de leur différence de centralité et on obtient une nouvelle matrice d\u0027adjacence asymétrique. Ces deux dernières étapes sont déroulées jusqu\u0027à convergence du score de centralité. Plusieurs modifications de la pondération de l\u0027arête w n ab entre des noeuds a et b sont envisageables; ci-dessous, nous avons opté pour une modification en fonction des scores c En choisissant une valeur faible du paramètre ?, on a une adaptation très progressive de l\u0027orientation des arcs à la structure de centralité, ce qui permet de ne pas \"figer\" brutalement la structure obtenue à l\u0027initialisation.\nLe paramètre p permet de faire varier l\u0027importance accordée à un faible écart de centralité entre noeuds : choisir une valeur p\u003e1 accorde une importance faible à de faibles écarts de centralité, ce qui permet d\u0027explorer plus finement la notion de \"peering\" entre noeuds.\n3.5 Résultats expérimentaux pour l\u0027heuristique proposée 3.5.1 Classement de systèmes autonomes Le tableau 3 montre le classement obtenu à convergence pour p\u003d1. Ce classement paraît satisfaisant aux experts du domaine; nous notons en particulier le classement correct des systèmes autonomes du Tier 1. Pour mémoire, le classement des dix premiers systèmes est le suivant : Uunet, Sprint, ATT WorldNet, Level 3, Qwest, Cogent, Abovenet, Globix,Colt, NTT Verio.\nFIG. 2 -Rang obtenu par l\u0027heuristique proposée en fonction du rang déduit du degré\nLa figure 2 montre la variation du classement obtenu en fonction du classement déduit du degré; on peut observer que pour les systèmes autonomes les mieux classés, la corrélation est forte mais que quelques différences significatives sont néanmoins observables. Noter que les grands écarts de rangs observés pour les systèmes mal classés ne sont guère significatifs, tous ces systèmes étant partiquement ex-aequo avec des scores de centralité très proches de 1. Comme on peut le constater, l\u0027utilisation de p\u003d3 permet de faire émerger une plage de systèmes autonomes qui sont dans une relation quasi-symétrique vis-à-vis de OPENTRANSIT (poids de l\u0027arête proche de ½ ) et pourraient donc être considérés comme des \"peers\" potentiels de OPENTRANSIT au sens du classement obtenu. Ce classement dépendant de la valeur du paramètre p choisi, nous proposons ci-dessous une façon de rechercher la valeur du paramètre la plus adaptée. Nous soulignons ici que notre objectif n\u0027est pas d\u0027identifier précisément les peerings existants 3 mais plutôt d\u0027identifier les peerings potentiels, c\u0027est-à-dire les connexions concernant des systèmes autonomes d\u0027importances comparables.\nPour cela nous nous restreignons à l\u0027étude des connexions mettant en jeu au moins un système autonome dont la valeur du critère de centralité spectrale dépasse 2. Comme nous avons choisi uniformément une valeur d\u0027importance intrinsèque de 1, tous les systèmes autonomes ont une valeur de centralité spectrale au moins égale à 1. Ce seuil à 2 permet de ne considérer que des connexions mettant en jeu au moins un système autonome pour lequel l\u0027effet de réseau a une importance supérieure à cette importance intrinsèque uniforme; en effet, la notion de \"peering\" n\u0027a de sens que pour des systèmes autonomes jouant le rôle de fournisseur pour une partie du réseau. . Les arêtes sont ensuite séparées aléatoirement en un ensemble d\u0027apprentissage et un ensemble de test en proportions égales et pour chaque valeur du paramètre p, on recherche le seuil t(p) optimal pour la règle de classification :\n• si |w ij -1/2|\u003c t(p), l\u0027arête (i,j) est de type \"peering\"\n• si |w ij -1/2|\u003e t(p), l\u0027arête (i,j) est de type \"non-peering\" Pour chaque valeur de p, le seuil optimal t*(p) est déterminé à partir du seul ensemble d\u0027apprentissage. Le critère de performance est défini comme la demi-somme du taux de bonne classification dans les classes \"peering\" et \"non-peering\". Nous avons choisi ce critère pour donner la même importance à la petite classe des \"peerings\" face à la classe largement majoritaire des \"non-peerings\" (voir le tableau 2). La variation du critère de performance (pour le seuil optimal) est donnée sur la figure 6, pour l\u0027ensemble d\u0027apprentissage et pour l\u0027ensemble de test.\nLes performances sont proches sur l\u0027ensemble d\u0027apprentissage et sur l\u0027ensemble de test, ce qui montre que le classifieur défini par la règle simple au-dessus possède de bonnes capacités de généralisation; la valeur optimale de p se situe autour de p*\u003d2.2 (associée à une valeur optimale de seuil t*(p*)\u003d0.34). Le taux de bonne classification est de 0.96 dans la classe peering et 0.77 dans la classe non-peering. Le fait que beaucoup de connexions \"client-fournisseur\" (23%) soient classées \"peering\" par le modèle reflète simplement le fait que du point de vue du classement, de nombreuses connexions se font entre systèmes autonomes d\u0027importance équivalentes qui pourraient se traduire par des accords de peerings en fonction des volontés politiques des acteurs concernés.\nConclusion\nL\u0027approche décrite dans cette communication emprunte la notion de centralité spectrale généralisée au domaine de l\u0027analyse des réseaux sociaux et identifie l\u0027importance d\u0027un système autonome de l\u0027Internet à cette centralité.\nS\u0027appuyant sur la différence de centralité entre systèmes autonomes pour orienter et pondérer progressivement la matrice d\u0027adjacence du (di-)graphe d\u0027interconnexion, l\u0027heuristique proposée réalise le passage de la description du graphe en termes de connectivité \"physique\" à une description du graphe en termes de connectivité \"logique\". Le classement obtenu est en accord avec les attentes des experts du domaine.\nLa méthode ne repose que sur des données publiquement disponibles, est reproductible et permet : 1. de simuler les conséquences de l\u0027ajout ou du retrait d\u0027une relation de connectivité\n"
  },
  {
    "id": "1088",
    "text": "Introduction\nNotre étude a été motivée par le problème suivant : nous disposons de données concernant plusieurs dizaines de milliers d\u0027individus décrits par quelques milliers d\u0027attributs binaires assez rares et nous recherchons les éventuels liens entre certains attributs ou groupes d\u0027attributs. La similitude de nos données avec des données de transactions nous a naturellement amenés à utiliser un algorithme de recherche de règles d\u0027association. Cependant, le nombre élevé d\u0027attributs conjugué à leur rareté conduit à un très grand nombre de règles dont les supports sont très faibles et les confiances très élevées. C\u0027est pourquoi nous avons cherché à compléter l\u0027approche support-confiance pour extraire les règles les plus pertinentes. De nombreux indices ont été proposés dans la littérature pour évaluer l\u0027intérêt des règles d\u0027association. Quelques uns font l\u0027objet d\u0027une analyse graphique à l\u0027aide de courbes de niveaux. Nous exposons ensuite une application sur données industrielles.\nContexte\nCe travail est issu d\u0027un projet industriel où l\u0027objectif est d\u0027exploiter une partie de l\u0027informationnel d\u0027un grand constructeur automobile afin d\u0027extraire de nouvelles connaissances. Les données, issues du process de fabrication des véhicules, sont sous la forme d\u0027une matrice où chaque véhicule est décrit par la présence ou l\u0027absence d\u0027attributs binaires. La connaissance d\u0027éventuelles corrélations entre certains attributs ou groupes d\u0027attributs représente un avantage non négligeable pour le constructeur automobile qui met un point d\u0027honneur à améliorer son niveau de qualité de façon continuelle. Pour répondre à cette problématique, nous utilisons la méthode de recherche de règles d\u0027association.\nSoit la règle d\u0027association A?C où l\u0027ensemble A, la partie antécédent ou prémisse, implique l\u0027ensemble C, la partie conséquent ou conclusion. A et C sont des ensembles disjoints d\u0027attributs binaires. Dans le contexte particulier de notre application, il est nécessaire de préciser que le sens de l\u0027implication n\u0027a pas d\u0027importance. Une règle d\u0027association est entiè-rement caractérisée par son tableau de contingence (TAB.1).\nPlusieurs algorithmes permettent de rechercher les règles d\u0027association de façon détermi-niste à partir d\u0027une base de données contenant n cas décrits par des variables binaires. Parmi eux, on peut citer Apriori (Agrawal et al., 1993(Agrawal et al., , 1994, l\u0027algorithme fondateur de la recherche de règles d\u0027association, ou l\u0027algorithme Eclat (Zaki, 2000), qui est plus rapide. Tous les algorithmes procèdent en deux étapes. Tout d\u0027abord, ils recherchent les sous-ensembles fréquents, c\u0027est-à-dire les conjonctions d\u0027attributs qui apparaissent avec un support (P(AC)) supérieur à un seuil fixé par l\u0027utilisateur. Puis, la seconde étape consiste à construire les règles à partir des sous-ensembles fréquents trouvés lors de la première étape. Seules les règles dotées d\u0027une confiance (P(C/A)) supérieure à un seuil minimum défini par l\u0027utilisateur seront conservées.\nBien souvent, l\u0027approche support-confiance précédemment décrite conduit à l\u0027obtention de règles en trop grand nombre. Par conséquent, il est impossible de les faire valider par un expert. Dès lors, il est utile de les trier par ordre décroissant de leur intérêt au sens d\u0027un indice de pertinence, tel que le lift (Brin et al., 1997), pour citer un des plus connus.\nChoix de quelques indices de pertinence\nUne typologie existante\nIl existe tellement d\u0027indices de pertinence des règles d\u0027association qu\u0027il est très compliqué pour l\u0027utilisateur de savoir lequel choisir. Nous trouvant dans cette situation, pour aider à orienter notre choix, nous nous sommes appuyés sur une suite de travaux réalisés sur ce sujet. Afin d\u0027évaluer les indices de pertinence, Lenca et al. (2004) définissent huit propriétés formelles telles que la décroissance en fonction du nombre d\u0027occurrences du conséquent ou la facilité à fixer un seuil d\u0027acceptation de l\u0027indice. Ces propriétés permettent d\u0027évaluer les indices de pertinence et de leur attribuer des notes. Les auteurs proposent ainsi un classement d\u0027une vingtaine d\u0027indices. Cette étude formelle a ensuite été complétée par une étude expéri-mentale  où les auteurs illustrent le fait que les indices ont un comportement différent en fonction des données traitées : une classification ascendante hiérarchique de 18 indices de pertinence est réalisée à partir d\u0027une matrice de distance déduite de la matrice de décision issue de l\u0027évaluation formelle ; elle aboutit à la partition suivante : \nSélection des meilleurs indices\nAfin de mieux compléter l\u0027approche support-confiance, nous nous intéressons aux indices qui ont obtenu les notes les plus élevées selon Lenca et al. (2004) parmi ceux des classes 1 et 3 de la typologie précédente : la confiance centrée dans la classe 1, à laquelle nous rajoutons le lift en raison de son utilisation très répandue et de son interprétation facile ; le multiplicateur de cotes et le Loevinger dans la classe 3. Le tableau 2 rappelle les propriétés de ces indices dans les cas extrêmes :\nTAB. 2 -Indices de pertinence retenus et leurs valeurs de référence.\nProposition de deux indices de pertinence supplémentaires\nDans notre cas, les quatre indices de pertinence détaillés ci-dessus prennent des valeurs extrêmement élevées car de nombreuses règles ont un conséquent très fréquent par rapport au support de la règle. Dans le cas des données de transaction, cela équivaut à une règle du type {dictionnaire?lait}. Le lait est un achat tellement commun que de nombreux caddies en contiennent en sortie de caisse. L\u0027achat d\u0027un dictionnaire est moins fréquent mais toutes les transactions contenant un dictionnaire risquent de contenir aussi du lait. La règle {diction-naire?lait} aura un faible support étant donnée la rareté de dictionnaire, mais sa confiance sera proche de 100%. A titre d\u0027exemple, considérons 100 consommateurs : 8 ont acheté un dictionnaire, 40 ont acheté du lait et 7 ont acheté les deux en même temps. Cette règle qui n\u0027a, en réalité, aucun intérêt va tout de même présenter des indices de pertinence élevés (TAB. 3 ET 4). Selon le lift, le nombre d\u0027exemples de {dictionnaire?lait} est deux fois plus grand que sous l\u0027indépendance de {dictionnaire} et {lait}. Cela nous amène à proposer un autre indice de pertinence qui pénalise les règles où le conséquent est fréquent par rapport à l\u0027antécédent, l\u0027indice d\u0027accords désaccords (IAD), qui correspond à un indice proposé par Kulczynski (1927) :\naccords positifs IAD P( AC ) P( AC ) désaccords \u003d \u003d + Plus cet indice est grand, plus l\u0027antécédent et le conséquent sont présents simultanément. IAD peut également s\u0027exprimer de la manière suivante :\nL\u0027indice d\u0027accords désaccords est proche de l\u0027indice de Jaccard :\nLa différence entre les deux indices se situe au niveau du dénominateur : pour l\u0027indice IAD, c\u0027est un \"ou\" exclusif (différence symétrique) alors que c\u0027est un \"ou\" inclusif pour celui de Jaccard (union). Malgré cette différence, les deux indices sont parfaitement équivalents : ils conduisent à un classement identique des règles d\u0027association car :\nL\u0027indice de Jaccard présente l\u0027intérêt d\u0027être borné entre 0 et 1 (TAB. 5).\nTAB. 5 -Valeurs de référence pour les indices d\u0027accords désaccords et de Jaccard.\nDe manière empirique, IAD et Jaccard permettent une meilleure sélection des règles issues de nos données. Sur un exemple typique, comme celui du tableau 3, ils se montrent plus sévères que les autres indices retenus tels que le lift, en effet : IAD\u003d0,21 et Jaccard \u003d0,17. Ce résultat est généralisable à l\u0027ensemble de nos données mais en aucun cas à toutes les applications. \nComparaison graphique des indices de pertinences\nAfin de comparer graphiquement les indices, nous les avons exprimés en fonction des probabilités conditionnelles, notées A P(C / A)\nTAB. 6 -Equations des différentes courbes de niveaux.\nEnsuite, nous avons tracé des courbes de niveaux pour chaque indice (FIG. 1 ET 2). \nTAB. 7 -Comportement des indices de Jaccard et IAD.\nNote : D\u0027un point de vue marketing, la règle inverse {Caviar?Vodka} est aussi intéressante car le caviar est un produit de luxe rarement acheté. Aussi, il est intéressant de savoir que les consommateurs de caviar achètent systématiquement de la Vodka en même temps. Cette règle présente un conséquent beaucoup plus fréquent que son antécédent. Elle est donc de la même famille que {dictionnaire?lait}, qui est le type de règles que nous cherchons à sanctionner avec les indices de Jaccard et IAD, et qui ne sera donc pas retenu.\nUne application à un ensemble de règles\nNous disposons d\u0027un ensemble de plus de 80000 véhicules décrits par plus de 3000 attributs binaires rares. La recherche de règles d\u0027association sur ce jeu de données, avec un support minimum de 100 véhicules et une confiance minimum de 75%, conduit à plus de 1,5 millions de règles. Une classification de variables préalable (Plasse et al., 2005) a permis de réduire considérablement le nombre de règles candidates. Après avoir obtenu une partition en 10 classes, nous avons recherché les règles d\u0027association à l\u0027intérieur de chaque groupe. Nous avons ensuite établi un classement des règles obtenues selon les indices de pertinence décrits ci-dessus.\nClassements des règles\nLes graphiques suivants montrent les différences de classement selon les indices de pertinence, des règles contenues dans deux des dix classes obtenues : les classes A et B dans lesquelles se trouvent respectivement 19 et 29 règles candidates. \nAnalyse factorielle des classements\nUne analyse en composantes principales des rangs attribués à chaque règle par les diffé-rents indices confirme ce qui précède. Les deux premiers facteurs expliquent 95% de l\u0027inertie. Les cercles de corrélation des classes A et B sont identiques et aboutissent à une typologie des indices légèrement différente de celle de Vaillant et al. (2004) \n"
  },
  {
    "id": "1089",
    "text": "Introduction\nL\u0027exploitation de données textuelles issues de fonds scientifiques est un objectif de recherche ambitieux dans le domaine de la gestion et de l\u0027acquisition des connaissances. Une des premières étapes pour la mise en place d\u0027un système d\u0027information est la construction d\u0027une ontologie du domaine. Dans cette étude, nous abordons le problème de construction d\u0027une ontologie spécialisée avec une approche mixte (ou semi-automatique). Pour cela, nous nous intéressons à l\u0027étape d\u0027extraction automatique de classes terminologiques susceptibles d\u0027être ensuite validées comme concepts puis structurées par un expert du domaine, l\u0027embryon d\u0027ontologie résultant devant par la suite être enrichi automatiquement.\nLa tâche de regroupement de mots peut être envisagée de différentes manières (selon l\u0027application visée, les connaissances disponibles sur le domaine ou les traitements possibles). Les études proposées dans ce domaine s\u0027intéressent généralement à l\u0027une des deux étapes suivantes : la définition d\u0027une mesure de proximité entre mots et/ou la proposition d\u0027une méthode de regroupement efficace.\nIl existe de nombreuses mesures destinées à évaluer la proximité sémantique entre des mots. On peut classer ces mesures en trois grandes catégories : statistiques, syntaxiques ou utilisant une base de connaissances. Les mesures statistiques proposées se fondent le plus souvent sur l\u0027étude des cooccurrences de mots dans les textes ou parties de textes en utilisant l\u0027hypothèse de Harris et al. (1989) selon laquelle deux mots sémantiquement proches apparaissent souvent dans des contextes similaires. Ces contextes d\u0027utilisation peuvent être plus précisément repérés en identifiant la syntaxe des phrases. Par exemple Bouaud et al. (1997) analysent les relations de type nom-adjectif extraites de syntagmes nominaux et évaluent la proximité entre deux noms en comparant les ensembles de modifieurs (adjectifs) associés. Enfin, la proximité sémantique entre deux mots peut être appréhendée relativement à une base de connaissances structurée, comme par exemple un thésaurus ou une ontologie pré-existante dans le domaine. Les travaux de Rada et Bicknell (1989); Wu et Palmer (1994); Lin et Kondadadi (2001) consistent ainsi à rechercher dans le thésaurus WordNet la position relative des mots dans la hiérarchie de concepts.\nLes travaux dans le domaine du regroupement offrent également un assez large éventail de choix d\u0027algorithmes pour organiser un ensemble de mots en classes via une mesure de proximité sur cet ensemble. Les méthodes génériques de regroupement (par exemple k-moyennes (MacQueen, 1967), classification ascendante hiérarchique (Sneath et Sokal, 1973)) restent les plus utilisées malgré quelques propositions récentes d\u0027approches plus adaptées (Lelu, 1993;Turenne, 2000;Lin et Kondadadi, 2001;Pantel et Lin, 2002;Cleuziou et al., 2004).\nL\u0027orientation que nous proposons dans cette étude est fondée sur la définition d\u0027une nouvelle mesure de proximité utilisant les informations syntaxiques contenues dans les documents d\u0027un corpus spécialisé. Cette mesure est couplée avec une méthode de regroupement agglomé-ratif hiérarchique, adaptée aux besoins de l\u0027étude.\nL\u0027article est organisé comme suit : la section 2 présente le projet BIOTIM ainsi que certaines notions fondamentales du domaine de recherche. Les deux sections suivantes sont destinées respectivement à l\u0027étude des proximités entre mots (section 3) et à la proposition d\u0027une méthode de regroupement adaptée (section 4). Cette dernière partie présente également les premiers résultats expérimentaux sur un corpus de botanique. Une synthèse des avancées proposées dans cet article ainsi qu\u0027une discussion sur les nombreuses perspectives de ce travail sont présentées dans la dernière partie.\nContexte de l\u0027étude\nLe projet BIOTIM\nL\u0027étude menée s\u0027inscrit dans le cadre du projet BIOTIM 1 dont l\u0027objectif est de concevoir des méthodes génériques d\u0027analyse automatique de masses de données regroupant textes et images dans le domaine de la biodiversité. Nous nous intéressons, pour notre part, à la construction semi-automatique d\u0027une ontologie textuelle du domaine à partir de corpus botaniques.\nLa complémentarité des équipes associées au projet BIOTIM (Traitement du Langage Naturel, Apprentissage, experts du domaine, etc.), permet d\u0027assurer un traitement adapté aux particularités des données. L\u0027utilisation de termes spécialisés, la structure complexe des phrases rencontrées dans le corpus (longues descriptions, souvent sans verbe) et la masse importante de données à traiter sont autant de spécificités à prendre en compte.\nLe choix a été fait de ne pas laisser à l\u0027expert la difficile tâche d\u0027identifier seul les concepts du domaine. Il nous a semblé préférable de l\u0027assister pour cette étape stratégique en proposant des embryons de concepts potentiels, émergeant directement et automatiquement des corpus. Ainsi, le travail de l\u0027expert consistera à juger si un groupe de mots peut traduire ou non un concept du domaine.\nDans la suite de l\u0027étude nous utilisons le corpus de la \"Flore du Cameroun\", composé de 37 volumes et commercialisé par l\u0027Herbier National Camerounais. Chaque volume a fait l\u0027objet d\u0027une procédure de numérisation, à l\u0027origine de quelques erreurs d\u0027OCR (Optical Character Recognition).\nOntologies et dépendances syntaxiques\nLa chaîne de traitements 2 effectuée pour extraire un ensemble de termes à partir du texte brut est détaillée dans Rousse et de la Clergerie (2005). Les sorties de ce traitement linguistique sont des termes de la forme Nom-Adjectif ou Nom- (Prép.(Dét.))-Nom. Au total, près de 35 000 termes construits sur une base de plus de 12 000 lemmes (noms et adjectifs) ont ainsi été extraits sur le corpus de la \"Flore du Cameroun\". On dénombre par exemple 102 termes différents contenant le lemme \"foliole\" ; parmi les plus fréquents dans le corpus on peut citer les termes suivants : \"foliole terminal\", \"foliole oblong\", \"foliole à sommet\", \"paire de foliole\" ou encore \"feuille à foliole\".\nOn Nous présentons ci-dessous le principe de rapprochement de ces lemmes à partir de l\u0027analyse de leurs contextes d\u0027apparition dans les textes.\nModélisation en graphes\nPartant de l\u0027hypothèse de Harris, nous utilisons les \"dépendances syntaxiques\" entre lemmes à l\u0027intérieur des termes pour construire un graphe dont les sommets correspondent aux lemmes présents dans les termes. L\u0027existence d\u0027une arête entre deux sommets indique que les deux lemmes associés partagent des contextes identiques (Bouaud et al., 1997).\nConsidérons par exemple les termes \"arbre à feuille\" et \"arbre à foliole\" ; le contexte \"arbre à ?\" est partagé par les deux lemmes \"feuille\" et \"foliole\", favorisant ainsi leur liaison dans le graphe. Réciproquement on note que \"? à feuille\" et \"? à foliole\" correspondent à deux contextes d\u0027apparition pour le lemme \"arbre\".\nNous présentons en figure 1, un exemple de sous-graphe obtenu sur le corpus botanique. Il est d\u0027usage, pour cette modélisation en graphes, de recourir à un seuil afin de ne retenir que les dépendances dites non artificielles \nFIG. 1 -Exemple de sous-graphe représentant les dépendances syntaxiques entre lemmes dans le domaine de la botanique.\nclasses de sous-graphes (cliques, composantes connexes, etc.) on peut alors tenter de faire émerger des embryons de catégories sémantiques. Sur le corpus botanique que nous utilisons un traitement similaire à celui proposé par Bouaud et al. (1997) (un seuil minimum de 10 contextes partagés est requis pour placer une arête entre deux sommets) conduit aux mêmes observations, à savoir la présence dans le graphe d\u0027une composante connexe de taille importante accompagnée de très petites composantes connexes assez pertinentes d\u0027un point de vue sémantique.\nModélisation numérique\nParallèlement à cette modélisation en graphes, une approche numérique est possible. Le Moigno et al. (2002) \nintroduit alors plusieurs coefficients dérivés des graphes précédents :\nLe coefficient a correspond au nombre de contextes partagés par deux lemmes (par exemple a(feuille,fleur)\u003d10 d\u0027après le graphe de la figure 1).\nLa productivité d\u0027un lemme, notée prod(m), correspond au nombre de contextes différents dans lesquels ce lemme apparaît. De manière analogue, la productivité d\u0027un contexte, prod(c) correspond au nombre de lemmes différents apparaissant dans ce contexte. Par exemple, une analyse du lemme \"foliole\" sur le corpus botanique montre que 102 termes diffé-rents contiennent ce lemme ; \"foliole\" apparaît alors dans 102 contextes distincts (prod(foliole) \u003d 102). Inversement, seuls les lemmes \"sommet\", \"nervation\", \"marge\" et \"pé-tiole\" apparaissent dans le contexte \"foliole à ?\" (prod(foliole à ?) \u003d 4).\nLe coefficient prox utilise cette notion de productivité. Ce coefficient formalise l\u0027intuition que si un contexte est très productif sa contribution dans le rapprochement de deux mots est plus faible que celle d\u0027un contexte peu productif. Soit C i (resp. C j ) l\u0027ensemble des contextes d\u0027apparition du lemme m i (resp. m j ), prox est défini par\nLe coefficient J (non symétrique) tente enfin de formaliser le déséquilibre pouvant exister entre un mot très productif et un autre peu productif :\nJ(m i , m j ) sera d\u0027autant plus élevé que m i partage beaucoup de ses contextes avec m j . La formalisation numérique entraîne nécessairement une perte d\u0027information : par exemple on ne retient que le nombre de contextes partagés par deux lemmes et non la liste de ces contextes. Cependant nous montrerons qu\u0027il est possible de tenir compte de ce dernier aspect dans le processus de regroupement. Dans la suite, nous nous attachons à définir une mesure globale de proximité entre deux lemmes, définie à partir des différentes notions précédentes.\n3 Une mesure de proximité non symétrique Le Moigno et al. (2002) ont introduit, via le coefficient J, la notion de déséquilibre à propos de la proximité entre deux mots. L\u0027idée est alors de considérer à la fois ce qui rapproche deux mots (leurs contextes partagés) et ce qui les différencie (leurs contextes propres).\nConsidérons par exemple les deux mots \"pétale\" et \"fleur\". On observe sur le corpus les caractéristiques suivantes : a(fleur,pétale)\u003d54, prod(fleur)\u003d284 et prod(pétale)\u003d196. Nos connaissances générales dans le domaine nous permettent de dire qu\u0027un \"pétale\" est une partie d\u0027une \"fleur\". La notion de \"pétale\" est donc sémantiquement très dépendante de celle de \"fleur\" tandis que l\u0027inverse n\u0027est pas vrai. La seule donnée du coefficient a ne permet pas d\u0027observer cette propriété tandis que l\u0027information supplémentaire apportée par les productivités respectives des deux mots le permet : \"pétale\" partage plus de 27% de ses contextes avec \"fleur\" alors que \"fleur\" n\u0027en partage que 19% avec \"pétale\".\nCette vision relative du nombre de contextes partagés permet de faire émerger des dissymétries dans les proximités et nous encourage alors à proposer une mesure qui tienne compte de ces deux informations (nombre de contextes partagés et non partagés) mais également du fait que la proximité entre deux mots n\u0027est pas nécessairement une notion symétrique.\nDe même que prox est une extension du coefficient a, nous définissons le coefficient ? par extension du coefficient J, en introduisant la notion de productivité sur les contextes. Soient m i et m j deux lemmes, C i et C j les contextes d\u0027apparition associés :\nLe coefficient J concerne l\u0027aspect quantitatif de la proportion de contextes partagés relativement à la productivité d\u0027un mot tandis que le coefficient ? introduit une dimension qualitative en considérant, en plus, la qualité de ces contextes à travers leur productivité. Ce coefficient sera donc d\u0027autant plus élevé que les coefficients partagés par les deux mots sont peu productifs.\nNous présentons ci-dessous les propriétés vérifiées par ? (pour tout couple de mots (m i , m j ) : Les résultats obtenus sont encourageants pour la suite du processus de construction de classes terminologiques. Nous nous attachons dans ce qui suit à présenter une méthode agglomérative adaptée au regroupement de mots d\u0027après leurs contextes d\u0027apparition dans les textes.\nLe regroupement de mots\nLa problématique générale du regroupement (ou clustering) consiste à organiser un ensemble d\u0027objets en groupes de façon à ce que deux objets similaires se retrouvent dans un même groupe et deux objets dissimilaires dans des groupes distincts. De nombreuses stratégies ont été proposées pour répondre à cette problématique (Jain et al., 1999), comme par exemple les approches par partitionnement (k-moyennes), les algorithmes hiérarchiques (agglomératifs ou divisifs), les méthodes utilisant des mélanges de densités de probabilité, des découpages en grilles, etc.\nLa plupart des travaux présentés dans le domaine du regroupement de données textuelles (chaînes graphiques, lemmes, termes, mots-clés, documents, etc.) se focalisent davantage sur le sens à donner à la notion de proximité que sur l\u0027algorithme permettant de regrouper les unités textuelles considérées. Certaines études proposent cependant des approches originales afin de regrouper des objets textuels en tenant compte de leurs spécificités telles que la polysémie d\u0027un mot ou l\u0027aspect multi-thématique d\u0027un document (Lelu, 1993;Pantel et Lin, 2002;Cleuziou et al., 2004). Malgré ces travaux récents et marginaux, l\u0027étape de regroupement reste généralement réalisée par les méthodes classiques (algorithme des k-moyennes ou approche agglomérative) car simples et maîtrisées par les utilisateurs.\nUne méthode de regroupement adaptée\nDans notre travail, nous proposons d\u0027adapter le processus de regroupement aux données dont nous disposons (lemmes et contextes associés) ainsi qu\u0027à la tâche visée (aide à l\u0027élabora-tion d\u0027une ontologie). L\u0027approche présentée ici est une adaptation de l\u0027algorithme agglomératif hiérarchique du lien moyen. Ce dernier procède par fusions successives des deux plus proches groupes 5 , en partant des feuilles (un objet par groupe) et aboutissant à une racine (tous les objets dans un même groupe). Ce type d\u0027approche présente l\u0027avantage de conserver une trace de l\u0027élaboration des groupes à travers l\u0027arbre hiérarchique (ou dendrogramme) construit. En revanche, un problème récurrent pour cette méthode est la recherche des groupes pertinents parmi l\u0027ensemble des noeuds de l\u0027arbre.\nPour cela nous choisissons d\u0027interdire l\u0027agglomération autour d\u0027un groupe lorsque cette fusion conduit à un groupe conceptuellement non pertinent (cf. définition 4.1 ci-dessous). La structure ainsi obtenue est une hiérarchie partielle, soit un ensemble de (petits) dendrogrammes (cf. définition 4.2 ci-dessous).\nDéfinition 4.1 Soient P un groupe constitué des lemmes {m 1 , . . . , m n } et C 1 , . . . , C n les ensembles de contextes d\u0027apparition associés à chacun d\u0027eux, P est conceptuellement non pertinent si il n\u0027existe aucun contexte dans lequel apparaissent l\u0027ensemble des lemmes de P :\nDéfinition 4.2 Soit H un ensemble de parties non-vides sur un ensemble d\u0027objets X, H est une hiérarchie partielle si les propriétés suivantes sont vérifiées :\nL\u0027ajout de la propriété \"X ? H\" permet de se ramener à la définition formelle classique d\u0027une hiérarchie (complète).\nL\u0027algorithme agglomératif hiérarchique adapté au regroupement de mots relativement à leurs contextes d\u0027apparition est le suivant : initialement chaque lemme constitue un groupe à lui seul (feuille) auquel est associé une caractérisation (ensemble des contextes d\u0027apparition du lemme) ; à chaque itération, on recherche parmi les fusions possibles (respect de la contrainte de pertinence) celle qui permet d\u0027agglomérer les deux groupes les plus proches selon la mesure de proximité spécifiée. De cette fusion résulte un nouveau groupe auquel est associée une nouvelle caractérisation \"mère\", intersection des deux caractérisations \"filles\". La matrice des proximités entre groupes est mise à jour. Lorsque la contrainte de pertinence interdit toute fusion, l\u0027agglomération est terminée et l\u0027algorithme retourne l\u0027ensemble des groupes constitués, les arbres hiérarchiques et les caractérisations associées. On pourra choisir de ne considérer par la suite que les items associés à des groupes de taille supérieure à 1.\nL\u0027ajout d\u0027une contrainte de pertinence est essentiel dans cet algorithme. L\u0027utilisation du lien moyen pour évaluer la proximité entre deux groupes P i et P j permet de considérer tous les couples de lemmes dans P i £ P j (contrairement aux liens simple et complet). Cependant cette information seule n\u0027apporte aucune garantie quant à l\u0027existence d\u0027une propriété commune à l\u0027ensemble des objets des deux groupes. Cette \"propriété commune\" est pourtant indispensable pour définir un concept. La contrainte de pertinence apporte cette garantie ; les lemmes d\u0027un groupe apparaissent tous dans un même contexte au minimum. De plus la caractérisation d\u0027un groupe aidera l\u0027expert à proposer une étiquette au concept associé.\nApplication aux données botaniques\nNous avons testé l\u0027algorithme de regroupement sur les lemmes extraits du corpus botanique. Parmi les 12 000 lemmes repérés, nous avons sélectionné ceux partageant au moins trois contextes avec un autre lemme, restreignant ainsi à 2 024 la quantité de données à traiter.\nLa mesure de proximité utilisée est la mesure p, en choisissant comme proximité pour deux lemmes donnés m i et m j , la plus petite des deux valeurs possibles par p :\nQuelques groupes obtenus sont présentés dans les figures 2 et 3. Les dendrogrammes proposés sont ceux mettant en jeux les 10 premières fusions (itérations de l\u0027algorithme). Ces arbres hiérarchiques sont représentatifs de l\u0027ensemble des résultats obtenus. On peut les organiser en trois catégories en fonction des termes mis en jeu : les termes spécifiques au domaine, les termes génériques et enfin ceux relevant des abréviations des noms propres ou des mots étrangers.\nLes arbres mettant en jeu des termes spécifiques au domaine (figures 2 et 3, arbres c, d, e, f et j) sont difficiles à évaluer pour des lecteurs non-experts du domaine. On peut malgré tout appréhender la sémantique globale de certains groupes : par exemple le groupe f est la repré-sentation textuelle du concept \"aspect du limbe\"\n6\n. D\u0027autres concepts spécifiques émergent de l\u0027analyse de l\u0027ensemble des résultats, par exemple la \"forme du sépale\" ou plus généralement du lobe (linéaire-lancéolé, ovale-lancéolé, ensiforme), la \"forme d\u0027une foliole ou d\u0027un lobe\" (deltoïde, linéaire, ovale, rhomboïde, falciforme, cunéiforme, polymorphe), l\u0027\"apparence\" que peut prendre une espèce végétale (plante, herbe, liane, arbrisseau), etc.\nLes arbres contenant des termes génériques sont cette fois plus faciles à évaluer (figure 3, arbres g, h et i). Leur analyse vient confirmer l\u0027impression de qualité puisque les groupes observés peuvent effectivement être associés à des concepts du domaine :\n-g est une représentation textuelle du concept de \"couleur d\u0027écorce\", -h est une représentation textuelle du concept de \"variantes de couleurs\" (en particulier pour les teintes noir, marron et jaune), -i est une représentation textuelle du concept d\u0027 \"unité de mesure\" (en particulier pour indiquer la hauteur des végétaux). Ces concepts peuvent notamment être identifiés plus facilement grâce à la caractérisation proposée. Parmi les résultats non présentés ici, on retrouve d\u0027autres concepts simples tels que : la \"forme\" associée à un élément d\u0027une plante (bec, dôme, languette, ruban, gouttière), les \"points cardinaux\" (nord, sud, ouest, Ouest), les \"mois\" du calendrier (janvier, mai, août, novembre, décembre), etc.\nEnfin, les arbres de la dernière catégorie (figure 2, arbres a et b), sont composés en grande partie de termes correspondant à des abréviations des noms propres ou des mots étrangers. Les termes de ce type pourront être supprimés en sélectionnant dans les documents, uniquement les parties descriptives de plantes (travail en cours de réalisation). Pour effectuer une synthèse de nos premières analyses, nous pouvons conclure à la pertinence globale des groupes obtenus et noter l\u0027aide précieuse apportée par la caractérisation associée à chaque groupe. Ce résultat est imputable pour partie à la mesure de proximité proposée mais également à l\u0027adaptation de l\u0027algorithme de classification.\nConclusion et perspectives\nNotre étude s\u0027est focalisée sur la tâche de regroupement de mots dans un domaine de recherche plus vaste qu\u0027est la construction semi-automatique d\u0027ontologies spécialisées. Nous avons défini une nouvelle mesure de proximité entre mots d\u0027une part, et proposé une méthode de regroupement adaptée d\u0027autre part.\nLa mesure de proximité présentée se place dans la lignée des mesures utilisant les dépen-dances syntaxiques. Contrairement aux précédentes propositions, nous ne considérons pas la proximité comme une notion symétrique. L\u0027algorithme de regroupement utilisé est une adaptation des méthodes de classifications ascendantes hiérarchiques. Plutôt que d\u0027aboutir à un arbre hiérarchique complet, c\u0027est un dendrogramme partiel qui est élaboré par l\u0027ajout d\u0027une contrainte de cohérence sur les fusions effectuées. Finalement, tous les objets initiaux ne sont pas nécessairement utilisés, les groupes obtenus sont de petite taille et complétés par des informations structurelles (dendrogramme) et conceptuelle (caractérisation).\nDans le cadre du projet BIOTIM, ce travail a été appliqué sur des textes du domaine de la botanique. Les résultats obtenus avec la mesure de proximité mettent en évidence des paires de lemmes pertinentes. Ce résultat est confirmé par l\u0027analyse des groupes finalement obtenus en utilisant cette mesure couplée à la méthode de classification proposée. En effet la plupart de ces groupes semble correspondre à des concepts du domaine.\nNous envisageons actuellement d\u0027améliorer la qualité des groupes obtenus en travaillant selon deux axes :\n-réduire l\u0027impureté par le renforcement des contraintes de cohérence appliquées aux fusions (extraction d\u0027ensembles de contextes fréquents), -proposer des groupes exhaustifs en autorisant la réutilisation de lemmes déjà agglomérés pour définir de nouveaux concepts (approches pyramidales).\nParallèlement à ces perspectives, nous tâcherons de mettre en évidence les fortes analogies qui existent entre les mesures de proximité étudiées dans cet article et les indices proposés dans le cadre d\u0027approches plutôt probabilistes utilisant les cooccurrences : information mutuelle, coefficient de Dice, etc. (Cleuziou et al., 2003).\nEnfin nous développerons une interface de validation destinée aux experts du domaine. Cette interface permettra d\u0027une part d\u0027aider les experts à construire l\u0027ontologie en intervenant sur le processus de classification (validation/structuration des concepts) et d\u0027autre part d\u0027éva-luer quantitativement la pertinence de la méthodologie proposée.\n"
  },
  {
    "id": "1090",
    "text": "Introduction\nAssociées notamment au succès des nouveaux langages du Web sémantique, les ontologies suscitent un intérêt croissant au sein des communautés de l\u0027ingénierie et de la gestion des connaissances (Gruber, 1993;Fürst, 2004). Cependant, malgré le développement d\u0027outils d\u0027aide à leur manipulation, le développement et l\u0027exploitation des ontologies restent des phases complexes dans un processus global de gestion de connaissances. En amont, une des difficultés majeures concerne la structuration des ensembles de concepts dont la taille ne cesse de croître. Et en aval, le problème consiste à rechercher efficacement des sous-ensembles de concepts à la fois en temps de calcul et en pertinence sémantique des résultats.\nPour faciliter ces tâches, le recours à des mesures sémantiques semble judicieux ; il permet de constituer une « connaissance heuristique » directement exploitable. De façon générale, une mesure sémantique est une application de l\u0027ensemble C×C des paires de concepts d\u0027une ontologie dans IR + qui permet d\u0027évaluer quantitativement la proximité ou l\u0027éloignement sémantique de deux concepts. Quelque soit le domaine applicatif, la pertinence de la mesure utilisée est étroitement associée à l\u0027efficacité des algorithmes qui l\u0027intègrent. Cependant, son choix reste un problème délicat. Pour comparer les mesures existantes, plusieurs approches complémen-taires sont envisageables (Budanitsky, 1999). L\u0027analyse formelle vise à étudier précisément leurs propriétés à la fois algorithmiques et statistiques. La comparaison avec le jugement humain analyse la corrélation entre les valeurs des mesures et les évaluations subjectives de sujets humains. L\u0027évaluation applicative restreint l\u0027expérimentation à un ou plusieurs cadres applicatifs bien identifiés. Dans cet article, nous nous centrons sur une analyse formelle. Nous nous restreignons ici aux relations d\u0027hyperonymie et d\u0027hyponymie associées au lien hiérarchique (is-a). Ce lien qui est commun à la majorité des ontologies est généralement celui autour duquel s\u0027organise une partie de la structuration des concepts (Rada et al., 1989). Notons que la plupart des mesures sémantiques proposées dans la littérature se restreignent également à ce lien.\nDans une première partie nous rappelons dans un cadre formel unifié les définitions des principales mesures utilisées dans la littérature. Nous distinguons les mesures basées uniquement sur l\u0027information issue de l\u0027ontologie de celles utilisant en complément un corpus de textes. Aucune des mesures étudiées n\u0027exploite complètement l\u0027information qui caractérise la proximité sémantique entre concepts sans utiliser un corpus de textes en complément de l\u0027ontologie. Pour palier ces limitations, nous proposons ici une nouvelle mesure de similarité : la PSS (Proportion of Shared Specificity). Celle-ci est indépendante de tout corpus et intègre l\u0027ensemble des paramètres mis en évidence lors de notre étude.\nPrincipales mesures existantes\nSoit C l\u0027ensemble des concepts de l\u0027ontologie considérée, A ? C × C l\u0027ensemble des arcs traduisant une relation soit d\u0027hyperonymie soit d\u0027hyponymie entre les concepts de C, e : A ?? {hyper, hypo} une fonction qui associe à chaque arc un type de relation. Une ontologie peut être représentée par un 1-graphe G(C) \u003d (C, A, e) connexe orienté sans boucle (Berge, 1973)  D\u0027une façon générale, on peut distinguer deux grandes familles de mesures : celles qui extraient de l\u0027information uniquement à partir d\u0027une ontologie et celles qui utilisent un corpus de textes en complément de l\u0027ontologie. Le corpus de textes est utilisé comme échantillon statistique dont on extrait le nombre d\u0027ocurrences de chaque concept de l\u0027ontologie. On en déduit alors pour chaque concept, la fréquence d\u0027occurrence de ce concept ou de l\u0027un des concepts qu\u0027il subsume directement ou indirectement. Cette fréquence est souvent interprétée -parfois abusivement -comme une probabilité dans la littérature ; nous la notons donc P (c i ) pour c i ? C. Parmi les mesures les plus fréquentes dans la littérature, considérons ici deux mesures qui se basent uniquement sur une ontologie et deux autres qui utilisent un corpus en complément : Rada et al. (1989). Cette distance sémantique est simplement fonction du chemin élémen-taire entre deux concepts c i et Wu et Palmer (1994). Cette similarité tient également compte de la longueur du chemin d\u0027origine c i et d\u0027extrémité c j mais aussi de la profondeur de leur subsumant commun le plus spécifique, autrement dit de la longueur du chemin d\u0027\nlenn(che(ci,cj ))+2 * lenn(che(mscs(ci,cj ),c0)) . Resnik (1995). Cette similarité repose sur l\u0027hypothèse selon laquelle plus deux concepts partagent d\u0027information en commun, plus ils sont similaires. Sur la base de la théorie de l\u0027information, l\u0027auteur propose de considérer le contenu informationnel des concepts : CI(c i ) \u003d ? log(P (c i )). L\u0027information partagée par deux concepts est alors égale au contenu informationnel de leur subsumant commun le plus spécifique : sim r (c i , c j ) \u003d ? log P (mscs(c i , c j )).\nLin (1998). Cette similarité, qui fait partie des plus étudiées sur le plan théorique, tient compte de l\u0027information partagée par les deux concepts comme Resnik, mais aussi de ce qui les distingue : sim l (c i , c j ) \u003d Nous avons également étudié d\u0027autres mesures intéressantes d\u0027un même point de vue théo-rique (Sussna, 1993;Leacock et Chodorow, 1998;Hirst et St-Onge, 1998) La comparaison de mesures issues des deux différentes familles n\u0027est pas évidente a priori. La clé de cette comparaison réside dans l\u0027algorithme de calcul du contenu informationnel. Les occurrences de chaque concept sont comptabilisées par un balayage du corpus et l\u0027occurrence d\u0027un concept est prise en compte également pour tous les concepts qui le subsument. Cet algorithme de construction confère des caractéristiques à P (c i ) relatives à la structure de l\u0027ontologie. En effet, si on considère c i appartenant à un chemin élémentaire allant de la racine à un concept quelconque, P (c i ) décroît exponentiellement en fonction de la profondeur de c i , et ce plus ou moins vite en fonction des densités locales (nombre de fils d\u0027un concept) des concepts appartenant à ce chemin élémentaire.\nChacune des mesures repose sur une axiomatisation qui a guidé son élaboration. Notre étude (Blanchard et al., 2005) nous a permis de cerner toutes les propriétés de l\u0027ontologie exploitées par ces mesures et d\u0027en faire une synthèse sous la forme de quatre paramètres dans 0 , mscs(c i , c j )\nG(C). Les longueurs des chaînes élémentaires che(c\n)).\nLes mesures basées sur le contenu informationnel sont sensibles à la densité locale au niveau des concepts appartenant à l\u0027un des chemins élémentaires che(c i , c j ) et che(c 0 , mscs(c i , c j )). On dégage alors les deux nouveaux paramètres\nNotons qu\u0027une mesure sensible à p 3 (resp. p 4 ) est sensible à p 1 (resp. p 2 ) tandis que la réci-proque n\u0027est pas vraie.\nIl faut souligner que la mesure de Sussna est la seule qui prenne en compte la densité des concepts sur che(c i , c j ) sans utiliser un corpus. Finalement, aucune des mesures étudiées qui n\u0027utilisent pas de corpus n\u0027est sensible aux quatre paramètres.\n3 Une nouvelle similarité sémantique : la proportion de spé-cificité partagée\nDevant les limites des mesures existantes, nous avons cherché à proposer une nouvelle mesure qui n\u0027utilise que l\u0027ontologie et qui soit sensible à tous les paramètres évoqués. Seule la mesure de Lin est sensible à l\u0027ensemble des paramètres, mais elle utilise un corpus. Nous avons donc adapté sa définition de manière à s\u0027en passer. Sans corpus, on ne peut pas calculer la probabilité P (c i ), c\u0027est pourquoi nous estimons cette probabilité par la seule considération de la structure de l\u0027ontologie. La mesure de Lin permet de tenir compte à la fois de ce que les concepts ont en commun et de ce qu\u0027ils ont de différent. Sur la base de ces propositions, Lin propose une définition générique de sa mesure qui n\u0027est pas utilisable en l\u0027état puisqu\u0027elle doit être instanciée : sim(c i , c j ) \u003d\nIC(commun(ci,cj )\nIC (description(ci,cj ) . Le calcul de la quantité d\u0027information IC se base comme dans la définition de Resnik sur la théorie de l\u0027information en utilisant la notion d\u0027information propre qui correspond au logarithme négatif de la probabilité d\u0027occurrence IC(c i ) \u003d ? log(P (c i )). Cette notion traduit l\u0027évolution de l\u0027information portée par un concept qui croît avec sa rareté. La comparaison de deux concepts c i et c j de C, revient à comparer deux instances quelconques x i et x j de ces deux concepts. Les deux propositions précédentes peuvent être traduites par les événements suivants : \nSous l\u0027hypothèse d\u0027une distribution uniforme du nombre d\u0027instances associées à chaque concept, on peut montrer que :\nCette formule intègre à la fois la profondeur du concept c i par une définition récursive et la densité de liens pour les concepts qui subsument c i . Notons que dans le cadre plus large d\u0027une hiérarchie non disjonctive (un concept peut alors avoir plusieurs hyperonymes) nous utilisons pour des contraintes de complexité algorithmique cette estimation comme approximation de P (c i ) en considérant le plus court chemin élémentaire.\nSi l\u0027ontologie comporte une racine virtuelle dont on peut considérer qu\u0027il n\u0027existe pas de subsumant, il faut considérer que deux concepts quelconques de la taxonomie n\u0027ayant que la racine en commun ont une similarité nulle et pour cela P (c 0 ) est fixé à 1. Dans le cas contraire, le choix d\u0027un entier k devra être fait pour fixer P (c 0 ) à 1/k : P (c 0 ) \u003d 1 (ou 1/k)\nConclusion\nCet article met en avant les points clés de certaines mesures qui évaluent les liens séman-tiques entre deux concepts d\u0027une ontologie. L\u0027étude des paramètres propres à l\u0027ontologie qui influençent ces mesures nous a conduit à l\u0027élaboration d\u0027une nouvelle mesure que présente cet article. Nous nous sommes basé principalement sur la mesure de Lin pour définir une mesure de similarité -la proportion de spécificité partagée -qui est sensible à l\u0027ensemble des paramètres précédemment isolés.\nUne comparaison des différentes mesures sur un échantillon d\u0027un millier de concepts de WordNet 2.0 nous a permis de mettre en évidence d\u0027une part les bonnes capacités de discrimination de la PSS, et d\u0027autre part une corrélation positive avec la mesure de Lin, qui elle nécessite un corpus additionnel. Cette mesure pourra être utilisée dans des applications né-cessitant une certaine précision et où aucun corpus n\u0027est disponible. Un autre intérêt de cette mesure est d\u0027avoir une sémantique basée sur des propriétés formelles explicites qui peuvent être appréhendées plus facilement par un expert que les mesures recourant à un corpus.\nNous avons choisi comme cadre expérimental un des référentiels organisé sous forme d\u0027une ontologie parmi les plus accessibles actuellement. Nos premières comparaisons numériques nous ont permis de confirmer la pertinence de notre indice par rapport aux indices précédem-ment proposés dans la littérature. Dans le cadre de nos recherches actuelles en gestion des connaissances (Berio et Harzallah, 2005), nous prévoyons de mener une analyse comparative sur une ontologie d\u0027entreprise.\n"
  },
  {
    "id": "1091",
    "text": "Introduction\nLa fouille de données définie comme étant l\u0027extraction à partir de données brutes de connaissances potentiellement exploitables, n\u0027en demeure pas moins un processus complexe dès lors qu\u0027il s\u0027agit d\u0027interpréter les résultats fournis. Les techniques de fouille de données représentent une étape fondamentale du processus d\u0027Extraction de Connaissances dans les Bases de Données connu sous le nom ECD ou KDD (Knowledge Discovery in Databases) (Han 2001).\nDans ce papier nous nous intéressons à l\u0027une de ces techniques : la classification non supervisée. Celle-ci est définie comme un ensemble de processus aptes à être exécutés sur ordinateur pour constituer des hiérarchies de classes ou de simples partitions établies à partir de tableaux de données (Jambu 1978). Les règles d\u0027interprétation des structures classificatoires obtenues (hiérarchies, partitions, etc.) à l\u0027issue de ces classifications n\u0027ont pas la simplicité des méthodes descriptives uni-dimensionnelles.\nNotre objectif, dans ce travail, est de proposer une aide aux utilisateurs afin d\u0027interpréter les résultats des méthodes de classification. En effet, les modules de classification proposent des techniques de visualisation des résultats très intéressantes et conviviales (Song 1998), (Sprenger et al. 2000), (Wills 1998) mais la plupart ont fait l\u0027impasse sur la structuration des résultats.\nEn partant de ce constat, nous proposons une nouvelle approche qui consiste à utiliser les métadonnées comme moyen de représentation des connaissances capitalisées au cours du processus de classification.\nLes métadonnées sont souvent définies comme étant des données sur les données (Grossmann 1999), (Kent et al. 1997). Elles sont aussi définies comme un ensemble d\u0027informations pertinentes pour la collecte, le traitement, la diffusion, l\u0027accès, la compréhension et l\u0027utilisation des données (Zeila, 2004). En ce sens, elles peuvent aider à comprendre dans quelles circonstances les données originales ont été collectées et de quelle manière elles ont été agrégées puis classifiées.\nDans ce travail, nous proposons une architecture basée sur notre modèle de métadonnées (Baldé et Aufaure, 2005)  \nNotre approche\nL\u0027aide à l\u0027interprétation consiste en toute technique ou calcul qui permet d\u0027éprouver le bien fondé des classes obtenues en rendant raison de la formation de celles-ci (Jambu 1978). Dans la section suivante nous allons présenter notre architecture basée sur le modèle de métadonnées.\nArchitecture\nL\u0027architecture que nous présentons exploite les métadonnées produites au cours du processus de classification automatique (figure 1).\nCette architecture est constituée d\u0027un ensemble de couches définies ci-après : -un modèle de métadonnées; -un gestionnaire de métadonnées : qui va servir de tampon entre notre modèle de métadonnées et les manipulations qui y seront effectuées; -une couche gérant des requêtes d\u0027utilisateurs en interrogeant le gestionnaire de métadonnées. C\u0027est cette dernière couche qui traite les requêtes des utilisateurs exprimées en Xquery. Cette couche traite les requêtes exprimées en Xquery (Chamberlain 2004). Pour implémenter ces requêtes nous avons utilisé le processeur Saxon 2 . Contrairement à d\u0027autres processeurs comme Berkeley DB XML 3 , Saxon est un ensemble d\u0027outils dédiés aux traitements des documents XML et est performant en terme de rapidité et conforme aux spécifications du W3C.\nNotre approche présente l\u0027avantage qu\u0027il ne soit plus nécessaire de procéder à des modifications de certains critères (relatifs à l\u0027homogénéité par exemple) et de relancer le module de classification pour observer le gain ou la perte d\u0027homogénéité. Le but est, à partir des résultats des requêtes, de pouvoir comparer plusieurs scénarios d\u0027interprétation.\nFIG. 1 -Architecture de production et de traitement des métadonnées\nNotre objectif étant d\u0027aider les utilisateurs dans l\u0027interprétation de leurs résultats, nous avons mis à leur disposition un certain nombre de scénarios d\u0027interprétation (exprimés en Xquery) définis par les experts du domaine. Ceux-ci ont permis d\u0027interpréter des résultats de modules de classification tels que Sclust 4 (Chavent et al. 2003). Cependant l\u0027utilisateur a la possibilité de modifier les critères utilisés pour réaliser son propre scénario. A travers cet outil nous lui donnons la possibilité d\u0027interpréter les résultats dont il dispose et surtout de manipuler automatiquement ceux-ci.\nProcessus d\u0027extraction\nLes éléments de métadonnées extraits sont de deux types : les métadonnées correspondant aux informations fournies par l\u0027utilisateur et celles qui sont associées aux données et aux résultats de la classification. Voici des exemples de métadonnées fournies par l\u0027utilisateur : le nombre de classes, les informations sur l\u0027auteur, la description de la méthode de classification, la distance utilisée, l\u0027unité de mesure utilisée pour certaines valeurs de données, etc… Les métadonnées liées aux données et aux résultats sont par exemple : les paramètres de la méthode de classification, le nombre d\u0027individus dans une classe, la source des données originales, la description des variables, l\u0027usage des variables (active, prédictive,…), les valeurs des critères d\u0027hétérogénéité et/ou d\u0027isolation pour chaque classe, la contribution de chaque variable dans la construction de chaque classe, etc… Ces métadonnées sont extraites au cours de l\u0027exécution de l\u0027algorithme de classification. En sortie de l\u0027algorithme, nous obtenons le fichier de métadonnées. Ce fichier va nous servir de base à la réalisation des requêtes d\u0027interprétation. L\u0027utilisateur pourra effectuer les calculs qu\u0027il souhaite afin de mieux affiner son interprétation. Cette flexibilité permettra d\u0027interpréter au mieux les résultats suivant le domaine visé par la classification. Par exemple un utilisateur privilégiera le critère f-mesure ou le critère d\u0027inertie intraclasse ou encore la contribution marginale des variables, etc… Pour rendre ces fichiers de métadonnées plus lisibles et mieux compréhensibles par les utilisateurs nous avons utilisé le processeur XSLT (Kay 2001). Pour le traitement des requêtes posées par les utilisateurs, nous avons utilisé le processeur Saxon.\nLa description des critères utilisés dans ce travail est dans (Jambu 1978). Il faut savoir que la relation fondamentale dans une interprétation, basée sur l\u0027inertie, est : T \u003d B +W où T est l\u0027inertie totale indépendante de la partition, B l\u0027inertie de nuage des centres de gravité munis de poids (l\u0027inertie interclasse) et W l\u0027inertie d\u0027une classe k par rapport à son propre centre de gravité (l\u0027inertie intra classe).\nExpérimentation\nPour cette étude de cas, le module de classification utilisé est Sclust. Ce module est utilisé pour partitionner un ensemble d\u0027individus décrits par des données symboliques (Bock et Diday 2000) en un nombre k de classes homogènes. Pour interpréter ces classes obtenues, l\u0027utilisateur dispose uniquement d\u0027un fichier listing inexploitable algorithmiquement et mal structuré.\nLes données qui sont traitées proviennent des navigations recensées sur les deux serveurs de l\u0027INRIA (siège et Sophia) pour la période du 1 er au 15 janvier 2003. Ces navigations ont été prétraitées suivant un certain nombre de critères (El Golli et al.2005) avant de procéder à leur classification. Les informations sur l\u0027outil de prétraitement sont décrites dans (Tanassa et Trousse 2004) ou sur le site web www-sop.inria.fr/axis/axislogminer. Le but de la classification est de voir si les projets de recherche qui ont des activités scientifiques ou tout au moins des centres d\u0027intérêt communs se retrouvent en analysant uniquement le parcours des internautes sur ces deux sites. Le tableau de données est composé de 100 groupes de navigations décrits par 127 variables.\nSupposons que l\u0027utilisateur soit intéressé par la contribution de chacune des variables dans la formation des classes, alors en utilisant notre outil il obtiendra l\u0027ensemble des variables ayant une contribution supérieure au seuil qu\u0027il a fixé. Par exemple, l\u0027utilisateur cherche des variables ayant une contribution supérieure à 1.5 fois la moyenne. Il obtient : Pour conclure, nous pouvons dire que les métadonnées peuvent assister les utilisateurs dans la recherche de l\u0027information, dans l\u0027interprétation de leur contenu et elles peuvent aider dans les post-traitements des classes construites. Nos travaux futurs s\u0027articuleront autour de la création d\u0027une ontologie du domaine de la classification. Pour ce faire nous nous appuierons sur les travaux réalisés au niveau des méthodes de fouille de données par (Cannataro 2003). Cette ontologie permettrait une interprétation automatique des classes et des partitions obtenues par des modules de classification.\nUne autre perspective à ce travail serait d\u0027utiliser des techniques de visualisation pour améliorer le processus d\u0027interprétation des résultats. Les métadonnées pourront aussi servir à aider dans la détermination du bon nombre de classes.\nSummary\nA huge volume of data is produced by many applications. Data mining techniques are used to extract knowledge from this mass of information since it is no longer possible to manually examine this data. In the meantime, the interpretation of the results obtained by applying data mining techniques is not easy. In this paper, we focus on unsupervised learning, and we propose a tool in order to help the end-user to interpret the clusters obtained. Our objective is to facilitate the interpretation process and to point out that metadata can play a major role for this purpose. Metadata will help the user to understand how the original data has been collected, aggregated and then classified. One of the characteristics of this work is that users have the possibility of carrying out the calculations that they wish. These calculations were done by using Xquery queries. To validate our work, we present an example.\n"
  },
  {
    "id": "1092",
    "text": "Introduction\nUn des objectifs de l\u0027extraction de connaissances à partir de données consiste à fournir des énoncés valides et utiles aux utilisateurs propriétaires de ces données. L\u0027utilité de ces énoncés est d\u0027autant plus grande qu\u0027ils décrivent une réalité du domaine non encore explicitée jusqu\u0027ici, autrement dit, une nouvelle connaissance. Nous nous intéressons à l\u0027extraction de connaissances au moyen de règles descriptives comme les règles d\u0027association (Agrawal et al., 1993). Les problèmes posés par l\u0027extraction de telles règles ont été étudiés intensivement ces dix dernières années. Bien que l\u0027extraction de toutes les règles fréquentes et valides soit difficile dans de grands jeux de données, des dizaines d\u0027algorithmes efficaces ont été proposés (Goethals et Zaki, 2003, par exemple). Un second problème concerne le nombre considérable de règles qui peuvent être fréquentes et valides et donc extraites. Une première solution consiste à rechercher des couvertures des ensembles de règles, ou si l\u0027on préfère, à éliminer des règles redondantes. Des travaux importants dans cette direction concernent l\u0027exploitation de représentations condensées des ensembles fréquents comme les ensembles fermés (Pasquier et al., 1999;Boulicaut et al., 2000) ou bien les ensembles ?-libres (Boulicaut et al., 2003). (Jeudy, 2002) est une étude assez complète de ces propositions.\n-569 -\nRNTI-E-6\nPar exemple, les règles dites ?-fortes, car construites à partir d\u0027ensembles fréquents ?-libres, ont des propriétés intéressantes : membre gauche minimal, fréquence minimale mais aussi niveau de confiance controlé par le nombre d\u0027exceptions toléré (le paramètre ?) pour la règle (voir Becquet et al., 2002, pour une application en biologie moléculaire). Cependant, l\u0027élimination des redondances indépendamment du domaine d\u0027application montre clairement ses limites. Pour éviter de présenter aux utilisateurs experts des milliers de règles fréquentes, valides et non redondantes, il faut alors travailler soit avec d\u0027autres mesures d\u0027intérêt objectives (i.e., au delà des seules mesures de fréquence et de confiance), soit assister la prise en compte de l\u0027intérêt subjectif de l\u0027analyste. La première direction de travail a donné lieu à de multiples propositions (voir par exemple Azé, 2003, pour une synthèse récente). Une seconde direction de travail consiste à assister le post-traitement des collections de règles extraites pour la prise en compte de la connaissance du domaine et ainsi éviter de présen-ter des informations triviales et/ou attendues. Notre hypothèse de travail est que les règles dites « intéressantes » sont celles qui non seulement satisfont certaines contraintes sur des mesures d\u0027intérêt objectives (e.g., fréquence minimale, confiance suffisante) mais aussi sortent du cadre des connaissances existantes pour l\u0027utilisateur. Ainsi, il est nécessaire de s\u0027intéresser à la modélisation et l\u0027exploitation des connaissances de l\u0027expert dans un contexte d\u0027extraction de règles d\u0027association. Les travaux de Padmanabhan et Tuzhilin (1998) ont montré l\u0027utilisation des connaissances de l\u0027utilisateur par la définition de règles. Cette approche a ensuite été formalisée par un réseau de croyances (Padmanabhan et Tuzhilin, 2000)   (Agrawal et al., 1996). La différence entre le support estimé sur les données et le support inféré à partir du réseau bayésien est calculée pour chaque ensemble d\u0027attributs. Les motifs les plus intéressants sont ceux pour lesquels la divergence entre les connaissances de l\u0027utilisateur (i.e., l\u0027évaluation au moyen du réseau) et ce qui est observé dans les données réelles est la plus forte. Ces ensembles d\u0027attributs sont ensuite soumis à l\u0027utilisateur pour une éventuelle mise à jour de la structure ou des paramètres du réseau bayésien. Dans cet article, nous proposons également une approche méthodologique pour exploiter des connaissances du domaine dans le cadre de la découverte de motifs locaux intéressants, typiquement des règles d\u0027association. Nous validerons cette approche sur un cas d\u0027application réel en aéronautique. La section 2 présente l\u0027approche envisagée. La section 3 introduit les notations et détaille la solution proposée. La section 4 décrit le cas d\u0027application utilisé et les expérimentations qui ont été menées. Enfin la dernière section est une brève conclusion.\nApproche envisagée\nNous décrivons l\u0027approche méthodologique envisagée pour faciliter le processus de décou-verte de connaissances à bases de motifs fréquents. On peut considérer quatre étapes importantes :\n-explicitation et modélisation des connaissances a priori de l\u0027expert, - \nExplicitation et modélisation des connaissances a priori\nCette phase de modélisation a pour principal objectif de représenter les connaissances dont l\u0027expert dispose par rapport au domaine d\u0027application. Il est possible, au départ, de modéliser uniquement les connaissances les plus évidentes. Puis, au fur et à mesure de l\u0027exploration des différents résultats d\u0027extractions, l\u0027expert peut souhaiter améliorer son modèle pour faciliter la découverte de motifs toujours plus intéressants en éliminant ceux qui apparaissent triviaux au regard des connaissances déjà connues. Pour ce faire, il faut disposer d\u0027un formalisme de représentation adapté. L\u0027approche par réseau bayésien pour la modélisation des connaissances apparaît comme particulièrement adaptée. En effet, les réseaux bayésiens permettent de considérer dans un formalisme commun les modèles de causalité 1 et les probabilités. Ils ont aussi été utilisés de manière intensive pour des applications de modélisation de la connaissance (Naïm et al., 2004, chapitre 8) et il existe de nombreux outils pour les construire et les exploiter. Dans notre cadre d\u0027application, il n\u0027est pas souhaitable de réaliser un apprentissage automatique de la structure et des paramètres du réseau bayésien. En effet, on s\u0027intéresse aux réseaux bayésiens pour leur capacité à représenter de manière compacte et intelligible la connaissance d\u0027un expert plutôt qu\u0027à une utilisation « directe » (prédiction, aide à la décision, etc.). Ainsi, la structure du réseau est définie, puis mise à jour par l\u0027utilisateur expert du domaine. L\u0027apprentissage automatique des paramètres du réseau est possible (Heckerman, 1997, notamment), mais cette solution n\u0027a pour l\u0027instant pas été implémentée.\nExtraction d\u0027une représentation condensée des ensembles fréquents\nCette phase concerne l\u0027utilisation d\u0027un algorithme d\u0027extractions de motifs, en l\u0027occurrence des règles d\u0027association. Les algorithmes de type Apriori permettent d\u0027extraire toutes les règles d\u0027association au dessus d\u0027un certain seuil de fréquence et de confiance (spécifiés par l\u0027utilisateur). Un premier reproche classique vis à vis des algorithmes de ce type est qu\u0027ils ne sont pas utilisables sur des volumes denses et/ou fortement corrélées, tout du moins pour des seuils de fréquences qui paraissent pertinents aux experts. Un second problème vient du fait que toutes les règles qui satisfont les contraintes de fréquence et de confiance sont extraites. La question de la redondance de ces collections, quel que soit le domaine d\u0027application a été très étudiée (voir les nombreuses propositions de couvertures de collections de règles). Par contre, il y a encore peu de travaux pour l\u0027élimination de motifs redondants au regard des connaissances déjà acquises par l\u0027expert. Pour résoudre le premier problème, on utilisera un algorithme (Boulicaut et al., 2000) capable d\u0027extraire une représentation condensées des ensembles fréquents, les ensembles dits ?-libres fréquents. Cet algorithme permet également, en calculant la ?-fermetures de tels ensembles, de produire une collection concise de règles d\u0027association à forte confiance appelées règles ?-fortes. En effet, le paramètre ? détermine le nombre d\u0027exceptions toléré pour les règles et sa valeur est supposée être petite au regard du seuil de fréquence utilisé. Pasquier (2000) a d\u0027ailleurs étudié les propriétés de ces collections lorsque ? \u003d 0. Pour résoudre le second problème, il faut intégrer la connaissance de l\u0027expert au calcul de l\u0027intérêt des règles d\u0027association extraites. A partir de la proposition décrite dans Jaroszewicz et Simovici (2004), nous définirons une mesure d\u0027intérêt des règles ?-fortes. Cependant, l\u0027objectif de notre démarche ne consiste pas seulement à définir une nouvelle mesure de similarité, mais plutôt à mettre en place un cadre méthodologique permettant l\u0027exploitation de la connaissance expert et s\u0027appuyant sur des algorithmes d\u0027extraction efficaces. Certains liens importants entre les réseaux bayésiens et les règles d\u0027association seront aussi explicités.\nUtilisation du modèle de connaissance pour faciliter la lecture des motifs extraits\nEn utilisant le formalisme des réseaux bayésiens, l\u0027expert explicite les connaissances qui vont lui être utiles pour le processus de fouille de données. Ce modèle de connaissance a pour but de faciliter la découverte de motifs pertinents, c\u0027est-à-dire ceux qui ne sont pas pris en compte par le modèle de connaissance, ou ceux qui le contredisent. Plus précisément, ce modèle permet à l\u0027utilisateur de définir -a priori-des dépendances entre des attributs qu\u0027il ne souhaite pas retrouver dans les résultats de l\u0027extraction. Pour cela, on utilise les capacités d\u0027inférence du réseau bayésien pour mesurer l\u0027intérêt des règles extraites, en comparant les dépendances déduites du réseau bayésien (construit à partir des connaissances du domaine) et les règles d\u0027association (extraites à partir des données réelles). Une divergence forte indique un motif potentiellement intéressant ; inversement, une convergence entre les données réelles et l\u0027estimation effectuée à partir du réseau bayésien indique des motifs déjà connus.\n-572 -RNTI-E-6\nAnalyse et interprétation des motifs extraits\nIl est peu probable que les premières itérations du modèle parviennent à éliminer correctement les motifs non intéressants. Par contre, nous pensons que des mises à jour successives du modèle de connaissance vont pouvoir s\u0027appuyer sur les extractions réalisées. Ainsi, à chaque itération du processus, deux possibilités se présentent : -Les résultats de l\u0027extraction font apparaître des motifs connus ; ce cas de figure nécessite de la part de l\u0027expert une reformulation des motifs découverts, de manière à pouvoir intégrer de nouvelles dépendances dans le modèle de connaissance. Le but est à la fois de pouvoir éviter par la suite la présentation de ce type de motifs, mais aussi de capitaliser une certaine connaissance du domaine sous la forme de dépendances quantitatives et qualitatives entres les variables du domaine. -Les résultats de l\u0027extraction présentent des motifs potentiellement intéressants (du point de vue de l\u0027expert) ; cela implique généralement un travail d\u0027analyse et de recherche dans l\u0027ensemble des données relatives au domaine d\u0027application. L\u0027expert peut ensuite déterminer si les motifs sont effectivement porteurs de nouvelles connaissances ou si ils révèlent une insuffisance (en terme de représentativité ou d\u0027exhaustivité des attributs pris en compte) des données disponibles. L\u0027expert peut, par exemple, décider d\u0027enrichir la base de données en intégrant de nouvelles variables (attributs), avant de procéder à une nouvelle itération du processus. Ainsi, l\u0027expert doit reformuler les connaissances induites par les motifs découverts afin de pouvoir les intégrer progressivement à son modèle. Nous allons ainsi pouvoir éliminer progressivement les règles triviales ou connues et faciliter l\u0027émergence de règles plus intéressantes. Notons que, dans ce contexte, le réseau bayésien représente un modèle partiel (et dégradé) des connaissances de l\u0027expert. Il n\u0027est défini et utilisé que pour faciliter la lecture et l\u0027interprétation de règles d\u0027association extraites (post-traitement des règles).\nRéseaux bayésiens et motifs fréquents\nDéfinitions et notations\nSoit BD une base de données booléenne, et H \u003d {A 1 , A 2 , . . . , A n } l\u0027ensemble de ses attributs booléens.\n(i) dénote la probabilité pour que l\u0027ensemble d\u0027attributs I ? H prenne comme valeur le vecteur i. Un itemset est représenté par la paire (I, i) avec I ? H ensemble d\u0027attributs fini non vide et i ensemble des valeurs des attributs de I. Lorsque cela n\u0027est pas strictement nécessaire, l\u0027itemset (I, i) sera désigné simplement par I. Un réseau bayésien RB est un graphe dirigé acyclique défini par un ensemble de noeuds correspondants aux attributs de H et par E ? H × H l\u0027ensemble des arcs du graphe. A chaque noeud on associe une distribution de probabilité conditionnelle P Ai|?A i , où ? Ai \u003d {A j |(V Aj , V Ai ) ? E} représente les parents du noeud A i . Pour une discussion détaillée sur les réseaux bayésiens consulter Pearl (1988). Une des propriétés du réseau bayésien est de définir de manière unique la distribution de probabilité jointe de H : \net par estimation sur les données :\nOn travaille sur une représentation condensée des itemsets fréquents au moyen d\u0027itemsets ?-libres et fréquents. En fait, l\u0027algorithme utilisé produit une collection de couples (I, ? ? f ermeture(I)\\I). Chaque élément I est un itemset ?-libre fréquent. Sa ?-fermeture est l\u0027ensemble de tous les attributs qui sont vrais pour un enregistrement lorsque ceux de I le sont à ? exceptions près. Il s\u0027agit donc d\u0027une généralisation de la notion classique de cloture au sens de la connection de Galois puisque, lorsque ? \u003d 0, I ? ? ? f ermeture(I) est un ensemble fermé fréquent. L\u0027entier positif ? permet donc de borner le nombre d\u0027exceptions 2 d\u0027une règle dite ?-forte, i.e., une règle R de la forme I ? ? ? f ermeture(I)\\I. Il faut comprendre que -par construction-les règles ainsi générées ont une partie gauche minimale et une partie droite maximale, ce qui implique une confiance maximale de 1 sur BD lorsque la règle ne comporte pas d\u0027exceptions (par exemple, lorsque l\u0027on exige ? \u003d 0. En s\u0027inspirant de Jaroszewicz et Simovici (2004), nous définissons maintenant une mesure de l\u0027intérêt d\u0027une règle d\u0027association. Cette mesure est basée sur la différence entre la confiance de la règle estimée à partir des données et celle inférée par le réseau bayésien. Elle s\u0027exprime de la manière suivante :\nExploitation du modèle de connaissance\nNous disposons d\u0027un algorithme qui calcule une collection de règles d\u0027association ?-fortes, d\u0027un formalisme pour modéliser les connaissances a priori de l\u0027expert, ainsi que d\u0027une mesure prenant en compte ces connaissances pour évaluer l\u0027intérêt des règles. Pour se faire une idée du comportement de la mesure d\u0027intérêt et des liens existants entre les règles extraites et les implications au niveau du modèle de connaissance, nous allons décrire de manière empirique deux cas de découverte de règles. Une règle peut représenter (1) un motif connu de l\u0027expert, mais qui n\u0027est pas encore pris en compte par le réseau bayésien ou (2) un motif connu et pris en compte par le réseau.\nSoit R \u003d A ? B le motif extrait (conf BD (R) est proche de 1 puisque nous ne calculons que des règles ?-fortes). Dans le cas (1), R n\u0027est pas prise en compte par le réseau bayésien. On se place alors sous l\u0027hypothèse d\u0027indépendance entre A et B pour calculer l\u0027intérêt de R selon le réseau bayésien. Ainsi, à partir de l\u0027équation 4, on obtient : Int(R) \u003d |conf BD (R) ? P B |. Ici, l\u0027intérêt dépend donc principalement de la distribution de probabilité P B définie dans le réseau bayésien. Par exemple, si l\u0027expert a défini P B \u003d 0, 95, la règle R aura un intérêt presque nul car B est un événement très fréquent. Dans ce cas, l\u0027association n\u0027apporte pas de connaissance supplémentaire. Inversement, si P B est faible, alors l\u0027intérêt de la règle sera élevé, signifiant alors à l\u0027utilisateur la possible existance d\u0027une dépendance entre A et B. On se place maintenant dans le cas (2) où la règle R est prise en compte par le réseau bayésien. Cela signifie que l\u0027on a défini explicitement un lien de dépendance A ? B ainsi que la probabilité P B|A . On a alors Int(R) \u003d |conf BD (R) ? P B|A |, soit un intérêt dépendant de P B|A ; ce qui correspond bien au comportement souhaité. En effet, dans le cas où P B|A est proche de 1, l\u0027association n\u0027est pas jugée intéressante car elle est correspond au modèle défini par l\u0027expert. Inversement, si l\u0027on a défini P B|A faible alors que conf BD (R) est proche de 1, l\u0027intérêt de la règle sera plus important, mettant ainsi en évidence une contradiction entre les données réelles et la modélisation de l\u0027expert.\nApplication à la fouille de données d\u0027interruptions opéra-tionnelles\nDans le domaine aéronautique, une interruption opérationnelle est un retard au départ (dé-collage) de plus de quinze minutes, une annulation ou une interruption de vol suite à un problème technique (panne ou dysfonctionnement). Un tel événement est aujourd\u0027hui considéré comme important par les compagnies aériennes pour le coût et les mécontentements engendrés. De ce fait, lors du lancement de nouveaux projets avions, les ingénieurs doivent fournir dès la phase de conception une prédiction la plus réaliste possible de la fréquence des interruptions opérationnelles, qui sera mesurée lors de la future exploitation commerciale des avions. Ces prédictions initient, guident et valident les choix de conception. Pour effectuer cette activité, les ingénieurs utilisent un outil informatique implémentant un modèle mathématique stochastique intégrant les paramètres dont les impacts sur la fréquence des interruptions opérationnelles sont connus. Cet outil est calibré et paramétré par le retour d\u0027expérience obtenu à partir d\u0027avions, de systèmes ou d\u0027équipements en service comparables. Les besoins de recherche portent sur l\u0027amélioration des modèles de calcul utilisés par cet outil de prédiction. Ainsi, la fouille des données en service est intéressante car elle permet de découvrir de nouveaux facteurs qui pourraient être intégrés à ces modèles pour améliorer la prédiction de la fréquence des interruptions opérationnelles. On se propose d\u0027encadrer ce processus de découverte par l\u0027approche méthodologique présentée dans la section 2. L\u0027analyse des données doit permettre de valider les hypothèses qui ont été prises et d\u0027enrichir le modèle de prédiction. Plus précisément, il s\u0027agit d\u0027aider l\u0027expert à détecter, ou à vérifier, la présence de contributeurs de la fiabilité opérationnelle par la fouille des données en service.\nExpérimentations\nLa base de données relative aux interruptions opérationnelles regroupe les détails de tous les problèmes techniques. Pour notre étude nous avons pris, en accord avec l\u0027expert, un sousensemble de la base de données initiale. Après pré-traitement, on dispose de 23 attributs discrétisés et de plus de 12000 enregistrements décrivant les interruptions opérationnelles. On se propose, dans un premier temps, de regarder les résultats issus de l\u0027extraction des règles d\u0027association ?-fortes. Ces règles sont présentées à l\u0027expert en fonction de différentes mesures d\u0027intérêt : confiance (Agrawal et al., 1993), J-mesure (Smyth et Goodman, 1992) et moindre contradiction (Azé, 2003). Cette première extraction comporte de nombreuses règles connues de l\u0027expert, ce qui permet de se rendre compte de la nécessité d\u0027expliciter certaines connaissances exactes (taxonomies) mais aussi des croyances fortes de l\u0027expert sur son domaine. On peut donc identifier plusieurs catégories de connaissances que l\u0027on peut expliciter dans le réseau bayésien :\n-Taxonomie, la présence d\u0027une telle structure dans les données est intéressante car elle permet de capturer différents niveaux de détails dans les règles d\u0027association. Cependant, l\u0027existence de taxonomies introduit des dépendances exactes qui vont être capturées par un grand nombre de règles, occultant ainsi la lecture de règles potentiellement intéressantes. -Valeur d\u0027attribut prépondérante, lorsqu\u0027un attribut de la base de données a une valeur très dominante (e.g. 95% des problèmes ont eu pour conséquence un retard) alors il est important de pouvoir intégrer cette information au modèle de connaissance afin d\u0027éviter la production de règles triviales. -Croyance forte, elle peut être représentée comme une dépendance entre un ou plusieurs attributs du réseau bayésien et par la définition des tables de probabilités jointes correspondantes. L\u0027outil le plus connu et le plus facile à mettre en oeuvre pour décrire les probabilités est l\u0027échelle de probabilité (Druzdzel et van der Gaag, 2000). Après intégration de ces connaissances dans un réseau bayésien, nous pouvons procéder au calcul de l\u0027intérêt des règles d\u0027association. Les résultats mettent alors en avant d\u0027autres connaissances, plus pertinentes, pour le domaine d\u0027application.\nRésultats obtenus\nConsidérons d\u0027abord l\u0027extraction sans exploitation d\u0027un réseau bayésien. L\u0027extraction a donné 17760 règles ?-fortes. Le tableau 1 montre des exemples -choisis-de telles règles au moyen de l\u0027algorithme décrit dans (Boulicaut et al., 2000) (support min \u003d 100, ? \u003d 15). Sur une configuration PC de bureau, l\u0027extraction a demandé 2 minutes et 55 secondes.\nAfin d\u0027éclaircir la lecture de ces résultats, il peut être utile de faire quelques précisions. Les mots-clés remove, ecam, mel, etc. indiquent que l\u0027analyse du texte libre rédigé par un technicien a permis de déceler une action particulière : « pose/dépose » d\u0027un équipement, apparition de messages d\u0027alertes, application d\u0027une procédure spécifique. Lorsqu\u0027un mot-clé est préfixé de last\u003d cela signifie qu\u0027il s\u0027agit du dernier mot-clé, correspondant à une action, détecté dans la description du problème. Les nombres de 2, 4 ou 6 chiffres désignent les équipements incriminés dans l\u0027interruption opérationnelle. Ces nombres obéissent à une taxonomie bien précise : la norme ATA 100. Ainsi l\u0027équipement 286322 est un sous-équipement de 2863, -576 -RNTI-E-6 Une analyse des résultats du tableau 1 permet de mettre en avant la présence d\u0027associations correspondant : -à la taxonomie des équipements (règles 2, 4, 5, 7, 9 et 10), -à des relations triviales entre l\u0027identification du dernier mot-clé et la présence de ce mot clé dans le texte (règles 1, 3, 8), -à la prépondérance dans les données de certaines valeurs d\u0027attributs comme DY ou CS.\n-ou encore à des connaissances plus spécifiques du domaine, telles que les liens entre les compagnies et leur aéroport principal (règle 8), ou encore entre un mois de l\u0027année et l\u0027apparition d\u0027incidents sur un équipement spécifique (e.g., système de conditionnement d\u0027air pendant les périodes d\u0027été, règle 10), etc. Ainsi, beaucoup de règles contiennent des connaissances bien connues de l\u0027expert. Nous montrons maintenant que l\u0027approche proposée permet d\u0027améliorer la découverte d\u0027informations pertinentes grâce à l\u0027exploitation d\u0027un modèle des connaissances du domaine, de type réseau bayésien.\nTaxonomie et associations évidentes L\u0027intégration de ce type de connaissance au réseau bayésien est triviale. L\u0027expert va ajouter un lien de causalité entre les attributs qui correspondent à cette information. Par exemple, on peut définir un lien entre ATA4d et ATA2d, puis un lien entre ATA2d et Category, etc. Les tables de probabilités sont ensuite définies de manière à exclure toute autre relation entre ces attributs, e.g., P (ATA2d \u003d43|ATA4d \u003d 4345) \u003d 1.0 ; et ainsi de suite pour tous les attributs obéissant à ce type de relation.\nItemsets prépondérants Il est important de définir la probabilité d\u0027apparition des événe-ments fréquents. En effet, la définition des distributions de probabilités des événements fré-quents permet de limiter le facteur d\u0027intérêt lié à la présence de ces attributs dans la partie \nTAB. 2 -Utilisation du réseau bayésien pour mesurer l\u0027intérêt des règles d\u0027association\ndroite d\u0027une règle. Par exemple, on pourra définir P (Effect \u003d DY) \u003d 0, 97 ou encore P (Remove \u003d true) \u003d 0, 95. Ainsi la présence de l\u0027attribut DY dans la partie droite d\u0027une règle d\u0027association n\u0027influera pas sur le calcul de l\u0027intérêt de cette règle.\nConnaissances plus spécifiques L\u0027expert peut vouloir définir des connaissances fortes du domaine, par exemple, le lien entre une compagnie et sa base principale, ou encore entre un mois de l\u0027année et l\u0027apparition d\u0027un problème sur un équipement particulier. L\u0027expert doit reformuler sa connaissance du domaine pour l\u0027adapter à la modélisation du réseau bayésien : il doit créer des liens de causalité entre un ou plusieurs attributs du réseau puis définir les distributions de probabilités correspondantes. Pour cela, nous pouvons employer la méthode de l\u0027échelle des probabilités pour expliciter, par exemple, qu\u0027il est probable que la compagnie OP1 soit associée à l\u0027aéroport ST1, etc. La figure 2 reprend les différentes informations que l\u0027expert a pu extraire à partir des premiers résultats obtenus et montre une modélisation partielle de ces connaissances, après analyse et reformulation des motifs déjà extraits. Les tables de probabilités jointes ont elles aussi été dé-finies par l\u0027expert mais elles ne sont pas présentées pour des raisons de clarté. Ce réseau est ensuite utilisé pour calculer l\u0027intérêt des règles d\u0027association présentées dans le tableau 1. Le calcul montre (tableau 2) que les différentes règles ont un intérêt faible. Ce résultat est cohérent puisque le réseau bayésien a été défini de manière à pouvoir éliminer la plupart de ces motifs. Néanmoins les règles (6) et (7) ont un intérêt supérieur aux autres et nécessitent une analyse plus poussée. La règle (6) montre un lien entre un aéroport et l\u0027absence d\u0027action de maintenance (last \u003d none) associés à une compagnie aérienne, ce qui pousse à s\u0027intéresser sur le fonctionnement de cette compagnie. En effet, certaines compagnies préfèrent effectuer les opérations de maintenance « lourdes » dans leur base principale. La règle (7) met en avant une relation entre une tranche de retard assez importante (0.5_1.5) et un équipement particulier (434512). Ce motif paraît particulièrement intéressant, mais une analyse plus poussée reste nécessaire pour valider ou non la réalité de cette association.\nConclusion\nA partir des travaux de Boulicaut et al. (2000) et de Jaroszewicz et Simovici (2004), on a mis en place une méthodologie permettant de faciliter la découverte de règles d\u0027association potentiellement intéressantes. Notre approche met en avant la collaboration entre réseaux bayé-siens et collections de règles d\u0027association (et donc ensemble fréquents). Cette méthodologie a été testée sur un cas d\u0027application concret concernant la fouille des données d\u0027interruptions opérationnelles dans l\u0027aéronautique et elle montre des résultats encourageants.\n"
  },
  {
    "id": "1093",
    "text": "Introduction\nLa fouille de données a pour objectif d\u0027identifier des relations cachées entre les motifs de grandes bases de données. La recherche de règles d\u0027association est une des tâches les plus importantes de la fouille de données. L\u0027extraction de règles d\u0027association est un domaine de l\u0027extraction de connaissances dans les bases de données (ECBD), qui se définit comme un procédé pour trouver des motifs valides, utiles et compréhensibles dans les données (Fayyad et al., 1996). Une règle d\u0027association est une proposition de la forme \"80% des étudiants qui suivent le cours Introduction à Unix suivent également Programmation en C\" (Han et Kamber, 2001).\nJusqu\u0027à présent, la littérature s\u0027est intéressée à la recherche des règles d\u0027association valides fréquentes (c\u0027est-à-dire les règles d\u0027association avec un support et une confiance suffisamment élevés). Cela requiert d\u0027abord l\u0027extraction des motifs fréquents de l\u0027ensemble des données. Le problème de l\u0027extraction des motifs fréquents était au départ un sous-problème de la fouille de règles d\u0027association (Agrawal et al., 1996), mais il s\u0027est révélé plus tard utile dans différents domaines, tels que la fouille de motifs séquentiels (Agrawal et Srikant, 1995), de règles d\u0027association spatiales (Koperski et Han, 1995), de règles d\u0027association cycliques (Özden et al., 1998), de règles d\u0027association négatives (Savasere et al., 1998), la recherche de motifs fermés fréquents (voir Section 1.1), de motifs fréquents maximaux (voir Section 1.1), etc.\nNous faisons l\u0027hypothèse que certains phénomènes rares dans les bases de données peuvent également véhiculer une connaissance. C\u0027est donc plus particulièrement l\u0027extraction de motifs rares que nous étudions dans cet article.\nTravaux en relation\nL\u0027extraction de motifs rares et la génération de règles d\u0027association rares n\u0027ont pas encore été étudiées en détail dans la littérature. Dans cet article, nous partons d\u0027une vue d\u0027ensemble de la recherche de motifs fréquents pour introduire notre méthode d\u0027extraction des motifs rares.\nPlusieurs approches ont été proposées pour trouver les motifs fréquents dans les bases de données. La première est basée sur l\u0027algorithme Apriori, qui fut le premier algorithme par niveau à réaliser cette tâche (Agrawal et al., 1996). Cette méthode identifie les i-motifs à chaque i eme itération puis génère les (i+1)-motifs fréquents à partir des i-motifs 1 . A chaque itération il requiert un passage sur la base de données pour compter le support des motifs candidats et ensuite élague les candidats infréquents. Cet algorithme est très simple et efficace pour des données peu corrélées. Apriori a été suivi par de nombreuses variations dans le but d\u0027en amé-liorer l\u0027efficacité (Brin et al., 1997;Toivonen, 1996). La deuxième approche s\u0027intéresse à la recherche de motifs fermés fréquents dans la base de données (Pasquier et al., 1999). Les motifs fermés fréquents permettent une représentation condensée et sans perte d\u0027information des motifs fréquents, puisque l\u0027ensemble des motifs fréquents (et leur support) peut être retrouvé à partir des motifs fermés fréquents. Cette idée fut implémentée dans Close (Pasquier et al., 1999), qui est aussi un algorithme par niveau. Depuis Close d\u0027autres algorithmes ont été proposés pour la recherche de motifs fermés fré-quents Zaki et Hsiao, 2002;Wang et al., 2003).\nUn autre sous-ensemble intéressant de motifs fréquents est l\u0027ensemble des générateurs minimaux. Bastide et al. ont montré comment utiliser les générateurs minimaux pour trouver les règles d\u0027association informatives 2 (Bastide et al., , 2000b. Parmi les règles partageant les mêmes individus comme support et ayant la même confiance, les règles construites à partir d\u0027un motif fermé et ayant un motif générateur en partie gauche sont celles qui contiennent le plus d\u0027information (Pasquier, 2000).\nLe premier algorithme pour trouver les générateurs minimaux fut Pascal (Bastide et al., 2000a). Pascal peut réduire le nombre de passages sur la base de données et compter le support des candidats plus efficacement. Pascal trouve tous les motifs fréquents et tous les générateurs minimaux, mais ce n\u0027est pas suffisant pour trouver les règles informatives. Pour la génération des règles d\u0027association informatives, il faut identifier parmi les motifs fréquents, les motifs fermés et les associer aux générateurs minimaux. Pour résoudre cette insuffisance, un autre algorithme appelé Zart a été proposé récemment . Zart est un algorithme multifonctionnel d\u0027extraction de motifs qui étend Pascal de manière à ce qu\u0027il soit conforme à la génération de règles d\u0027association informatives. Zart trouve les motifs fréquents, les motifs fermés fréquents et les générateurs minimaux. De plus, les générateurs minimaux sont associés à leur fermeture. En conséquence, la génération des règles informatives peut être réalisée très rapidement et aisément avec Zart.\nUne quatrième approche est basée sur l\u0027extraction des motifs fréquents maximaux. Un motif fréquent maximal a les propriétés suivantes : tous ses sur-motifs sont infréquents et tous ses sous-motifs sont fréquents. Des expériences ont montré que cette approche est très efficace pour trouver de grands motifs dans les bases de données (Bayardo, 1998;Agarwal et al., 2000;Lin et Kedem, 1998;Gouda et Zaki, 2001). Les algorithmes basés sur cette approche identifient, comme Apriori, l\u0027ensemble des règles d\u0027association.\nContributions et motivations\nNous présentons une nouvelle méthode pour trouver les motifs rares dans une base de données en deux étapes. La première étape identifie un ensemble générateur minimal appelé ensemble des motifs rares minimaux. Dans la seconde étape, ces motifs sont utilisés pour retrouver tous les motifs rares.\nLa découverte des motifs rares peut se révéler très intéressante, en particulier en méde-cine et en biologie. Prenons d\u0027abord un exemple simulé d\u0027une base de données médicale où nous nous intéressons à l\u0027identification de la cause des maladies cardio-vasculaires (MCV). Une règle d\u0027association fréquente telle que \"{niveau élevé de cholestérol} ? {MCV}\" peut valider l\u0027hypothèse que les individus qui ont un fort taux de cholestérol ont un risque élevé de MCV. A l\u0027opposé, si notre base de données contient un grand nombre de végétariens, une règle d\u0027association rare \"{végétarien} ? {MCV}\" peut valider l\u0027hypothèse que les végétariens ont un risque faible de contracter une MCV. Dans ce cas, les motifs{végétarien} et {MCV} sont tous deux fréquents, mais le motif {végétarien, MCV} est rare. Un autre exemple est en rapport avec la pharmacovigilance, qui est une partie de la pharmacologie dédiée à la détection et l\u0027étude des effets indésirables des médicaments. L\u0027utilisation de l\u0027extraction des motifs rares dans une base de données des effets indésirables des médicaments pourrait contribuer à un suivi plus efficace des effets indésirables graves et ensuite à prévenir les accidents fatals qui aboutissent au retrait de certains médicaments (par exemple en août 2001, la cérivastatine, mé-dicament hypolipémiant). Finalement, un troisième exemple basé sur les données réelles de la cohorte STANISLAS (Siest et al., 1998;Maumus et al., 2005) montre l\u0027intérêt de l\u0027extraction des motifs rares pour la fouille de données dans des cohortes supposées saines. Cette cohorte est composée d\u0027un millier de familles françaises présumées saines. Son principal objectif est de mettre en évidence l\u0027influence des facteurs génétiques et environnementaux sur la variabilité des risques cardio-vasculaires. Une information intéressante à extraire de cette base de données pour l\u0027expert dans ce domaine consiste en des profils qui associent des données génétiques à des valeurs extrêmes ou limites de paramètres biologiques. Cependant, ces types d\u0027associations sont plutôt rares dans les cohortes saines. Dans ce contexte, l\u0027extraction de motifs rares pourrait être très utile pour atteindre les objectifs de l\u0027expert.\nOrganisation de l\u0027article\nDans la section suivante, nous donnons une vue d\u0027ensemble des concepts de base. La Section 3 détaille notre approche pour l\u0027énumération des motifs rares basée sur les treillis, et contient également les définitions essentielles. Nous décrivons ensuite dans la Section 4 les deux étapes de notre méthode et nous en fournissons les algorithmes, ainsi que des exemples les appliquant. Enfin, les conclusions sont présentées dans la dernière section.\nConcepts de base\nCi-dessous nous utilisons les définitions usuelles de la fouille de données. Nous considérons un ensemble d\u0027objets O \u003d {o 1 , o 2 , . . . , o m }, un ensemble d\u0027attributs A \u003d {a 1 , a 2 , . . . , a n } et une relation R ? O × A, où R(o, a) signifie que l\u0027objet o possède l\u0027attribut a. En analyse de concepts formels (Ganter et Wille, 1999), le triplet (O, A, R) est appelé contexte formel. Un ensemble d\u0027attributs est appelé motif. Un motif de taille i est appelé i-motif. Nous disons qu\u0027un objet o ? O contient le motif P ? A, si (o, p) ? R pour tout p ? P . Le support d\u0027un motif P indique combien d\u0027objets contiennent le motif. Un motif est dit fréquent si son support est supérieur ou égal à un support minimum donné (noté min_supp par la suite). Un motif est dit rare ou infréquent si son support est inférieur ou égal à un support maximum (noté max_supp par la suite). P 2 est un sur-motif de P 1 ssi P 1 ? P 2 . Dans cet article, nous nous sommes placés dans le cas particulier où max_supp \u003d min_supp ? 1, c\u0027est-à-dire qu\u0027un motif est rare s\u0027il n\u0027est pas fréquent. Cela implique l\u0027existence d\u0027une seule frontière entre motifs rares et fréquents. Boulicaut et al. (2003) fait par ailleurs lui aussi mention de cette frontière. Un motif X est dit fermé s\u0027il n\u0027existe pas de sur-motif Y (X ? Y ) de même support. L\u0027extraction de motifs fréquents consiste à générer tous les motifs (fermés) fréquents (avec leur support) dont le support est supérieur ou égal à min_supp. L\u0027extraction de motifs rares consiste à générer tous les motifs (avec leur support) dont le support est inférieur ou égal à max_supp.\nUne approche basée sur les treillis pour l\u0027énumération des motifs rares\nAvant d\u0027exposer nos algorithmes pour trouver les motifs rares, nous présenterons notre méthode du point de vue des treillis (voir Ganter et Wille (1999) pour une description détaillée des treillis).\nLa Figure 1 montre le treillis de l\u0027ensemble des parties P (D) de l\u0027ensemble des attributs dans notre base de données exemple D 3 (voir Tableau 1). L\u0027ensemble des motifs rares forme un semi-treillis \"join\" car il est fermé pour l\u0027opération \"join\", c\u0027est-à-dire que pour tous motifs rares X et Y , X ? Y est aussi rare. D\u0027un autre côté, il ne forme pas un semi-treillis \"meet\", car la rareté de X et Y n\u0027implique pas celle de X ? Y . Notons que les motifs fréquents forment un semi-treillis \"meet\", c\u0027est-à-dire que pour tous motifs fréquents X et Y , X ? Y est aussi fréquent.\nPrenons l\u0027exemple de la base de données D (Tableau 1) et fixons min_supp \u003d 3, ce qui signifie que max_supp \u003d 2. Les motifs peuvent être séparés en deux ensembles formant une partition : les motifs rares et les motifs fréquents. Une frontière peut être dessinée entre ces deux ensembles. En bas du treillis nous trouvons le plus petit motif, l\u0027ensemble vide. A chaque niveau se situent les motifs de même taille. Au sommet du treillis on trouve le motif le plus long qui contient tous les attributs. Le support de chaque motif est indiqué dans le coin en haut à droite (voir Figure 1).\nAvant d\u0027énoncer les définitions essentielles, nous empruntons à Apriori (Agrawal et al., 1996) ses deux principes fondamentaux que nous rappelons ici :\nPropriété 1 (propriété de fermeture vers le bas). Tous les sous-ensembles d\u0027un motif fré-quent sont fréquents.\nPropriété 2 (propriété d\u0027anti-monotonocité). Tous les sur-motifs d\u0027un motif infréquent sont infréquents.\nL\u0027ensemble des motifs rares et l\u0027ensemble des motifs fréquents ont tous deux un sousensemble minimal générateur. Dans le cas des motifs fréquents, ce sous-ensemble est appelé ensemble des motifs fréquents maximaux (MFM). Définition 1. Un motif est un MFM s\u0027il est fréquent (et ainsi tous ses sous-motifs sont fréquents) et si tous ses sur-motifs ne sont pas fréquents.\nCes motifs sont dits maximaux, parce qu\u0027ils n\u0027ont pas de sur-motifs fréquents. Du point de vue du nombre de ces motifs ils sont minimaux, c\u0027est-à-dire qu\u0027ils forment un ensemble générateur minimal à partir duquel tous les motifs fréquents peuvent être retrouvés 4 . Nous pouvons définir les motifs rares minimaux (MRM) en tant que complémentaires des MFMs, de la manière suivante : Définition 2. Un motif est un MRM s\u0027il est rare (et ainsi tous ses sur-motifs sont rares) et si tous ses sous-motifs ne sont pas rares.\nCes motifs forment un ensemble générateur minimal à partir duquel tous les motifs rares peuvent être retrouvés. Tous les motifs fréquents peuvent être retrouvés à partir des MFM. Dans un premier temps, nous devons prendre tous les sous-ensembles possibles des MFM. Dans un deuxième temps, le support des motifs fréquents peut être calculé grâce à un passage sur la base de données. Un processus similaire est mis en oeuvre pour retrouver les motifs rares. Nous devons d\u0027abord générer tous les sur-motifs possibles des motifs rares minimaux, puis calculer le support des motifs rares grâce à un passage sur la base de données.\nParmi les motifs rares, nous distinguons deux sous-ensembles : a) les motifs rares de support 0, et b) les motifs rares de support supérieur à 0. Cette distinction est importante, car le nombre total de motifs rares peut être élevé, et ainsi nous avons privilégié les motifs dont le support est non nul.\nDéfinition 3. Un motif est appelé motif à support nul si son support est égal à 0. Autrement, il est appelé motif à support non nul.\nPour tous les motifs rares nous avons déjà décrit l\u0027ensemble des motifs rares minimaux. Pour les motifs à support nul, un sous-ensemble générateur minimal semblable peut être défini : Définition 4. Un motif est un générateur minimal à support nul (GMSN) si c\u0027est un motif à support nul (ainsi tous ses sur-ensembles sont des motifs à support nul) et si tous ses sousmotifs sont des motifs à support non nul.\nSur la Figure 1 se trouvent deux GMSN : {BD} et {DE}. De plus, les GMSN forment une représentation condensée et sans perte d\u0027information des motifs à support nul, c\u0027est-à-dire qu\u0027à partir des GMSN tous les motifs à support nul peuvent être retrouvés avec leur support (qui est toujours 0). Pour cela, nous avons seulement besoin de générer tous les sur-motifs possibles des GMSN en utilisant les attributs de la base de données.\nTrouver les motifs rares\nDans cette section nous présentons les deux étapes de notre méthode pour trouver les motifs rares. La première étape trouve seulement les motifs rares minimaux, tandis que la seconde retrouve les motifs rares non nuls à partir de l\u0027ensemble des motifs rares minimaux.\nNous ne générons pas les motifs à support nul à cause de leur grand nombre. Pour éviter les motifs à support nul, nous utiliserons les GMSN. La seconde étape de notre méthode (voir Section 4.2) permet de restaurer tous les motifs rares non nuls à partir des MRM à l\u0027aide d\u0027une approche par niveau. Si un candidat a un sous-motif GMSN, alors ce candidat est de manière sûre un motif à support nul et peut être ainsi élagué. Autrement dit, à l\u0027aide des GMSN nous pouvons réduire l\u0027espace de recherche pendant que nous retrouvons tous les motifs rares.\nTrouver les motifs rares minimaux\nDe manière surprenante, les motifs rares minimaux peuvent être trouvés simplement à l\u0027aide de l\u0027algorithme bien connu Apriori. Apriori est basé sur deux principes (voir Propriétés 1 et 2). Il est conçu pour trouver les motifs fréquents, mais, puisque nous sommes dans le cas où non fréquent signifie rare, cela a pour \"effet collatéral\" d\u0027explorer également les motifs rares minimaux. Quand Apriori trouve un motif rare, il ne générera plus tard aucun de ses surmotifs car ils sont de manière sûre rares. Puisque Apriori explore le treillis des motifs niveau par niveau du bas vers le haut, il comptera le support des motifs rares minimaux. Ces motifs seront élagués et plus tard l\u0027algorithme peut remarquer qu\u0027un candidat a un sous-motif rare. En fait Apriori vérifie si tous les (k ? 1)-sous-motifs d\u0027un k-candidat sont fréquents. Si l\u0027un d\u0027entre eux n\u0027est pas fréquent, alors le candidat est rare. Autrement dit, cela signifie que le candidat a un sous-motif rare minimal. Grâce à cette technique d\u0027élagage, Apriori peut réduire significativement l\u0027espace de recherche dans le treillis des motifs.\nUne légère modification d\u0027Apriori suffit pour conserver les MRM. Si le support d\u0027un candidat est inférieur au support minimum, alors à la place de l\u0027effacer nous l\u0027enregistrons dans l\u0027ensemble des motifs rares minimaux (voir Algorithme 1).\nAlgorithme 1 (Apriori-Rare) :\nDescription : modification d\u0027Apriori pour trouver les motifs rares minimaux Entrée : base de données + min_supp Sortie : tous les motifs fréquents + motifs rares minimaux\nSupportCount(C i ) ; // compte le support des motifs candidats 6) R i ? {r ? C i | support(r) \u003c min_supp} ; // R -pour les motifs rares 7)\n; // C -pour les candidats 9)\ni ? i + 1 ; 10) } 11) I MR ? R i ; // motifs rares minimaux 12) I F ? F i ; // motifs fréquents Fonction Apriori-Gen : à l\u0027aide des k-motifs fréquents, génère les potentiellement fréquent candidats de taille (k + 1). Potentiellement fréquent signifie ne pas avoir de sous-motif rare, c\u0027est-à-dire pas de sous-motif rare minimal. Inclure un motif rare implique être rare (voir Propriété 2). Pour une description détaillée de cette fonction consulter Agrawal et al. (1996).\nL\u0027exécution de l\u0027algorithme sur la base de données D (Tableau 1) avec un support minimum de 3 (équivalent à un support maximum de 2) est illustrée dans le Tableau 2.\nEn prenant l\u0027union des R i , l\u0027algorithme trouve les motifs rares minimaux ({D} avec support 1, {AB} et {AE} avec support 2).\nDans la prochaine sous-section, nous montrons comment restaurer les sur-motifs des MRM (c\u0027est-à-dire comment reconstruire tous les motifs rares) en évitant les motifs à support nul.\nRetrouver les motifs rares\nTous les motifs rares sont retrouvés à partir des motifs rares minimaux. Pour cela nous avons besoin de générer tous les sur-motifs possibles des MRM. Les générateurs minimaux à support nul sont utilisés pour filtrer les motifs à support nul pendant la génération des surmotifs. De cette manière l\u0027espace de recherche peut être réduit de manière considérable. Dans cette section nous présentons un algorithme prototype pour cette tâche appelé Arima 5 (A Rare Itemset Miner Algorithm, voir Algorithme 2). L\u0027exécution de l\u0027algorithme sur la base de données D (Tableau 1) avec un support minimum de 3 (équivalent à un support maximum de 2) est illustrée dans le Tableau 3.\nL\u0027algorithme prend d\u0027abord le plus court MRM, {D}, qui est rare et ainsi copié dans R 1 . Ses sur-motifs de taille 2 sont générés et stockés dans C 2 ({AD}, {BD}, {CD}, et {DE}). Avec un passage sur la base de données leur support peut être compté. Puisque {BD} et {DE} sont des motifs à support nul, ils sont copiés dans la liste des GMSN. A partir des MRM, les \n2-motifs sont ajoutés à C 2 et les motifs non nuls sont stockés dans R 2 . Pour chaque motif rare dans R 2 tous ses sur-motifs sont générés. Par exemple, à partir de {AD} nous pouvons générer les candidats suivants : {ABD}, {ACD} et {ADE}. Si un candidat possède un sousmotif GMSN, alors le candidat est de manière sûre un motif à support nul et peut être élagué ({ABD}, {ADE}). Les candidats potentiels non nuls sont stockés dans C 3 . Dans les C i les doublons ne sont pas permis. L\u0027algorithme s\u0027arrête quand R i est vide. L\u0027union des R i donne tous les motifs rares à support non nul. A la fin nous avons aussi collecté tous les GMSN. Ainsi si on a besoin des motifs à support nul, cette liste peut être utilisée pour les retrouver. Le procédé est similaire : nous aurions besoin de générer tous les sur-motifs possibles des GMSN. Dans notre cas nous ne nous sommes intéressés qu\u0027aux motifs non nuls, mais il est possible de travailler avec les motifs à support nul.\nConclusions et travaux futurs\nDans cet article, nous avons présenté une méthode pour extraire les motifs rares d\u0027une base de données. Notre méthode est composée de deux étapes : 1) nous trouvons un sous-ensemble générateur minimal des motifs rares appelés MRM (algorithme Apriori-Rare) ; 2) à l\u0027aide des MRM nous retrouvons les motifs rares dont le support est strictement supérieur à 0 (algorithme Arima).\nNotre méthode fait partie des premières à s\u0027intéresser spécifiquement aux motifs rares. Apriori fut le premier algorithme pour trouver les motifs fréquents et a été suivi par de nombreux algorithmes plus efficaces. De manière similaire, il ne fait aucun doute que nos algorithmes prototypes pourraient être améliorés de nombreuses manières. Dans le futur nous aimerions travailler sur ce sujet.\nParmi les motifs fréquents un certain nombre de sous-ensembles utiles ont été découverts, parmi lesquels les motifs fermés fréquents, les motifs fréquents maximaux, les générateurs (clés) minimaux, etc. Nous sommes curieux de découvrir si de tels sous-ensembles peuvent être définis pour les motifs rares. Nous connaissons déjà le complémentaire des motifs fréquents maximaux, qui est l\u0027ensemble des motifs rares minimaux. Une autre question intéressante est la suivante. Les motifs fermés fréquents déterminent sans ambiguïté tous les motifs fréquents et leur support. Existe-t-il un sous-ensemble similaire qui déterminerait les autres motifs rares avec leur support ?\nUn domaine important de l\u0027utilisation des motifs rares est la génération des règles d\u0027association rares. Par manque de place, nous n\u0027avons pas pu développer ce sujet ici mais nous prévoyons d\u0027étudier cette question en détail dans un autre article.\nD\u0027autre part, dans un futur proche, nous prévoyons de décrire des expériences réalisées sur des données réelles de la cohorte STANISLAS, dans le but de fournir un exemple concret de ce nouvel aspect prometteur de la découverte de connaissances dans les bases de données.\nRéférences\nAgarwal, R. C., C. C. Aggarwal, et V. V. V. Prasad (2000). Depth first generation of long patterns. In KDD \u002700 : Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 108-118. ACM Press.\n"
  },
  {
    "id": "1094",
    "text": "Introduction\nL\u0027explosion des quantités de données stockées sur différents supports informatique conjointement à l\u0027avènement des Technologies de l\u0027Information et de la Communication a introduit des bouleversements importants dans le management des entreprises. En plus des connaissances explicites (courriers électroniques, procédures, notes de services, ...), il faut capitaliser l\u0027ensemble des connaissances tacites, c\u0027est à dire les connaissances qui ne sont pas formalisables aisément avec des mots (bonnes pratiques, savoir-faire, ...) (Alavi et Leidner, 2001;Earl, 2001). L\u0027objectif est de rendre cette connaissance accessible aux utilisateurs concernés, de la conserver et de l\u0027analyser pour la faire évoluer et par ce biais faire ainsi évoluer l\u0027entreprise. La capitalisation, l\u0027exploitation et l\u0027enrichissement des connaissances se fait de plus en plus souvent par l\u0027intermédiaire d\u0027un système de gestion des connaissances (SGC) informatisé que l\u0027on peut qualifier de mémoire d\u0027entreprise (Dieng- Kuntz et al., 2001). Le processus de créa-tion d\u0027une mémoire d\u0027entreprise est un passage d\u0027une mémoire de travail à une mémoire organisationnelle qui se définit comme un capital de connaissances accessible indépendamment des acteurs qui l\u0027ont créée (Prax, 2003).\nLa construction de cette mémoire dépend des sources de connaissances disponibles et valides qui peuvent être utilisées tels que de la documentation technique, les experts humains ou des courriers électroniques. Selon les sources et les objectifs définis conjointement avec les utilisateurs, différentes approches ont été proposées : GED, mémoire documentaire, mémoire à base de connaissances, mémoire à base de cas, ... Chaque approche est associée à des techniques spécifiques de recueil de données comme des entretiens, des observations de l\u0027expert en situation de travail ou de la transmission via un éditeur. Pour de nombreuses formes d\u0027interaction, des outils de visualisation adaptés s\u0027avèrent des médiateurs efficaces, facilitateurs de dialogue (Aubertin et al., 2003;Colloque ESIEE, 2002). Ils permettent d\u0027apporter aux intervenants humains un substrat artificiel qui transcrive un grand nombre d\u0027informations et qui soit un support à leurs connaissances et à leurs intuitions pour qu\u0027ils puissent non seulement plus facilement exprimer leurs savoirs tacites et implicites mais aussi découvrir des nouvelles connaissances (e.g. relations). D\u0027une façon plus générale, la visualisation de connaissances est un domaine naissant en plein essor (Eppler et Burkhard, 2005) et l\u0027analyse de cet aspect fondamental dans un processus de gestion des connaissances (GC) n\u0027en est qu\u0027à ses débuts (Burkhard, 2004).\nDans cet article nous nous focalisons sur l\u0027intégration de la visualisation dans la phase d\u0027opérationnalisation d\u0027une mémoire à base de connaissances. Le serveur de connaissances Atanor qui nous sert d\u0027environnement de référence utilise une approche orientée vers le dé-ploiement de connaissances portant sur des systèmes complexes et provenant de sources multiples (Guillet et al., 2002). On peut citer comme exemples réels d\u0027applications l\u0027aide à la maintenance de machines de tri de courrier, le maintien en fonctionnement de sous-marins ou de navires de surface, le dépannage automobile.\nDès la conception d\u0027Atanor, une formalisation graphique des connaissances basée sur une adaptation des arbres de décision et de défaillances a été proposée (Guillet et al., 2000). Cependant, l\u0027instanciation de ce modèle visuel dans différents contextes applicatifs a mis en évidence des redondances qui peuvent entraver l\u0027interprétation synthétique du fonctionnement d\u0027un processus et masquer des points critiques. Bien que pour des raisons implicites de simplicité, les modèles d\u0027arbres soient souvent privilégiés en GC, nous discutons ici de l\u0027intérêt de l\u0027introduction d\u0027un modèle graphique basé sur des graphes qui sont des outils privilégiés bien connus pour modéliser un système de relations entre des entités. Ils permettent de caractériser préci-sément des propriétés d\u0027un tel système via un arsenal combinatoire sophistiqué (Berge, 1973) tout en facilitant l\u0027accès au profane à des structures complexes via notamment des représen-tations visuelles adaptées et nous présentons une comparaison entre ces deux approches pour une application réelle. Dans cet article nous proposons une extension du modèle d\u0027arbre à un modèle de graphe orienté en niveaux.\nLe reste de cet article est organisé de la façon suivante : le paragraphe 2 propose un état de l\u0027art des différentes approches pour la visualisation des connaissances dans les systèmes de GC. Un descriptif général du serveur de connaissances Atanor est rappelé dans le paragraphe 3. Le paragraphe 4 décrit la représentation par logigrammes sous forme arborescente et présente le nouveau modèle de graphes. Une illustration dans le cadre de la maintenance de machines de tri de courrier de la Poste est décrite dans le paragraphe 5.\nVisualisation graphique en Gestion des Connaissances\nLes représentations visuelles proposées dans les systèmes de gestion des connaissances sont pour la plupart basées, au moins implicitement, d\u0027un point de vue formel sur des modèles d\u0027arbres ou plus généralement de graphes. D\u0027un point de vue théorique, ces méthodes ont pour la plupart été initialement développées dans la communauté de la visualisation d\u0027information (Herman et al., 2000). D\u0027un point de vue applicatif en GC, la paire indissociable {modèle, représentation visuelle} dépend à la fois des connaissances dont on dispose, du mode de raisonnement sur ces connaissances et des différents points de vue \"utilisateurs\" considérés dans le SGC. Nous nous restreignons ici aux références associés à des applications en GC.\nDe façon générale, les modèles de visualisation sont souvent basés sur le modèle générique des réseaux sémantiques (Lehmann, 1992). Ils représentent avec les sommets d\u0027un graphe des concepts et avec les arcs des relations sémantiques entre ces concepts. La majeure partie des techniques présentées ci-dessous pourrait être considérée, dans le cadre d\u0027une représentation descriptive, comme des spécialisations des réseaux sémantiques.\nReprésentation par arbres\nLes représentations sous forme d\u0027arbres qui sont parmi les plus abouties, regroupent des techniques très différentes :\n-Les cartes cognitives (Buzan et Buzan, 1996)  \nReprésentation par graphes\nLa plupart des représentations par graphes en GC se retrouvent associées à trois grandes classes de modèles dont les intersections peuvent être importantes : les graphes conceptuels, les ontologies et les réseaux bayésiens.\n-Les graphes conceptuels ont été à l\u0027origine proposés comme une représentation graphique de la logique de premier ordre. Ils permettent de simplifier la mise en relation entre la logique et les langues naturelles (Sowa, 1992)   (Corby et Dieng, 1998).\nDescriptif général du serveur de connaissances Atanor\nLe serveur de connaissances Atanor dans lequel s\u0027intègre notre travail est construit autour de quatre modèles associés à des représentations graphiques :\n1. un \"modèle d\u0027expertise\" représentant les processus métiers qui permet de maintenir des connaissances procédurales exprimées sous formes de règles de raisonnement ; 2. un \"modèle organique\" qui permet de décrire la structure du système à travers sa décom-position organique ;\n3. un \"modèle des compétences\" associées au système décrivant les compétences des acteurs sur le système (de l\u0027équipe à l\u0027entreprise) en les hiérarchisant du plus global au plus spécifique ;\n4. un organigramme des personnes associé à un modèle de compétences pouvant intervenir sur le système (Vergnaud et al., 2004 Ces modèles interagissent entre eux : les connaissances portent sur des composants d\u0027un système dont la manipulation nécessite des compétences elles-mêmes portées par des individus de l\u0027organisation en charge de ce système (voir la figure 1). L\u0027architecture technique d\u0027Atanor est basée sur un serveur de connaissances réalisé majoritairement en Prolog. Le choix de ce langage pour l\u0027implémentation du serveur facilite la gestion interne des connaissances recueillies, mais surtout permet d\u0027activer ces connaissances. Le serveur est composé d\u0027un ensemble de modules proposant différentes vues sur les modèles dont le module Expert qui permet aux experts de capitaliser la connaissance en décrivant les modèles et leurs associations.\nDes logigrammes aux graphes en niveaux pour le module Expert\nNous nous focalisons dans la suite sur les représentations du modèle d\u0027expertise qui est associé au module Expert car il représente ici la mémoire organisationnelle. Il permet de représenter des connaissances procédurales actionnables liées à un savoir-faire se décomposant en une suite d\u0027étapes. Par exemple, dans le cas d\u0027un diagnostic pour l\u0027aide à la maintenance d\u0027un système industriel, la stratégie mise en oeuvre par les experts consiste à tester successivement des hypothèses sur l\u0027état des composants ou des fonctionnalités du système, et ceci en procédant généralement des hypothèses les plus simples aux plus complexes.\nReprésentation visuelle par des logigrammes\nLe premier modèle proposé, appelé logigramme d\u0027expertise, associe chaque étape du raisonnement de l\u0027expert à un sommet d\u0027un arbre. La représentation graphique des logigrammes a été présentée à l\u0027origine comme une généralisation enrichie des arbres de décision et des arbres de défaillance (Figure 2). Deux types de sommets structurants sont mis en évidence :\n(1) Les sommets tests associés à une variable, typiques des arbres de décision, dont les fils ne sont pas ordonnés, mais dont chaque arc est associé à une valeur de la variable. La variable est généralement associée à l\u0027état dans lequel se trouve un élément du système sous-jacent ; (2) les sommets modules, absents des arbres de décision et de défaillance, dont les fils sont ordonnés de gauche à droite et généralement du plus simple au plus complexe, au sens de l\u0027expert. Chacun de ces sommets permet de définir un \"module de connaissances\" permettant d\u0027intégrer -315 -RNTI-E-6 des principes cognitifs caractéristiques des stratégies de décision expertes dont un principe de parcimonie/décidabilité (Barthélemy et Mullet, 1992) : les premiers sommets fils d\u0027un module permettent d\u0027arriver à une décision à moindre coût par des opérations simples (parcimonie) et les sommets fils suivant offrent la possibilité de réaliser des opérations de plus en plus complexes afin d\u0027arriver à une prise de décision même si elle s\u0027avère coûteuse (décidabilité). Il existe aussi deux types de sommets terminaux : les sommets associés à un diagnostic indiquant une résolution du problème ainsi que la réparation à effectuer et les sommets associés à un échec indiquant une non résolution du problème et provoquant la mise en oeuvre d\u0027un mécanisme de retour au dernier sommet module traité et la transition au sommet suivant au sens de l\u0027ordre induit par ce sommet module.\nUne propriété importante de cette formalisation graphique réside dans la possibilité de transformer un logigramme en un ensemble de règles de production, en traduisant l\u0027ensemble des chemins menant de la racine à chacune des feuilles. Ainsi le logigramme de la figure 2 se transforme en 4 règles : \nFIG. 3 -Exemple de graphe en niveaux pour le modèle d\u0027expertise.\nLa représentation graphique des connaissances par logigramme a l\u0027avantage d\u0027être beaucoup plus intelligible et synthétique qu\u0027un ensemble équivalent composé d\u0027une liste de règles de production. Cependant, des sous-arbres correspondant à des sous-ensembles de règles de production utilisés dans différentes phases peuvent être dupliqués à l\u0027issue de la phase d\u0027expertise. Cette duplication lorsqu\u0027elle est fréquente peut nuire à l\u0027intelligibilité de la représentation visuelle.\nLe modèle Graph\u0027Atanor et sa représentation visuelle\nPour palier aux limites du logigramme d\u0027expertise, nous avons développé un modèle de graphe en niveaux. Ce modèle a pour avantage de pouvoir exploiter directement le modèle Prolog qui associe directement un sommet avec l\u0027ensemble de ses fils sans redondance. Dans le modèle de graphes, les sommets représentent comme dans les logigrammes les tests, les modules ou les diagnostics. La différence majeure est ici l\u0027unicité ; un sommet ne peut pas être dupliqué. Ces sommets sont ordonnés dans des niveaux : en notant 1 le premier niveau, le niveau i contient les sommets dont la longueur du plus long chemin originaire du niveau 1 vaut i. Les arcs représentent pour les sommets tests les différentes valeurs possibles de la variable associée à ce sommet. Pour les sommets modules, comme dans les logigrammes, les arcs sont associés à un numéro d\u0027ordre qui définit l\u0027ordre de priorité de la transition (voir la figure 3).\nL\u0027objectif général de la représentation visuelle d\u0027un tel graphe est de fournir un tracé intelligible sur un support standard. La qualité du dessin est décisive pour l\u0027appropriation de la représentation par l\u0027utilisateur (Purchase, 2000). Lorsqu\u0027une convention de tracé est donnée (ici le tracé en niveaux), on retient en général deux concepts de base (Di-Battista et al., 1999) :\n1. les contraintes du support et de l\u0027oeil humain qui imposent notamment des écarts minimums à respecter entre les sommets et les arcs.\n-317 -RNTI-E-6\nVisualisation graphique en GC -le modèle Graph\u0027Atanor 2. les critères \"esthétiques\" qui définissent les propriétés à satisfaire pour faciliter l\u0027intelligibilité. Ces critères sont définis par des contraintes combinatoires : minimisation du nombre de croisement d\u0027arêtes, minimisation de la somme des longueurs des arêtes, minimisation des coudes dans certains types de tracés, ... La plupart de ces critères ne peuvent cependant pas être satisfaits simultanément. Des travaux récents en psychologie cognitive ont montré que la réduction des croisements d\u0027arêtes est le critère prépondérant pour la lisibilité et la mémorisation (Purchase, 1997).\nDans le cas d\u0027un tracé en niveaux, la minimisation du nombre de croisements d\u0027arcs peut sembler plus simple que le problème plus général de minimiser le nombre de croisement d\u0027arêtes sur un graphe quelconque puisque le choix de coordonnées géométriques pour les sommets est ici remplacé par le choix d\u0027un ordre des sommets sur chaque niveau. Le problème reste néanmoins NP-complet (Garey et Johnson, 1983). Un grand nombre d\u0027heuristiques pour ce problème suivent le principe du balayage successif des différents niveaux : les sommets de chaque niveau sont réordonnés de façon à réduire le nombre de croisements d\u0027arcs. Des stratégies très variées ont été proposées pour le réor-donnancement (e.g. Laguna et al. (1997) pour plus de détails). Les plus utilisées sont basées sur des méthodes de tris qui utilisent le nombre de croisements d\u0027une façon proche des tris classiques et des heuristiques basées sur le principe selon lequel le nombre de croisements diminue si un sommet se trouve à peu près au milieu de ses voisins sur les niveaux adjacents (Sugiyama et al., 1981). Plus récemment différentes méthaheuristiques ont été développées pour ce problème : recherche tabou (Laguna et al., 1997), GRASP (Marti, 2001) et les algorithmes génétiques hybridés dont on a expérimentalement montré qu\u0027ils sont très compétitifs et peuvent présenter des avantages certains dans le cadre d\u0027un tracé interactif (Pinaud et al., 2004), cadre privilégié en GC. Ainsi, la représentation visuelle de Graph\u0027Atanor a été effectuée par un algorithme génétique hybridé avec deux spécificités majeures (Kuntz et al., 2006) : deux opérateurs de croisements adaptés aux représentations en niveaux, et une recherche locale combinant différentes transformations locales.\nAnalyse comparative des modèles graphiques\nNous avons comparé les logigrammes et les graphes en niveaux dans le cadre applicatif de l\u0027aide au diagnostic pour la maintenance de machines de tri automatique de courrier (La Poste). Après une période d\u0027adaptation et de formation à l\u0027éditeur de connaissances, les experts ont commencé à assurer la construction, la mise à jour et l\u0027évolution des connaissances maintenues par l\u0027outil. La phase de recueil des connaissances s\u0027est étalée sur deux ans et s\u0027est appuyée sur quatre experts géographiquement dispersés. Elle a permis de mettre en évidence une trentaine de pannes possibles, nécessitant la construction d\u0027un logigramme par panne. Les experts ont ainsi fait apparaître plus de 400 sommets tests et environ 200 diagnostics différents ont été répertoriés.\nLa figure 4-a propose un extrait d\u0027un logigramme pour la résolution d\u0027une panne précise. La figure 4-b est un extrait du graphe en niveaux pour la même panne. Pour obtenir un dessin le plus lisible possible, la contrainte qui impose un ordre des fils dans les sommets modules dans le modèle d\u0027arbres est relaxée. Pour les sommets tests, les réponses aux tests permettant de choisir le chemin à suivre ne sont pas affichées. Plusieurs avantages apparaissent clairement sur la représentation avec un graphe. Le principal est ici que les parties dupliquées dans le logigramme ne le sont plus dans le graphe. Cette non-duplication permet une meilleure exploitation du tracé par l\u0027expert ou le manager :\n-on voit clairement qu\u0027un même diagnostic peut être effectué rapidement en un nombre minimum de tests ou plus lentement avec plus d\u0027étapes, -un même diagnostic peut avoir des effets multiples (non représenté ici), -les sommets avec un degré important ont statistiquement plus de chances d\u0027être utilisés dans les diagnostics et donc une attention particulière préventive pourra être portée sur les parties du système concernées (maintenance préventive). Une conséquence immédiate est de pouvoir améliorer la répartition des experts ou des techniciens pour être certain d\u0027avoir toujours une personne compétente sur les pannes risquant de se produire souvent. De plus, le comptage des passages dans chaque sommet différent est plus simple dans le graphe que dans le logigramme. Cette statistique permet à l\u0027expert ou au manager de recenser les composants du graphe peu utilisés (pannes peu fréquentes) de ceux qui le sont fréquemment (pannes fréquentes pouvant indiquer une faiblesse dans le système).\nConclusion\nLa visualisation graphique des données en GC est d\u0027une importance majeure pour une bonne utilisation des SGC et leur appropriation par les utilisateurs. Nous avons montré dans le cadre d\u0027un véritable exemple industriel que les représentations en arbres, qui sont parmis les plus abouties, sont limitées par rapport à un modèle de graphes. Ce modèle qui peut paraître à priori plus compliqué, permet d\u0027améliorer les représentations visuelles pour s\u0027assurer d\u0027une meilleure exploitation des modèles de connaissances par les experts.\nBien qu\u0027étant de taille limitée et utilisant peu de données, les représentations de la figure 4 sont déjà trop grandes pour tenir correctement sur un support standard (feuille de papier ou écran) tout en restant lisible. Par exemple, le graphe permettant de représenter l\u0027ensemble des pannes de la machine de tri de courrier compte 553 sommets et 625 arcs. Ce graphe permet d\u0027apporter des informations supplémentaires intéressantes pour les experts mais même après optimisation du tracé, il reste encore environ un millier de croisements d\u0027arcs. Ce graphe ne peut donc pas être représenté entièrement sur un écran et d\u0027autres méthodes complémentaires de visualisation adaptées pour ces grandes structures de données sont à envisager (Munzner, 2000). De plus, lors de la saisie des graphes par les experts, il est intéressant d\u0027optimiser le tracé après l\u0027ajout de sommets et d\u0027arcs pour conserver un graphe lisible et compréhensible. Dans ce cas le tracé devient intéractif et les algorithmes de visualisation doivent prendre en compte le tracé obtenu à l\u0027instant t avant de produire celui de l\u0027instant t + 1 pour respecter au mieux la carte mentale de l\u0027utilisateur (Eades et al., 1991) et ainsi éviter à ce dernier de dépenser une énergie cognitive importante inutilement pour redécouvrir le tracé.\nRéférences\nAissaoui, G., D. Genest, et S. Loiseau (2003). Le modèle des cartes cognitives de graphes conceptuels : un modèle graphique d\u0027aide à la prise de décision. In Actes 2° journées francophones Modèles Formels de l\u0027Interaction (MFI), pp. 243-248. Cepaduès.\n"
  },
  {
    "id": "1095",
    "text": "Introduction\nLes méthodes de fouille visuelle de données (\"Visual data mining\") tentent de résoudre les problèmes d\u0027interprétation et d\u0027interaction dans les processus de découverte de connaissances en faisant appel à des visualisations dynamiques et à des requêtes graphiques sur les données et connaissances représentées (Cleveland, 1993), (Shneiderman, 1996), (Wong et Bergeron, 1997). A titre d\u0027exemples classiques, nous pouvons citer les visages de Chernoff (Chernoff, 1973) qui représentent des données sous la forme d\u0027icones en s\u0027appuyant sur le fait que l\u0027esprit humain analyse facilement les ressemblances et différences entre visages. Nous pouvons citer également les \"scatter plots\" (Becker et Cleveland, 1987) qui permettent d\u0027obtenir des vues multiples sur les données et d\u0027observer les données à l\u0027aide de techniques graphiques comme le \"brushing\" qui donne la possibilité de sélectionner des données dans une vue tout en soulignant ces mêmes données dans les autres vues.\nCes méthodes apportent des nouveautés et poursuivent des objectifs qui sont prometteurs pour le domaine de la fouille de données : utilisation de la perception visuelle et souvent de la perception pré-attentive (Healey et al., 1993), interaction dynamique avec les données, simplicité d\u0027utilisation, exploitation directe des résultats. Cependant, ces méthodes ont également des limites en ce qui concerne la fouille de données : les données visualisées sont le plus souvent numériques, les visualisations et leur manipulation nécessitent un apprentissage (comme c\u0027est le cas par exemple pour interpréter des graphiques de types \"parallel coordinates\" (Inselberg, 1985)), l\u0027interaction dynamique demande beaucoup de ressources de calcul (modifications en temps réel) et doit donc faire appel à des algorithmes les plus rapides possible (mais qui doivent par ailleurs fournir le plus d\u0027information possible).\nDans ce travail, nous proposons une nouvelle méthode de fouille visuelle de données, adaptée elle-même des méthodes à base de points d\u0027intérêt utilisées pour la visualisation de données textuelles. Nos objectifs, outre ceux poursuivis par la fouille visuelle de données, sont les suivants : pouvoir représenter tous les types de données en se basant uniquement sur l\u0027existence d\u0027une fonction de similarité (ou de distance) entre les données, avoir des affichages très rapides lors des interactions dynamiques et traiter si possible de grands volumes de données (algorithmes de complexités temporelle et spatiale linéaires en fonction du nombre de données), utiliser une visualisation nécessitant un temps d\u0027apprentissage le plus court possible (donc compréhensible par la majorité des utilisateurs potentiels qui ne sont pas considérés comme des experts en fouille de données).\nL\u0027article est organisé comme suit : la section 2 décrit les techniques initiales utilisant les points d\u0027intérêts dans le contexte de la visualisation de données textuelles. Dans la section 3 nous décrivons notre approche en commençant par spécifier l\u0027utilisation des points d\u0027intérêt pour la fouille de données puis en évaluant cette méthode sur des données aux caractéristiques connues. Dans la section 4 nous décrivons l\u0027application de notre méthode sur des données classiques, puis le contexte d\u0027application réelle de cette étude, à savoir extraire visuellement des connaissances à partir de données issues d\u0027enquêtes de satisfaction. Nous concluons et dégageons des perspectives dans la section 5.\nSurvol des méthodes visuelles à base de points d\u0027intérêts\nCette méthode est désignée par les termes \"points d\u0027intérêts\" ou \"points de références\" (en anglais POIs pour \"Points Of Interest\"). Il s\u0027agit dans cette méthode de positionner sur une surface, représentée par un disque, quelques icônes (POIs) relatifs aux attributs d\u0027une donnée et ensuite d\u0027afficher les icônes des données à des positions déterminées par la similarité entre les POIs et les données. Par exemple, cette visualisation a été utilisée comme méthode d\u0027affichage de documents issus d\u0027un moteur de recherche, facilitant la navigation dans l\u0027ensemble des documents retournés par une requête. Les POIs sélectionnés sont généralement des motsclés utilisés dans la requête et les données sont les documents se positionnant par rapport à ces mots-clés. Le choix des mots-clés dépend de la fréquence de leur occurrence dans les documents. Pour visualiser ces données, on utilise en général pour cela des techniques à base de ressorts et de forces, la force s\u0027exerçant entre un POI et une donnée étant proportionnelle à la similitude entre ce POI et cette donnée. Les systèmes VIBE (Korfhage, 1991), ou un système rigoureusement simplifié de VIBE (Morse et al., 2002), SQWID (McCrickard et Kehoe, 1997), Radviz (Hoffman et al., 1999) ou encore la visualisation radiale du système Information Na- (Au et al., 2000) (image gracieusement fournie par Stefan Rüger).\nFIG. 1 -Exemple de visualisation radiale\nvigator (Au et al., 2000) utilisent ces principes. Parfois il est difficile de voir exactement vers quel point d\u0027intérêt est attiré une donnée. Dans ce cas ces systèmes permettent alors de supprimer et d\u0027ajouter des points d\u0027intérêt sur le disque pour permettre une meilleure représentation des données. Ce sont les principales opérations interactives proposées par ces méthodes.\nRadial (Au et al., 2000) est une visualisation qui est très semblable au système VIBE (Korfhage, 1991), à Radviz (Hoffman et al., 1999) et à Lyberworld (Hemmje et al., 1994).\nInitialement, après extraction du résultat de la requête, Radial identifie une série de termes clés relatifs à ce résultat. Ensuite les 12 premiers termes les plus recensés dans l\u0027ensemble du résultat sont arrangés tout autour d\u0027un cercle. Il est possible de modifier la liste des termes affichés, le choix se faisant sur deux listes placées à gauche de l\u0027écran. Un nuage de points est alors affiché à l\u0027intérieur du cercle, un point représente une donnée. Il n\u0027est affiché que les données en rapport avec les mots-clés alignés autour du cercle. Un point est comme suspendu par des ressorts reliés aux mots-clés en rapport avec celui-ci. Il est donc impossible de déplacer un point en cliquant dessus, du fait des forces exercées par les ressorts. Par contre, en cliquant sur un point, les mots-clés en rapport avec ce point sont éclairés et une bulle affiche des informations sur cette donnée. Il est possible de déplacer les termes à l\u0027extérieur du cercle et ainsi de déplacer tous les noeuds des données en rapport avec ces termes. Ceci permet de faire un classement manuel des résultats en catégories (voir la figure 1).\nTous ces systèmes ont montré que ce type de visualisation dynamique apporte un grand intérêt pour l\u0027utilisateur qui peut extraire lui-même de l\u0027information en toute simplicité. La rapidité d\u0027affichage couplée à la possibilité d\u0027interaction apportent un plus à ces méthodes. Par ailleurs, elles peuvent a priori visualiser des données de différentes natures comme des données symboliques ou numériques. A notre connaissance, ces méthodes à base de points d\u0027intérêt n\u0027ont pas encore été utilisées pour la fouille visuelle de données comme nous allons le présenter dans les sections qui suivent.\nFIG. 2 -Principes de base de la visualisation avec illustration du positionnement d\u0027une donnée D i en fonction des k points d\u0027intérêt.\n3 Utilisation des points d\u0027intérêt pour la fouille de données\nPrincipes de base de la visualisation\nOn considère n données D 1 , ..., D n et une matrice de similarité Sim entre ces données. Sim(i, j) est la similarité entre les données D i et D j , cette matrice étant symétrique et avec une diagonale à 1. On note également que si Sim(i, j) \u003d 1 alors les données D i et D j sont identiques, et que si Sim(i, j) \u003d 0 alors elles sont totalement différentes.\nDans un premier temps, nous allons considérer que les POIs sont un sous-ensemble de ces données notés D 1 , ..., D k . Nous affichons ces k données sur un cercle avec un arc de longueur constante entre chaque POI (voir la figure 2). On note par\nOn veut ensuite positionner les n ? k données restantes en fonction de leur similarité aux POIs D 1 , ..., D k . On utilise la formule suivante pour calculer les coordonnées d\u0027affichage\nle poids w j est calculé de la manière suivante :\nAjoute comme POI \nFIG. 3 -Principales interactions avec les données et les POIs.\nSi D i est identiquement similaire à l\u0027ensemble des POIs, elle sera affichée au milieu du cercle. Inversement, si elle est totalement similaire à un POI et totalement différente des autres, sa position sera confondue avec celle de ce POI. Si sa similarité est biaisée vers certains POIs, celle-ci aura tendance à se rapprocher de ces POIs.\nPlus généralement, notre méthode est telle que deux données proches l\u0027une de l\u0027autre dans l\u0027espace d\u0027origine le seront donc également vis à vis des POIs, et elles se retrouveront donc proches dans l\u0027espace 2D. L\u0027espace visualisé devient donc un espace de distance entre des points choisis (les POIs) et les données. C\u0027est de cette manière que cette méthode peut traiter tout type de données. Par contre, la réciproque de cette propriété n\u0027est pas vraie : deux données proches dans l\u0027espace 2D ne le sont pas forcément dans l\u0027espace d\u0027origine (tous les points à distance égale de deux POIs dans l\u0027espace d\u0027origine forment une droite médiatrice, et ne sont pas systématiquement proches les uns des autres). Il faudra utiliser d\u0027autres méthodes pour lever ces ambiguïtés (voir la dernière section).\nEnfin, l\u0027affichage, comme nous le recherchons, nécessite très peu de calcul et ne demande que de calculer qu\u0027une partie seulement des similarités (k × (n ? k)).\nPlusieurs interrogations sont soulevées par cette méthode. Tout d\u0027abord, le choix initial des points d\u0027intérêt doit être effectué. Dans un premier temps, nous considérons que si les données sont supervisées (on dispose d\u0027un label de classe), alors nous prenons le premier représen-tant de chaque classe comme POI initial. Il y aura donc autant de POIs que de classes dans la première visualisation proposée à l\u0027utilisateur. Si les données sont non supervisées, nous choisissons les k premières données. D\u0027autres choix automatiques sont possible (et certainement plus judicieux) comme nous le décrivons dans la dernière section, et nous rappelons qu\u0027il s\u0027agit ici de proposer des choix initiaux que l\u0027utilisateur va pouvoir modifier interactivement et dynamiquement en fonction de ce qui est affiché (voir section suivante). Une deuxième interrogation vient de l\u0027ordre des POIs : si un grand nombre de données sont attirées par deux POIs, alors il y a tout intérêt à ce que ces POIs soient proches les uns des autres sur le cercle. Une situation critique consisterait à placer ces POIs de manière diamétralement opposée, ce qui engendrerait une visualisation peu lisible (beaucoup de données au centre). Nous proposons une solution interactive à ce problème dans la section suivante, mais il est évident que des solutions automatiques peuvent être trouvées comme ordonner les POIs en fonction de leur similarité (voir dernière section). Il serait également possible de ne pas conserver un arc de longueur fixe entre les POIs, afin de représenter les similarités qui existent entre POIs.\nInteraction avec la visualisation\nPour être réellement efficace, la visualisation d\u0027information doit être interactive et permettre d\u0027affiner dynamiquement l\u0027affichage et de répondre aux requêtes graphiques de l\u0027utilisateur. Dans la visualisation avec des POIs, l\u0027utilisateur peut se poser au moins les questions suivantes : quelle est cette donnée (ou ce POIs), comment agrandir cette partie de la visualisation (zoom sans perte de contexte), comment changer de POIs (en enlever, en rajouter, changer leur ordre, et comme on va le voir dans cette section, définir des POIs qui ne soient pas nécessairement des données de l\u0027ensemble de départ).\nLors du passage de la souris sur un point, nous indiquons donc quel est ce point. Ensuite, il est possible de \"zoomer\" sur une donnée par clic de la souris. Le zoom qui se déclenche alors effectue les opérations suivantes : il centre la donnée sur le centre du cercle, il agrandit la zone centrée sur cette donnée et repousse les autres données vers les bords de la visualisation. La déformation est calculée à l\u0027aide d\u0027une fonction hyperbolique. Ce zoom permet de grossir la vue tout en conservant l\u0027ensemble des données. Actuellement cette fonction utilise des coordonnées cartésiennes en considérant le carré dans lequel est contenu le cercle de la repré-sentation, mais nous allons définir un zoom utilisant des coordonnées polaires, afin de laisser toutes les données à l\u0027intérieur du cercle. La zone d\u0027agrandissement sera plus petite mais les données resteront dans la zone habituelle.\nEn ce qui concerne les POIs, nous avons représenté sur la figure 3 les principales interactions possibles : tout d\u0027abord, il est possible d\u0027enlever un POI. Cela s\u0027effectue très simplement en faisant glisser un POI à l\u0027intérieur du cercle. Ce POI reprend alors sa place au sein des données. La vue est recalculée dynamiquement. Une transition dynamique et progressive est mise en place pour que l\u0027utilisateur puisse suivre le changement de représentation. Ce dernier a la possibilité d\u0027annuler son action, ce qui a pour effet de remettre le POI sur le cercle. Il est possible également de choisir une donnée et de la définir comme POI. Pour cela, on fait glisser la donnée sur le cercle. Si la donnée est placée sur un POI, elle remplace celui-ci, et si elle est placée entre deux POIs, elle s\u0027insère en décalant les autres POIs de manière à maintenir constant les longueurs des arcs entre les POIs. Ces fonctionnalités sont très importantes puisqu\u0027elles vont permettre à l\u0027utilisateur de redéfinir à volonté la représentation.\nEnfin, il est possible de généraliser les POIs de manière à ce qu\u0027ils ne soient plus nécessai-rement des données, mais plus généralement tout point de l\u0027espace de représentation et même tout objet pour lequel il est possible de calculer une similarité avec les données. Ainsi, on peut représenter des données \"idéales\", n\u0027existant pas réellement, et par rapport auxquelles l\u0027utilisateur voudrait positionner les données réelles. Nous présentons dans la section 4 une application -340 -RNTI-E-6 typique de cette fonctionnalité. Egalement, il serait possible de représenter par exemple une règle de décision comme un POI, et de placer les cas qui se rapprochent le plus de cette règle. Cette fonctionnalité offre de nombreuses perspectives en visualisant non plus seulement des données mais également des connaissances.\nAutres propriétés\nPour illustrer le fonctionnement de notre méthode, nous avons représenté sur la figure 4 un exemple \"jouet\" de données dont on connaît parfaitement les caractéristiques. Dans cette figure, nous avons utilisé comme POIs les centres de chacune des classes. Après calcul de la matrice de similarité sur la base de la distance Euclidienne (les attributs sont préalablement normalisés), on constate que la représentation respecte l\u0027organisation d\u0027origine des classes et les relations de voisinage qui existent entre les données. A titre de comparaison, nous avons représenté les mêmes données avec les coordonnées parallèles, dont la représentation semble moins intuitive que notre approche. Nous montrons également dans cette figure qu\u0027il peut être important de faire apparaître des rayons particuliers sur notre cercle. Ainsi, les rayons médians (en traits pleins) marquent la séparation entre les données attirées par l\u0027un ou l\u0027autre des deux POIs considérés. Ils permettent de dire qu\u0027une donnée est plus proche de tel POI que de tel autre. Les rayons issus de chaque POI (traits en pointillés) marquent au contraire le fait qu\u0027une donnée est à similarité égale entre les deux POIs opposés.\nSur la figure 5, nous avons utilisé une configuration différente des données \"jouet\". Une des classes est allongée, et l\u0027on constate que l\u0027on retrouve cette information dans notre visualisation. Cependant, d\u0027autres dispositions des données (voir figure 6) donnent des informations locales plus difficiles à interpréter. Les relations de voisinage sont respectées.\nRésultats\nBases artificielles et classiques\nNous avons évalué cette méthode sur un ensemble de bases composées de différentes données artificielles et classiques. La figure 7 représente la base de données artificielles Art1 composée de 400 données et de 4 classes. Nous illustrons en particulier les effets du zoom. Lorsque l\u0027on dispose seulement de deux classes, les données des deux classes sont positionnées sur le segment de droite allant du POI1 au POI2 (cf. figure 8 où nous utilisons une base de 1000 données avec 2 classes). Pour aider l\u0027utilisateur à mieux visualiser les données, on a permis l\u0027ajout d\u0027un POI supplémentaire tel que dans la figure 8(b) ou de plusieurs POIs ( figure 8(c)).\nEnfin, nous avons testé notre approche sur des bases classiques (issues du \"Machine Learning Repository\", (Blake et Merz, 1998)). Nous avons ainsi représenté sur la figure 9 les données Iris (150 données, 3 classes), Wine (178 données, 3 classes) et Segment (2310 données, 7 classes). On retrouve les formes de classes usuelles (comme pour Iris et Wine par exemple).\nApplication réelle\nDans le cadre de ses activités, Agicom collecte des données issues d\u0027enquêtes de satisfaction à l\u0027aide de questionnaires. Ces données se présentent sous la forme d\u0027un tableau\nFIG. 4 -Exemple \"jouet\" de visualisation de données représentées par deux attributs numé-riques x1 et x2 (voir en a)), avec visualisation sous la forme de coordonnées parallèles (en b)), puis représentation avec des POIs (en (c) et (d)).\nindividus×variables où ces variables sont qualitatives, i.e. des variables dont les modalités sont symboliques et naturellement ordonnées (\"ravi\", \"satisfait\", \"insatisfait\", \"déçu\"\u0027 et \"NSP\" (Ne Sais Pas)). Pour qu\u0027un consultant, ou qu\u0027un expert, puisse exploiter ces données, il est important de pouvoir visualiser graphiquement la satisfaction des clients afin de détecter des correspondances possibles entre individus, de connaître l\u0027évolution des clients d\u0027un segment à l\u0027autre, mais aussi de visualiser la relation existante entre une variable définie et les autres variables. Notre but est donc d\u0027élaborer un outil de représentation graphique des résultats d\u0027enquêtes de satisfaction contribuant au but de savoir comment améliorer la satisfaction des clients.\nNous avons évalué et testé notre méthode sur une première base Agicom1 composée de 31 données non supervisées. La figure 10(a) illustre cette première application dans laquelle les POIs ne sont pas des données mais des profils type de variables. Un profil correspond donc à une répartition de modalités (réponses) de cette variable. Ainsi, les POIs représentent différentes typologies connues de variables ou à priori (réponses très positives, combinaisons de modalités de variables).\nNous présentons un deuxième exemple sur la base Agicom2 (cf. la figure 10(b)). Dans cette (a) (b) (c)\nFIG. 5 -Autres données \"jouet\" initiales (a), puis visualisation avec notre méthode, (b) et (c).\n(a) (b) (c)\nFIG. 6 -Autre disposition de données initiales (a) et visualisation, (b) et (c).\napplication, nous avons permis aux utilisateurs d\u0027Agicom d\u0027interagir sur les caractéristiques des POIs et sur le zoom (cf. la figure 10(b)). De plus, sur cette base nous avons permis la visualisation des différentes classes. Une validation avec des utilisateurs réels est en cours.\nConclusions et perspectives\nNous avons décrit dans cet article une nouvelle méthode de visualisation inspirée des travaux réalisés dans le contexte de la recherche documentaire sur les points d\u0027intérêt. Elle consiste à transformer un espace d\u0027origine représenté par une matrice de similarité en une représentation visuelle 2D de ces similarités. Cette méthode possède des atouts comme la rapidité d\u0027affichage, une présentation intuitive des données et d\u0027apprentissage plutôt rapide, des capacités interactives. Nous avons détaillé son comportement sur des données jouet, sur des données classiques et enfin dans le cadre d\u0027une application réelle en cours de déploiement.\nPlusieurs perspectives peuvent se dégager. Nous avons mentionné l\u0027importance du choix des POIs ainsi que de leur disposition sur le cercle. Une première extension consiste à étudier l\u0027utilisation d\u0027un algorithme d\u0027optimisation afin de trouver la disposition de POIs la plus efficace visuellement. Il s\u0027agit donc de trouver, dans l\u0027espace des permutations des k POIs choisis, l\u0027ordre qui permettra de maximiser certaines propriétés comme la ressemblance entre deux POIs successifs sur le cercle. Une autre perspective importante consiste à étendre la visualisation de manière à enlever les ambiguïtés liées au chevauchement des points. Nous comptons utiliser une méthode d\u0027affichage de graphe à base de forces et de ressorts afin d\u0027éloigner les points qui se trouvent trop proches sur le graphe. Il s\u0027agit aussi de distinguer les points qui sont placés au même endroit mais qui ont une similarité moyenne différente avec les POIs. Une approche 3D sera testée. Enfin, comme nous l\u0027avons mentionné, nous allons ajouter une méthode classique de zoom hyperbolique travaillant sur des coordonnées polaires afin que les points ne sortent pas du cercle. \n"
  },
  {
    "id": "1096",
    "text": "Introduction\nDe plus en plus de connaissances scientifiques sont accessibles soit grâce à des documents publiés sur le web, soit dans des bases de données. Certaines de ces connaissances reposent sur des interprétations humaines de résultats d\u0027expériences. Ces connaissances sont, entre autres, indispensables pour la vérification, la validation ou l\u0027enrichissement du travail des chercheurs du domaine considéré. Mais la quantité énorme de données provenant de sources internes ou externes aux organisations rend très difficile la détection, le stockage et l\u0027exploitation de ces connaissances. Ceci est le cas de la recherche dans le domaine de la biologie moléculaire et plus particulièrement dans le domaine des puces à ADN.\nLes biologistes travaillant dans ce domaine manipulent de grandes quantités de données dans différentes conditions expérimentales et doivent se référer à des milliers de publications scientifiques liées à leurs expériences. Ces biologistes ont donc sollicité un support méthodo-logique et logiciel qui les aiderait dans la validation et/ou l\u0027interprétation de leurs résultats et qui leur faciliterait la planification de nouvelle expérimentation.\nC\u0027est dans ce contexte que le projet MEAT a été proposé en fournissant des solutions permettant de remédier à ces problèmes.\nAprès la présentation du contexte général et de la problématique de ce travail, nous dé-taillons notre approche adoptée pour MEAT (Khelif et al, 2005) ainsi que les différentes composantes de notre architecture et nous concluons avec une comparaison avec des travaux similaires.\nContexte\nLa technologie des puces à ADN a été développée après le séquençage dans le but de dé-couvrir les fonctions des gènes dans différents contextes biologiques. Ces expériences permettent l\u0027accès à des milliers de gènes simultanément et fournissent une masse énorme de données, ce qui engendre des difficultés pour les biologistes particulièrement dans la validation et l\u0027interprétation des résultats obtenus.\nLes besoins exprimés par les biologistes travaillant dans ce domaine sont: ? Une vue sur les expériences connexes : essayer d\u0027identifier des relations entre les expé-riences (bds locales ou en ligne) et de découvrir des nouvelles pistes à explorer. ? Aide à la validation des résultats expérimentaux : en recherchant dans les articles traitant le phénomène étudié des informations qui argumentent, confirment ou infirment leurs hypothèses de départ, ce qui nécessite une richesse dans les annotations. ? Aide à l\u0027interprétation des résultats : en identifiant de nouvelles relations entre gènes et/ou les interactions pouvant exister entre eux, avec des composants cellulaires ou des processus biologiques. Ces besoins nous ont conduits à réaliser le projet MEAT en collaboration avec les biologistes de la plate-forme puce à ADN de Sophia Antipolis (localisée à l\u0027IPMC 1 ) qui distribue des puces pour les autres laboratoires français. Ce projet nous permet ainsi d\u0027explorer l\u0027intérêt d\u0027un « web sémantique organisationnel » sur l\u0027échelle d\u0027une communauté.\nMémoire d\u0027entreprise et web sémantique\nActuellement, les techniques du web sémantique peuvent jouer un rôle très important dans la gestion des connaissances et la construction des mémoires d\u0027entreprise. En effet, les ontologies peuvent être utilisées dans la représentation des connaissances en fournissant un cadre formel pour décrire les différentes sources de connaissances et en guidant la création d\u0027annotations sémantiques facilitant la description, le partage et l\u0027accès à ces sources.\nDans (Dieng-kuntz, 2005), il est proposé de matérialiser une mémoire d\u0027entreprise à travers un « web sémantique d\u0027entreprise » en utilisant les ontologies pour formaliser le vocabulaire partagé dans une communauté, et les annotations sémantiques basées sur ces ontologies pour décrire les sources de connaissances hétérogènes (corpus textuels, base de données, experts…) et faciliter leurs accès via Internet/Intranet. riences ou les articles ; ce qui a mené au développement du module MeatEditor. 5. L\u0027aide à la validation des résultats expérimentaux en proposant une recherche documentaire guidée par MeatOnto et utilisant les annotations sémantiques; nous avons donc dé-veloppé un module nommé MeatSearch. 6. L\u0027aide à l\u0027interprétation des résultats en faisant des inférences plus avancées sur les annotations sémantiques pour expliquer un comportement particulier; ce qui mène à d\u0027autres fonctionnalités du module MeatSearch.\nBase d\u0027annotations\nMeatOnto chargement\nMeatSearch\nMEDIANTE\nInterfaces\nFIG. 1 -Architecture de Meat\nLes composantes de MEAT\nL\u0027ontologie : MeatOnto\nComme nous l\u0027avons décrit précédemment notre but était de construire une ontologie qui décrit toutes les ressources du domaine des puces à ADN, nous avons donc opté pour une ontologie modulaire composée de trois sous-ontologies dédiées à différentes parties : ? UMLS : ce projet élaboré par la NLM (National Library of Medecine de Bethesda) propose depuis 1986 de mettre au point un langage médical unifié (Humphreys et al, 1993). Pour ce faire, ce langage repose sur : (1) un métathesaurus qui énumère tout le vocabulaire médical existant et qui comprend des millions de termes ; (2) un réseau sémantique constitué d\u0027une hiérarchie de 134 types sémantiques et de 54 relations ; il représente une classification de tous les concepts représentés dans le métathesaurus ainsi que les relations pouvant exister entre eux. Par analogie, nous avons considéré, le réseau sémanti-que de UMLS comme une ontologie : la hiérarchie des types sémantiques est la hiérar-chie des concepts et les termes du métathesaurus sont des instances de ces concepts. ? MGED 4 : c\u0027est une ontologie proposée pour décrire les expériences des puces à ADN afin de faciliter le partage des résultas (Stoeckert, 2003). Nous avons utilisé cette ontologie dans MEAT afin de décrire les expériences stockées dans MEDIANTE. ? DocOnto : Nous avons développé cette ontologie pour décrire des métadonnées sur les articles (auteurs, sources…) et sur les annotations (generated_by, validated_by…). Elle représente aussi la structure des articles (abstract, sentence, relation…) et fait le lien entre les articles et les concepts de UMLS (has_relation, speaks_about_genes…). Nous avons effectué le codage de l\u0027ontologie UMLS automatiquement à l\u0027aide d\u0027un script permettant de traduire le réseau sémantique de son format textuel vers une ontologie représentée dans le format RDFS (McBride, 2004). L\u0027ontologie DocOnto a été construite progressivement pour couvrir tous nos besoins concernant la description des connaissances contenues dans les articles.\nMeatAnnot 3.2.1 Génération des annotations\nMalgré ses avantages, la création d\u0027une annotation sémantique est un processus difficile et coûteux pour les biologistes. Ceci nous a amené à développer le système MeatAnnot qui à partir d\u0027un texte (articles fournis par les biologistes) permet la génération d\u0027une annotation structurée, basée sur MeatOnto et qui décrit le contenu sémantique de ce texte.\nMeatAnnot repose sur des outils de TALN (Traitement Automatique de la Langue Naturelle) : GATE (Cunningham et al, 2002), TreeTagger (Helmut, 1994) et RASP (Briscoe et al, 2002) et sur nos propres extensions dédiées à la détection des relations sémantiques et l\u0027extraction des instances des concepts de UMLS.\nDans chaque phrase où il détecte une instance d\u0027une relation sémantique de UMLS, MeatAnnot essaie d\u0027extraire les instances des concepts liés par cette relation et génère une annotation décrivant cette interaction.\nLa méthode de génération est composée de trois phases : Phase 1 : Détection des relations Dans cette phase nous avons utilisé JAPE (Cunningham et al, 2002), un langage basé sur les expressions régulières et qui offre la possibilité de créer des grammaires permettant l\u0027extraction d\u0027informations d\u0027un texte traité par GATE. Pour chaque relation de UMLS (interacts_with, plays_role…), nous avons créé manuellement une grammaire permettant d\u0027extraire toutes ses occurrences dans le texte. Nous nous sommes basés sur l\u0027analyse de telles occurrences dans un corpus initial d\u0027articles scientifiques fournis par les biologistes.\nL\u0027exemple ci-dessous montre une grammaire permettant la détection des instances de la relation sémantique « Has an effect » dans toutes ces formes lexicales (has an effect, had effects, have a positive effect…).\n{Token.lemme \u003d\u003d \"have\"} | {SpaceToken} ({Token.string \u003d\u003d \"a\"}| {Token.string \u003d\u003d \"an\"})? ({SpaceToken})? ({Token.string \u003d\u003d \"additive\"} | ({Token.string \u003d\u003d \"synergistic\"}| {Token.string \u003d\u003d \"inhibitory\"}| {Token.string \u003d\u003d \"greater\"} | {Token.string \u003d\u003d \"functional\"} | {Token.string \u003d\u003d \"protective\"}| {Token.string \u003d\u003d \"monogenic\"}| {Token.string \u003d\u003d \"positive\"})? Après ces deux phases, vient l\u0027extraction des termes candidats. MeatAnnot utilise une fenêtre de taille quatre (quatre mots successifs peuvent représenter un terme) et pour chaque terme candidat: s\u0027il appartient à UMLS, il passe au mot suivant, sinon, la fenêtre d\u0027extraction est diminuée jusqu\u0027à ce qu\u0027elle devienne nulle. L\u0027interrogation de UMLS se fait à travers le serveur de connaissances UMLSKS qui offre l\u0027accès à toutes les ressources de UMLS et qui permet de les interroger et d\u0027y naviguer à distance. UMLSKS nous renvoie une réponse en XML (si le terme existe dans le métathesau-rus). Ce résultat est analysé par MeatAnnot qui en extrait toutes les informations nécessaires sur le terme (type sémantique, définition…). L\u0027utilisation de ce serveur nous a offert une analyse linguistique plus fine car ce dernier traite quelques variations linguistiques simples (« development of lung » est reconnu comme « lung development ») et qui complètent le traitement fait par MeatAnnot à savoir la lemmatisation (récupérer la racine des mots).\nPhase 3 : Génération de l\u0027annotation Dans cette phase, MeatAnnot utilise le module RASP, qui affecte à chaque mot son rôle linguistique dans la phrase (sujet, objet…), ce qui permet d\u0027identifier les concepts liés par la relation : Pour chaque relation détectée, MeatAnnot vérifie si les sujets et les objets sont des instances de concepts de UMLS et génère une annotation décrivant l\u0027instance de la relation.\nL\u0027exemple ci-dessous résume les différentes étapes. Considérons la phrase suivante : \"In vitro assays demonstrated that only p38alpha and p38beta are inhibited by csaids.\"\nÉtape 1: En appliquant les grammaires d\u0027extraction de relations sur cette phrase, MeatAnnot détecte la présence de la relation « inhibits » (appartenant à l\u0027ontologie UMLS). \nFIG. 3 -Résultat de RASP\n-180 -RNTI-E-6 p38alpha et p38beta sont détectés comme étant les objets de la relation inhibits. csaids est détecté comme étant le sujet.\nMeatAnnot génère ensuite une annotation RDF 5 pour chacune des instances.\n\u003cm: Pharmacologic_Substance rdf:about\u003d\u0027csaids#\u0027\u003e \u003cm:inhibits\u003e \u003cm:Gene_or_Genome rdf:about\u003d\u0027p38alpha#\u0027/\u003e \u003c/m:inhibits \u003e \u003cm:inhibits \u003e \u003cm: Gene_or_Genome rdf:about\u003d\u0027p38beta#\u0027/\u003e \u003c/m:inhibits \u003e \u003c/m:Pharmacologic_Substance\u003e\nFIG. 4 -Exemple d\u0027annotation RDF générée par MeatAnnot\nToutes les annotations décrivant les interactions dans un article sont stockées dans un ré-pertoire contenant les articles fournis par le biologiste. Ces annotations sont ensuite utilisées, soit pour faire de la recherche documentaire (retrouver un article parlant d\u0027un gène particulier ou d\u0027un phénomène biologique), soit dans un scénario plus complexe de recherche d\u0027informations, comme la recherche de relations entre gènes ou autres entités biomédicales.\nValidation des annotations\nAfin de valider nos annotations, nous avons adopté une approche centrée utilisateur : nous avons choisi au hasard un corpus de test (2540 phrases) parmi les documents fournis par les biologistes et nous avons présenté les suggestions proposées par MeatAnnot aux biologistes à travers une interface de validation pour qu\u0027ils évaluent leur qualité. Cette interface a été conçue de manière à présenter les annotations dans un format compréhensible (textuel) pour les biologistes, qui ne sont pas spécialistes de RDF.\nEtant dans un contexte de recherche d\u0027informations (RI), nous nous sommes basés sur des mesures classiques de RI et nous les avons adaptés à notre cas.\nAu cours de cette phase, nous avons remarqué aussi que quelques suggestions de MeatAnnot sont considérées correctes mais inutiles pour les biologistes car elles décrivent soit des connaissances de base soit des connaissances vagues. Nous avons donc introduit une nouvelle mesure de qualité nommée utilité pour mesurer le taux des suggestions utiles. La troisième colonne décris le nombre de relations existant dans le texte mais que MeatAnnot n\u0027a pas pu extraire. Ce silence est dû dans certains cas aux erreurs générées par les outils de TALN mais principalement aux relations déduites par les biologistes en lisant la phrase mais qui ne peuvent pas être générées automatiquement.\nMesures\nExample de phrase: \"Upon interferon-gamma induction, after viral infection for example, a regulator of the proteasome, PA28 plays a role in antigen processing.\"\nDans cet exemple, MeatAnnot extrait automatiquement la relation \"PA28 plays_role antigen processing\", mais le biologiste en lisant la phrase peut déduire, en utilisant ses connaissances implicites, une autre relation qui est \"interferon-gamma have_effect PA28\".\nEnfin, MeatAnnot a une bonne utilité puisque 96% des suggestions correctes sont considérées utiles par les biologistes. Ces résultats montrent bien que MeatAnnot génère des annotations de bonne qualité, condition essentielle dans un contexte de recherche d\u0027informations.\nL\u0027utilisation des annotations : MeatSearch\nLe but étant d\u0027utiliser les annotations générées par MeatAnnot ainsi que celles éditées par MeatEditor afin de faciliter la validation/l\u0027interprétation des résultats des expériences, nous avons développé le système MeatSearch basé sur le moteur CORESE (Corby et al, 2004)  MeatSearch traduit les résultats de CORESE en présentation graphique et/ou textuelle qui est plus compréhensible par les biologistes. Il fournit aussi des informations complémen-taires, telles que le document ou la phrase à partir desquels elle est extraite, les auteurs et les personnes qui ont validé l\u0027annotation ou fourni les articles. Cette richesse en information et cette traçabilité des annotations offrent de très intéressants scénarios d\u0027utilisation.\nL\u0027utilisation de CORESE\nPour la formalisation de nos ontologies ainsi que nos annotations, nous avons choisi les langages RDFS et RDF, qui sont deux recommandations du W3C, respectivement pour la représentation des ontologies légères et pour la description des ressources du web en utilisant les annotations basées sur les ontologies.\nCe choix nous a permis d\u0027utiliser CORESE afin de permettre de: ? Naviguer dans la base d\u0027annotations en tenant compte de la structure des ontologies.\n? Ajouter des règles qui complètent la base d\u0027annotations. ? Raisonner sur des d\u0027annotations construites à partir de sources différentes et hétérogè-nes afin de déduire des connaissances à la fois implicites et explicites sur un gène. ? Utiliser différents niveaux d\u0027accès (admin, public, groupe…) à la base d\u0027annotations.\nExemples d\u0027utilisation\nCORESE fournit un langage de requêtes pour les données RDF qui est proche du langage SPARQL 6 en cours d\u0027élaboration au W3C; Ce langage de requêtes permet d\u0027écrire des requêtes composées de combinaisons booléennes de triplets RDF.\nComme exemple, la requête suivante permet de retrouver toutes les relations entre le gène « cav3. Notre approche propose des solutions à quelques problèmes posés dans la discussion finale du groupe de travail du W3C dans le domaine des sciences de la vie 7 :\n? \nTravaux connexes\nLa méthode sur laquelle MeatAnnot repose peut être comparée avec (a) les travaux exploitant l\u0027extraction d\u0027informations dans le domaine biomédical (Alamarguy et al, 2005) (Staab, 2002) (b) ceux sur la génération d\u0027annotations sémantiques pour le web sémantique (Handschuh et al, 2003). Reposant sur des techniques linguistiques, notre approche diffère des méthodes basées sur les techniques d\u0027apprentissage proposées par (Nédellec, 2002). Contrairement à (Golebiowska et al, 2001) notre approche permet de créer des annotations consistantes, non seulement, en des instances de concepts mais en des instances de relations, et le tout en reposant sur une ontologie déjà existante. Ces instances de relations peuvent relier les différents concepts de l\u0027ontologie et pas seulement les gènes ou les protéines comme décrit dans (Proux, 2000).\nPar rapport aux approches linguistiques pour extraire des relations sémantiques (Séguéla, 1999), nous ne visons pas l\u0027aide à la création ou enrichissement d\u0027une ontologie mais plutôt l\u0027extraction d\u0027informations à partir de textes pour générer des annotations sémantiques sur lesquelles raisonner pour la recherche d\u0027information.\nLe couple MeatAnnot/MeatSearch qui permet de générer des annotations sémantiques basées sur une ontologie et extraites à partir des textes et qui offre un système de recherche sémantique sur ces annotations, a plusieurs points communs avec le système proposé par (Muller et al, 2004) qui repose sur une ontologie mais ne l\u0027utilise pas pour faire la recherche.\nPerspectives\nComme perspective pour ces travaux, nous allons étudier l\u0027extraction d\u0027information à partir des graphiques et les tableaux, compte tenu de leur importance pour les biologistes, de manière à intégrer un nouveau module à MeatAnnot pour les prendre en compte. Nous approfondirons aussi les problèmes de la gestion de l\u0027évolution de UMLS, ce qui impliquera d\u0027approfondir l\u0027évolution des annotations. Enfin, nous sommes en train d\u0027étudier avec les biologistes plusieurs scénarios d\u0027utilisation avec des requêtes typiques afin de leur faciliter l\u0027utilisation de notre système et la navigation contextuelle dans la base d\u0027annotations. \nRéférences\n"
  },
  {
    "id": "1097",
    "text": "Introduction\nL\u0027analyse du comportement des utilisateurs d\u0027un site Web, également connue sous le nom de Web Usage Mining, est un domaine de recherche qui consiste à adapter des techniques de fouille de données sur les enregistrements contenus dans les fichiers logs d\u0027accès Web (ou fichiers \"access log\") afin d\u0027en extraire des relations entre les différentes données stockées Cooley et al. (1999), Masseglia et al. (2003), Mobasher et al. (2002), Spiliopoulou et al. (1999). Ces derniers regroupent des informations sur l\u0027adresse IP de la machine, l\u0027URL demandée, la date, et d\u0027autres renseignements concernant la navigation de l\u0027utilisateur. Parmi les méthodes développées, celles qui consistent à extraire des motifs séquentiels Agrawal et Srikant (1995) s\u0027adaptent particulièrement bien au cas des logs mais dépendent du découpage qui est fait des données. Ce découpage provient soit d\u0027une décision arbitraire de produire un log tous les x jours (e.g. un log par mois), soit d\u0027un désir de trouver des comportements particuliers (e.g. les comportements des internautes du 15 novembre au 23 décembre lors des achats de Noël). Pour comprendre l\u0027enjeu de ces travaux, prenons l\u0027exemple d\u0027étudiants connectés lors d\u0027une séance de TP. Imaginons que ces étudiants soient répartis en 2 groupes. Le groupe 1 était en TP le lundi 31 janvier. Le groupe 2 en revanche était en TP le mardi 1 er février. Chacun de ces\nMotifs séquentiels et Web Usage Mining\nCe paragraphe expose et illustre la problématique liée à l\u0027extraction de motifs séquentiels dans de grandes bases de données. Il reprend les différentes définitions proposées dans Agrawal et Srikant (1995), Masseglia et al. (1998 Les principes généraux de l\u0027utilisation des motifs séquentiels dans le cas des logs d\u0027accès Web sont similaires à ceux du processus d\u0027extraction de connaissances exposé dans Fayad et al. (1996). Les données brutes sont collectées dans des fichiers access log des serveurs Web. La démarche se décompose en trois phases principales (prétraitement, fouille de données et interprétation par l\u0027utilisateur des résultats obtenus).\nDans les grandes lignes, notre objectif est d\u0027énumérer l\u0027ensemble des périodes issues du log à analyser afin de déterminer quelles sont celles qui contiennent des motifs séquentiels fréquents. Soit C l\u0027ensemble des clients du log et D l\u0027ensemble des dates enregistrées.\nDéfinition 3 L\u0027ensemble P des périodes possibles sur le log est défini de la manière suivante :\nDans la définition suivante, nous considérons que d min (c) et d max (c) sont les dates d\u0027entrée et de sortie de c dans le log (première et dernière action enregistrées pour c). \nt).}\nDans la définition 5, la condition 1 exprime le fait qu\u0027il n\u0027existe pas de période plus grande qui soit en \"contact\" avec P stab et qui concerne les mêmes clients. La condition 2 exprime le fait que toutes les périodes de P stab concernent les mêmes clients.\nDéfinition 6\nUne période stable p est dite dense si C p contient au moins un motif séquentiel fréquent respectant le support minimum spécifié par l\u0027utilisateur proportionnellement à |C p |.\nExtraire les motifs séquentiels fréquents sur chacune de ces périodes avec une méthode classique n\u0027est pas une solution envisageable en raison du nombre de périodes stables (environ 2 millions pour 14 mois de log dans nos expérimentations). Dans la mesure où notre proposition est basée sur une heuristique, notre but est de fournir un résultat répondant aux critères suivants : Pour chaque période p appartenant à l\u0027historique du log, soit resultatReel le résul-tat à obtenir (le résultat qu\u0027obtiendrait un algorithme de fouille de données qui explore tout l\u0027ensemble des solutions après analyse des clients de C p ). resultatReel est alors l\u0027ensemble des motifs séquentiels à trouver. Soit resultatP er les résultats obtenus par la méthode proposée dans cet article. Nous voulons minimiser taille(resultatP er)\nEn d\u0027autres termes, nous voulons trouver toutes les séquences apparaissant dans resultatReel tout en évitant que le résultat soit plus grand qu\u0027il ne le devrait (sinon l\u0027ensemble de toutes les séquences, de toutes les navigations, pourrait constituer un résultat, car il englobe le résultat réel). Cette heuristique emprunte aux algorithmes génétiques leur conception du voisinage, en y intégrant les propriétés des motifs fréquents pour optimiser les candidats proposés. La principale idée, sur laquelle PERIO se base, consiste à parcourir l\u0027ensemble P stab des périodes stables et, pour chaque période p de P stab , à générer des populations de candidats grâce aux items fréquents et aux opérateurs de voisinage. Ensuite ces candidats sont comparés avec les séquences de C p afin d\u0027évaluer leur pertinence (ou tout au moins leur distance par rapport à une séquence fréquente). L\u0027heuristique PERIO est décrite par l\u0027algorithme suivant : ? taille(c) ; End algorithm Noter Pour chaque période de P stab , PERIO génère les nouveaux candidats et compare ensuite chacun des candidats aux séquences de C p . La comparaison consiste à obtenir un pourcentage qui représente la distance entre la séquence s du candidat et chaque séquence c de C p . Si s est incluse dans c, le pourcentage sera de 100% et ce taux va décroitre avec l\u0027apparition de \"parasites\" (différences entre le candidat et la séquence de navigation). Pour évaluer cette distance, le pourcentage est obtenu en divisant la taille de la plus longue séquence commune (PLSC) entre s et c par la taille de s. Par exemple, si s, de taille 4, contient une sous-séquence de taille 3 en commun avec c, alors la note de s pour c sera de 3/4. De plus, dans le but d\u0027obtenir des séquences les plus longues possible, nous utilisons un algorithme qui favorise les séquences les plus grandes, si elles sont incluses dans c. D\u0027un autre côté, l\u0027algorithme sanctionne les séquences trop longues si elles ne sont pas incluses (plus la séquences est longue, plus sa distance à la séquence de navigation sera pénalisante). Pour prendre en compte tous ces paramètres, le calcul effectué pour comparer les candidats à chaque séquence de C p est décrit par l\u0027algorithme NOTER. Enfin, les candidats évalués par l\u0027algorithme NOTER seront insérés dans SP si leur support est supérieur au support minimum ou s\u0027ils correspondent à un \"critère de sélection naturelle\". Ce critère, spécifié par l\u0027utilisateur prend la forme d\u0027un pourcentage qui définit la distance entre le support du candidat et le support minimum. Dans un nombre de cas (choisis de façon aléatoire), les candidats dont le support correspond au critère peuvent être insérés dans SP afin d\u0027éviter à l\u0027heuristique PERIOD de converger vers un optimum local.\nAlgorithm\n-406 -RNTI-E-6\nExpérimentations\nFIG. 1 -Pics de fréquence pour un comportement sur une période longue\nLes programmes d\u0027extraction sont réalisés en C++ sur des machines de type PC équipés de processus pentium 2,1 Ghz et exploités par un système RedHat. Nous avons effectué nos expérimentations sur les logs de l\u0027Inria Sophia Antipolis. Ces logs sont découpés à raison de un log par jour. A la fin du mois, les logs journaliers sont regroupés sous forme d\u0027un log mensuel. Nous avons donc travaillé sur les 14 logs mensuels disponibles, que nous avons considérés comme un seul log global recouvrant 14 mois d\u0027enregistrements (de janvier 2004 à mars 2005). Ce log de 14 mois représente environ 14 Go de données. Il contient 3,5 millions de séquences (clients), la longueur moyenne de ces séquences est de 2, 68 et la longueur maximale est de 174 requêtes. Le log contient environ 2 millions de périodes et 300000 items. Le temps d\u0027exé-cution de PERIOD sur ce log est d\u0027environ 6 heures avec un support minimum de 2% (nous avons trouvé environ 400 groupes de comportements différents). Voici quelques exemples de comportements extraits :\nC1 \u003d\u003c(semir/restaurant) (semir/restaurant/consult.php) (semir/restaurant/index.php) (semir/restaurant/index.php)\u003e C2 \u003d\u003c(eg06) (eg06/dureve_040702.pdf) (eg06/fer_040701.pdf) (eg06)\u003e C3 \u003d\u003c(requete.php3) (requete.php3) (requete.php3)\u003e C4 \u003d\u003c(Hello.java) (HelloClient.java) (HelloServer.java)\u003e C5 \u003d\u003c(mimosa/fp/Skribe) (mimosa/fp/Skribe/skribehp.css) (mimosa/fp/Skribe/index-5.html)\u003e C6 \u003d\u003c(sgp2004) (navbar.css) (submission.html)\u003e C7 \u003d\u003c(css/inria_sophia.css) (commun/votre_profil_en.shtml) (presentation/chiffres_en.shtml) (actu/actu_scient_colloque_ encours_fr.shtml)\u003e Le comportement C1 correspond à une navigation typiquement périodique. En effet, la cantine de l\u0027Inria Sophia Antipolis a été en travaux pendant cette période et les membre de l\u0027unité pouvaient, via les pages web \"semir/restaurant\", commander leurs repas froids de la semaine. Le comportement C2 correspond à une navigation sur des pages relatives aux \"états généraux\" de la recherche. C3 se trouve sur le site du projet \"mascotte\" et C4 sur celui de \"oasis\". Ces deux comportements correspondent à la consultation de pages de TD/TP sur les sites des chercheurs qui les proposent. Le comportement C5 est interprété par l\u0027auteur des pages comme une conséquence de nombreux échanges en mars 2004 sur la mailing-list de Skribe. Avec le préfixe \"geometrica/events/\" pour C6. Le comportement C6 connaît deux pics (début avril et mi-avril). Il s\u0027agit d\u0027un comportement relatif à la soumission d\u0027articles pour le symposium sgp2004, dont la date limite de soumission était le 7 avril pour les résumés et le 14 avril pour les articles.\n"
  }
]